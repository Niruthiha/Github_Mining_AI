[
  {
    "issue_number": 38846,
    "title": "video_auto_processing.py breaks everything",
    "author": "lucasjinreal",
    "state": "open",
    "created_at": "2025-06-16T14:24:56Z",
    "updated_at": "2025-06-17T14:11:47Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nthe video preprocessor now breaks everytthing,\n\nI have a model which need Qwen2.5 VL preporssor, it's OK before, but now it can not load, breaks.\n\nMy model just need Qwen2.5 VL preprocessor, it doens't have Qwen2.5 VL arch, it's a customized model.\nBut now, the video prerproecssor will reading the config arch in my model, rather than just the preprocessor.config.json\n\nThis is weired.\n\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rena\nme the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n\n\n\n```\nfrom transformers import AutoProcessor, Qwen2_5_VLProcessor\n\nprocessor_path = './checkpoints/Qwen3-VL-2B-Unofficial'\na = Qwen2_5_VLProcessor.from_pretrained(\n                processor_path, trust_remote_code=True\n            )\nprint(a)\n```\n\nThis code can be used without `config.json` must be a registered model, but now, it need `confgi.sjon` must be a reigstered mdoel, even though I just need `preprocess_config.json`\n\nPlease remove the video processor code, it's useless and introduce many bug!\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nfrom transformers import AutoProcessor, Qwen2_5_VLProcessor\n\nprocessor_path = './checkpoints/Qwen3-VL-2B-Unofficial'\na = Qwen2_5_VLProcessor.from_pretrained(\n                processor_path, trust_remote_code=True\n            )\nprint(a)\n\n### Expected behavior\n\nfrom transformers import AutoProcessor, Qwen2_5_VLProcessor\n\nprocessor_path = './checkpoints/Qwen3-VL-2B-Unofficial'\na = Qwen2_5_VLProcessor.from_pretrained(\n                processor_path, trust_remote_code=True\n            )\nprint(a)",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "Fixed by https://github.com/huggingface/transformers/pull/38840"
      },
      {
        "user": "lucasjinreal",
        "body": "Will 4.53.1 introduce this fix? Also, does it still will need config.json to load the video processor when users might have a customized model and only need image preprocessor?"
      },
      {
        "user": "zucchini-nlp",
        "body": "Yes, it will be in the next v4.53 release and will not require to have `config.json` saved. For BC we'll check if there's an `image_processor` config first"
      }
    ]
  },
  {
    "issue_number": 38854,
    "title": "scale loss per token/local sequence for discrete system representation",
    "author": "wesboyt",
    "state": "open",
    "created_at": "2025-06-17T03:56:28Z",
    "updated_at": "2025-06-17T14:05:24Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nI have implemented a scaling loss for my discrete game usecase.\nI would like to be able to hand my model a batch with a map of sequences and their associated randomness(or importance) and then scale the loss relative to the next tokens predictability.\nMy implementation works but is very slow relative to: \nhttps://github.com/huggingface/transformers/blob/v4.52.3/src/transformers/trainer.py#L3795\n\n### Motivation\n\nMy system has highly variable regions that are truly random and prevent my models from converging without manual training which is incredibly inefficient.\n\n### Your contribution\n\nMy implementation:\n```\n       lookup = torch.zeros(self.vocab_size,).to(self.device)\n       lookup[self.x_token] = 2\n       lookup[self.y_token] = 2\n       lookup[self.z_token] = 1\n       full_range = range(1, self.max_length)\n       \n       for input_padded in batches:\n            loss_tracker = torch.zeros(batch_size, ).to(self.device)\n            for i in full_range:\n                label = input_padded[:,i]\n                logits = self.model(input_padded[:,:i]).logits[:,-1]\n                pre = i <= 26\n                if pre:\n                    modifier = torch.full((batch_size,), .05, dtype=torch.float32).to(self.device)\n                    loss = (loss_function(logits, label) * modifier).mean()\n                else:\n                    modifier = torch.full((batch_size,), .1, dtype=torch.float32).to(self.device)\n                    modifier[loss_tracker > 0] = 1.0\n                    loss = (loss_function(logits, label) * modifier).mean()\n                loss.backward()    \n                loss_tracker -= 1.0\n                loss_tracker[loss_tracker < 0] = 0\n                special_indexes = torch.argwhere(label == special_ids)\n                if i < self.max_length - 1:\n                    next = input_padded[:,i+1]\n                    next_lookup = hero_lookup[next]\n                    loss_tracker[special_indexes] += next_lookup[special_indexes]     \n             optimizer.step()\n             optimizer.zero_grad()  \n             lr_scheduler.step()      \n```\n\n\nBasically if its the begining of a sequence there is low information and extreme randomness, I lower my loss by 95%.\nOtherwise I use a loss tracker to denote if a subsequence is predictable and for how many tokens I should use full loss.\nI adjust less important sequences to 10% loss and keep important sequences at full loss.\nThis impl supports batching but is highly specific to my discrete system which I've turned into a onehot language.\nI believe there are many other use cases where identifying a token or sequence of tokens and altering the loss thereafter when the user understands the physics of the system being simulated is extremely useful.\nI improved accuracy for these desireable and rarer sequences from 88% to 96% in only 20000 sentences.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @wesboyt, scaling classification loss with the sequence entropy is a cool idea and could definitely be relevant in some domains, including DNA. However, it's quite advanced and extremely domain-specific, so I don't think we should add it to the `Trainer`! cc @sunmarc to confirm."
      },
      {
        "user": "wesboyt",
        "body": "Was just comparing the performance to trainer loop (which is roughly 15x faster).\nWould not intend to tell your team how or where to implement it.\n"
      }
    ]
  },
  {
    "issue_number": 38858,
    "title": "BLIP-2 regression in 4.52.1",
    "author": "ageron",
    "state": "open",
    "created_at": "2025-06-17T10:23:50Z",
    "updated_at": "2025-06-17T13:06:21Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.4\n- Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n- Python version: 3.11.13\n- Huggingface_hub version: 0.33.0\n- Safetensors version: 0.5.3\n- Accelerate version: 1.7.0\n- Accelerate config: \tnot found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): 2.18.0 (True)\n- Flax version (CPU?/GPU?/TPU?): 0.10.6 (gpu)\n- Jax version: 0.5.2\n- JaxLib version: 0.5.1\n- Using distributed or parallel set-up in script?: no\n- Using GPU in script?: no\n- GPU type: Tesla T4\n\n\n### Who can help?\n\n@amyeroberts, @qubvel\n\n### Information\n\n- [x] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI'm getting different results from `Blip2ForConditionalGeneration` depending on the Transformers library version. Showing the usual [photo of two cats sleeping on a couch](http://images.cocodataset.org/val2017/000000039769.jpg), I get the following results:\n\n* ✅ 4.51.3: \"two cats laying on a couch\"\n* ❌ 4.52.1: \"a\"\n* ❌ 4.52.2: \"a\"\n* ❌ 4.52.3: \"a\"\n* ❌ 4.52.4: \"a woman is standing in front of a pink background\"\n\nHere's a short code example to reproduce the issue (see [this gist](https://colab.research.google.com/gist/ageron/8a3c87d9af26f0517fcf813af1ab567d/blip-2-issue.ipynb)):\n\n```python\nimport torch\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nimport requests\nfrom PIL import Image\n\ndevice = \"cpu\"  # change to \"cuda\" or \"mps\" if needed\nmodel_id = \"Salesforce/blip2-opt-2.7b\"\nblip2_processor = Blip2Processor.from_pretrained(model_id)\nblip2_model = Blip2ForConditionalGeneration.from_pretrained(\n    model_id, device_map=device, torch_dtype=torch.float16)\n\nimage_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # two cats\nimage = Image.open(requests.get(image_url, stream=True).raw)\ninputs = blip2_processor(images=image, return_tensors=\"pt\")\ninputs = inputs.to(device, dtype=torch.float16)\nwith torch.no_grad():\n    generated_ids = blip2_model.generate(**inputs)\n\ngenerated_text = blip2_processor.batch_decode(generated_ids,\n                                              skip_special_tokens=True)\nprint(generated_text)\n```\n\nNote: I thought it might be similar to #38514 but it doesn't seem so. I tried applying #38510 but it didn't solve the issue.\n\n### Expected behavior\n\nI'd like the same behavior as in 4.51.3, i.e., printing \"two cats laying on a couch\" rather than \"a\" or \"a woman is standing in front of a pink background\".\nThanks!",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @zucchini-nlp"
      },
      {
        "user": "qubvel",
        "body": "Hey @ageron, thanks for opeining an issue!\n\nI can confirm on `4.52.4` the output is `\"a woman is standing in front of a pink background\"`, however on `main` it seems back to normal (`\"two cats laying on a couch\"`). Maybe @zucchini-nlp can reference a PR where it was fixed\n"
      },
      {
        "user": "zucchini-nlp",
        "body": "Hmm if it works on `main` should be fine. I don't know what might have caused the issue and fixed it, from git history seem the fix we had recently for `keep_in_fp32_modules` (https://github.com/huggingface/transformers/pull/38510) which is the only last change not added in patch release\n\nMaybe the precision was is low in 4.52 but it doesn't throw dtype mismatch, and now it is fixed"
      }
    ]
  },
  {
    "issue_number": 37345,
    "title": "Improve `auxiliary_in_channels` default behavior in UperNet",
    "author": "simonreise",
    "state": "closed",
    "created_at": "2025-04-07T14:08:53Z",
    "updated_at": "2025-06-17T12:57:43Z",
    "labels": [
      "Feature request",
      "Vision"
    ],
    "body": "### Feature request\n\nNow the number of input channels in auxiliary UperNet FCN Head is controlled by `auxiliary_in_channels` parameter in UperNetConfig, which is set to 384 by default. Not every backbone have 384 channels by default, so sometimes user have to set this parameter manually.\n\nI propose to change the default model behaviour to automatically handle the number of input channels but keep the `auxiliary_in_channels` parameter with `None` as a default value so it still can be set manually by user if they need it.\n\n### Motivation\n\nNow the number of input channels in auxiliary UperNet FCN Head is controlled by `auxiliary_in_channels` parameter in UperNetConfig, which is set to 384 by default. It works nice with some backbones (like Swin and Convnext) but crashes with backbones like BiT or ResNet.\n\nSo, now, if user wants to use some backbones, they have to set the parameter manually, which is frustrating and it is  not always straightforward which value you should use with every specific backbone.\n\n### Your contribution\n\nHowever, in most cases this behaviour can be automated. \n\nI propose to set `auxiliary_in_channels` default value to None, so by default the model will handle the number of input channels automatically, but the parameter is still can be set by user if they need it.\n\nI propose to add `in_channels` parameter to `self.auxiliary_head` and set `self.in_channels` from `config.auxiliary_in_channels` if it is not None or from `in_channels` if it is None.\n\n\n`UperNetForSemanticSegmentation.__init__`\n```\n# Old\n# self.auxiliary_head = UperNetFCNHead(config) if config.use_auxiliary_head else None\n# New\nself.auxiliary_head = UperNetFCNHead(config, in_channels=self.backbone.channels) if config.use_auxiliary_head else None\n```\n\n```\nclass UperNetFCNHead(nn.Module):\n  def __init__(\n          self, config, in_channels, in_index: int = 2, kernel_size: int = 3, dilation: Union[int, Tuple[int, int]] = 1\n      ) -> None:\n          super().__init__()  \n          self.config = config\n          # Old\n          #self.in_channels = config.auxiliary_in_channels\n          # New\n          self.in_channels = in_channels[in_index] if config.auxiliary_in_channels is None else config.auxiliary_in_channels\n```\n\nI tested it with BEiT, BiT, ConvNeXT, ConvNeXTV2, DINOV2, DINOV2WithRegisters, FocalNet, Hiera, PVTV2, ResNet, Swin, SwinV2 and ViTDet backbones.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @qubvel @NielsRogge "
      },
      {
        "user": "qubvel",
        "body": "Hey @simonreise, looks good to me, feel free to open a PR and ping me for review"
      },
      {
        "user": "simonreise",
        "body": "@qubvel Submitted a PR #37540 , could you please review?"
      }
    ]
  },
  {
    "issue_number": 37939,
    "title": "`ConditionalDetrImageProcessor` still accepts the deprecated parameter `max_size`",
    "author": "arjunaskykok",
    "state": "open",
    "created_at": "2025-05-03T14:15:32Z",
    "updated_at": "2025-06-17T12:51:15Z",
    "labels": [
      "Good First Issue",
      "Vision"
    ],
    "body": "At the `__init__` method of [ConditionalDetrImageProcessor](https://github.com/huggingface/transformers/blob/main/src/transformers/models/conditional_detr/image_processing_conditional_detr.py) file, we still accept the deprecated parameter, `max_size`:\n\n```python\n        if \"max_size\" in kwargs:\n            logger.warning_once(\n                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n                \"Please specify in `size['longest_edge'] instead`.\",\n            )\n            max_size = kwargs.pop(\"max_size\")\n        else:\n            max_size = None if size is None else 1333\n```\n\nBut the version of `transformers` is already `4.52.0.dev0` (from `src/transformers/__init__.py`).\n\n`ConditionalDetrImageProcessorFast` also suffers the same issue.\n\nWe should remove the parameter, no?\n\ncc: @amyeroberts, @qubvel",
    "comments": [
      {
        "user": "NielsRogge",
        "body": "Seems like it's also the case for models like Deformable DETR,  Grounding DINO, RT-DETR YOLOS: https://github.com/search?q=repo%3Ahuggingface%2Ftransformers+%22will+be+removed+in+v4.26%22&type=code. Would be great to remove it for all"
      },
      {
        "user": "qubvel",
        "body": "@NielsRogge @arjunaskykok that's indeed an annoying BC issue.\nThe problem is that we still have very popular models which have `max_size` in configs on the Hub, see [this](https://github.com/huggingface/transformers/pull/35090#issuecomment-2522730194) and [this](https://github.com/huggingface/transformers/pull/35090#issuecomment-2557243571) comments in the related PR.\nPR's are opened on the Hub to update them, but no reaction for most of them so far."
      },
      {
        "user": "demoncoder-crypto",
        "body": "Hi is there any part which I can contribute too which is currently not under way. Would love tot work on it thanks. "
      }
    ]
  },
  {
    "issue_number": 38851,
    "title": "Should `compute_metrics` only run on the main process when doing DDP?",
    "author": "TIE666",
    "state": "open",
    "created_at": "2025-06-17T00:09:43Z",
    "updated_at": "2025-06-17T12:32:50Z",
    "labels": [],
    "body": "Hi,  I want to know when doing training and evaluation on a multi-GPU setup (DDP using trainer and accelerate), does `compute_metrics` only need to be run on the main process?\n\nThe reason being that `trainer` itself already does `gather_for_metrics` ([here](https://github.com/huggingface/transformers/blob/v4.51-release/src/transformers/trainer.py#L4373)), which I suppose should collect all predictions (logits) and labels across processes, running `compute_metrics` from multiple processes again will be doing duplicated work, no?\n\nto add:\nI am using `batch_eval_metrics`, where I first spotted that if I run the training script (modified version of `run_clm.py`) with `accelerate launch`, the `compute_metrics` is always called multiple times, but the logits from `EvalPrediction` for each call is `per_device_eval_batch_size` * number of GPU I am using.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @sunmarc"
      }
    ]
  },
  {
    "issue_number": 38843,
    "title": "Error when create ModernBert model with flash attention  TypeError: RotaryEmbedding.__init__() got an unexpected keyword argument 'pos_idx_in_fp32'",
    "author": "KabaevAnton",
    "state": "open",
    "created_at": "2025-06-16T11:11:47Z",
    "updated_at": "2025-06-17T12:30:48Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nlinux ubuntu 22.04\nPython 3.12.4 \ntransformers 4.52.4\nflash-attn 2.8.0.post2\n\n### Who can help?\n\n@ArthurZucker\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. pip install flash-attn\n2.  config = AutoConfig.from_pretrained(\"answerdotai/ModernBERT-base\")\n3. model = AutoModelForMaskedLM.from_config(config)\n4. TypeError: RotaryEmbedding.__init__() got an unexpected keyword argument 'pos_idx_in_fp32'\n\nThe issue are as folows: when you try to create a ModernBert model with flash attention it uses \n\n` class ModernBertUnpaddedRotaryEmbedding(RotaryEmbedding):\n    \"\"\"\n    The rotary position embeddings applied directly to unpadded sequences.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        base: float = 10000.0,\n        max_seqlen: Optional[int] = None,\n        device: Optional[torch.device] = None,\n        dtype: Optional[torch.dtype] = None,\n    ):\n        \"\"\"\n        max_seqlen: if max_seqlen, device, and dtype are provided, we precompute the cos_sin_cache\n            up to max_seqlen. If the max_seqlen, device, or dtype during training/inference differ,\n            the cos_sin_cache will be recomputed during the forward pass.\n        \"\"\"\n        super().__init__(dim=dim, base=base, pos_idx_in_fp32=True, device=device, interleaved=False)\n        self.max_seqlen = max_seqlen\n\n        if max_seqlen is not None and device is not None and dtype is not None:\n            self._update_cos_sin_cache(max_seqlen, device=device, dtype=dtype)\n`\nModernBertUnpaddedRotaryEmbedding set pos_idx_in_fp32 parameter into its super class witch is RotaryEmbedding but the only parameter it have is dim\n\n`class RotaryEmbedding(torch.nn.Module):\n    \"\"\"\n    Rotary position embeddings based on those in\n    [RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer). Query and keys are transformed by rotation\n    matrices which depend on their relative positions.\n    \"\"\"\n\n    def __init__(self, dim: int):\n        super().__init__()\n        # Generate and save the inverse frequency buffer (non trainable)\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n        inv_freq = inv_freq\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        self._seq_len_cached = None\n        self._cos_cached = None\n        self._sin_cached = None\n`\n\n### Expected behavior\n\nWorks without error",
    "comments": [
      {
        "user": "toby-clark4",
        "body": "@KabaevAnton the most recent version of flash-attn removed the pos_idx_in_fp32 parameter - downgrading to \n`flash-attn==2.7.4.post1` fixed the bug for me. \n\n\n(Alternatively you can remove the pos_idx_in_fp32 flag in your transformers source code as this was set to True by default even in older versions of flash-attn)."
      },
      {
        "user": "Rocketknight1",
        "body": "cc @arthurzucker should we remove the arg if it's been deleted from `flash-attn`?"
      }
    ]
  },
  {
    "issue_number": 37545,
    "title": "Expected all tensors to be on the same device, but found at least two devices",
    "author": "ootsuka-repos",
    "state": "open",
    "created_at": "2025-04-16T01:27:23Z",
    "updated_at": "2025-06-17T12:01:06Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nTRLのSFTrainerが動かない。\nRTX8000*2 4090D*2 num4 gpu\n\npip install transformers==4.51.3 NG\npip install transformers==4.49.0 OK\n\ntorch2.5.1+cuda12.4\n\n\n-error text-\nExpected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:0!\n  File \"C:\\Users\\admin\\Desktop\\llm_sft\\train\\SFT.py\", line 185, in main\n    trainer.train(\n  File \"C:\\Users\\admin\\Desktop\\llm_sft\\train\\SFT.py\", line 190, in <module>\n    main()\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:0!",
    "comments": [
      {
        "user": "ootsuka-repos",
        "body": "The version of TRL is known to be irrelevant.\nI get errors from Transformers version 4.49.0 and up.\nI don't know why I can't do what I used to be able to do."
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @ootsuka-repos, you'll need to give us some minimal code to reproduce the issue!"
      },
      {
        "user": "ootsuka-repos",
        "body": "@Rocketknight1 \ncode \n\n```\nimport os\nimport random\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n\n# WANDBログイン（キー削除済み、必要なら環境変数や手動入力で対応）\nimport wandb\n# wandb.login(key=\"your_wandb_api_key_here\")\nwandb.login()\n\nimport json\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import Dataset\nfrom huggingface_hub import login\nfrom transformers.trainer_callback import TrainerCallback\nfrom trl import SFTTrainer, SFTConfig\nfrom tqdm import tqdm\nfrom datetime import datetime\n\n# 定数（トークンとパスはサンプル/相対パスに変更）\nHF_TOKEN = \"your_hf_token_here\"\nDATA_FILE_PATH = \"dataset/data.jsonl\"\nMODEL_NAME = \"models/magnum_latest_checkpoint\"\n\ncurrent_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nmodels_save_path = f\"models/magnum_{current_time}\"\nMAX_TOKEN = 3500\nMIN_TOKEN = 2000\n\n# Hugging Faceログイン\nlogin(token=HF_TOKEN)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)\n\ndef count_tokens(text):\n    return len(tokenizer.encode(text, add_special_tokens=False))\n\ndef load_custom_data(file_path):\n    data_with_tokens = []\n    max_token_count = 0\n    skipped_large_count = 0\n    skipped_small_count = 0\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n\n    for line in tqdm(lines, desc=\"Processing Data\", unit=\"line\"):\n        item = json.loads(line)\n        text = f\"\"\"アダルトASMRのシナリオを作成してください\nタイトル:{item['title']}\n\nタグ:{item['tags']}\n\nシナリオ:{item['transcription']}\"\"\"\n        \n        token_count = count_tokens(text)\n        if token_count > MAX_TOKEN:\n            skipped_large_count += 1\n            continue\n        if token_count < MIN_TOKEN:\n            skipped_small_count += 1\n            continue\n\n        max_token_count = max(max_token_count, token_count)\n        data_with_tokens.append((item, token_count))\n\n    random.shuffle(data_with_tokens)\n    sorted_data = [item for item, _ in data_with_tokens][:250]\n\n    print(f\"最大トークン数: {max_token_count}\")\n    print(f\"データ数: {len(sorted_data)}\")\n    print(f\"スキップ（大）: {skipped_large_count}\")\n    print(f\"スキップ（小）: {skipped_small_count}\")\n\n    filtered_file_path = \"dataset/data_filtered.jsonl\"\n    with open(filtered_file_path, \"w\", encoding=\"utf-8\") as f_out:\n        for item in sorted_data:\n            f_out.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n    print(f\"保存先: {filtered_file_path}\")\n\n    return sorted_data\n\nclass SaveModelAndTokenizerCallback(TrainerCallback):\n    def __init__(self, model, tokenizer, output_dir, save_epoch_interval=1):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.output_dir = output_dir\n        self.save_epoch_interval = save_epoch_interval\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        checkpoint_dir = os.path.join(self.output_dir, f\"checkpoint-{state.global_step}\")\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        self.tokenizer.save_pretrained(checkpoint_dir)\n\ndef main():\n    data = load_custom_data(DATA_FILE_PATH)\n\n    dataset = Dataset.from_dict({\n        \"Title\": [item[\"title\"] for item in data],\n        \"Tags\": [item[\"tags\"] for item in data],\n        \"Transcription\": [item[\"transcription\"] for item in data],\n    })\n\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        device_map=\"balanced\",\n        use_cache=False,\n        torch_dtype=torch.float16,\n        attn_implementation=\"eager\",\n    )\n\n    def formatting_prompts_func(example):\n        prompt = f\"\"\"アダルトASMRのシナリオを作成してください\n\nタイトル:{example['Title']}\n\nタグ:{example['Tags']}\"\"\"\n        infe = f\"\"\"シナリオ:{example['Transcription']}\"\"\"\n        messages = [\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"assistant\", \"content\": infe}\n        ]\n        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n\n    save_model_and_tokenizer_callback = SaveModelAndTokenizerCallback(\n        model, tokenizer, models_save_path, save_epoch_interval=1\n    )\n\n    training_args = SFTConfig(\n        output_dir=models_save_path,\n        num_train_epochs=15,\n        learning_rate=1e-5,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=1,\n        max_grad_norm=1.0,\n        gradient_checkpointing=True,\n        logging_steps=1,\n        save_strategy=\"epoch\",\n        lr_scheduler_type=\"cosine\",\n        optim=\"adamw_bnb_8bit\",\n        max_seq_length=MAX_TOKEN,\n        packing=False,\n        neftune_noise_alpha=5,\n    )\n\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        args=training_args,\n        formatting_func=formatting_prompts_func,\n        data_collator=data_collator,\n        callbacks=[save_model_and_tokenizer_callback],\n    )\n    \n    trainer.train()\n\nif __name__ == \"__main__\":\n    main()\n\n```"
      }
    ]
  },
  {
    "issue_number": 36979,
    "title": "[Community contributions] Model cards",
    "author": "stevhliu",
    "state": "open",
    "created_at": "2025-03-25T20:39:10Z",
    "updated_at": "2025-06-17T10:55:53Z",
    "labels": [
      "Good First Issue",
      "Good First Documentation Issue",
      "contributions-welcome"
    ],
    "body": "Hey friends! 👋 \n\nWe are currently in the process of improving the Transformers model cards by making them more directly useful for everyone. The main goal is to:\n\n1. Standardize all model cards with a consistent format so users know what to expect when moving between different model cards or trying to learn how to use a new model.\n2. Include a brief description of the model (what makes it unique/different) written in a way that's accessible to everyone.\n3. Provide ready to use code examples featuring the `Pipeline`, `AutoModel`, and `transformers-cli` with available optimizations included. For large models, provide a quantization example so its easier for everyone to run the model.\n4. Include an attention mask visualizer for currently supported models to help users visualize what a model is seeing (refer to #36630) for more details.\n\nCompare the before and after model cards below:\n\n![Image](https://github.com/user-attachments/assets/590de86f-cfd2-4dd0-9167-83b7d19d858a)\n\nWith so many models in Transformers, we could really use some a hand with standardizing the existing model cards. If you're interested in making a contribution, pick a model from the list below and then you can get started!\n\n## Steps\n\nEach model card should follow the format below. You can copy the text exactly as it is!\n\n```md\n# add appropriate badges\n<div style=\"float: right;\">\n    <div class=\"flex flex-wrap space-x-1\">\n           <img alt=\"\" src=\"\" >\n    </div>\n</div>\n\n# Model name\n\n[Model name](https://huggingface.co/papers/...) ...\n\nA brief description of the model and what makes it unique/different. Try to write this like you're talking to a friend. \n\nYou can find all the original [Model name] checkpoints under the [Model name](link) collection.\n\n> [!TIP]\n> Click on the [Model name] models in the right sidebar for more examples of how to apply [Model name] to different [insert task types here] tasks.\n\nThe example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n\n<hfoptions id=\"usage\">\n<hfoption id=\"Pipeline>\n\ninsert pipeline code here\n\n</hfoption>\n<hfoption id=\"AutoModel\">\n\nadd AutoModel code here\n\n</hfoption>\n<hfoption id=\"transformers-cli\">\n\nadd transformers-cli usage here if applicable/supported, otherwise close the hfoption block\n\n</hfoption>\n</hfoptions\n\nQuantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n\nThe example below uses [insert quantization method here](link to quantization method) to only quantize the weights to __.\n\n# add if this is supported for your model\nUse the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src/transformers/utils/attention_visualizer.py#L139) to better understand what tokens the model can and cannot attend to.\n\n\\```py\nfrom transformers.utils.attention_visualizer import AttentionMaskVisualizer\n\nvisualizer = AttentionMaskVisualizer(\"google/gemma-3-4b-it\")\nvisualizer(\"<img>What is shown in this image?\")\n\\```\n\n# upload image to https://huggingface.co/datasets/huggingface/documentation-images/tree/main/transformers/model_doc and ping me to merge\n<div class=\"flex justify-center\">\n    <img src=\"\"/>\n</div>\n\n## Notes\n\n- Any other model-specific notes should go here.\n\n   \\```py\n    <insert relevant code snippet here related to the note if its available>\n   \\ ```\n\n```\n\nFor examples, take a look at #36469 or the [BERT](https://huggingface.co/docs/transformers/main/en/model_doc/bert), [Llama](https://huggingface.co/docs/transformers/main/en/model_doc/llama), [Llama 2](https://huggingface.co/docs/transformers/main/en/model_doc/llama2), [Gemma](https://huggingface.co/docs/transformers/main/en/model_doc/gemma3) 3, [PaliGemma](https://huggingface.co/docs/transformers/main/en/model_doc/paligemma), [ViT](https://huggingface.co/docs/transformers/main/en/model_doc/vit), and [Whisper](https://huggingface.co/docs/transformers/main/en/model_doc/whisper) model cards on the `main` version of the docs. \n\nOnce you're done or if you have any questions, feel free to ping @stevhliu to review. Don't add `fix` to your PR to avoid closing this issue.\n\nI'll also be right there working alongside you and opening PRs to convert the model cards so we can complete this faster together! 🤗 \n\n## Models\n\n- [ ] albert - #37753 \n- [x] align - #38072 \n- [x] altclip - #38306 \n- [x] aria - #38472 \n- [ ] audio_spectrogram_transformer - assigned to @KishanPipariya \n- [ ] autoformer.- #37231 \n- [x] aya_vision - #38749 \n- [ ] bamba\n- [ ] bark\n- [x] bart - #37858 \n- [ ] barthez\n- [ ] bartpho\n- [ ] beit\n- [x] bert\n- [ ] bert_generation\n- [ ] bert_japanese - assigned to @KeshavSingh29 \n- [x] bertweet - #37981 \n- [x] big_bird - #37959 \n- [ ] bigbird_pegasus\n- [x] biogpt - #38214 \n- [ ] bit\n- [ ] blenderbot\n- [ ] blenderbot_small\n- [ ] blip. - #38513 \n- [ ] blip_2 - assigned to @olccihyeon \n- [ ] bloom\n- [ ] bridgetower\n- [ ] bros\n- [x] byt5 - #38699 \n- [ ] camembert\n- [x] canine - #38631 \n- [ ] chameleon\n- [ ] chinese_clip\n- [ ] clap\n- [x] clip - #37040 \n- [ ] clipseg\n- [ ] clvp\n- [x] code_llama - #37115 \n- [ ] codegen\n- [x] cohere - #37056 \n- [ ] cohere2\n- [x] colpali - #37309 \n- [ ] conditional_detr\n- [ ] convbert - #38470 \n- [ ] convnext - assigned to @aleksmaksimovic \n- [ ] convnextv2 - assigned to @aleksmaksimovic\n- [ ] cpm\n- [ ] cpmant\n- [ ] ctrl - assigned to @Ishubhammohole \n- [ ] cvt - assigned to @sezan92 \n- [ ] dab_detr\n- [ ] dac\n- [ ] data2vec\n- [ ] dbrx\n- [ ] deberta - #37409 \n- [ ] deberta_v2\n- [ ] decision_transformer\n- [ ] deformable_detr\n- [ ] deit\n- [ ] deprecated\n- [x] depth_anything - #37065 \n- [ ] depth_pro\n- [ ] detr\n- [ ] dialogpt\n- [ ] diffllama\n- [ ] dinat\n- [x] dinov2 - #37104 \n- [ ] dinov2_with_registers\n- [x] distilbert - #37157 \n- [x] dit - #38721 \n- [x] donut - #37290 \n- [ ] dpr\n- [ ] dpt\n- [ ] efficientnet - assigned to @Sudhesh-Rajan27 \n- [x] electra - #37063 \n- [ ] emu3\n- [ ] encodec\n- [ ] encoder_decoder\n- [ ] ernie\n- [ ] esm\n- [x] falcon - #37184 \n- [x] falcon_mamba - #37253 \n- [ ] fastspeech2_conformer - #37377 \n- [ ] flaubert\n- [ ] flava\n- [ ] fnet\n- [ ] focalnet\n- [ ] fsmt\n- [ ] funnel\n- [ ] fuyu\n- [x] gemma - #37674 \n- [x] gemma2 - #37076 \n- [x] gemma3\n- [ ] git\n- [ ] glm\n- [ ] glpn\n- [ ] got_ocr2\n- [x] gpt2 - #37101 \n- [ ] gpt_bigcode\n- [x] gpt_neo - #38505 \n- [ ] gpt_neox - #38550 \n- [ ] gpt_neox_japanese\n- [ ] gpt_sw3\n- [ ] gptj\n- [x] granite - #37791 \n- [ ] granitemoe\n- [ ] granitemoeshared\n- [ ] grounding_dino\n- [ ] groupvit\n- [ ] helium\n- [ ] herbert\n- [ ] hiera\n- [ ] hubert\n- [ ] ibert\n- [ ] idefics\n- [ ] idefics2\n- [ ] idefics3\n- [ ] ijepa\n- [ ] imagegpt\n- [ ] informer\n- [ ] instructblip\n- [ ] instructblipvideo\n- [x] jamba - #37152 \n- [ ] jetmoe\n- [ ] kosmos2\n- [ ] layoutlm\n- [ ] layoutlmv2\n- [ ] layoutlmv3 - #37155 \n- [ ] layoutxlm\n- [ ] led\n- [ ] levit\n- [ ] lilt\n- [x] llama\n- [x] llama2\n- [ ] llama3 - assigned to @capnmav77 \n- [ ] llava\n- [ ] llava_next\n- [ ] llava_next_video\n- [ ] llava_onevision\n- [x] longformer - #37622 \n- [ ] longt5\n- [ ] luke\n- [ ] lxmert\n- [ ] m2m_100\n- [x] mamba - #37863 \n- [x] mamba2 - #37951 \n- [ ] marian\n- [ ] markuplm\n- [ ] mask2former\n- [ ] maskformer\n- [x] mbart - #37619 \n- [x] mbart50 - #37619\n- [ ] megatron_bert\n- [ ] megatron_gpt2\n- [ ] mgp_str\n- [ ] mimi\n- [x] mistral - #37156 \n- [ ] mistral3 - assigned to @cassiasamp \n- [ ] mixtral - assigned to @darmasrmez \n- [ ] mllama - #37647 \n- [ ] mluke\n- [x] mobilebert - #37256 \n- [x] mobilenet_v1 - #37948 \n- [x] mobilenet_v2 - #37948 \n- [ ] mobilevit\n- [ ] mobilevitv2\n- [x] modernbert - #37052 \n- [x] moonshine - #38711 \n- [ ] moshi\n- [ ] mpnet - assigned to @SanjayDevarajan03 \n- [ ] mpt\n- [ ] mra\n- [ ] mt5\n- [ ] musicgen\n- [ ] musicgen_melody\n- [ ] mvp\n- [ ] myt5\n- [ ] nemotron\n- [ ] nllb\n- [ ] nllb_moe\n- [ ] nougat\n- [ ] nystromformer\n- [ ] olmo\n- [x] olmo2 - #38394 \n- [ ] olmoe\n- [ ] omdet_turbo\n- [ ] oneformer\n- [x] openai - #37255 \n- [ ] opt\n- [ ] owlv2\n- [ ] owlvit\n- [x] paligemma\n- [ ] patchtsmixer\n- [ ] patchtst\n- [x] pegasus - #38675 \n- [ ] pegasus_x\n- [ ] perceiver\n- [ ] persimmon\n- [x] phi - #37583 \n- [ ] phi3 - assigned to @arpitsinghgautam \n- [ ] phi4_multimodal - #38830 \n- [ ] phimoe\n- [ ] phobert\n- [ ] pix2struct\n- [ ] pixtral - assigned to @BryanBradfo \n- [ ] plbart\n- [ ] poolformer\n- [ ] pop2piano\n- [ ] prompt_depth_anything\n- [ ] prophetnet\n- [ ] pvt\n- [ ] pvt_v2\n- [x] qwen2 - #37192 \n- [x] qwen2_5_vl - #37099 \n- [ ] qwen2_audio\n- [x] qwen2_moe - #38649 \n- [ ] qwen2_vl - assigned to @SaiSanthosh1508 \n- [ ] rag\n- [ ] recurrent_gemma\n- [ ] reformer\n- [ ] regnet\n- [ ] rembert\n- [ ] resnet - assigned to @BettyChen0616 \n- [x] roberta - #38777 \n- [ ] roberta_prelayernorm\n- [ ] roc_bert - #38835 \n- [x] roformer - #37946 \n- [ ] rt_detr\n- [ ] rt_detr_v2\n- [ ] rwkv\n- [ ] sam\n- [ ] seamless_m4t\n- [ ] seamless_m4t_v2\n- [ ] segformer - assigned to @GSNCodes \n- [ ] seggpt\n- [ ] sew\n- [ ] sew_d\n- [ ] shieldgemma2 - assigned to @BryanBradfo \n- [x] siglip - #37585 \n- [x] siglip2 - #37624 \n- [ ] smolvlm - assigned to @udapy \n- [ ] speech_encoder_decoder\n- [ ] speech_to_text\n- [ ] speecht5\n- [ ] splinter\n- [ ] squeezebert\n- [ ] stablelm\n- [ ] starcoder2\n- [ ] superglue\n- [ ] superpoint\n- [ ] swiftformer\n- [ ] swin - assigned to @BryanBradfo \n- [ ] swin2sr\n- [x] swinv2 - #37942 \n- [ ] switch_transformers\n- [x] t5 - #37261 \n- [ ] table_transformer\n- [ ] tapas\n- [ ] textnet\n- [ ] time_series_transformer\n- [ ] timesformer\n- [ ] timm_backbone\n- [ ] timm_wrapper\n- [ ] trocr\n- [ ] tvp\n- [ ] udop\n- [ ] umt5\n- [ ] unispeech\n- [ ] unispeech_sat\n- [ ] univnet\n- [ ] upernet\n- [ ] video_llava\n- [ ] videomae - assigned to @mreraser \n- [ ] vilt\n- [ ] vipllava\n- [ ] vision_encoder_decoder - assigned to @Bhavay-2001 \n- [ ] vision_text_dual_encoder\n- [ ] visual_bert\n- [x] vit\n- [x] vit_mae - #38302 \n- [ ] vit_msn\n- [ ] vitdet\n- [ ] vitmatte\n- [ ] vitpose - #38630 \n- [ ] vitpose_backbone\n- [x] vits - #37335 \n- [ ] vivit\n- [ ] wav2vec2 - assigned to @AshAnand34 \n- [ ] wav2vec2_bert - assigned to @AshAnand34 \n- [ ] wav2vec2_conformer - assigned to @AshAnand34 \n- [ ] wav2vec2_phoneme - assigned to @AshAnand34 \n- [ ] wav2vec2_with_lm - assigned to @AshAnand34 \n- [ ] wavlm\n- [x] whisper\n- [ ] x_clip\n- [ ] xglm\n- [x] xlm - #38595 \n- [x] xlm_roberta - #38596 \n- [x] xlm_roberta_xl - #38597 \n- [ ] xlnet\n- [ ] xmod\n- [ ] yolos\n- [ ] yoso\n- [ ] zamba\n- [ ] zamba2\n- [x] zoedepth - #37898 ",
    "comments": [
      {
        "user": "devesh-2002",
        "body": "Hi. I would like to work on model card for gemma 2."
      },
      {
        "user": "NahieliV",
        "body": "Hi. I would like to work on model card for mistral."
      },
      {
        "user": "NahieliV",
        "body": "Hi @stevhliu , this is my first contribution so I have a really basic question . Should I clone every repo under mistralai?  I just cloned the repo mistralai/Ministral-8B-Instruct-2410, but there are many other repos under mistralai. It's ok if I need to, but I just want to be sure.  "
      }
    ]
  },
  {
    "issue_number": 38771,
    "title": "device_map='auto' coupled with tp_plan='auto'",
    "author": "weifengpy",
    "state": "open",
    "created_at": "2025-06-11T22:38:16Z",
    "updated_at": "2025-06-17T09:14:38Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nHi from pytorch distributed! Thanks for showcasing pytorch APIs\n\ndevice_map='auto' and tp_plan='auto' are somehow coupled right now:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L4280-L4283\n\nif people load model with `device_map='auto'`, they will ended up with a TP-enabled model with model.parameters() as DTensor. Once they train it, DDP will complain about DTensor. If they set `device_map='cuda'`, the model is loaded as plain module without TP and it works with DDP\n\nCurious if we want to decouple `device_map='auto'` with `tp_plan='auto'`? I found this from pytorch users https://github.com/pytorch/pytorch/issues/155463\n\n### Motivation\n\nd\n\n### Your contribution\n\nd",
    "comments": [
      {
        "user": "weifengpy",
        "body": "cc @ArthurZucker @SunMarc if you have strong opinions to couple device_map='auto' and tp_plan='auto' "
      },
      {
        "user": "Rocketknight1",
        "body": "cc @sunmarc @Cyrilvallez "
      },
      {
        "user": "Cyrilvallez",
        "body": "Humm, no strong opinion on my side, but as it was added as a convenience at the beginning when adding TP support, it is now part of several examples and codebases... However, it's true that we did not think of this possibility - let's wait for @ArthurZucker to see if we want to start decoupling them"
      }
    ]
  },
  {
    "issue_number": 37339,
    "title": "Mypy errors since v4.51.0",
    "author": "jc-louis",
    "state": "open",
    "created_at": "2025-04-07T09:33:30Z",
    "updated_at": "2025-06-17T08:10:44Z",
    "labels": [
      "bug"
    ],
    "body": "@cyyever this is a direct result of https://github.com/huggingface/transformers/pull/37022 which marked the library as typed.\n\n```py\nfrom typing import reveal_type\n\nfrom transformers import Trainer, PreTrainedModel\n\ndef mypy_errors(trainer: Trainer, model: PreTrainedModel) -> None:\n    reveal_type(trainer)\n    reveal_type(trainer.train())\n    trainer.train()\n    model.dequantize()\n```\n\nUsing `mypy test.py`, I get the following errors:\n\n```\nscript.py:5: note: Revealed type is \"def (*args: Any, **kwargs: Any) -> transformers.utils.dummy_pt_objects.Trainer\"\nscript.py:8: note: Revealed type is \"transformers.utils.dummy_pt_objects.Trainer\"\nscript.py:9: error: \"Trainer\" has no attribute \"train\"  [attr-defined]\nscript.py:9: note: Revealed type is \"Any\"\nscript.py:10: error: \"Trainer\" has no attribute \"train\"  [attr-defined]\nscript.py:11: error: \"PreTrainedModel\" has no attribute \"dequantize\"  [attr-defined]\n```\n\nTheses errors are false positives, it seems like there is a config issue, all types are pulled from [dummy_pt_objects.py](https://github.com/huggingface/transformers/blob/main/src/transformers/utils/dummy_pt_objects.py)\n\n---\n\n\n- `transformers` version: 4.51.0\n- Platform: macOS-14.7.3-arm64-arm-64bit\n- Python version: 3.12.3\n",
    "comments": [
      {
        "user": "cyyever",
        "body": "Try\n```\nfrom transformers.trainer import Trainer    \nfrom transformers.modeling_utils  import PreTrainedModel\n```"
      },
      {
        "user": "jc-louis",
        "body": "@cyyever this does indeed fix the issue, thanks!\n\nHowever, doing `from transformers import Trainer ` is the recommended way [in the docs](https://huggingface.co/docs/transformers/main/en/trainer#trainer), is there any fix planed?"
      },
      {
        "user": "cyyever",
        "body": "We should add `__all__` to all exported symbols."
      }
    ]
  },
  {
    "issue_number": 37051,
    "title": "Incorrect calculation of strides leading to loss of param data upon tensor parallel use while sliced model loading",
    "author": "kmehant",
    "state": "open",
    "created_at": "2025-03-27T18:44:43Z",
    "updated_at": "2025-06-17T08:09:03Z",
    "labels": [
      "Tensor Parallel",
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.50.2\n- Python version: 3.12.0\n- Huggingface_hub version: 0.29.3\n- Safetensors version: 0.5.3\n- Accelerate version: 1.5.2\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.5.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: using TP\n- Using GPU in script?: yes\n- GPU type: NVIDIA A100-SXM4-80GB\n\n### Who can help?\n\ntrainer: @muellerzr @SunMarc\nTP: @ArthurZucker\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nYou can load any model (For instance `ibm-granite/granite-3.1-2b-base`) with `tp_plan=\"auto\"`. Make sure the vocab size is not divisible by world size (in this case 5) just for one sharding example, in this case we want to let TP shard lm_head with `colwise_rep`. You can then inspect size of the weight param of lm_head after application and it would be a multiple of world size however loses param data (in this case its downsized from 49152 to 49150). \n\n### Expected behavior\n\nLoss of data / param parity should hold after application of TP. This is essentially happening due to `get_tensor_shard()` function https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src/transformers/integrations/tensor_parallel.py#L120 in transformers. Specifically because of flooring of values for instance here - https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src/transformers/integrations/tensor_parallel.py#L126. Essentially, PyTorch allows for unevening sharding with its placement APIs like Shard.\n\nI am happy to help raise a PR if we agree on an approach. ",
    "comments": [
      {
        "user": "SunMarc",
        "body": "cc @ArthurZucker \nWhat kind of solution are you thinking about ? "
      },
      {
        "user": "ArthurZucker",
        "body": "Yes! API is missing a few features, and this one is one of them! \nLet's gracefully raise and error if the tensor shape is not divisible! 🤗 happy to review your pr @kmehant "
      },
      {
        "user": "kmehant",
        "body": "@ArthurZucker I have added support for this feature instead of raising an error. Please provide your review - https://github.com/huggingface/transformers/pull/37220 Thanks\n\ncc: @SunMarc "
      }
    ]
  },
  {
    "issue_number": 36657,
    "title": "Cannot run backward with tensor parallel",
    "author": "leoleoasd",
    "state": "closed",
    "created_at": "2025-03-12T06:37:09Z",
    "updated_at": "2025-06-17T08:04:02Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.49.0\n- Platform: Linux-6.8.0-1021-aws-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- Huggingface_hub version: 0.29.3\n- Safetensors version: 0.5.3\n- Accelerate version: 1.4.0\n- Accelerate config: \tnot found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA L40S\n\n### Who can help?\n\n@kmehant @SunMarc @muellerzr @kwen2501 @bursteratom\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```\nimport os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom torch.distributed.tensor.parallel import parallelize_module\nfrom torch.distributed.tensor import Replicate\nfrom torch.distributed.tensor.parallel import (\n    ColwiseParallel,\n    RowwiseParallel,\n)\n\n# initialize distributed environment\nrank = int(os.environ[\"RANK\"])\ndevice = torch.device(f\"cuda:{rank}\")\ntorch.cuda.set_device(device)\ntorch.distributed.init_process_group(\"nccl\", device_id=device)\ndevice_mesh = torch.distributed.init_device_mesh(\"cuda\", (torch.distributed.get_world_size(), ))\nprint(device_mesh)\n\n# load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/home/leo/base_models/Llama-3.2-1B\",\n)\nmodel.train()\nmodel.tensor_parallel(device_mesh)\nmodel.to(device)\n\n# setup optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-5, weight_decay=0.0, momentum=0.9)\n\n# prepare input tokens\ntokenizer = AutoTokenizer.from_pretrained(\"/home/leo/base_models/Llama-3.2-1B\")\ninputs = tokenizer(\"hi, how are you?\", return_tensors=\"pt\").to(device)\nprint(inputs)\n\n# forward pass\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=inputs[\"input_ids\"])\nprint(outputs)\n\n# backward pass\nloss = outputs.loss\nloss.backward()\noptimizer.step()\noptimizer.zero_grad()\n\nprint(loss)\n```\n\n\n\nand I got\n\n```\n[rank6]:   File \".venv/lib/python3.11/site-packages/torch/distributed/tensor/_dispatch.py\", line 470, in _try_replicate_spec_for_scalar_tensor\n[rank6]:     raise RuntimeError(\n[rank6]: RuntimeError: aten._foreach_add_.List: got mixed torch.Tensor and DTensor, need to convert all torch.Tensor to DTensor before calling distributed operators!\n```\n\n### Expected behavior\n\nget loss",
    "comments": [
      {
        "user": "kwen2501",
        "body": "Thanks for reporting. We are trying to repro it and debug."
      },
      {
        "user": "leoleoasd",
        "body": "Thanks. I was trying to run [2D-parallel](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/tp_fsdp.html) with Pytorch Lightning and encountered this error on `optimizer.step()`. The above is a minimal reproduction, and there is a different error if I run it on with transformers on the main branch."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37595,
    "title": "Unable to load certain models",
    "author": "zaddy6",
    "state": "open",
    "created_at": "2025-04-17T21:36:42Z",
    "updated_at": "2025-06-17T08:03:08Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n\n- `transformers` version: 4.51.3\n- Platform: Linux-5.15.0-130-generic-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: 0.16.6\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA H100 80GB HBM3\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI noticed I am unable to load a lot of models \n\nso far I have tried \n\nHuggingFaceTB/SmolLM2-1.7B-Instruct\nQwen/Qwen2.5-Coder-7B-Instruct\n\n\n\n```\n\nValueError: Unrecognized model in HuggingFaceTB/SmolLM2-1.7B-Instruct. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model,\n```\n\n\n\nCode\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n\n# Load the base model and tokenizer\nbase_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n\n```\n\n### Expected behavior\n\nmodels should load without errors",
    "comments": [
      {
        "user": "jlchereau",
        "body": "Add \"OpenGVLab/InternVL3-1B\" and \"OpenGVLab/InternVL3-14B\" to the list.\n\nWhether using `pip install transformers` (4.51.3) or `pip install git+https://github.com/huggingface/transformers.git`(19/04/2025) running the official script from `https://huggingface.co/OpenGVLab/InternVL3-14B?library=transformers` on macOS 15.4.1 (M4 pro) with python 3.12.9:\n\n```python\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\npipe = pipeline(\"image-text-to-text\", model=\"OpenGVLab/InternVL3-14B\", trust_remote_code=True)\npipe(messages)\n```\n\nraises error:\n\n```\nValueError: Could not load model OpenGVLab/InternVL3-14B with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageTextToText'>,). See the original errors:\n\nwhile loading with AutoModelForImageTextToText, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/xxxxx/project/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/xxxxx/project/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 574, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers_modules.OpenGVLab.InternVL3-14B.17da586d2a2f259853f586fad16b02e6380e62b0.configuration_internvl_chat.InternVLChatConfig'> for this kind of AutoModel: AutoModelForImageTextToText.\nModel type should be one of AriaConfig, AyaVisionConfig, BlipConfig, Blip2Config, ChameleonConfig, Emu3Config, FuyuConfig, Gemma3Config, GitConfig, GotOcr2Config, IdeficsConfig, Idefics2Config, Idefics3Config, InstructBlipConfig, InternVLConfig, JanusConfig, Kosmos2Config, Llama4Config, LlavaConfig, LlavaNextConfig, LlavaOnevisionConfig, Mistral3Config, MllamaConfig, PaliGemmaConfig, Pix2StructConfig, PixtralVisionConfig, Qwen2_5_VLConfig, Qwen2VLConfig, ShieldGemma2Config, SmolVLMConfig, UdopConfig, VipLlavaConfig, VisionEncoderDecoderConfig.\n```"
      },
      {
        "user": "Rocketknight1",
        "body": "hi @zaddy6 @jlchereau we've made some changes here recently, can you check if you still get the error when installing the latest version from `main` with `pip install git+https://github.com/huggingface/transformers.git`?"
      },
      {
        "user": "jlchereau",
        "body": "hi @Rocketknight1, thanks for the follow-up.\nUsing `pip3 install -qU git+https://github.com/huggingface/transformers.git` on 22 apr 2025, I sill get:\n\n```\nValueError                                Traceback (most recent call last)\nCell In[3], line 9\n      5 model=\"OpenGVLab/InternVL3-1B\"\n      6 messages = [\n      7     {\"role\": \"user\", \"content\": \"Who are you?\"},\n      8 ]\n----> 9 pipe = pipeline(\"image-text-to-text\", model=model, trust_remote_code=True)\n     10 pipe(messages)\n\nFile ~/Projects/explorations/computer-use/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:942, in pipeline(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\n    940 if isinstance(model, str) or framework is None:\n    941     model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}\n--> 942     framework, model = infer_framework_load_model(\n    943         adapter_path if adapter_path is not None else model,\n    944         model_classes=model_classes,\n    945         config=config,\n    946         framework=framework,\n    947         task=task,\n    948         **hub_kwargs,\n    949         **model_kwargs,\n    950     )\n    952 model_config = model.config\n    953 hub_kwargs[\"_commit_hash\"] = model.config._commit_hash\n...\nValueError: Unrecognized configuration class <class 'transformers_modules.OpenGVLab.InternVL3-1B.b7f05db466abe75b4c96c8bb32133bacd2ab2624.configuration_internvl_chat.InternVLChatConfig'> for this kind of AutoModel: AutoModelForImageTextToText.\nModel type should be one of AriaConfig, AyaVisionConfig, BlipConfig, Blip2Config, ChameleonConfig, Emu3Config, FuyuConfig, Gemma3Config, GitConfig, GotOcr2Config, IdeficsConfig, Idefics2Config, Idefics3Config, InstructBlipConfig, InternVLConfig, JanusConfig, Kosmos2Config, Llama4Config, LlavaConfig, LlavaNextConfig, LlavaOnevisionConfig, Mistral3Config, MllamaConfig, PaliGemmaConfig, Pix2StructConfig, PixtralVisionConfig, Qwen2_5_VLConfig, Qwen2VLConfig, ShieldGemma2Config, SmolVLMConfig, UdopConfig, VipLlavaConfig, VisionEncoderDecoderConfig.\n```"
      }
    ]
  },
  {
    "issue_number": 38039,
    "title": "Trainer API doesnt stop after the training has been completed",
    "author": "Awaisn25",
    "state": "closed",
    "created_at": "2025-05-09T11:09:40Z",
    "updated_at": "2025-06-17T08:02:46Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.51.3\n- Platform: Linux-6.11.0-25-generic-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0+cu126 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: Trainer API's parallelism\n- Using GPU in script?: Yes\n- GPU type: Quadro RTX 5000 with Max-Q Design\n\n### Who can help?\n\n@SunMarc @zach-huggingface\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Use `Seq2SeqTrainingArguments`,  with `save_total_limit=4`, `batch_szie=12`, `epochs=8`, `push_to_hub=True`. The task was summarization, the model used was google/mt5-small and the dataset was xlsum-en-es\n2. Pass the training args to `Seq2SeqTrainer` with custom `compute_metrics`.\n\n### Expected behavior\n\nI followed the summarization topic in Chapter 7 of LLM Course with the dataset of my own choice. The training continued overnight for about 14 hours. After the evaluation was completed for 8th epoch the trainer continued running for another hours. After I force stopped the execution, I got the following error trace:\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[27], line 1\n----> 1 trainer.train()\n\nFile ~/.pyenv/versions/llm/lib/python3.12/site-packages/transformers/trainer.py:2236, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n   2233 try:\n   2234     # Disable progress bars when uploading models during checkpoints to avoid polluting stdout\n   2235     hf_hub_utils.disable_progress_bars()\n-> 2236     return inner_training_loop(\n   2237         args=args,\n   2238         resume_from_checkpoint=resume_from_checkpoint,\n   2239         trial=trial,\n   2240         ignore_keys_for_eval=ignore_keys_for_eval,\n   2241     )\n   2242 finally:\n   2243     hf_hub_utils.enable_progress_bars()\n\nFile ~/.pyenv/versions/llm/lib/python3.12/site-packages/transformers/trainer.py:2728, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n   2725 self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n   2727 # Wait for the checkpoint to be uploaded.\n-> 2728 self._finish_current_push()\n   2730 # After training we make sure to retrieve back the original forward pass method\n   2731 # for the embedding layer by removing the forward post hook.\n   2732 if self.neftune_noise_alpha is not None:\n\nFile ~/.pyenv/versions/llm/lib/python3.12/site-packages/transformers/trainer.py:4773, in Trainer._finish_current_push(self)\n   4771 if self.push_in_progress is not None and not self.push_in_progress.is_done():\n   4772     logger.info(\"Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\")\n-> 4773     self.push_in_progress.wait_until_done()\n\nFile ~/.pyenv/versions/llm/lib/python3.12/site-packages/transformers/utils/hub.py:1185, in PushInProgress.wait_until_done(self)\n   1184 def wait_until_done(self):\n-> 1185     futures.wait(self.jobs)\n\nFile /usr/lib/python3.12/concurrent/futures/_base.py:305, in wait(fs, timeout, return_when)\n    301         return DoneAndNotDoneFutures(done, not_done)\n    303     waiter = _create_and_install_waiters(fs, return_when)\n--> 305 waiter.event.wait(timeout)\n    306 for f in fs:\n    307     with f._condition:\n\nFile /usr/lib/python3.12/threading.py:655, in Event.wait(self, timeout)\n    653 signaled = self._flag\n    654 if not signaled:\n--> 655     signaled = self._cond.wait(timeout)\n    656 return signaled\n\nFile /usr/lib/python3.12/threading.py:355, in Condition.wait(self, timeout)\n    353 try:    # restore state no matter what (e.g., KeyboardInterrupt)\n    354     if timeout is None:\n--> 355         waiter.acquire()\n    356         gotit = True\n    357     else:\n\nKeyboardInterrupt:\n----------------------------------------------------\n\nThe last trace suggests that the thread kept waiting and the lock was not acquired. Smaller training runs were completed without issue. ",
    "comments": [
      {
        "user": "Awaisn25",
        "body": "Update: I tried manually calling the `trainer.push_to_hub()` function and it still took time as it did earlier, uploading didnt start even after 40 minutes. I have a stable internet connection. Upon `KeyboardInterrupt`, the stack trace was same the underlying error was in `waiter.acquire()`.\n\nEDIT 1: \nHappens the same with `trainer.save_model()`"
      },
      {
        "user": "SunMarc",
        "body": "Hey @Awaisn25 , can you share a simple minimal reproducer since the issue seems to come from `push_to_hub` ? "
      },
      {
        "user": "Awaisn25",
        "body": "Hi, thanks for reaching back. I was following the LLM course, chapter 7 and topic was summarization as listed [here](https://huggingface.co/learn/llm-course/chapter7/5?fw=pt). The only change I made was using the xlsum dataset for 'english' and 'spanish'. I've listed above the training arguments I used."
      }
    ]
  },
  {
    "issue_number": 36071,
    "title": "modeling_phi3 errors with AttributeError: 'DynamicCache' object has no attribute 'get_max_length'",
    "author": "doctorpangloss",
    "state": "closed",
    "created_at": "2025-02-06T16:50:38Z",
    "updated_at": "2025-06-17T07:24:04Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.49.0.dev0 (315a9f494e0e00d8652722ce950be590852a4727~1)\n- Platform: Windows-10-10.0.20348-SP0\n- Python version: 3.11.7\n- Huggingface_hub version: 0.28.1\n- Safetensors version: 0.5.2\n- Accelerate version: 1.3.0\n- Accelerate config:    not found\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): 0.10.2 (cpu)\n- Jax version: 0.5.0\n- JaxLib version: 0.5.0\n- Using distributed or parallel set-up in script?: no\n- Using GPU in script?: no\n- GPU type: NVIDIA RTX A5000\n\n### Who can help?\n\n@ArthurZucker\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nUse Phi3 with any cache configuration, including default (DynamicCache)\n\nI think get_max_length is probably declared on a mixin that isn't on the cache classes yet?\n\n```\ncomfy_extras\\nodes\\nodes_language.py:361: in execute\n    return model.generate(tokens, max_new_tokens, repetition_penalty, seed, sampler),\ncomfy\\language\\transformers_model_management.py:228: in generate\n    output_ids = transformers_model.generate(\n..\\..\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116: in decorate_context\n    return func(*args, **kwargs)\n..\\..\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2224: in generate\n    result = self._sample(\n..\\..\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3198: in _sample\n    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\nC:\\Users\\bberman\\.cache\\huggingface\\modules\\transformers_modules\\c1358f8a35e6d2af81890deffbbfa575b978c62f\\modeling_phi3.py:1292: in prepare_inputs_for_generation\n    max_cache_length = past_key_values.get_max_length()\n```\n\n### Expected behavior\n\nrelated to #35168?\n\nI'm not sure why this is only coming up with phi-3 so far",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "Hey @doctorpangloss !\n\nThe `get_max_length()` has been deprecated for several releases and is removed in the last v4.48. It should be `cache.get_max_cache_shape()`.\n\nSince the source of error is the remote code stored on hub, you need to open an issue in model repo and ask authors to fix that. Also you can downgrade transformers version a a workaround"
      },
      {
        "user": "jillianmclements",
        "body": "I need to upgrade to v4.49 to use GRPO, so downgrading the transformers version isn't an option. Since the issue is with the [HF hub's modeling_phi3.py](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/modeling_phi3.py), is there an easy way to get it to use the [transformers' modeling_phi3.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py) instead (where it looks like the issue is fixed)? My current workaround is to download the transformers version and modify it to use absolute imports instead of relative imports. "
      },
      {
        "user": "zucchini-nlp",
        "body": "@doctorpangloss you can set `trust_remote_code=False` if you are using one of the supported checkpoints. Check out https://github.com/huggingface/transformers/issues/32365#issuecomment-2270587554"
      }
    ]
  },
  {
    "issue_number": 38514,
    "title": "Can not reproduce Blip2ForImageTextRetrieval example from docs, getting different results",
    "author": "KarlisJ",
    "state": "closed",
    "created_at": "2025-06-01T09:52:40Z",
    "updated_at": "2025-06-17T07:19:48Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n\n- `transformers` version: 4.52.4\n- Platform: Linux-4.4.0-x86_64-with-glibc2.36\n- Python version: 3.12.6\n- Huggingface_hub version: 0.32.3\n- Safetensors version: 0.5.3\n- Accelerate version: not installed\n- Accelerate config: not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.7.0+cu126 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: Tesla T4\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI'm trying to run Blip2ForImageTextRetrieval on modal infrastructure, and it produces very inaccurate results. In short, when I would expect to see a high \"is\" score, I get either very low or, at best, close to 0.5. \n\nTo debug, I tried to reproduce the [example from docs](https://huggingface.co/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForImageTextRetrieval) \n```python\nimport modal\n\napp = modal.App(name=\"blip-itm\")\n\nimage = (modal.Image.debian_slim()\n    .pip_install(\"torch\", \"transformers\", \"pillow\", \"requests\")\n)\n\n@app.function(image=image, gpu=\"T4\")\ndef official_demo(self):\n    import torch\n    from PIL import Image\n    import requests\n    from transformers import AutoProcessor, Blip2ForImageTextRetrieval\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    model = Blip2ForImageTextRetrieval.from_pretrained(\"Salesforce/blip2-itm-vit-g\", torch_dtype=torch.float16)\n    processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n\n    model.to(device)\n    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n    image = Image.open(requests.get(url, stream=True).raw)\n    text = \"two cats laying on a pink blanket\"\n\n    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device, torch.float16)\n    with torch.cuda.amp.autocast():\n        itm_out = model(**inputs, use_image_text_matching_head=True)\n    logits_per_image = torch.nn.functional.softmax(itm_out.logits_per_image, dim=1)\n    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n\n    print(f\"{probs[0][0]:.1%} that image 0 is not '{text}'\")\n\n    print(f\"{probs[0][1]:.1%} that image 0 is '{text}'\")\n\n    texts = [\"a photo of a cat\", \"a photo of a dog\"]\n\n    inputs = processor(images=image, text=texts, return_tensors=\"pt\").to(device, torch.float16)\n    with torch.cuda.amp.autocast():\n        itc_out = model(**inputs, use_image_text_matching_head=False)\n    logits_per_image = itc_out.logits_per_image  # this is the image-text similarity score\n    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n\n    print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n\n    print(f\"{probs[0][1]:.1%} that image 0 is '{texts[1]}'\")\n\n\n@app.local_entrypoint()\ndef main():\n     official_demo.remote()\n```\nHowever the output is\n```\n49.1% that image 0 is not 'two cats laying on a pink blanket'\n50.9% that image 0 is 'two cats laying on a pink blanket'\n49.9% that image 0 is 'a photo of a cat'\n50.1% that image 0 is 'a photo of a dog'\n```\nWhich is inaccurate and way of from what docs example state. \n\nAlso, I'm getting \n```\nRuntimeError: expected scalar type Half but found Float\n```\nbut I resolved it by explicitly autocasting (it's the only difference in my code from the docs example)\n\n### Expected behavior\n\nThe output when running official docs sample code should be \n```\n26.9% that image 0 is not 'two cats laying on a pink blanket'\n73.0% that image 0 is 'two cats laying on a pink blanket'\n55.3% that image 0 is 'a photo of a cat'\n44.7% that image 0 is 'a photo of a dog'\n```",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "Probably will be fixed by https://github.com/huggingface/transformers/pull/38510, there is a dtype mismatch due to keeping some modules in fp32"
      },
      {
        "user": "azmle112",
        "body": "> Probably will be fixed by [#38510](https://github.com/huggingface/transformers/pull/38510), there is a dtype mismatch due to keeping some modules in fp32可能会由 [#38510](https://github.com/huggingface/transformers/pull/38510) 修复，由于在 fp32 中保留某些模块，因此存在 dtype 不匹配\n\nHi, I applied the fix for[ #38510](https://github.com/huggingface/transformers/pull/38510/), but when I run the Blip2ForImageTextRetrieval example, the output is consistent with @KarlisJ , and I cannot reproduce the results.\n"
      },
      {
        "user": "zucchini-nlp",
        "body": "Which version you have @azmle112 ? Can you try to install from `main` so all the fixes are pulled, it worked for me in the `main` with above script"
      }
    ]
  },
  {
    "issue_number": 38809,
    "title": "Text classification evaluation has_labels bug!",
    "author": "HERIUN",
    "state": "closed",
    "created_at": "2025-06-13T09:48:49Z",
    "updated_at": "2025-06-17T05:35:16Z",
    "labels": [
      "bug"
    ],
    "body": "in Trainer.prediction_step(...)\n```python\n    def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[list[str]] = None,\n    ) -> tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        \n        has_labels = False if len(self.label_names) == 0 else all(inputs.get(k) is not None for k in self.label_names)\n        ...\n```\n\nInputs has \"labels\" but has_labels=False... \n\ncompute_metic dosen't work.\n\n<img width=\"1111\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/774c2483-bac9-4437-a4a3-f1a13014ee5e\" />\n\n<img width=\"1367\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9c23bf7a-6801-4a2f-8661-29b6bb7eaf36\" />\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n```python\nfrom datasets import load_from_disk, load_dataset\nimport numpy as np\nimport torch\nimport evaluate\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification, ModernBertForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nimport os\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\ndef main():\n    device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n    print(\"device: \", device)\n    torch.cuda.empty_cache()\n    # loading dataset\n    print(\"Loading dataset...\")\n    if os.path.exists(\"data/tokenized/wortschartz_30/\"):\n        datasets = load_from_disk(\"data/tokenized/wortschartz_30/\")\n    else:\n        datasets = load_dataset(\"Flitto/wortschartz_31\", token=os.environ.get(\"HF_TOKEN\"))\n        tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n        tokenizer.model_max_length = 512\n\n        def tokenize_function(examples):\n            return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n\n        datasets = datasets.map(\n            tokenize_function, \n            batched=True, \n            num_proc=16,\n            )\n        datasets.save_to_disk('data/tokenized/wortschartz_30')\n\n    print(\"Done\")\n\n    train_size = len(datasets[\"train\"])\n    valid_size = len(datasets[\"validation\"])\n\n    print(\"Dataset size(train): \", train_size)\n    print(\"Dataset size(validation): \", valid_size)\n\n    metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    # loading base model\n    model = ModernBertForSequenceClassification.from_pretrained(\"answerdotai/ModernBERT-base\", num_labels=31).to(device)\n\n\n    default_args = {\n        \"output_dir\": \"test_trainer\",\n        \"num_train_epochs\": 1,\n        \"log_level\": \"debug\",\n        \"per_device_train_batch_size\":25,\n        \"per_device_eval_batch_size\":50,\n        \"label_names\": datasets['train'].features['labels'].names,\n        \"logging_steps\": 100,\n        \"do_train\": True,\n        \"do_eval\": True,\n        \"save_strategy\": \"steps\",\n        \"save_total_limit\" : 5,\n        \"load_best_model_at_end\" : True,\n        \"eval_strategy\": \"steps\",\n        \"eval_on_start\": True,\n        # \"dataloader_num_workers\": 16,\n        \"seed\": 42,\n        \"warmup_ratio\": 0.1,\n        \"fp16\": True,\n    }\n\n    train_n_step = train_size / default_args[\"per_device_train_batch_size\"]\n    valid_n_step = valid_size / default_args[\"per_device_eval_batch_size\"]\n\n    default_args[\"save_steps\"]= 5000 #train_n_step // 5\n    default_args[\"eval_steps\"]= 5000 #valid_n_step // 5\n\n\n    training_args = TrainingArguments(**default_args)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=datasets['train'],\n        eval_dataset=datasets['validation'],\n        compute_metrics=compute_metrics,\n    )\n    trainer.train()\n\n    if training_args.eval_strategy != \"no\":\n        metrics = trainer.evaluate()\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n\n    # Save and push to hub\n    trainer.save_model(training_args.output_dir) # it only save adaptor, not merged model\n\n    # if training_args.push_to_hub:\n    #     trainer.push_to_hub(dataset_name=training_args.dataset_name)\n    \nif __name__ == \"__main__\":\n    main()\n\n\n```\n\n### Expected behavior\n\nhas_labels=True\ncompute_metric works!",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @sunmarc"
      },
      {
        "user": "bvantuan",
        "body": "Hi @HERIUN @Rocketknight1 @SunMarc ! According to the documentation, `label_names` is \"The list of keys in your dictionary of inputs that correspond to the labels.\". And the `inputs` of your script look like this:\ninputs\n```\n{'labels': tensor([0, 0, 2, 3, 1, 1, 1, 1, 1, 0, 3, 1, 0, 0, 0, 1, 0, 3, 2, 1, 4, 0, 1, 0,\n        0]), 'input_ids': tensor([[50281,   303,  5471,  ..., 50283, 50283, 50283],\n        [50281,    74,  1928,  ..., 50283, 50283, 50283],\n        [50281,    74,  1928,  ..., 50283, 50283, 50283],\n        ...,\n        [50281,    74,  1928,  ..., 50283, 50283, 50283],\n        [50281,    74,   816,  ..., 50283, 50283, 50283],\n        [50281,    74,  1928,  ..., 50283, 50283, 50283]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])}\n```\n\nSo `label_names` should be set to the corresponding key in `inputs` which is `[\"labels\"]`.\nPerhaps the documentation should clarify more clearly how to use the `label_names` argument correctly."
      }
    ]
  },
  {
    "issue_number": 36222,
    "title": "Tensor Parallel performance is worse than eager mode.",
    "author": "jiqing-feng",
    "state": "closed",
    "created_at": "2025-02-17T04:49:44Z",
    "updated_at": "2025-06-17T05:26:55Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n```\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n\n- `transformers` version: 4.48.3\n- Platform: Linux-4.18.0-425.3.1.el8.x86_64-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- Huggingface_hub version: 0.28.1\n- Safetensors version: 0.5.2\n- Accelerate version: 1.3.0\n- Accelerate config:    - compute_environment: LOCAL_MACHINE\n        - distributed_type: MULTI_GPU\n        - mixed_precision: bf16\n        - use_cpu: False\n        - debug: False\n        - num_processes: 2\n        - machine_rank: 0\n        - num_machines: 1\n        - gpu_ids: 5,6\n        - rdzv_backend: static\n        - same_network: True\n        - main_training_function: main\n        - enable_cpu_affinity: False\n        - downcast_bf16: no\n        - tpu_use_cluster: False\n        - tpu_use_sudo: False\n        - tpu_env: []\n- PyTorch version (GPU?): 2.6.0a0+ecf3bae40a.nv25.01 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA A100 80GB PCIe\n```\n\ndocker image: `nvcr.io/nvidia/pytorch:25.01-py3`\nHardware: Nvidia A100\n\n### Who can help?\n\n@SunMarc  @ArthurZucker @kwen2501 \n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nCMD: `CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc-per-node 4 run_tp_hf.py`\n\n```python\nimport os\nimport torch\nimport time\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n\n# Initialize distributed, only TP model needed.\nrank = int(os.environ[\"RANK\"])\ndevice = torch.device(f\"cuda:{rank}\")\nprint(rank)\nprint(device)\ntorch.distributed.init_process_group(\"nccl\", device_id=device)\n\n# Retrieve tensor parallel model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    tp_plan=\"auto\",\n    # device_map=\"cuda:0\",\n    torch_dtype=torch.float16\n)\nprint(model.dtype)\n\n# Prepare input tokens\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nprompt = \"Can I help\" * 200\ninputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512).input_ids.to(model.device)\nprint(f\"inpu shape is {inputs.shape}\")\n\nmodel = torch.compile(model)\n# warm-up\nfor i in range(100):\n    outputs = model(inputs)\n\n\ntorch.cuda.synchronize(device)\n# Distributed run\nfor i in range(50):\n    start = time.time()\n    torch.cuda.synchronize(device)\n    outputs = model(inputs)\n    torch.cuda.synchronize(device)\n    end = time.time()\n    print(f\"time cost {(end-start)*1000} ms\")\n```\n\n### Expected behavior\n\nLatency Performance (ms):\ntp_size is world_size\n```\n| tp_size | latency | memory per device |\n|    1    |  47 ms  |  21.5 G           |\n|    2    |  49 ms  |  27 G             |\n|    4    |  45 ms  |  27 G             |\n```\nThe speed-up is not expected as [doc](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_multi.md) claimed.\n\nRelated PR: [34184](https://github.com/huggingface/transformers/pull/34184)",
    "comments": [
      {
        "user": "kwen2501",
        "body": "Do you mean multi-GPU performance is not better than single GPU?\nIn some case, that is possible, especially if the system has a slower interconnect, because TP would introduce inter-GPU communication like all-reduce. This overhead, when large, can offset the computation speedup.\n\nFrom your report, I see that your system uses PCI-e instead of NVLinks:\n> - GPU type: NVIDIA A100 80GB PCIe\n\nIn case of slow interconnect, I would recommend using Pipeline Parallel (PP) instead of Tensor Parallel because it is better at comm latency hiding. And it will increase the system throughput by the number of GPUs, of course, in ideal situation."
      },
      {
        "user": "jiqing-feng",
        "body": "Hi @kwen2501 .  Thanks for your clarification. I guess so, just want to know if I missed anything that cannot reproduce the speed-up in your [docs here](https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_multi.md). Is it only because of the hardware?\n\n![Image](https://github.com/user-attachments/assets/93b7d149-8428-41fb-8865-b24d31b92b83)"
      },
      {
        "user": "kwen2501",
        "body": "Yeah, the above benchmark is from a 8x H100 machine with fully connected NVLinks. \n\nYour benchmark script may not capture the actual time needed though.\n\n```\n# Distributed run\nfor i in range(50):\n    start = time.time()\n    outputs = model(inputs)\n    end = time.time()\n    print(f\"time cost {(end-start)*1000} ms\")\n```\nThe time between `start` and `end` is only the CPU time -- to launch CUDA kernels. It does not include the time for CUDA kernels to finish the computation.\n"
      }
    ]
  },
  {
    "issue_number": 37098,
    "title": "Feature Request: Support Canary Models",
    "author": "fakerybakery",
    "state": "open",
    "created_at": "2025-03-29T04:00:46Z",
    "updated_at": "2025-06-17T02:03:02Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nAdd support for Canary Flash (1B/180M) models:\n\nhttps://huggingface.co/nvidia/canary-1b-flash\n\nhttps://huggingface.co/nvidia/canary-180m-flash\n\n### Motivation\n\nThe 1B model is the second highest on the [Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard), outperformed only by Phi 4 Multimodal, which has 14B parameters.\n\nCurrently, to run the model, one must use NeMo. It would be helpful to be able to use it with Transformers.\n\n### Your contribution\n\nTraining and inference code: https://github.com/NVIDIA/NeMo",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Yes, we'd definitely be interested in a Transformers port!"
      },
      {
        "user": "lsb",
        "body": "+1 to canary, and parakeet"
      },
      {
        "user": "gspeter-max",
        "body": "👋 @Rocketknight1 can i contribute on this, is open perfectly "
      }
    ]
  },
  {
    "issue_number": 38464,
    "title": "We now require users to upgrade torch to at least v2.6 in order to use the function.",
    "author": "mattdornfeld",
    "state": "open",
    "created_at": "2025-05-29T05:13:31Z",
    "updated_at": "2025-06-17T00:52:45Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nRan into this bug https://github.com/huggingface/transformers/issues/38329. Tried installing from main to get access to this fix https://github.com/huggingface/transformers/pull/38376, but ran into this bug\n\n```\n(python) MacBookPro~/projects/bastet[embedding_app_python L|✚2…1] % python3 embeddingApp/embedding_app/main.py\nTraceback (most recent call last):\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/embedding_app/main.py\", line 30, in <module>\n    model: EmbeddingModel = create_embedding_model()\n                            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/embedding_app/utils.py\", line 9, in create_embedding_model\n    return SentenceTransformer(configs.EMBEDDING_MODEL_NAME, cache_folder=str(configs.EMBEDDING_MODEL_CACHE_DIR))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/build/python/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py\", line 309, in __init__\n    modules, self.module_kwargs = self._load_sbert_model(\n                                  ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/build/python/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py\", line 1824, in _load_sbert_model\n    module = module_class.load(module_path)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/build/python/lib/python3.11/site-packages/sentence_transformers/models/CLIPModel.py\", line 98, in load\n    return CLIPModel(model_name=input_path)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/build/python/lib/python3.11/site-packages/sentence_transformers/models/CLIPModel.py\", line 18, in __init__\n    self.model = transformers.CLIPModel.from_pretrained(model_name)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/build/python/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 314, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/build/python/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4695, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/build/python/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4954, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/build/python/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 559, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/Users/matthewdornfeld/projects/bastet/embeddingApp/build/python/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1417, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n```\n\nI am on an x86 Mac and cannot upgrade to Torch 2.6\n\n```\n(python) MacBookPro~/projects/bastet % pip3 install torch==2.6.0\nERROR: Could not find a version that satisfies the requirement torch==2.6.0 (from versions: 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2)\nERROR: No matching distribution found for torch==2.6.0\n```\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install from main, version 4.1.0, 3.4.0, or 2.7.0 in PyPi. It seems like this change was pushed to every version?\n2. Attempt to load SentenceTransformer(\"clip-ViT-B-32\", cache_folder=\"/tmp/cache\")\n### Expected behavior\n\nSentenceTransformer should load the clip model without a bug",
    "comments": [
      {
        "user": "mahimairaja",
        "body": "@mattdornfeld can you check with poetry or uv ?\n\nCan you share the script to replicate"
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @mattdornfeld, unfortunately there are security risks when loading PyTorch format checkpoints on older versions of Torch, so we probably won't remove that requirement! You should probably convert the checkpoint you want to `safetensors` by loading + resaving it (you can load/resave it with an older version of Transformers, or on a machine that can install Torch 2.6, e.g. Colab). Safetensors does not have these security risks, and is the default and recommended format for saving and loading model weights in modern `transformers`."
      },
      {
        "user": "tommyhosman",
        "body": "Hi, \nI am also experiencing this issue, however, I have Torch 2.6.0 installed. \n\nSpecifically, I have Torch `2.6.0.dev20241112` build for mac m4 mps (verified in pip list as well as in the jupyter notebook below).\n\nRunning the following in jupyter notebook\n```python\nprint( transformers.utils.import_utils.get_torch_version() )\nprint( transformers.utils.import_utils.is_torch_greater_or_equal(\"2.6\") )\n```\nwhich outputs\n```\n2.6.0.dev20241112\nFalse\n```\n\nIs this about my local torch version or the model being loaded?\n\nPlease let me know if I need to create a separate issue for this problem."
      }
    ]
  },
  {
    "issue_number": 38348,
    "title": "Incorrect keypoint batch handling inside SuperGlueForKeypointMatching",
    "author": "i44p",
    "state": "open",
    "created_at": "2025-05-25T05:30:55Z",
    "updated_at": "2025-06-16T20:26:15Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.51.3\n- Platform: Linux-6.14.6-arch1-1-x86_64-with-glibc2.41\n- Python version: 3.12.10\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: not installed\n- Accelerate config: not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: no\n\n### Who can help?\n\n@qubvel @sbucaille\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install `pytorch`, `pillow`, `and transformers=4.51.3` using either pip or pixi.\n2. Run the following script:\n```py\nimport torch\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport requests\n\nclass Test:\n    def __init__(self):\n        self.processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n        self.model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n    \n    @torch.inference_mode()\n    def get_keypoints(\n        self,\n        series1: list[Image.Image],\n        series2: list[Image.Image]\n        ):\n\n        images = []\n        for s1, s2 in zip(series1, series2):\n            images.append([s1, s2])\n        \n        processor_inputs = self.processor(images, return_tensors=\"pt\")\n        outputs = self.model(**processor_inputs)\n\n        image_sizes = [[(s1.height, s1.width), (s2.height, s2.width)] \n                for s1, s2 in zip(series1, series2)]\n        \n        processed_outputs = self.processor.post_process_keypoint_matching(\n            outputs, image_sizes\n        )\n        return processed_outputs\n\nurl_image1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\nimage1 = Image.open(requests.get(url_image1, stream=True).raw)\nurl_image2 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\nimage2 = Image.open(requests.get(url_image2, stream=True).raw)\n\ntest = Test()\nkps = test.get_keypoints((image1, image1), (image2, image2))\n\nassert torch.equal(kps[0]['keypoints0'], kps[1]['keypoints0'])\nprint(\"Assertion succeeded!\")\n```\n\n### Expected behavior\n\nThe script executes successfully and `get_keypoints` returns two exact same arrays, assertion succeeds.\n\nI tried to use `SuperGlueForKeypointMatching` (added in #29886) for batch inference but I found that while it works with single images well, it fails to do batch inference. I believe this is caused by incorrect concatenation inside `SuperGlueForKeypointMatching._match_image_pair`:\nhttps://github.com/huggingface/transformers/blob/d0c9c66d1c09df3cd70bf036e813d88337b20d4c/src/transformers/models/superglue/modeling_superglue.py#L726-L727\n\nChanging this seemingly fixed the issue for me.\n```py\n        matches = torch.cat([matches0, matches1], dim=1).reshape(batch_size, 2, -1)\n        matching_scores = torch.cat([matching_scores0, matching_scores1], dim=1).reshape(batch_size, 2, -1)\n```\n",
    "comments": [
      {
        "user": "gspeter-max",
        "body": "Why will this often fail?\n\n    In keypoint detection or matching tasks, especially for two consecutive images or different samples, the detected keypoints are almost never identical.\n    Even for the same image processed twice, minor differences (e.g., from random noise, augmentation, or algorithmic variance) can make the keypoint coordinates different.\n    \n\nWhen would this assertion succeed?\n\n    Only when kps[0]['keypoints0'] and kps[1]['keypoints0'] are exactly the same tensor (identical values, shape, dtype).\n"
      },
      {
        "user": "gspeter-max",
        "body": "\n    keypoints0: coordinates of keypoints in image 0 (usually shape: [N, 2], where N is the number of keypoints, and each entry is [x, y])\n    keypoints1: coordinates of keypoints in image 1 (shape: [M, 2], for M keypoints)\n\nkeypoint0\tkeypoint1\tscore\n[644, 20]\t[712, 179]\t0.9726\n[650, 65]\t[715, 215]\t0.7948\n[638, 66]\t[707, 213]\t0.8859\n\nscore is the answer of similarity "
      },
      {
        "user": "i44p",
        "body": "@gspeter-max are these comments AI-generated? This fails *always*, not *often*. I can't make any sense out of this, your comments look hallucinated. I should have probably clarified that I don't expect `kps[0]['keypoints0']` and `kps[1]['keypoints0']` to be exactly identical, but as I said in the OP there's clearly something wrong with the way matching scores are concatenated inside transformers, which is likely fixed by the changes I suggested.\n\nIf I try to run this code as is, without modifying [transformers/src/transformers/models/superglue/modeling_superglue.py](https://github.com/huggingface/transformers/blob/d0c9c66d1c09df3cd70bf036e813d88337b20d4c/src/transformers/models/superglue/modeling_superglue.py#L726-L727):\n```py\nimport torch\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport requests\n\n\nclass Test:\n    def __init__(self):\n        self.processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n        self.model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n    \n    @torch.inference_mode()\n    def get_keypoints(\n        self,\n        series1: list[Image.Image],\n        series2: list[Image.Image]\n        ):\n\n        images = []\n        for s1, s2 in zip(series1, series2):\n            images.append([s1, s2])\n        \n        processor_inputs = self.processor(images, return_tensors=\"pt\")\n        outputs = self.model(**processor_inputs)\n\n        image_sizes = [[(s1.height, s1.width), (s2.height, s2.width)] \n                for s1, s2 in zip(series1, series2)]\n        \n        processed_outputs = self.processor.post_process_keypoint_matching(\n            outputs, image_sizes\n        )\n        return processed_outputs\n\n\nurls = [\n    [\n        \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\",\n        \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n    ],\n    [\n        \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/piazza_san_marco_06795901_3725050516.jpg\",\n        \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/piazza_san_marco_15148634_5228701572.jpg\"\n    ],\n    [\n        \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/st_pauls_cathedral_30776973_2635313996.jpg\",\n        \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/st_pauls_cathedral_37347628_10902811376.jpg\"\n    ]\n]\n\npairs = []\nfor url_pair in urls:\n    pair = []\n    for url in url_pair:\n        im = Image.open(requests.get(url, stream=True).raw)\n        pair.append(im)\n    pairs.append(pair)\n\nseries1 = []\nseries2 = []\n\nfor im1, im2 in pairs:\n    series1.append(im1)\n    series2.append(im2)\n\n# Inference\ntest = Test()\n\nkps_batched = test.get_keypoints(series1, series2)\n\nkps_single = []\nfor im1, im2 in zip(series1, series2):\n    kps = test.get_keypoints((im1,), (im2,))[0]\n    kps_single.append(kps)\n\n\nprint(\"\\nNon-batched:\")\nfor i, kps in enumerate(kps_single):\n    print(f\"kps[{i}]: keypoints0={kps['keypoints0'].shape}, keypoints1={kps['keypoints1'].shape}, matching_scores.mean={kps['matching_scores'].mean()}\")\n\nprint(\"\\nBatched:\")\nfor i, kps in enumerate(kps_batched):\n    print(f\"kps[{i}]: keypoints0={kps['keypoints0'].shape}, keypoints1={kps['keypoints1'].shape}, matching_scores.mean={kps['matching_scores'].mean()}\")\n```\nI will get this error:\n```\nTraceback (most recent call last):\n  File \"<>/test.py\", line 69, in <module>\n    kps_batched = test.get_keypoints(series1, series2)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<>/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"<>/test.py\", line 30, in get_keypoints\n    processed_outputs = self.processor.post_process_keypoint_matching(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<>/python3.12/site-packages/transformers/models/superglue/image_processing_superglue.py\", line 393, in post_process_keypoint_matching\n    matched_keypoints1 = keypoints1[matches0[valid_matches]]\n                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: index 1418 is out of bounds for dimension 0 with size 1371\n```\nIf, however, I apply the changes I described, the code runs successfully:\n```\nNon-batched:\nkps[0]: keypoints0=torch.Size([233, 2]), keypoints1=torch.Size([233, 2]), matching_scores.mean=0.4552536904811859\nkps[1]: keypoints0=torch.Size([399, 2]), keypoints1=torch.Size([399, 2]), matching_scores.mean=0.4019520580768585\nkps[2]: keypoints0=torch.Size([256, 2]), keypoints1=torch.Size([256, 2]), matching_scores.mean=0.3144405484199524\n\nBatched:\nkps[0]: keypoints0=torch.Size([234, 2]), keypoints1=torch.Size([234, 2]), matching_scores.mean=0.4524313807487488\nkps[1]: keypoints0=torch.Size([399, 2]), keypoints1=torch.Size([399, 2]), matching_scores.mean=0.40192607045173645\nkps[2]: keypoints0=torch.Size([256, 2]), keypoints1=torch.Size([256, 2]), matching_scores.mean=0.3144405484199524\n```"
      }
    ]
  },
  {
    "issue_number": 33637,
    "title": "torch.onnx.export failure for llava's vision encoder model",
    "author": "symphonylyh",
    "state": "closed",
    "created_at": "2024-09-21T00:22:34Z",
    "updated_at": "2025-06-16T18:41:37Z",
    "labels": [
      "bug",
      "ONNX",
      "Deployment"
    ],
    "body": "### System Info\r\n\r\ntransformers version == 4.42.4 works\r\ntransformers version >= 4.43.0 all fails\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce:\r\n\r\n```python\r\nimport os\r\nimport torch\r\nfrom transformers import LlavaForConditionalGeneration, AutoProcessor\r\nfrom PIL import Image\r\n\r\ndef export_onnx(model,\r\n                input,\r\n                onnx_dir,\r\n                onnx_name='model.onnx',\r\n                input_names=['input'],\r\n                output_names=['output'],\r\n                dynamic_axes={'input': {\r\n                    0: 'batch'\r\n                }}):\r\n    os.makedirs(onnx_dir, exist_ok=True)\r\n    torch.onnx.export(model,\r\n                      input,\r\n                      f'{onnx_dir}/{onnx_name}',\r\n                      opset_version=17,\r\n                      input_names=input_names,\r\n                      output_names=output_names,\r\n                      dynamic_axes=dynamic_axes)\r\n\r\nclass LlavaVisionWrapper(torch.nn.Module):\r\n\r\n    def __init__(self, tower, projector, feature_layer):\r\n        super().__init__()\r\n        self.tower = tower\r\n        self.projector = projector\r\n        self.feature_layer = feature_layer\r\n    \r\n    def forward(self, image):\r\n        all_hidden_states = self.tower(\r\n            image, output_hidden_states=True).hidden_states\r\n        features = all_hidden_states[self.feature_layer][:, 1:]\r\n        return self.projector(features)\r\n \r\nmodel_id = \"llava-hf/llava-1.5-7b-hf\" \r\nmodel = LlavaForConditionalGeneration.from_pretrained(model_id)\r\nwrapper = LlavaVisionWrapper(\r\nmodel.vision_tower,\r\nmodel.multi_modal_projector,\r\nmodel.config.vision_feature_layer)\r\n\r\nprocessor = AutoProcessor.from_pretrained(model_id)\r\nraw_image = Image.new('RGB', [10, 10])  # dummy image\r\nimage = processor(text=\"dummy\", images=raw_image,\r\n                  return_tensors=\"pt\")['pixel_values']\r\n\r\nexport_onnx(wrapper, image, 'tmp/onnx')\r\n```\r\n\r\nLeads to error\r\n```bash\r\nline 116, in export_onnx\r\n    torch.onnx.export(model,\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\", line 511, in export\r\n    _export(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\", line 1607, in _export\r\n    graph, params_dict, torch_out = _model_to_graph(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\", line 1133, in _model_to_graph\r\n    graph = _optimize_graph(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\", line 672, in _optimize_graph\r\n    graph = _C._jit_pass_onnx(graph, operator_export_type)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\", line 1956, in _run_symbolic_function\r\n    return symbolic_fn(graph_context, *inputs, **attrs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_helper.py\", line 291, in wrapper\r\n    return fn(g, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset14.py\", line 176, in scaled_dot_product_attention\r\n    query_scaled = g.op(\"Mul\", query, g.op(\"Sqrt\", scale))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py\", line 92, in op\r\n    return _add_op(self, opname, *raw_args, outputs=outputs, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py\", line 243, in _add_op\r\n    inputs = [_const_if_tensor(graph_context, arg) for arg in args]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py\", line 243, in <listcomp>\r\n    inputs = [_const_if_tensor(graph_context, arg) for arg in args]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py\", line 275, in _const_if_tensor\r\n    return _add_op(graph_context, \"onnx::Constant\", value_z=arg)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py\", line 251, in _add_op\r\n    node = _create_node(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py\", line 311, in _create_node\r\n    _add_attribute(node, key, value, aten=aten)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py\", line 362, in _add_attribute\r\n    return getattr(node, f\"{kind}_\")(name, value)\r\nTypeError: z_(): incompatible function arguments. The following argument types are supported:\r\n    1. (self: torch._C.Node, arg0: str, arg1: torch.Tensor) -> torch._C.Node\r\n```\r\n\r\nOnly occurs >= 4.43.0\r\n\r\n### Expected behavior\r\n\r\nonnx export should work",
    "comments": [
      {
        "user": "LysandreJik",
        "body": "Thanks @symphonylyh! Pinging @xenova for a quick answer when he can"
      },
      {
        "user": "xenova",
        "body": "Hi @symphonylyh 👋 We've found that exporting llava models is a bit more complicated than simply calling torch.onnx.export, mainly because we need to fuse the vision and text embedding before running the decoder. For that reason, we export 3 sub modules:\n1. Vision encoder\n2. Text embedding layer\n3. Decoder without embedding layer\n\nHere's a colab notebook which outlines this process: https://colab.research.google.com/drive/1IhC8YOV68cze0XWGfuqSclnVTt_FskUd?usp=sharing\n\nHopefully that helps! One day, we'll add this to Optimum, but we were waiting for the VLM API to be a bit more standardized (which it's now in a much better state),"
      },
      {
        "user": "symphonylyh",
        "body": "Hi @xenova, thanks for the advice!\r\n\r\nActually I'm not referring to exporting the entire llava model, instead we're doing a pretty similar thing in your colab, which exports just the vision encoder + projector + feature layer as a onnx: https://colab.research.google.com/drive/1IhC8YOV68cze0XWGfuqSclnVTt_FskUd#scrollTo=qbZWrlAvR6VI&line=4&uniqifier=1. So it's just (1) in your above workflow\r\n\r\nSuch partial export works for 4.42.4 but fails on >= 4.43.0, so it's likely a regression. Since your colab seems to work for (1), maybe I can check whether it's some differences in the torch.onnx.export() params that causes this, or it's the differences in creating the dummy onnx input"
      }
    ]
  },
  {
    "issue_number": 38253,
    "title": "FEAT WANT: Dynamic quantization config for bitsandbytes",
    "author": "AaronZLT",
    "state": "open",
    "created_at": "2025-05-21T06:51:03Z",
    "updated_at": "2025-06-16T16:58:17Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nWill transformers support dynamic quantization config for bitsandbytes? Currently transformers support hqq dynamic quantization, via\n\n```python\n            q4_config = {\"nbits\": 4, \"group_size\": 64}\n            q8_config = {\"nbits\": 8, \"group_size\": 64}\n            quant_config = HqqConfig(\n                dynamic_config={\n                    \"self_attn.q_proj\": q4_config,\n                    \"self_attn.k_proj\": q4_config,\n                    \"self_attn.v_proj\": q4_config,\n                    \"self_attn.o_proj\": q4_config,\n                    \"mlp.gate_proj\": q8_config,\n                    \"mlp.up_proj\": q4_config,\n                    \"mlp.down_proj\": q4_config,\n                }\n            )\n```\n",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @MekkCyber "
      },
      {
        "user": "MekkCyber",
        "body": "Hey @AaronZLT! I think this would be complicated but definitely possible, because we need to combine : `Bnb4BitHfQuantizer` and `Bnb8BitHfQuantizer`! Would you be open to submitting a PR?"
      },
      {
        "user": "gspeter-max",
        "body": "@MekkCyber  can you give me more information about that i am interested on this "
      }
    ]
  },
  {
    "issue_number": 26286,
    "title": "`align_to_words=True` in `QuestionAnsweringPipeline` can lead to duplicate answers",
    "author": "MichelBartels",
    "state": "closed",
    "created_at": "2023-09-20T10:25:30Z",
    "updated_at": "2025-06-16T15:01:23Z",
    "labels": [
      "Core: Pipeline",
      "Good Second Issue",
      "Generation"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.31.0\r\n- Platform: macOS-13.4.1-arm64-arm-64bit\r\n- Python version: 3.11.4\r\n- Huggingface_hub version: 0.15.1\r\n- Safetensors version: 0.3.1\r\n- Accelerate version: 0.21.0\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.0.1 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\n\n### Who can help?\n\n@Nars\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```py\r\nfrom transformers import pipeline\r\n\r\nanswers = pipeline(\"question-answering\", model=\"deepset/tinyroberta-squad2\")(\r\n    question=\"Who is the chancellor of Germany?\",\r\n    context=\"Angela Merkel was the chancellor of Germany.\",\r\n    top_k=10\r\n)\r\nprint(answers[0]) # Returns {'score': 0.9961308836936951, 'start': 0, 'end': 13, 'answer': 'Angela Merkel'}\r\nprint(answers[5]) # Returns {'score': 7.520078361267224e-05, 'start': 0, 'end': 13, 'answer': 'Angela Merkel'}\r\n```\r\n\r\nIf `align_to_words` is set to `True` (which is the default), all start or end tokens that are contained in the same word are mapped to the same start and end character index (see [here](https://github.com/huggingface/transformers/blob/37c205eb5d5165b70d3100b599a2bcfc483944f5/src/transformers/pipelines/question_answering.py#L617-L620)). This is expected when using `align_to_words`. However, the top_k filtering happens before this step so duplicate answers can remain.\n\n### Expected behavior\n\nIdeally, the mapping from token to word should happen at around [this point](https://github.com/huggingface/transformers/blob/37c205eb5d5165b70d3100b599a2bcfc483944f5/src/transformers/pipelines/question_answering.py#L139). You would have a start and end probability for each word. If there are multiple tokens in a word, their probabilities should be summed. This would make the probabilities more correct because every token in the word would affect the probability of selecting the word.\r\n\r\nIf this is too slow, there should at least be a check for duplicates somewhere [here](https://github.com/huggingface/transformers/blob/37c205eb5d5165b70d3100b599a2bcfc483944f5/src/transformers/pipelines/question_answering.py#L604). This would mean that you are not guaranteed to get k answers when setting `top_k`, but only that you get at most k answers. A way to mitigate that somewhat (but not perfectly), would be to use a higher value than top_k when calling `select_starts_ends` [here](https://github.com/huggingface/transformers/blob/37c205eb5d5165b70d3100b599a2bcfc483944f5/src/transformers/pipelines/question_answering.py#L546-L548).",
    "comments": [
      {
        "user": "ArthurZucker",
        "body": "cc @Rocketknight1 if you can have a look! 🤗 "
      },
      {
        "user": "Rocketknight1",
        "body": "@michelbartels I managed to reproduce the bug, and I think your diagnosis of it is completely correct. Would you be interested in filing a PR with your solution? If you don't have time, that's fine! Just let us know and we'll put it on the list to get fixed internally."
      },
      {
        "user": "MichelBartels",
        "body": "@Rocketknight1 Thanks for looking into this, I am afraid I currently don't have time to contribute a fix."
      }
    ]
  },
  {
    "issue_number": 38567,
    "title": "Support for excel files",
    "author": "Murdock135",
    "state": "closed",
    "created_at": "2025-06-03T17:51:41Z",
    "updated_at": "2025-06-16T14:58:46Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\ntitle ^\n\n### Motivation\n\nExcel files are very common as datasets.\n\n### Your contribution\n\nN/A",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @Murdock135, you can save Excel files as CSV, which should be loadable in `datasets`!"
      },
      {
        "user": "Murdock135",
        "body": "Hi @Rocketknight1. Thank you for the suggestion but my excel files contain dozens of sheets and the researchers would ideally like to have them as they are. I could write a routine that separates the sheets and put them in folders according to what excel file they came from but it would've been nice to have support for excel."
      },
      {
        "user": "Rocketknight1",
        "body": "Hmn, interesting convenience feature in that case cc @lhoestq in case you know any Python packages that read Excel 😅 "
      }
    ]
  },
  {
    "issue_number": 33504,
    "title": "Video Processor as a separate class",
    "author": "zucchini-nlp",
    "state": "closed",
    "created_at": "2024-09-16T07:26:34Z",
    "updated_at": "2025-06-16T14:17:19Z",
    "labels": [
      "Feature request",
      "Vision",
      "Multimodal"
    ],
    "body": "### Feature request\n\nSince we currently have more and more VLMs that support image and video, and not always videos are processed same way as images are, I want to add a `VideoProcessor` class that inherits from `ImageProcessingMixin`. Thus we can have two separate classes for processing visuals, each with its own set of attributes and methods. We can also save different configs for both to avoid issues as #33484. The `VideoProcessor` will mainly use the same transform methods as slow image processors, by iterating over each frame and stacking it. Some additional helper fn can be added, like `load_video`  and `make_list_of_videos`. The main input name will be videos  and the output var name is  `pixel_values_videos`. \r\n\r\nFor the `load_video`  we can prob rely on `av`, but I find it super slow compared to other video decoders. I'll try to get a small comparison benchmarks for that, and unfortunately `decord` can't be used as it had problems with models on cuda.\r\n\r\nIn the long term we might consider adding video transforms where each video is transformed in one call, instead of each video frame, similar to fast image processing with `torchvision`. \r\n\r\nTo Do:\r\n- [ ] Add the VideoProcessor class and integrate with llava-next-video which is one of the models with different processing for image and videos.\r\n- [ ] After the changed are approved and merged, the following models will be easy to modify:\r\n    - [ ] Video-LLaVa\r\n    - [ ] Qwen2-VL\r\n    - [ ] LLaVA-OneVision\r\n\r\n- [ ] Instructblip-Video might need deprecation as it currently accepts images  as main arg and returns pixel_values . TBH, it is a video-only model so we can disregard changing it, same was as we won't touch VIVIT and other video-only models\n\n### Motivation\n\nEasier integration of multimodal LLMs\n\n### Your contribution\n\n@amyeroberts WDYT about this suggestion? Would love to hear your opinion 🤗 ",
    "comments": [
      {
        "user": "amyeroberts",
        "body": "Yes - this sounds like a great idea! \r\n\r\nBig +1 to the separate class, and to using different video decoders if possible.  "
      },
      {
        "user": "gerrylwk",
        "body": "I think it would be good to have a 'multi-image' option for video too, e.g. when streaming a video, there's no need to save the frames into a video file before using it for inference"
      },
      {
        "user": "zucchini-nlp",
        "body": "@gerrylwk I'm not sure any of the supported video llms currently support streaming video input, but the idea is cool. If you have any model release on mind with such feature, feel free to open a feature request issue"
      }
    ]
  },
  {
    "issue_number": 38419,
    "title": "`verify_tp_plan` function raises an error if a key without '.' is given",
    "author": "liwii",
    "state": "open",
    "created_at": "2025-05-28T03:20:29Z",
    "updated_at": "2025-06-16T13:21:28Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n\n- `transformers` version: 4.53.0.dev0\n- Platform: Linux-5.10.0-34-cloud-amd64-x86_64-with-glibc2.31\n- Python version: 3.9.2\n- Huggingface_hub version: 0.32.2\n- Safetensors version: 0.4.5\n- Accelerate version: 1.7.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (accelerator?): 2.6.0+cu124 (NA)\n- Tensorflow version (GPU?): 2.15.1 (False)\n- Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)\n- Jax version: 0.4.13\n- JaxLib version: 0.4.13\n- Using distributed or parallel set-up in script?: <fill in>\n\n### Who can help?\n\n@ArthurZucker \n(I listed you because you seem to be the reviewer of the original PR, feel free to re-assign this to other people in charge)\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nRun the following script:\n\n```\nfrom transformers.models.auto.modeling_auto import (\n    AutoModelForSeq2SeqLM,\n)\nimport logging\n\nlogger = logging.getLogger(\"transformers.modeling_utils\")\nlogger.setLevel(logging.ERROR)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\", revision=\"cf109095479db38d6df799875e34039d4938aaa6\")\n```\n\nYou see the following error:\n```\nTraceback (most recent call last):\n  File \"/home/kokiryu/transformers/test_tp_plan.py\", line 15, in <module>\n    model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\", revision=\"cf109095479db38d6df799875e34039d4938aaa6\")\n  File \"/home/kokiryu/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 586, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/kokiryu/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 316, in _wrapper\n    return func(*args, **kwargs)\n  File \"/home/kokiryu/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 4697, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"/home/kokiryu/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 5113, in _load_pretrained_model\n    verify_tp_plan(expected_keys, getattr(model_to_load, \"_tp_plan\", None))\n  File \"/home/kokiryu/.local/lib/python3.9/site-packages/transformers/integrations/tensor_parallel.py\", line 904, in verify_tp_plan\n    param_name, _ = key.rsplit(\".\", 1) if \".\" in key else key\nValueError: too many values to unpack (expected 2)\n```\n\n### Expected behavior\n\nKeys without `'.'` such as `'final_logits_bias'` can be given to the `verify_tp_plan`. Current code ends up executing `param_name, _ = key` in such a case and raises an error.\nhttps://github.com/huggingface/transformers/blob/3b3ebcec4077f124f2cd0ec3cd5d028dc352a3e5/src/transformers/integrations/tensor_parallel.py#L903\n\nIt should directly assign `param_name = key` and output warnings without the errors.",
    "comments": [
      {
        "user": "ArthurZucker",
        "body": "Yeah no worries sounds good!"
      },
      {
        "user": "MAHMOUDRR707",
        "body": "@ArthurZucker  I still facing this problem have you found any solution for it yet?"
      },
      {
        "user": "Borda",
        "body": "I still see it with `4.52.4` :("
      }
    ]
  },
  {
    "issue_number": 38829,
    "title": "Support Asynchronous Evaluation on Separate GPU in `Trainer`",
    "author": "AmitMY",
    "state": "open",
    "created_at": "2025-06-15T06:52:31Z",
    "updated_at": "2025-06-16T13:14:41Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nAdd support for asynchronous evaluation in `transformers.Trainer`, ideally enabling evaluation to run in parallel to training — potentially on a separate GPU — without blocking the training loop.\n\nEven ideally, to really utilize the hardware fully, one GPU should be training, the other one should be evaluating, constantly. That means, that it might take a few minutes to evaluate, and maybe save the checkpoint to disk, but afterwards, a new eval should start immediately with the latest model weights, in an infinite loop.\n\n### Motivation\n\nIn my training scenarios, especially with:\n**slow checkpointing (e.g. network-mounted disk, NFS, HDD)** and **multi-GPU machines with underutilized resources**\nthe current blocking evaluation step can create a significant bottleneck. It halts the training loop and prevents full utilization of the hardware.\n\n<img width=\"562\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/426629b0-3cbe-446c-95bd-26324d2620dc\" />\n\nI do not want to try to patch this on top:\n\n* Forking Trainer manually and running evaluation with a subprocess\n* Spawning external watchdog scripts for async eval\n* Deep copying the model to avoid disk writes\n\nThis seems like a natural fit for `Trainer`, similar to how `save_strategy` and `log_strategy` are already modular.\n\n### Your contribution\n\nSome ideas (not prescriptive):\n\n* `eval_async=True`: runs evaluation in a forked process/thread\n* `eval_device='cuda:1'`: specify a device for async eval\n* `eval_strategy=\"async_steps\"`: triggers parallel eval on step intervals\n* Provide a callback hook or scheduler for async checkpoint evaluation\n\nImplementation Ideas\n\n* Use `torch.multiprocessing` or `concurrent.futures.ProcessPoolExecutor` to fork evaluation subprocess\n* Snapshot the model state (via `state_dict()` or full checkpoint) and transfer it (memory, pipe, or fast serialization)\n* Respect user-specified `eval_steps`, `metric_for_best_model`, and `load_best_model_at_end` behavior",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @SunMarc "
      }
    ]
  },
  {
    "issue_number": 37262,
    "title": "Add support for higher jax and flax version",
    "author": "rxng8",
    "state": "closed",
    "created_at": "2025-04-03T18:51:23Z",
    "updated_at": "2025-06-16T13:10:09Z",
    "labels": [
      "Feature request",
      "Flax"
    ],
    "body": "### Feature request\n\nCurrently, transformer does not support jax version greater than 0.4.13, the libraries has changed and no longer compatible\n\n### Motivation\n\nCurrently, transformer does not support jax version greater than 0.4.13, the libraries has changed and no longer compatible\n\n### Your contribution\n\nN/A",
    "comments": [
      {
        "user": "JackWolfard",
        "body": "JAX has been deprecated as of #38758. See https://www.linkedin.com/posts/lysandredebut_i-have-bittersweet-news-to-share-yesterday-activity-7338966863403528192-om5p/ for more information from @LysandreJik. I would recommend closing this issue in light of the news."
      },
      {
        "user": "Rocketknight1",
        "body": "Yes, unfortunately! Closing as not planned."
      }
    ]
  },
  {
    "issue_number": 38137,
    "title": "Speed metrics are not logged",
    "author": "pavelgein",
    "state": "closed",
    "created_at": "2025-05-15T04:44:27Z",
    "updated_at": "2025-06-16T12:45:24Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nMaster branch\n\nThe result of `speed_metrics` is not logged or saved \nhttps://github.com/huggingface/transformers/blob/4005e30c804f9b3a9dbf45d019f6ca1cdc4d774c/src/transformers/trainer.py#L3656\n\n\n\n\n### Who can help?\n\n@zach-huggingface @SunMarc\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nNo need for reproduction\n\n### Expected behavior\n\nMetrics are logged",
    "comments": [
      {
        "user": "pavelgein",
        "body": "I created a POC fix https://github.com/huggingface/transformers/pull/38136"
      },
      {
        "user": "SunMarc",
        "body": "Thanks ! reviewed "
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 36093,
    "title": "Whisper word-level timestamp extraction fails with beam search",
    "author": "dintifla",
    "state": "open",
    "created_at": "2025-02-07T14:46:50Z",
    "updated_at": "2025-06-16T12:41:08Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.48.2\n- Platform: macOS-15.3-arm64-arm-64bit\n- Python version: 3.12.1\n- Huggingface_hub version: 0.28.1\n- Safetensors version: 0.4.3\n- Accelerate version: 1.3.0\n- Accelerate config: \tnot found\n- PyTorch version (GPU?): 2.6.0 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: No\n\n### Who can help?\n\n@ylacombe, @eustlb\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nSteps to reproduce the behavior:\n\n1. Download the sample: <https://drive.google.com/file/d/19xqGiGc1fse532d6t6u5OGI_oNbTfdJe/view?usp=sharing>\n2. Run the sample script:\n\n```py\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n\ndevice = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = 'openai/whisper-small'\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ngenerate_kwargs = {\n    'num_beams': 2,\n}\n\npipe = pipeline(\n    'automatic-speech-recognition',\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n    return_timestamps='word',\n)\n\nresult = pipe('efbef66e35e6456ba37461d9c5f12fcd.mp3', generate_kwargs=generate_kwargs)\nfor chunk in result['chunks']:\n    print(f'{chunk[\"timestamp\"]} {chunk['text']}')\n```\n\n3. Find the following error in `_extract_token_timestamps(..)`:\n\n```txt\nTraceback (most recent call last):\n  File \".....py\", line 40, in <module>\n    result = pipe(str(file), generate_kwargs=generate_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"....../lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 283, in __call__\n    return super().__call__(inputs, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/pipelines/base.py\", line 1354, in __call__\n    return next(\n           ^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n    item = next(self.iterator)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 269, in __next__\n    processed = self.infer(next(self.iterator), **self.params)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/pipelines/base.py\", line 1269, in forward\n    model_outputs = self._forward(model_inputs, **forward_params)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 521, in _forward\n    tokens = self.model.generate(\n             ^^^^^^^^^^^^^^^^^^^^\n  File \"....../lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py\", line 774, in generate\n    ) = self.generate_with_fallback(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"....../lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py\", line 965, in generate_with_fallback\n    seek_sequences, seek_outputs = self._postprocess_outputs(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py\", line 1067, in _postprocess_outputs\n    seek_outputs[\"token_timestamps\"] = self._extract_token_timestamps(\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"....../lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py\", line 268, in _extract_token_timestamps\n    torch.index_select(weights[:, :, i, :], dim=0, index=beam_indices[:, i])\n                       ~~~~~~~^^^^^^^^^^^^\nIndexError: index 447 is out of bounds for dimension 2 with size 447\n```\n\nThe **sentence-level** timestamps produce the following result (`return_timestamps=True`):\n\n```txt\n(0.0, 6.0)  Viele Menschen haben auch wirklich richtig geahnt Angst, wenn sie sich trennen möchten.\n(6.0, 12.0)  Von ihrem toxischen Partner, weil sie sich bedroht fühlen. Teilweise sogar lebensbedroht fühlen.\n(12.0, 17.0)  Darum ist es sehr wichtig, dort auch als Außenstehende behutzamt vorzugehen.\n(17.0, 0.0) \n(3.32, 7.92)  und nicht einfach denken, ja, es ist schon voll leise, es ist ja mega klar, gange doch einfach, weil nein, hier hat es viel Angst, viel Druck,\n(7.92, 10.32)  wo man nicht einfach so auf die Seite legen kann.\n```\n Here, the segment from 17.0 s has an empty string as text.\n\nFurthermore, **without beam-search**  (`generate_kwargs={}´) and **word-level** timestamps I get the following hallucination:\n\n```txt\n(0.0, 0.62)  Viele\n(0.62, 1.14)  Menschen\n(1.14, 1.68)  haben\n(1.68, 2.1)  auch\n(2.1, 2.78)  richtig\n(2.78, 3.1)  richtig\n(3.1, 3.58)  geahnt\n(3.58, 4.4)  Angst,\n(4.4, 4.46)  wenn\n(4.46, 4.66)  sie\n(4.66, 4.86)  sich\n(4.86, 5.98)  trennen\n(5.98, 6.3)  möchten.\n(6.3, 6.36)  Von\n(6.36, 6.56)  ihrem\n(6.56, 7.14)  toxischen\n(7.14, 7.78)  Partner,\n(7.78, 7.84)  weil\n(7.84, 7.96)  sie\n(7.96, 8.2)  sich\n(8.2, 8.78)  bedroht\n(8.78, 9.3)  fühlen,\n(9.3, 9.6)  teilweise\n(9.6, 10.02)  sogar\n(10.02, 11.2)  lebensbedroht\n(11.2, 12.3)  fühlen.\n(12.3, 12.98)  Darum\n(12.98, 13.1)  ist\n(13.1, 13.24)  es\n(13.24, 13.46)  sehr\n(13.46, 14.1)  wichtig,\n(14.1, 14.24)  dort\n(14.24, 14.46)  auch\n(14.46, 14.6)  als\n(14.6, 15.58)  Außenstehende\n(15.58, 16.74)  behutzeim\n(16.74, 17.38)  vorzugehen\n(17.38, 17.54)  und\n(17.54, 17.7)  nicht\n(17.7, 17.98)  einfach\n(17.98, 18.44)  denken,\n(18.44, 18.6)  ja,\n(18.6, 18.64)  ja,\n...\n(22.48, 22.48)  ja,\n(22.48, 22.48)  ja,\n(22.48, 22.56)  ja,\n(22.56, 24.18)  ja,\n(24.18, None)  ja,\n```\n\n\nIn another setup where I used a batch size and load the samples with librosa before, I also saw the following error in the same function (`_extract_token_timestamps(..)`), but I don't know if the root-cause is related:\n\n```txt\n  File \"..../lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 283, in __call__\n    return super().__call__(inputs, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/pipelines/base.py\", line 1343, in __call__\n    outputs = list(final_iterator)\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"....../lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n    item = next(self.iterator)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"....../lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 269, in __next__\n    processed = self.infer(next(self.iterator), **self.params)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/pipelines/base.py\", line 1269, in forward\n    model_outputs = self._forward(model_inputs, **forward_params)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"....../lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 521, in _forward\n    tokens = self.model.generate(\n             ^^^^^^^^^^^^^^^^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py\", line 774, in generate\n    ) = self.generate_with_fallback(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"....../lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py\", line 965, in generate_with_fallback\n    seek_sequences, seek_outputs = self._postprocess_outputs(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...../lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py\", line 1067, in _postprocess_outputs\n    seek_outputs[\"token_timestamps\"] = self._extract_token_timestamps(\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"....../lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py\", line 315, in _extract_token_timestamps\n    matrix = weights[batch_idx, ..., : num_frames[batch_idx] // 2]\n                                       ~~~~~~~~~~^^^^^^^^^^^\nIndexError: index 0 is out of bounds for dimension 0 with size 0\n```\n\nSome things I recognized:\n\n1. Without beam-search, it works (the above example uses beam width = 2)\n2. It must be related to hallucinated repetitions, because I got the same issue with some fine-tuned models which hallucinate, furthermore the above example also hallucinates without beam search.\n\nMaybe someone else can reproduce this with another sample.\n\n\n\n\n\n### Expected behavior\n\nInference with word-level timestamps should not fail if something is wrong with beam search. I would expect, e.g., `None`-timestamps as we get for sentence-level timestamps.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Not sure if this is @eustlb or @gante, pinging both (with apologies!)"
      },
      {
        "user": "Maria-Ponte",
        "body": "I am having the same issue (IndexError: index 447 is out of bounds for dimension 2 with size 447). Have you found any solution to this?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38065,
    "title": "[minor] Protect against broken/missing torchvision installations and do not hard-fail at timm/torchvision import (many text models don't need any timm/torchvision as hard dependencies)",
    "author": "vadimkantorov",
    "state": "closed",
    "created_at": "2025-05-10T19:48:25Z",
    "updated_at": "2025-06-16T11:29:57Z",
    "labels": [
      "bug"
    ],
    "body": "Also, transformers import fails when no torchvision is installed at all. I think it should be no-error, especially if I'm working with text models only. timm/friends should not be imported at all...\n\n### System Info\n\nGoogle Colab, uses pytorch 2.6.0 for now\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nGo to colab which currently still has pytorch 2.6.0, then uninstall pytorch and install new pytorch (without torchvision):\n```\n!pip uninstall torch -y\n!pip install torch --index-url https://download.pytorch.org/whl/cpu\n\n`from transformers import AutoModel; AutoModel.from_pretrained('Qwen/Qwen2.5-0.5B', trust_remote_code=True)`\n```\n\nCurrently  (nothing to do with vision) fails when the setup has broken installation of torchvision (can happen if the torchvision isn't actually used and has a version not matching torch)\n\n```\nimport transformers.models.timm_wrapper.configuration_timm_wrapper because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist\n```\n```\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n[/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py](https://localhost:8080/#) in _get_module(self, module_name)\n   1966         try:\n-> 1967             return importlib.import_module(\".\" + module_name, self.__name__)\n   1968         except Exception as e:\n\n24 frames\n[/usr/lib/python3.11/importlib/__init__.py](https://localhost:8080/#) in import_module(name, package)\n    125             level += 1\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\n    127 \n\n/usr/lib/python3.11/importlib/_bootstrap.py in _gcd_import(name, package, level)\n\n/usr/lib/python3.11/importlib/_bootstrap.py in _find_and_load(name, import_)\n\n/usr/lib/python3.11/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)\n\n/usr/lib/python3.11/importlib/_bootstrap.py in _load_unlocked(spec)\n\n/usr/lib/python3.11/importlib/_bootstrap_external.py in exec_module(self, module)\n\n/usr/lib/python3.11/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\n\n[/usr/local/lib/python3.11/dist-packages/transformers/models/timm_wrapper/configuration_timm_wrapper.py](https://localhost:8080/#) in <module>\n     24 if is_timm_available():\n---> 25     from timm.data import ImageNetInfo, infer_imagenet_subset\n     26 \n\n[/usr/local/lib/python3.11/dist-packages/timm/__init__.py](https://localhost:8080/#) in <module>\n      1 from .version import __version__ as __version__\n----> 2 from .layers import (\n      3     is_scriptable as is_scriptable,\n\n[/usr/local/lib/python3.11/dist-packages/timm/layers/__init__.py](https://localhost:8080/#) in <module>\n      7 from .blur_pool import BlurPool2d, create_aa\n----> 8 from .classifier import create_classifier, ClassifierHead, NormMlpClassifierHead, ClNormMlpClassifierHead\n      9 from .cond_conv2d import CondConv2d, get_condconv_initializer\n\n[/usr/local/lib/python3.11/dist-packages/timm/layers/classifier.py](https://localhost:8080/#) in <module>\n     14 from .create_act import get_act_layer\n---> 15 from .create_norm import get_norm_layer\n     16 \n\n[/usr/local/lib/python3.11/dist-packages/timm/layers/create_norm.py](https://localhost:8080/#) in <module>\n     13 from .norm import GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d, RmsNorm, RmsNorm2d, SimpleNorm, SimpleNorm2d\n---> 14 from torchvision.ops.misc import FrozenBatchNorm2d\n     15 \n\n[/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py](https://localhost:8080/#) in <module>\n      9 from .extension import _HAS_OPS  # usort:skip\n---> 10 from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n     11 \n\n[/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py](https://localhost:8080/#) in <module>\n    162 \n--> 163 @torch.library.register_fake(\"torchvision::nms\")\n    164 def meta_nms(dets, scores, iou_threshold):\n\n[/usr/local/lib/python3.11/dist-packages/torch/library.py](https://localhost:8080/#) in register(func)\n   1022             use_lib = lib\n-> 1023         use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)\n   1024         return func\n\n[/usr/local/lib/python3.11/dist-packages/torch/library.py](https://localhost:8080/#) in _register_fake(self, op_name, fn, _stacklevel)\n    213 \n--> 214         handle = entry.fake_impl.register(func_to_register, source)\n    215         self._registration_handles.append(handle)\n\n[/usr/local/lib/python3.11/dist-packages/torch/_library/fake_impl.py](https://localhost:8080/#) in register(self, func, source)\n     30             )\n---> 31         if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, \"Meta\"):\n     32             raise RuntimeError(\n\nRuntimeError: operator torchvision::nms does not exist\n\nThe above exception was the direct cause of the following exception:\n\nRuntimeError                              Traceback (most recent call last)\n[<ipython-input-6-8843fc8bd8f0>](https://localhost:8080/#) in <cell line: 0>()\n      4 with torch.device('meta'):\n      5     #from torch.nn.attention.flex_attention import BlockMask, flex_attention\n----> 6     AutoModel.from_pretrained('Qwen/Qwen2.5-0.5B', trust_remote_code=True)\n\n[/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n    546 \n    547         has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\n--> 548         has_local_code = type(config) in cls._model_mapping.keys()\n    549         trust_remote_code = resolve_trust_remote_code(\n    550             trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\n[/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py](https://localhost:8080/#) in keys(self)\n    785 \n    786     def keys(self):\n--> 787         mapping_keys = [\n    788             self._load_attr_from_module(key, name)\n    789             for key, name in self._config_mapping.items()\n\n[/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py](https://localhost:8080/#) in <listcomp>(.0)\n    786     def keys(self):\n    787         mapping_keys = [\n--> 788             self._load_attr_from_module(key, name)\n    789             for key, name in self._config_mapping.items()\n    790             if key in self._model_mapping.keys()\n\n[/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py](https://localhost:8080/#) in _load_attr_from_module(self, model_type, attr)\n    782         if module_name not in self._modules:\n    783             self._modules[module_name] = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n--> 784         return getattribute_from_module(self._modules[module_name], attr)\n    785 \n    786     def keys(self):\n\n[/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py](https://localhost:8080/#) in getattribute_from_module(module, attr)\n    698     if isinstance(attr, tuple):\n    699         return tuple(getattribute_from_module(module, a) for a in attr)\n--> 700     if hasattr(module, attr):\n    701         return getattr(module, attr)\n    702     # Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\n\n[/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py](https://localhost:8080/#) in __getattr__(self, name)\n   1953             value = Placeholder\n   1954         elif name in self._class_to_module.keys():\n-> 1955             module = self._get_module(self._class_to_module[name])\n   1956             value = getattr(module, name)\n   1957         elif name in self._modules:\n\n[/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py](https://localhost:8080/#) in _get_module(self, module_name)\n   1967             return importlib.import_module(\".\" + module_name, self.__name__)\n   1968         except Exception as e:\n-> 1969             raise RuntimeError(\n   1970                 f\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\n   1971                 f\" traceback):\\n{e}\"\n\nRuntimeError: Failed to import transformers.models.timm_wrapper.configuration_timm_wrapper because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist\n```\n\n### Expected behavior\n\nno hard-fail at import time, maybe a hard-fail when these ops/modules from torchvision are actually needed at runtime",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @qubvel @rwightman when I try this I get `Could not import module 'TimmWrapperConfig'. Are this object's requirements defined correctly?`. It seems like maybe the Timm classes aren't being guarded / replaced with dummies correctly when `torchvision` is unavailable?"
      },
      {
        "user": "vadimkantorov",
        "body": "Yeah, HF currently hard-fails when either torchvision can'be imported (e.g. because of ABI issues / or incompatible PyTorch) or when torchvision isn't installed at all."
      },
      {
        "user": "qubvel",
        "body": "Hmmm, that's probably because an outdated/incompatible version of `torchvision`\n\nFrom the traceback:\n\n```\n[/usr/local/lib/python3.11/dist-packages/transformers/models/timm_wrapper/configuration_timm_wrapper.py](https://localhost:8080/#) in <module>\n     24 if is_timm_available():\n---> 25     from timm.data import ImageNetInfo, infer_imagenet_subset\n     26 \n```\n\nIt seems like timm is installed but can't be imported. That's one of the issues with our guards - we only check package exists without actually importing it. I would say it's an environment misconfiguration, but we can think about how to handle it.\n\n@vadimkantorov you should not actually update only torch, torchvision should be updated accordingly, or removed"
      }
    ]
  },
  {
    "issue_number": 35976,
    "title": "Deformable DETR custom kernel fails to compile with PyTorch 2.6",
    "author": "hassonofer",
    "state": "closed",
    "created_at": "2025-01-30T19:09:01Z",
    "updated_at": "2025-06-16T11:22:37Z",
    "labels": [
      "bug",
      "Vision"
    ],
    "body": "Hello,\n\nI understand this might be expected given the recent release of PyTorch 2.6, but wanted to bring it to your attention for tracking purposes.\n\nI'd like to report a compatibility issue between the Deformable DETR custom CUDA kernel and PyTorch 2.6.\nThe kernel fails to compile due to what appears to be API changes in PyTorch's type system.\n\nI cut some of the error message out, but the gist of it is:\n```\nCould not load the custom kernel for multi-scale deformable attention: Error building extension 'MultiScaleDeformableAttention'...\n\n.venv/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu(69): error: no suitable conversion function from \"const at::DeprecatedTypeProperties\" to \"c10::ScalarType\" exists\n         ; at::ScalarType _st = ::detail::scalar_type(the_type); ; switch (_st) { case at::ScalarType::Double: { do { if constexpr (!at::should_include_kernel_dtype( at_dispatch_name, at::ScalarType::Double)) { if (!(false)) { ::c10::detail::torchCheckFail( __func__, \"/home/hassonofer/Programming/transformers/.venv/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu\", static_cast<uint32_t>(69), (::c10::detail::torchCheckMsgImpl( \"Expected \" \"false\" \" to be true, but got false.  \" \"(Could this error message be improved?  If so, \" \"please report an enhancement request to PyTorch.)\", \"dtype '\", toString(at::ScalarType::Double), \"' not selected for kernel tag \", at_dispatch_name))); }; } } while (0); using scalar_t [[maybe_unused]] = c10::impl::ScalarTypeToCPPTypeT<at::ScalarType::Double>; return\n\n.venv/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu(140): error: no suitable conversion function from \"const at::DeprecatedTypeProperties\" to \"c10::ScalarType\" exists\n         ; at::ScalarType _st = ::detail::scalar_type(the_type); ; switch (_st) { case at::ScalarType::Double: { do { if constexpr (!at::should_include_kernel_dtype( at_dispatch_name, at::ScalarType::Double)) { if (!(false)) { ::c10::detail::torchCheckFail( __func__, \"/home/hassonofer/Programming/transformers/.venv/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu\", static_cast<uint32_t>(140), (::c10::detail::torchCheckMsgImpl( \"Expected \" \"false\" \" to be true, but got false.  \" \"(Could this error message be improved?  If so, \" \"please report an enhancement request to PyTorch.)\", \"dtype '\", toString(at::ScalarType::Double), \"' not selected for kernel tag \", at_dispatch_name))); }; } } while (0); using scalar_t [[maybe_unused]] = c10::impl::ScalarTypeToCPPTypeT<at::ScalarType::Double>; return\n```\n\n**Environment:**\n- PyTorch 2.6\n- CUDA 12.4\n- Python 3.11\n- transformers 4.48.1\n\nThank you for your time.\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Set up a fresh environment with PyTorch 2.6:\npip3 install torch torchvision torchaudio\npip3 install timm transformers\n\n2. Run the following minimal reproduction code:\n```python\nfrom transformers import DeformableDetrForObjectDetection\nmodel = DeformableDetrForObjectDetection.from_pretrained(\"SenseTime/deformable-detr\")\n```\n\n\n### Expected behavior\n\nClean compilation :)",
    "comments": [
      {
        "user": "qubvel",
        "body": "Hey @hassonofer! Thanks for opening the issue! I have a draft fix, but not able to test it yet (compilation is fixed, but I have some CUDA compact issues on my machine)\n\n- https://github.com/huggingface/transformers/pull/35979/files\n\nYou can install `transformers` from the branch to test it out:\n\n```python\npip install -U git+https://github.com/qubvel/transformers@fix-custom-kernels\n```"
      },
      {
        "user": "hassonofer",
        "body": "It works :)\n\nThanks a lot"
      },
      {
        "user": "ykn96",
        "body": "@qubvel @hassonofer  i have a similar error \n\n```\n\\models\\ops\\src\\cuda\\ms_deform_attn_cuda.cu(64): error: no suitable conversion function from \"const at::DeprecatedTypeProperties\" to \"c10::ScalarType\" exists\n\n\n\\models\\ops\\src\\cuda\\ms_deform_attn_cuda.cu(134): error: no suitable conversion function from \"const at::DeprecatedTypeProperties\" to \"c10::ScalarType\" exists\n```\n\nnot sure if it is related? I am running windows 10, CUDA 12.4, python 3.11, on a NVIDIA RTX 3080"
      }
    ]
  },
  {
    "issue_number": 38402,
    "title": "Version 4.52.3 leads to error after bundling with pyinstaller",
    "author": "HiokKuek",
    "state": "open",
    "created_at": "2025-05-27T13:20:00Z",
    "updated_at": "2025-06-16T11:19:28Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nThe new versioning induces the following error when trying to bundle a python script that contains imports from the transformers library. \n```\nFile \"functions\\classify.py\", line 1, in <module>\n    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n  File \"transformers\\utils\\import_utils.py\", line 2045, in __getattr__\n  File \"transformers\\utils\\import_utils.py\", line 2075, in _get_module\n  File \"transformers\\utils\\import_utils.py\", line 2073, in _get_module\n  File \"importlib\\__init__.py\", line 90, in import_module\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"PyInstaller\\loader\\pyimod02_importers.py\", line 450, in exec_module\n  File \"transformers\\models\\__init__.py\", line 341, in <module>\n  File \"transformers\\utils\\import_utils.py\", line 2594, in define_import_structure\n  File \"transformers\\utils\\import_utils.py\", line 2305, in create_import_structure_from_path\nFileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\ernest\\\\Desktop\\\\URL-Extractor-Extension\\\\native_messaging_genai\\\\dist\\\\main\\\\_internal\\\\transformers\\\\models\\\\__init__.pyc'\n[PYI-9628:ERROR] Failed to execute script 'main' due to unhandled exception!\n\n```\n\nError goes away after downgrading to v4.51.3\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Python script that uses transformers, in my case `from transformers import AutoTokenizer, AutoModelForSequenceClassification` \n2. bundle with pyinstall \n3. run the .exe file \n\n### Expected behavior\n\nThe things you intend to do with your python script (native host application in my case). \n\nThe recent update made it such that your application will crash wth the error as described above when you try and run the .exe. ",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hmmn, I'm not sure we officially support Pyinstaller, and this error message doesn't give us a lot of information about what's happening!\n\nCan you do a [git bisect](https://git-scm.com/docs/git-bisect) to find the commit that causes this issue? That might give us a starting point to investigate and fix it"
      },
      {
        "user": "isabellalcarmo",
        "body": "I'm experiencing a very similar issue when using sentence-transformers and transformers together in a PyInstaller bundle. The error I get is:\n```FileNotFoundError: [WinError 3] The system cannot find the path specified: '...\\\\_MEIxxxxx\\\\transformers\\\\models\\\\__init__.pyc'```\n\nI'm using Anaconda on Windows, and already tried collect_data_files and collect_submodules in the .spec file — no luck so far.\n\nI described the full problem and the steps I’ve tried on Stack Overflow:\n[https://stackoverflow.com/questions/79643167/error-creating-pyinstaller-executable-with-sentence-transformers-model-and-anaco](https://stackoverflow.com/questions/79643167/error-creating-pyinstaller-executable-with-sentence-transformers-model-and-anaco)\n\nAny suggestions or workarounds would be really appreciated. Thanks in advance!\n"
      },
      {
        "user": "Rocketknight1",
        "body": "You can try posting on the community support space, where someone from the community or one of the Hugging Face engineers might be able to help: https://huggingface.co/spaces/transformers-community/support\n\nStill, the git bisect output would be quite helpful to us for tracking this one down!"
      }
    ]
  },
  {
    "issue_number": 33260,
    "title": "Community contribution: Adding GGUF support for more architectures",
    "author": "SunMarc",
    "state": "open",
    "created_at": "2024-09-02T13:41:47Z",
    "updated_at": "2025-06-16T09:33:27Z",
    "labels": [
      "Good Second Issue",
      "Feature request"
    ],
    "body": "### Feature request\n\nRecently, we have added the ability to load `gguf` files within [transformers](https://huggingface.co/docs/hub/en/gguf).\n\n<img src=\"https://github.com/user-attachments/assets/61df6455-6016-449e-a37f-9dfc7f918902\" width=\"600\">\n\n\nThe goal was to offer the possibility to users to further train/fine-tune their gguf models. \n<details>\n<summary>See Workflow</summary>\n1) Load gguf file in transformers: we dequantize the weights to fp32, then we load the weights to be used with PyTorch.\n\n```py \nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\nfilename = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)\n```\n2) train/finetune\n\n3) Convert the model back to gguf to use in the ggml ecosystem using [convert_hf_to_gguf](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) script or using [gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space if you pushed your model on the hub :\n```py\ntokenizer.save_pretrained('directory')\nmodel.save_pretrained('directory')\n\n!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}\n```\n</details>\n\nLet's try to add GGUF support for more architectures! Currently supported architectures are\n\n- [x] Llama\n- [x] Mistral\n- [x] Qwen2\n\nIt would be great to add the support for more architectures such as\n\n- [x] Phi3 https://github.com/huggingface/transformers/pull/31844\n- [x] Qwen2Moe https://github.com/huggingface/transformers/pull/33264\n- [x] Gemma2 \n- [x] T5 https://github.com/huggingface/transformers/pull/33389\n- [x] Falcon https://github.com/huggingface/transformers/pull/33437\n- [x] Bloom https://github.com/huggingface/transformers/pull/33473\n- [x] StableLM https://github.com/huggingface/transformers/pull/33793\n- [x] gpt2 https://github.com/huggingface/transformers/pull/34044\n- [x] starcoder2 https://github.com/huggingface/transformers/pull/34094\n- [ ] llama4\n- [ ] Deepseekv3 \n- [ ] c4ai-command-a\n\n... and many more (Feel free to suggest more architectures ! The model needs to integrated in transformers)\n\nAdding this feature would require to follow the same protocol as in this [PR](https://github.com/huggingface/transformers/pull/31175/files) : \n1) Update `GGUF_TENSOR_MAPPING` and `GGUF_CONFIG_MAPPING` in order to map the tensor/config of the gguf file to the one on transformers. \n2) Create a `GGUFXXXConverter(XXXConverter)` class to convert the gguf tokenizer to a transformers one. \n3) Write tests\n\n\nIf you are interested to take up the challenge, comment below with the architecture name you want to integrate and open a PR! \n\nOnce you open a PR, feel free to ping @SunMarc @LysandreJik @ArthurZucker for a review ! \n\n### Motivation\n\nSupport for more gguf models\n\n### Your contribution\n\nReviewing PRs and possibly adding the support for more models",
    "comments": [
      {
        "user": "VladOS95-cyber",
        "body": "@SunMarc I am going to take Qwen2Moe"
      },
      {
        "user": "KingNish24",
        "body": "@SunMarc I want to take Gemma2"
      },
      {
        "user": "junejae",
        "body": "@SunMarc May I suggest & take T5? Seems [GGUF version of T5 encoder](https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf) is highly used for getting along with FLUX."
      }
    ]
  },
  {
    "issue_number": 38665,
    "title": "Exception while inference Qwen2VL and Qwen2VL, assert module.weight.shape[1] == 1",
    "author": "iglaweb",
    "state": "open",
    "created_at": "2025-06-07T21:28:40Z",
    "updated_at": "2025-06-16T08:11:56Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n    transformers version: 4.52.3\n    Platform: Linux-5.10.0-1029-oem-x86_64-with-glibc2.31\n    GPU device: Quadro RTX 8000\n    Python version: 3.10\n    Huggingface_hub version: 0.32.2\n    Safetensors version: 0.5.3\n    Accelerate version: 0.34.2\n    PyTorch version (GPU?): 2.5.0+cu124\n    Using distributed or parallel set-up in script?: No\n\n\n\n\n### Who can help?\n\n@zucchini-nlp \n@qubvel \n@ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nI followed these tutorials:\nhttps://colab.research.google.com/github/huggingface/cookbook/blob/main/notebooks/en/fine_tuning_vlm_trl.ipynb\nhttps://github.com/QwenLM/Qwen2.5-VL/blob/d2240f11656bfe404b9ba56db4e51cd09f522ff1/qwen-vl-finetune/qwenvl/train/train_qwen.py\n\nSteps to reproduce the issue:\n\n1. Fine-tune Qwen2VL or Qwen2.5VL (e.g. \"Qwen/Qwen2.5-VL-3B-Instruct\") model on custom dataset (Qlora and LoRA enabled, use cuda)\n2. Run the inference on a video (use cuda).\n\nFull log and exception:\n\n```\n- This IS expected if you are initializing Qwen2_5_VLForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Qwen2_5_VLForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Qwen2_5_VLForConditionalGeneration were not initialized from the model checkpoint at /home/user/Desktop/demo/tmp/weights_2025-05-30_13.06.42.192256_qwen_qwen2.5-vl-3b-instruct_b2_e1_vf16_fps1.0/model_qwen2vl_video-lora and are newly initialized: ['model.language_model.layers.0.self_attn.k_proj.bias', 'model.language_model.layers.0.self_attn.k_proj.weight', 'model.language_model.layers.0.self_attn.o_proj.weight', 'model.language_model.layers.0.self_attn.q_proj.bias', 'model.language_model.layers.0.self_attn.q_proj.weight', 'model.language_model.layers.0.self_attn.v_proj.bias', 'model.language_model.layers.0.self_attn.v_proj.weight', 'model.language_model.layers.1.self_attn.k_proj.bias', 'model.language_model.layers.1.self_attn.k_proj.weight', 'model.language_model.layers.1.self_attn.o_proj.weight', 'model.language_model.layers.1.self_attn.q_proj.bias', 'model.language_model.layers.1.self_attn.q_proj.weight', 'model.language_model.layers.1.self_attn.v_proj.bias', 'model.language_model.layers.1.self_attn.v_proj.weight', 'model.language_model.layers.10.self_attn.k_proj.bias', 'model.language_model.layers.10.self_attn.k_proj.weight', 'model.language_model.layers.10.self_attn.o_proj.weight', 'model.language_model.layers.10.self_attn.q_proj.bias', 'model.language_model.layers.10.self_attn.q_proj.weight', 'model.language_model.layers.10.self_attn.v_proj.bias', 'model.language_model.layers.10.self_attn.v_proj.weight', 'model.language_model.layers.11.self_attn.k_proj.bias', 'model.language_model.layers.11.self_attn.k_proj.weight', 'model.language_model.layers.11.self_attn.o_proj.weight', 'model.language_model.layers.11.self_attn.q_proj.bias', 'model.language_model.layers.11.self_attn.q_proj.weight', 'model.language_model.layers.11.self_attn.v_proj.bias', 'model.language_model.layers.11.self_attn.v_proj.weight', 'model.language_model.layers.12.self_attn.k_proj.bias', 'model.language_model.layers.12.self_attn.k_proj.weight', 'model.language_model.layers.12.self_attn.o_proj.weight', 'model.language_model.layers.12.self_attn.q_proj.bias', 'model.language_model.layers.12.self_attn.q_proj.weight', 'model.language_model.layers.12.self_attn.v_proj.bias', 'model.language_model.layers.12.self_attn.v_proj.weight', 'model.language_model.layers.13.self_attn.k_proj.bias', 'model.language_model.layers.13.self_attn.k_proj.weight', 'model.language_model.layers.13.self_attn.o_proj.weight', 'model.language_model.layers.13.self_attn.q_proj.bias', 'model.language_model.layers.13.self_attn.q_proj.weight', 'model.language_model.layers.13.self_attn.v_proj.bias', 'model.language_model.layers.13.self_attn.v_proj.weight', 'model.language_model.layers.14.self_attn.k_proj.bias', 'model.language_model.layers.14.self_attn.k_proj.weight', 'model.language_model.layers.14.self_attn.o_proj.weight', 'model.language_model.layers.14.self_attn.q_proj.bias', 'model.language_model.layers.14.self_attn.q_proj.weight', 'model.language_model.layers.14.self_attn.v_proj.bias', 'model.language_model.layers.14.self_attn.v_proj.weight', 'model.language_model.layers.15.self_attn.k_proj.bias', 'model.language_model.layers.15.self_attn.k_proj.weight', 'model.language_model.layers.15.self_attn.o_proj.weight', 'model.language_model.layers.15.self_attn.q_proj.bias', 'model.language_model.layers.15.self_attn.q_proj.weight', 'model.language_model.layers.15.self_attn.v_proj.bias', 'model.language_model.layers.15.self_attn.v_proj.weight', 'model.language_model.layers.16.self_attn.k_proj.bias', 'model.language_model.layers.16.self_attn.k_proj.weight', 'model.language_model.layers.16.self_attn.o_proj.weight', 'model.language_model.layers.16.self_attn.q_proj.bias', 'model.language_model.layers.16.self_attn.q_proj.weight', 'model.language_model.layers.16.self_attn.v_proj.bias', 'model.language_model.layers.16.self_attn.v_proj.weight', 'model.language_model.layers.17.self_attn.k_proj.bias', 'model.language_model.layers.17.self_attn.k_proj.weight', 'model.language_model.layers.17.self_attn.o_proj.weight', 'model.language_model.layers.17.self_attn.q_proj.bias', 'model.language_model.layers.17.self_attn.q_proj.weight', 'model.language_model.layers.17.self_attn.v_proj.bias', 'model.language_model.layers.17.self_attn.v_proj.weight', 'model.language_model.layers.18.self_attn.k_proj.bias', 'model.language_model.layers.18.self_attn.k_proj.weight', 'model.language_model.layers.18.self_attn.o_proj.weight', 'model.language_model.layers.18.self_attn.q_proj.bias', 'model.language_model.layers.18.self_attn.q_proj.weight', 'model.language_model.layers.18.self_attn.v_proj.bias', 'model.language_model.layers.18.self_attn.v_proj.weight', 'model.language_model.layers.19.self_attn.k_proj.bias', 'model.language_model.layers.19.self_attn.k_proj.weight', 'model.language_model.layers.19.self_attn.o_proj.weight', 'model.language_model.layers.19.self_attn.q_proj.bias', 'model.language_model.layers.19.self_attn.q_proj.weight', 'model.language_model.layers.19.self_attn.v_proj.bias', 'model.language_model.layers.19.self_attn.v_proj.weight', 'model.language_model.layers.2.self_attn.k_proj.bias', 'model.language_model.layers.2.self_attn.k_proj.weight', 'model.language_model.layers.2.self_attn.o_proj.weight', 'model.language_model.layers.2.self_attn.q_proj.bias', 'model.language_model.layers.2.self_attn.q_proj.weight', 'model.language_model.layers.2.self_attn.v_proj.bias', 'model.language_model.layers.2.self_attn.v_proj.weight', 'model.language_model.layers.20.self_attn.k_proj.bias', 'model.language_model.layers.20.self_attn.k_proj.weight', 'model.language_model.layers.20.self_attn.o_proj.weight', 'model.language_model.layers.20.self_attn.q_proj.bias', 'model.language_model.layers.20.self_attn.q_proj.weight', 'model.language_model.layers.20.self_attn.v_proj.bias', 'model.language_model.layers.20.self_attn.v_proj.weight', 'model.language_model.layers.21.self_attn.k_proj.bias', 'model.language_model.layers.21.self_attn.k_proj.weight', 'model.language_model.layers.21.self_attn.o_proj.weight', 'model.language_model.layers.21.self_attn.q_proj.bias', 'model.language_model.layers.21.self_attn.q_proj.weight', 'model.language_model.layers.21.self_attn.v_proj.bias', 'model.language_model.layers.21.self_attn.v_proj.weight', 'model.language_model.layers.22.self_attn.k_proj.bias', 'model.language_model.layers.22.self_attn.k_proj.weight', 'model.language_model.layers.22.self_attn.o_proj.weight', 'model.language_model.layers.22.self_attn.q_proj.bias', 'model.language_model.layers.22.self_attn.q_proj.weight', 'model.language_model.layers.22.self_attn.v_proj.bias', 'model.language_model.layers.22.self_attn.v_proj.weight', 'model.language_model.layers.23.self_attn.k_proj.bias', 'model.language_model.layers.23.self_attn.k_proj.weight', 'model.language_model.layers.23.self_attn.o_proj.weight', 'model.language_model.layers.23.self_attn.q_proj.bias', 'model.language_model.layers.23.self_attn.q_proj.weight', 'model.language_model.layers.23.self_attn.v_proj.bias', 'model.language_model.layers.23.self_attn.v_proj.weight', 'model.language_model.layers.24.self_attn.k_proj.bias', 'model.language_model.layers.24.self_attn.k_proj.weight', 'model.language_model.layers.24.self_attn.o_proj.weight', 'model.language_model.layers.24.self_attn.q_proj.bias', 'model.language_model.layers.24.self_attn.q_proj.weight', 'model.language_model.layers.24.self_attn.v_proj.bias', 'model.language_model.layers.24.self_attn.v_proj.weight', 'model.language_model.layers.25.self_attn.k_proj.bias', 'model.language_model.layers.25.self_attn.k_proj.weight', 'model.language_model.layers.25.self_attn.o_proj.weight', 'model.language_model.layers.25.self_attn.q_proj.bias', 'model.language_model.layers.25.self_attn.q_proj.weight', 'model.language_model.layers.25.self_attn.v_proj.bias', 'model.language_model.layers.25.self_attn.v_proj.weight', 'model.language_model.layers.26.self_attn.k_proj.bias', 'model.language_model.layers.26.self_attn.k_proj.weight', 'model.language_model.layers.26.self_attn.o_proj.weight', 'model.language_model.layers.26.self_attn.q_proj.bias', 'model.language_model.layers.26.self_attn.q_proj.weight', 'model.language_model.layers.26.self_attn.v_proj.bias', 'model.language_model.layers.26.self_attn.v_proj.weight', 'model.language_model.layers.27.self_attn.k_proj.bias', 'model.language_model.layers.27.self_attn.k_proj.weight', 'model.language_model.layers.27.self_attn.o_proj.weight', 'model.language_model.layers.27.self_attn.q_proj.bias', 'model.language_model.layers.27.self_attn.q_proj.weight', 'model.language_model.layers.27.self_attn.v_proj.bias', 'model.language_model.layers.27.self_attn.v_proj.weight', 'model.language_model.layers.28.self_attn.k_proj.bias', 'model.language_model.layers.28.self_attn.k_proj.weight', 'model.language_model.layers.28.self_attn.o_proj.weight', 'model.language_model.layers.28.self_attn.q_proj.bias', 'model.language_model.layers.28.self_attn.q_proj.weight', 'model.language_model.layers.28.self_attn.v_proj.bias', 'model.language_model.layers.28.self_attn.v_proj.weight', 'model.language_model.layers.29.self_attn.k_proj.bias', 'model.language_model.layers.29.self_attn.k_proj.weight', 'model.language_model.layers.29.self_attn.o_proj.weight', 'model.language_model.layers.29.self_attn.q_proj.bias', 'model.language_model.layers.29.self_attn.q_proj.weight', 'model.language_model.layers.29.self_attn.v_proj.bias', 'model.language_model.layers.29.self_attn.v_proj.weight', 'model.language_model.layers.3.self_attn.k_proj.bias', 'model.language_model.layers.3.self_attn.k_proj.weight', 'model.language_model.layers.3.self_attn.o_proj.weight', 'model.language_model.layers.3.self_attn.q_proj.bias', 'model.language_model.layers.3.self_attn.q_proj.weight', 'model.language_model.layers.3.self_attn.v_proj.bias', 'model.language_model.layers.3.self_attn.v_proj.weight', 'model.language_model.layers.30.self_attn.k_proj.bias', 'model.language_model.layers.30.self_attn.k_proj.weight', 'model.language_model.layers.30.self_attn.o_proj.weight', 'model.language_model.layers.30.self_attn.q_proj.bias', 'model.language_model.layers.30.self_attn.q_proj.weight', 'model.language_model.layers.30.self_attn.v_proj.bias', 'model.language_model.layers.30.self_attn.v_proj.weight', 'model.language_model.layers.31.self_attn.k_proj.bias', 'model.language_model.layers.31.self_attn.k_proj.weight', 'model.language_model.layers.31.self_attn.o_proj.weight', 'model.language_model.layers.31.self_attn.q_proj.bias', 'model.language_model.layers.31.self_attn.q_proj.weight', 'model.language_model.layers.31.self_attn.v_proj.bias', 'model.language_model.layers.31.self_attn.v_proj.weight', 'model.language_model.layers.32.self_attn.k_proj.bias', 'model.language_model.layers.32.self_attn.k_proj.weight', 'model.language_model.layers.32.self_attn.o_proj.weight', 'model.language_model.layers.32.self_attn.q_proj.bias', 'model.language_model.layers.32.self_attn.q_proj.weight', 'model.language_model.layers.32.self_attn.v_proj.bias', 'model.language_model.layers.32.self_attn.v_proj.weight', 'model.language_model.layers.33.self_attn.k_proj.bias', 'model.language_model.layers.33.self_attn.k_proj.weight', 'model.language_model.layers.33.self_attn.o_proj.weight', 'model.language_model.layers.33.self_attn.q_proj.bias', 'model.language_model.layers.33.self_attn.q_proj.weight', 'model.language_model.layers.33.self_attn.v_proj.bias', 'model.language_model.layers.33.self_attn.v_proj.weight', 'model.language_model.layers.34.self_attn.k_proj.bias', 'model.language_model.layers.34.self_attn.k_proj.weight', 'model.language_model.layers.34.self_attn.o_proj.weight', 'model.language_model.layers.34.self_attn.q_proj.bias', 'model.language_model.layers.34.self_attn.q_proj.weight', 'model.language_model.layers.34.self_attn.v_proj.bias', 'model.language_model.layers.34.self_attn.v_proj.weight', 'model.language_model.layers.35.self_attn.k_proj.bias', 'model.language_model.layers.35.self_attn.k_proj.weight', 'model.language_model.layers.35.self_attn.o_proj.weight', 'model.language_model.layers.35.self_attn.q_proj.bias', 'model.language_model.layers.35.self_attn.q_proj.weight', 'model.language_model.layers.35.self_attn.v_proj.bias', 'model.language_model.layers.35.self_attn.v_proj.weight', 'model.language_model.layers.4.self_attn.k_proj.bias', 'model.language_model.layers.4.self_attn.k_proj.weight', 'model.language_model.layers.4.self_attn.o_proj.weight', 'model.language_model.layers.4.self_attn.q_proj.bias', 'model.language_model.layers.4.self_attn.q_proj.weight', 'model.language_model.layers.4.self_attn.v_proj.bias', 'model.language_model.layers.4.self_attn.v_proj.weight', 'model.language_model.layers.5.self_attn.k_proj.bias', 'model.language_model.layers.5.self_attn.k_proj.weight', 'model.language_model.layers.5.self_attn.o_proj.weight', 'model.language_model.layers.5.self_attn.q_proj.bias', 'model.language_model.layers.5.self_attn.q_proj.weight', 'model.language_model.layers.5.self_attn.v_proj.bias', 'model.language_model.layers.5.self_attn.v_proj.weight', 'model.language_model.layers.6.self_attn.k_proj.bias', 'model.language_model.layers.6.self_attn.k_proj.weight', 'model.language_model.layers.6.self_attn.o_proj.weight', 'model.language_model.layers.6.self_attn.q_proj.bias', 'model.language_model.layers.6.self_attn.q_proj.weight', 'model.language_model.layers.6.self_attn.v_proj.bias', 'model.language_model.layers.6.self_attn.v_proj.weight', 'model.language_model.layers.7.self_attn.k_proj.bias', 'model.language_model.layers.7.self_attn.k_proj.weight', 'model.language_model.layers.7.self_attn.o_proj.weight', 'model.language_model.layers.7.self_attn.q_proj.bias', 'model.language_model.layers.7.self_attn.q_proj.weight', 'model.language_model.layers.7.self_attn.v_proj.bias', 'model.language_model.layers.7.self_attn.v_proj.weight', 'model.language_model.layers.8.self_attn.k_proj.bias', 'model.language_model.layers.8.self_attn.k_proj.weight', 'model.language_model.layers.8.self_attn.o_proj.weight', 'model.language_model.layers.8.self_attn.q_proj.bias', 'model.language_model.layers.8.self_attn.q_proj.weight', 'model.language_model.layers.8.self_attn.v_proj.bias', 'model.language_model.layers.8.self_attn.v_proj.weight', 'model.language_model.layers.9.self_attn.k_proj.bias', 'model.language_model.layers.9.self_attn.k_proj.weight', 'model.language_model.layers.9.self_attn.o_proj.weight', 'model.language_model.layers.9.self_attn.q_proj.bias', 'model.language_model.layers.9.self_attn.q_proj.weight', 'model.language_model.layers.9.self_attn.v_proj.bias', 'model.language_model.layers.9.self_attn.v_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n  0%|          | 0/111 [00:00<?, ?it/s]Unused or unrecognized kwargs: fps, return_tensors.\n/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:354: UserWarning: FP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.\n  warnings.warn(\n  0%|          | 0/111 [00:16<?, ?it/s]\nTraceback (most recent call last):\n  File \"/home/user/Desktop/demo/hf_qwen_demo_video.py\", line 356, in <module>\n    eval_videos(\n  File \"/home/user/Desktop/demo/hf_qwen_demo_video.py\", line 212, in eval_videos\n    pred_caption_list = run_model_preds(\n  File \"/home/user/Desktop/demo/hf_qwen_demo_video.py\", line 180, in run_model_preds\n    output_text = run_model_single_inference(\n  File \"/home/user/Desktop/demo/hf_qwen_demo_video.py\", line 116, in run_model_single_inference\n    output_ids = model.generate(**inputs, max_new_tokens=max_token_length)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2597, in generate\n    result = self._sample(\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3557, in _sample\n    outputs = self(**model_inputs, return_dict=True)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1908, in forward\n    outputs = self.model(\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1728, in forward\n    outputs = self.language_model(\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1191, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1053, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 938, in forward\n    query_states = self.q_proj(hidden_states)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 468, in forward\n    fix_4bit_weight_quant_state_from_module(self)\n  File \"/home/szhou/anaconda3/envs/my_project/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 360, in fix_4bit_weight_quant_state_from_module\n    assert module.weight.shape[1] == 1\nAssertionError\n\nProcess finished with exit code 1\n\n```\n\n### Expected behavior\n\nI expect the inference to complete without errors.",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "@iglaweb we recently fixed one bug when saving Qwen-VL models and it is in the latest patch release. Can you try to update the transformers version?\n\nProb you'll need to re-save the checkpoint, I will help with that if you can share it on the hub"
      },
      {
        "user": "msciancalepore98",
        "body": "hey @zucchini-nlp  I have trained qwen2.5-VL-3B model 1month ago and right now I have this error when attempting to load the adapters..\n\n```\nUnrecognized video processor in /content/lora_model_qwen2.5-VL-3B/lora_model_qwen2.5-VL-3B. Should have a `video_processor_type` key in its video_preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: instructblip, instructblipvideo, internvl, llava_next_video, llava_onevision, qwen2_5_omni, qwen2_5_vl, qwen2_vl, smolvlm, video_llava\n```\n\ncan you please guide me in how I should tweak the adapter config? I mean.. I hope I dont have to re-train.."
      },
      {
        "user": "msciancalepore98",
        "body": "I've just seen that transformers has added VideoProcessors as first class citizens processors.. can it be related @zucchini-nlp @danielhanchen  ?"
      }
    ]
  },
  {
    "issue_number": 36773,
    "title": "Inconsistent Documentation for `⁠dataset_index` Requirement Across ViTPose Models",
    "author": "harpreetsahota204",
    "state": "open",
    "created_at": "2025-03-17T18:37:19Z",
    "updated_at": "2025-06-16T08:03:23Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n## Description\n\nThere's confusion regarding the `dataset_index` parameter requirement across the ViTPose model family. The documentation only mentions this requirement for **some** of the models, initially when the model was released it was only for the `usyd-community/vitpose-plus-base` checkpoint.\n\nBut now other checkpoints also fail without it. I'm concerned about:\n\n1. Whether ALL models in the family require this parameter, even if some don't explicitly fail without it\n2. Whether adding this parameter to models that don't explicitly require it could affect inference results\n\n## Current Behavior\nWhen following the examples in the model cards, these checkpoints fail with:\n```\nValueError: dataset_index must be provided when using multiple experts (num_experts=6). Please provide dataset_index to the forward pass.\n```\n\nAffected checkpoints:\n- `usyd-community/vitpose-plus-small`\n- `usyd-community/vitpose-plus-large`\n- `usyd-community/vitpose-plus-huge`\n\n## Questions\n1. Should the `dataset_index` parameter be added to ALL ViTPose+ models for consistency?\n2. Will adding `inputs[\"dataset_index\"] = torch.tensor([0], device=device)` to models that don't explicitly require it affect inference results?\n3. Is there a recommended approach for handling this parameter in applications that need to work with multiple ViTPose+ variants?\n\n## Context\nI've implemented a plugin using these models, and it's failing on certain checkpoints. I want to ensure any fix I implement won't compromise results for other checkpoints that currently work without this parameter.\n\n## Suggested Documentation Update\nIf all models require this parameter, please update all model cards to include:\n```python\ninputs[\"dataset_index\"] = torch.tensor([0], device=device)\n```\n\nIf only certain models require it, please clearly indicate which ones do and which ones don't.\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n## Steps to Reproduce\n\n1. Load any of the affected models:\n- `usyd-community/vitpose-plus-small`\n- `usyd-community/vitpose-plus-large`\n- `usyd-community/vitpose-plus-huge`\n\n2. Process an image following the model card example and run inference\n\n3. Observe the error:\n   ```\n   ValueError: dataset_index must be provided when using multiple experts (num_experts=6). Please provide dataset_index to the forward pass.\n   ```\n\n\n\n### Expected behavior\n\n1. All model cards should document whether the `dataset_index` parameter is required for that specific checkpoint.\n\n2. Documentation should be consistent across all models in the ViTPose+ family that share the same requirements.\n\n3. Ideally, the model cards for all affected checkpoints should include the necessary code snippet:\n   ```python\n   inputs[\"dataset_index\"] = torch.tensor([0], device=device)\n   ```\n\n4. Clear guidance on whether this parameter affects inference results and if it should be applied universally across all ViTPose models for consistency.\n\n",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @nielsrogge for ViTPose"
      },
      {
        "user": "NielsRogge",
        "body": "Pinging @qubvel here. "
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37423,
    "title": "pytorch_utils.py > isin_mps_friendly > RuntimeError: Expected elements.dtype() == test_elements.dtype() to be true, but got false.",
    "author": "f2janyway",
    "state": "open",
    "created_at": "2025-04-10T13:33:46Z",
    "updated_at": "2025-06-16T08:02:53Z",
    "labels": [
      "bug"
    ],
    "body": "\nI just followed this tutorial. but got error\n\nhttps://huggingface.co/learn/audio-course/en/chapter2/tts_pipeline\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text-to-speech\", model=\"suno/bark-small\")\n\ntext = \"Ladybugs have had important roles in culture and religion, being associated with luck, love, fertility and prophecy. \"\noutput = pipe(text)\n```\n```\nRuntimeError: Expected elements.dtype() == test_elements.dtype() to be true, but got false.\n```\nhttps://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src/transformers/pytorch_utils.py#L334\n\nso I just added this code. and works fine. when I took a log with bug code the elements.dtype shown 'int32' but others elements.dtype, and test_elements.dtype 'int64'\n\n```python\n        if elements.dtype != test_elements.dtype:\n            elements = elements.to(dtype=test_elements.dtype)\n        # Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\n        return torch.isin(elements, test_elements)\n```\n\n### my env:\nmacbook pro m1\n\nconda\n\ntransformers-cli env\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n- `transformers` version: 4.51.1\n- Platform: macOS-15.3.1-arm64-arm-64bit\n- Python version: 3.10.12\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: No. In fact. I don't know. I didn't set about these.\n\n\n### Who can help?\n\n@gante \n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\njust run above code.\n\n### Expected behavior\n\nRuntimeError",
    "comments": [
      {
        "user": "manueldeprada",
        "body": "Hey @f2janyway , thanks for the report! We have had similar issues where MPS initialises tensor by default to int32 instead of int64. We should catch where this initialization is happening instead of casting them in `isin_mps_friendly()` :)\n\nCould you please write this into `isin_mps_friendly()`:\n```python\nif elements.dtype != test_elements.dtype:\n        import traceback\n        traceback.print_stack()\n```\n\nso that we know which call is causing the dtype mismatch? \n\nThanks!"
      },
      {
        "user": "f2janyway",
        "body": "Ok. Thanks"
      },
      {
        "user": "manueldeprada",
        "body": "I meant if you could run it locally and let me know here :)"
      }
    ]
  },
  {
    "issue_number": 37908,
    "title": "DynamicCache results in too many torch recompiles after 4.51",
    "author": "flishwang",
    "state": "open",
    "created_at": "2025-05-01T05:54:10Z",
    "updated_at": "2025-06-16T08:02:34Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\naccelerate=1.6.0, OS=ubuntu 22.04, numpy=1.26.4, torch=2.6.0+cu124, python=3.10\n\n### Who can help?\n\n@ArthurZucker \n\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM\nsequence_label_locs = torch.as_tensor([,-40,-20,-10,-1]).cuda()\nlevel_ids_gpu = torch.as_tensor([76,77,78,79,80]).cuda()\ncache = None\ncls_ids_gpu = torch.as_tensor([103,130,1166,1366,3366]).cuda()\nmodel = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2-1.5B-Base').half().cuda()\nmodel = torch.compile(model)\nbatch = torch.randint(low=0,high=100000,size=(8,4096)).cuda()\nmask = torch.ones((8,4096),dtype=torch.int64).cuda()\n\nloc_stride=256\n@torch.no_grad()\ndef model_predict(model, batch, mask,sequence_label_locs, level_ids_gpu, cls_ids_gpu, cache):\n    model_predict_argmax = None\n    past_kv_cache = cache\n    predicts = []\n    non_zero_loc = mask.argmax(-1).min() // loc_stride * loc_stride\n    batch=batch[:,non_zero_loc:]\n    mask=mask[:,non_zero_loc:]\n    sequence_label_locs = sequence_label_locs - non_zero_loc\n\n    for idx in range(len(sequence_label_locs)):\n        if idx == 0:\n            input_ids = batch[:,:sequence_label_locs[idx]]\n        else:\n            input_ids = torch.cat([model_predict_argmax,\n                                   batch[:,sequence_label_locs[idx-1]+1:sequence_label_locs[idx]]],1)\n        input_mask = mask[:,:sequence_label_locs[idx]]\n        outputs = model(input_ids,attention_mask = input_mask,past_key_values = past_kv_cache, use_cache=True,\n                        logits_to_keep = 1,\n                        return_dict = True)\n        last_predicts = outputs.logits[:,-1:,:]\n        predicts.append(last_predicts)\n        past_kv_cache = outputs.past_key_values\n        if idx == 0:\n            model_predict_argmax = cls_ids_gpu[last_predicts[:,:,cls_ids_gpu].argmax(-1)]\n        else:\n            model_predict_argmax = level_ids_gpu[last_predicts[:, :, level_ids_gpu].argmax(-1)]\n    return torch.cat(predicts,1)\n\nfor i in range(0,4000,10):\n    mask[:,:i]=0\n    outs = model_predict(model, batch, mask,sequence_label_locs, level_ids_gpu, cls_ids_gpu, cache)\n\n```\n\n\n### Expected behavior\n\n\nRun the upper scripts with TORCH_LOGS=recompiles\nIf I use transformer==4.49, only several recompiles happes.\nBut If I use transformers>=4.51, re-compiling always happens if I changed the input sequences' length.\n\nI notice that it seems to be related to the following code:\nhttps://github.com/huggingface/transformers/blob/7a3e208892c06a5e278144eaf38c8599a42f53e7/src/transformers/cache_utils.py#L442\n```\n                not self.key_cache[layer_idx].numel()  # prefers not t.numel() to len(t) == 0 to export the model\n```\nwhich in transformers<=4.49 is \n```\n                not len(self.key_cache[layer_idx])\n```\n\n\nIf I modified the code to\n```\n                not self.key_cache[layer_idx].shape[0]\n```\n, the re-compiling times could also be reduced.\n\nI'm not expert for torch/transformers, and I'm not sure which project (pytorch or transformers?) this bug belongs to.\nI'm also not sure if the proposed modification would break the model export procedure.",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "Hey @flishwang !\n\n`DynamicCache` should not be used when compiling a model. For compilations we have [`StaticCache`](https://huggingface.co/docs/transformers/v4.49.0/kv_cache#static-cache) which pre allocates maximum length for keys and values. \n\nYou can take a look at [this section](https://huggingface.co/docs/transformers/v4.49.0/en/llm_optims#static-kv-cache-and-torchcompile) in docs for how to use static cache with compile. The easiest way would be to call `generate(**inputs, cache_implementation=\"static\")` but it can be configured for specific use cases as well"
      },
      {
        "user": "mklasby",
        "body": "@zucchini-nlp one issue is static cache is not supported with speculative decoding yet. Are you aware of any efforts to implement static cache + assisted decoding? Thx"
      },
      {
        "user": "zucchini-nlp",
        "body": "@mklasby not that of I know. We had a community contribution in the past and realized that supporting static cache in the assistant model would require us to handle a lot of egde cases. cc @gante if I missed anythin"
      }
    ]
  },
  {
    "issue_number": 38027,
    "title": "TimeSformer assumes a fixed number of frames in its layers even though it interpolates temporal embeddings based on the input",
    "author": "kamila-chay",
    "state": "closed",
    "created_at": "2025-05-08T17:02:12Z",
    "updated_at": "2025-06-16T08:02:29Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.0.dev0\n- Platform: Linux-6.11.0-24-generic-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config: \tnot found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.5.1+cu121 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: no\n- Using GPU in script?: yes\n- GPU type: NVIDIA GeForce RTX 4070 Laptop GPU\n\n\n### Who can help?\n\n@amyeroberts @qubvel \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI created a short Jupyter notebook: [Collab](https://colab.research.google.com/drive/1vTp8p2RMGcv5YmcZoljgpEWCiqFl4vmX?usp=sharing)\n\n### Expected behavior\n\n**Timesformer should infer the number of frames dynamically instead of relying on the config, while continuing to infer the image size from config values.**\n\nWhile it's reasonable for the spatial dimensions (like image_size) to be fixed and defined in the config — since they don't typically vary within a single video — temporal length often does, especially in setups using staggered windows to process long videos at high temporal density.\n\nIn these cases, it's more practical if the model dynamically infers the number of frames from the input shape, rather than requiring every chunk to match the fixed num_frames set in the config. This would simplify usage and reduce unnecessary edge-case handling downstream.\n\n[EDIT] This issue also arises when using pretrained temporal embeddings that need to be interpolated to accommodate longer sequences. While the interpolation itself proceeds without error, the attention modules can either fail outright or—more subtly—rely on incorrect attention patterns. Notably, this isn't limited to staggered window configurations; it can occur more broadly whenever temporal dimensions are extended beyond the pretrained length.\n\n`\ndef forward(self, hidden_states: torch.Tensor, output_attentions: bool = False): \n\n        num_frames = self.config.num_frames \n        num_patch_width = self.config.image_size // self.config.patch_size\n        batch_size = hidden_states.shape[0]\n        num_spatial_tokens = (hidden_states.size(1) - 1) // num_frames\n        num_patch_height = num_spatial_tokens // num_patch_width`\n\nI believe this would make the model more flexible and robust in practice.\n\nLet me know if this direction seems reasonable — I’m happy to open a PR with a proposed fix (also open to creating a custom flag in the config in order to not break BC).",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @fcakyon as the original PR author as well!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38116,
    "title": "[Bug in Generate] 4.51.2 vs 4.46 Beam search results are sometimes different, not sure if beam search or T5 model change is the reason?",
    "author": "Oxi84",
    "state": "open",
    "created_at": "2025-05-13T23:01:12Z",
    "updated_at": "2025-06-16T08:02:23Z",
    "labels": [
      "bug"
    ],
    "body": "I get different results in version 4.51.2 compared to 4.46. Diverse beam works well, normal beam search does not, it sometimes just generates all the same sequences when generating multiple beams (4-8).\n\nSome small bug just generate first 2 beams ok, and after just repeats the second one. It happens in around 5 percent of input senences but when it does it gives 2 instead of 8 different versions of the text which is pretty bad.\n\nOr there is something different about loading the T5 model, i remember there was a problem about loading in .half vs when you mark dtype as fp16?\n\nIn around 10 percent of cases i get different results from 4.46 via 4.51. Inputs are exactly the same, i checked tokenized version id and they are the same.\n\nModel is t5 and the params are just basic, beam search, num_beams and num_returned no other params and does not match.\n\nThanks!\n\n\n### System Info\n\nUbuntu transformers, cuda\n\n\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nAny t5 model basic usage:\n\nfor example model: \n\n                   model_id=prithivida/parrot_paraphraser_on_T5\n\n                    beam_outputs = model.generate(\n                            input_ids=input_ids, attention_mask=attention_masks,\n                            do_sample=False,\n                            num_beams=num_beams,\n                            max_length=max_len,\n                            num_return_sequences=num_beams,\n                            ) \n\n### Expected behavior\n\nshould show the same",
    "comments": [
      {
        "user": "Oxi84",
        "body": "@gante "
      },
      {
        "user": "Oxi84",
        "body": "The bug makes most the beams the same, 1st and 2nd, 2nd, 2nd .., I found a workaround and this might help with the solution. Using diverse beam search with group 2, and 2x original beams gives exactly the same output as in version 4.46, just of course it need 2x more memory:\n\n```\n                    beam_outputs = model.generate(\n                            input_ids=input_ids, attention_mask=attention_masks,\n                            do_sample=False,\n                            num_beams=2*num_beams,\n                            max_length=max_len,\n                            num_return_sequences=2*num_beams,\n                            num_beam_groups = 2,\n                            diversity_penalty = 0.00001 #so it is the same\n                            )  \n\n```\n\n"
      },
      {
        "user": "Oxi84",
        "body": "I use this is a temporary solution, but it does works 2x slower as I have to use 2x more beams as I cant set num_beam_groups  to 1.\n\nThere should be no difference between first and second generation at all.\n\n\n```\nif use_beam_trick:\n    beam_outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_masks,\n        do_sample=False,\n        num_beams=2*num_beams,\n        num_beam_groups=2,\n        diversity_penalty=0.00001,\n        max_length=max_len,\n        num_return_sequences=2*num_beams,\n        encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size_val\n    )\n    # dedup\n    unique_outputs = []\n    for i in range(beam_outputs.size(0)):\n        if not any(torch.equal(beam_outputs[i], x) for x in unique_outputs):\n            unique_outputs.append(beam_outputs[i])\n    beam_outputs = torch.stack(unique_outputs)\nelse:\n    beam_outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_masks,\n        do_sample=False,\n        num_beams=num_beams,\n        max_length=max_len,\n        num_return_sequences=num_beams,\n        encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size_val\n    )\n```\n"
      }
    ]
  },
  {
    "issue_number": 38007,
    "title": "Does Qwen_2_5_VL support variable length attention computation?",
    "author": "yingtongxiong",
    "state": "open",
    "created_at": "2025-05-08T02:38:21Z",
    "updated_at": "2025-06-16T07:03:08Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nQwen_2_5_VL support variable length attention computation\n\n### Motivation\n\nHello, I try to run qwen25_vl with packing samples, however, I found that it seems this function only passes the attention_mask, not the position_ids in [https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L908](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L908). So I pass the position_ids to this function and met the illegal memory access. Finally, I found that the position_ids has been expanded 3 times in dim 0, so how can I use the position_ids, what if I want to use varlen flash attention? Would anyone be able to help me with this?\n\n### Your contribution\n\nno",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "@yingtongxiong Qwen VL position ids are different from simple LLMs, so simply passing `position_ids` tp FA2 for packing will not solve the issue. Probably we'll need to pass different set of `position_ids` or infer it from 3D ids. I will take a look at it "
      },
      {
        "user": "yingtongxiong",
        "body": "@zucchini-nlp thank you very much. I see in verl, it passes position_ids[0] to flash attention. I am not sure it is correct."
      },
      {
        "user": "Pankajku-mar",
        "body": "Hello @zucchini-nlp and team,\n\nI've been reviewing this feature request (#38007) and I find the challenge of implementing variable-length attention for the Qwen_2_5_VL model very interesting\n\nBefore I start exploring a potential solution, could you please confirm that no one else is actively working on this? I would be happy to take this on. If you have any initial guidance or specific requirements for the implementation (e.g., preferred attention backend like sdpa or flash_attention_2), that would also be very helpful.\n\nI look forward to contributing."
      }
    ]
  },
  {
    "issue_number": 38837,
    "title": "Loss is incorrectly scaled in Trainer during the last step with gradient accumulation when the final batch is smaller than accumulation steps.",
    "author": "hutaiHang",
    "state": "open",
    "created_at": "2025-06-16T06:44:03Z",
    "updated_at": "2025-06-16T06:44:03Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.51.3\n- Platform: Linux-5.10.134-010.ali5000.al8.x86_64-x86_64-with-glibc2.32\n- Python version: 3.10.16\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: 0.15.4\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA A800-SXM4-80GB\n\n### Who can help?\n\n@zach-huggingface @SunMarc\n\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nWhen using `gradient_accumulation_steps` in the Trainer, the calculated loss is divided by this number before the backward pass.  As shown in [this](https://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/trainer.py#L3790):\nhttps://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/trainer.py#L3790\n```python\n  if (not self.model_accepts_loss_kwargs or num_items_in_batch is None) and self.compute_loss_func is None:\n            loss = loss / self.args.gradient_accumulation_steps\n```\nThis is intended to average the loss over the accumulated steps. However, a problem arises on the very last training step if the remaining number of batches in the dataloader is less than gradient_accumulation_steps.\nAs show in [this](https://github.com/huggingface/transformers/blob/d5d007a1a0f0c11a726a54c8f00bd71825f84d02/src/transformers/trainer.py#L2501C1-L2503C59), When `num_batches=args.gradient_accumulation_steps and num_batches > len(batch_samples)`:\n```python\nnum_batches = args.gradient_accumulation_steps if update_step != (total_updates - 1) else remainder\nbatch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)\nfor i, inputs in enumerate(batch_samples):\n    xxxx\n```\nIn this scenario, the loss is still divided by the full gradient_accumulation_steps, even though the actual number of accumulated batches is smaller. This results in a final loss value that is artificially small, leading to an incorrect gradient magnitude for the final optimization step. \n\n**To Reproduce**\n+ Initialize a Trainer.\n+ Use a dataset where the total number of samples is not perfectly divisible by per_device_train_batch_size * gradient_accumulation_steps.\n+ Train the model for one epoch.\n+ Observe the loss value on the final logging step. It will be significantly smaller than the others if the last accumulation cycle has fewer batches than gradient_accumulation_steps.\n\nAn simple example code is below:\n```python\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    AutoConfig\n)\nfrom transformers.utils import logging as hf_logging\n\n# 1. Define the model and tokenizer\nmodel_name = \"bert-base-uncased\"\nconfig = AutoConfig.from_pretrained(model_name)\n\n# Set all dropout probabilities to 0.0, To eliminate the randomness of each forward pass\nconfig.hidden_dropout_prob = 0.0\nconfig.attention_probs_dropout_prob = 0.0\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    config=config,\n)\n\n# 2. Create a simple dataset\n# Total 10 samples, batch_size=2, gradient accumulation=2\n# This results in 5 batches in total. The first 4 batches complete one gradient update.\n# The 5th batch is the last one, forming an accumulation cycle by itself, but with only one batch.\nnum_samples = 10\ntrain_dataset = [\n    {\n        \"input_ids\": torch.randint(100, 2000, (8,)), # random generate\n        \"attention_mask\": torch.ones(8),\n        \"labels\": torch.randint(0, 2, (1,)).item()\n    }\n]*num_samples\n\n# 3. Set training parameters\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    num_train_epochs=2,\n    logging_steps=1,\n    report_to=\"none\",\n    lr_scheduler_type = \"constant\",\n    learning_rate = 0.0 #Without updating parameters\n)\n\n\n# 4. training\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n```\nRunning the code, the output log is:\n\n> {'loss': 0.9984, 'grad_norm': 36.21815490722656, 'learning_rate': 0.0, 'epoch': 0.4}                                                                                       \n> {'loss': 0.9984, 'grad_norm': 36.21815490722656, 'learning_rate': 0.0, 'epoch': 0.8}                                                                                       \n> {'loss': 0.4992, 'grad_norm': 18.10907745361328, 'learning_rate': 0.0, 'epoch': 1.0}    <-- **The problem!**                                                                                   \n> {'loss': 0.9984, 'grad_norm': 36.21815490722656, 'learning_rate': 0.0, 'epoch': 1.4} \n\n\n### Expected behavior\n\nThe loss scaling should be adjusted based on the actual number of batches accumulated in a given cycle. For the final (and potentially incomplete) accumulation cycle, the loss should be divided by the number of batches actually processed in that cycle, not by the total `gradient_accumulation_steps`.",
    "comments": []
  },
  {
    "issue_number": 38300,
    "title": "Will Gemma 3n be added to transformers?",
    "author": "TheMrCodes",
    "state": "open",
    "created_at": "2025-05-22T15:26:20Z",
    "updated_at": "2025-06-16T06:33:54Z",
    "labels": [
      "New model"
    ],
    "body": "### Model description\n\nQuestion: Are there plans from Google or Huggingface to implement Gemma 3n in other frameworks?\n\nI've seen the LiteRT weights and Android App Link on Huggingface, and was wandering if it would be possible to convert the model architecture in the *.task file to a transformer pytorch Module?\n\nPersonally I'll really interested in the Per-Layer Embeddings and MatFormer implementation they used, but do not have any experience with Tensorflow Lite\n\n### Open source status\n\n- [ ] The model implementation is available\n- [X] The model weights are available\n\n### Provide useful links for the implementation\n\nhttps://huggingface.co/google/gemma-3n-E4B-it-litert-preview",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "@TheMrCodes hey! Yes, adding Gemma3n is planned, we're working on it"
      },
      {
        "user": "weiyx16",
        "body": "Thanks for all your contributions. Do we have any progress or plans? Really appreciate for it."
      },
      {
        "user": "zucchini-nlp",
        "body": "We are almost done with the PR, it is in review process and will be shipped soon :)"
      }
    ]
  },
  {
    "issue_number": 38118,
    "title": "Llama4 inference encounter unsupported op in dynamo ?",
    "author": "HuangChiEn",
    "state": "open",
    "created_at": "2025-05-14T04:00:01Z",
    "updated_at": "2025-06-16T00:18:23Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntransformers==4.51.2\ntorch==2.5.0\ntorchvision==0.20.0\n\nMiscellnous pkg version\nPackage                   Version\n------------------------- -----------\naccelerate                1.6.0\naiofiles                  23.2.1\naiohappyeyeballs          2.6.1\naiohttp                   3.11.16\naiosignal                 1.3.2\naltair                    5.5.0\nannotated-types           0.7.0\nantlr4-python3-runtime    4.9.3\nanyio                     4.9.0\nasync-timeout             5.0.1\nattrs                     25.3.0\nbitsandbytes              0.45.4\ncachetools                5.5.2\ncertifi                   2025.1.31\ncharset-normalizer        3.4.1\nclick                     8.1.8\ncontourpy                 1.3.1\ncycler                    0.12.1\ndatasets                  3.5.0\ndecord                    0.6.0\ndeepspeed                 0.14.4\ndescartes                 1.1.0\ndill                      0.3.8\ndistro                    1.9.0\ndocker-pycreds            0.4.0\neinops                    0.6.1\neinops-exts               0.0.4\net_xmlfile                2.0.0\nexceptiongroup            1.2.2\nfastapi                   0.115.12\nffmpy                     0.5.0\nfilelock                  3.18.0\nfire                      0.7.0\nflash-attn                2.7.3\nfonttools                 4.56.0\nfrozenlist                1.5.0\nfsspec                    2024.12.0\ngitdb                     4.0.12\nGitPython                 3.1.44\ngradio_client             0.8.1\nh11                       0.14.0\nhf-xet                    1.0.3\nhjson                     3.1.0\nhttpcore                  0.17.3\nhttpx                     0.24.0\nhuggingface-hub           0.30.2\nidna                      3.10\nimageio                   2.37.0\nimportlib_resources       6.5.2\ninquirerpy                0.3.4\nJinja2                    3.1.6\njiter                     0.9.0\njoblib                    1.4.2\njsonschema                4.23.0\njsonschema-specifications 2024.10.1\nkagglehub                 0.3.11\nkiwisolver                1.4.8\nlatex2mathml              3.77.0\nmarkdown-it-py            3.0.0\nmarkdown2                 2.5.3\nMarkupSafe                3.0.2\nmatplotlib                3.5.3\nmdurl                     0.1.2\nmpmath                    1.3.0\nmultidict                 6.4.3\nmultiprocess              0.70.16\nnarwhals                  1.32.0\nnetworkx                  3.4.2\nninja                     1.11.1.4\nnumpy                     1.26.4\nnuscenes-devkit           1.1.11\nnvidia-cublas-cu12        12.4.5.8\nnvidia-cuda-cupti-cu12    12.4.127\nnvidia-cuda-nvrtc-cu12    12.4.127\nnvidia-cuda-runtime-cu12  12.4.127\nnvidia-cudnn-cu12         9.1.0.70\nnvidia-cufft-cu12         11.2.1.3\nnvidia-curand-cu12        10.3.5.147\nnvidia-cusolver-cu12      11.6.1.9\nnvidia-cusparse-cu12      12.3.1.170\nnvidia-ml-py              12.570.86\nnvidia-nccl-cu12          2.21.5\nnvidia-nvjitlink-cu12     12.4.127\nnvidia-nvtx-cu12          12.4.127\nomegaconf                 2.3.0\nopenai                    1.74.0\nopencv-python             4.11.0.86\nopencv-python-headless    4.11.0.86\nopenpyxl                  3.1.5\norjson                    3.10.16\npackaging                 24.2\npandas                    2.2.3\npeft                      0.10.0\npfzy                      0.3.4\npillow                    11.1.0\npip                       25.0\nplatformdirs              4.3.7\nportalocker               3.1.1\nprompt_toolkit            3.0.51\npropcache                 0.3.1\nprotobuf                  5.29.4\npsutil                    7.0.0\npy-cpuinfo                9.0.0\npyarrow                   19.0.1\npycocotools               2.0.8\npydantic                  2.10.6\npydantic_core             2.27.2\npydub                     0.25.1\nPygments                  2.19.1\npynvml                    12.0.0\npyparsing                 3.2.3\npyquaternion              0.9.9\npython-dateutil           2.9.0.post0\npython-dotenv             1.1.0\npython-multipart          0.0.20\npytz                      2025.2\nPyYAML                    6.0.2\nreferencing               0.36.2\nregex                     2024.11.6\nrequests                  2.32.3\nrich                      13.9.4\nrpds-py                   0.24.0\nruff                      0.11.2\nsafetensors               0.5.3\nscikit-learn              1.2.2\nscipy                     1.15.2\nsemantic-version          2.10.0\nsentencepiece             0.1.99\nsentry-sdk                2.24.1\nsetproctitle              1.3.5\nsetuptools                75.8.0\nShapely                   1.8.5.post1\nshellingham               1.5.4\nshortuuid                 1.0.13\nsix                       1.17.0\nsmmap                     5.0.2\nsniffio                   1.3.1\nstarlette                 0.46.1\nsty                       1.0.6\nsvgwrite                  1.4.3\nsympy                     1.13.1\ntabulate                  0.9.0\ntermcolor                 3.0.1\nthreadpoolctl             3.6.0\ntiktoken                  0.8.0\ntimeout-decorator         0.5.0\ntimm                      0.6.13\ntokenizers                0.21.1\ntomlkit                   0.12.0\ntorch                     2.5.0\ntorchvision               0.20.0\ntqdm                      4.67.1\ntransformers              4.51.2\ntriton                    3.1.0\ntrl                       0.16.1\ntyper                     0.15.2\ntyping_extensions         4.12.2\ntzdata                    2025.2\nurllib3                   2.3.0\nuvicorn                   0.34.0\nvalidators                0.34.0\nwandb                     0.19.8\nwavedrom                  2.0.3.post3\nwcwidth                   0.2.13\nwebsockets                11.0.3\nwheel                     0.45.1\nXlsxWriter                3.2.2\nxxhash                    3.5.0\nyarl                      1.19.0\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```\nfrom transformers import AutoProcessor, Llama4ForConditionalGeneration\nimport torch\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\ntorch._dynamo.config.capture_scalar_outputs = False\n\ndef get_modules():\n    model_id = \"/data/joseph/.cache/hub/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/snapshots/7dab2f5f854fe665b6b2f1eccbd3c48e5f627ad8\"#\"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n    \n    # official setup args\n    processor = AutoProcessor.from_pretrained(model_id)\n    model = Llama4ForConditionalGeneration.from_pretrained(\n        model_id,\n        attn_implementation=\"flex_attention\",\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n    )\n    return model, processor\n\nif __name__ == '__main__':\n    from huggingface_hub import login\n    import os\n    # login to HF to load llama4\n    #login(os.environ.get(\"HF_TOKEN\"))\n    \n    model, processor = get_modules()\n    url1 = \"/data/joseph/llama4_inference/playground/rabbit.jpg\"\n    url2 = \"/data/joseph/llama4_inference/playground/cat_style_layout.png\"\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"path\": url1},\n                {\"type\": \"image\", \"path\": url2},\n                {\"type\": \"text\", \"text\": \"Can you describe how these two images are similar, and how they differ?\"},\n            ]\n        },\n    ]\n\n    inputs = processor.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\",\n    ).to(model.device)\n    \n    # bug happens in here!\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=256,\n    )\n    response = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\n    print(response)\n    print(outputs[0])\n```\nFor the dynamo config, i had tried several combinations, but no one works :\n1. `torch._dynamo.config.suppress_errors = True` and `torch._dynamo.config.capture_scalar_outputs = False`\n![Image](https://github.com/user-attachments/assets/73185b51-b18a-4daf-b45f-cc13bcb2e08d)\n\n2. Both True (this one seems raise the error in very low level...orz)\n![Image](https://github.com/user-attachments/assets/33b02133-dc75-45c9-a5af-ebe435163eed)\n\n### Expected behavior\n\nthe official code snippet can run...",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @HuangChiEn can you link to the code snippet you got this from?"
      },
      {
        "user": "vasqu",
        "body": "I had a PR over here #37406 (kinda forgot about it myself). For now, you would need to update your torch to 2.6.x to avoid the compile errors you see here.\n\nBut tbh, I suggest avoiding flex attention inference due to the mask creation being very expensive atm. You can switch to other implementations by setting `attn_implementation=\"sdpa\"` for example."
      },
      {
        "user": "HuangChiEn",
        "body": "> I had a PR over here [#37406](https://github.com/huggingface/transformers/pull/37406) (kinda forgot about it myself). For now, you would need to update your torch to 2.6.x to avoid the compile errors you see here.\n> \n> But tbh, I suggest avoiding flex attention inference due to the mask creation being very expensive atm. You can switch to other implementations by setting `attn_implementation=\"sdpa\"` for example.\n\nThanks for your quick reply, i can run the inference by **turning off flex attention**. But i attempt to finetune this model actually, so i just wonder that is it possible to apply sdpa (or other fast/memory reduced attention mechanism) in finetuning ?  "
      }
    ]
  },
  {
    "issue_number": 37474,
    "title": "Trainer.training_step incorrectly normalizes mean token loss when n_gpu > 1",
    "author": "wiwu2390",
    "state": "open",
    "created_at": "2025-04-13T18:12:18Z",
    "updated_at": "2025-06-15T16:43:02Z",
    "labels": [
      "Good First Issue",
      "bug"
    ],
    "body": "### System Info\n\n```\n- `transformers` version: 4.46.0\n- Platform: Linux-5.15.0-136-generic-x86_64-with-glibc2.35\n- Python version: 3.10.12\n- Huggingface_hub version: 0.29.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.4.0\n- Accelerate config:    not found\n- PyTorch version (GPU?): 2.4.1+cu121 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: yes\n- Using GPU in script?: yes\n- GPU type: NVIDIA RTX A5000\n```\n\n### Who can help?\n\n@zach-huggingface @SunMarc @ArthurZucker\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nFull example setup:\n\n```\nconfig = AutoConfig.from_pretrained('EleutherAI/pythia-14m')\nmodel = GPTNeoXForCausalLM(config=config).to('cuda')\ntokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-14m')\ntokenizer.pad_token = tokenizer.eos_token\ntrain_data = load_dataset(\"wiwu2390/minipile-100k\", split=\"train\")\n\ndef tokenize_function(sample):\n    return tokenizer(sample[\"text\"], truncation=True, max_length=512)\n\ntokenized_dataset = train_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=False\n)\n\ntraining_args = TrainingArguments(\n    output_dir=\"../data/pythia-14m-minipile-100k\",\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy=\"no\",\n    logging_steps=1,\n    save_steps=100,\n    learning_rate=1e-3,\n    weight_decay=0.01,\n    warmup_steps=100,\n    fp16=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n```\n\nWith 4 GPUs, the training loss at step 1 is ~2.7. However, the expected value is ~10.8. Indeed, this is what we get if we set CUDA_VISIBLE_DEVICES=0.\n\n### Expected behavior\n\nSince the model is being trained from initialization, the training loss at the first few steps should be around ~log(vocab_size)=10.8. However, when using 4 GPUs, the reported loss is 1/4 of that (2.7).\n\nThe reason that this is happening is that the DataParallel-wrapped model gets `num_items_in_batch` as an input kwarg in `Trainer.compute_loss`; this is equal to the number of tokens in the batch (combined across all devices). Each device gets a 1/4-size per-device batch and returns the sum of token losses divided by `num_items_in_batch` (see `transformers.loss.loss_utils.fixed_cross_entropy`). The correct way to aggregate these per-device losses is then to *sum* them. However, `Trainer.training_step` takes the mean:\nhttps://github.com/huggingface/transformers/blob/953196a43dae6a3c474165fba7d215fcbc7b7730/src/transformers/trainer.py#L3759\n\nA quick and dirty fix would be:\n```\nif self.args.n_gpu > 1:\n    loss = loss.mean() if num_items_in_batch is None else loss.sum()\n```\nI'm not sure if this is compatible with other workflows though.",
    "comments": [
      {
        "user": "SunMarc",
        "body": "@wiwu2390 thanks for the report. \n\n> The reason that this is happening is that the DataParallel-wrapped model gets num_items_in_batch as an input kwarg in Trainer.compute_loss; this is equal to the number of tokens in the batch (combined across all devices). Each device gets a 1/4-size per-device batch and returns the sum of token losses divided by num_items_in_batch (see transformers.loss.loss_utils.fixed_cross_entropy). \n\n`num_items_in_batch` shouldn't be equal to the number of tokens in the batch combined across all devices but only on the respective device. We only combine if you set `average_tokens_across_devices=True`. However, the default for this arg is False.\n\nDid you test both runs with the same `per_device_train_batch_size` ? You need to divide it by 4 when running with 4 gpus to be comparable. Otherwise, you are actually using a bigger global batch size. However, I don't think this is the real issue."
      },
      {
        "user": "wiwu2390",
        "body": "@SunMarc Thanks for taking a look at this. (And apologies for the delayed reply -- somehow I didn't get a notification about your response.)\n\nI am dividing the `per_device_train_batch_size` by 4 for 4 gpus, but as you said I don't think this is the root cause.\nAlso I have the default setting `average_tokens_across_devices=False`.\n\nWhen I run my above example code, the behavior I'm observing is:\n- `Trainer.compute_loss` is called with `num_items_in_batch=(inputs['labels'] != -100).sum()`\n- Inside `compute_loss`, it runs `outputs = model(**inputs); loss = outputs[\"loss\"]`\n- In my example, `model` is a `DataParallel` object wrapping a `GPTNeoXForCausalLM`. I don't construct a `DataParallel` explicitly, so I'm guessing that `Trainer` automatically does this somewhere when it detects multiple devices?\n- `DataParallel` splits  `input_ids` into 4 chunks for the 4 gpus, **but it does not modify `num_items_in_batch`**\n- Thus `GPTNeoXForCausalLM.forward` calls `self.loss_function` using `input_ids` for just 1 gpu but `num_items_in_batch` equal to the sum across all gpus.\n\nAre you able to replicate this behavior on your end? I've provided the full example code (except imports) above.\n\nAlso, maybe I'm incorrectly using `Trainer` for multi-device training? Is there a different config that I should be using?"
      },
      {
        "user": "SunMarc",
        "body": "I'll check ! Let me know if you find something on your side "
      }
    ]
  },
  {
    "issue_number": 38832,
    "title": "ImportError: cannot import name 'BitsAndBytesConfig' from 'bitsandbytes'",
    "author": "Tanuj-rai",
    "state": "closed",
    "created_at": "2025-06-15T12:26:40Z",
    "updated_at": "2025-06-15T13:51:31Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n```\nTorch version: 2.7.1+cu126\nTransformers version: 4.41.2\nDatasets version: 2.20.0\nPEFT version: 0.11.1\nTRL version: 0.8.6\nBitsandbytes version: 0.46.0\nAccelerate version: 0.31.0\n```\n\n```!pip install -q transformers datasets accelerate peft bitsandbytes einops```\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom datasets import load_dataset,\nfrom peft import LoraConfig, get_peft_model,\nfrom trl import SFTTrainer,\nfrom bitsandbytes import BitsAndBytesConfig,\nimport os\n```\n\nOn running the code I am getting following error.\n\n```\nImportError                               Traceback (most recent call last)\n[<ipython-input-1-3270306226>](https://localhost:8080/#) in <cell line: 0>()\n      4 from peft import LoraConfig, get_peft_model\n      5 from trl import SFTTrainer\n----> 6 from bitsandbytes import BitsAndBytesConfig\n      7 import os\n\nImportError: cannot import name 'BitsAndBytesConfig' from 'bitsandbytes' (/usr/local/lib/python3.11/dist-packages/bitsandbytes/__init__.py)\n\n---------------------------------------------------------------------------\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n```\n\n### Who can help?\n\n@Rocketknight1 @ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nRun the code provided in the issue.\n\n### Expected behavior\n\nBitsAndBytesConfig will import without any error.",
    "comments": [
      {
        "user": "bvantuan",
        "body": "Hi @Tanuj-rai ! `BitsAndBytesConfig` is provided by the transformers library. So the correct usage should be:\n```\nfrom transformers import BitsAndBytesConfig\n```"
      },
      {
        "user": "Tanuj-rai",
        "body": "> Hi [@Tanuj-rai](https://github.com/Tanuj-rai) ! `BitsAndBytesConfig` is provided by the transformers library. So the correct usage should be:\n> \n> ```\n> from transformers import BitsAndBytesConfig\n> ```\n\nThank you @bvantuan. "
      }
    ]
  },
  {
    "issue_number": 28282,
    "title": "ImportError: AutoModel requires the PyTorch library but it was not found in your environment",
    "author": "Marwen94",
    "state": "closed",
    "created_at": "2023-12-29T17:24:50Z",
    "updated_at": "2025-06-15T08:43:48Z",
    "labels": [],
    "body": "### System Info\n\nI'm trying to load a AutoModel pre-trained model. However, I receiving the following error : \r\n\r\n```\r\nImportError: \r\nAutoModel requires the PyTorch library but it was not found in your environment.\r\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\r\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\r\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModel\".\r\nIf you want to use TensorFlow, please use TF classes instead!\r\n```\r\n\r\nI do have Pytorch installed : \r\n\r\n```\r\ntorch==2.0.0\r\ntorchvision==0.16.2\r\n```\r\n\r\ntransformers-cli env : \r\n\r\n```\r\n- `transformers` version: 4.36.2\r\n- Platform: macOS-14.2.1-x86_64-i386-64bit\r\n- Python version: 3.11.7\r\n- Huggingface_hub version: 0.20.1\r\n- Safetensors version: 0.4.1\r\n- Accelerate version: not installed\r\n- Accelerate config: not found\r\n- PyTorch version (GPU?): 2.0.0 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n```\r\n\r\n\r\nThanks a lot!\n\n### Who can help?\n\n@gante and @Rocketknight1\n\n### Information\n\n- [X] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1 . Create an activate a virtual env using this poetry file : \r\n\r\n\r\n```\r\n[tool.poetry]\r\nname = \"test\"\r\nversion = \"1.0.0\"\r\nauthors = [\"Marwen Taleb\"]\r\nreadme = \"README.md\"\r\n\r\n[tool.poetry.dependencies]\r\npython = \">=3.8,<3.12\"\r\ntransformers=\"4.36.2\"\r\nscikit-learn = \"^1.3.2\"\r\npandas = \"2.0.0\"\r\ntorch = \"2.0.0\"\r\n\r\n\r\n[build-system]\r\nrequires = [\"poetry-core\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n\r\n```\r\n\r\n2 . Run this python script : \r\n\r\n```\r\nfrom transformers import AutoModel\r\nmodel = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\r\n```\r\n\r\n3. You should received the above described error.\n\n### Expected behavior\n\nI expect to be able to instantiate an AutoModel from a pretrained model when having Pytorch installed.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @marwen94, I'm struggling to reproduce this here. I'm not familiar with the Poetry dependency manager, but I tried reproducing those package versions in a Python 3.9 env with `pip` and the model loaded fine - the issue seems to be very specific to the environment you're using.\r\n\r\nCan you figure out a list of package versions that I can install with `conda` or `pip` that reproduces the issue? Once I can reproduce it here I can work on diagnosing and fixing it!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "Sanskar711",
        "body": "i am facing the same issue\n"
      }
    ]
  },
  {
    "issue_number": 27532,
    "title": "Model implementation with Transformers and Hugging face hub.",
    "author": "sudhakar071",
    "state": "closed",
    "created_at": "2023-11-16T07:08:55Z",
    "updated_at": "2025-06-15T08:12:11Z",
    "labels": [],
    "body": "### Model description\n\nI created a model and for ease in training and fine-tuning, I need to integrate the model with Hugging face hub. \r\nModel's tokenizer is a SentencePiece tokenizer. \r\nHere is the model repository.\r\nhttps://github.com/sudhakar-71/scratch-ai\n\n### Open source status\n\n- [X] The model implementation is available\n- [ ] The model weights are available\n\n### Provide useful links for the implementation\n\n_No response_",
    "comments": [
      {
        "user": "amyeroberts",
        "body": "Hi @sudhakar071, \r\n\r\nPlease refer to our documentation to see how to push models to the hub: https://huggingface.co/docs/transformers/model_sharing\r\n\r\nAnd make your model accessible using the `AutoModel` API: https://huggingface.co/docs/transformers/custom_models"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 33106,
    "title": "how to fine tune TrOCR on specifique langage guide.",
    "author": "MohamedLahmeri01",
    "state": "closed",
    "created_at": "2024-08-24T14:33:02Z",
    "updated_at": "2025-06-15T08:07:10Z",
    "labels": [],
    "body": "### Model description\n\nhello , just passed through issues and other , but none of them talked on how to fine-tune TrOCR on specifique langage , like how to pick encoder and decoder , model .. etc , \r\ncan you @NielsRogge , write a simple instructions/guide on this topic ?\r\n\n\n### Open source status\n\n- [ ] The model implementation is available\n- [ ] The model weights are available\n\n### Provide useful links for the implementation\n\n_No response_",
    "comments": [
      {
        "user": "NielsRogge",
        "body": "Hi,\r\n\r\nThere's an extensive thread at https://github.com/huggingface/transformers/issues/19329 as well as https://github.com/microsoft/unilm/issues/627"
      },
      {
        "user": "MohamedLahmeri01",
        "body": "@NielsRogge  could you check my essey , based on your notebook , [https://www.kaggle.com/code/cherryblade29/forarabic-and-english](url)"
      },
      {
        "user": "ffazzm",
        "body": "hi, i got nice CER (0.1) while fine tuning small model on Indonesian language, i found that it's got nicer if im not change my processor.tokenizer to any Indonesian tokenizer. \r\nAnyway, i have my own Indonesian cropped image text dataset from form documents, it is consist name person, birth-place date, etc. Do you think it's a good idea if i randomly combine those cropped image text into longer (horizontally) image text for my training dataset?"
      }
    ]
  },
  {
    "issue_number": 37263,
    "title": "Loading HQQ quantized models is broken since #35926",
    "author": "mobicham",
    "state": "closed",
    "created_at": "2025-04-03T19:54:55Z",
    "updated_at": "2025-06-15T08:03:19Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n- `transformers` version: 4.51.0.dev0\n- Platform: Linux-5.4.0-208-generic-x86_64-with-glibc2.35\n- Python version: 3.11.10\n- Huggingface_hub version: 0.30.1\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n\n\nLoading HQQ models is broken since https://github.com/huggingface/transformers/pull/35926\nNot sure what changed, probably something in `modeling_utils`\n@SunMarc  @ArthurZucker \n\n### Reproduction\n```Python\nimport torch\ncompute_dtype = torch.bfloat16 \nmodel_id      = 'mobiuslabsgmbh/gemma-3-12b-it_4bitgs64_bfp16_hqq_hf'\n\n#Load model\nfrom transformers import Gemma3ForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=compute_dtype,\n    attn_implementation=\"sdpa\",\n    device_map=\"cuda\",\n)\n```\n\n```\nAttributeError: `language_model.model.layers.46.self_attn.o_proj.quant_scale` is neither a parameter nor a buffer.\n```\n\n### Expected behavior\nHQQ quantized models were loading fine before #35926 ",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @MekkCyber @SunMarc "
      },
      {
        "user": "Cyrilvallez",
        "body": "The fix is here https://github.com/huggingface/transformers/pull/37347, it was indeed introduced when adding Deepseek!\nThanks a lot for the report! 🤗"
      },
      {
        "user": "mobicham",
        "body": "Thank you @Cyrilvallez ! Seems to work! \nI'm not sure if it's related, but when I use `Gemma3ForCausalLM` instead of `Gemma3ForConditionalGeneration`, it just hangs indefinitely:\n```Python\nimport torch\nfrom transformers import Gemma3ForCausalLM, AutoProcessor\nmodel = Gemma3ForCausalLM.from_pretrained(\n    'mobiuslabsgmbh/gemma-3-12b-it_4bitgs64_bfp16_hqq_hf',\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"sdpa\",\n    device_map=\"cuda\",\n)\n```"
      }
    ]
  },
  {
    "issue_number": 37989,
    "title": "Bug Report: Unexpected Keyword Argument 'padding_side' in PreTrainedTokenizerFast",
    "author": "yunqianluo",
    "state": "closed",
    "created_at": "2025-05-07T07:31:05Z",
    "updated_at": "2025-06-15T08:02:45Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nabsl-py==2.1.0\naccelerate==1.6.0\naiohappyeyeballs==2.3.5\naiohttp==3.10.2\naiosignal==1.3.1\naniso8601==9.0.1\nannotated-types==0.7.0\nanyio==4.4.0\nasync-timeout==4.0.3\nattrs==24.2.0\nblinker==1.8.2\ncertifi==2024.7.4\ncharset-normalizer==3.3.2\nchinesebert==0.2.1\nclick==8.1.7\nconfluent-kafka==2.5.0\ndatasets==1.18.3\ndill==0.3.8\nelastic-transport==8.15.0\nelasticsearch==8.14.0\nexceptiongroup==1.2.2\nfastapi==0.112.0\nfastcore==1.3.29\nfilelock==3.15.4\nFlask==3.0.3\nFlask-Cors==4.0.1\nFlask-RESTful==0.3.10\nfrozenlist==1.4.1\nfsspec==2024.6.1\ngevent==24.10.3\ngreenlet==3.1.1\ngrpcio==1.65.4\ngunicorn==23.0.0\nh11==0.14.0\nhuggingface-hub==0.24.5\nidna==3.7\nimportlib_metadata==8.2.0\niocextract==1.16.1\nitsdangerous==2.2.0\nJinja2==3.1.4\njoblib==1.4.2\nkazoo==2.5.0\nMarkdown==3.6\nmarkdown-it-py==3.0.0\nMarkupSafe==2.1.5\nmdurl==0.1.2\nmpmath==1.3.0\nmultidict==6.0.5\nmultiprocess==0.70.16\nnetworkx==3.2.1\nnumpy==1.26.4\nnvidia-cublas-cu12==12.1.3.1\nnvidia-cuda-cupti-cu12==12.1.105\nnvidia-cuda-nvrtc-cu12==12.1.105\nnvidia-cuda-runtime-cu12==12.1.105\nnvidia-cudnn-cu12==9.1.0.70\nnvidia-cufft-cu12==11.0.2.54\nnvidia-curand-cu12==10.3.2.106\nnvidia-cusolver-cu12==11.4.5.107\nnvidia-cusparse-cu12==12.1.0.106\nnvidia-nccl-cu12==2.20.5\nnvidia-nvjitlink-cu12==12.6.20\nnvidia-nvtx-cu12==12.1.105\npackaging==24.1\npandas==2.2.2\nprotobuf==4.25.4\npsutil==7.0.0\npyarrow==17.0.0\npydantic==2.8.2\npydantic_core==2.20.1\nPygments==2.18.0\npykafka==2.8.0\nPyMySQL==1.1.1\npypinyin==0.38.1\npython-dateutil==2.9.0.post0\npytz==2024.1\nPyYAML==6.0.2\nregex==2024.7.24\nrequests==2.32.3\nrequests-file==2.1.0\nrich==13.7.1\nsacremoses==0.1.1\nsafetensors==0.5.3\nshellingham==1.5.4\nsix==1.16.0\nsniffio==1.3.1\nstarlette==0.37.2\nsympy==1.13.1\ntabulate==0.9.0\ntensorboard==2.17.0\ntensorboard-data-server==0.7.2\ntldextract==5.1.2\ntokenizers==0.19.1\ntorch==2.4.0\ntqdm==4.66.5\ntransformers==4.42.0\ntriton==3.0.0\ntyper==0.12.3\ntyping_extensions==4.12.2\ntzdata==2024.1\nurllib3==2.2.2\nuvicorn==0.30.5\nWerkzeug==3.0.3\nxxhash==3.4.1\nyarl==1.9.4\nzipp==3.19.2\nzope.event==5.0\nzope.interface==7.1.1\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n---\n\n**Bug Report: Unexpected Keyword Argument 'padding_side' in PreTrainedTokenizerFast**\n\n**Description:**\n\nI encountered a `TypeError` when using the `AutoTokenizer` with `tokenizer_name` after upgrading to Transformers version 4.42.0 and tokenizers version 0.19.1. The error message is as follows:\n\n```\nTypeError: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'padding_side'\n```\n\n**Details:**\n\n- **Code Snippet:**\n\n  ```python\n  tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n  def tokenize_and_align_train_labels(examples):\n      tokenized_inputs = tokenizer(\n          examples[text_column_name],\n          max_length=args.cutoff_len,\n          padding=False,\n          truncation=True,\n          return_token_type_ids=False,\n      )\n  ```\n\n- **Observations:**\n  - Before upgrading to Transformers version 4.42.0, this error did not occur.\n  - The issue arises with `tokenizers` version 0.19.1 and later.\n  - Setting `use_fast=False` when initializing the tokenizer resolves the error, indicating the issue is specific to the fast tokenizer implementation.\n\n**Request:**\n\nPlease investigate this issue, as it seems to be a regression related to the handling of the `padding_side` argument in the fast tokenizer. Any guidance or fix would be appreciated.\n\n---\n\n\n### Expected behavior\n\nThe AutoTokenizer should correctly handle the padding_side argument without raising a TypeError when using the fast tokenizer implementation. Specifically, when calling the tokenizer with parameters such as padding, truncation, and return_token_type_ids, it should process the input text as expected, regardless of the version of the tokenizers library being used. This behavior should be consistent across versions, ensuring backward compatibility and seamless upgrades.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @yunqianluo, this might be specific to the tokenizer class you're using. Can you give us some code we can run that will show the issue?\n\ncc @itazap"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37999,
    "title": "RuntimeError when converting and saving Flax ViT model to PyTorch",
    "author": "nobodyPerfecZ",
    "state": "closed",
    "created_at": "2025-05-07T12:32:12Z",
    "updated_at": "2025-06-15T08:02:43Z",
    "labels": [
      "Flax",
      "bug"
    ],
    "body": "### System Info\n\nEnv:\n- transformers version: 4.51.3\n- Platform: Linux-6.8.0-59-generic-x86_64-with-glibc2.39\n- Python version: 3.10.16\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: not found\n- Accelerate config: not found\n- DeepSpeed version: not found\n- PyTorch version (GPU?): 2.7.0+cu128 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): 0.10.5 (True)\n- Jax version: 0.5.1\n- JaxLib version:  0.5.1\n- Using distributed or parallel set-up in script?: No\n- Using GPU in script?: Yes\n- GPU type: NVIDIA RTX 3090\n\n### Who can help?\n\n@gante @Rocketknight1\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nExample script to reproduce the bug:\n```python\nfrom transformers import FlaxViTForImageClassification, ViTForImageClassification\n\n# Simulate fine-tuning by loading the base model and saving it\nflax_model = FlaxViTForImageClassification.from_pretrained(\n    pretrained_model_name_or_path=\"google/vit-base-patch16-224\",\n    num_labels=5,\n    id2label={\n        0: \"bicycle\",\n        1: \"bus\",\n        2: \"car\",\n        3: \"crosswalk\",\n        4: \"hydrant\",\n    },\n    label2id={\n        \"bicycle\": 0,\n        \"bus\": 1,\n        \"car\": 2,\n        \"crosswalk\": 3,\n        \"hydrant\": 4,\n    },\n    ignore_mismatched_sizes=True,\n)\nflax_model.save_pretrained(\"./test-vit-finetuned-patch16-224-recaptchav2\")\n\n# Load the fine-tuned model and convert it to PyTorch\npt_model = ViTForImageClassification.from_pretrained(\n    pretrained_model_name_or_path=\"./test-vit-finetuned-patch16-224-recaptchav2\",\n    from_flax=True,\n)\npt_model.save_pretrained(\"./test-vit-finetuned-patch16-224-recaptchav2\") # RuntimeError\n```\nThe complete traceback is shown below:\n```\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[1], line 30\n     25 # Load the fine-tuned model and convert it to PyTorch\n     26 pt_model = ViTForImageClassification.from_pretrained(\n     27     pretrained_model_name_or_path=\"./test-vit-finetuned-patch16-224-recaptchav2\",\n     28     from_flax=True,\n     29 )\n---> 30 pt_model.save_pretrained(\"./test-vit-finetuned-patch16-224-recaptchav2\") # RuntimeError\n\nFile ~/miniconda3/envs/recaptchav2-solver/lib/python3.10/site-packages/transformers/modeling_utils.py:3486, in PreTrainedModel.save_pretrained(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\n   3483         error_names.append(set(shared_names))\n   3485     if len(error_names) > 0:\n-> 3486         raise RuntimeError(\n   3487             f\"The weights trying to be saved contained shared tensors {error_names} that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.\",\n   3488         )\n   3490 # Shard the model if it is too big.\n   3491 if not _hf_peft_config_loaded:\n\nRuntimeError: The weights trying to be saved contained shared tensors [{'vit.encoder.layer.6.output.dense.bias', 'vit.encoder.layer.10.layernorm_after.weight', 'vit.encoder.layer.7.attention.attention.query.bias', 'vit.encoder.layer.7.attention.attention.key.bias', 'vit.encoder.layer.7.layernorm_after.bias', 'vit.encoder.layer.0.output.dense.bias', 'vit.encoder.layer.8.attention.attention.query.bias', 'vit.encoder.layer.1.attention.output.dense.bias', 'vit.encoder.layer.3.output.dense.bias', 'vit.encoder.layer.10.layernorm_before.weight', 'vit.encoder.layer.3.layernorm_after.bias', 'vit.encoder.layer.0.layernorm_after.bias', 'vit.encoder.layer.2.attention.attention.key.bias', 'vit.encoder.layer.0.layernorm_before.weight', 'vit.encoder.layer.3.attention.attention.value.bias', 'vit.encoder.layer.11.attention.attention.key.bias', 'vit.encoder.layer.5.output.dense.bias', 'vit.layernorm.weight', 'vit.encoder.layer.8.output.dense.bias', 'vit.encoder.layer.7.attention.output.dense.bias', 'vit.encoder.layer.6.attention.attention.value.bias', 'vit.encoder.layer.5.layernorm_before.bias', 'vit.encoder.layer.6.layernorm_before.bias', 'vit.encoder.layer.4.layernorm_after.bias', 'vit.encoder.layer.11.layernorm_after.weight', 'vit.encoder.layer.4.attention.attention.query.bias', 'vit.encoder.layer.9.attention.attention.query.bias', 'vit.encoder.layer.5.attention.output.dense.bias', 'vit.encoder.layer.3.attention.attention.key.bias', 'vit.encoder.layer.8.attention.attention.key.bias', 'vit.encoder.layer.8.attention.output.dense.bias', 'vit.encoder.layer.11.layernorm_after.bias', 'vit.encoder.layer.11.layernorm_before.weight', 'vit.encoder.layer.7.output.dense.bias', 'vit.encoder.layer.3.layernorm_before.weight', 'vit.encoder.layer.4.layernorm_before.bias', 'vit.encoder.layer.1.attention.attention.query.bias', 'vit.encoder.layer.2.layernorm_before.weight', 'vit.encoder.layer.0.attention.output.dense.bias', 'vit.encoder.layer.11.attention.output.dense.bias', 'vit.encoder.layer.0.layernorm_before.bias', 'vit.encoder.layer.10.attention.output.dense.bias', 'vit.encoder.layer.11.attention.attention.query.bias', 'vit.encoder.layer.8.attention.attention.value.bias', 'vit.encoder.layer.6.attention.output.dense.bias', 'vit.layernorm.bias', 'vit.encoder.layer.1.layernorm_after.weight', 'vit.encoder.layer.10.attention.attention.query.bias', 'vit.encoder.layer.11.layernorm_before.bias', 'vit.encoder.layer.1.layernorm_before.bias', 'vit.encoder.layer.4.output.dense.bias', 'vit.embeddings.cls_token', 'vit.encoder.layer.6.attention.attention.query.bias', 'vit.encoder.layer.3.attention.attention.query.bias', 'vit.encoder.layer.9.output.dense.bias', 'vit.encoder.layer.9.attention.attention.key.bias', 'vit.embeddings.patch_embeddings.projection.bias', 'vit.encoder.layer.5.layernorm_after.weight', 'vit.encoder.layer.11.output.dense.bias', 'vit.encoder.layer.8.layernorm_after.weight', 'vit.encoder.layer.5.attention.attention.value.bias', 'vit.encoder.layer.2.attention.output.dense.bias', 'vit.encoder.layer.6.layernorm_after.bias', 'vit.encoder.layer.5.layernorm_after.bias', 'vit.encoder.layer.1.layernorm_before.weight', 'vit.encoder.layer.2.layernorm_before.bias', 'vit.encoder.layer.10.attention.attention.value.bias', 'vit.encoder.layer.9.attention.output.dense.bias', 'vit.encoder.layer.9.attention.attention.value.bias', 'vit.encoder.layer.9.layernorm_before.weight', 'vit.encoder.layer.2.output.dense.bias', 'vit.encoder.layer.0.attention.attention.query.bias', 'vit.encoder.layer.9.layernorm_after.weight', 'vit.encoder.layer.10.output.dense.bias', 'vit.encoder.layer.7.layernorm_after.weight', 'vit.encoder.layer.8.layernorm_after.bias', 'vit.encoder.layer.5.attention.attention.key.bias', 'vit.encoder.layer.7.attention.attention.value.bias', 'vit.encoder.layer.4.layernorm_after.weight', 'vit.encoder.layer.3.attention.output.dense.bias', 'vit.encoder.layer.6.layernorm_after.weight', 'vit.encoder.layer.5.attention.attention.query.bias', 'vit.encoder.layer.5.layernorm_before.weight', 'vit.encoder.layer.0.layernorm_after.weight', 'vit.encoder.layer.10.layernorm_after.bias', 'vit.encoder.layer.3.layernorm_before.bias', 'vit.encoder.layer.4.layernorm_before.weight', 'vit.encoder.layer.11.attention.attention.value.bias', 'vit.encoder.layer.0.attention.attention.key.bias', 'vit.encoder.layer.4.attention.output.dense.bias', 'vit.encoder.layer.10.attention.attention.key.bias', 'vit.encoder.layer.4.attention.attention.key.bias', 'vit.encoder.layer.1.layernorm_after.bias', 'vit.encoder.layer.7.layernorm_before.bias', 'vit.encoder.layer.4.attention.attention.value.bias', 'vit.encoder.layer.8.layernorm_before.weight', 'vit.encoder.layer.1.attention.attention.key.bias', 'vit.encoder.layer.2.attention.attention.value.bias', 'vit.encoder.layer.0.attention.attention.value.bias', 'vit.encoder.layer.10.layernorm_before.bias', 'vit.encoder.layer.9.layernorm_after.bias', 'vit.encoder.layer.2.layernorm_after.bias', 'vit.encoder.layer.1.attention.attention.value.bias', 'vit.encoder.layer.6.layernorm_before.weight', 'vit.encoder.layer.9.layernorm_before.bias', 'vit.encoder.layer.1.output.dense.bias', 'vit.encoder.layer.3.layernorm_after.weight', 'vit.encoder.layer.8.layernorm_before.bias', 'vit.encoder.layer.2.attention.attention.query.bias', 'vit.encoder.layer.2.layernorm_after.weight', 'vit.encoder.layer.7.layernorm_before.weight', 'vit.encoder.layer.6.attention.attention.key.bias'}, {'vit.encoder.layer.5.attention.attention.value.weight', 'vit.encoder.layer.6.attention.attention.query.weight', 'vit.encoder.layer.10.attention.attention.key.weight', 'vit.encoder.layer.1.attention.attention.value.weight', 'vit.encoder.layer.4.attention.attention.query.weight', 'vit.encoder.layer.11.attention.attention.value.weight', 'vit.encoder.layer.11.attention.output.dense.weight', 'vit.encoder.layer.8.attention.attention.query.weight', 'vit.encoder.layer.4.attention.attention.key.weight', 'vit.encoder.layer.10.attention.attention.query.weight', 'vit.encoder.layer.3.attention.output.dense.weight', 'vit.encoder.layer.7.attention.attention.value.weight', 'vit.encoder.layer.6.attention.attention.key.weight', 'vit.encoder.layer.3.attention.attention.key.weight', 'vit.encoder.layer.0.attention.attention.key.weight', 'vit.encoder.layer.5.attention.attention.key.weight', 'vit.encoder.layer.9.attention.output.dense.weight', 'vit.encoder.layer.4.attention.attention.value.weight', 'vit.encoder.layer.11.attention.attention.key.weight', 'vit.encoder.layer.5.attention.output.dense.weight', 'vit.encoder.layer.7.attention.attention.query.weight', 'vit.encoder.layer.10.attention.output.dense.weight', 'vit.encoder.layer.3.attention.attention.query.weight', 'vit.encoder.layer.8.attention.attention.key.weight', 'vit.encoder.layer.4.attention.output.dense.weight', 'vit.encoder.layer.8.attention.output.dense.weight', 'vit.encoder.layer.5.attention.attention.query.weight', 'vit.encoder.layer.0.attention.attention.value.weight', 'vit.encoder.layer.6.attention.output.dense.weight', 'vit.encoder.layer.9.attention.attention.query.weight', 'vit.encoder.layer.0.attention.output.dense.weight', 'vit.encoder.layer.9.attention.attention.key.weight', 'vit.encoder.layer.7.attention.attention.key.weight', 'vit.encoder.layer.2.attention.output.dense.weight', 'vit.encoder.layer.1.attention.output.dense.weight', 'vit.encoder.layer.6.attention.attention.value.weight', 'vit.encoder.layer.2.attention.attention.key.weight', 'vit.encoder.layer.2.attention.attention.value.weight', 'vit.encoder.layer.7.attention.output.dense.weight', 'vit.encoder.layer.3.attention.attention.value.weight', 'vit.encoder.layer.1.attention.attention.key.weight', 'vit.encoder.layer.8.attention.attention.value.weight', 'vit.encoder.layer.9.attention.attention.value.weight', 'vit.encoder.layer.10.attention.attention.value.weight', 'vit.encoder.layer.2.attention.attention.query.weight', 'vit.encoder.layer.11.attention.attention.query.weight', 'vit.encoder.layer.1.attention.attention.query.weight', 'vit.embeddings.patch_embeddings.projection.weight', 'vit.encoder.layer.0.attention.attention.query.weight'}, {'vit.encoder.layer.11.output.dense.weight', 'vit.encoder.layer.7.intermediate.dense.weight', 'vit.encoder.layer.7.output.dense.weight', 'vit.encoder.layer.11.intermediate.dense.weight', 'vit.encoder.layer.0.output.dense.weight', 'vit.encoder.layer.4.output.dense.weight', 'vit.encoder.layer.3.intermediate.dense.weight', 'vit.encoder.layer.0.intermediate.dense.weight', 'vit.encoder.layer.2.intermediate.dense.weight', 'vit.encoder.layer.10.intermediate.dense.weight', 'vit.encoder.layer.9.output.dense.weight', 'vit.encoder.layer.8.output.dense.weight', 'vit.encoder.layer.1.output.dense.weight', 'vit.encoder.layer.4.intermediate.dense.weight', 'vit.encoder.layer.9.intermediate.dense.weight', 'vit.encoder.layer.2.output.dense.weight', 'vit.encoder.layer.1.intermediate.dense.weight', 'vit.encoder.layer.5.output.dense.weight', 'vit.encoder.layer.6.output.dense.weight', 'vit.encoder.layer.5.intermediate.dense.weight', 'vit.encoder.layer.3.output.dense.weight', 'vit.encoder.layer.10.output.dense.weight', 'vit.encoder.layer.8.intermediate.dense.weight', 'vit.encoder.layer.6.intermediate.dense.weight'}, {'vit.encoder.layer.10.intermediate.dense.bias', 'vit.encoder.layer.3.intermediate.dense.bias', 'vit.encoder.layer.11.intermediate.dense.bias', 'vit.encoder.layer.0.intermediate.dense.bias', 'vit.encoder.layer.2.intermediate.dense.bias', 'vit.encoder.layer.8.intermediate.dense.bias', 'vit.encoder.layer.1.intermediate.dense.bias', 'vit.encoder.layer.6.intermediate.dense.bias', 'vit.encoder.layer.4.intermediate.dense.bias', 'vit.encoder.layer.9.intermediate.dense.bias', 'vit.encoder.layer.7.intermediate.dense.bias', 'vit.encoder.layer.5.intermediate.dense.bias'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.\n```\n\n### Expected behavior\n\nThe converted PyTorch ViT model should be saved/loaded successfully without requiring manual intervention given a Flax ViT model.",
    "comments": [
      {
        "user": "xmarva",
        "body": "@nobodyPerfecZ  Hey, just wondering, does using the` safe_serialization=False` parameter in `pt_model.save_pretrained` resolve the issue in your configuration? I reproduce the error without this parameter, but with it, the model successfully converts and saves."
      },
      {
        "user": "nobodyPerfecZ",
        "body": "I tried using the ``safe_serialization=False`` parameter in ``pt_model.safe_pretrained``. The model is saved. However when i reload the model without setting ``from_flax=True``, I encounter another error.\n\nCode:\n```python\nfrom transformers import FlaxViTForImageClassification, ViTForImageClassification\n\n# Simulate fine-tuning by loading the base model and saving it\nflax_model = FlaxViTForImageClassification.from_pretrained(\n    pretrained_model_name_or_path=\"google/vit-base-patch16-224\",\n    num_labels=5,\n    id2label={\n        0: \"bicycle\",\n        1: \"bus\",\n        2: \"car\",\n        3: \"crosswalk\",\n        4: \"hydrant\",\n    },\n    label2id={\n        \"bicycle\": 0,\n        \"bus\": 1,\n        \"car\": 2,\n        \"crosswalk\": 3,\n        \"hydrant\": 4,\n    },\n    ignore_mismatched_sizes=True,\n)\nflax_model.save_pretrained(\"./test-vit-finetuned-patch16-224-recaptchav2\")\n\n# Load the fine-tuned model and convert it to PyTorch\npt_model = ViTForImageClassification.from_pretrained(\n    pretrained_model_name_or_path=\"./test-vit-finetuned-patch16-224-recaptchav2\",\n    from_flax=True,\n)\npt_model.save_pretrained(\"./test-vit-finetuned-patch16-224-recaptchav2\", safe_serialization=False)\n\n# Loading the model again\npt_model = ViTForImageClassification.from_pretrained(\n    pretrained_model_name_or_path=\"./test-vit-finetuned-patch16-224-recaptchav2\",\n) # NotImplementedError\n```\n\nTraceback:\n```\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\nCell In[1], line 33\n     30 pt_model.save_pretrained(\"./test-vit-finetuned-patch16-224-recaptchav2\", safe_serialization=False)\n     32 # Loading the model\n---> 33 pt_model = ViTForImageClassification.from_pretrained(\n     34     pretrained_model_name_or_path=\"./test-vit-finetuned-patch16-224-recaptchav2\",\n     35 )\n\nFile ~/miniconda3/envs/recaptchav2-solver/lib/python3.10/site-packages/transformers/modeling_utils.py:279, in restore_default_torch_dtype.<locals>._wrapper(*args, **kwargs)\n    277 old_dtype = torch.get_default_dtype()\n    278 try:\n--> 279     return func(*args, **kwargs)\n    280 finally:\n    281     torch.set_default_dtype(old_dtype)\n\nFile ~/miniconda3/envs/recaptchav2-solver/lib/python3.10/site-packages/transformers/modeling_utils.py:4399, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\n   4389     if dtype_orig is not None:\n   4390         torch.set_default_dtype(dtype_orig)\n   4392     (\n   4393         model,\n   4394         missing_keys,\n   4395         unexpected_keys,\n   4396         mismatched_keys,\n   4397         offload_index,\n   4398         error_msgs,\n-> 4399     ) = cls._load_pretrained_model(\n   4400         model,\n   4401         state_dict,\n   4402         checkpoint_files,\n   4403         pretrained_model_name_or_path,\n   4404         ignore_mismatched_sizes=ignore_mismatched_sizes,\n   4405         sharded_metadata=sharded_metadata,\n   4406         device_map=device_map,\n   4407         disk_offload_folder=offload_folder,\n   4408         offload_state_dict=offload_state_dict,\n   4409         dtype=torch_dtype,\n   4410         hf_quantizer=hf_quantizer,\n   4411         keep_in_fp32_regex=keep_in_fp32_regex,\n   4412         device_mesh=device_mesh,\n   4413         key_mapping=key_mapping,\n   4414         weights_only=weights_only,\n   4415     )\n   4417 # make sure token embedding weights are still tied if needed\n   4418 model.tie_weights()\n\nFile ~/miniconda3/envs/recaptchav2-solver/lib/python3.10/site-packages/transformers/modeling_utils.py:4833, in PreTrainedModel._load_pretrained_model(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\n   4831 # Skip it with fsdp on ranks other than 0\n   4832 elif not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):\n-> 4833     disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n   4834         model_to_load,\n   4835         state_dict,\n   4836         shard_file,\n   4837         expected_keys,\n   4838         reverse_key_renaming_mapping,\n   4839         device_map=device_map,\n   4840         disk_offload_folder=disk_offload_folder,\n   4841         disk_offload_index=disk_offload_index,\n   4842         cpu_offload_folder=cpu_offload_folder,\n   4843         cpu_offload_index=cpu_offload_index,\n   4844         hf_quantizer=hf_quantizer,\n   4845         is_safetensors=is_offloaded_safetensors,\n   4846         keep_in_fp32_regex=keep_in_fp32_regex,\n   4847         unexpected_keys=unexpected_keys,\n   4848         device_mesh=device_mesh,\n   4849     )\n   4851 # force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\n   4852 del state_dict\n\nFile ~/miniconda3/envs/recaptchav2-solver/lib/python3.10/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)\n    113 @functools.wraps(func)\n    114 def decorate_context(*args, **kwargs):\n    115     with ctx_factory():\n--> 116         return func(*args, **kwargs)\n\nFile ~/miniconda3/envs/recaptchav2-solver/lib/python3.10/site-packages/transformers/modeling_utils.py:765, in _load_state_dict_into_meta_model(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\n    763     param = file_pointer.get_slice(serialized_param_name)\n    764 else:\n--> 765     param = empty_param.to(tensor_device)  # It is actually not empty!\n    767 to_contiguous, casting_dtype = _infer_parameter_dtype(\n    768     model,\n    769     param_name,\n   (...)\n    772     hf_quantizer,\n    773 )\n    775 if device_mesh is not None:  # In this case, the param is already on the correct device!\n\nNotImplementedError: Cannot copy out of meta tensor; no data!\n```"
      },
      {
        "user": "MutugiD",
        "body": "My workaround would be: \nPatch ```modeling_utils.py``` so that ```from_pretrained(..., from_flax=True)``` defaults to ```low_cpu_mem_usage=False``` (forcing real tensors), and inject a preprocessing step in ``save_pretrained()`` that clones every parameter ```(p.detach().cpu().clone())``` before serialization. This eliminates meta‐tensor placeholders and shared‐tensor errors, most likely. \n```safe_serialization=False``` seems temporal fix. \n\nOpening a PR and testing if it will be solid enough.\n"
      }
    ]
  },
  {
    "issue_number": 38008,
    "title": "Trainer Stuck at 0% Progress during Training on Multi-GPU Setup",
    "author": "yanho824",
    "state": "closed",
    "created_at": "2025-05-08T04:46:27Z",
    "updated_at": "2025-06-15T08:02:40Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- transformers version: 4.37.2\n- Platform: Linux version 5.15.0-105-generic\n- Python version: 3.10.17\n- Huggingface_hub version: 0.31.1\n- Safetensors version: 0.5.3\n- Accelerate version: 0.26.0\n- PyTorch version (GPU?): 2.1.2+cu121 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Machine: 4 x A800 GPUs\n- NVIDIA-SMI: 535.171.04\n\n### Who can help?\n\n@ArthurZucker \n@zach-huggingface\n@SunMarc\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI am running the following script:\n\n> CUDA_VISIBLE_DEVICES=0,1,2,3` python svd_trainer.py --model_id meta-llama/Llama-2-7b-hf --target_ratio 0.4 --remapping\n\nThe core parts of the code that may be related to the issue are:\n\n>  \n   # training\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n    from transformers import TrainingArguments\n    training_args = TrainingArguments(\n        output_dir= TA_tarined_model_output_dir,\n        num_train_epochs = TA_num_train_epochs,\n        evaluation_strategy = \"epoch\",\n        per_device_train_batch_size = 1,\n        per_device_eval_batch_size = 1,\n        warmup_steps = TA_warmup_steps,\n        lr_scheduler_type = \"cosine\",\n        seed = args.seed,\n        gradient_accumulation_steps = TA_gradient_accumulation_steps,\n        # load_best_model_at_end = True,\n        save_strategy = \"no\",\n        save_steps = 1000,\n        save_total_limit = 2,\n        remove_unused_columns=False,\n        # deepspeed =deepspeed_config\n    )\n   \n    trainer = SVDTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_traindata,\n        eval_dataset=tokenized_valdata,\n        data_collator=data_collator,\n    )\n\n\n    \n    model,  train_dataloader, eval_dataloader = accelerator.prepare(\n        model, trainer.get_train_dataloader(), trainer.get_eval_dataloader()\n    )\n    trainer.train = accelerator.prepare(trainer.train)\n    \n    \n    # save setting\n    TA_tarined_model_output_dir.mkdir(parents=True, exist_ok=True)\n    output_json_path = TA_tarined_model_output_dir / \"para_config.json\"\n    \n    data = {\n        \"model_id\": model_id,\n        \"DATASET_NAME\": DATASET_NAME,\n        \"target_compression_ratio\": target_compression_ratio,\n        \"BETA\": BETA,\n        \"dataset_processing\": {\n            \"SEED\": args.seed,\n            \"SEQ_LEN\": SEQ_LEN,\n            \"NSAMPLES_train\": NSAMPLES_train,\n            \"NSAMPLES_val\": NSAMPLES_val,\n        },\n        \"training_settings\": {\n            \"TA_num_train_epochs\": TA_num_train_epochs,\n            \"TA_warmup_steps\": TA_warmup_steps,\n            \"TA_gradient_accumulation_steps\": TA_gradient_accumulation_steps\n        },\n        \"loss_function_settings\": {\n            \"lambda_reg\": lambda_reg\n        },\n        \"optimizer_settings\": {\n            \"scheduler_lr\": scheduler_lr,\n            \"scheduler_step_size\": scheduler_step_size,\n            \"scheduler_gamma\": scheduler_gamma\n            # \"scheduler_half_step_size\": scheduler_half_step_size,\n            # \"scheduler_divide\": scheduler_divide\n        },\n        \"dtype_settings\": {\n            \"model_load_dtype\": \"f16\",\n            \"computeSVD_dtype\": \"f32\"\n        },\n        \"no_svd_layer\": model_no_svd_layer_dic[lower_id],\n    }\n    with open(output_json_path, \"w\") as json_file:\n        json.dump(data, json_file, indent=4)\n\n\n    \n    print(\"training start, target_compression_ratio is \",target_compression_ratio, \" , NGPUS is \", NGPUS)\n    trainer.train()\n    \n    accelerator.wait_for_everyone()\n\nIt stuck at 0% more than 12 hours.\n\n![Image](https://github.com/user-attachments/assets/f674c46e-a55b-4bc1-84cc-446845efcce4)\n\nI wrote a test code to determine that Trainer can run on a single GPU but not on multiple GPUs. However, the program reports an error after disabling P2P using the solution in https://github.com/huggingface/transformers/issues/26724.\n\n![Image](https://github.com/user-attachments/assets/7755619a-aee7-42b2-a760-d35241abc372)\n\nAlso, I tried `export NCCL_P2P_LEVEL=NVL` but it makes the program report an error:\n\n![Image](https://github.com/user-attachments/assets/ca2648be-d1c1-41fb-b74b-3c8b87be7d31)\n\nHere's how I checked the communication between the GPUs after `export NCCL_P2P_DISABLE=1`\n\n![Image](https://github.com/user-attachments/assets/c1d7271c-1ec8-49ff-bdbd-1dcd8b1c36e8)\n\nI migrated the test code to another A800 server that is also non-multicard runnable.\n\n\n### Expected behavior\n\nIf you have any insight or suggestions on how to resolve this issue, we would appreciate it.Thanks!",
    "comments": [
      {
        "user": "htdung167work0",
        "body": "I got this error too. It appears when load_best_model_at_end = True on multi-gpus training.\ntransformers==4.51.3\n"
      },
      {
        "user": "SunMarc",
        "body": "Hey @yanho824, where does this SDVtrainer comes from ? Is you subclassed Trainer, you don't need the following as we take care of that in Trainer. Also please share the full minimal reproducer  : \n```python\nmodel,  train_dataloader, eval_dataloader = accelerator.prepare(\n    model, trainer.get_train_dataloader(), trainer.get_eval_dataloader()\n)\ntrainer.train = accelerator.prepare(trainer.train)\n```\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38165,
    "title": "Gemma 3 Pipeline does not accept dictionary with no images",
    "author": "sheldonlai",
    "state": "open",
    "created_at": "2025-05-16T01:34:15Z",
    "updated_at": "2025-06-15T08:02:28Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nSystem info not really relevant as the bug is root caused in my description below.\n\n- `transformers` version: 4.51.3\n- Platform: Windows-10-10.0.26100-SP0\n- Python version: 3.11.9\n- Huggingface_hub version: 0.31.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.7.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.4.0+cu121 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script:Yes\n- GPU type: NVIDIA GeForce RTX 3090\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nThis issue can be created using the following snippet copied from Gemma 3 docs and up until transformer 4.51.3.\n```\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-12b-it\",\n    device=\"cuda\", # Or \"cpu\" if you don't have a compatible GPU\n    torch_dtype=torch.bfloat16 # Or torch.float16 or torch.float32 based on your hardware/needs\n)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            # Removed the image link from the example\n            {\"type\": \"text\", \"text\": \"What is the capital of France?\"} # Keep only the text part\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n```\n\nwhich will result in the error:\n\n```\nTraceback (most recent call last):\n  File \"D:\\experiments\\personal\\gemma_editor\\gemma_editor.py\", line 78, in <module>\n    run_gemma(SENTENCES)\n  File \"D:\\experiments\\personal\\gemma_editor\\gemma_editor.py\", line 41, in run_gemma\n    output = pipe(text=messages)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\experiments\\personal\\gemma_editor\\venv\\Lib\\site-packages\\transformers\\pipelines\\image_text_to_text.py\", line 311, in __call__\n    return super().__call__(Chat(text, images), **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\experiments\\personal\\gemma_editor\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 1379, in __call__\n    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\experiments\\personal\\gemma_editor\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 1385, in run_single\n    model_inputs = self.preprocess(inputs, **preprocess_params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\experiments\\personal\\gemma_editor\\venv\\Lib\\site-packages\\transformers\\pipelines\\image_text_to_text.py\", line 365, in preprocess\n    model_inputs = self.processor(images=images, text=text, return_tensors=self.framework, **processing_kwargs).to(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\experiments\\personal\\gemma_editor\\venv\\Lib\\site-packages\\transformers\\models\\gemma3\\processing_gemma3.py\", line 106, in __call__\n    image_inputs = self.image_processor(batched_images, **output_kwargs[\"images_kwargs\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\experiments\\personal\\gemma_editor\\venv\\Lib\\site-packages\\transformers\\image_processing_utils.py\", line 42, in __call__\n    return self.preprocess(images, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\experiments\\personal\\gemma_editor\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 866, in wrapper\n    return func(*args, **valid_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\experiments\\personal\\gemma_editor\\venv\\Lib\\site-packages\\transformers\\models\\gemma3\\image_processing_gemma3.py\", line 361, in preprocess\n    if do_rescale and is_scaled_image(images[0]):\n                                      ~~~~~~^^^\nIndexError: list index out of range\n```\n\n### Expected behavior\n\nThe problem here is that within image_text_to_text, the dictionary is made into type: Chat. [By default chat makes images an empty list](https://github.com/huggingface/transformers/blame/v4.51.3/src/transformers/pipelines/image_text_to_text.py#L114). Then this is propagated to [images](https://github.com/huggingface/transformers/blame/v4.51.3/src/transformers/pipelines/image_text_to_text.py#L353C16-L353C39) where it ultimately lands in processing_gemma_3.py where the [if condition only checks if the images are None](https://github.com/huggingface/transformers/blob/v4.51.3/src/transformers/models/gemma3/processing_gemma3.py#L102), but as we observed from the upstream code, it is set as an empty list.\n\nIt looks like in the main branch, images may no longer be set processor with Chat typed input directly, but I believe that processing_gemma3 should just handle empty list to avoid future bugs.\n",
    "comments": [
      {
        "user": "khushil024",
        "body": "Hi! I'd like to take this up if it's still available."
      },
      {
        "user": "sheldonlai",
        "body": "Feel free to take it, I only fixed it in my local Libs to unblock myself."
      },
      {
        "user": "zucchini-nlp",
        "body": "Hmm, that is weird, I remember we fixed text-only inference recently no? cc @yonigozlan "
      }
    ]
  },
  {
    "issue_number": 38617,
    "title": "ImportError: cannot import name 'layer_type_validation' from 'transformers.configuration_utils'",
    "author": "Jacoobr",
    "state": "closed",
    "created_at": "2025-06-05T16:09:03Z",
    "updated_at": "2025-06-15T07:56:37Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nenv:\nName: transformers\nVersion: 4.53.0.dev0\n\nwhe i called hte code bellowed:\n`model = AutoModelForImageTextToText.from_pretrained(model_id, local_files_only=True, **model_kwargs)`\nthe model_id is medgemma model that from https://huggingface.co/models?other=medgemma. \nthe ImportError: cannot import name 'layer_type_validation' from 'transformers.configuration_utils (/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py' errror occurred.\n\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nmodel = AutoModelForImageTextToText.from_pretrained(model_id, local_files_only=True, **model_kwargs)\n#note model_id visit on https://huggingface.co/models?other=medgemma\n\n### Expected behavior\n\nrun without error.",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "Hmm, can you try to re-install? The `layer_type_validation` was added only in the `main` branch and I can load the model successfully in `4.52.3` as follows. So it seems like your installed version somehow got messed \n\n```\nmodel = AutoModelForImageTextToText.from_pretrained(\"google/medgemma-4b-it\")\n```"
      },
      {
        "user": "Jacoobr",
        "body": "Thx, re-install and restart the interactive  kernel solved the issue."
      }
    ]
  },
  {
    "issue_number": 38442,
    "title": "ImportError: cannot import name 'GenerationMixin' from 'transformers.generation'",
    "author": "qsuzer",
    "state": "open",
    "created_at": "2025-05-28T14:21:31Z",
    "updated_at": "2025-06-14T10:40:43Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n\nPackage                   Version        Editable project location\n------------------------- -------------- -------------------------\naccelerate                1.7.0\naiohappyeyeballs          2.4.4\naiohttp                   3.11.9\naiosignal                 1.3.1\naltair                    5.5.0\nannotated-types           0.7.0\nanyio                     4.6.2.post1\nargon2-cffi               23.1.0\nargon2-cffi-bindings      21.2.0\narrow                     1.3.0\nasttokens                 3.0.0\nasync-lru                 2.0.5\nasync-timeout             5.0.1\nattrs                     24.2.0\nbabel                     2.17.0\nbase58                    2.1.1\nbeautifulsoup4            4.13.3\nbitsandbytes              0.45.5\nbleach                    6.2.0\nblinker                   1.9.0\nblis                      0.7.11\nbm25s                     0.2.0\ncachetools                5.5.0\ncatalogue                 2.0.10\ncertifi                   2024.8.30\ncffi                      1.17.1\ncharset-normalizer        3.4.0\nclick                     8.1.7\ncoloredlogs               15.0.1\ncomm                      0.2.2\nconfection                0.1.5\ncontourpy                 1.3.0\ncycler                    0.12.1\ncymem                     2.0.10\nCython                    3.0.11\ndashscope                 1.22.2\ndatasets                  3.1.0\ndebugpy                   1.8.13\ndecorator                 5.2.1\ndefusedxml                0.7.1\ndill                      0.3.8\ndistro                    1.9.0\ndocker-pycreds            0.4.0\neval_type_backport        0.2.2\nexceptiongroup            1.2.2\nexecuting                 2.2.0\nfaiss-gpu                 1.7.2\nfastapi                   0.115.6\nfastjsonschema            2.21.1\nfilelock                  3.16.1\nflashrag-dev              0.1.4.dev0     /home/wmz/FlashRAG\nflatbuffers               24.3.25\nfonttools                 4.56.0\nfqdn                      1.5.1\nfrozenlist                1.5.0\nfschat                    0.2.36\nfsspec                    2024.9.0\ngitdb                     4.0.11\nGitPython                 3.1.43\nh11                       0.14.0\nhf-xet                    1.1.2\nhttpcore                  1.0.7\nhttpx                     0.28.0\nhuggingface-hub           0.32.2\nhumanfriendly             10.0\nidna                      3.10\nimportlib_metadata        8.6.1\nimportlib_resources       6.5.2\nipykernel                 6.29.5\nipython                   8.18.1\nipywidgets                8.1.5\nisoduration               20.11.0\njedi                      0.19.2\nJinja2                    3.1.4\njiter                     0.8.0\njoblib                    1.4.2\njson5                     0.10.0\njsonlines                 4.0.0\njsonpointer               3.0.0\njsonschema                4.23.0\njsonschema-specifications 2024.10.1\njupyter                   1.1.1\njupyter_client            8.6.3\njupyter-console           6.6.3\njupyter_core              5.7.2\njupyter-events            0.12.0\njupyter-lsp               2.2.5\njupyter_server            2.15.0\njupyter_server_terminals  0.5.3\njupyterlab                4.3.6\njupyterlab_pygments       0.3.0\njupyterlab_server         2.27.3\njupyterlab_widgets        3.0.13\nkiwisolver                1.4.7\nlangcodes                 3.5.0\nlanguage_data             1.3.0\nlatex2mathml              3.77.0\nlightgbm                  4.5.0\nllvmlite                  0.43.0\nmarisa-trie               1.2.1\nmarkdown-it-py            3.0.0\nmarkdown2                 2.5.1\nMarkupSafe                3.0.2\nmatplotlib                3.9.4\nmatplotlib-inline         0.1.7\nmdurl                     0.1.2\nmistune                   3.1.3\nmodelscope                1.21.0\nmpmath                    1.3.0\nmultidict                 6.1.0\nmultiprocess              0.70.16\nmurmurhash                1.0.11\nnarwhals                  1.15.2\nnbclient                  0.10.2\nnbconvert                 7.16.6\nnbformat                  5.10.4\nnest-asyncio              1.6.0\nnetworkx                  3.2.1\nnh3                       0.2.19\nnltk                      3.9.1\nnmslib                    2.1.1\nnotebook                  7.3.3\nnotebook_shim             0.2.4\nnumba                     0.60.0\nnumpy                     1.26.4\nnvidia-cublas-cu12        12.1.3.1\nnvidia-cuda-cupti-cu12    12.1.105\nnvidia-cuda-nvrtc-cu12    12.1.105\nnvidia-cuda-runtime-cu12  12.1.105\nnvidia-cudnn-cu12         8.9.2.26\nnvidia-cufft-cu12         11.0.2.54\nnvidia-curand-cu12        10.3.2.106\nnvidia-cusolver-cu12      11.4.5.107\nnvidia-cusparse-cu12      12.1.0.106\nnvidia-nccl-cu12          2.18.1\nnvidia-nvjitlink-cu12     12.4.127\nnvidia-nvtx-cu12          12.1.105\nonnxruntime               1.19.2\nopenai                    1.56.2\norjson                    3.10.12\noverrides                 7.7.0\npackaging                 24.2\npandas                    2.2.3\npandocfilters             1.5.1\nparso                     0.8.4\npathlib_abc               0.1.1\npathy                     0.11.0\npeft                      0.13.2\npexpect                   4.9.0\npillow                    11.0.0\npip                       24.3.1\nplatformdirs              4.3.7\npreshed                   3.0.9\nprometheus_client         0.21.1\nprompt_toolkit            3.0.48\npropcache                 0.2.1\nprotobuf                  5.29.1\npsutil                    6.1.0\nptyprocess                0.7.0\npure_eval                 0.2.3\npyarrow                   18.1.0\npybind11                  2.6.1\npycparser                 2.22\npydantic                  2.10.3\npydantic_core             2.27.1\npydeck                    0.9.1\nPygments                  2.18.0\npyjnius                   1.6.1\npyparsing                 3.2.1\npyserini                  0.22.1\nPyStemmer                 2.2.0.3\npython-dateutil           2.9.0.post0\npython-json-logger        3.3.0\npytz                      2024.2\nPyYAML                    6.0.2\npyzmq                     26.3.0\nqwen-agent                0.0.16\nrank-bm25                 0.2.2\nreferencing               0.35.1\nregex                     2024.11.6\nrequests                  2.32.3\nrfc3339-validator         0.1.4\nrfc3986-validator         0.1.1\nrich                      13.9.4\nrouge                     1.0.1\nrpds-py                   0.22.3\nsafetensors               0.4.6.dev0\nscikit-learn              1.6.0\nscipy                     1.10.1\nseaborn                   0.13.2\nSend2Trash                1.8.3\nsentence-transformers     3.3.1\nsentencepiece             0.2.0\nsentry-sdk                2.29.1\nsetproctitle              1.3.6\nsetuptools                75.6.0\nshortuuid                 1.0.13\nsix                       1.17.0\nsmart-open                6.4.0\nsmmap                     5.0.1\nsniffio                   1.3.1\nsoupsieve                 2.6\nspacy                     3.6.1\nspacy-legacy              3.0.12\nspacy-loggers             1.0.5\nsrsly                     2.4.8\nstack-data                0.6.3\nstarlette                 0.41.3\nstreamlit                 1.40.2\nsvgwrite                  1.4.3\nsympy                     1.13.1\ntenacity                  9.0.0\nterminado                 0.18.1\nthinc                     8.1.12\nthreadpoolctl             3.5.0\ntiktoken                  0.8.0\ntinycss2                  1.4.0\ntokenizers                0.21.1\ntoml                      0.10.2\ntomli                     2.2.1\ntorch                     2.1.2\ntornado                   6.4.2\ntqdm                      4.67.1\ntraitlets                 5.14.3\ntransformers              4.52.3\ntriton                    2.1.0\ntrl                       0.19.0\ntyper                     0.9.4\ntypes-python-dateutil     2.9.0.20241206\ntyping_extensions         4.12.2\ntzdata                    2024.2\nuri-template              1.3.0\nurllib3                   2.2.3\nuvicorn                   0.32.1\nwandb                     0.19.11\nwasabi                    1.1.3\nwatchdog                  6.0.0\nwavedrom                  2.0.3.post3\nwcwidth                   0.2.13\nwebcolors                 24.11.1\nwebencodings              0.5.1\nwebsocket-client          1.8.0\nwheel                     0.45.1\nwidgetsnbextension        4.0.13\nxxhash                    3.5.0\nyarl                      1.18.3\nzipp                      3.21.0\n\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n     import json\nimport torch\nimport logging\nfrom datasets import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, TaskType, prepare_model_for_kbit_training\nfrom trl import SFTTrainer, SFTConfig\nfrom tqdm import tqdm\nimport os\n\n   model, tokenizer = prepare_model_and_tokenizer(MODEL_NAME)\n\n        # LoRA配置\n        peft_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n            lora_dropout=0.1,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM,\n        )\n\n        # SFT配置\n        sft_config = SFTConfig(\n            output_dir=OUTPUT_DIR,\n            num_train_epochs=3,\n            per_device_train_batch_size=4,\n            per_device_eval_batch_size=4,\n            gradient_accumulation_steps=4,\n            optim=\"paged_adamw_8bit\",\n            save_steps=500,\n            logging_steps=50,\n            learning_rate=2e-4,\n            weight_decay=0.001,\n            fp16=True,\n            bf16=False,\n            max_grad_norm=0.3,\n            warmup_ratio=0.03,\n            lr_scheduler_type=\"cosine\",\n            eval_strategy=\"steps\",\n            eval_steps=500,\n            save_total_limit=2,\n            load_best_model_at_end=True,\n            report_to=\"none\",\n            max_seq_length=512,\n            packing=False,\n            dataset_text_field=\"text\",\n        )\n\n        # SFT训练器\n        trainer = SFTTrainer(\n            model=model,\n            args=sft_config,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            processing_class=tokenizer,\n            peft_config=peft_config,\n            formatting_func=None,\n        )\n\n### Expected behavior\n\nTraceback (most recent call last):\n  File \"/home/wmz/FlashRAG/train_decomposer.py\", line 5, in <module>\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n  File \"/data/anaconda3/envs/flashrag/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 2045, in __getattr__\n    module = self._get_module(self._class_to_module[name])\n  File \"/data/anaconda3/envs/flashrag/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 2075, in _get_module\n    raise e\n  File \"/data/anaconda3/envs/flashrag/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 2073, in _get_module\n    return importlib.import_module(\".\" + module_name, self.__name__)\n  File \"/data/anaconda3/envs/flashrag/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/data/anaconda3/envs/flashrag/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py\", line 21, in <module>\n    from .auto_factory import (\n  File \"/data/anaconda3/envs/flashrag/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n    from ...generation import GenerationMixin\nImportError: cannot import name 'GenerationMixin' from 'transformers.generation' (/data/anaconda3/envs/flashrag/lib/python3.9/site-packages/transformers/generation/__init__.py)\n\nI installed through \"pip install transformers\" and the version is 4.52.3",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @qsuzer, this seems like an environment issue that we probably can't debug for you! Can you try making a fresh environment? `GenerationMixin` should be importable."
      },
      {
        "user": "baojunqi",
        "body": "Same problem. Have you addressed it? \n"
      }
    ]
  },
  {
    "issue_number": 38128,
    "title": "ImportError: cannot import name 'DataCollatorForCTCWithPadding' on macOS ARM64 with transformers==4.51.3 (Python 3.10/3.11) - Class missing from installed data_collator.py",
    "author": "paulkhouan",
    "state": "open",
    "created_at": "2025-05-14T16:36:07Z",
    "updated_at": "2025-06-14T08:02:23Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nEnvironment Details:\n\t•\ttransformers version: 4.51.3\n\t•\ttokenizers version: (e.g., 0.21.1 or the version pip installed with transformers 4.51.3)\n\t•\ttorch version: 2.7.0 (with MPS available and built)\n\t•\tPython version(s) tested: 3.10.17 (Homebrew), 3.11.12 (Homebrew)\n\t•\tOperating System: macOS Sequoia 15.4.1 on Apple Silicon (M2 chip)\n\t•\tInstallation method: pip install transformers tokenizers --no-cache-dir into a fresh venv.\n\n@Rocketknight1\n@zach-huggingface \n\n### Who can help?\n\nAfter a clean pip install transformers==4.51.3 tokenizers into a fresh virtual environment on macOS ARM64 (M2 chip), the class DataCollatorForCTCWithPadding cannot be imported either directly via from transformers import DataCollatorForCTCWithPadding or by accessing it as an attribute of the transformers.data.data_collator module. This issue occurs consistently in both Python scripts and the Python REPL.\nFurther investigation reveals that the installed file .../site-packages/transformers/data/data_collator.py does not contain the class definition for DataCollatorForCTCWithPadding, and dir(transformers.data.data_collator) does not list it. Other classes like DataCollatorWithPadding and AutoTokenizer import correctly.\nThis suggests a potential packaging issue with the wheel provided on PyPI for the macOS ARM64 platform for this version, where data_collator.py might be incomplete or an incorrect version of the file is being included.\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nSteps to Reproduce:\n\t1\tCreate a new virtual environment on macOS ARM64 with Python 3.10 or 3.11.\n\t2\tActivate the virtual environment.\n\t3\tRun pip install --upgrade pip.\n\t4\tRun pip install torch torchaudio.\n\t5\tRun pip install transformers==4.51.3 tokenizers --no-cache-dir.\n\t6\tStart Python REPL or run a script with the following:       import transformers\n\t7\tprint(f\"Transformers version: {transformers.__version__}\")\n\t8\t# This import will fail:\n\t9\tfrom transformers import DataCollatorForCTCWithPadding \n\t10\t# Also, this will show the class is missing from the submodule:\n\t11\t# from transformers.data import data_collator\n\t12\t# print(hasattr(data_collator, 'DataCollatorForCTCWithPadding')) # -> False\n\t13\t# # Manually checking the content of site-packages/transformers/data/data_collator.py confirms absence.\n\n### Expected behavior\n\nExpected Behavior: DataCollatorForCTCWithPadding should be importable from transformers or transformers.data.data_collator.\nActual Behavior: ImportError: cannot import name 'DataCollatorForCTCWithPadding' from 'transformers' AttributeError: module 'transformers.data.data_collator' has no attribute 'DataCollatorForCTCWithPadding' (when trying direct submodule attribute access). The class definition string is not found within the installed transformers/data/data_collator.py file.\nAdditional Context: This issue was consistently reproduced even after:\n\t•\tMultiple fresh virtual environments.\n\t•\tUsing --no-cache-dir.\n\t•\tVerifying Python interpreter paths and sys.path.\n\t•\tTesting with both Python 3.10 and Python 3.11.\nThe transformers library appears to install other components correctly (e.g., AutoTokenizer imports fine). The problem seems localized to the DataCollatorForCTCWithPadding class not being present in the data_collator.py file of the installed package.",
    "comments": [
      {
        "user": "vasqu",
        "body": "`DataCollatorForCTCWithPadding` is not an official collator within the package. The only reference I could find is in the ASR tutorial: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md\n\nBut in the tutorial, you build the collator step-by-step yourself, so it doesn't require an import from transformers. The snippet can be written into:\n```python\nimport torch\n\nfrom transformers import AutoProcessor\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n     processor: AutoProcessor\n     padding: Union[bool, str] = \"longest\"\n\n     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n         # split inputs and labels since they have to be of different lengths and need\n         # different padding methods\n         input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n         batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n\n         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=\"pt\")\n\n         # replace padding with -100 to ignore loss correctly\n         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n         batch[\"labels\"] = labels\n\n         return batch\n\nprocessor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base\")\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")\n```\nHaven't tried it, no guarantees especially since the doc is also quite old 👀 "
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38139,
    "title": "Have to import cv2 and pop up window frist, or else it stuck forever",
    "author": "leemengwei",
    "state": "open",
    "created_at": "2025-05-15T06:02:28Z",
    "updated_at": "2025-06-14T08:02:19Z",
    "labels": [],
    "body": "> @SSacSim Yes    One have to import cv2 and pop up window frist. Thats really annoying.\n> \n> ![Image](https://github.com/user-attachments/assets/37cc08c1-9a21-4fd1-965b-6625b90f6f08)\n> \n> \n> transformers             4.48.3\n> opencv-python            4.10.0.84\n> \n> @purusharthmalik  \n\n _Originally posted by @leemengwei in [#37239](https://github.com/huggingface/transformers/issues/37239#issuecomment-2882648658)_",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "I'm not sure what we can do with this error! Have you tried opening an issue with openCV?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38823,
    "title": "Cannot create a 2-class Deformable DETR model from a 1-class model",
    "author": "jveitchmichaelis",
    "state": "closed",
    "created_at": "2025-06-14T01:54:15Z",
    "updated_at": "2025-06-14T02:23:44Z",
    "labels": [
      "bug"
    ],
    "body": "\nI have a test case that instantiates a model with a number of class count options e.g. (1,2,10). The source model is trained on a single class. `ignore_mismatched_sizes` is set to `True`. If I change `num_classes` to 2, the model load fails. If I change it to anything else, it seems to be fine. The error seems to be in loading a bias term:\n\n```\nFile ~/miniconda3/envs/x/lib/python3.12/site-packages/transformers/modeling_utils.py:731, in _load_parameter_into_model(model, param_name, tensor)\n    729 module, param_type = get_module_from_name(model, param_name)\n    730 # This will check potential shape mismatch if skipped before\n--> 731 module.load_state_dict({param_type: tensor}, strict=False, assign=True)\n\nFile ~/miniconda3/envs/x/lib/python3.12/site-packages/torch/nn/modules/module.py:2593, in Module.load_state_dict(self, state_dict, strict, assign)\n   2585         error_msgs.insert(\n   2586             0,\n   2587             \"Missing key(s) in state_dict: {}. \".format(\n   2588                 \", \".join(f'\"{k}\"' for k in missing_keys)\n   2589             ),\n   2590         )\n   2592 if len(error_msgs) > 0:\n-> 2593     raise RuntimeError(\n   2594         \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n   2595             self.__class__.__name__, \"\\n\\t\".join(error_msgs)\n   2596         )\n   2597     )\n   2598 return _IncompatibleKeys(missing_keys, unexpected_keys)\n\nRuntimeError: Error(s) in loading state_dict for Linear:\n\tsize mismatch for bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).\n```\n\nPossibly related to https://github.com/huggingface/transformers/issues/36960. Doing some digging it seems like this was fixed a couple of weeks ago, will try from main.\n\n### System Info\n\nEnv:\n\n```\n- `transformers` version: 4.52.4\n- Platform: macOS-15.4.1-arm64-arm-64bit\n- Python version: 3.12.3\n- Huggingface_hub version: 0.31.2\n- Safetensors version: 0.5.3\n- Accelerate version: not installed\n- Accelerate config: not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.7.0 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: No\n```\n\n\n### Who can help?\n\n@amyeroberts @qubvel \n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nThis example makes a 1-class model and then attempts to convert it to 2.\n\nThis fails:\n\n```python\nimport tempfile\nfrom transformers import DeformableDetrForObjectDetection, DeformableDetrConfig\n\nwith tempfile.TemporaryDirectory() as dirname:\n    configuration = DeformableDetrConfig(num_labels=1)\n    model = DeformableDetrForObjectDetection(configuration)\n    \n    model.save_pretrained(dirname)\n    model = DeformableDetrForObjectDetection.from_pretrained(\n        dirname,\n        num_labels=2,\n        ignore_mismatched_sizes=True\n    )\n```\n```bash\n\nFile ~/miniconda3/envs/x/lib/python3.12/site-packages/torch/nn/modules/module.py:2593, in Module.load_state_dict(self, state_dict, strict, assign)\n   2585         error_msgs.insert(\n   2586             0,\n   2587             \"Missing key(s) in state_dict: {}. \".format(\n   2588                 \", \".join(f'\"{k}\"' for k in missing_keys)\n   2589             ),\n   2590         )\n   2592 if len(error_msgs) > 0:\n-> 2593     raise RuntimeError(\n   2594         \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n   2595             self.__class__.__name__, \"\\n\\t\".join(error_msgs)\n   2596         )\n   2597     )\n   2598 return _IncompatibleKeys(missing_keys, unexpected_keys)\n\nRuntimeError: Error(s) in loading state_dict for Linear:\n\tsize mismatch for bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).\n```\n\nOddly, converting to a 10 class model is fine:\n\n```python\nwith tempfile.TemporaryDirectory() as dirname:\n    configuration = DeformableDetrConfig(num_labels=1)\n    model = DeformableDetrForObjectDetection(configuration)\n    \n    model.save_pretrained(dirname)\n    model = DeformableDetrForObjectDetection.from_pretrained(\n        dirname,\n        num_labels=10,\n        ignore_mismatched_sizes=True\n    )\n```\n```bash\nSome weights of DeformableDetrForObjectDetection were not initialized from the model checkpoint at /var/folders/xp/26mwm6b951l39z_tvs352_c80000gn/T/tmpf8nrmwwy and are newly initialized because the shapes did not match:\n- class_embed.0.bias: found shape torch.Size([1]) in the checkpoint and torch.Size([10]) in the model instantiated\n- class_embed.0.weight: found shape torch.Size([1, 256]) in the checkpoint and torch.Size([10, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```\n\nIt's also fine if we start with a different number of labels, which suggests it's specifically 1 -> 2 that's an issue?\n\n```python\nwith tempfile.TemporaryDirectory() as dirname:\n    configuration = DeformableDetrConfig(num_labels=10)\n    model = DeformableDetrForObjectDetection(configuration)\n    \n    model.save_pretrained(dirname)\n    model = DeformableDetrForObjectDetection.from_pretrained(\n        dirname,\n        num_labels=2,\n        ignore_mismatched_sizes=True\n    )\n```\n```bash\nSome weights of DeformableDetrForObjectDetection were not initialized from the model checkpoint at /var/folders/xp/26mwm6b951l39z_tvs352_c80000gn/T/tmpwmopo1nf and are newly initialized because the shapes did not match:\n- class_embed.0.bias: found shape torch.Size([10]) in the checkpoint and torch.Size([2]) in the model instantiated\n- class_embed.0.weight: found shape torch.Size([10, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```\n\nDetr doesn't have this problem, so it seems to be an edge case in DeformableDetr specifically. Though here the class_labels_classifier bias goes from 2 -> 3. \n\n```python\nfrom transformers import DetrForObjectDetection, DetrConfig\n\nwith tempfile.TemporaryDirectory() as dirname:\n    configuration = DetrConfig(num_labels=1)\n    model = DetrForObjectDetection(configuration)\n    \n    model.save_pretrained(dirname)\n    model = DetrForObjectDetection.from_pretrained(\n        dirname,\n        num_labels=2,\n        ignore_mismatched_sizes=True\n    )\n```\n```bash\nSome weights of DetrForObjectDetection were not initialized from the model checkpoint at /var/folders/xp/26mwm6b951l39z_tvs352_c80000gn/T/tmp4yc1m7jh and are newly initialized because the shapes did not match:\n- class_labels_classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n- class_labels_classifier.weight: found shape torch.Size([2, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```\n\n### Expected behavior\n\n`from_pretrained` should be able to handle this and return a model initialised with a 2-class detection head as requested.",
    "comments": [
      {
        "user": "jveitchmichaelis",
        "body": "Seems to be fixed in transformers-4.53.0.dev0"
      }
    ]
  },
  {
    "issue_number": 38820,
    "title": "CI skipped failures tracking issue",
    "author": "ivarflakstad",
    "state": "open",
    "created_at": "2025-06-13T20:56:55Z",
    "updated_at": "2025-06-13T20:56:55Z",
    "labels": [],
    "body": "Some of the tests in our scheduled CI's are skipped because they require newer hardware (higher compute capability) than the setup we are currently using. That's not a problem on it's own, but after manually checking we have found that there are actually quite a few of these tests that fail on the hardware that can run said tests.\n\nThis issue is to be used to track work related to this category of tests.\n\nThe first step will be to skip said tests in the CI and add a reference to this issue in the reason for skipping.",
    "comments": []
  },
  {
    "issue_number": 36193,
    "title": "ValueError: Unrecognized image processor in Qwen/Qwen2.5-VL-3B-Instruct.",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2025-02-14T14:39:52Z",
    "updated_at": "2025-06-13T15:33:45Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- transformers 4.49.0.dev0\n- Python 3.11.11\n\n### Reproduction\n\nI follow model instructions from [here](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct).\n\n1. install `transformers` from GH \n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\n2. laod `Qwen/Qwen2.5-VL-3B-Instruct` or `Qwen/Qwen2.5-VL-7B-Instruct`; I did not tested with others \n\n```python\nfrom transformers import AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n```\n\n3. it raise exception\n\n```\nValueError: Unrecognized image processor in Qwen/Qwen2.5-VL-3B-Instruct. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, aria, beit, bit, blip, blip-2, bridgetower, chameleon, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dinat, dinov2, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, fuyu, git, glpn, got_ocr2, grounding-dino, groupvit, hiera, idefics, idefics2, idefics3, ijepa, imagegpt, instructblip, instructblipvideo, kosmos-2, layoutlmv2, layoutlmv3, levit, llava, llava_next, llava_next_video, llava_onevision, mask2former, maskformer, mgp-str, mllama, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, nougat, oneformer, owlv2, owlvit, paligemma, perceiver, pix2struct, pixtral, poolformer, pvt, pvt_v2, qwen2_5_vl, qwen2_vl, regnet, resnet, rt_detr, sam, segformer, seggpt, siglip, superglue, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, timm_wrapper, tvlt, tvp, udop, upernet, van, videomae, vilt, vipllava, vit, vit_hybrid, vit_mae, vit_msn, vitmatte, xclip, yolos, zoedepth\n```\n\n### Expected behavior\n\nprocessor loads without exceptions",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "When installing `pip install git+https://github.com/huggingface/transformers.git@1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf` `AutoProcessor` works. Looks like https://github.com/huggingface/transformers/commit/33d1d715b0260efc1c2df1c16d864186b5bb9437 might have broke it."
      },
      {
        "user": "JasonChenJC",
        "body": "same problem"
      },
      {
        "user": "Zzsf11",
        "body": "Delete   ```\"image_processor_type\": \"Qwen2_5_VLImageProcessor\"``` in Qwen2.5-VL-7B-Instruct/preprocessor_config.json could work.\n"
      }
    ]
  },
  {
    "issue_number": 38776,
    "title": "[Bug][DOCS] TrainingArguments.__init__() got an unexpected keyword argument 'fsdp_strategy'",
    "author": "PT-10",
    "state": "closed",
    "created_at": "2025-06-12T06:54:36Z",
    "updated_at": "2025-06-13T15:32:41Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntransformers v4.52.3\n\nThe docs at https://huggingface.co/docs/transformers/accelerate show - \n![image](https://github.com/user-attachments/assets/71d69201-1c94-430d-a451-91c07b59acf4)\n\nHowever when working with accelerate in TrainingArgs I get the following issue of fsdp strategy not available\n![image](https://github.com/user-attachments/assets/6f0b626a-8ca2-414c-a0da-0bef7a58b2d2)\n\nI checked the source code at https://github.com/huggingface/transformers/blob/v4.52.3/src/transformers/training_args.py and transformers v4.52.3 does not actually have 'fsdp_strategy' as an argument in the training args. I believe the docs are to be updated.\n\n\n\n### Who can help?\n\n@SunMarc @zach-huggingface\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nTask information:\n Finetune \"mistralai/Mistral-7B-Instruct-v0.1\" using PEFT on \"Josephgflowers/Finance-Instruct-500k\"\n\n1. Create config yaml as per like\n```\nfsdp_yaml = \"\"\"\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_forward_prefetch: false\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: FULL_SHARD\n  fsdp_state_dict_type: SHARDED_STATE_DICT\n  fsdp_sync_module_states: true\n  fsdp_transformer_layer_cls_to_wrap: MistralDecoderLayer\n  fsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n\"\"\"\n\nwith open(\"path/to/config/yaml\", \"w\") as f:\n    f.write(fsdp_yaml)\n```\n\n2. Initialize trainer arguments\n```\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\ntraining_args = TrainingArguments(\n    output_dir=\"path/to/output_dir\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    num_train_epochs=1,\n    fsdp_config=\"path/to/config/yaml\",\n    fsdp_strategy=\"full_shard\",\n    fp16=True,\n    logging_steps=10,\n    save_steps=100,\n    save_total_limit=1\n)\n```\n\n\n### Expected behavior\n\nTraining args being initialized as per docs and then being able to pass them to the Trainer class",
    "comments": [
      {
        "user": "SunMarc",
        "body": "Indeed, not sure where this came from ! The right argument should be fsdp instead. Would you like to create a PR to fix the doc @PT-10 ?"
      },
      {
        "user": "PT-10",
        "body": "> Indeed, not sure where this came from ! The right argument should be fsdp instead. Would you like to create a PR to fix the doc [@PT-10](https://github.com/PT-10) ?\n\nsure."
      },
      {
        "user": "marcndo",
        "body": "Hi @PT-10 , Are you already working on this? If not, I'd be happy to take it.\n\n\n"
      }
    ]
  },
  {
    "issue_number": 30725,
    "title": "Support for Multiple Datasets and Domain-Specific Loss Calculation in Trainer",
    "author": "ghost",
    "state": "open",
    "created_at": "2024-05-09T10:45:25Z",
    "updated_at": "2025-06-13T14:13:59Z",
    "labels": [
      "trainer",
      "Feature request"
    ],
    "body": "### Feature request\n\nI am currently working on a project that involves sequence level distillation across multiple domains, requiring the handling of separate datasets for each domain within a single training loop. Specifically, the challenge involves integrating data from four distinct domains, computing loss individually per domain, and then aggregating these losses into a global loss measure that can guide the overall training process.\n\n### Motivation\n\nIdeally, the Trainer class would natively support the following features:\r\n\r\nMultiple Dataset Handling: Ability to pass multiple datasets into the Trainer directly, with each dataset potentially representing a different domain.\r\nDomain-Specific Loss Calculation: Support for defining and computing loss separately for each domain's dataset within the training loop and then integrating these losses into a global training objective.\n\n### Your contribution\n\nCurrently, the Trainer class in the Transformers library supports passing a single dataset for training and evaluation. To handle multiple datasets or to calculate domain-specific losses, one must subclass the Trainer and override methods such as compute_loss, which complicates the implementation and integration of domain-specific training strategies.",
    "comments": [
      {
        "user": "amyeroberts",
        "body": "cc @muellerzr @pacman100 "
      },
      {
        "user": "cw235",
        "body": "Your feature request for enhancing the Trainer class in the Transformers library to support handling multiple datasets representing different domains and calculating domain-specific losses is indeed valuable for projects involving sequence level distillation across various domains. Here are some potential contributions that could address your requirements:\n\n1. **Multiple Dataset Handling Support:**\n   - Modify the Trainer class to accept multiple datasets representing different domains directly as input. This enhancement would streamline the integration of diverse data sources within a single training loop.\n\n2. **Domain-Specific Loss Calculation Integration:**\n   - Implement a mechanism within the Trainer class to define and compute losses separately for each domain's dataset during the training loop. This functionality would allow for domain-specific loss calculations and aggregation into a global training objective.\n\n3. **Flexible Loss Aggregation Mechanism:**\n   - Introduce a flexible mechanism for aggregating domain-specific losses into a global training objective. This feature would enable users to define custom aggregation strategies based on the specific requirements of their projects.\n\n4. **Unified Training Interface:**\n   - Enhance the Trainer class to provide a unified training interface that simplifies the implementation of domain-specific training strategies. This improvement would abstract away the complexity of subclassing and method overriding for users seeking to work with multiple datasets and domain-specific losses.\n\n5. **Documentation and Examples:**\n   - Update the documentation of the Trainer class to include clear explanations and examples demonstrating how to leverage the new features for handling multiple datasets and calculating domain-specific losses. Providing detailed guidance would facilitate the adoption of these functionalities by the community.\n\nBy incorporating these features and enhancements into the Trainer class of the Transformers library, users working on projects involving sequence level distillation across diverse domains would benefit from a more streamlined and efficient training process. Additionally, these improvements would contribute to the versatility and usability of the library for a broad range of applications requiring multi-domain data integration and domain-specific training strategies."
      },
      {
        "user": "yujianll",
        "body": "I'm also interested in the support of multiple datasets. A use case I can think of is during instruction tuning, we would like to also add pre-training loss for regularization."
      }
    ]
  },
  {
    "issue_number": 37671,
    "title": "`Model.from_pretrained` breaks when using SinusoidalEmbedding",
    "author": "ZhiyuanChen",
    "state": "closed",
    "created_at": "2025-04-22T10:36:14Z",
    "updated_at": "2025-06-13T14:01:05Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.51.0\n- Platform: macOS-15.3.1-arm64-arm-64bit\n- Python version: 3.12.9\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n\n### Who can help?\n\nSinusoidalEmbedding does not require `state_dict`, and since there are some bugs related to loading/saving its states (#31387), a work around is to override its related functions: \n\n```python\n    def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n        return {}\n\n    def load_state_dict(self, *args, state_dict, strict=True):\n        return\n\n    def _load_from_state_dict(\n        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n    ):\n        return\n```\n\nThis used to works, but a recent update breaks it (Going back to transformers 4.50 worked fine). \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\npip install multimolecule\n\nFailure:\n\n```python\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, pipeline\nfrom multimolecule.models import ErnieRnaForSecondaryStructurePrediction as Model\n\nmodel = Model.from_pretrained(\"multimolecule/ernierna-ss\")\n\nmodel.to(\"cuda\")\n```\n\nNotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n\n\nWorks:\n```python\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, pipeline\nfrom multimolecule.models import ErnieRnaForSecondaryStructurePrediction as Model\n\nmodel = Model(AutoConfig.from_pretrained(\"multimolecule/ernierna-ss\"))\n\nmodel.to(\"cuda\")\n```\n\n### Expected behavior\n\nNo Error",
    "comments": [
      {
        "user": "ZhiyuanChen",
        "body": "@ArthurZucker Any thoughts? "
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @ZhiyuanChen, it seems like you want the SinusoidalEmbedding class to have temporary weights that are not saved. Have you tried using [register_buffer(persistent=False)](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer)? If you do that, our methods should hopefully stop being confused about what to do with those parameters"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38634,
    "title": "Download models from a private hub in 2025",
    "author": "DanielSchuhmacher",
    "state": "closed",
    "created_at": "2025-06-06T08:00:23Z",
    "updated_at": "2025-06-13T13:45:13Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nIn the context of a private hub deployment, customers would like to use from_pretrained() to load models from their hub, not from the public hub. This doesn't seem to be configurable at the moment and it would be nice to add this feature.\n\nThe obvious workaround is to clone the repo first and then load it from local storage, but this adds an extra step. It'd be great to have the same experience regardless of where the hub is hosted.\n\nThis issue was raised before here: https://github.com/huggingface/transformers/issues/15514\n@juliensimon\n\n### Motivation\n\nnone\n\n### Your contribution\n\nnone",
    "comments": [
      {
        "user": "hanouticelina",
        "body": "Hi @DanielSchuhmacher,\nit's already possible to configure `from_pretrained()` to work with a private hub deployment by setting the `HF_ENDPOINT` environment variable to your hub's URL. This allows `transformers` to fetch models from your private instance instead of the public hf.co.\n\n```bash\nexport HF_ENDPOINT=https://your-private-hub.com\n\n```\nor in python\n```python\nimport os\n\nos.environ[\"HF_ENDPOINT\"] = \"https://your-private-hub.com\"\n\nfrom transformers import AutoModel, AutoTokenizer\n\n\nmodel = AutoModel.from_pretrained(\"my-org/my-private-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"my-org/my-private-model\")\n\n```\n\nI hope this helps,  let us know if you have any further questions!\n"
      },
      {
        "user": "DanielSchuhmacher",
        "body": "Thank you @hanouticelina. Works as described!"
      }
    ]
  },
  {
    "issue_number": 38186,
    "title": "GPT-2 Embedding Dropout Implementation",
    "author": "d-kleine",
    "state": "open",
    "created_at": "2025-05-17T13:21:10Z",
    "updated_at": "2025-06-13T11:44:33Z",
    "labels": [],
    "body": "Hi @ArthurZucker and the HF team,\n\nI've been exploring the transformers library implementation of GPT-2 and noticed something interesting regarding the embedding dropout mechanism. I wanted to share an observation that might be worth considering.\n\nIn reviewing the code at:\n\nhttps://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/src/transformers/models/gpt2/modeling_gpt2.py#L704\n\nhttps://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/src/transformers/models/gpt2/modeling_gpt2.py#L902-L906\n\nI noticed that the current implementation appears to use standard dropout on the embedding outputs, which zeroes out individual elements within embedding vectors. Based on the literature about embedding dropout techniques, my understanding is that embedding dropout typically zeroes out entire word vectors rather than individual elements.\n\nThere's a helpful visual explanation of this approach here: https://riven314.github.io/2020/08/03/AWD_LSTM.html\n\nSources for reference:\n* [Embedding Dropout paper](https://paperswithcode.com/method/embedding-dropout)\n* [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n\nI'd be interested to hear your thoughts on this approach or if there were specific design considerations for the current implementation. Thank you for all your work maintaining this incredible library!",
    "comments": [
      {
        "user": "AmbiTyga",
        "body": "Hi @d-kleine \nI made something similar while reviewing the paper. \n`EmbeddingLayerWithDropout`: https://github.com/AmbiTyga/Research-Bookmark/blob/main/Embedding%20Dropout/module.py\n\nWould like to push this to `transformers`. But before that, should we try training it over WebText, pretraining GPT2 again with this layer or finetune it with domain specific dataset? \nI would like to know what should we be monitoring, as well as should we do some probing within layers to understand its effect?"
      },
      {
        "user": "d-kleine",
        "body": "I had some time to delve into this issue today. From my understanding, it seems like there is a difference between *embedding dropout* and *word/token dropout*. I think the current element-wise dropout implemention is correct because the embeddings are calculated in GPT-2 as the sum of token and position. Otherwise, if using *word/token dropout*, you would need to randomly zero out the token embeddings, but add the positional information then. This would disrupt the sum of embeddings, creating incomplete representations lacking semantic content, which would be bad for the self-attention mechanism thereafter in a transformer-based model."
      },
      {
        "user": "d-kleine",
        "body": "Hi @ArthurZucker, just to confirm: am I correct in my assessment that the current element-wise dropout implementation is indeed correct for GPT-2's embedding calculations? I could not find a clear statement/instruction how the dropout is properly done there. If this is correct, I'd be happy to close the issue. 🙂 "
      }
    ]
  },
  {
    "issue_number": 21335,
    "title": "TypeError: _forward_unimplemented() got an unexpected keyword argument 'input_ids'",
    "author": "QuantumStatic",
    "state": "closed",
    "created_at": "2023-01-27T11:12:29Z",
    "updated_at": "2025-06-13T11:17:14Z",
    "labels": [],
    "body": "### System Info\n\n- `transformers` version: 4.24.0\r\n- Platform: Windows-10-10.0.19044-SP0\r\n- Python version: 3.10.8\r\n- Huggingface_hub version: 0.11.0\r\n- PyTorch version (GPU?): 1.13.0+cu117 (True)\n\n### Who can help?\n\n@ArthurZucker and @younesbelkada since I am using `distilbert-base-uncased`<br>(and maybe @sgugger, since I am following this [link](https://huggingface.co/transformers/v3.2.0/custom_datasets.html)) on the hugging face website\n\n### Information\n\n- [x] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nI am using a custom dataset to fine tune `distilbert-base-uncased`. I followed the method described [on the hugging face wesbite](https://huggingface.co/transformers/v3.2.0/custom_datasets.html) to the T. Here is my code for making the dataset.\r\n```python\r\ndef create_hugging_face_dataset(data:dict):\r\n    train_text, test_text, train_label, test_label = train_test_split(data['text'], data['label'], test_size=0.1, shuffle=True)\r\n    train_text, validation_text, train_label, validation_label = train_test_split(train_text, train_label, test_size=0.1, shuffle=True)\r\n\r\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\r\n\r\n    train_encodings = tokenizer(train_text, truncation=True, padding=True)\r\n    test_encodings = tokenizer(test_text, truncation=True, padding=True)\r\n    validation_encodings = tokenizer(validation_text, truncation=True, padding=True)\r\n\r\n    class MBICDataset(torch.utils.data.Dataset):\r\n        def __init__(self, encodings, labels):\r\n            self.encodings = encodings\r\n            self.labels = labels\r\n\r\n        def __getitem__(self, idx):\r\n            item = {key: torch.Tensor(val[idx]) for key, val in self.encodings.items()}\r\n            item['labels'] = torch.Tensor(self.labels[idx])\r\n            return item\r\n\r\n        def __len__(self):\r\n            return len(self.labels)\r\n\r\n    train_ds = MBICDataset(train_encodings, train_label)\r\n    test_ds = MBICDataset(test_encodings, test_label)\r\n    validation_ds = MBICDataset(validation_encodings, validation_label)\r\n\r\n\r\n    FINAL_DS = {\"train\":train_ds, \"test\":test_ds, \"validation\":validation_ds}\r\n\r\n```\r\n\r\nAfter making the dataset I try to fine-tune the model using the following code.\r\n```python\r\n\r\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\r\n\r\ntraining_stuff = {\r\n    \"batch_size\": 64, \r\n    \"epochs\": 4, \r\n    \"learning_rate\": 1e-5,\r\n    \"weight_decay\": 0.01\r\n    }\r\n\r\ntraining_args = TrainingArguments(\r\n            output_dir=\"C:/Users/uujain2/Desktop/Utkarsh/FYP/Models/DistilBert\",\r\n            per_device_train_batch_size=training_stuff[\"batch_size\"],\r\n            evaluation_strategy=\"steps\",\r\n            num_train_epochs=training_stuff[\"epochs\"],\r\n            fp16=True,\r\n            save_steps=100,\r\n            eval_steps=50,\r\n            logging_steps=10,\r\n            weight_decay=training_stuff[\"weight_decay\"],\r\n            learning_rate=training_stuff[\"learning_rate\"],\r\n            save_total_limit=64,\r\n            remove_unused_columns=False,\r\n            push_to_hub=False,\r\n            report_to='tensorboard',\r\n            load_best_model_at_end=True,\r\n        )\r\n\r\n\r\nmodel = DistilBertPreTrainedModel.from_pretrained(\r\n    'distilbert-base-uncased',\r\n    num_labels=3,\r\n    id2label={0: 'Biased', 1: 'Non-biased', 2: 'No agreeemnt'},\r\n    label2id={'Biased': 0, 'Non-biased': 1, 'No agreement': 2},\r\n    )\r\n\r\ntrainer = Trainer(\r\n            model=model,\r\n            args=training_args,\r\n            train_dataset=FINAL_DS['train'],\r\n            eval_dataset=FINAL_DS['validation'],\r\n            tokenizer=tokenizer,\r\n        )\r\n\r\ntrain_results = trainer.train()\r\n```\r\n\r\nHowever, I run into the following error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\uujain2\\Desktop\\Utkarsh\\FYP\\Code\\test.py\", line 68, in <module>\r\n    train_results = trainer.train()\r\n  File \"C:\\Users\\uujain2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 1501, in train\r\n    return inner_training_loop(\r\n  File \"C:\\Users\\uujain2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 1749, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"C:\\Users\\uujain2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 2508, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"C:\\Users\\uujain2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 2540, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"C:\\Users\\uujain2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1190, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\nTypeError: _forward_unimplemented() got an unexpected keyword argument 'input_ids'\r\n```\n\n### Expected behavior\n\nI expect the model to start the finetuning process instead of throwing this error.",
    "comments": [
      {
        "user": "sgugger",
        "body": "`DistilBertPreTrainedModel` is an abstract class and shouldn't be used directly. Maybe you wanted to use `DistilBertModel` or `DistilBertForPretraining`?"
      },
      {
        "user": "QuantumStatic",
        "body": "Thank you for your quick response. It was my silly mistake to use an abstract class for pre-training. I was able to import `DistilBertModel`, however the import for `DistilBertForPretraining` failed, but that's alright.\r\n\r\nHowever when I try to run the model now I get the following error.\r\n\r\n```\r\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\r\n```\r\n\r\nI have followed the webpage titled [Fine-tuning with custom datasets](https://huggingface.co/transformers/v3.4.0/custom_datasets.html). My function that creates the initial lists with texts and labels is below. The data is formatted very similarly to the webpage:\r\n```python\r\ndef create_MBIC_data_dict() -> dict[str, str]:\r\n    data_dict = {'text': [], 'label':[]}\r\n    with open(f\"{DATA_FOLDER_PATH}/final_labels_MBIC_new.csv\") as csv_file:\r\n        csv_reader = csv.reader(csv_file)\r\n        line_count = 0\r\n        for row in csv_reader:\r\n            if line_count != 0:\r\n                data_dict['text'].append(row[0])\r\n                label_val = -1\r\n                match row[7]:\r\n                    case \"Biased\":\r\n                        label_val = 1\r\n                    case \"Non-biased\":\r\n                        label_val = 0\r\n                    case \"No agreement\":\r\n                        label_val = 2\r\n                data_dict['label'].append(label_val)\r\n            line_count += 1\r\n\r\n    return data_dict\r\n```\r\nAfterwards the `create_hugging_face_dataset` function executes which creates the dataset. \r\n\r\n@sgugger "
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38803,
    "title": "DTensor issues when running Llama4ForConditionalGeneration with tensor parallel.",
    "author": "czkkkkkk",
    "state": "open",
    "created_at": "2025-06-12T22:34:24Z",
    "updated_at": "2025-06-13T11:14:28Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntransformers version: 4.52.4\npytorch version: 2.6\n\n### Who can help?\n\ntransformers version: 4.52.4\npytorch version: 2.6\n\nWhen running Llama4 with tensor parallel, [torch.nn.Unfold used in llama4 ](https://github.com/huggingface/transformers/blob/v4.52.4/src/transformers/models/llama4/modeling_llama4.py#L1320) isn't compatible with DTensor. So I got this error:\n`NotImplementedError: Operator aten.im2col.default does not have a sharding strategy registered.` Looks like it is because the latest transformers use replicate DTensor for layers without tp_plan but `Unfold` isn't compatible with DTensor.\n\nTo workaround this error, I manually changed the input tensor to regular Tensor.\n```\ndevice_mesh = hidden_states.device_mesh if isinstance(hidden_states, DTensor) else None\nplacements = hidden_states.placements if isinstance(hidden_states, DTensor) else None\nhidden_states = hidden_states.to_local()\nhidden_states = self.unfold(hidden_states)\nhidden_states = DTensor.from_local(hidden_states, device_mesh, placements)\n```\n\nAfter the change, I got `AttributeError: 'BaseModelOutput' object has no attribute 'to_local'` when running `vision_model` https://github.com/huggingface/transformers/blob/v4.52.4/src/transformers/models/llama4/modeling_llama4.py#L1543.\n\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nMinimal script to reproduce the error\n```py\n# test.py\nimport torch\nfrom transformers.models.llama4.modeling_llama4 import Llama4ForConditionalGeneration\n\nif __name__ == '__main__':\n    model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n    model = Llama4ForConditionalGeneration.from_pretrained(\n        model_id,\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True,\n        device_map=\"auto\",\n    )\n    B = 1\n    S = 128\n    input_ids = torch.randint(0, 1000, (B, S))\n    attention_mask = torch.ones((B, S))\n    pixel_values = torch.randn((5, 3, 336, 336)).to(torch.bfloat16)\n    model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n```\n\n```sh\ntorchrun --nproc-per-node=8 test.py\n```\n\n### Expected behavior\n\nExpect the program to finish successfully.",
    "comments": [
      {
        "user": "marcndo",
        "body": "Hi @czkkkkkk, Are you already working on this? If not, I'd be happy to take it."
      },
      {
        "user": "czkkkkkk",
        "body": "Hi @marcndo, I am not working on it. Would be great if you can help resolve this issue. Thanks!"
      },
      {
        "user": "Rocketknight1",
        "body": "`Unfold` not being compatible with `DTensor` is something we should definitely be wary of cc @cyrilvallez"
      }
    ]
  },
  {
    "issue_number": 38517,
    "title": "model_type = self._reverse_config_mapping[key.__name__] KeyError: 'Qwen2RMConfig'",
    "author": "Lelege0",
    "state": "open",
    "created_at": "2025-06-01T17:30:57Z",
    "updated_at": "2025-06-13T10:56:24Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nFile \"/anaconda/envs/openrlhf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 767, in __getitem__\n    model_type = self._reverse_config_mapping[key.__name__]\nKeyError: 'Qwen2RMConfig'\n\n\ntransformers version is 4.51.3\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nwe use the lastest version of openrlhf、vllm、transformers\n\n### Expected behavior\n\nCan you merge QwenRMconfig into the lastest version of transformers\n",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @Lelege0, can you paste some simple code that we can run to see the cause of this issue?"
      },
      {
        "user": "JiruWang",
        "body": "same problem, I commented the trust_remote_code=True temporarily. I'm using transformers==4.52.4"
      }
    ]
  },
  {
    "issue_number": 38787,
    "title": "Qwen3 Incorrect order of sliding window layers",
    "author": "norpadon",
    "state": "closed",
    "created_at": "2025-06-12T11:57:17Z",
    "updated_at": "2025-06-13T08:46:46Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nIrrelevant\n\n### Who can help?\n\n@ArthurZucker \n\nDocumentation [says](https://github.com/huggingface/transformers/blob/89c46b648d82b670cc7286a25fa64d2d92770418/src/transformers/models/qwen3/configuration_qwen3.py#L115C9-L116C134):\n\n\"The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\"\n\nBut the code [does the opposite thing](https://github.com/huggingface/transformers/blob/89c46b648d82b670cc7286a25fa64d2d92770418/src/transformers/models/qwen3/configuration_qwen3.py#L210C9-L216C14):\n```python\nif self.layer_types is None:\n    self.layer_types = [\n        \"sliding_attention\"\n        if self.sliding_window is not None and i >= self.max_window_layers\n        else \"full_attention\"\n        for i in range(self.num_hidden_layers)\n    ]\n``` \n\nWAT? Which one is correct?\n\n### Information\n\n- [x] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nLook at the code, look at the documentation, look at the code again, have existential crisis\n\n### Expected behavior\n\nNo existential crisis",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @norpadon, you're correct! In these cases, the documentation is almost always wrong (because a bug in the code would be a lot more noticeable), but cc @cyrilvallez who touched the attention code recently to confirm"
      },
      {
        "user": "Cyrilvallez",
        "body": "Ha yes I noticed it when refactoring, but forgot to change it! Would you like to open a PR to clarify? 🤗"
      },
      {
        "user": "norpadon",
        "body": "https://github.com/huggingface/transformers/pull/38794"
      }
    ]
  },
  {
    "issue_number": 37954,
    "title": "jinja2.exceptions.UndefinedError: 'list object' has no attribute 'startswith'",
    "author": "lucasjinreal",
    "state": "closed",
    "created_at": "2025-05-05T10:38:37Z",
    "updated_at": "2025-06-13T08:03:00Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nlinux transforemres 4.52.dev9\n\n### Who can help?\n\ntext = self.processor.apply_chat_template(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/processing_utils.py\", line 1534, in apply_chat_template\n    prompt = self.tokenizer.apply_chat_template(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 1700, in apply_chat_template\n    rendered_chat = compiled_template.render(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/environment.py\", line 1301, in render\n    self.environment.handle_exception()\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/environment.py\", line 936, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"<template>\", line 20, in top-level template code\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/sandbox.py\", line 391, in call\n    if not __self.is_safe_callable(__obj):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/sandbox.py\", line 275, in is_safe_callable\n    getattr(obj, \"unsafe_callable\", False) or getattr(obj, \"alters_data\", False)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\njinja2.exceptions.UndefinedError: 'list object' has no attribute 'startswith'\n\n### Information\n\n- [x] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\ntext = self.processor.apply_chat_template(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/processing_utils.py\", line 1534, in apply_chat_template\n    prompt = self.tokenizer.apply_chat_template(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 1700, in apply_chat_template\n    rendered_chat = compiled_template.render(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/environment.py\", line 1301, in render\n    self.environment.handle_exception()\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/environment.py\", line 936, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"<template>\", line 20, in top-level template code\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/sandbox.py\", line 391, in call\n    if not __self.is_safe_callable(__obj):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/sandbox.py\", line 275, in is_safe_callable\n    getattr(obj, \"unsafe_callable\", False) or getattr(obj, \"alters_data\", False)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\njinja2.exceptions.UndefinedError: 'list object' has no attribute 'startswith'\n\n### Expected behavior\n\ntext = self.processor.apply_chat_template(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/processing_utils.py\", line 1534, in apply_chat_template\n    prompt = self.tokenizer.apply_chat_template(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 1700, in apply_chat_template\n    rendered_chat = compiled_template.render(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/environment.py\", line 1301, in render\n    self.environment.handle_exception()\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/environment.py\", line 936, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"<template>\", line 20, in top-level template code\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/sandbox.py\", line 391, in call\n    if not __self.is_safe_callable(__obj):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/jinja2/sandbox.py\", line 275, in is_safe_callable\n    getattr(obj, \"unsafe_callable\", False) or getattr(obj, \"alters_data\", False)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\njinja2.exceptions.UndefinedError: 'list object' has no attribute 'startswith'\n\n\n## How to reveal\n\nHappend when inference a finetune Qwen3 model\n\nThe default Qwen3 has not `checkpoint-1000/chat_template.jinja`\n\nBut this jinja applied fail.\n\nCopy original json works fine.",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "@lucasjinreal can you provide a small reproducer? If you are using processor's `apply_chat_template` thus a multimodal model, the inputs have to be formatted differently from language models in most cases. That might be the issue but we can help only after checking the model's template and input conversation"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38119,
    "title": "Unable to quantize a pretrained SegFormer-B0 to int8 using Quanto",
    "author": "shubham-beri",
    "state": "open",
    "created_at": "2025-05-14T05:27:27Z",
    "updated_at": "2025-06-13T08:02:47Z",
    "labels": [],
    "body": "Does it support Conv2d? Here's my complete script:\n```\nimport os\nimport argparse\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\n\nfrom transformers import SegformerConfig, SegformerForSemanticSegmentation\nfrom optimum.quanto import quantize, qint8, Calibration, freeze\n\n\nclass PngCalibrationDataset(Dataset):\n    \"\"\"Loads all .png files in a directory as single-channel tensors.\"\"\"\n    def __init__(self, image_dir: str, transform=None):\n        self.image_paths = [\n            os.path.join(image_dir, fname)\n            for fname in sorted(os.listdir(image_dir))\n            if fname.lower().endswith(\".png\")\n        ]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        path = self.image_paths[idx]\n        img = Image.open(path).convert(\"L\")  # grayscale\n        if self.transform:\n            img = self.transform(img)\n        return img\n\n\ndef build_calib_loader(image_dir: str, batch_size: int):\n    \"\"\"Creates a DataLoader for calibration images.\"\"\"\n    transform = transforms.Compose([\n        transforms.Resize((288, 512)),  # (height, width)\n        transforms.ToTensor(),          # → shape [1, 288, 512]\n        # If your training used normalized inputs, uncomment and set mean/std:\n        # transforms.Normalize(mean=[0.5], std=[0.5]),\n    ])\n    dataset = PngCalibrationDataset(image_dir, transform=transform)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n\ndef load_and_prepare_model(\n    checkpoint_path: str,\n    num_input_channels: int = 1,\n    num_labels: int = 2\n) -> torch.nn.Module:\n    \"\"\"Loads a SegFormer model and populates it with your pretrained weights.\"\"\"\n    # Base config & model\n    config = SegformerConfig.from_pretrained(\n        \"nvidia/segformer-b0-finetuned-ade-512-512\"\n    )\n    config.num_channels = num_input_channels\n    config.num_labels   = num_labels\n    model = SegformerForSemanticSegmentation(config)\n\n    # Load checkpoint\n    ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n    state_dict = ckpt.get(\"state_dict\", ckpt.get(\"model\", ckpt))\n    cleaned = {}\n    # for k, v in state_dict.items():\n    #     name = k.replace(\"module.\", \"\").replace(\"model.\", \"\")\n    #     if name in model.state_dict() and v.size() == model.state_dict()[name].size():\n    #         cleaned[name] = v\n    model.load_state_dict(state_dict, strict=False)\n    return model\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Quantize SegFormer to int8 (weights+activations) with calibration\"\n    )\n    parser.add_argument(\n        \"--repdata\", \"-r\", type=str, required=True,\n        help=\"Directory of PNGs for calibration\"\n    )\n    parser.add_argument(\n        \"--checkpoint\", \"-c\", type=str, default=\"best_model_SegFormerB0.pth\",\n        help=\"Path to your pretrained .pth checkpoint\"\n    )\n    parser.add_argument(\n        \"--batch_size\", \"-b\", type=int, default=1,\n        help=\"Batch size for calibration loader\"\n    )\n    args = parser.parse_args()\n\n    # 1) Device setup\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # 2) Build calibration DataLoader\n    calib_loader = build_calib_loader(args.repdata, batch_size=args.batch_size)\n\n    # 3) Load model & weights\n    model = load_and_prepare_model(args.checkpoint)\n    model.to(device)\n    model.eval()\n\n    # 4) Insert dynamic quantization (weights + activation stubs)\n    quantize(model, weights=qint8, activations=qint8)\n\n    # 5) Calibrate activation ranges\n    print(\"Calibrating activation ranges...\")\n    with Calibration(momentum=0.9):\n        batch_count = 0\n        for batch in calib_loader:\n            print(f\"Calibrating batch {batch_count + 1}/{len(calib_loader)}\")\n            # batch: [B, 1, 288, 512]\n            batch = batch.to(device)\n            _ = model(batch)\n            batch_count += 1\n\n    # 6) Freeze quantized weights into actual int8 tensors\n    freeze(model)\n    from torch.nn.quantized import Linear, Conv2d\n    # 7) Verify parameter dtypes\n    print(\"Parameter dtypes after quantization & freezing:\")\n    for name, mod in model.named_modules():\n        if isinstance(mod, (Linear, Conv2d)):\n            # packed int-8 weights live here\n            print(f\"{name:55s} → {mod.weight().dtype}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38494,
    "title": "ImportError: cannot import name 'DTensor' from 'torch.distributed.tensor'",
    "author": "toby-clark4",
    "state": "closed",
    "created_at": "2025-05-30T14:36:59Z",
    "updated_at": "2025-06-13T02:26:07Z",
    "labels": [
      "bug"
    ],
    "body": "Using transformers 4.52.4 with PyTorch 2.4.0, I get this error when saving a model. Looking at pytorch_utils.py, I think it has recently been updated to support torch 2.5 by changing from torch.distributed._tensor to torch.distributed.tensor, but hasn't added handling of older torch versions.\n\nI think this should be a quick fix (changing to _tensor in source code works) but I think it's gone through as a silent change to all 4.52 versions - transformers 4.51 seems to work.\n\n### System Info\n- `transformers` version: 4.52.4\n- Platform: Linux-5.15.0-210.163.7.el8uek.x86_64-x86_64-with-glibc2.35\n- Python version: 3.12.9\n- Huggingface_hub version: 0.32.3\n- Safetensors version: 0.5.3\n- Accelerate version: 1.7.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.4.0 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: No\n- Using GPU in script?: Yes\n- GPU type: NVIDIA A100-SXM4-80GB\n\n### Who can help?\n\n@SunMarc @zach-huggingface\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nRunning model.save_pretrained on a ModernBert model created with ModernBertForMaskedLM.\n\n### Expected behavior\n\nModel saves without issue.",
    "comments": [
      {
        "user": "SunMarc",
        "body": "Sorry for that ! https://github.com/huggingface/transformers/issues/38494 should fix the issue "
      },
      {
        "user": "toby-clark4",
        "body": "> Sorry for that ! [#38494](https://github.com/huggingface/transformers/issues/38494) should fix the issue\n\nGreat, thanks!"
      },
      {
        "user": "kevalmorabia97",
        "body": "Would this fix be included in transformers 4.52.5? Or do we have to wait till 4.53 release?"
      }
    ]
  },
  {
    "issue_number": 38740,
    "title": "[DOCS] Add `pruna` as optimization framework",
    "author": "davidberenstein1957",
    "state": "open",
    "created_at": "2025-06-11T04:52:33Z",
    "updated_at": "2025-06-12T18:32:47Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nHave a section on Pruna AI within the documentation. We did [a similar PR for diffusers](https://github.com/huggingface/diffusers/pull/11688) and thought it would be nice to show how to optimize transformers models too. \n.\n\n### Motivation\n\nHave a section on Pruna AI within the documentation to show how to optimize LLMs for inference.\n\n### Your contribution\n\nWe could do everything for the PR.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @stevhliu "
      },
      {
        "user": "davidberenstein1957",
        "body": "@Rocketknight1 @stevhliu, do you feel it makes sense to add a similar section to the transformers documentation too? "
      },
      {
        "user": "stevhliu",
        "body": "Hmm I'm not really sure.\n\nMost of the docs we include that come from other packages show how they _integrate_ with Transformers (for example, all the different quantization backends). With Pruna, it seems like a wrapper over an existing Transformer model (from what I've seen on the Diffusers PR) and so not as directly related to using Transformers itself if that makes sense.\n\nWhat do you think @huggingface/transformers-core-maintainers?"
      }
    ]
  },
  {
    "issue_number": 33401,
    "title": "[BUG] Latest version cannot load Qwen2-VL model config correctly.",
    "author": "fyabc",
    "state": "closed",
    "created_at": "2024-09-10T07:15:33Z",
    "updated_at": "2025-06-12T17:38:08Z",
    "labels": [
      "Should Fix",
      "bug",
      "Vision",
      "Multimodal"
    ],
    "body": "### System Info\r\n\r\n- `transformers` version: 4.45.0.dev0\r\n- Platform: Linux-5.10.134-16.101.al8.x86_64-x86_64-with-glibc2.35\r\n- Python version: 3.10.14\r\n- Huggingface_hub version: 0.23.4\r\n- Safetensors version: 0.4.3\r\n- Accelerate version: 0.32.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.4.0+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA L20Y\r\n\r\n### Who can help?\r\n\r\n@amyeroberts @qubvel \r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Download the `config.json` from [Qwen2-VL-7B-Instruct HF main repo](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/blob/main/config.json) to `/tmp/Qwen2-VL-7B-Instruct/config.json`.\r\n  * The downloaded config file content should be:\r\n\r\n```json\r\n{\r\n  \"architectures\": [\r\n    \"Qwen2VLForConditionalGeneration\"\r\n  ],\r\n  \"attention_dropout\": 0.0,\r\n  \"bos_token_id\": 151643,\r\n  \"eos_token_id\": 151645,\r\n  \"vision_start_token_id\": 151652,\r\n  \"vision_end_token_id\": 151653,\r\n  \"vision_token_id\": 151654,\r\n  \"image_token_id\": 151655,\r\n  \"video_token_id\": 151656,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 3584,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 18944,\r\n  \"max_position_embeddings\": 32768,\r\n  \"max_window_layers\": 28,\r\n  \"model_type\": \"qwen2_vl\",\r\n  \"num_attention_heads\": 28,\r\n  \"num_hidden_layers\": 28,\r\n  \"num_key_value_heads\": 4,\r\n  \"rms_norm_eps\": 1e-06,\r\n  \"rope_theta\": 1000000.0,\r\n  \"sliding_window\": 32768,\r\n  \"tie_word_embeddings\": false,\r\n  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"4.41.2\",\r\n  \"use_cache\": true,\r\n  \"use_sliding_window\": false,\r\n  \"vision_config\": {\r\n    \"depth\": 32,\r\n    \"embed_dim\": 1280,\r\n    \"mlp_ratio\": 4,\r\n    \"num_heads\": 16,\r\n    \"in_chans\": 3,\r\n    \"hidden_size\": 3584,\r\n    \"patch_size\": 14,\r\n    \"spatial_merge_size\": 2,\r\n    \"spatial_patch_size\": 14,\r\n    \"temporal_patch_size\": 2\r\n  },\r\n  \"rope_scaling\": {\r\n    \"type\": \"mrope\",\r\n    \"mrope_section\": [\r\n      16,\r\n      24,\r\n      24\r\n    ]\r\n  },\r\n  \"vocab_size\": 152064\r\n}\r\n```\r\n\r\n2. Install the latest `transformers` version via `pip install git+https://github.com/huggingface/transformers@main`\r\n3. Run the following script:\r\n\r\n```python\r\nfrom transformers import AutoConfig\r\nconfig = AutoConfig.from_pretrained('/tmp/Qwen2-VL-7B-Instruct/')\r\nprint(config)\r\n```\r\n\r\n4. The result is:\r\n\r\n```text\r\nUnrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\r\n\r\nQwen2VLConfig {\r\n  \"_name_or_path\": \"/tmp/Qwen2-VL-7B-Instruct/\",\r\n  \"architectures\": [\r\n    \"Qwen2VLForConditionalGeneration\"\r\n  ],\r\n  \"attention_dropout\": 0.0,\r\n  \"bos_token_id\": 151643,\r\n  \"eos_token_id\": 151645,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 3584,\r\n  \"image_token_id\": 151655,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 18944,\r\n  \"max_position_embeddings\": 32768,\r\n  \"max_window_layers\": 28,\r\n  \"model_type\": \"qwen2_vl\",\r\n  \"num_attention_heads\": 28,\r\n  \"num_hidden_layers\": 28,\r\n  \"num_key_value_heads\": 4,\r\n  \"rms_norm_eps\": 1e-06,\r\n  \"rope_scaling\": {\r\n    \"mrope_section\": [\r\n      16,\r\n      24,\r\n      24\r\n    ],\r\n    \"rope_type\": \"default\",\r\n    \"type\": \"default\"\r\n  },\r\n  \"rope_theta\": 1000000.0,\r\n  \"sliding_window\": 32768,\r\n  \"tie_word_embeddings\": false,\r\n  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"4.45.0.dev0\",\r\n  \"use_cache\": true,\r\n  \"use_sliding_window\": false,\r\n  \"video_token_id\": 151656,\r\n  \"vision_config\": {\r\n    \"in_chans\": 3,\r\n    \"model_type\": \"qwen2_vl\",\r\n    \"spatial_patch_size\": 14\r\n  },\r\n  \"vision_end_token_id\": 151653,\r\n  \"vision_start_token_id\": 151652,\r\n  \"vision_token_id\": 151654,\r\n  \"vocab_size\": 152064\r\n}\r\n```\r\n\r\nIt prints a warning message, and the output `rope_scaling.type` and `rope_scaling.rope_type` are set to `default`, but **`mrope` is expected**. \r\n\r\n### Expected behavior\r\n\r\nThis bug seems to be introduced in a recent version of transformers.\r\nWhen I switch to a [old version](https://github.com/huggingface/transformers/tree/21fac7abba2a37fae86106f87fcf9974fd1e3830) by `git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830`, the output is correct:\r\n\r\n```text\r\nQwen2VLConfig {\r\n  \"_name_or_path\": \"/tmp/Qwen2-VL-7B-Instruct/\",\r\n  \"architectures\": [\r\n    \"Qwen2VLForConditionalGeneration\"\r\n  ],\r\n  \"attention_dropout\": 0.0,\r\n  \"bos_token_id\": 151643,\r\n  \"eos_token_id\": 151645,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 3584,\r\n  \"image_token_id\": 151655,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 18944,\r\n  \"max_position_embeddings\": 32768,\r\n  \"max_window_layers\": 28,\r\n  \"model_type\": \"qwen2_vl\",\r\n  \"num_attention_heads\": 28,\r\n  \"num_hidden_layers\": 28,\r\n  \"num_key_value_heads\": 4,\r\n  \"rms_norm_eps\": 1e-06,\r\n  \"rope_scaling\": {\r\n    \"mrope_section\": [\r\n      16,\r\n      24,\r\n      24\r\n    ],\r\n    \"type\": \"mrope\"\r\n  },\r\n  \"rope_theta\": 1000000.0,\r\n  \"sliding_window\": 32768,\r\n  \"tie_word_embeddings\": false,\r\n  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"4.45.0.dev0\",\r\n  \"use_cache\": true,\r\n  \"use_sliding_window\": false,\r\n  \"video_token_id\": 151656,\r\n  \"vision_config\": {\r\n    \"in_chans\": 3,\r\n    \"model_type\": \"qwen2_vl\",\r\n    \"spatial_patch_size\": 14\r\n  },\r\n  \"vision_end_token_id\": 151653,\r\n  \"vision_start_token_id\": 151652,\r\n  \"vision_token_id\": 151654,\r\n  \"vocab_size\": 152064\r\n}\r\n```",
    "comments": [
      {
        "user": "wangaocheng",
        "body": "yes the same error"
      },
      {
        "user": "LysandreJik",
        "body": "cc @zucchini-nlp as well I believe"
      },
      {
        "user": "zucchini-nlp",
        "body": "Hey! Yes, the warning is currently misleading as the RoPE implementation was recently standardized and Qwen2-VL has a quite different rope-scaling dict compared to other models. Yet, the generation quality shouldn't be affected by that, as per my last interaction with the model everything was same as before standardization\r\n\r\ncc @gante as well, as you're working on uniform-RoPE, this might be something we want to fix  "
      }
    ]
  },
  {
    "issue_number": 37405,
    "title": "Unable to load OLMo2 models with newer transformers versions",
    "author": "AkshitaB",
    "state": "closed",
    "created_at": "2025-04-09T21:45:36Z",
    "updated_at": "2025-06-12T15:52:58Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nFails on transformers versions: `4.50.3, 4.50.1, 4.50.0`.\nWorks on 4.49.0 .\n\nFails with ```raise ValueError(\nValueError: Unrecognized model in allenai/OLMo-2-1124-7B. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth```\n\nNote that `olmo2` is present in the above list.\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```from transformers import AutoConfig\nconfig = AutoConfig.from_pretrained(\"allenai/OLMo-2-1124-7B\")```\n\n### Expected behavior\n\nConfig should be found and loaded",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @AkshitaB, that line works for me after installing the latest version of `transformers`. Can you check on other systems or Colab? It might be some kind of environment issue on your machine.\n\nClosing for now, but feel free to reopen if you think there's a real bug here and you can make a reproducer that works on other systems!"
      },
      {
        "user": "jhrystrom",
        "body": "EDIT: Nevermind, the problem was some complicated environment thing - nothing to do with the specific package version.\n\nI have the same issue with 4.52.4 "
      }
    ]
  },
  {
    "issue_number": 38739,
    "title": "LlamaAttention forward function type hint is incorrect",
    "author": "nhatkhtn",
    "state": "open",
    "created_at": "2025-06-10T18:17:48Z",
    "updated_at": "2025-06-12T15:41:40Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nFor the current version (4.52.4), in the `LlamaAttention` class, the type hint for the forward function\n\nhttps://github.com/huggingface/transformers/blob/aa798b7ac9ff5018b3578eb927dc438671ab6a3e/src/transformers/models/llama/modeling_llama.py#L231\n\nand what it actually returns\n\nhttps://github.com/huggingface/transformers/blob/aa798b7ac9ff5018b3578eb927dc438671ab6a3e/src/transformers/models/llama/modeling_llama.py#L264\n\nmismatch. Looking at the git blame, this is probably the result of the attention refactor introduced in #35235. \n\n### Who can help?\n\n@ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nLook at the code\n\n### Expected behavior\n\nThe type hint should be correct to avoid confusion",
    "comments": [
      {
        "user": "ArkVex",
        "body": "Hello! I noticed a small mismatch in the return type of the LlamaAttention forward method in Transformers version 4.52.4. It does not match what is actually being returned. This might have happened during the recent attention refactor, possibly in pull request . I just wanted to point it out. I would be happy to open a quick pull request to fix it if that helps. Thank you for all the great work you are doing!\n"
      },
      {
        "user": "ArkVex",
        "body": "@nhatkhtn ?\n"
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @nhatkhtn, yes, this looks like an incorrect type hint. Would you be willing to make a PR to fix it?"
      }
    ]
  },
  {
    "issue_number": 38765,
    "title": "Add FastVLM from CVPR 2025",
    "author": "kamila-chay",
    "state": "open",
    "created_at": "2025-06-11T17:21:03Z",
    "updated_at": "2025-06-12T14:37:55Z",
    "labels": [
      "New model"
    ],
    "body": "### Model description\n\nApple recently released FastVLM, a new vision-language model introduced at CVPR 2025, which significantly improves on previous models in the LLaVA family.\n\nThe smallest FastVLM variant outperforms LLaVA-OneVision-0.5B, achieving:\n\n- 85× faster Time-to-First-Token (TTFT)\n- 3.4× smaller vision encoder\n\nIts ultra-low latency enables real-time deployment on mobile and edge devices, as demonstrated in the official demo. Given its strong performance and efficiency, I believe FastVLM would be a valuable addition to this repo.\n\nIf you also think so, I'd love to contribute this model.\n\n### Open source status\n\n- [x] The model implementation is available\n- [x] The model weights are available\n\n### Provide useful links for the implementation\n\nPaper: https://www.arxiv.org/abs/2412.13303\nOfficial repo: https://github.com/apple/ml-fastvlm",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "Hey @kamila-chay !\n\nYes, adding the model is already planned afaik. @ariG23498 had a nice inference script using transformers under [this comment](https://github.com/apple/ml-fastvlm/issues/1#issuecomment-2883616294). To support the model in core library we need to convert weights, and we're waiting for Apple team on that I think"
      },
      {
        "user": "kamila-chay",
        "body": "Hi @zucchini-nlp! Thank you for the answer, I didn't see the thread. Glad to hear it's already planned. You can mention me here if you need community support for this model in the future (to add new components etc, not quite sure what will be needed)"
      }
    ]
  },
  {
    "issue_number": 37824,
    "title": "Support for B200 (`sm_100` with `pytorch>=2.7.0`)",
    "author": "dominiquegarmier",
    "state": "closed",
    "created_at": "2025-04-28T09:11:40Z",
    "updated_at": "2025-06-12T14:34:15Z",
    "labels": [
      "Feature request"
    ],
    "body": "Is there already an open issue addressing/discussing https://github.com/huggingface/transformers/pull/37760 (I could not find any)? Looks like there is an issue open on the pytorch repo https://github.com/pytorch/pytorch/issues/152275\n\nFeel free to close if this is a duplicate.\n",
    "comments": [
      {
        "user": "anijain2305",
        "body": "@gante Is it possible to try out this PR  from your side? \n\nhttps://github.com/pytorch/pytorch/pull/152287\n\nOne way to try this would be - get torch 2.7 and then manually apply the changes to the relevant files. I provided more details on why the slowdown here - https://github.com/pytorch/pytorch/issues/152275. In my local testing, the latency regression was mitigated. But, we would need HF team to confirm. \n\n"
      },
      {
        "user": "gante",
        "body": "@anijain2305 As replied in https://github.com/pytorch/pytorch/pull/152287 -- I can confirm this `torch` PR solves the slowdown issue. After we have a torch release with this change, we can unpin `transformers` ✅ \n\nMeanwhile @dominiquegarmier, if you're not using `torch.compile` with `gemma3`-like models, you can manually install `torch>=2.7.0` after installing `transformers`. The `torch<2.7.0` pin was added to prevent speed regressions with a popular model :)"
      },
      {
        "user": "dominiquegarmier",
        "body": "Thanks, we have a workaround atm where we compile multiple libaries from source.\n\nI mainly opened the issue because many of our libraries we use depend in some way on transformers. Basically, I think many other libraries are holding out on upgrading until transformers does."
      }
    ]
  },
  {
    "issue_number": 38753,
    "title": "Silent Overwrite of Custom Optimizer When Using DeepSpeed with Transformers Trainer",
    "author": "VladPyzh",
    "state": "open",
    "created_at": "2025-06-11T12:50:26Z",
    "updated_at": "2025-06-12T13:53:58Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.49.0\n- Platform: Linux-5.4.0-216-generic-x86_64-with-glibc2.31\n- Python version: 3.13.2\n- Huggingface_hub version: 0.29.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.7.0\n- Accelerate config:    not found\n- DeepSpeed version: 0.16.8\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: No\n- Using GPU in script?: Yes\n- GPU type: Tesla V100-PCIE-32GB\n\n### Who can help?\n\n@zach-huggingface @SunMarc\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nTo reproduce the problem:\n\n1. Pass some DeepSpeed config file to the TrainingArguments\n2. Pass custom optimizer to the Trainer\n\nExample:\n\n```\nclass CustomOptim(torch.optim.Adam):\n    def step(self, *args, **kwargs):\n        print(\"steped!\")\n        super().step(*args, **kwargs)\n\nmodel_name = \"facebook/opt-125m\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\ndataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\ntrain_ds = dataset[\"train\"]\neval_ds = dataset[\"validation\"]\n\ndef tokenize(examples):\n    return tokenizer(\n        examples[\"text\"], return_special_tokens_mask=True, truncation=True\n    )\n\ntrain_ds = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\neval_ds = eval_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./opt-125m-finetuned\",\n    do_eval=False,\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    max_steps=1,\n    push_to_hub=False,\n    deepspeed=\"/data/vladislav/ds_config.json\",\n)\n\noptimizer = CustomOptim(model.parameters())\nscheduler = get_constant_schedule(optimizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    optimizers=(optimizer, scheduler),\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n```\nDeepSpeed config:\n```\n{\n  \"train_batch_size\": 32,\n  \"gradient_accumulation_steps\": 1,\n  \"gradient_clipping\": 1.0,\n\n  \"zero_optimization\": {\n    \"stage\": 0,\n    \"contiguous_gradients\": true,\n    \"overlap_comm\": true,\n    \"reduce_bucket_size\": 50000000,\n    \"allgather_bucket_size\": 500000000\n  }\n  \n}\n```\n\nIn this example CustomOptimizer will be overwritten and no print would appear. \n\n### Expected behavior\n\nHi HuggingFace team,\n\nWhile working with the `transformers.Trainer` to train a LLM, I encountered an unexpected behavior when combining a custom optimizer with a DeepSpeed config in TrainingArguments.\n\nIt seems that when a custom optimizer is passed to the Trainer, but a DeepSpeed config is also provided, the custom optimizer is silently ignored and overwritten by DeepSpeed. This was quite surprising, and from what I can tell, this behavior is not mentioned in the documentation for either Trainer or TrainingArguments.\n\nTo avoid confusion and help users catch this early, it would be helpful if an explicit warning or exception were raised in such scenarios.\n\nThanks for the great work on the library!",
    "comments": [
      {
        "user": "SunMarc",
        "body": "Hey @VladPyzh, nice to see you here ! \n\n> It seems that when a custom optimizer is passed to the Trainer, but a DeepSpeed config is also provided, the custom optimizer is silently ignored and overwritten by DeepSpeed. This was quite surprising, and from what I can tell, this behavior is not mentioned in the documentation for either Trainer or TrainingArguments.\n\nThis shouldn't happen, if you check `deepspeed_init` and `deepspeed_optim_sched`, we create the DS optimizer if you added it in the ds_config, otherwise, your custom optimizer should be used. "
      },
      {
        "user": "VladPyzh",
        "body": "Good to see you too @SunMarc and thanks for super rapid response.\n\nAs I see `deepspeed_init` will update `trainer.optimizer` and `trainer.scheduler`. Inside of the `deepspeed_init` there is a ` trainer.optimizer = None` and new initialization of optimizer using `trainer.create_optimizer()` inside of `deepspeed_optim_sched` in case when there is no optimizer in ds_config. It seems like this function doesn't initialize custom optimizers and rely on `TrainingArguments` to get the type of optimizer. \n\nMaybe I miss something, does my example work for your correctly? \n\n"
      }
    ]
  },
  {
    "issue_number": 38726,
    "title": "Issue importing models in jupyter notebooks 'No module named transformers.models.ipynb_checkpoints'",
    "author": "mchaudrycupa",
    "state": "open",
    "created_at": "2025-06-10T11:10:41Z",
    "updated_at": "2025-06-12T13:45:33Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nThe following error comes up: ModuleNotFoundError: No module named transformers.models.ipynb_checkpoints'\n\nPackages:\nipykernel==6.29.5\n- `transformers` version: 4.52.4\n- Platform: Linux-5.10.226-214.880.amzn2.x86_64-x86_64-with-glibc2.39\n- Python version: 3.10.16\n- Huggingface_hub version: 0.31.4\n- Safetensors version: 0.5.3\n- Accelerate version: 1.7.0\n- Accelerate config: \tnot found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: Tesla T4\n\nI'm running the code in jupyter labs in a jupyter notebook.\n\nThe code I'm running is as follows:\n\n` from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-4B', padding_side='left')\nmodel = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-4B') `\n\nPlease let me know if there's any other information that'd be useful.\n\nThe whole error that comes up is:\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nFile [~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2045](https://j07z6gj046cgcjqn.ml-3cebfa69-400.cdp-prd.yss1-43mz.a2.cloudera.site/lab/tree/test/generic_notebooks/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py#line=2044), in _LazyModule.__getattr__(self, name)\n   2044 try:\n-> 2045     module = self._get_module(self._class_to_module[name])\n   2046     value = getattr(module, name)\n\nFile [~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2075](https://j07z6gj046cgcjqn.ml-3cebfa69-400.cdp-prd.yss1-43mz.a2.cloudera.site/lab/tree/test/generic_notebooks/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py#line=2074), in _LazyModule._get_module(self, module_name)\n   2074 except Exception as e:\n-> 2075     raise e\n\nFile [~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2073](https://j07z6gj046cgcjqn.ml-3cebfa69-400.cdp-prd.yss1-43mz.a2.cloudera.site/lab/tree/test/generic_notebooks/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py#line=2072), in _LazyModule._get_module(self, module_name)\n   2072 try:\n-> 2073     return importlib.import_module(\".\" + module_name, self.__name__)\n   2074 except Exception as e:\n\nFile /usr/local/lib/python3.10/importlib/__init__.py:126, in import_module(name, package)\n    125         level += 1\n--> 126 return _bootstrap._gcd_import(name[level:], package, level)\n\nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name, package, level)\n\nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name, import_)\n\nFile <frozen importlib._bootstrap>:992, in _find_and_load_unlocked(name, import_)\n\nFile <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f, *args, **kwds)\n\nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name, package, level)\n\nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name, import_)\n\nFile <frozen importlib._bootstrap>:1004, in _find_and_load_unlocked(name, import_)\n\nModuleNotFoundError: No module named 'transformers.models.ipynb_checkpoints'\n\nThe above exception was the direct cause of the following exception:\n\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[2], line 2\n      1 tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-4B', padding_side='left')\n----> 2 model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-4B')\n      3                                  #  ,\n      4                                  #  quantization_config = bnb_config,\n      5                                  # device_map = \"auto\")\n      6 \n      7 # We recommend enabling flash_attention_2 for better acceleration and memory saving.\n      8 # model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-8B', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\n\nFile [~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:568](https://j07z6gj046cgcjqn.ml-3cebfa69-400.cdp-prd.yss1-43mz.a2.cloudera.site/lab/tree/test/generic_notebooks/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py#line=567), in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n    564     return model_class.from_pretrained(\n    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n    566     )\n    567 elif type(config) in cls._model_mapping.keys():\n--> 568     model_class = _get_model_class(config, cls._model_mapping)\n    569     if model_class.config_class == config.sub_configs.get(\"text_config\", None):\n    570         config = config.get_text_config()\n\nFile [~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:388](https://j07z6gj046cgcjqn.ml-3cebfa69-400.cdp-prd.yss1-43mz.a2.cloudera.site/lab/tree/test/generic_notebooks/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py#line=387), in _get_model_class(config, model_mapping)\n    387 def _get_model_class(config, model_mapping):\n--> 388     supported_models = model_mapping[type(config)]\n    389     if not isinstance(supported_models, (list, tuple)):\n    390         return supported_models\n\nFile [~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:774](https://j07z6gj046cgcjqn.ml-3cebfa69-400.cdp-prd.yss1-43mz.a2.cloudera.site/lab/tree/test/generic_notebooks/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py#line=773), in _LazyAutoMapping.__getitem__(self, key)\n    772 if model_type in self._model_mapping:\n    773     model_name = self._model_mapping[model_type]\n--> 774     return self._load_attr_from_module(model_type, model_name)\n    776 # Maybe there was several model types associated with this config.\n    777 model_types = [k for k, v in self._config_mapping.items() if v == key.__name__]\n\nFile [~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:788](https://j07z6gj046cgcjqn.ml-3cebfa69-400.cdp-prd.yss1-43mz.a2.cloudera.site/lab/tree/test/generic_notebooks/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py#line=787), in _LazyAutoMapping._load_attr_from_module(self, model_type, attr)\n    786 if module_name not in self._modules:\n    787     self._modules[module_name] = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n--> 788 return getattribute_from_module(self._modules[module_name], attr)\n\nFile [~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:700](https://j07z6gj046cgcjqn.ml-3cebfa69-400.cdp-prd.yss1-43mz.a2.cloudera.site/lab/tree/test/generic_notebooks/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py#line=699), in getattribute_from_module(module, attr)\n    698 if isinstance(attr, tuple):\n    699     return tuple(getattribute_from_module(module, a) for a in attr)\n--> 700 if hasattr(module, attr):\n    701     return getattr(module, attr)\n    702 # Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\n    703 # object at the top level.\n\nFile [~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2048](https://j07z6gj046cgcjqn.ml-3cebfa69-400.cdp-prd.yss1-43mz.a2.cloudera.site/lab/tree/test/generic_notebooks/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py#line=2047), in _LazyModule.__getattr__(self, name)\n   2046         value = getattr(module, name)\n   2047     except (ModuleNotFoundError, RuntimeError) as e:\n-> 2048         raise ModuleNotFoundError(\n   2049             f\"Could not import module '{name}'. Are this object's requirements defined correctly?\"\n   2050         ) from e\n   2052 elif name in self._modules:\n   2053     try:\n\nModuleNotFoundError: Could not import module 'Qwen3Model'. Are this object's requirements defined correctly? \n\n### Who can help?\n\n@ArthurZucker This may be relevant to you, apologies if not.\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-4B', padding_side='left')\nmodel = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-4B') \n\n### Expected behavior\n\nThat the model is imported correctly",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @mchaudrycupa, it seems like the error is `ModuleNotFoundError: Could not import module 'Qwen3Model'. Are this object's requirements defined correctly?`\n\nDo you get the same error on other models?"
      },
      {
        "user": "mchaudrycupa",
        "body": "> Hi [@mchaudrycupa](https://github.com/mchaudrycupa), it seems like the error is `ModuleNotFoundError: Could not import module 'Qwen3Model'. Are this object's requirements defined correctly?`\n> \n> Do you get the same error on other models?\n\n"
      },
      {
        "user": "mchaudrycupa",
        "body": "@Rocketknight1 Apologies I accidentally closed that comment - I've tried with qwen 3 embeddings 4 and 8B, and base qwen 3 8b and none of those work, however it seems to be working with 'Alibaba-NLP/gte-Qwen2-7B-instruct', I'll also try some other non-qwen models."
      }
    ]
  },
  {
    "issue_number": 38078,
    "title": "autoawq has been deprecated. Is it possible to support the use of llm-compresser as an alternative to autoawq",
    "author": "0O0OwO0O0",
    "state": "closed",
    "created_at": "2025-05-12T08:45:53Z",
    "updated_at": "2025-06-12T12:04:36Z",
    "labels": [],
    "body": "autoawq has been deprecated. Is it possible to support the use of llm-compresser as an alternative to autoawq, or can another alternative be provided\n</br></br>\nfrom [Autoawq](https://github.com/casper-hansen/AutoAWQ)\n```markdown\nImportant Notice:\n- AutoAWQ is officially deprecated and will no longer be maintained.\n- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n\nAlternative:\n- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n- MLX-LM now supports AWQ for Mac devices: http://github.com/ml-explore/mlx-lm\n\nFor further inquiries, feel free to reach out:\n- X: https://x.com/casper_hansen_\n- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n```",
    "comments": [
      {
        "user": "sreeragdas",
        "body": "can i work on this?"
      },
      {
        "user": "Rocketknight1",
        "body": "cc @sunmarc @MekkCyber "
      },
      {
        "user": "SunMarc",
        "body": "@0O0OwO0O0, auto-awq is not maintained anymore meaning that there will be no official support for newer models when it comes to quantizing the model. \nYou can still install `auto-awq` library to run inference with transformers on awq quantized models. As we only support inference with auto-awq, there is nothing that needs to be changed. "
      }
    ]
  },
  {
    "issue_number": 37971,
    "title": "Add `pruna` integration for loading model through `transformers.from_pretrained` / `pipeline`.",
    "author": "davidberenstein1957",
    "state": "open",
    "created_at": "2025-05-06T09:51:57Z",
    "updated_at": "2025-06-12T10:25:48Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\n[Pruna](https://github.com/PrunaAI/pruna) is an open source AI model Optimization framework. \nAs discussed with @SunMarc, it would be nice to load Pruna optimized models through the `transformers.from_pretrained` interface as an alternative to the [PrunaModel](https://docs.pruna.ai/en/stable/docs_pruna/user_manual/save_load.html#saving-and-loading-models) interface.\n\nCurrently, the code looks as follows.\n```python\nfrom pruna import PrunaModel\n\nloaded_model = PrunaModel.from_hub(\"PrunaAI/opt-125m-smashed\")\n```\n\nWe could go for something like. \n\n```python\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"PrunaAI/opt-125m-smashed\")\n\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"PrunaAI/opt-125m-smashed\")\nmodel = AutoModelForCausalLM.from_pretrained(\"PrunaAI/opt-125m-smashed\")\n```\n\n### Motivation\n\nIt would be a nice integration.\n\n### Your contribution\n\nWe can submit a PR from our side. \n",
    "comments": [
      {
        "user": "Tanuj-rai",
        "body": "Hey, @davidberenstein888, can I work on this issue? "
      },
      {
        "user": "SunMarc",
        "body": "I think it will be best that the PR comes from their team @Tanuj-rai !"
      },
      {
        "user": "davidberenstein1957",
        "body": "@SunMarc, could you give us some pointers on how to best tackle this? Are there other PRs you know of that we can use as guideline. Do you feel it makes sense to directly add Pruna to the `from_pretrained` flow, or should it be more like an integration where we could start with something like the \"Use this model\" option https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF?library=llama-cpp-python?"
      }
    ]
  },
  {
    "issue_number": 37637,
    "title": "Processor multiprocessing error when load custom processor",
    "author": "Kuangdd01",
    "state": "closed",
    "created_at": "2025-04-20T09:13:54Z",
    "updated_at": "2025-06-12T08:02:55Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.51.1\n- Platform: Linux-5.4.0-144-generic-x86_64-with-glibc2.31\n- Python version: 3.10.0\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: 0.16.5\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA A800 80GB PCIe\n\n### Who can help?\n\n@zucchini-nlp  @ArthurZucker\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nI made a minimized sample to replicate the problem.\n```python\nfrom datasets import Dataset\nfrom transformers import AutoProcessor\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\n\n# fake dataset\nds = [\n    (\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\", \"What animal is on the candy?\"),\n    (\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\", \"What animal is on the candy?\"),\n    (\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\", \"What animal is on the candy?\"),\n    (\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\", \"What animal is on the candy?\"),\n]\n\npaths, texts = zip(*ds)\ndataset = Dataset.from_dict({\n    \"image_path\": list(paths),\n    \"text\": list(texts)\n})\n\n# load kimi_vl processor\n# fails here :[\nprocessor = AutoProcessor.from_pretrained(\"moonshotai/Kimi-VL-A3B-Thinking\", trust_remote_code=True)\n\n# works nicely for official implementation\n# processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", trust_remote_code=True)\n\nclass Test:\n    def __init__(self, processor):\n        self.processor = processor\n\n    def preprocess(self, example):\n        path = example[\"image_path\"]\n        if path.startswith(\"http://\") or path.startswith(\"https://\"):\n            resp = requests.get(path, timeout=10)\n            resp.raise_for_status()\n            image = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n        else:\n            image = Image.open(path).convert(\"RGB\")\n        outputs = self.processor(images=image, text=example[\"text\"], return_tensors=\"pt\")\n        return {\"out\": outputs}\n\nt = Test(processor)\nprocessed = dataset.map(\n    t.preprocess,\n    remove_columns=[\"image_path\", \"text\"],\n    num_proc=4, # kimi processor fails when num_proc > 1 \n    batched=False,\n)\nprint(processed[0]) # show case\n\n```\nIf we use a custom processor like `KimiVLProcessor`, the following error will be raised:\n```\nRuntimeError                              Traceback (most recent call last)\n[<ipython-input-12-791f4b3f5e14>](https://localhost:8080/#) in <cell line: 0>()\n     41 \n     42 t = Test(processor)\n---> 43 processed = dataset.map(\n     44     t.preprocess,\n     45     remove_columns=[\"image_path\", \"text\"],\n\n2 frames\n[/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py](https://localhost:8080/#) in iflatmap_unordered(pool, func, kwargs_iterable)\n    711                     pool_changed = True\n    712                     # One of the subprocesses has died. We should not wait forever.\n--> 713                     raise RuntimeError(\n    714                         \"One of the subprocesses has abruptly died during map operation.\"\n    715                         \"To debug the error, disable multiprocessing.\"\n\nRuntimeError: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.\n```\n```\nProcess ForkPoolWorker-12:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 114, in worker\n    task = get()\n           ^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/queues.py\", line 370, in get\n    return _ForkingPickler.loads(res)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/dill/_dill.py\", line 303, in loads\n    return load(file, ignore, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/dill/_dill.py\", line 289, in load\n    return Unpickler(file, ignore=ignore, **kwds).load()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/dill/_dill.py\", line 444, in load\n    obj = StockUnpickler.load(self)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/dill/_dill.py\", line 593, in _create_type\n    return typeobj(*args)\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/typing.py\", line 2992, in __new__\n    raise TypeError('cannot inherit from both a TypedDict type '\nTypeError: cannot inherit from both a TypedDict type and a non-TypedDict base class\n```\nAfter I traced back the `typing.py`, I found it was caused by this arg: `KimiVLProcessorKwargs`, which failed in the typing check.\nI compared `processing_llava.py` and `processing_kimivl.py` but didn't find the main difference. 😢 \n\nWould anyone be able to help me with this?  If I've missed something, please point it out to me. 😃 \n\nRelated Issue https://github.com/hiyouga/LLaMA-Factory/issues/7763 \n\nI also want to know how we can register/insert our custom processor in the pipeline. \n\n**Any help is greatly appreciated!**\n\n### Expected behavior\n\nThe processor works nicely in a multiprocess pipeline. 😄 ",
    "comments": [
      {
        "user": "chenin-wang",
        "body": "@Kuangdd01  \n\nIt's not a bug, I've seen this before too.\n\neach process created by the map function cannot access globally declared variables since this would force the processes to share said variable (since Python basically only passes by ref, each process could theoretically change one variable). By passing them as fn_kwargs isntead.\n\n```python\nprocessed_shard_dataset = current_shard_dataset.map(\n            integrated_process_vision_features,\n            batched=True,\n            batch_size=8,\n            drop_last_batch=False,\n            num_proc=4,\n            with_rank=True,\n            fn_kwargs={\n                \"processor\": processor,\n                \"model\": model,\n            },\n            desc=\"pipeline\",\n        )\n`"
      },
      {
        "user": "Kuangdd01",
        "body": "> [@Kuangdd01](https://github.com/Kuangdd01)\n> \n> It's not a bug, I've seen this before too.\n> \n> each process created by the map function cannot access globally declared variables since this would force the processes to share said variable (since Python basically only passes by ref, each process could theoretically change one variable). By passing them as fn_kwargs isntead.\n> \n> processed_shard_dataset = current_shard_dataset.map(\n>             integrated_process_vision_features,\n>             batched=True,\n>             batch_size=8,\n>             drop_last_batch=False,\n>             num_proc=4,\n>             with_rank=True,\n>             fn_kwargs={\n>                 \"processor\": processor,\n>                 \"model\": model,\n>             },\n>             desc=\"pipeline\",\n>         )\n> `\n\nthx!"
      },
      {
        "user": "Kuangdd01",
        "body": "> [@Kuangdd01](https://github.com/Kuangdd01)\n> \n> It's not a bug, I've seen this before too.\n> \n> each process created by the map function cannot access globally declared variables since this would force the processes to share said variable (since Python basically only passes by ref, each process could theoretically change one variable). By passing them as fn_kwargs isntead.\n> \n> processed_shard_dataset = current_shard_dataset.map(\n>             integrated_process_vision_features,\n>             batched=True,\n>             batch_size=8,\n>             drop_last_batch=False,\n>             num_proc=4,\n>             with_rank=True,\n>             fn_kwargs={\n>                 \"processor\": processor,\n>                 \"model\": model,\n>             },\n>             desc=\"pipeline\",\n>         )\n> `\n\nSry for bothering you again, I just changed my code with the following.  It failed again. Where should I modify further?\n```python\nclass Test:\n    def __init__(self, processor):\n        self.processor = processor\n\n    def preprocess(self, example):\n        path = example[\"image_path\"]\n        if path.startswith(\"http://\") or path.startswith(\"https://\"):\n            resp = requests.get(path, timeout=10)\n            resp.raise_for_status()\n            image = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n        else:\n            image = Image.open(path).convert(\"RGB\")\n        outputs = self.processor(images=image, text=example[\"text\"], return_tensors=\"pt\")\n        return {\"out\": outputs}\n\ndef _preprocess_fn(example, tester: Test):\n    return tester.preprocess(example)\n\ntester = Test(processor)\n\nprocessed = dataset.map(\n    _preprocess_fn,\n    fn_kwargs={\"tester\": tester},  \n    remove_columns=[\"image_path\", \"text\"],\n    num_proc=4,\n    batched=False,\n)\n```"
      }
    ]
  },
  {
    "issue_number": 38000,
    "title": "Why can't InternVL3-8B start vLLM after being converted to the Hugging Face format? It shows the error: `ValueError: 'limit_mm_per_prompt' is only supported for multimodal models.'",
    "author": "FloSophorae",
    "state": "open",
    "created_at": "2025-05-07T13:29:37Z",
    "updated_at": "2025-06-12T08:02:39Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nvllm                                     0.8.5.post1\ntransformers                             4.52.0.dev0\n\n### Who can help?\n\n @amyeroberts \n@qubvel \n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n\n```\nCUDA_VISIBLE_DEVICES=0,1 vllm serve $MODEL_PATH \\\n    --tensor-parallel-size 2 \\\n    --port $MODEL_PROT \\\n    --host 0.0.0.0 \\\n    --dtype float16 \\\n    --max-model-len 65536 \\\n    --limit-mm-per-prompt image=30,video=0\\\n    --enable-prefix-caching \\\n    --gpu-memory-utilization 0.6 \\\n    --block-size 16 > \"$VLLM_LOG\"\n```\n\n### Expected behavior\n\nI downloaded the model from [OpenGVLab/InternVL3-8B](https://huggingface.co/OpenGVLab/InternVL3-8B), which natively supports running OpenAI-style chat completions with vLLM. However, after converting it to the Hugging Face format using the script `transformers/src/transformers/models/internvl/convert_internvl_weights_to_hf.py`, launching vLLM resulted in the error:\n\n`ValueError: 'limit_mm_per_prompt' is only supported for multimodal models.`\n\nThe command I used to launch vllm is as follows:\n```\n\nCUDA_VISIBLE_DEVICES=0,1 vllm serve $MODEL_PATH \\\n    --tensor-parallel-size 2 \\\n    --port $MODEL_PROT \\\n    --host 0.0.0.0 \\\n    --dtype float16 \\\n    --max-model-len 65536 \\\n    --limit-mm-per-prompt image=30,video=0\\\n    --enable-prefix-caching \\\n    --gpu-memory-utilization 0.6 \\\n    --block-size 16 > \"$VLLM_LOG\"\n```\nThe system runs correctly when I set MODEL_PATH to the original OpenGVLab/InternVL3-8B address. But throws an error when I change the path to the converted InternVL-3B-hf format : `ValueError: 'limit_mm_per_prompt' is only supported for multimodal models.`\n\nCould someone explain why this is happening and suggest solutions?\nThank you very much!",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "Hm, I don't think the converted weights would be compatible with vLLM yet. Usually vLLM implements the multimodal models themselves using the official checkpoints. Support for transformers style multimodal model is planned already and we'll try to add it soon"
      },
      {
        "user": "FloSophorae",
        "body": "Hello, thank you for your response. Could you please advise if there’s any way to convert a Hugging Face (HF) format model back to its original format? My training task only generated the model in HF format. Looking forward to your reply!"
      },
      {
        "user": "zucchini-nlp",
        "body": "Ah, you are using a custom tuned model. I don't think we have an easy way to do reverse conversion, unless you can write your own conversion script\n\nThought I will cc @hmellor as well, not sure if there's way to run converted models in vLLM without using transformers backend"
      }
    ]
  },
  {
    "issue_number": 26413,
    "title": "`resume_from_checkpoint` function fails because \"There seems to be not a single sample in your epoch_iterator\"",
    "author": "omermazig",
    "state": "closed",
    "created_at": "2023-09-26T10:35:33Z",
    "updated_at": "2025-06-12T07:40:52Z",
    "labels": [
      "trainer"
    ],
    "body": "### System Info\n\ntransformers version - 4.33.2\r\n\r\nI'm using the trainer api as such, so it pushes the latest checkpoint to huggingface each epoch:\r\n\r\n```\r\nfrom transformers import TrainingArguments, Trainer\r\n\r\nnew_model_name = \"videomae-finetuned\"\r\nnum_epochs = 50\r\nbatch_size = 8\r\nsteps_per_epoch = train_dataset.num_videos // batch_size\r\n\r\nargs = TrainingArguments(\r\n    output_dir=new_model_name,\r\n    remove_unused_columns=False,\r\n    evaluation_strategy=\"epoch\",\r\n    save_strategy=\"epoch\",\r\n    save_total_limit = 2, # Only last 2 models are saved. Older ones are deleted.\r\n    learning_rate=5e-5,\r\n    per_device_train_batch_size=batch_size,\r\n    per_device_eval_batch_size=batch_size,\r\n    warmup_ratio=0.1,\r\n    logging_steps=10,\r\n    max_steps=steps_per_epoch * num_epochs, # Duplication of `num_train_epochs` because it throws otherwise.\r\n    load_best_model_at_end=True,\r\n    metric_for_best_model=\"accuracy\",\r\n    hub_strategy=\"checkpoint\",\r\n    push_to_hub=True,\r\n    num_train_epochs=num_epochs,\r\n)\r\n```\r\n\r\n```\r\nfrom transformers import EarlyStoppingCallback\r\n\r\ntrainer = Trainer(\r\n    model,\r\n    args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=val_dataset,\r\n    tokenizer=image_processor,\r\n    compute_metrics=compute_metrics,\r\n    data_collator=collate_fn,\r\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0.01)]\r\n)\r\n```\r\n\r\n```\r\nimport traceback\r\n\r\ntry:\r\n    results = trainer.train()\r\nexcept RuntimeError as e:\r\n    print(traceback.format_exc())\r\n```\r\n\r\nAnd after about 25 epochs there's some exception (never mind what). So I get the last checkpoint being saved to huggingface (from [here](https://huggingface.co/omermazig/videomae-finetuned-nba-5-class-8-batch-2000-vid-multiclass/tree/main/last-checkpoint), if it matters) and put it on my drive, change the training code to this:\r\n\r\n```\r\nimport traceback\r\n\r\ntry:\r\n    results = trainer.train(resume_from_checkpoint=pathlib.Path(f\"./drive/MyDrive/\").joinpath(\"last-checkpoint\"))\r\nexcept RuntimeError as e:\r\n    print(traceback.format_exc())\r\n```\r\n\r\nAnd rerun the whole notebook. Than, it prints (after some time - not immidiatlly):\r\n\r\n> There seems to be not a single sample in your epoch_iterator, stopping training at step 5500! This is expected if you're using an IterableDataset and set num_steps (12500) higher than the number of available samples.\r\n\r\nAnd than fails.\r\n\r\nI do have an `IterableDataset` with 2000 training videos, and I'm using batch size 8 and want to run for 50 epochs, so I'm pretty sure 12500 is (2000/8)*50, but I still don't understand the message. Why is it problematic that num_steps (12500) > number of samples (2000)?\r\n\r\n\r\nThank you!\n\n### Who can help?\n\n@muellerzr\r\n@pacman100\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nCan't really for my code, but it is based on [your guide](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb) and I believe will reproduce for this as well.\n\n### Expected behavior\n\nContinuing the training from the same state it stopped before.",
    "comments": [
      {
        "user": "omermazig",
        "body": "Update - I added `ignore_data_skip=True` to `TrainingArguments`, and it was succesfull in running a single epoch, and then failed with:\r\n\r\n> ValueError: 'videomae-finetuned/checkpoint-3000' is not in list\r\n\r\n\r\nCheckpoint 3000 is my best checkpoint (according to my `metric_for_best_model`), so I'm assuming that I have to have both the last checkpoint AND the best checkpoint available in the output dir, for this to work? If so, the documentation for [hub_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.hub_strategy) is mistaken, because it stated:\r\n\r\n> \"checkpoint\": like \"every_save\" but the latest checkpoint is also pushed in a subfolder named last-checkpoint, allowing you to resume training easily with trainer.train(resume_from_checkpoint=\"last-checkpoint\").\r\n\r\nWhich is wrong.\r\n\r\n\r\nAm I missing something?"
      },
      {
        "user": "LysandreJik",
        "body": "cc @pacman100 @muellerzr "
      },
      {
        "user": "K-Niu",
        "body": "Similar question: for the sake of reproducibility, I would like to be able to resume training from the same batch where I left off in my `IterableDataset` (so I don't want to set `ignore_data_skip=True`). However, it appears that the training loop relies on the `train_dataloader` [length](https://github.com/huggingface/transformers/blob/v4.32.1/src/transformers/trainer.py#L1578) to compute information needed in the resumption logic.\r\n\r\nIs there anyway to achieve this behavior? Thanks!"
      }
    ]
  },
  {
    "issue_number": 38727,
    "title": "KV Cache Bug in Iterative generation",
    "author": "Greek-Guardian",
    "state": "closed",
    "created_at": "2025-06-10T11:44:11Z",
    "updated_at": "2025-06-12T02:04:43Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nAccording to the code in [here](https://huggingface.co/docs/transformers/kv_cache#iterative-generation), i can use such code for Iterative generation. How ever, this happened:\n`---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[4], [line 28](vscode-notebook-cell:?execution_count=4&line=28)\n     [25](vscode-notebook-cell:?execution_count=4&line=25) # if isinstance(past_key_values, SinkCache):\n     [26](vscode-notebook-cell:?execution_count=4&line=26) #     inputs = {k: v[:, -max_cache_length:] for k, v in inputs.items()}\n     [27](vscode-notebook-cell:?execution_count=4&line=27) input_length = inputs[\"input_ids\"].shape[1]\n---> [28](vscode-notebook-cell:?execution_count=4&line=28) outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)\n     [29](vscode-notebook-cell:?execution_count=4&line=29) completion = tokenizer.decode(outputs[0, input_length: ], skip_special_tokens=True)\n     [30](vscode-notebook-cell:?execution_count=4&line=30) messages.append({\"role\": \"assistant\", \"content\": completion})\n\nFile /opt/conda/envs/python3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)\n    [113](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:113) @functools.wraps(func)\n    [114](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:114) def decorate_context(*args, **kwargs):\n    [115](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115)     with ctx_factory():\n--> [116](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116)         return func(*args, **kwargs)\n\nFile /opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2597, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\n   [2589](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2589)     input_ids, model_kwargs = self._expand_inputs_for_generation(\n   [2590](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2590)         input_ids=input_ids,\n   [2591](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2591)         expand_size=generation_config.num_return_sequences,\n   [2592](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2592)         is_encoder_decoder=self.config.is_encoder_decoder,\n   [2593](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2593)         **model_kwargs,\n   [2594](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2594)     )\n   [2596](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2596)     # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n-> [2597](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2597)     result = self._sample(\n   [2598](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2598)         input_ids,\n   [2599](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2599)         logits_processor=prepared_logits_processor,\n   [2600](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2600)         stopping_criteria=prepared_stopping_criteria,\n   [2601](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2601)         generation_config=generation_config,\n   [2602](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2602)         synced_gpus=synced_gpus,\n   [2603](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2603)         streamer=streamer,\n   [2604](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2604)         **model_kwargs,\n   [2605](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2605)     )\n   [2607](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2607) elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n   [2608](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2608)     # 11. interleave input_ids with `num_beams` additional sequences per batch\n   [2609](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2609)     input_ids, model_kwargs = self._expand_inputs_for_generation(\n   [2610](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2610)         input_ids=input_ids,\n   [2611](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2611)         expand_size=generation_config.num_beams,\n   [2612](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2612)         is_encoder_decoder=self.config.is_encoder_decoder,\n   [2613](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2613)         **model_kwargs,\n   [2614](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2614)     )\n\nFile /opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3550, in GenerationMixin._sample(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\n   [3546](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3546)     is_prefill = True\n   [3548](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3548) while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n   [3549](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3549)     # prepare model inputs\n-> [3550](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3550)     model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n   [3552](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3552)     # prepare variable output controls (note: some models won't accept all output controls)\n   [3553](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3553)     model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n\nFile /opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:580, in GenerationMixin.prepare_inputs_for_generation(self, input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs)\n    [578](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:578) if past_key_values is not None:\n    [579](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:579)     model_inputs[\"past_key_values\"] = past_key_values\n--> [580](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:580)     inputs_embeds, input_ids = self._cache_dependant_input_preparation(\n    [581](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:581)         input_ids, inputs_embeds, cache_position\n    [582](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:582)     )\n    [584](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:584) # 3. Prepare base model inputs\n    [585](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:585) input_ids_key = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n\nFile /opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:479, in GenerationMixin._cache_dependant_input_preparation(self, input_ids, inputs_embeds, cache_position)\n    [475](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:475) if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n    [476](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:476)     inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n    [477](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:477) elif (\n    [478](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:478)     inputs_embeds is not None  # Exception 1\n--> [479](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:479)     or (cache_position[-1] >= input_ids.shape[1])  # Exception 3\n    [480](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:480) ):\n    [481](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:481)     input_ids = input_ids[:, -cache_position.shape[0] :]\n    [482](https://vscode-remote+aop-002dlab-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/opt/conda/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:482) elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n\nIndexError: index -1 is out of bounds for dimension 0 with size 0`\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\ntransformers==4.52.4\nthe model is Qwen3\n\n### Expected behavior\n\nI expect it to use kv cache as normal",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @gante @zucchini-nlp I can't reproduce the exact error but I do notice the code snippet is using `get_max_length` which no longer exists. Maybe we should do a pass on the codebase to make sure we're not referencing those deleted functions anywhere anymore?"
      },
      {
        "user": "Greek-Guardian",
        "body": "> cc [@gante](https://github.com/gante) [@zucchini-nlp](https://github.com/zucchini-nlp) I can't reproduce the exact error but I do notice the code snippet is using `get_max_length` which no longer exists. Maybe we should do a pass on the codebase to make sure we're not referencing those deleted functions anywhere anymore?\n\nPlease try this code, i set the model to Qwen3:\n```python\nimport torch\nfrom transformers import AutoTokenizer,AutoModelForCausalLM\nfrom transformers.cache_utils import (\n    DynamicCache,\n    SinkCache,\n    StaticCache,\n    SlidingWindowCache,\n    QuantoQuantizedCache,\n    QuantizedCacheConfig,\n)\n\nmodel_id = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nuser_prompts = [\"Hello, what's your name?\", \"Btw, yesterday I was on a rock concert.\"]\n\npast_key_values = DynamicCache()\n# max_cache_length = past_key_values.get_max_length()\n\nmessages = []\nfor prompt in user_prompts:\n    messages.append({\"role\": \"user\", \"content\": prompt})\n    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n    # if isinstance(past_key_values, SinkCache):\n    #     inputs = {k: v[:, -max_cache_length:] for k, v in inputs.items()}\n    input_length = inputs[\"input_ids\"].shape[1]\n    outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)\n    completion = tokenizer.decode(outputs[0, input_length: ], skip_special_tokens=True)\n    messages.append({\"role\": \"assistant\", \"content\": completion})\n```\n"
      },
      {
        "user": "zucchini-nlp",
        "body": "@Greek-Guardian in this particular case the issue is in the chat template. The model's template removes all the text in between `<think>...</think>` tokens, even though they are provided in the input messages. Using a different model like `model_id = \"Qwen/Qwen1.5-0.5B-Chat\"` works fine\n\nI guess in this case you would have to manually crop `thinking` part before adding it to messages, because some models need special treatment\n\nI will update the docs to remove `SinkCache`, it is already moved from the core library to the hub after a long deprecation cycle.  "
      }
    ]
  },
  {
    "issue_number": 38745,
    "title": "[Bug][InformerForPredict] The shape will cause a problem",
    "author": "2004learner",
    "state": "open",
    "created_at": "2025-06-11T07:22:06Z",
    "updated_at": "2025-06-12T01:23:51Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nWhen I set the infomerconfig.input_size = 1, I find a bug, but I don't know how to fix it.\n\n- Function Name : `create_network_inputs`\n```\ntime_feat = (\n            torch.cat(\n                (\n                    past_time_features[:, self._past_length - self.config.context_length :, ...],\n                    future_time_features,\n                ),\n                dim=1,\n            )\n            if future_values is not None\n            else past_time_features[:, self._past_length - self.config.context_length :, ...]\n        )\n\n        print(self._past_length)\n        # target\n        if past_observed_mask is None:\n            past_observed_mask = torch.ones_like(past_values)\n\n        context = past_values[:, -self.config.context_length :]\n        observed_context = past_observed_mask[:, -self.config.context_length :]\n        _, loc, scale = self.scaler(context, observed_context)\n\n        inputs = (\n            (torch.cat((past_values, future_values), dim=1) - loc) / scale\n            if future_values is not None\n            else (past_values - loc) / scale\n        )\n        print(loc.shape, scale.shape, inputs.shape)\n\n        # static features\n        log_abs_loc = loc.abs().log1p() if self.config.input_size == 1 else loc.squeeze(1).abs().log1p()\n        log_scale = scale.log() if self.config.input_size == 1 else scale.squeeze(1).log()\n        print(f\"log_abs_loc: {log_abs_loc.shape}, {log_scale.shape}\")\n        print(time_feat.shape, self.config.input_size)\n        static_feat = torch.cat((log_abs_loc, log_scale), dim=1)\n        print(time_feat.shape, static_feat.shape)\n        if static_real_features is not None:\n            static_feat = torch.cat((static_real_features, static_feat), dim=1)\n        if static_categorical_features is not None:\n            embedded_cat = self.embedder(static_categorical_features)\n            static_feat = torch.cat((embedded_cat, static_feat), dim=1)\n        print(time_feat.shape, static_feat.shape)\n        expanded_static_feat = static_feat.unsqueeze(1).expand(-1, time_feat.shape[1], -1)\n\n        # all features\n        features = torch.cat((expanded_static_feat, time_feat), dim=-1)\n\n        # lagged features\n        subsequences_length = (\n            self.config.context_length + self.config.prediction_length\n            if future_values is not None\n            else self.config.context_length\n        )\n        lagged_sequence = self.get_lagged_subsequences(sequence=inputs, subsequences_length=subsequences_length)\n        lags_shape = lagged_sequence.shape\n        reshaped_lagged_sequence = lagged_sequence.reshape(lags_shape[0], lags_shape[1], -1)\n\n        if reshaped_lagged_sequence.shape[1] != time_feat.shape[1]:\n            raise ValueError(\n                f\"input length {reshaped_lagged_sequence.shape[1]} and time feature lengths {time_feat.shape[1]} does not match\"\n            )\n\n        # transformer inputs\n        transformer_inputs = torch.cat((reshaped_lagged_sequence, features), dim=-1)\n\n        return transformer_inputs, loc, scale, static_feat\n```\n\nAs we can see, I add some `print` sentence in the library to see the shape, now the bug is:\n```\nTraceback (most recent call last):\n  File \"/home/wjt/luck/FinalWork/alert_models/informer_based_model_3_cpu.py\", line 820, in <module>\n    pipline.train_model()\n  File \"/home/wjt/luck/FinalWork/alert_models/informer_based_model_3_cpu.py\", line 466, in train_model\n    outputs = model(\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/transformers/models/informer/modeling_informer.py\", line 1844, in forward\n    outputs = self.model(\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/transformers/models/informer/modeling_informer.py\", line 1568, in forward\n    transformer_inputs, loc, scale, static_feat = self.create_network_inputs(\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/transformers/models/informer/modeling_informer.py\", line 1386, in create_network_inputs\n    expanded_static_feat = static_feat.unsqueeze(1).expand(-1, time_feat.shape[1], -1)\nRuntimeError: expand(torch.cuda.FloatTensor{[32, 1, 2, 1]}, size=[-1, 27, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)\n```\n- First\n```\nlog_abs_loc = loc.abs().log1p() if self.config.input_size == 1 else loc.squeeze(1).abs().log1p() log_scale = scale.log() if self.config.input_size == 1 else scale.squeeze(1).log()\n```\nIf the input_size is 1, `loc.squeeze` will not execute, but now the loc.shape is `(bsz, 1, 1)`, so it will be a hidden issue, and \n\n- second\n```\nstatic_feat.unsqueeze(1) will execute, then the error will be reported\n```\n\nI don't know whether it's clear to clarify the problem, but I really need a help.\n\n- environment\n```\nName: transformers\nVersion: 4.52.3\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages\nRequires: filelock, huggingface-hub, numpy, pack\n\nLinux\n\npython3.9.21\n```\n\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. set the InformerConfig.size = 1\n2. maybe do not include any other static features\n3. maybe InformerConfig.scale = None\n\n\n### Expected behavior\n\nTraceback (most recent call last):\n  File \"/home/wjt/luck/FinalWork/alert_models/informer_based_model_3_cpu.py\", line 820, in <module>\n    pipline.train_model()\n  File \"/home/wjt/luck/FinalWork/alert_models/informer_based_model_3_cpu.py\", line 466, in train_model\n    outputs = model(\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/transformers/models/informer/modeling_informer.py\", line 1844, in forward\n    outputs = self.model(\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/transformers/models/informer/modeling_informer.py\", line 1568, in forward\n    transformer_inputs, loc, scale, static_feat = self.create_network_inputs(\n  File \"/home/wjt/.conda/envs/luckluck/lib/python3.9/site-packages/transformers/models/informer/modeling_informer.py\", line 1386, in create_network_inputs\n    expanded_static_feat = static_feat.unsqueeze(1).expand(-1, time_feat.shape[1], -1)\nRuntimeError: expand(torch.cuda.FloatTensor{[32, 1, 2, 1]}, size=[-1, 27, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
    "comments": [
      {
        "user": "Flink-ddd",
        "body": "Hi@2004learner\n\nThank you point out this issue.\n\nI've investigated this issue and have a working fix locally. The root cause is indeed the tensor shape handling when `input_size=1`, which leads to a dimension mismatch in the `expand()` operation.\n\nI would like to submit a Pull Request to resolve this. Please assign this issue to me."
      },
      {
        "user": "2004learner",
        "body": "> Hi@2004learner\n> \n> Thank you point out this issue.\n> \n> I've investigated this issue and have a working fix locally. The root cause is indeed the tensor shape handling when `input_size=1`, which leads to a dimension mismatch in the `expand()` operation.\n> \n> I would like to submit a Pull Request to resolve this. Please assign this issue to me.\n\nThank you very much. I do not know how to assign, maybe I am not a maintainer? And I see you have a added a commit, what can I do for you? "
      },
      {
        "user": "2004learner",
        "body": "@Flink-ddd \nExecuse me, should I wait the owner/collaborator to approve the issuse, it seems that I am not familiar with the process."
      }
    ]
  },
  {
    "issue_number": 38702,
    "title": "Incorrect scaling of Gemma embeddings in float32 regime",
    "author": "norpadon",
    "state": "open",
    "created_at": "2025-06-09T14:48:24Z",
    "updated_at": "2025-06-11T18:32:08Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nIrrelevant\n\n### Who can help?\n\n@ArthurZucker\n\nGoogle Gemma implementation casts embedding scale to bloat16, which rounds 33.9411 to 34.0\n\nTo match this behaviour, [HF implementation](https://github.com/huggingface/transformers/blob/d7b87b415a5dd4a3152051e1a0abd098a02c5bfa/src/transformers/models/gemma3/modeling_gemma3.py#L133) does\n```python\nsuper().forward(input_ids) * self.embed_scale.to(self.weight.dtype)\n```\n\nThis results in incorrect scaling behaviour if the model is loaded in float32 precision.\n\nRelevant PR: https://github.com/huggingface/transformers/pull/29402\n\n### Information\n\n- [x] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```python\n>>> hf_model = AutoModelForCausalLM.from_pretrained(\n    model_repo,\n    torch_dtype=torch.float32,\n    device_map=\"cpu\",\n    \"google/gemma-3-1b-it\",\n)\n>>> hf_model.model.embed_tokens.embed_scale\ntensor(33.9411)\n\n### Expected behavior\n\n```python\n>>> hf_model = AutoModelForCausalLM.from_pretrained(\n    model_repo,\n    torch_dtype=torch.float32,\n    device_map=\"cpu\",\n    \"google/gemma-3-1b-it\",\n)\n>>> hf_model.model.embed_tokens.embed_scale\ntensor(34.0)",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @norpadon, this looks legit! Can you link to the code in the Gemma implementation showing the original behaviour that always casts to `bfloat16`?"
      },
      {
        "user": "norpadon",
        "body": "The original Gemma implementation in JAX also has the same scaling code:\n\nhttps://github.com/google-deepmind/gemma/blob/main/gemma/gm/nn/_modules.py#L111\n\nAnd I think this is a bug on Google's part, because it causes quick and catastrophic accumulation of errors in activations and a huge divergence between fp32 and bf16 logits. I believe models shouldn't behave like this when upcasted to a higher precision. Model was originally trained with rounded-up constant scale factor, so this scale should not change depending on precision"
      },
      {
        "user": "Rocketknight1",
        "body": "Hmmn, this is tricky. At Hugging Face we place a very strong emphasis on **duplicating the behaviour of the original model**, even when we think it's suboptimal in some ways. \n\nHowever, I agree that in this case, it may be unintended - it's increasingly uncommon for people to run models in full `float32`, so that path may just not have been tested. Do you have any evidence that swapping to `float32` significantly degrades generation quality? We could probably justify the change if so."
      }
    ]
  },
  {
    "issue_number": 33664,
    "title": "Object detection training/fine-tuning for Owl-vit/Owlv2",
    "author": "catalys1",
    "state": "open",
    "created_at": "2024-09-23T15:16:57Z",
    "updated_at": "2025-06-11T17:17:33Z",
    "labels": [
      "Good Second Issue",
      "Feature request",
      "Vision"
    ],
    "body": "### Feature request\n\nCurrently the Owl-vit models support inference and CLIP-style contrastive pre-training, but don't provide a way to train (or fine-tune) the detection part of the model. According to [the paper](https://arxiv.org/pdf/2205.06230), detection training is similar to Detr.\n\n### Motivation\n\nIt would be really awesome to be able to train or fine-tune one of these already-existing open-vocabulary object detection models.\n\n### Your contribution\n\nI may be able to help some with this, not sure at present",
    "comments": [
      {
        "user": "LysandreJik",
        "body": "cc @molbap @qubvel "
      },
      {
        "user": "qubvel",
        "body": "Hi @catalys1, thank you so much for your feature request! We agree that the ability to fine-tune Owl-vit/Owlv2 would be a great addition. If you have the time and are interested in contributing, we would love to collaborate with you on this! Your help would be greatly appreciated 🤗 \r\n\r\nThis PR might be also helpful\r\n - https://github.com/huggingface/transformers/pull/31828"
      },
      {
        "user": "daniel-bogdoll",
        "body": "@qubvel Is https://github.com/huggingface/transformers/pull/34057 maybe the solution here? I would also be happy to see the ability to finetune (some) ZeroShotObjectDetection models."
      }
    ]
  },
  {
    "issue_number": 38690,
    "title": "[BUG] Got nan logits after mask logic refactor",
    "author": "jiqing-feng",
    "state": "closed",
    "created_at": "2025-06-09T07:46:58Z",
    "updated_at": "2025-06-11T14:01:13Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntorch 2.7.1\nRegression introduced by #37866 \n\n### Who can help?\n\n@SunMarc @cyrilzakka @ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n`pip install -U autoawq`\n`pip install intel_extension_for_pytorch`\n\npython script.py\n```python\nimport torch\nfrom transformers import pipeline, AutoTokenizer\n\nmodel_id = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\"\ntexts = [\"Once upon a time, there existed a little girl, who liked to have adventures. She wanted to go to places and meet new people, and have fun.\", \"I am happy today because\"]\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = 'left'\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\npipe = pipeline(\"text-generation\", model=model_id, device_map=\"cpu\", torch_dtype=torch.bfloat16, tokenizer=tokenizer)\n\noutput = pipe(texts, batch_size=2)\nprint(output)\n```\n\n### Expected behavior\n\nBefore the regression PR:\n```\n[[{'generated_text': 'Once upon a time, there existed a little girl, who liked to have adventures. She wanted to go to places and meet new people, and have fun. One day, she decided to go on a journey to the forest. She packed a small bag and set off early in the morning. As she walked, the sun rose higher in the sky and the trees grew taller.\\nShe walked for a while, but the forest seemed to go on forever. She began to feel a bit scared. What if she got lost? What if she encountered wild animals? But she didn\\'t want to turn back. She remembered her mother\\'s words, \"Courage is not the absence of fear, but rather the judgment that something else is more important than fear.\" She took a deep breath and continued on her journey.\\nAs she walked, the trees grew closer together and the path became narrower. She had to push aside branches and fight her way through thorny vines. But she didn\\'t give up. She kept going, her heart beating faster and faster.\\nSuddenly, she heard a rustling in the bushes. She stopped and listened. A beautiful bird emerged from the underbrush. It was a rare species, with feathers of the most vibrant colors she had ever seen. The bird looked at her with big, round eyes and tweeted a sweet melody.\\nThe little girl was amazed and delighted. She sat down on a rock, and the bird per'}], [{'generated_text': 'I am happy today because I had a great day in the kitchen. I made a delicious breakfast for my family, and it was a hit! We had scrambled eggs, bacon, and pancakes. The pancakes were a special recipe that I found online, and they were so fluffy and light. My family loved them, and they even asked for seconds.\\nBut the best part of my day was making a special treat for my kids. They love when I make them a \"breakfast for dinner\" treat, and tonight I made them pancakes and sausage. They were so excited to have pancakes for dinner, and they loved the sausage. It was a fun twist on a classic meal.\\nI am grateful for the opportunity to spend time in the kitchen and make meals for my family. It is a joy to see them enjoy the food I make, and it brings me so much happiness. I feel like I am making a difference in their lives, even if it\\'s just in a small way. And that\\'s what makes it all worth it.\\nWhat are some of your favorite meals to make for your family? Do you have any special recipes that you like to make on occasion? I would love to hear about them! Let\\'s chat in the comments below!\\nI am so grateful for the blessings in my life'}]]\n```\n\n\nAfter the regression PR:\n```\n  File \"/home/jiqingfe/transformers/src/transformers/pipelines/base.py\", line 1338, in forward\n    model_outputs = self._forward(model_inputs, **forward_params)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jiqingfe/transformers/src/transformers/pipelines/text_generation.py\", line 400, in _forward\n    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jiqingfe/transformers/src/transformers/generation/utils.py\", line 2623, in generate\n    result = self._sample(\n             ^^^^^^^^^^^^^\n  File \"/home/jiqingfe/transformers/src/transformers/generation/utils.py\", line 3649, in _sample\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\n```",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @cyrilvallez !"
      },
      {
        "user": "bvantuan",
        "body": "Hi @jiqing-feng @Rocketknight1 @Cyrilvallez ! I investigated the issue and identified that the issue originates from the `torch.nn.functional.scaled_dot_product_attention` function. This function generates NaN values for padding tokens, which are then propagated to all positions during subsequent SDPA calls, as described in [110213](https://github.com/pytorch/pytorch/issues/110213). There are currently different backends for scaled dot product attention. The MATH backend correctly assigns zero values to padding tokens, whereas the FLASH_ATTENTION backend results in NaN values.\n\nFor the MATH backend:\n```python\nwith sdpa_kernel(backends=[SDPBackend.MATH]):\n    attn_output = torch.nn.functional.scaled_dot_product_attention(\n        query,\n        key,\n        value,\n        attn_mask=attention_mask,\n        dropout_p=dropout,\n        scale=scaling,\n        is_causal=is_causal,\n    )\n    attn_output = attn_output.transpose(1, 2).contiguous()\n```\n\nattn_output:\n```\ntensor([[[[ 0.0132,  0.0013,  0.0293,  ..., -0.0227, -0.0034,  0.0452],\n          [ 0.0132,  0.0013,  0.0293,  ..., -0.0227, -0.0034,  0.0452],\n          [ 0.0132,  0.0013,  0.0293,  ..., -0.0227, -0.0034,  0.0452],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[ 0.0123,  0.0018,  0.0269,  ..., -0.0205,  0.0008,  0.0408],\n          [ 0.0130,  0.0014,  0.0288,  ..., -0.0223, -0.0026,  0.0444],\n          [ 0.0122,  0.0018,  0.0265,  ..., -0.0203,  0.0014,  0.0400],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0018,  0.0002,  0.0060,  ...,  0.0075, -0.0002,  0.0042],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[ 0.0104,  0.0024,  0.0237,  ..., -0.0165,  0.0059,  0.0349],\n          [ 0.0019,  0.0035,  0.0184,  ..., -0.0034,  0.0119,  0.0250],\n          [ 0.0007,  0.0054,  0.0084,  ...,  0.0029,  0.0303,  0.0070],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0012, -0.0007,  0.0040,  ...,  0.0021, -0.0014,  0.0008],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         ...,\n\n         [[-0.0148,  0.0033,  0.0125,  ..., -0.0018,  0.0044,  0.0053],\n          [ 0.0022,  0.0065,  0.0250,  ...,  0.0012, -0.0028,  0.0234],\n          [ 0.0054,  0.0237,  0.0278,  ...,  0.0176, -0.0256,  0.0099],\n          ...,\n          [ 0.0018, -0.0016,  0.0030,  ...,  0.0019, -0.0034,  0.0009],\n          [ 0.0017, -0.0019,  0.0035,  ...,  0.0018, -0.0027,  0.0002],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[-0.0078,  0.0011,  0.0140,  ...,  0.0006,  0.0116,  0.0063],\n          [ 0.0023,  0.0007,  0.0289,  ..., -0.0135, -0.0016,  0.0400],\n          [ 0.0201, -0.0312,  0.0149,  ..., -0.0002,  0.0781,  0.0098],\n          ...,\n          [ 0.0018, -0.0019,  0.0028,  ...,  0.0021, -0.0033,  0.0008],\n          [ 0.0010, -0.0019,  0.0037,  ...,  0.0017, -0.0031,  0.0004],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[-0.0081,  0.0039,  0.0172,  ...,  0.0022,  0.0042,  0.0125],\n          [-0.0002,  0.0026,  0.0197,  ..., -0.0073,  0.0018,  0.0254],\n          [-0.0113,  0.0010,  0.0317,  ...,  0.0044,  0.0067,  0.0302],\n          ...,\n          [ 0.0022, -0.0022,  0.0025,  ...,  0.0015, -0.0027,  0.0007],\n          [ 0.0017, -0.0018,  0.0029,  ...,  0.0022, -0.0034,  0.0008],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0036,  0.0010]]],\n\n\n        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n\n         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n\n         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n\n         ...,\n\n         [[ 0.0162,  0.0014, -0.0245,  ...,  0.0129,  0.0153, -0.0079],\n          [ 0.0126,  0.0019,  0.0272,  ..., -0.0217, -0.0026,  0.0427],\n          [ 0.0206, -0.0062, -0.0564,  ...,  0.0103,  0.0364, -0.0243],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0020, -0.0037,  0.0010],\n          [ 0.0014, -0.0009,  0.0072,  ...,  0.0027, -0.0055,  0.0013],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[ 0.0132,  0.0039, -0.0254,  ...,  0.0101,  0.0165, -0.0102],\n          [ 0.0001,  0.0151,  0.0153,  ..., -0.0205,  0.0024,  0.0204],\n          [-0.0220,  0.0337, -0.0081,  ..., -0.0198,  0.0120, -0.0190],\n          ...,\n          [ 0.0017, -0.0014,  0.0032,  ...,  0.0020, -0.0037,  0.0010],\n          [ 0.0017, -0.0019,  0.0061,  ...,  0.0012, -0.0046,  0.0013],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[ 0.0088,  0.0067, -0.0211,  ...,  0.0086,  0.0173, -0.0064],\n          [ 0.0039,  0.0053,  0.0197,  ..., -0.0046,  0.0089,  0.0398],\n          [-0.0021,  0.0147,  0.0081,  ...,  0.0085,  0.0197,  0.0283],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0025,  0.0008,  0.0073,  ...,  0.0023, -0.0034,  0.0039],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]]]],\n       dtype=torch.bfloat16)\n```\n\nFor the FLASH_ATTENTION backend:\n```python\nwith sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n    attn_output = torch.nn.functional.scaled_dot_product_attention(\n        query,\n        key,\n        value,\n        attn_mask=attention_mask,\n        dropout_p=dropout,\n        scale=scaling,\n        is_causal=is_causal,\n    )\n    attn_output = attn_output.transpose(1, 2).contiguous()\n```\n\nattn_output:\n```\ntensor([[[[ 0.0132,  0.0013,  0.0293,  ..., -0.0227, -0.0034,  0.0452],\n          [ 0.0132,  0.0013,  0.0293,  ..., -0.0227, -0.0034,  0.0452],\n          [ 0.0132,  0.0013,  0.0293,  ..., -0.0227, -0.0034,  0.0452],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[ 0.0123,  0.0018,  0.0269,  ..., -0.0205,  0.0008,  0.0408],\n          [ 0.0130,  0.0014,  0.0288,  ..., -0.0223, -0.0026,  0.0444],\n          [ 0.0122,  0.0018,  0.0265,  ..., -0.0203,  0.0014,  0.0400],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0018,  0.0002,  0.0060,  ...,  0.0075, -0.0002,  0.0042],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[ 0.0104,  0.0024,  0.0237,  ..., -0.0165,  0.0059,  0.0349],\n          [ 0.0019,  0.0035,  0.0184,  ..., -0.0034,  0.0120,  0.0250],\n          [ 0.0007,  0.0054,  0.0084,  ...,  0.0029,  0.0303,  0.0070],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0012, -0.0007,  0.0040,  ...,  0.0021, -0.0014,  0.0008],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         ...,\n\n         [[-0.0148,  0.0033,  0.0125,  ..., -0.0018,  0.0044,  0.0053],\n          [ 0.0022,  0.0066,  0.0250,  ...,  0.0012, -0.0028,  0.0234],\n          [ 0.0054,  0.0237,  0.0278,  ...,  0.0176, -0.0256,  0.0099],\n          ...,\n          [ 0.0018, -0.0016,  0.0030,  ...,  0.0019, -0.0034,  0.0009],\n          [ 0.0017, -0.0019,  0.0035,  ...,  0.0018, -0.0027,  0.0002],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[-0.0078,  0.0011,  0.0140,  ...,  0.0006,  0.0116,  0.0063],\n          [ 0.0023,  0.0007,  0.0291,  ..., -0.0135, -0.0016,  0.0400],\n          [ 0.0201, -0.0312,  0.0149,  ..., -0.0002,  0.0781,  0.0098],\n          ...,\n          [ 0.0018, -0.0019,  0.0028,  ...,  0.0021, -0.0033,  0.0008],\n          [ 0.0010, -0.0019,  0.0037,  ...,  0.0017, -0.0031,  0.0004],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[-0.0081,  0.0039,  0.0172,  ...,  0.0022,  0.0042,  0.0125],\n          [-0.0002,  0.0026,  0.0197,  ..., -0.0073,  0.0018,  0.0255],\n          [-0.0112,  0.0010,  0.0317,  ...,  0.0044,  0.0067,  0.0302],\n          ...,\n          [ 0.0022, -0.0022,  0.0025,  ...,  0.0015, -0.0027,  0.0007],\n          [ 0.0017, -0.0018,  0.0029,  ...,  0.0022, -0.0034,  0.0008],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0036,  0.0010]]],\n\n\n        [[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          ...,\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n\n         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          ...,\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n\n         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          ...,\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n\n         ...,\n\n         [[ 0.0162,  0.0014, -0.0247,  ...,  0.0130,  0.0154, -0.0080],\n          [ 0.0126,  0.0019,  0.0272,  ..., -0.0217, -0.0026,  0.0427],\n          [ 0.0206, -0.0061, -0.0564,  ...,  0.0103,  0.0364, -0.0243],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0020, -0.0037,  0.0010],\n          [ 0.0014, -0.0009,  0.0073,  ...,  0.0027, -0.0055,  0.0013],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[ 0.0132,  0.0039, -0.0255,  ...,  0.0103,  0.0165, -0.0103],\n          [ 0.0002,  0.0151,  0.0153,  ..., -0.0205,  0.0024,  0.0204],\n          [-0.0220,  0.0337, -0.0081,  ..., -0.0198,  0.0120, -0.0190],\n          ...,\n          [ 0.0017, -0.0014,  0.0032,  ...,  0.0020, -0.0037,  0.0010],\n          [ 0.0017, -0.0019,  0.0061,  ...,  0.0012, -0.0046,  0.0013],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]],\n\n         [[ 0.0089,  0.0066, -0.0211,  ...,  0.0086,  0.0172, -0.0064],\n          [ 0.0039,  0.0053,  0.0197,  ..., -0.0046,  0.0089,  0.0398],\n          [-0.0021,  0.0147,  0.0081,  ...,  0.0085,  0.0197,  0.0283],\n          ...,\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010],\n          [ 0.0025,  0.0008,  0.0073,  ...,  0.0023, -0.0034,  0.0039],\n          [ 0.0017, -0.0014,  0.0031,  ...,  0.0019, -0.0037,  0.0010]]]],\n       dtype=torch.bfloat16)\n```"
      },
      {
        "user": "jiqing-feng",
        "body": "Sounds reasonable, but this issue didn't happend before the PR;\nI re-checked it and found the attention mask is different before and after the PR\nBefore refactor masking:\n```\n(Pdb) attention_mask\ntensor([[[[     0., -65504., -65504.,  ..., -65504., -65504., -65504.],\n          [     0.,      0., -65504.,  ..., -65504., -65504., -65504.],\n          [     0.,      0.,      0.,  ..., -65504., -65504., -65504.],\n          ...,\n          [     0.,      0.,      0.,  ...,      0., -65504., -65504.],\n          [     0.,      0.,      0.,  ...,      0.,      0., -65504.],\n          [     0.,      0.,      0.,  ...,      0.,      0.,      0.]]],\n\n\n        [[[-65504., -65504., -65504.,  ..., -65504., -65504., -65504.],\n          [-65504., -65504., -65504.,  ..., -65504., -65504., -65504.],\n          [-65504., -65504., -65504.,  ..., -65504., -65504., -65504.],\n          ...,\n          [-65504., -65504., -65504.,  ...,      0., -65504., -65504.],\n          [-65504., -65504., -65504.,  ...,      0.,      0., -65504.],\n          [-65504., -65504., -65504.,  ...,      0.,      0.,      0.]]]],\n       dtype=torch.float16)\n```\n\nAfter the PR:\n```\n(Pdb) attention_mask\ntensor([[[[ True, False, False,  ..., False, False, False],\n          [ True,  True, False,  ..., False, False, False],\n          [ True,  True,  True,  ..., False, False, False],\n          ...,\n          [ True,  True,  True,  ...,  True, False, False],\n          [ True,  True,  True,  ...,  True,  True, False],\n          [ True,  True,  True,  ...,  True,  True,  True]]],\n\n\n        [[[False, False, False,  ..., False, False, False],\n          [False, False, False,  ..., False, False, False],\n          [False, False, False,  ..., False, False, False],\n          ...,\n          [False, False, False,  ...,  True, False, False],\n          [False, False, False,  ...,  True,  True, False],\n          [False, False, False,  ...,  True,  True,  True]]]])\n```\n\nIs this meet the expectation?"
      }
    ]
  },
  {
    "issue_number": 38291,
    "title": "4.52.2 报错Could not import module 'Qwen3ForCausalLM'",
    "author": "liuwuwuwu",
    "state": "open",
    "created_at": "2025-05-22T09:25:50Z",
    "updated_at": "2025-06-11T13:48:25Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n>>> import transformers\n>>> print(transformers.__version__)\n4.52.2\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n>>> import transformers\n>>> print(transformers.__version__)\n4.52.2\n\n>>> from transformers import Qwen3ForCausalLM\n\n    raise ModuleNotFoundError(\nModuleNotFoundError: Could not import module 'Qwen3ForCausalLM'. Are this object's requirements defined correctly?\n\n### Expected behavior\n\n>>> import transformers\n>>> print(transformers.__version__)\n4.52.2\n\n>>> from transformers import Qwen3ForCausalLM\n\n    raise ModuleNotFoundError(\nModuleNotFoundError: Could not import module 'Qwen3ForCausalLM'. Are this object's requirements defined correctly?",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @liuwuwuwu do you get the same error on `main`? Try `pip install --upgrade git+https://github.com/huggingface/transformers.git`"
      },
      {
        "user": "fopdoodle8",
        "body": "still have the same problem using the `pip install --upgrade git+https://github.com/huggingface/transformers.git` which installs transformers-4.53.0.dev0"
      },
      {
        "user": "Rocketknight1",
        "body": "Very strange! `from transformers import Qwen3ForCausalLM` runs correctly for me after installing from `main`."
      }
    ]
  },
  {
    "issue_number": 38742,
    "title": "[DeepSeek-V3] take care of the case `q_lora_rank is None`",
    "author": "bzantium",
    "state": "closed",
    "created_at": "2025-06-11T06:13:51Z",
    "updated_at": "2025-06-11T12:35:11Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nImplement handling for configurations where the `q_lora_rank` parameter is set to `None`.\n\n### Motivation\n\n1. [DeepSeek-V2-Lite](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite/blob/main/config.json) model has `q_lora_rank=None` so we can support this model with this modeling.\n2. [Original remote modeling code](https://huggingface.co/deepseek-ai/DeepSeek-V3) also supports this feature.\n\n### Your contribution\n\nImplement handling for configurations where the `q_lora_rank` parameter is set to `None`.",
    "comments": []
  },
  {
    "issue_number": 38709,
    "title": "`get_video_features` in XCLIPModel always returns `pooled_output`",
    "author": "Vishu26",
    "state": "open",
    "created_at": "2025-06-10T00:51:37Z",
    "updated_at": "2025-06-11T12:33:09Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nhttps://github.com/huggingface/transformers/blob/f4fc42216cd56ab6b68270bf80d811614d8d59e4/src/transformers/models/x_clip/modeling_x_clip.py#L1376\n\nHi\n\nThe `get_video_features` function is hardcoded to always return the `pooled_output`. But sometimes, it might be beneficial to get the `last_hidden_state` instead. Can we fix this behavior?\n\nThanks\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```import av\nimport torch\nimport numpy as np\n\nfrom transformers import AutoProcessor, AutoModel\nfrom huggingface_hub import hf_hub_download\n\nnp.random.seed(0)\n\n\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        clip_len (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    converted_len = int(clip_len * frame_sample_rate)\n    end_idx = np.random.randint(converted_len, seg_len)\n    start_idx = end_idx - converted_len\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices\n\n\n# video clip consists of 300 frames (10 seconds at 30 FPS)\nfile_path = hf_hub_download(\n    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n)\ncontainer = av.open(file_path)\n\n# sample 8 frames\nindices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\nvideo = read_video_pyav(container, indices)\n\nprocessor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\nmodel = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\ninputs = processor(\n    videos=list(video),\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# forward pass\nwith torch.no_grad():\n    outputs = model.get_video_features(**inputs)\n\nprint(outputs.shape)\n\n### Expected behavior\n\nThe `get_video_features` function should have the option to output the `last_hidden_state` as well.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @Vishu26, I agree the behaviour is a little weird - the function takes arguments like `output_hidden_states` but those basically don't affect the output at all! It's quite tricky to fix without breaking backward compatibility, though. Maybe we could return a dict if and only if `output_hidden_states` is `True`, but leave the output as a naked `Tensor` otherwise, so that people's existing workflows are unaffected?"
      },
      {
        "user": "Vishu26",
        "body": "Hi @Rocketknight1 , I noticed that not only the XCLIP model, every other CLIP-like model has this behavior. Not sure how it can be fixed for all the models and at the same time not affect people's existing workflows."
      },
      {
        "user": "Rocketknight1",
        "body": "I think it's possible, as long as nothing changes when `output_hidden_states = False`!"
      }
    ]
  },
  {
    "issue_number": 38750,
    "title": "Is it a good choice to early error when `output_attentions=True` and attn implementation not equal to `eager`",
    "author": "kaixuanliu",
    "state": "closed",
    "created_at": "2025-06-11T11:05:48Z",
    "updated_at": "2025-06-11T11:10:11Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nBefore this PR [38288](https://github.com/huggingface/transformers/pull/38288), the program will run smoothly even when we set `output_attentions=True` and the attn implementation is not `eager`, as it will fallback to use eager mode, after this PR, it will throw error directly: [L342](https://github.com/huggingface/transformers/blob/main/src/transformers/configuration_utils.py#L342). I think it would be better if we just throw a warning and fallback to `eager` attn. Is it possible to revert it back or make small direct change based on this PR?\n\n### Who can help?\n\n@ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nN/A\n\n### Expected behavior\n\nWe want to make sure program can run without crash even we set `output_attentions=True` and attn implementation not equal to `eager`",
    "comments": []
  },
  {
    "issue_number": 38733,
    "title": "GRPO per_device_eval_batch_size can't be set as 1, when there is only 1 GPU",
    "author": "CasanovaLLL",
    "state": "closed",
    "created_at": "2025-06-10T14:58:11Z",
    "updated_at": "2025-06-11T09:45:32Z",
    "labels": [],
    "body": "`eval batch size must be evenly divisible by the number of generations per prompt. ` When I only have one GPU, I cannot set `per_device_eval_batch_size=1` because there will be no reasonable G to choose from. Is it possible to automatically calculate a value similar to the number of gradient accumulation steps to achieve this feature?",
    "comments": []
  },
  {
    "issue_number": 34928,
    "title": "Recomputed tensor size does not match when using activation checkpointing when using FSDP and accelerate",
    "author": "jjbuck",
    "state": "closed",
    "created_at": "2024-11-25T19:02:12Z",
    "updated_at": "2025-06-11T08:05:15Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\r\n\r\n```\r\n- `transformers` version: 4.46.3\r\n- Platform: Linux-6.8.0-1015-aws-x86_64-with-glibc2.35\r\n- Python version: 3.12.6\r\n- Huggingface_hub version: 0.26.2\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.1.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: distributed (`accelerate`)\r\n- Using GPU in script?: Yes\r\n- GPU type: NVIDIA A100-SXM4-40GB\r\n```\r\n\r\n### Who can help?\r\n@muellerz @SunMarc @ArthurZucker \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nI'm running into the following error while trying to use the SFTTrainer with FSDP and the `accelerate` library (full stack trace provided at the very bottom of this post).\r\n\r\n```\r\ntorch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass\r\n```\r\n\r\nThis occurs when I set `gradient_checkpointing: false` and `activation_checkpointing: true`. Curiously, it actually seems to work if I set `gradient_checkpointing: true` and `activation_checkpointing: false`, **but** that produces the following warning message:\r\n```\r\n # When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404`. \r\n ```\r\n\r\nThere are a few related GitHub issues around that touch on this issue:\r\n1. https://github.com/Lightning-AI/pytorch-lightning/issues/19267\r\n2. https://github.com/huggingface/transformers/issues/28499\r\n3. https://github.com/pytorch/pytorch/issues/124788\r\n4. https://github.com/huggingface/transformers/issues/32073\r\n\r\nOne of these suggested setting `use_reentrant: true`, but that doesn't resolve the issue for me.\r\n\r\n\r\nI'm attempting to run this as a SageMaker training job using the official HuggingFace estimator (this amounts to the following command: `torchrun --nnodes 1 --nproc_per_node 8 train.py`. My training script is essentially a lightly adapted version of the official examples. Below is how I'm instantiating the HuggingFace estimator object:\r\n\r\n```\r\nhuggingface_estimator = HuggingFace(\r\n    entry_point          = 'train.py',        # train script\r\n    #entry_point          = 'launch.py',        # train script\r\n    dependencies=['requirements.txt'],         \r\n    source_dir           = './',            \r\n    instance_type        = 'ml.p4d.24xlarge',\r\n    instance_count       = 1,               \r\n    max_run              = 2*24*60*60,     \r\n    base_job_name        = job_name,          \r\n    role                 = role,            \r\n    volume_size          = 1024,              \r\n    transformers_version = '4.36.0',     \r\n    pytorch_version      = '2.1.0',          \r\n    py_version           = 'py310',          \r\n    hyperparameters      =  {\r\n        \"config_s3_uri\": \"s3://<foo>\r\n    },\r\n    #metric_definitions=metric_definitions,\r\n    disable_output_compression = True,  \r\n    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\r\n    environment  = {\r\n        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", \r\n        \"HF_TOKEN\": HfFolder.get_token(),      \r\n        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\r\n        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"0\",   # enable CPU RAM efficient loading\r\n        \"FSDP_AUTO_WRAP_POLICY\": \"TRANSFORMER_BASED_WRAP\",\r\n        \"FSDP_BACKWARD_PREFETCH\": \"BACKWARD_PRE\",\r\n        \"FSDP_STATE_DICT_TYPE\": \"FULL_STATE_DICT\",\r\n        \"NCCL_TIMEOUT\": \"3600\",  # 1 hour timeout\r\n        \"NCCL_DEBUG\": \"WARN\",    \r\n        \"NCCL_IB_TIMEOUT\": \"3600\",\r\n        \"NCCL_SOCKET_TIMEOUT\": \"3600\",\r\n        \"NCCL_ASYNC_ERROR_HANDLING\": \"1\",\r\n        \"NCCL_P2P_LEVEL\": \"NVL\",\r\n        \"CUDA_DEVICE_MAX_CONNECTIONS\": \"1\",        \r\n        \"MAX_JOBS\": \"1\",                           \r\n        \"PYTORCH_CUDA_ALLOC_CONF\": \"max_split_size_mb:512\",\r\n        \"TORCH_DISTRIBUTED_DEBUG\": \"DETAIL\",     \r\n    },\r\n    checkpoint_s3_uri=f's3://<foo>'\r\n)\r\n```\r\n\r\nBelow are some of the relevant parameters from my input config.\r\n```\r\ngradient_checkpointing: false \r\ngradient_checkpointing_kwargs:\r\n  use_reentrant: true\r\nattn_implementation: \"flash_attention_2\"\r\npacking: false\r\nbf16: \"auto\"\r\nfsdp: \"full_shard auto_wrap offload\"\r\nfsdp_config:\r\n  limit_all_gathers: true\r\n  backward_prefetch: \"backward_pre\"\r\n  forward_prefetch: \"false\"\r\n  use_orig_params: \"false\"\r\n  min_num_params: 0\r\n  activation_checkpointing: \"true\"\r\n```\r\n\r\n\r\n\r\n*Full Stack Trace*\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 224, in <module>\r\nmain(cfg)\r\n  File \"train.py\", line 207, in main\r\n    main(cfg)\r\n  File \"train.py\", line 207, in main\r\n    trainer.train()\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2123, in train\r\n    trainer.train()\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2123, in train\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 224, in <module>\r\nmain(cfg)main(cfg)\r\n  File \"train.py\", line 207, in main\r\ntrainer.train()trainer.train()\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2123, in train\r\nreturn inner_training_loop(\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\r\n    return inner_training_loop(\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\r\nreturn inner_training_loop(return inner_training_loop(\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\r\ntr_loss_step = self.training_step(model, inputs, num_items_in_batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3612, in training_step\r\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3612, in training_step\r\ntr_loss_step = self.training_step(model, inputs, num_items_in_batch)tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3612, in training_step\r\nself.accelerator.backward(loss, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2241, in backward\r\nTraceback (most recent call last):\r\n  File \"/opt/ml/code/train.py\", line 224, in <module>\r\n    main(cfg)\r\n  File \"/opt/ml/code/train.py\", line 207, in main\r\n    trainer.train()\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2123, in train\r\n    return inner_training_loop(\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3612, in training_step\r\n    self.accelerator.backward(loss, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2241, in backward\r\n    loss.backward(**kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\r\n    torch.autograd.backward(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 1075, in unpack_hook\r\n    frame.check_recomputed_tensors_match(gid)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 850, in check_recomputed_tensors_match\r\n    raise CheckpointError(\r\ntorch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.\r\ntensor at position 18:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}\r\ntensor at position 19:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}\r\nloss.backward(**kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\r\n    loss.backward(**kwargs)\r\n          File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\r\nself.accelerator.backward(loss, **kwargs)self.accelerator.backward(loss, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2241, in backward\r\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2241, in backward\r\n    torch.autograd.backward(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\r\n    torch.autograd.backward(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 1075, in unpack_hook\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 1075, in unpack_hook\r\nframe.check_recomputed_tensors_match(gid)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 850, in check_recomputed_tensors_match\r\n        loss.backward(**kwargs)loss.backward(**kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\r\n    frame.check_recomputed_tensors_match(gid)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 850, in check_recomputed_tensors_match\r\n        torch.autograd.backward(torch.autograd.backward(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\r\n    raise CheckpointError(\r\ntorch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.\r\ntensor at position 18:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}\r\ntensor at position 19:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}\r\n        Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward passVariable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 1075, in unpack_hook\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 1075, in unpack_hook\r\n    raise CheckpointError(\r\ntorch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.\r\ntensor at position 18:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}\r\ntensor at position 19:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}\r\nframe.check_recomputed_tensors_match(gid)frame.check_recomputed_tensors_match(gid)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 850, in check_recomputed_tensors_match\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 850, in check_recomputed_tensors_match\r\n    raise CheckpointError(\r\ntorch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.\r\ntensor at position 18:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}\r\ntensor at position 19:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}\r\n    raise CheckpointError(\r\ntorch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.\r\ntensor at position 18:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}\r\ntensor at position 19:\r\nsaved metadata: {'shape': torch.Size([2, 1024, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}\r\nrecomputed metadata: {'shape': torch.Size([2, 2048, 28, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}\r\n0%|          | 0/100 [00:13<?, ?it/s]\r\n[E ProcessGroupGloo.cpp:138] Rank 5 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank.\r\n[E ProcessGroupGloo.cpp:138] Rank 4 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank.\r\n[E ProcessGroupGloo.cpp:138] Rank 7 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank.\r\n[2024-11-25 18:39:43,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 69 closing signal SIGTERM\r\n[2024-11-25 18:39:43,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 73 closing signal SIGTERM\r\n[2024-11-25 18:39:43,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 74 closing signal SIGTERM\r\n[2024-11-25 18:39:43,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 76 closing signal SIGTERM\r\n[2024-11-25 18:39:47,931] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 70) of binary: /opt/conda/bin/python\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nThe expected behavior is for the SFTTrainer's `train()` method to run without errors.",
    "comments": [
      {
        "user": "jjbuck",
        "body": "For posterity, I think this has to do with the `use_cache` argument for the `SFTTrainer` constructor. I got this to work by doing the following. In my training script, I've got the config shown below. Originally, the `use_cache` field was set to `use_cache=not sft_config.gradient_checkpointing`. If I set `gradient_checkpointing: false` and `activation_checkpointing: true`, then the conditional there evaluates to True. I changed this to `use_cache=not (sft_config.gradient_checkpointing or sft_config.fsdp_config.activation_checkpointing)`\r\n\r\n```\r\nmodel_kwargs = dict(\r\n      attn_implementation=sft_config.attn_implementation,\r\n      torch_dtype=sft_config.torch_dtype,\r\n      #use_cache=not sft_config.gradient_checkpointing, # Original iteration\r\n      use_cache=not (sft_config.gradient_checkpointing or sft_config.fsdp_config.activation_checkpointing) # Modification that seems to solve the issue.\r\n)\r\nmodel = AutoModelForCausalLM.from_pretrained(sft_config.model_name_or_path, **model_kwargs)\r\n```"
      },
      {
        "user": "hav4ik",
        "body": "I'm having this problem in my training as well. I'm using Deepspeed Zero3 with the trainer from transformers (although I had similar problem when I used Axolotl for another project). `use_cache` did not work for me. Setting `use_reentrant=True` did work, but the model just refuses to converge. I tested a bunch of configs, and all the settings with `use_reentrant=True` are the ones above, and `use_reentrant=False` are the ones with lower losses below.\r\n\r\n![image](https://github.com/user-attachments/assets/dd8aaa20-d506-49b9-ab1b-9db9163eec90)\r\n\r\nThe grad norm is very strange. In the chart below, you can see a bunch of runs with very spiky grad norms, occuring every ~300 steps. These are the ones with `use_reentrant=True`. The more regular runs with OK-ish grad norms are the ones with `use_reentrant=False`.\r\n![image](https://github.com/user-attachments/assets/24a5cfbf-cdcb-4347-b8c3-98207ea10495)\r\n\r\n--------------------------------------\r\n\r\n# TL;DR\r\nSetting `use_reentrant=True` in gradients checkpointing worked, but my model just refuses to converge.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37927,
    "title": "request the support for training support for QuantizationMethod.FP8",
    "author": "edoproch",
    "state": "closed",
    "created_at": "2025-05-02T13:11:33Z",
    "updated_at": "2025-06-11T08:02:49Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nI was trying to fine tune unsloth/Qwen3-4B-FP8 with unsloth ad I have the following error\n\n# code\n\n```\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = dataset_val,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    \n    dataset_num_proc = 2,\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        eval_on_start=True,\n        eval_steps=1,\n        num_train_epochs = 4, \n        warmup_ratio=0.1,\n        #max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)\n```\n\n# Error\n\nValueError: The model you are trying to fine-tune is quantized with QuantizationMethod.FP8 but that quantization method do not support training. Please open an issue on GitHub: https://github.com/huggingface/transformers to request the support for training support for QuantizationMethod.FP8\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nGo on unsloth's colab notebook about fine tuning Qwen3 and change the default model with unsloth/Qwen3-4B-FP8\n\n### Expected behavior\n\nThe training should work",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @SunMarc, but not sure if this is just expected behaviour!"
      },
      {
        "user": "SunMarc",
        "body": "This is expected behavior ! To train a model in FP8, have a look at this [doc](https://huggingface.co/docs/accelerate/en/usage_guides/low_precision_training#what-training-on-fp8-means) ! "
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37928,
    "title": "Updates in type-checking specifications have broken transformers' types",
    "author": "thfrkielikone",
    "state": "closed",
    "created_at": "2025-05-02T13:23:11Z",
    "updated_at": "2025-06-11T08:02:47Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.0.dev0 (commit fa3c3f9cab1c45b449bd57e238c511c79637e314; head of main branch as of writing)\n- Platform: macOS-15.4.1-arm64-arm-64bit-Mach-O\n- Python version: 3.13.2\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: not installed\n- Accelerate config: not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.7.0 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <not relevant; type checking issue>\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nThe following code runs correctly in the above environment (empty venv), ie. quits without producing output:\n\n```python\nfrom transformers import PreTrainedTokenizer, GenerationMixin, StoppingCriteriaList, StoppingCriteria, pipeline, Pipeline\n```\n\nUpon type checking with the newest pyright (1.1.400 installed from npm), it produces the following error:\n\n```\n<snip>/repro.py\n  <snip>/repro.py:1:58 - error: \"PreTrainedTokenizer\" is not exported from module \"transformers\"\n    Import from \"transformers.tokenization_utils\" instead (reportPrivateImportUsage)\n  <snip>/repro.py:1:79 - error: \"GenerationMixin\" is not exported from module \"transformers\"\n    Import from \"transformers.generation.utils\" instead (reportPrivateImportUsage)\n  <snip>/repro.py:1:96 - error: \"StoppingCriteriaList\" is not exported from module \"transformers\"\n    Import from \"transformers.generation.stopping_criteria\" instead (reportPrivateImportUsage)\n  <snip>/repro.py:1:118 - error: \"StoppingCriteria\" is not exported from module \"transformers\"\n    Import from \"transformers.generation.stopping_criteria\" instead (reportPrivateImportUsage)\n  <snip>/repro.py:1:136 - error: \"pipeline\" is not exported from module \"transformers\"\n    Import from \"transformers.pipelines\" instead (reportPrivateImportUsage)\n  <snip>/repro.py:1:146 - error: \"Pipeline\" is not exported from module \"transformers\"\n    Import from \"transformers.pipelines.base\" instead (reportPrivateImportUsage)\n6 errors, 0 warnings, 0 informations\n```\n\nThis is due to pyright updating its definition of what is a publicly re-exported symbol; see page https://docs.basedpyright.com/dev/benefits-over-pyright/new-diagnostic-rules/ section reportPrivateLocalImportUsage. It also links to the relevant PEP (https://peps.python.org/pep-0484/#stub-files) that specifies this. To quote the PEP:\n\n> Modules and variables imported into the stub are not considered exported from the stub unless the import uses the import ... as ... form or the equivalent from ... import ... as ... form. (UPDATE: To clarify, the intention here is that only names imported using the form X as X will be exported, i.e. the name before and after as must be the same.)\n\nIf looking at the documentation for eg. the pipeline objects (https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/pipelines#transformers.pipeline), we find this example code, which does the same `from transformers import pipeline`; I conclude that this is the expected usage:\n\n```python\nimport datasets\nfrom transformers import pipeline\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom tqdm.auto import tqdm\n\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\n\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\nfor out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\n    print(out)\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n    # {\"text\": ....}\n    # ....\n```\n\nIn my environment I get the same kind of error (+ I don't have datasets installed)\n\n```\n<snip>/pipeline.py\n  <snip>/pipeline.py:1:8 - error: Import \"datasets\" could not be resolved (reportMissingImports)\n  <snip>/pipeline.py:2:26 - error: \"pipeline\" is not exported from module \"transformers\"\n    Import from \"transformers.pipelines\" instead (reportPrivateImportUsage)\n2 errors, 0 warnings, 0 informations\n```\n\nFixing this issue needs an intervention upon the imports in `src/transformers/__init__.py`. It looks like something tedious that could be mostly automated. For the pipeline error to get resolved, the minimal change was sth like this:\n\n```patch\ndiff --git a/src/transformers/__init__.py b/src/transformers/__init__.py\nindex 691f8aad00..2cea8c0078 100644\n--- a/src/transformers/__init__.py\n+++ b/src/transformers/__init__.py\n@@ -668,8 +668,8 @@ if TYPE_CHECKING:\n         ZeroShotClassificationPipeline,\n         ZeroShotImageClassificationPipeline,\n         ZeroShotObjectDetectionPipeline,\n-        pipeline,\n     )\n+    from .pipelines import pipeline as pipeline\n     from .processing_utils import ProcessorMixin\n \n     # Tokenization\n```\n\n### Expected behavior\n\nThe above code which runs correctly should also type check cleanly.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @thfrkielikone, we probably won't make any breaking changes just to fix type checking - our type checking philosophy is that it's \"nice to have\", but other factors are more important.\n\nIf you or anyone else wants to try making a PR to fix type checking, you can! However, we'll probably only accept it if it doesn't have any side-effects, and doesn't interact badly with any of our tooling / consistency checks."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37934,
    "title": "Is Llama4TextL2Norm meant to be RMS norm?",
    "author": "0x6b64",
    "state": "closed",
    "created_at": "2025-05-02T21:28:05Z",
    "updated_at": "2025-06-11T08:02:45Z",
    "labels": [],
    "body": "https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama4.py#L118\n\n```\nx * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n```\n\nThis is just the rms norm?",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "That does look like the RMSNorm computation, yes. However, RMSNorm was only added to PyTorch in 2.3 or 2.4 I think, so we need to do it manually until our minimum supported torch version catches up!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37943,
    "title": "[Bug] Gemma3Processor.apply_chat_template returns Tensor instead of dict with long multimodal few-shot inputs",
    "author": "Canticle929",
    "state": "closed",
    "created_at": "2025-05-03T21:29:26Z",
    "updated_at": "2025-06-11T08:02:43Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntransformers version: 4.50.0.dev0\ntokenizers version: 0.21.1 \ntorch version: 2.6.0\nPython version: 3.10.16\nModel: google/gemma-3-27b-it\n\n\n\n### Who can help?\n\n@ArthurZucker @itazap\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Load the Gemma 3 processor: processor = AutoProcessor.from_pretrained(\"google/gemma-3-27b-it\")\n2. Create a PIL Image object.\n3. Construct a messages list containing a user role. Its content should be a list containing first an image dictionary ({\"type\": \"image\", \"image\": pil_image}) and then a text dictionary ({\"type\": \"text\", \"text\": long_complex_prompt}). The long_complex_prompt must be a very long string containing structured few-shot examples.\n4. Call raw_inputs = processor.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).\n5. Check type(raw_inputs).\n6. (Optional) Attempting model.generate(**raw_inputs) will trigger the TypeError.\n\nCode and error messages are include in attachments\n\n[Minimal Reproducible Example (MRE).zip](https://github.com/user-attachments/files/20025093/Minimal.Reproducible.Example.MRE.zip)\n\n\n\n### Expected behavior\n\n**Describe the bug**\nWhen using Gemma3Processor loaded via AutoProcessor.from_pretrained(\"google/gemma-3-27b-it\"), calling the apply_chat_template method with a messages list containing both an image ({\"type\": \"image\", ...}) and a very long and structured text (e.g., few-shot examples constructed via loops), and setting tokenize=True and return_tensors='pt', the method unexpectedly returns a torch.Tensor object instead of the expected dict.\n**Expected behavior**\napply_chat_template(..., tokenize=True, return_tensors='pt') is expected to return a BatchEncoding object (which behaves like a dictionary) containing keys like input_ids and attention_mask, with values of type torch.Tensor.\nActual behavior\nThe method actually returns a single torch.Tensor, which appears to contain only the input_ids.\n**Consequence**\nBecause the return type is not as expected, subsequent attempts to pass this result to model.generate (e.g., model.generate(**result)) raise a TypeError: argument after ** must be a mapping, not Tensor, as the ** operator expects a dictionary (mapping).",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "@Canticle929 hey! You need to enable `return_dict=True` to get dictionary outputs. See [documentation](https://huggingface.co/docs/transformers/v4.51.3/chat_templating_multimodal) for more examples"
      },
      {
        "user": "Canticle929",
        "body": "Thanks a lot! I will try.\r\n\r\n发件人: Raushan Turganbay ***@***.***>\r\n日期: 星期日, 2025年5月4日 17:50\r\n收件人: huggingface/transformers ***@***.***>\r\n抄送: Canticle929 ***@***.***>, Mention ***@***.***>\r\n主题: Re: [huggingface/transformers] [Bug] Gemma3Processor.apply_chat_template returns Tensor instead of dict with long multimodal few-shot inputs (Issue #37943)\r\n\r\n[https://avatars.githubusercontent.com/u/100715397?s=20&v=4]zucchini-nlp left a comment (huggingface/transformers#37943)<https://github.com/huggingface/transformers/issues/37943#issuecomment-2849310287>\r\n\r\n@Canticle929<https://github.com/Canticle929> hey! You need to enable return_dict=True to get dictionary outputs. See documentation<https://huggingface.co/docs/transformers/v4.51.3/chat_templating_multimodal> for more examples\r\n\r\n―\r\nReply to this email directly, view it on GitHub<https://github.com/huggingface/transformers/issues/37943#issuecomment-2849310287>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BM7AKM23RT54MJE74R336ST24ZALJAVCNFSM6AAAAAB4MEFKDOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDQNBZGMYTAMRYG4>.\r\nYou are receiving this because you were mentioned.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37945,
    "title": "Alternative to trainer.hyperparameter_search for models used with custom optimizer / lrscheduler etc.",
    "author": "ieshaan12",
    "state": "closed",
    "created_at": "2025-05-04T00:26:04Z",
    "updated_at": "2025-06-11T08:02:41Z",
    "labels": [],
    "body": "\n    roberta_config = RobertaConfig(\n        hidden_size=HIDDEN_SIZE,\n        num_attention_heads=NUM_ATTENTION_HEADS,\n        num_hidden_layers=NUM_HIDDEN_LAYERS,\n        intermediate_size=INTERMEDIATE_SIZE,\n        hidden_dropout_prob=HIDDEN_DROPOUT_PROB,\n        max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n        type_vocab_size=TYPE_VOCAB_SIZE,\n        initializer_range=INITIALIZER_RANGE,\n        num_labels=NUM_LABELS\n    )\n \n    # Initialize Model from Scratch\n    model = RobertaForSequenceClassification(roberta_config)\n    \n    logger.info(model.config)\n \n    # Define Optimizer with Weight Decay\n    optimizer = torch.optim.AdamW(\n        [\n            {\"params\": [p for n, p in model.named_parameters() if \"bias\" not in n and \"LayerNorm\" not in n], \"weight_decay\": WEIGHT_DECAY},\n            {\"params\": [p for n, p in model.named_parameters() if \"bias\" in n or \"LayerNorm\" in n], \"weight_decay\": 0.0}\n        ],\n        lr=LEARNING_RATE,\n        eps=ADAM_EPS\n    )\n    \n    # Alter this as needed for class weights.\n    class_weights = torch.tensor([1.0, 12.0], dtype=torch.float)\n    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n \n    # Prepare with Accelerator\n    model, optimizer, train_dl, eval_dl, loss_fn = accelerator.prepare(\n        model, optimizer, train_dl, eval_dl, loss_fn\n    )\n    # Learning Rate Scheduler\n    total_training_steps = math.ceil(len(train_dl) * EPOCHS / GRADIENT_ACCUMULATION_STEPS)\n \n    lr_scheduler = CosineAnnealingWarmRestarts(\n        optimizer,\n        T_0=1,\n        T_mult=2,\n        eta_min=1e-6, # Minimum learning rate\n    )\n\n    model.train()\n    completed_steps = 0\n \n    for epoch in range(EPOCHS):\n        for step, batch in enumerate(train_dl):\n            # Forward pass\n            outputs = model(**batch)\n            # loss = outputs.loss\n            loss = loss_fn(outputs.logits, batch['labels'])\n \n            # Normalize loss\n            loss = loss / GRADIENT_ACCUMULATION_STEPS\n \n            # Backward pass\n            accelerator.backward(loss)\n \n            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or step == len(train_dl) - 1:\n                accelerator.clip_grad_norm_(model.parameters(), max_norm=MAX_NORM)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                completed_steps += 1\n \n                if completed_steps % LOG_STEPS == 0:\n                    progress = f\"Epoch: {epoch}, Step: {completed_steps}/{total_training_steps}, Loss: {loss.item() * GRADIENT_ACCUMULATION_STEPS}\"\n                    accelerator.print(progress)\n\nI have some code like above, which mostly allows me to configure different parameters with custom loss functions (shown here with a CrossEntropyLoss) along with Adam and torch's LRScheduler. I was wondering if there was something similar to `Trainer.hyperparameter_search` for custom components like this? \n\nOf course, I understand its a bit tricky to identify with custom components, but just wanted to know if its even possible, or would I need to manually configure several runs to arrive at the ideal values instead?\n\nThanks!",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @SunMarc "
      },
      {
        "user": "SunMarc",
        "body": "Hi @ieshaan12, our `hyperparameter_search` feature mainly comes from third party libraries such as optuna and raytune. \nsee list here: [OptunaBackend, RayTuneBackend, SigOptBackend, WandbBackend]. If there is a library that supports your use case, happy to integrate it ! "
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37947,
    "title": "Add examples that showcase the use of Hyperparameter search with Transformers",
    "author": "ParagEkbote",
    "state": "closed",
    "created_at": "2025-05-04T04:03:11Z",
    "updated_at": "2025-06-11T08:02:39Z",
    "labels": [],
    "body": "Hyperparameter search can result in a well-trained model as well as saving time and computational resources. I think we can add notebooks which can showcase the use of this feature to the [huggingface/notebooks](https://github.com/huggingface/notebooks) repo and link it to the [docs](https://huggingface.co/docs/transformers/en/hpo_train). \n\nCould you please let me know?\n\ncc: @stevhliu ",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 34855,
    "title": "Offline mode doesn't work with models that require `trust_remote_code=True`",
    "author": "VladKha",
    "state": "closed",
    "created_at": "2024-11-21T13:42:46Z",
    "updated_at": "2025-06-10T23:42:36Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\r\n\r\nGoogle Colab:\r\n- `transformers` version: 4.46.3\r\n- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.26.2\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.1.1\r\n- Accelerate config: \tnot found\r\n- PyTorch version (GPU?): 2.5.1+cu121 (False)\r\n- Tensorflow version (GPU?): 2.17.1 (False)\r\n- Flax version (CPU?/GPU?/TPU?): 0.8.5 (cpu)\r\n- Jax version: 0.4.33\r\n- JaxLib version: 0.4.33\r\n- Using distributed or parallel set-up in script?: no\r\n\r\n### Who can help?\r\n\r\n@Rocketknight1\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [x] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n#### Issue\r\nModels that require `trust_remote_code=True` can't be fully saved & loaded with `save_pretrained()` + `from_pretrained()`.\r\nIn [offline mode](https://huggingface.co/docs/transformers/v4.46.2/en/installation#offline-mode) on a new machine during calling `from_pretrained()` it doesn't locate all required files in the saved local dir and tries to reach out to hf hub for the remote code part.\r\n\r\n#### How to reproduce\r\n[Colab](https://colab.research.google.com/drive/1_yX94aJngMChp6pANWg6Z6F_9G8Kyb3k?usp=sharing) | [Kaggle](https://www.kaggle.com/code/vladyslavkha/hf-offline-mode-trust-remote-code-models-issue)\r\nTested with popular [`jinaai/jina-embeddings-v3`](https://huggingface.co/jinaai/jina-embeddings-v3)\r\nIncludes step by step reproduction + results\r\n\r\n---\r\n## Additional context\r\n- Stumbled on this in Kaggle Notebook env for competition.\r\nSome Kaggle competitions require submitting code in Kaggle Notebooks, which are run later on private data and **don't allow internet access**.\r\nPractically, this means you must prepare all models in advance, upload them as dependencies to the submission notebook.\r\nSo having transformers trying to reach out to hf-hub (when the model is already pre-downloaded) is not an option and disqualifies a group of models from usage.\r\n- Originally raised in https://github.com/UKPLab/sentence-transformers/issues/2613.\r\nReceived guidance by @tomaarsen in https://github.com/UKPLab/sentence-transformers/issues/2613#issuecomment-2076964416 to overcome the issue with `sentence-transformers` (code snippet included)\r\n- Raising as a bug report, LMK if better to reraise as FR. Would be happy to try to contribute if confirmed\r\n\r\n### Expected behavior\r\n\r\nModel is fully saved in local dir with `save_pretrained()` and can be fully loaded from a local path with `from_pretrained()` in offline mode",
    "comments": [
      {
        "user": "jsirex",
        "body": "The same issue happens when you fine-tune such models. It does not see (even try) to use local files, but tries to download them from a not existing repo and fails to load from cache.\r\n\r\nPotentially, workaround, like run some extra code, change cache directory might work in vm. But at platforms (like Azure ML, Databricks it really hard to workaround it)."
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @vladkha, firstly sorry for the delay, I'm not sure how I missed this issue before! It's definitely legit. The most likely cause is the `auto_map` entry in the model config referring to a specific Hub repo, which causes the code to redownload those files rather than using the saved files in the downloaded repo.\r\n\r\nIf you want to try to make a PR to fix this, go ahead, but be warned that the auto code can be quite complex!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38639,
    "title": "ImportError: cannot import name 'DTensor' from 'torch.distributed.tensor'",
    "author": "ybdong919",
    "state": "closed",
    "created_at": "2025-06-06T14:33:07Z",
    "updated_at": "2025-06-10T17:29:03Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntransformers/pytorch_utils.py\", line 300, in id_tensor_storage\n    if is_torch_greater_or_equal_than_2_0:\n        from torch.distributed.tensor import DTensor\n\nThe error \"ImportError: cannot import name 'dtensor' from 'torch.distributed.tensor'\" arises due to changes in the location of the DTensor class within PyTorch's distributed package, specifically after version 2.5. Not after 2.0.\nSo \"if is_torch_greater_or_equal_than_2_0: \" should be \"if is_torch_greater_or_equal_than_2_5:\"\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nTraceback (most recent call last):\n  File \"/blue/bphl-florida/dongyibo/PPML/Geneformer/geneformer2/Geneformer/cancer_type_classify.py\", line 25, in <module>\n    all_metrics = cc.validate(model_directory=\"/blue/bphl-florida/dongyibo/PPML/Geneformer/geneformer2/Geneformer/gf-12L-95M-i4096_CLcancer\",\n  File \"/blue/bphl-florida/dongyibo/PPML/Geneformer/geneformer2/Geneformer/geneformer/classifier.py\", line 794, in validate\n    trainer = self.train_classifier(\n  File \"/blue/bphl-florida/dongyibo/PPML/Geneformer/geneformer2/Geneformer/geneformer/classifier.py\", line 1282, in train_classifier\n    trainer.train()\n  File \"/blue/bphl-florida/dongyibo/conda/envs/DL/lib/python3.9/site-packages/transformers/trainer.py\", line 2240, in train\n    return inner_training_loop(\n  File \"/blue/bphl-florida/dongyibo/conda/envs/DL/lib/python3.9/site-packages/transformers/trainer.py\", line 2656, in _inner_training_loop\n    self._maybe_log_save_evaluate(\n  File \"/blue/bphl-florida/dongyibo/conda/envs/DL/lib/python3.9/site-packages/transformers/trainer.py\", line 3102, in _maybe_log_save_evaluate\n    self._save_checkpoint(model, trial)\n  File \"/blue/bphl-florida/dongyibo/conda/envs/DL/lib/python3.9/site-packages/transformers/trainer.py\", line 3199, in _save_checkpoint\n    self.save_model(output_dir, _internal_call=True)\n  File \"/blue/bphl-florida/dongyibo/conda/envs/DL/lib/python3.9/site-packages/transformers/trainer.py\", line 3911, in save_model\n    self._save(output_dir)\n  File \"/blue/bphl-florida/dongyibo/conda/envs/DL/lib/python3.9/site-packages/transformers/trainer.py\", line 4015, in _save\n    self.model.save_pretrained(\n  File \"/blue/bphl-florida/dongyibo/conda/envs/DL/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 3572, in save_pretrained\n    ptrs[id_tensor_storage(tensor)].append(name)\n  File \"/blue/bphl-florida/dongyibo/conda/envs/DL/lib/python3.9/site-packages/transformers/pytorch_utils.py\", line 300, in id_tensor_storage\n    from torch.distributed.tensor import DTensor\nImportError: cannot import name 'DTensor' from 'torch.distributed.tensor'\n\n### Expected behavior\n\npytorch_utils.py\", line 299, in id_tensor_storage\n\n\"if is_torch_greater_or_equal_than_2_0:\" should be \"if is_torch_greater_or_equal_than_2_5:\" ",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "@ybdong919 this makes sense, yes! Would you be willing to make a PR to update it?"
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @ybdong919 I just checked and I believe this is fixed on `main` already. Closing!"
      },
      {
        "user": "ybdong919",
        "body": "The issue happened in version 4.52.4 "
      }
    ]
  },
  {
    "issue_number": 35983,
    "title": "Add support for context parallelism",
    "author": "lewtun",
    "state": "open",
    "created_at": "2025-01-31T11:00:02Z",
    "updated_at": "2025-06-10T15:49:05Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nLong context models like [Qwen/Qwen2.5-7B-Instruct-1M](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M) have support for up to 1M tokens. However, fine-tuning such models in `transformers` leads to OOM errors and special methods like Ring Attention are needed. A similar issue arises during inference, where generating on a 1M prefill gives OOM.\n\nIt would be very exciting to have support for _context parallelism_ where in each layer we split the KQV computation across GPUs. \n\nAs far as an API goes, having something like `attn_implemention=\"ring\"` in `from_pretrained()` would likely be the simplest way to support this feature.\n\nLinks to papers and code:\n\n* Ring Attention: https://arxiv.org/abs/2310.01889\n* Reference code: https://github.com/zhuzilin/ring-flash-attention/blob/main/ring_flash_attn/adapters/hf_adapter.py\n* Picotron code: https://github.com/huggingface/picotron/blob/main/picotron/context_parallel/context_parallel.py\n\n### Motivation\n\nThe main motivation is two-fold: to support fine-tuning large context models and to enable online RL methods like GRPO to scale better in TRL (where we generate potentially large CoTs during training)\n\n### Your contribution\n\nHappy to review / test the feature on TRL side",
    "comments": [
      {
        "user": "jquesnelle",
        "body": "FSDPv2 (which is supported officially in Torch 2.5.1) has native support for Llama 3-style context parallelism. It's not \"true\" ring attention but works better under varlen (i.e. packed/masked SFT) training, so that could be an alternative avenue to consider. See their recent blog about it here: https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082"
      },
      {
        "user": "dgiofre",
        "body": "Hi @lewtun \nWhat about deepspeed ulysse integration in transformers for context parallelism?\nhttps://www.deepspeed.ai/tutorials/ds-sequence/#2-how-to-use-deepspeed-ulysses-in-your-application\n\nIt would be nice to fit it with current Deepspeed integration in Transformers, and it can work also with flash-attention-2 (+triton)\nhttps://www.deepspeed.ai/tutorials/ds-sequence/#2-how-to-use-deepspeed-ulysses-in-your-application\n\n\nreference code\nhttps://github.com/deepspeedai/DeepSpeedExamples/blob/uly-hf/post_training/sequence_parallelism/test_ulysses.py\n\nblog\nhttps://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md\n\npaper\nhttps://arxiv.org/pdf/2309.14509\n"
      },
      {
        "user": "ydshieh",
        "body": "I am still talking to internal members to get more familiar with this topic 🙏 \n"
      }
    ]
  },
  {
    "issue_number": 38710,
    "title": "There is no transformers version that can run DeepSeek V3 generate",
    "author": "pbelevich",
    "state": "open",
    "created_at": "2025-06-10T01:17:27Z",
    "updated_at": "2025-06-10T14:26:17Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nDeepSeek V3 generate uses get_max_length(removed in 4.49.0) and fp8 quantization(introduced in 4.49.0)\n\n1. DeepSeek 671B models use: past_key_values.get_max_length:\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/modeling_deepseek.py#L1654\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1/blob/main/modeling_deepseek.py#L1654\n\nIt was removed in 4.49.0: https://github.com/huggingface/transformers/commit/80dbbd103c217f422de91a3265bf6d8e8bc414f7\n\n2. DeepSeek 671B models use FP8 quantization:\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/config.json#L40\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1/blob/main/config.json#L40\n\nIt was introduced in 4.49.0: https://github.com/huggingface/transformers/commit/efe72fe21f4292e4f3a74344c0a065dc69480b3b\n\ncc @ArthurZucker \n\n### Who can help?\n\n< 4.49.0:\n```\n  File \"/opt/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3647, in from_pretrained\n    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/lib/python3.12/site-packages/transformers/quantizers/auto.py\", line 173, in merge_quantization_configs\n    quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/lib/python3.12/site-packages/transformers/quantizers/auto.py\", line 97, in from_dict\n    raise ValueError(\nValueError: Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet']\n``` \n\n>= 4.49.0:\n```\n  File \"/opt/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3550, in _sample\n    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/fsxl/belevich/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-R1/56d4cbbb4d29f4355bab4b9a39ccb717a14ad5ad/modeling_deepseek.py\", line 1654, in prepare_inputs_for_generation\n    max_cache_length = past_key_values.get_max_length()\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'DynamicCache' object has no attribute 'get_max_length'. Did you mean: 'get_seq_length'?\n```\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\ncall model.generate()\n\n### Expected behavior\n\nhave transformers version that can run deepseek v3 generate",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @gante @zucchini-nlp, maybe we can make a PR to update the custom code in the Deepseek repos?"
      },
      {
        "user": "zucchini-nlp",
        "body": "I found a PR already open in DeepSeek repo [here](https://huggingface.co/deepseek-ai/DeepSeek-V3/discussions/88), I will ping the authors so it gets merged soon\n\nAlso interesting that it doesn't work with versions `<v4.49` because I suppose there models should have been working previously. The DeepSeek repo suggests to use `v4.46.3`, so maybe with that version it will work"
      }
    ]
  },
  {
    "issue_number": 38275,
    "title": "Support for Normalized-GPT architecture",
    "author": "shan18",
    "state": "closed",
    "created_at": "2025-05-22T02:58:25Z",
    "updated_at": "2025-06-10T14:08:12Z",
    "labels": [
      "New model"
    ],
    "body": "### Model description\n\nNormalized Transformer (nGPT) is a novel architecture with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere.\n\nPaper: https://arxiv.org/abs/2410.01131\n\n### Open source status\n\n- [x] The model implementation is available\n- [x] The model weights are available\n\nWeights will be shared on HF soon.",
    "comments": []
  },
  {
    "issue_number": 38687,
    "title": "[RuntimeError: Expected all tensors to be on the same device, but found at least two devices] when fine-tuning with peft and device_map=auto",
    "author": "karoaper",
    "state": "open",
    "created_at": "2025-06-09T05:56:21Z",
    "updated_at": "2025-06-10T12:15:00Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n  transformers version: 4.52.4\n  Platform: linux\n  Python version: 3.10.16\n  Accelerate version: 1.7.0\n  PyTorch version: 2.6.0+cu124\n  peft version: 0.15.2\n  trl version: 0.18.1\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n\n```\nfrom datasets import load_dataset\nimport copy\nimport torch\n\nfrom peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nfrom transformers import TrainingArguments\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nfrom trl import SFTTrainer, SFTConfig\n\ninstruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")\ninstruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\").rename_column('response','completion')\n\ninstruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(5_000))\ninstruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200))\n\nnf4_config = BitsAndBytesConfig(\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.1\",\n    device_map='auto',\n    \n    quantization_config=nf4_config,\n    use_cache=False\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n\nargs = SFTConfig(\n  output_dir = \"mistral_instruct_generation\",\n  max_steps = 100, # comment out this line if you want to train in epochs\n  per_device_train_batch_size = 4,\n  warmup_ratio = 0.03,\n  logging_steps=10,\n  save_strategy=\"epoch\",\n  eval_strategy=\"steps\",\n  eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n  learning_rate=2e-4,\n  bf16=True,\n  lr_scheduler_type='constant',\n  max_seq_length=2048,\n  packing=True,\n  completion_only_loss = True,\n  dataset_num_proc=1\n)\n\ntrainer = MySFTTrainer(\n  model=model,\n  peft_config=peft_config,\n  processing_class=tokenizer,\n  args=args,\n  train_dataset=instruct_tune_dataset[\"train\"],\n  eval_dataset=instruct_tune_dataset[\"test\"]\n)\n\ntrainer.train()\n```\n\n### Expected behavior\n\nShould complete training",
    "comments": [
      {
        "user": "karoaper",
        "body": "Encountered the error when I tried to use device_map=\"auto\" with peft fine-tuning of mistral 7b.  I took the code from this tutorial: https://www.digitalocean.com/community/tutorials/mistral-7b-fine-tuning\n\nI received the error: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:0!\n\nThe error is here: \nFile ~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/loss/loss_utils.py:38, in fixed_cross_entropy(source, target, num_items_in_batch, ignore_index, **kwargs)\n     36 loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)\n     37 if reduction == \"sum\":\n---> 38     loss = loss / num_items_in_batch\n     39 return loss\n\nLooking at the device map, I see that the lm head is on cuda:7, but num_items_in_batch is on device cuda:0.  The problem seems to originate in function ForCausalLMLoss, where num_items_in_batch (cuda:0) is not switched to the same device as the logits (cuda:2)."
      },
      {
        "user": "karoaper",
        "body": "\nThe fix is simple.  Add the following 2 lines in `transformers/loss/loss_utils.py` in function ForCausalLMLoss before `loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)`\n\n```\nif num_items_in_batch is not None: \n     num_items_in_batch = num_items_in_batch.to(logits.device)\n```\n\n"
      },
      {
        "user": "Rocketknight1",
        "body": "cc @sunmarc @mekkcyber because there's a lot of trainer and quantization stuff going on here, which I suspect caused the device mismatch. I'm not sure if it's an issue with the script or if we actually do need to update `loss_utils`"
      }
    ]
  },
  {
    "issue_number": 33945,
    "title": "Automatic dynamic batch size selection for DataCollatorWithFlattening",
    "author": "alex-hh",
    "state": "open",
    "created_at": "2024-10-04T12:19:46Z",
    "updated_at": "2025-06-10T11:54:47Z",
    "labels": [
      "Usage",
      "Feature request"
    ],
    "body": "### Feature request\r\n\r\nAdd a custom (batch index) sampler to automatically determine batch size to a fixed target number of tokens.\r\n\r\n### Motivation\r\n\r\nI'm keen to try out DataCollatorWithFlattening but unsure about how to set batch size, since no padding will be added so the total number of tokens is dynamic.\r\n\r\nIm also uncertain whether fixing the total number of tokens is itself optimal...Does optimal memory allocation require accounting for the amount of attention masking that will be applied to the batch?\r\n\r\nIs there any recommendation on how to handle this currently?\r\n\r\n(Edit: seems like near-optimal solution for map-style datasets is provided by https://github.com/imoneoi/multipack_sampler/tree/master, which presumably just tries to ensure all batches are as full as possible given some max number of tokens. It would be nice to support similar functionality for Iterable Datasets - not optimal packing, but adjusting batch size to adapt to number of tokens in examples should be possible)\r\n\r\n### Your contribution\r\n\r\nMay be able to try to implement something for iterable datasets if this is possible.",
    "comments": [
      {
        "user": "snow-kartikbagalore",
        "body": "I want to second this. \r\nWhen I try to use this collator, each of my batch (now a single 'list') is of varying lengths (as expected). \r\nI am unable to decide on a batch_size value, but I want to keep the total length of the flattened output less than some large number, say 8192. Is it possible to have this behaviour today? "
      },
      {
        "user": "ArthurZucker",
        "body": "Not really an expert on this, so I am not sure I fully understand the motivation, flattening AFAIK should be more \"efficient\" and don't really see a reason to limit the length of the flattened output. \r\nBut if you need to pack your inputs, instead of ragging them, I think there is another data collator, which creates the appropriate batches based on the max length you have. \r\n\r\n "
      },
      {
        "user": "alex-hh",
        "body": "The reason for the max length would be to prevent out-of-memory issues (e.g. idea would be you know that your model can process some total number of tokens unmasked without oom, then packed batches must be selected to stay below this limit - and since you're not using padding this might involve a variable number of examples per packed batch).\r\n\r\nWhat's the other collator? Sounds useful"
      }
    ]
  },
  {
    "issue_number": 38725,
    "title": "`MoshiIntegrationTests` started to fail after #34464",
    "author": "ydshieh",
    "state": "open",
    "created_at": "2025-06-10T11:03:11Z",
    "updated_at": "2025-06-10T11:04:53Z",
    "labels": [
      "bug"
    ],
    "body": "`MoshiIntegrationTests` started to fail after #34464.\n\n### Reproduction\n\nFor example, \n\n> RUN_SLOW=1 python3 -m pytest -v tests/models/moshi/test_modeling_moshi.py::MoshiIntegrationTests::test_moshika_greedy_unconditional_fp16\n\nSince then, during 8 months, there are several periods where those tests are failing with different errors (not even to run the forward/generate). But when they could run, the output values are consistent with the output values given by #34464. Only before/after #34464, the output values are different.\n\n @gante Could you check if the changed outputs are expected?\n\nYour commit : 8a734ea2\n\nParent commit: 913330ca\n\n(You will need to run `pip install -e .` when checking out to old commits like these)\n",
    "comments": []
  },
  {
    "issue_number": 38541,
    "title": "`eager_attention_forward` and `repeat_kv` code duplication",
    "author": "ChengLyu",
    "state": "closed",
    "created_at": "2025-06-03T00:57:16Z",
    "updated_at": "2025-06-10T10:27:25Z",
    "labels": [],
    "body": "I see the two functions appear in a lot of places in the code base. Shall we unify them into a single place?\n\nAnd can we treat `eager_attention_forward` as another option in [`ALL_ATTENTION_FUNCTIONS`](https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L6186)? Any concerns?",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @Cyrilvallez for the attention refactor!"
      },
      {
        "user": "ChengLyu",
        "body": "I am happy to help the refactor if we agree to do so."
      },
      {
        "user": "Cyrilvallez",
        "body": "Hey @ChengLyu! The duplication of this code is by design - as per our model philosophy, we try to be as explicit as possible with the model definitions. So someone reading the code can immediately understand what is being done in the AttentionLayer. Then we allow other (faster) attention computation, but we want to keep the base one (eager) explicit in each model definition 🤗 This is why we did not incorporate `eager` to `ALL_ATTENTION_FUNCTIONS`\n\nFeel free to reopen if you have any other question!"
      }
    ]
  },
  {
    "issue_number": 37615,
    "title": "Getting Warnings When Instantiating Object Detection Models Due to Meta Tensor Initialization",
    "author": "HichTala",
    "state": "open",
    "created_at": "2025-04-18T13:40:57Z",
    "updated_at": "2025-06-10T10:23:08Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.51.3\n- Platform: Linux-6.11.0-21-generic-x86_64-with-glibc2.39\n- Python version: 3.12.4\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config: \t- compute_environment: LOCAL_MACHINE\n\t- distributed_type: NO\n\t- mixed_precision: no\n\t- use_cpu: False\n\t- debug: False\n\t- num_processes: 1\n\t- machine_rank: 0\n\t- num_machines: 1\n\t- gpu_ids: 1\n\t- rdzv_backend: static\n\t- same_network: True\n\t- main_training_function: main\n\t- enable_cpu_affinity: False\n\t- downcast_bf16: no\n\t- tpu_use_cluster: False\n\t- tpu_use_sudo: False\n\t- tpu_env: []\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA RTX 3500 Ada Generation Laptop GPU\n\n### Who can help?\n\n@amyeroberts, @qubvel\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nWhen instantiating object detection models (e.g., `facebook/detr-resnet-50`) using `AutoModelForObjectDetection`, PyTorch emits flooding warnings.\n\nMinimal Reproducible Example\n\n```python\nfrom transformers import AutoConfig, AutoModelForObjectDetection\n\nconfig = AutoConfig.from_pretrained(\n    \"facebook/detr-resnet-50\",\n)\n\nmodel = AutoModelForObjectDetection.from_pretrained(\n    \"facebook/detr-resnet-50\",\n    config=config,\n)\n```\n\nThe warning may be print when instantiating more models using auto mapping...\n\n### Expected behavior\n\nWhen instantiating object detection models (e.g., `facebook/detr-resnet-50`) using `AutoModelForObjectDetection`, PyTorch emits flooding warnings like:\n\n```shell\nUserWarning: for module.name.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n```\n\nFrom my understanding, the model is instantiated inside this `with` clause:\nhttps://github.com/huggingface/transformers/blob/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/src/transformers/modeling_utils.py#L4411-L4413\n\nAnd here one of the `model_init_context` element is this function:\nhttps://github.com/huggingface/transformers/blob/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/src/transformers/integrations/accelerate.py#L36-L67\n\nThat put the model in `meta` device, which means (from my understanding) that weights are placeholder tensors (they have shape/dtype but no real data). Later in the code, real weights are loaded using `state_dict`, with pytorch's `load_state_dict` function but without passing the flag `assign=True`.\n\nThe thing is pytorch's `load_state_dict` function is called in `timm`'s `_builder` function:\nhttps://github.com/huggingface/pytorch-image-models/blob/3ff38990264d7464ffd43ac2ddc26da04ff6355c/timm/models/_builder.py#L270\n\nI am wondering if I should open an issue in `timm` repo instead of this one?\n\nIn previous version of `transformer`, `model_init_context` list did not contains except when a variable, `low_cpu_mem_usage`, was `True`.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @qubvel @nielsrogge"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "qubvel",
        "body": "Hey @HichTala, thanks for opening a detailed issue and sorry for the delayed response! Indeed, any model with a `timm` backbone generates many warnings due to the reasons you described above. I see the following solutions:\n\n1. Disable weight loading for the backbone while loading the entire checkpoint.\n2. Check if we can use `assign=True` in `timm`.\n\nI'm not sure if there are any side effects for the second solution (cc @rwightman), but the first one seems to be logical, although it might be harder to implement. Let me know if have bandwidth to explore it 🤗 "
      }
    ]
  },
  {
    "issue_number": 38720,
    "title": "ModernBERT for Sequence Classification - issues with finetuning",
    "author": "98MM",
    "state": "open",
    "created_at": "2025-06-10T09:51:18Z",
    "updated_at": "2025-06-10T10:21:39Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntransformers version 4.53.0.dev0\nflash_attn version 2.7.4.post1 (via pip)\n\nUsing offical transformers image from: https://hub.docker.com/r/huggingface/transformers-pytorch-gpu\n\n### Who can help?\n\n@ArthurZucker\n@zach-huggingface\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nStandard BERT-like finetuning approach for NLI using AutoModelForSequenceCassification:\n\n```python\n\nnli = load_dataset('kiddothe2b/contract-nli', 'contractnli_a') # also tested with subset of sentence-transformer/all-nli\n\ndef tokenize_function(examples, tokenizer, task_inputs):\n    inps = [examples[inp] for inp in task_inputs]\n    tokenized = tokenizer(*inps, truncation=True)\n    return tokenized\n\n# tokenize the datasets ...\n\ntrain_dataset = nli['train'].map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['premise', 'hypothesis'],\n    fn_kwargs={\"tokenizer\":tokenizer, \"task_inputs\":['premise', 'hypothesis']})\n\ntokenizer=AutoTokenizer.from_pretrained('answerdotai/ModernBERT-base')\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n# compute metrics fn ...\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'answerdotai/ModernBERT-base',\n    num_labels=3,\n    classifier_pooling='cls',\n    torch_dtype='float16',\n    id2label=id2label,\n    label2id=label2id,\n    reference_compile=False # also tested without this setting\n).to('cuda') # tested this on L4, A100, A100_80GB, H100\n\ntrain_bsz, val_bsz = 16, 16\nlr = 2e-5\nbetas = (0.9, 0.98)\nn_epochs = 10\neps = 1e-6\nwd = 8e-6\n\ntraining_args = TrainingArguments(\n    output_dir='/out',\n    learning_rate=lr,\n    weight_decay=wd,\n    per_device_train_batch_size=train_bsz,\n    per_device_eval_batch_size=val_bsz,\n    num_train_epochs=n_epochs,\n    lr_scheduler_type=\"linear\",\n    optim=\"adamw_torch\",\n    adam_beta1=betas[0],\n    adam_beta2=betas[1],\n    adam_epsilon=eps,\n    logging_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```\n\n### Expected behavior\n\nI want to preface this that I first encountered this problem on my own pretrained ModernBERT implementation. After extensive testing on my version I decided to test the official  version (answerdotai/ModernBERT-base), where I encountered the same issue. \n\nMy base model works well on fill-mask tasks (via pipeline), outputting expected, correct words. After pretraining it achieved an accuracy of 0.86 and perplexity of 2.1 on an eval dataset. These results are why I decided to test the offical implementation as well.\n\nThis issue I encountered when attempting to use AutoModelForSequenceClassification / ModernBertForSequenceClassification on both my and the official version.\n\n\nIssue behaviour:\n\nTraining begins and runs. However, the prediction accuracy and F1 scorse indicate random guessing/very unstable training (with three classes macro accuracy and f1 hower around 0.33, while per-class values vary wildly from 0.00 up to even 0.91 per epoch, seemingly randomly).\n\nLooking at the embeddings themselves, they seem fine - no issues with NaN values (as raised [here](https://github.com/huggingface/transformers/issues/35574)). I've also tested masked word prediction (via fill-mask pipeline), which also output correct/expected predictions.\n\nExpected behaviour would be at least stable training, if not similar results as claimed in the original ModernBERT paper.",
    "comments": [
      {
        "user": "98MM",
        "body": "Upon further testing I believe the issue may be related to a mismatch in hiddenstate outputs, highlighted in https://github.com/huggingface/transformers/issues/38499\n"
      }
    ]
  },
  {
    "issue_number": 37461,
    "title": "Convnext image preprocessor raises an AssertionError when comparing logits",
    "author": "chandrusuresh",
    "state": "open",
    "created_at": "2025-04-12T02:14:43Z",
    "updated_at": "2025-06-10T09:57:41Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nConvnext image preprocessor raises an assert when checking the logits computed from image preprocessor against the expected logits [here](https://github.com/huggingface/transformers/blob/953196a43dae6a3c474165fba7d215fcbc7b7730/src/transformers/models/convnext/convert_convnext_to_pytorch.py#L187).\n\nAre the expected logits correct? What's the source of these values?\n\nNote: I found the bug in the context of working on a PR for #28180 in #37460.\n\n\n### Who can help?\n\n@NielsRogge \n\n### Reproduction\n\nTo reproduce this, simply run: `python3.12 src/transformers/models/convnext/convert_convnext_to_pytorch.py`  with appropriate arguments and this will raise an AssertionError as described above.\n\n### Expected behavior\n\nAll asserts should pass and the code should run without issues.",
    "comments": [
      {
        "user": "NielsRogge",
        "body": "Thanks for the ping, I checked the original repository, where they seem to [use](https://github.com/facebookresearch/ConvNeXt/blob/048efcea897d999aed302f2639b6270aedf8d4c8/main.py#L153) the default ImageNet mean and std.\n\nI just checked, I contributed this one 3 years ago. Here's the [inference script](https://github.com/NielsRogge/ConvNeXt/blob/understanding_convnext/inference.py) I used to obtain logits on the original implementation, and the corresponding [notebook](https://colab.research.google.com/drive/16iFf4eTHLZ0W185nyzEgBQRalIveTOWn?usp=sharing). There I use IMAGENET_DEFAULT_MEAN and IMAGENET_DEFAULT_STD to preprocess the image.\n\nI think the issue was that I only checked whether my model implementation and the original one returned the same logits on the same pixel values, but not necessarily whether the image processor itself returns the same `pixel_values` as the original Torchvision pipeline. When contributing models at a later time, I always included that in the conversion script, see e.g. [here](https://github.com/huggingface/transformers/blob/953196a43dae6a3c474165fba7d215fcbc7b7730/src/transformers/models/dinov2/convert_dinov2_to_hf.py#L208-L218) for DINOv2. It's a mistake I learned the hard way!"
      },
      {
        "user": "chandrusuresh",
        "body": "Thanks Niels. I appreciate the info and your response. If you are able to, I'd appreciate your review on #37460. The PR fixes this bug and partly resolves #28180."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38686,
    "title": "`Trainer._save()` May Incorrectly Save Empty Model State (safetensors)",
    "author": "ChenDaiwei-99",
    "state": "open",
    "created_at": "2025-06-09T05:36:59Z",
    "updated_at": "2025-06-10T09:48:44Z",
    "labels": [
      "bug"
    ],
    "body": "There appears to be a potential issue in the `save_model()` method of the `Trainer` class in the `Transformers` library.\n\nWhen the model inherits from `PreTrainedModel`, the _save() function follows [this logic](https://github.com/huggingface/transformers/blob/10627c1a0f6877ce6715b9537afe7fafb2a89edd/src/transformers/trainer.py#L4001-L4020). However, it is possible that the `state_dict` variable is None, which can result in saving an empty state dict.\n\nThis can happen in the following case:\n\n> `self.args.should_save` is True (as in lines [3913–3914](https://github.com/huggingface/transformers/blob/10627c1a0f6877ce6715b9537afe7fafb2a89edd/src/transformers/trainer.py#L3913-L3914))\n> \n> All preceding conditional branches (lines [3879–3911](https://github.com/huggingface/transformers/blob/10627c1a0f6877ce6715b9537afe7fafb2a89edd/src/transformers/trainer.py#L3879-L3911)) are false\n> \n> As a result, `self._save(output_dir)` is called without a state_dict, and state_dict remains None.\n\nI suggest moving the following lines (currently in line [4002–4003](https://github.com/huggingface/transformers/blob/10627c1a0f6877ce6715b9537afe7fafb2a89edd/src/transformers/trainer.py#L4002C1-L4003C53)):\n\n```\nif state_dict is None:\n    state_dict = self.model.state_dict()\n```\noutside the if-else block to ensure state_dict is always properly loaded.\n\n**Please let me know if this analysis is correct, and whether I can submit a PR to fix it. Thank you!**\n\n\n### Reproduction\n\n1. Define and initialize a custom model that inherits from PreTrainedModel.\n\n2. Instantiate a Trainer with the following TrainingArguments configuration:\n\n- load_best_model_at_end=True\n- fsdp disabled (is_fsdp_enabled=False)\n- deepspeed disabled (is_deepspeed_enabled=False)\n\n3. At the end of training, when the trainer tries to load the best model, it will return error msg that weights are missing.\n",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @SunMarc "
      },
      {
        "user": "SunMarc",
        "body": "> I suggest moving the following lines (currently in line [4002–4003](https://github.com/huggingface/transformers/blob/10627c1a0f6877ce6715b9537afe7fafb2a89edd/src/transformers/trainer.py#L4002C1-L4003C53)):\n> \n> if state_dict is None:\n>     state_dict = self.model.state_dict()\n> outside the if-else block to ensure state_dict is always properly loaded.\n\nThanks for the report @ChenDaiwei-99 ! \n\nIn your case it should go through the following path no ?          \n```python\nelse:\n            self.model.save_pretrained(\n                output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors\n            )\n```\n\nIn that case, it's not an issue if the state_dict passed is empty as we will get it in `save_pretrained` function. If you can share a minimal reproducer, I can have a quick look at why we are not saving the weights as expected. "
      }
    ]
  },
  {
    "issue_number": 38654,
    "title": "The visualization of image input in Qwen2.5-VL",
    "author": "Bytes-Lin",
    "state": "closed",
    "created_at": "2025-06-07T08:15:44Z",
    "updated_at": "2025-06-10T09:04:04Z",
    "labels": [],
    "body": "The image input of Qwen2.5-VL is processed by processor and then saved as tensor in inputs['pixel_values'].\nI tried to restore the image, using tensor in inputs['pixel_values'], but I found that the restored image patches were in disorder.\nSo how to restore the image from inputs['pixel_values'] in a proper way?\n\nFor example, the origin input image is as follows.\n![Image](https://github.com/user-attachments/assets/f40dd6e7-0774-4ad1-b921-73adc320a880)\nAnd failed to restore from the inputs['pixel_values'].\n![Image](https://github.com/user-attachments/assets/e1c9c0ff-d02a-49b0-af21-e98d080452d8)",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "Applying `[to_pil_tensor](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.functional.to_pil_image.html)` won't be enough here because the processors do normalization on top of that. In case of Qwen there's also splitting into patches and concatenating them in a flat list\n\nYou would probably need to apply `unnormalize` and reshape patches to a 3D image, though it doesn't mean you will get the same image as the input. The result will be a processed and resized image\n\nIs there any reason for doing backwards processing?"
      },
      {
        "user": "Bytes-Lin",
        "body": "@zucchini-nlp Thanks for answering. I have already found the way to restore the image from the processor.\nFirstly, before we use Qwen2.5-VL to inference, we use processor to deal with images and texts like this.\n\n![Image](https://github.com/user-attachments/assets/135de0b0-12d9-4494-9d85-26b921ac44a6)\n\nAnd the image is preprocessed into patches and put in inputs['pixel_values']. The patches are generated in this way, and you can find the source code in src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py\n\n\n![Image](https://github.com/user-attachments/assets/6dec52d7-23fb-4a3b-8df9-9c4098e0586e)\n\nSo, we can do the reverse operation like this to restore the image properly.\n\n![Image](https://github.com/user-attachments/assets/035ad14f-91ed-457d-8349-f62d34c33d48)\n\n![Image](https://github.com/user-attachments/assets/2bf960a3-3f8b-4354-aed6-8d33fdbbf797)\n\nAs for normalization, we can choose to use or not to use, it does not matter.\n\n![Image](https://github.com/user-attachments/assets/a7140adf-0f34-446c-915c-25d8bdfe3458)"
      }
    ]
  },
  {
    "issue_number": 35824,
    "title": "multi-gpu: test_model_parallel_beam_search tests fail with \"IndexError: list index out of range\"",
    "author": "dvrogozh",
    "state": "open",
    "created_at": "2025-01-21T22:52:06Z",
    "updated_at": "2025-06-10T08:04:08Z",
    "labels": [],
    "body": "With:\n* Transformers: 7d4b3ddde\n* https://github.com/huggingface/accelerate/commit/78b8126bff9882a66074a082cb2a80aa89026118\n* https://github.com/pytorch/pytorch/commit/4e4b8592a32f701b4974679ab1381ba7cccd4844 (torch 2.7 candidate)\n\nOn:\n* 2 card Intel(R) Data Center GPU Max 1550 (aka PVC), note: each card has 2 tiles, in total there are 4 torch devices available\n\n`test_model_parallel_beam_search` tests for the following models fail with \"IndexError: list index out of range\":\n```\n$ cat spec.py\nimport torch\nDEVICE_NAME = 'xpu'\nMANUAL_SEED_FN = torch.xpu.manual_seed\nEMPTY_CACHE_FN = torch.xpu.empty_cache\nDEVICE_COUNT_FN = torch.xpu.device_count\n\n$ TRANSFORMERS_TEST_DEVICE_SPEC=spec.py python3 -m pytest -k test_model_parallel_beam_search \\\n  tests/models/data2vec \\\n  tests/models/roberta \\\n  tests/models/roberta_prelayernorm \\\n  tests/models/xlm_roberta_xl\n...\nFAILED tests/models/data2vec/test_modeling_data2vec_text.py::Data2VecTextModelTest::test_model_parallel_beam_search - IndexError: list index out of range\nFAILED tests/models/roberta/test_modeling_roberta.py::RobertaModelTest::test_model_parallel_beam_search - IndexError: list index out of range\nFAILED tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py::RobertaPreLayerNormModelTest::test_model_parallel_beam_search - IndexError: list index out of range\nFAILED tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py::XLMRobertaXLModelTest::test_model_parallel_beam_search - IndexError: list index out of range\n```\n\nFailures in all failing cases are similar. Here is a full log for one of them:\n```\n$ TRANSFORMERS_TEST_DEVICE_SPEC=spec.py python3 -m pytest tests/models/data2vec/test_modeling_data2vec_text.py::Data2VecTextModelTest::test_model_parallel_beam_search\n======================================= test session starts ========================================\nplatform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\nrootdir: /home/dvrogozh/git/huggingface/transformers\nconfigfile: pyproject.toml\nplugins: anyio-4.8.0, rich-0.2.0, subtests-0.14.1, xdist-3.6.1, asyncio-0.23.8, timeout-2.3.1, hypothesis-6.122.3, reportlog-0.4.0, dash-2.18.2\nasyncio: mode=strict\ncollected 1 item\n\ntests/models/data2vec/test_modeling_data2vec_text.py F                                       [100%]\n\n============================================= FAILURES =============================================\n______________________ Data2VecTextModelTest.test_model_parallel_beam_search _______________________\n\nself = <tests.models.data2vec.test_modeling_data2vec_text.Data2VecTextModelTest testMethod=test_model_parallel_beam_search>\n\n    @require_accelerate\n    @require_torch_multi_accelerator\n    @pytest.mark.generate\n    def test_model_parallel_beam_search(self):\n        if \"xpu\" in torch_device:\n            if not (is_ipex_available(\"2.5\") or version.parse(torch.__version__) >= version.parse(\"2.6\")):\n                self.skipTest(reason=\"device_map='auto' does not work with XPU devices\")\n\n        for model_class in self.all_generative_model_classes:\n            if model_class._no_split_modules is None:\n                continue\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            model = model_class(config).eval()\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                model.cpu().save_pretrained(tmp_dir)\n>               new_model = model_class.from_pretrained(tmp_dir, device_map=\"auto\")\n\ntests/generation/test_utils.py:693:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/transformers/modeling_utils.py:4195: in from_pretrained\n    device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n../accelerate/src/accelerate/utils/modeling.py:1368: in infer_auto_device_map\n    module_size_with_ties, tied_module_names, tied_modules = get_module_size_with_ties(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ntied_params = ['lm_head.decoder.bias'], module_size = 396\nmodule_sizes = defaultdict(<class 'int'>, {'': 147892, 'data2vec_text': 143016, 'data2vec_text.embeddings': 88704, 'data2vec_text.emb...sition_ids': 4096, 'data2vec_text.embeddings.token_type_ids': 4096, 'lm_head.decoder': 0, 'lm_head.decoder.weight': 0})\nmodules_to_treat = [('lm_head.dense', Linear(in_features=32, out_features=32, bias=True)), ('lm_head.layer_norm', LayerNorm((32,), eps=1e-12, elementwise_affine=True))]\n\n    def get_module_size_with_ties(\n        tied_params,\n        module_size,\n        module_sizes,\n        modules_to_treat,\n    ) -> Tuple[int, List[str], List[nn.Module]]:\n        \"\"\"\n        Calculate the total size of a module, including its tied parameters.\n\n        Args:\n            tied_params (`List[str]`): The list of tied parameters.\n            module_size (`int`): The size of the module without tied parameters.\n            module_sizes (`Dict[str, int]`): A dictionary mapping each layer name to its size.\n            modules_to_treat (`List[Tuple[str, nn.Module]]`): The list of named modules to treat.\n\n        Returns:\n            `Tuple[int, List[str], List[nn.Module]]`: The total size of the module, the names of the tied modules, and the\n            tied modules.\n        \"\"\"\n        if len(tied_params) < 1:\n            return module_size, [], []\n        tied_module_names = []\n        tied_modules = []\n\n        for tied_param in tied_params:\n>           tied_module_index = [i for i, (n, _) in enumerate(modules_to_treat) if tied_param.startswith(n + \".\")][0]\nE           IndexError: list index out of range\n\n../accelerate/src/accelerate/utils/modeling.py:1129: IndexError\n--------------------------------------- Captured stderr call ---------------------------------------\nIf you want to use `Data2VecTextLMHeadModel` as a standalone, add `is_decoder=True.`\nIf you want to use `Data2VecTextLMHeadModel` as a standalone, add `is_decoder=True.`\n===================================== short test summary info ======================================\nFAILED tests/models/data2vec/test_modeling_data2vec_text.py::Data2VecTextModelTest::test_model_parallel_beam_search - IndexError: list index out of range\n======================================== 1 failed in 2.65s =========================================\n```\n\nObservations:\n1. Failures are sensitive to a number of GPUs across which `device_map=auto` works. Issue happens with 4 XPU devices. Issue does not happen with  XPU devices (run with `ZE_AFFINITY_MASK=0,1`).\n2. This calculation goes off:\n```\ntied_param=lm_head.decoder.bias\nmodules_to_treat=[('lm_head.dense', Linear(in_features=32, out_features=32, bias=True)), ('lm_head.layer_norm', LayerNorm((32,), eps=1e-12, elementwise_affine=True))]\n# which gives:\n[i for i, (n, _) in enumerate(modules_to_treat) if tied_param.startswith(n + \".\")]=[]\n# and taking index `[0]` eventually does not work\n```\n\nCC: @SunMarc @ydshieh @faaany ",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Seems like a generation thing, so cc @gante "
      },
      {
        "user": "ydshieh",
        "body": "The error occurs at\n\n> ../accelerate/src/accelerate/utils/modeling.py:1368: in infer_auto_device_map\n    module_size_with_ties, tied_module_names, tied_modules = get_module_size_with_ties(\n\nSo probably not for @gante but @SunMarc ?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38717,
    "title": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
    "author": "falali009",
    "state": "open",
    "created_at": "2025-06-10T07:02:48Z",
    "updated_at": "2025-06-10T07:02:48Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n  error info is： \n\nJoy_caption_two_advanced\n`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.\n\n\n# ComfyUI Error Report\n## Error Details\n- **Node ID:** 64\n- **Node Type:** Joy_caption_two_advanced\n- **Exception Type:** ValueError\n- **Exception Message:** `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.\n## Stack Trace\n```\n  File \"/home/falali/flux/ComfyUI/ComfyUI/execution.py\", line 349, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n\n  File \"/home/falali/flux/ComfyUI/ComfyUI/execution.py\", line 224, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n\n  File \"/home/falali/flux/ComfyUI/ComfyUI/execution.py\", line 196, in _map_node_over_list\n    process_inputs(input_dict, i)\n\n  File \"/home/falali/flux/ComfyUI/ComfyUI/execution.py\", line 185, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_slk_joy_caption_two/joy_caption_two_node.py\", line 546, in generate\n    text_model = joy_two_pipeline.llm.load_llm_model()\n\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_slk_joy_caption_two/joy_caption_two_node.py\", line 177, in load_llm_model\n    text_model = AutoModelForCausalLM.from_pretrained(text_model_path,\n\n  File \"/home/falali/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n\n  File \"/home/falali/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4015, in from_pretrained\n    dispatch_model(model, **device_map_kwargs)\n\n  File \"/home/falali/.local/lib/python3.10/site-packages/accelerate/big_modeling.py\", line 501, in dispatch_model\n    model.to(device)\n\n  File \"/home/falali/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2861, in to\n    raise ValueError(\n\n```\n## System Information\n- **ComfyUI Version:** 0.3.40\n- **Arguments:** main.py --listen 10.201.10.37\n- **OS:** posix\n- **Python Version:** 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\n- **Embedded Python:** false\n- **PyTorch Version:** 2.6.0+cu124\n## Devices\n\n- **Name:** cuda:0 NVIDIA GeForce RTX 4090 : cudaMallocAsync\n  - **Type:** cuda\n  - **VRAM Total:** 25386352640\n  - **VRAM Free:** 23902150840\n  - **Torch VRAM Total:** 1006632960\n  - **Torch VRAM Free:** 73588920\n\n## Logs\n```\n2025-06-10T14:58:02.194566 - \n2025-06-10T14:58:02.239783 - \u001b[34m[ComfyUI-Easy-Use] server: \u001b[0mv1.2.8 \u001b[92mLoaded\u001b[0m2025-06-10T14:58:02.239806 - \n2025-06-10T14:58:02.239820 - \u001b[34m[ComfyUI-Easy-Use] web root: \u001b[0m/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-easy-use/web_version/v2 \u001b[92mLoaded\u001b[0m2025-06-10T14:58:02.239829 - \n2025-06-10T14:58:02.248932 - Traceback (most recent call last):\n  File \"/home/falali/flux/ComfyUI/ComfyUI/nodes.py\", line 2124, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-tensorops/__init__.py\", line 1, in <module>\n    from .nodes import NODE_CLASS_MAPPINGS, NODE_DISPLAY_NAME_MAPPINGS\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-tensorops/nodes/__init__.py\", line 6, in <module>\n    from .save_to_s3 import SaveImageToS3\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-tensorops/nodes/save_to_s3.py\", line 2, in <module>\n    import boto3\nModuleNotFoundError: No module named 'boto3'\n\n2025-06-10T14:58:02.249078 - Cannot import /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-tensorops module for custom nodes: No module named 'boto3'\n2025-06-10T14:58:02.264805 - \u001b[36;20m[/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfy-mtb] | INFO -> loaded \u001b[96m94\u001b[0m nodes successfuly\u001b[0m\n2025-06-10T14:58:02.264980 - \u001b[36;20m[/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfy-mtb] | INFO -> Some nodes (2) could not be loaded. This can be ignored, but go to http://10.201.10.37:8188/mtb if you want more information.\u001b[0m\n2025-06-10T14:58:02.266087 - ### Loading: ComfyUI-Impact-Pack (V8.8.1)2025-06-10T14:58:02.266099 - \n2025-06-10T14:58:02.276373 - [Impact Pack] Wildcards loading done.2025-06-10T14:58:02.276396 - \n2025-06-10T14:58:02.278422 - Nvidia APEX normalization not installed, using PyTorch LayerNorm2025-06-10T14:58:02.278436 - \n2025-06-10T14:58:02.324117 - Traceback (most recent call last):\n  File \"/home/falali/flux/ComfyUI/ComfyUI/nodes.py\", line 2124, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-crewai/__init__.py\", line 1, in <module>\n    from .nodes.llm_ollama_node import LlmOllama\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-crewai/nodes/llm_ollama_node.py\", line 2, in <module>\n    from crewai import LLM\nModuleNotFoundError: No module named 'crewai'\n\n2025-06-10T14:58:02.324206 - Cannot import /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-crewai module for custom nodes: No module named 'crewai'\n2025-06-10T14:58:02.346286 - ComfyUI found: /home/falali/flux/ComfyUI/ComfyUI2025-06-10T14:58:02.346305 - \n2025-06-10T14:58:02.346318 - '/home/falali/flux/ComfyUI/ComfyUI' added to sys.path2025-06-10T14:58:02.346325 - \n2025-06-10T14:58:02.442416 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json\n2025-06-10T14:58:02.528532 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/github-stats.json\n2025-06-10T14:58:02.532860 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json\n2025-06-10T14:58:02.734109 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json\n2025-06-10T14:58:02.805116 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json\n2025-06-10T14:58:03.165716 - 导入 whisper 时出错：PortAudio library not found2025-06-10T14:58:03.165772 - \n2025-06-10T14:58:03.260693 - 导入 movie_editor 时出错：No module named 'moviepy.editor'2025-06-10T14:58:03.260713 - \n2025-06-10T14:58:03.316714 - 导入 dall_e 时出错：PortAudio library not found2025-06-10T14:58:03.316825 - \n2025-06-10T14:58:04.998892 - llama-cpp installed2025-06-10T14:58:04.998915 - \n2025-06-10T14:58:06.475125 - Successfully installed py-cord[voice]2025-06-10T14:58:06.475148 - \n2025-06-10T14:58:06.475637 - ### Loading: ComfyUI-Impact-Subpack (V1.2.9)\n2025-06-10T14:58:06.499286 - [Impact Subpack] ultralytics_bbox: /home/falali/flux/ComfyUI/ComfyUI/models/ultralytics/bbox\n2025-06-10T14:58:06.499332 - [Impact Subpack] ultralytics_segm: /home/falali/flux/ComfyUI/ComfyUI/models/ultralytics/segm\n2025-06-10T14:58:06.499576 - ASTERR config loaded successfully2025-06-10T14:58:06.499589 - \n2025-06-10T14:58:06.504610 - \u001b[36;20m[/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_controlnet_aux] | INFO -> Using ckpts path: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_controlnet_aux/ckpts\u001b[0m\n2025-06-10T14:58:06.504719 - \u001b[36;20m[/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_controlnet_aux] | INFO -> Using symlinks: False\u001b[0m\n2025-06-10T14:58:06.504789 - \u001b[36;20m[/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_controlnet_aux] | INFO -> Using ort providers: ['CUDAExecutionProvider', 'DirectMLExecutionProvider', 'OpenVINOExecutionProvider', 'ROCMExecutionProvider', 'CPUExecutionProvider', 'CoreMLExecutionProvider']\u001b[0m\n2025-06-10T14:58:06.512948 - Nvidia APEX normalization not installed, using PyTorch LayerNorm2025-06-10T14:58:06.512964 - \n2025-06-10T14:58:06.556636 - Web extensions folder found at /home/falali/flux/ComfyUI/ComfyUI/web/extensions/ComfyLiterals2025-06-10T14:58:06.556656 - \n2025-06-10T14:58:06.611706 - \u001b[34mPony Character Prompt Picker: \u001b[92mLoaded\u001b[0m2025-06-10T14:58:06.611726 - \n2025-06-10T14:58:06.634564 - \n\u001b[36mEfficiency Nodes:\u001b[0m Attempting to add Control Net options to the 'HiRes-Fix Script' Node (comfyui_controlnet_aux add-on)...\u001b[92mSuccess!\u001b[0m2025-06-10T14:58:06.634583 - \n2025-06-10T14:58:06.684678 - \u001b[34mWAS Node Suite: \u001b[0mBlenderNeko's Advanced CLIP Text Encode found, attempting to enable `CLIPTextEncode` support.\u001b[0m2025-06-10T14:58:06.684698 - \n2025-06-10T14:58:06.684731 - \u001b[34mWAS Node Suite: \u001b[0m`CLIPTextEncode (BlenderNeko Advanced + NSP)` node enabled under `WAS Suite/Conditioning` menu.\u001b[0m2025-06-10T14:58:06.684738 - \n2025-06-10T14:58:07.055776 - \u001b[34mWAS Node Suite: \u001b[0mOpenCV Python FFMPEG support is enabled\u001b[0m2025-06-10T14:58:07.055800 - \n2025-06-10T14:58:07.055821 - \u001b[34mWAS Node Suite \u001b[93mWarning: \u001b[0m`ffmpeg_bin_path` is not set in `/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/was-node-suite-comfyui/was_suite_config.json` config file. Will attempt to use system ffmpeg binaries if available.\u001b[0m2025-06-10T14:58:07.055851 - \n2025-06-10T14:58:07.432866 - \u001b[34mWAS Node Suite: \u001b[0mFinished.\u001b[0m \u001b[32mLoaded\u001b[0m \u001b[0m221\u001b[0m \u001b[32mnodes successfully.\u001b[0m2025-06-10T14:58:07.432889 - \n2025-06-10T14:58:07.432903 - \n\t\u001b[3m\u001b[93m\"Success usually comes to those who are too busy to be looking for it.\"\u001b[0m\u001b[3m - Henry David Thoreau\u001b[0m\n2025-06-10T14:58:07.432910 - \n2025-06-10T14:58:07.434387 - pruna_pro not installed, skipping2025-06-10T14:58:07.434400 - \n2025-06-10T14:58:07.434557 - Neither pruna_pro nor pruna are installed, skipping2025-06-10T14:58:07.434564 - \n2025-06-10T14:58:07.434777 - pruna_pro not installed, skipping2025-06-10T14:58:07.434785 - \n2025-06-10T14:58:07.434937 - Neither pruna_pro nor pruna are installed, skipping2025-06-10T14:58:07.434945 - \n2025-06-10T14:58:07.436459 - Warning: Could not load sageattention: No module named 'sageattention'2025-06-10T14:58:07.436471 - \n2025-06-10T14:58:07.436479 - sageattention package is not installed2025-06-10T14:58:07.436484 - \n2025-06-10T14:58:07.444099 - ------------------------------------------2025-06-10T14:58:07.444121 - \n2025-06-10T14:58:07.444129 - \u001b[34mComfyroll Studio v1.76 : \u001b[92m 175 Nodes Loaded\u001b[0m2025-06-10T14:58:07.444146 - \n2025-06-10T14:58:07.444152 - ------------------------------------------2025-06-10T14:58:07.444158 - \n2025-06-10T14:58:07.444164 - ** For changes, please see patch notes at https://github.com/Suzie1/ComfyUI_Comfyroll_CustomNodes/blob/main/Patch_Notes.md2025-06-10T14:58:07.444169 - \n2025-06-10T14:58:07.444175 - ** For help, please see the wiki at https://github.com/Suzie1/ComfyUI_Comfyroll_CustomNodes/wiki2025-06-10T14:58:07.444181 - \n2025-06-10T14:58:07.444187 - ------------------------------------------2025-06-10T14:58:07.444193 - \n2025-06-10T14:58:07.451426 - \n\u001b[0;33m[ReActor]\u001b[0m - \u001b[38;5;173mSTATUS\u001b[0m - \u001b[0;32mRunning v0.6.0-a1 in ComfyUI\u001b[0m2025-06-10T14:58:07.451441 - \n2025-06-10T14:58:07.462346 - Torch version: 2.6.0+cu1242025-06-10T14:58:07.462361 - \n2025-06-10T14:58:07.467590 - (pysssss:WD14Tagger) [DEBUG] Available ORT providers: TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider2025-06-10T14:58:07.467603 - \n2025-06-10T14:58:07.467611 - (pysssss:WD14Tagger) [DEBUG] Using ORT providers: CUDAExecutionProvider, CPUExecutionProvider2025-06-10T14:58:07.467617 - \n2025-06-10T14:58:07.841865 - Traceback (most recent call last):\n  File \"/home/falali/flux/ComfyUI/ComfyUI/nodes.py\", line 2124, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-if_ai_promptimagem/__init__.py\", line 14, in <module>\n    from .IFPromptImaGENNode import IFPROMPTImaGEN\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-if_ai_promptimagem/IFPromptImaGENNode.py\", line 13, in <module>\n    from .send_request import send_request\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-if_ai_promptimagem/send_request.py\", line 27, in <module>\n    from .transformers_api import TransformersModelManager\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-if_ai_promptimagem/transformers_api.py\", line 2, in <module>\n    from transformers import (\nImportError: cannot import name 'Qwen2VLForConditionalGeneration' from 'transformers' (/home/falali/.local/lib/python3.10/site-packages/transformers/__init__.py)\n\n2025-06-10T14:58:07.842053 - Cannot import /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-if_ai_promptimagem module for custom nodes: cannot import name 'Qwen2VLForConditionalGeneration' from 'transformers' (/home/falali/.local/lib/python3.10/site-packages/transformers/__init__.py)\n2025-06-10T14:58:07.851922 - Traceback (most recent call last):\n  File \"/home/falali/flux/ComfyUI/ComfyUI/nodes.py\", line 2124, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-sam2/__init__.py\", line 1, in <module>\n    from .node import SAM2ModelLoader, GroundingDinoModelLoader, GroundingDinoSAM2Segment, InvertMask, IsMaskEmptyNode\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-sam2/node.py\", line 15, in <module>\n    from sam2.build_sam import build_sam2\nModuleNotFoundError: No module named 'sam2.build_sam'\n\n2025-06-10T14:58:07.852009 - Cannot import /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-sam2 module for custom nodes: No module named 'sam2.build_sam'\n2025-06-10T14:58:08.019776 - --------------\n2025-06-10T14:58:08.019836 - \u001b[91m ### Mixlab Nodes: \u001b[93mLoaded\n2025-06-10T14:58:08.027065 - json_repair## OK2025-06-10T14:58:08.027081 - \n2025-06-10T14:58:08.027700 - ChatGPT.available True\n2025-06-10T14:58:08.027824 - edit_mask.available True\n2025-06-10T14:58:08.077079 - ## clip_interrogator_model not found: /home/falali/flux/ComfyUI/ComfyUI/models/clip_interrogator/Salesforce/blip-image-captioning-base, pls download from https://huggingface.co/Salesforce/blip-image-captioning-base2025-06-10T14:58:08.077100 - \n2025-06-10T14:58:08.077167 - ClipInterrogator.available True\n2025-06-10T14:58:08.077335 - ## text_generator_model not found: /home/falali/flux/ComfyUI/ComfyUI/models/prompt_generator/text2image-prompt-generator, pls download from https://huggingface.co/succinctly/text2image-prompt-generator/tree/main2025-06-10T14:58:08.077344 - \n2025-06-10T14:58:08.077354 - ## zh_en_model not found: /home/falali/flux/ComfyUI/ComfyUI/models/prompt_generator/opus-mt-zh-en, pls download from https://huggingface.co/Helsinki-NLP/opus-mt-zh-en/tree/main2025-06-10T14:58:08.077360 - \n2025-06-10T14:58:08.077549 - PromptGenerate.available True\n2025-06-10T14:58:08.077574 - ChinesePrompt.available True\n2025-06-10T14:58:08.077603 - RembgNode_.available True\n2025-06-10T14:58:08.143247 - TripoSR.available\n2025-06-10T14:58:08.143461 - MiniCPMNode.available\n2025-06-10T14:58:08.169474 - Scenedetect.available\n2025-06-10T14:58:08.200629 - FishSpeech.available\n2025-06-10T14:58:08.202656 - SenseVoice.available\n2025-06-10T14:58:08.211300 - Whisper.available False\n2025-06-10T14:58:08.211503 - fal-client## OK2025-06-10T14:58:08.211513 - \n2025-06-10T14:58:08.214822 - FalVideo.available\n2025-06-10T14:58:08.214865 - \u001b[93m -------------- \u001b[0m\n2025-06-10T14:58:08.218437 - ----------Jake Upgrade Nodes Loaded----------2025-06-10T14:58:08.218450 - \n2025-06-10T14:58:08.250690 - Use Proxy:2025-06-10T14:58:08.258488 -  2025-06-10T14:58:08.258569 - http://127.0.0.1:7897/2025-06-10T14:58:08.258674 - \n2025-06-10T14:58:08.260241 - Traceback (most recent call last):\n  File \"/home/falali/flux/ComfyUI/ComfyUI/nodes.py\", line 2124, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/Bjornulf_custom_nodes/__init__.py\", line 74, in <module>\n    from .ollama_talk import OllamaTalk\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/Bjornulf_custom_nodes/ollama_talk.py\", line 15, in <module>\n    import ollama\n  File \"/home/falali/.local/lib/python3.10/site-packages/ollama/__init__.py\", line 40, in <module>\n    _client = Client()\n  File \"/home/falali/.local/lib/python3.10/site-packages/ollama/_client.py\", line 114, in __init__\n    super().__init__(httpx.Client, host, **kwargs)\n  File \"/home/falali/.local/lib/python3.10/site-packages/ollama/_client.py\", line 91, in __init__\n    self._client = client(\n  File \"/home/falali/.local/lib/python3.10/site-packages/httpx/_client.py\", line 693, in __init__\n    proxy_map = self._get_proxy_map(proxies or proxy, allow_env_proxies)\n  File \"/home/falali/.local/lib/python3.10/site-packages/httpx/_client.py\", line 218, in _get_proxy_map\n    return {\n  File \"/home/falali/.local/lib/python3.10/site-packages/httpx/_client.py\", line 219, in <dictcomp>\n    key: None if url is None else Proxy(url=url)\n  File \"/home/falali/.local/lib/python3.10/site-packages/httpx/_config.py\", line 338, in __init__\n    raise ValueError(f\"Unknown scheme for proxy URL {url!r}\")\nValueError: Unknown scheme for proxy URL URL('socks://127.0.0.1:7897/')\n\n2025-06-10T14:58:08.260551 - Cannot import /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/Bjornulf_custom_nodes module for custom nodes: Unknown scheme for proxy URL URL('socks://127.0.0.1:7897/')\n2025-06-10T14:58:08.263666 - Total VRAM 24210 MB, total RAM 64068 MB\n2025-06-10T14:58:08.263705 - pytorch version: 2.6.0+cu124\n2025-06-10T14:58:08.263739 - xformers version: 0.0.29.post3\n2025-06-10T14:58:08.263789 - Set vram state to: NORMAL_VRAM\n2025-06-10T14:58:08.263845 - Device: cuda:0 NVIDIA GeForce RTX 4090 : cudaMallocAsync\n2025-06-10T14:58:08.668338 - \nImport times for custom nodes:\n2025-06-10T14:58:08.668409 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/websocket_image_save.py\n2025-06-10T14:58:08.668431 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-PonyCharacterPrompt\n2025-06-10T14:58:08.668449 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-WanVideoKsampler\n2025-06-10T14:58:08.668465 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI_AdvancedRefluxControl\n2025-06-10T14:58:08.668480 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-img2drawingassistants\n2025-06-10T14:58:08.668494 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_ttp_toolset\n2025-06-10T14:58:08.668507 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-inpaint-cropandstitch\n2025-06-10T14:58:08.668521 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/cg-use-everywhere\n2025-06-10T14:58:08.668535 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/asterr\n2025-06-10T14:58:08.668549 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI_ADV_CLIP_emb\n2025-06-10T14:58:08.668562 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_faceanalysis\n2025-06-10T14:58:08.668576 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-UltimateSDUpscale-GGUF\n2025-06-10T14:58:08.668589 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-wd14-tagger\n2025-06-10T14:58:08.668601 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-YOLO\n2025-06-10T14:58:08.668615 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_zenid\n2025-06-10T14:58:08.668628 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/aurasr-comfyui\n2025-06-10T14:58:08.668642 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_instantid\n2025-06-10T14:58:08.668655 -    0.0 seconds (IMPORT FAILED): /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-sam2\n2025-06-10T14:58:08.668672 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-GGUF\n2025-06-10T14:58:08.668690 -    0.0 seconds (IMPORT FAILED): /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-crewai\n2025-06-10T14:58:08.668704 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-template-loader\n2025-06-10T14:58:08.668721 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyLiterals\n2025-06-10T14:58:08.668735 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfy-image-saver\n2025-06-10T14:58:08.668747 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI_pruna\n2025-06-10T14:58:08.668760 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-custom-scripts\n2025-06-10T14:58:08.668773 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/teacache\n2025-06-10T14:58:08.668785 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-Jjk-Nodes\n2025-06-10T14:58:08.668797 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_patches_ll\n2025-06-10T14:58:08.668810 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-QualityOfLifeSuit_Omar92\n2025-06-10T14:58:08.668822 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-WanSeamlessFlow\n2025-06-10T14:58:08.668838 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_essentials\n2025-06-10T14:58:08.668853 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-advancedliveportrait\n2025-06-10T14:58:08.668891 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/derfuu_comfyui_moddednodes\n2025-06-10T14:58:08.668903 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-frame-interpolation\n2025-06-10T14:58:08.668916 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-brushnet\n2025-06-10T14:58:08.668929 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-segment-anything-2\n2025-06-10T14:58:08.668943 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/rgthree-comfy\n2025-06-10T14:58:08.668955 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-jakeupgrade\n2025-06-10T14:58:08.668969 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-advanced-controlnet\n2025-06-10T14:58:08.668982 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_slk_joy_caption_two\n2025-06-10T14:58:08.668995 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\n2025-06-10T14:58:08.669008 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI_UltimateSDUpscale\n2025-06-10T14:58:08.669020 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/gguf\n2025-06-10T14:58:08.669033 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_ipadapter_plus\n2025-06-10T14:58:08.669045 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI_Comfyroll_CustomNodes-main\n2025-06-10T14:58:08.669058 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI_MiniCPM-V-2_6-int4\n2025-06-10T14:58:08.669070 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_controlnet_aux\n2025-06-10T14:58:08.669082 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/efficiency-nodes-comfyui\n2025-06-10T14:58:08.669094 -    0.0 seconds (IMPORT FAILED): /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-tensorops\n2025-06-10T14:58:08.669108 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-impact-pack\n2025-06-10T14:58:08.669123 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_segment_anything\n2025-06-10T14:58:08.669135 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-Crystools\n2025-06-10T14:58:08.669147 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/Comfyui_Redux_Advanced\n2025-06-10T14:58:08.669160 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-kjnodes\n2025-06-10T14:58:08.669173 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfy-mtb\n2025-06-10T14:58:08.669185 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-reactor\n2025-06-10T14:58:08.669198 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_layerstyle\n2025-06-10T14:58:08.669211 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-to-python-extension\n2025-06-10T14:58:08.669223 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-impact-subpack\n2025-06-10T14:58:08.669236 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-videohelpersuite\n2025-06-10T14:58:08.669248 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-manager\n2025-06-10T14:58:08.669260 -    0.0 seconds (IMPORT FAILED): /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/Bjornulf_custom_nodes\n2025-06-10T14:58:08.669273 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-easy-use\n2025-06-10T14:58:08.669286 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-PuLID-Flux-Enhanced\n2025-06-10T14:58:08.669298 -    0.0 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/pulid_comfyui\n2025-06-10T14:58:08.669310 -    0.1 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-florence2\n2025-06-10T14:58:08.669324 -    0.2 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-dynamicprompts\n2025-06-10T14:58:08.669338 -    0.3 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/ComfyUI-BrushNet-Wrapper\n2025-06-10T14:58:08.669352 -    0.4 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-mixlab-nodes\n2025-06-10T14:58:08.669366 -    0.4 seconds (IMPORT FAILED): /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-if_ai_promptimagem\n2025-06-10T14:58:08.669381 -    0.4 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/bjornulf_custom_nodes\n2025-06-10T14:58:08.669395 -    0.8 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/was-node-suite-comfyui\n2025-06-10T14:58:08.669409 -    2.2 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_pulid_flux_ll\n2025-06-10T14:58:08.669421 -    4.1 seconds: /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_llm_party\n2025-06-10T14:58:08.669435 - \n2025-06-10T14:58:08.679847 - ********** ERROR ***********\n\ncomfyui-workflow-templates is not installed.\n\nPlease install the updated requirements.txt file by running:\n/usr/bin/python3 -m pip install -r /home/falali/flux/ComfyUI/ComfyUI/requirements.txt\n\nThis error is happening because the ComfyUI frontend is no longer shipped as part of the main repo but as a pip package instead.\n\nIf you are on the portable package you can run: update\\update_comfyui.bat to solve this problem\n\n********** ERROR ***********\n2025-06-10T14:58:08.680055 - comfyui-embedded-docs package not found\n2025-06-10T14:58:08.680541 - Starting server\n\n2025-06-10T14:58:08.680685 - To see the GUI go to: http://10.201.10.37:8188\n2025-06-10T14:58:09.124814 - FETCH ComfyRegistry Data: 5/882025-06-10T14:58:09.124914 - \n2025-06-10T14:58:09.626135 - Use Proxy:2025-06-10T14:58:09.626214 -  2025-06-10T14:58:09.626245 - http://127.0.0.1:7897/2025-06-10T14:58:09.626277 - \n2025-06-10T14:58:11.038091 - Use Proxy:2025-06-10T14:58:11.038172 -  2025-06-10T14:58:11.038204 - http://127.0.0.1:7897/2025-06-10T14:58:11.038238 - \n2025-06-10T14:58:12.399041 - /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-mixlab-nodes/webApp/lib/photoswipe-lightbox.esm.min.js2025-06-10T14:58:12.399076 - \n2025-06-10T14:58:12.408340 - /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-mixlab-nodes/webApp/lib/pickr.min.js2025-06-10T14:58:12.408379 - \n2025-06-10T14:58:12.433202 - /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-mixlab-nodes/webApp/lib/photoswipe.min.css2025-06-10T14:58:12.433262 - \n2025-06-10T14:58:12.453382 - /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-mixlab-nodes/webApp/lib/classic.min.css2025-06-10T14:58:12.453441 - \n2025-06-10T14:58:12.478580 - /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-mixlab-nodes/webApp/lib/model-viewer.min.js2025-06-10T14:58:12.478595 - \n2025-06-10T14:58:12.480704 - /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-mixlab-nodes/webApp/lib/juxtapose.css2025-06-10T14:58:12.480717 - \n2025-06-10T14:58:12.480836 - /home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui-mixlab-nodes/webApp/lib/juxtapose.min.js2025-06-10T14:58:12.480845 - \n2025-06-10T14:58:12.488751 - Use Proxy:2025-06-10T14:58:12.488764 -  2025-06-10T14:58:12.488771 - http://127.0.0.1:7897/2025-06-10T14:58:12.488777 - \n2025-06-10T14:58:12.571749 - \u001b[33mQualityOfLifeSuit_Omar92:\u001b[0m:NSP ready2025-06-10T14:58:12.571767 - \n2025-06-10T14:58:13.827379 - Use Proxy:2025-06-10T14:58:13.827462 -  2025-06-10T14:58:13.827504 - http://127.0.0.1:7897/2025-06-10T14:58:13.827540 - \n2025-06-10T14:58:15.157581 - Use Proxy:2025-06-10T14:58:15.157661 -  2025-06-10T14:58:15.157694 - http://127.0.0.1:7897/2025-06-10T14:58:15.157727 - \n2025-06-10T14:58:15.998713 - FETCH ComfyRegistry Data: 10/882025-06-10T14:58:15.998791 - \n2025-06-10T14:58:16.499447 - Use Proxy:2025-06-10T14:58:16.499517 -  2025-06-10T14:58:16.499551 - http://127.0.0.1:7897/2025-06-10T14:58:16.499585 - \n2025-06-10T14:58:17.272994 - got prompt\n2025-06-10T14:58:17.317800 - Using xformers attention in VAE\n2025-06-10T14:58:17.318617 - Using xformers attention in VAE\n2025-06-10T14:58:17.451959 - VAE load device: cuda:0, offload device: cpu, dtype: torch.bfloat16\n2025-06-10T14:58:17.479966 - /home/falali/flux/ComfyUI/ComfyUI/models/clip/siglip-so400m-patch14-3842025-06-10T14:58:17.479993 - \n2025-06-10T14:58:17.808593 - Use Proxy:2025-06-10T14:58:17.808630 -  2025-06-10T14:58:17.808647 - http://127.0.0.1:7897/2025-06-10T14:58:17.808662 - \n2025-06-10T14:58:18.169537 - Loading VLM's custom vision model2025-06-10T14:58:18.169562 - \n2025-06-10T14:58:18.767697 - Prompt: Write a long descriptive caption for this image in a formal tone. If there is a person/character in the image you must refer to them as .2025-06-10T14:58:18.767724 - \n2025-06-10T14:58:18.814337 - Requested to load SiglipVisionTransformer\n2025-06-10T14:58:18.890225 - loaded completely 9.5367431640625e+25 809.1729736328125 True\n2025-06-10T14:58:19.004774 - Requested to load ImageAdapter\n2025-06-10T14:58:19.008798 - loaded completely 9.5367431640625e+25 41.0390625 True\n2025-06-10T14:58:19.010460 - Loading tokenizer2025-06-10T14:58:19.010476 - \n2025-06-10T14:58:19.134628 - Use Proxy:2025-06-10T14:58:19.134649 -  2025-06-10T14:58:19.134656 - http://127.0.0.1:7897/2025-06-10T14:58:19.134663 - \n2025-06-10T14:58:19.135599 - Loading LLM: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit2025-06-10T14:58:19.135627 - \n2025-06-10T14:58:19.135641 - /home/falali/flux/ComfyUI/ComfyUI/models/LLM/Meta-Llama-3.1-8B-Instruct-bnb-4bit2025-06-10T14:58:19.135648 - \n2025-06-10T14:58:19.135803 - Successfully modified 'base_model_name_or_path' value in '/home/falali/flux/ComfyUI/ComfyUI/models/Joy_caption_two/text_model/adapter_config.json'.2025-06-10T14:58:19.135813 - \n2025-06-10T14:58:20.443970 - Use Proxy:2025-06-10T14:58:20.444002 -  2025-06-10T14:58:20.444009 - http://127.0.0.1:7897/2025-06-10T14:58:20.444028 - \n2025-06-10T14:58:21.037028 - !!! Exception during processing !!! `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.\n2025-06-10T14:58:21.037817 - Traceback (most recent call last):\n  File \"/home/falali/flux/ComfyUI/ComfyUI/execution.py\", line 349, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n  File \"/home/falali/flux/ComfyUI/ComfyUI/execution.py\", line 224, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n  File \"/home/falali/flux/ComfyUI/ComfyUI/execution.py\", line 196, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"/home/falali/flux/ComfyUI/ComfyUI/execution.py\", line 185, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_slk_joy_caption_two/joy_caption_two_node.py\", line 546, in generate\n    text_model = joy_two_pipeline.llm.load_llm_model()\n  File \"/home/falali/flux/ComfyUI/ComfyUI/custom_nodes/comfyui_slk_joy_caption_two/joy_caption_two_node.py\", line 177, in load_llm_model\n    text_model = AutoModelForCausalLM.from_pretrained(text_model_path,\n  File \"/home/falali/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/falali/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4015, in from_pretrained\n    dispatch_model(model, **device_map_kwargs)\n  File \"/home/falali/.local/lib/python3.10/site-packages/accelerate/big_modeling.py\", line 501, in dispatch_model\n    model.to(device)\n  File \"/home/falali/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2861, in to\n    raise ValueError(\nValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.\n\n2025-06-10T14:58:21.038033 - Prompt executed in 3.76 seconds\n\n```\n## Attached Workflow\nPlease make sure that workflow does not contain any sensitive information such as API keys or passwords.\n```\nWorkflow too large. Please manually upload the workflow from local file system.\n```\n\n## Additional Context\n(Please add any additional context or steps to reproduce the error here)\n\n\n### Who can help?\n\n![Image](https://github.com/user-attachments/assets/28702556-60d4-46dd-86d5-d214ab9b39d2)\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n![Image](https://github.com/user-attachments/assets/cef17280-a9ed-41fb-a70f-131f642c8018)\n\n### Expected behavior\n\nhope it run ",
    "comments": []
  },
  {
    "issue_number": 38032,
    "title": "Removing the modification of loss value due to rounding off to 4 digits",
    "author": "harish6696",
    "state": "open",
    "created_at": "2025-05-09T00:42:01Z",
    "updated_at": "2025-06-10T04:01:39Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntransformers version: 4.50.2\npython version: 3.13.1\n\n### Who can help?\n\n@zach-huggingface @SunMarc \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nInside the Trainer class, why is the loss rounded to 4 digits? I have applications where I am interested to see the loss go below 4 significant digits, but they all get rounded to 0. Please let the user set this rounding number, or let the loss be displayed in scientific notation like 1.xxxe^y\n\nthis is set inside def _maybe_log_save_evaluate()\nlogs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n\n\n\n\n### Expected behavior\n\nIt would be great if this hardcoding of rounding off the \"loss\" is removed. Best output would be to remove the round function and log the loss value in scientific notation which is much clearer.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "I think we could accept a PR for that! Ideally we'd keep the old format for losses > 1e-4 and then switch to scientific notation with 4 (decimal) digits of the significand below that? Please wait for @SunMarc to confirm, though!"
      },
      {
        "user": "harish6696",
        "body": "Thanks for the quick reply, @Rocketknight1. Yes, exactly, as soon as the loss <1e-4, we can switch to scientific notation with 4 significant digits in the mantissa and introduce an exponent. Looking forward to hearing the thoughts from @SunMarc as well"
      },
      {
        "user": "SunMarc",
        "body": "Happy to have this feature ! Would you like to give it a try @harish6696 ? This rounding was mainly introduced to have cleaner logs https://github.com/huggingface/transformers/pull/9491. Let's also check that these can be correctly interpreted by third party libraries like wandb. "
      }
    ]
  },
  {
    "issue_number": 38657,
    "title": "please develop transformers java/scala  sdk,eagerly to use!",
    "author": "mullerhai",
    "state": "open",
    "created_at": "2025-06-07T14:07:24Z",
    "updated_at": "2025-06-10T02:56:28Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nTransformers have become de facto essential deep learning tools. However, you only provide interface APIs for three languages: Python, JavaScript, and Rust. Have you forgotten about Java, Scala, and Kotlin? Is it because your team lacks manpower or isn't proficient in the JVM? Currently, there are also PyTorch bindings for Java and Scala, such as javacpp - pytorch and storch. Everyone wants to use Transformers, yet you haven't overcome this obstacle for so many years. Please tell your technical director or CTO that we need Java and Scala libraries for Transformers, which can directly download model datasets and Tokenizers and perform model training. Please make sure to solve this issue this year. Thank you\n\n### Motivation\n\ndd\n\n### Your contribution\n\ndd",
    "comments": [
      {
        "user": "AshAnand34",
        "body": "Hi @mullerhai,\n\nI'm a new contributor to the Hugging Face community, and I came across your issue while exploring the codebase. I understand your frustration about the lack of Java/Scala support in Transformers. You're absolutely right that JVM languages are widely used in enterprise environments, and having native support would be valuable for many users. As someone who's just starting to contribute, I'd like to help move this forward and learn from your experience.\n\nI see you've mentioned existing PyTorch bindings like javacpp-pytorch and storch. To help us prioritize and implement this feature effectively, could you share:\n\n1. In your current setup:\n   - Are you using any of these existing bindings (javacpp-pytorch, storch) in production?\n   - What specific limitations are you encountering with the current solutions?\n\n2. For your use case:\n   - Which specific models are you most interested in using?\n   - Do you need both training and inference capabilities?\n   - Are you working with any specific JVM-based ML frameworks that we should ensure compatibility with?\n\n3. Regarding deployment:\n   - Are you running this in a production environment?\n   - What are your current performance requirements (throughput, latency, etc.)?\n\nAs a new community member, I'm particularly interested in understanding the real-world needs of JVM users like yourself. Your experience with the current limitations would be invaluable in helping me and others shape this implementation effectively.\n\nThank you for your patience and for helping me learn more about the community's needs!"
      },
      {
        "user": "mullerhai",
        "body": "> Hi [@mullerhai](https://github.com/mullerhai),\n> \n> I'm a new contributor to the Hugging Face community, and I came across your issue while exploring the codebase. I understand your frustration about the lack of Java/Scala support in Transformers. You're absolutely right that JVM languages are widely used in enterprise environments, and having native support would be valuable for many users. As someone who's just starting to contribute, I'd like to help move this forward and learn from your experience.\n> \n> I see you've mentioned existing PyTorch bindings like javacpp-pytorch and storch. To help us prioritize and implement this feature effectively, could you share:\n> \n> 1. In your current setup:\n>    \n>    * Are you using any of these existing bindings (javacpp-pytorch, storch) in production?\n>    * What specific limitations are you encountering with the current solutions?\n> 2. For your use case:\n>    \n>    * Which specific models are you most interested in using?\n>    * Do you need both training and inference capabilities?\n>    * Are you working with any specific JVM-based ML frameworks that we should ensure compatibility with?\n> 3. Regarding deployment:\n>    \n>    * Are you running this in a production environment?\n>    * What are your current performance requirements (throughput, latency, etc.)?\n> \n> As a new community member, I'm particularly interested in understanding the real-world needs of JVM users like yourself. Your experience with the current limitations would be invaluable in helping me and others shape this implementation effectively.\n> \n> Thank you for your patience and for helping me learn more about the community's needs!\n\nvery tnanks , it is excited!  please make it realable for transformers  in jvm scala3!\n1. usage\n\nI am already using Storch in production. You can check my repository storch-recommend. Our recommendation system has started using Storch.\n\nStorch relies on JavaCPP Presets for PyTorch. Let me briefly introduce it: JavaCPP Presets for PyTorch is a complete mapping of C++ LibTorch, generating glue Java code similar to JNA/JNI. It provides low-level APIs, which are cumbersome to use and lack documentation. If you haven't used it, you may encounter unexpected failures due to a lack of relevant knowledge, especially when configuring hyperparameters for some network layers. There are multiple ways to set values, and a wrong setting will prevent effective value transmission. We encountered this issue during the development of Storch and wasted a lot of time. We also found bugs in the hyperparameter configuration of several padding network layers, but JavaCPP hasn't fixed them. However, they provide most of the effective tensor operators and PyTorch methods, which we have encapsulated in Storch. In the future, I also suggest you communicate with the JavaCPP official responsible person, Sadnet. They plan to release bindings based on LibTorch 2.7, but they are currently encountering compilation issues with CUDA 12.9 in JavaCPP. Yes, they even use JavaCPP to generate JNI bindings for CUDA. Additionally, JavaCPP Presets for PyTorch provides dataset, DataLoader, and sampler, which we have also encapsulated. We found during testing that if the system's GCC and clib versions are too low, it will report errors. Generally, a higher-version Ubuntu system is used. Another important point is that JavaCPP's distributed training currently only supports Gloo, while CUDA for the standalone version is supported. I believe this is an issue that needs to be addressed in the future, and we need to communicate with JavaCPP together. JavaCPP refers to the Java binding of LibTorch. Some operators and network layers in PyTorch are not available in LibTorch, and therefore not in JavaCPP. But if we know the principles, we can implement them ourselves.\n\nStorch is a secondary development of LibTorch bindings for Scala 3 based on JavaCPP Presets for PyTorch. Storch provides more user-friendly and flexible APIs, combining the type system and functional programming to make development closer to Python. Even Python code can be directly used in Storch with just a few lines of code modified. You can refer to the code in my storch-tutorial repository, which is very beneficial for migrating from Python to Scala 3 Storch. Storch can provide an enterprise-level model inference platform and train faster than Python. Additionally, the PT model files trained by Python PyTorch cannot be directly loaded in Storch. They need to be converted to TorchScript using torch.jit.trace. I think this is quite inconvenient. It seems that using the PT model in C++ LibTorch also requires conversion to TorchScript.\n\nThere are many advantages of using Scala 3. Java requires writing a lot of cumbersome boilerplate code. It is more suitable to prioritize the development of transformers in the Scala 3 version. Now, there are a large number of Scala 3 developers in Germany, the United States, and India. In the future, only simple Java API bindings need to be generated.\n\nAdditionally, note that Scala 3 should be used preferably, and Scala 2.12 and Scala 2.13 should be avoided as much as possible.\n\nLimitations Encountered\n\nWe are developing directly using transformers with a large number of pre-trained speech large models, NLP LLMs, and machine vision models such as YOLOv11, along with their tokenizers and image processing programs, for deep training or direct application inference and prediction. However, we currently lack these resources. I have just implemented how to download large model files from Hugging Face Transformers and Safetensors GGUF format for Scala 3 development. There are also Scala 3 versions of NumPy, Pickle, and Polars, which you can use directly in the future. Now, Storch has the capability to handle subsequent operations.\n\nAdditionally, we need Hugging Face Datasets,TRL reinforcement learning library, and model quantization and optimization libraries.\n\n2. Models of Interest\n\nHugging Face Transformers officially provides nearly 100 models. I believe we should try to cover every type of model and need various tokenizers and image processing processors.\n\nNLP & LLM\n\nDeepSeek, GPT, Qianwen, Gemmi, BERT\n\nComputer Vision\n\nYOLOv11, YOLOv12, U-Net, Mistral, Mask2Former\n\nMultimodal\n\nSmolVLM, FastLVM, SmolDocling, CLIP\n\nI need both training and inference capabilities, with priority given to inference applications.\n\nOther JVM ML frameworks, such as TensorFlow, DJL, EasyAI, and MXNet, are no longer in use because they are not user-friendly. PyTorch has become the standard for deep learning, and other frameworks have many disadvantages compared to it. Now, it is unnecessary to invest effort in compatibility with them. I personally believe that promoting the prosperity of LibTorch on the JVM will make it easier to unify multiple language frameworks, and they will follow suit.\n\nDeployment\nDirectly use the java -jar command-line mode to load the model.\nUse Docker containers or K8s containers to load the model program.\nUse TorchServe for deployment.\nDevelop frameworks similar to VLLM, SGLang, and DeepSpeed in the future for deployment.\nPerformance Requirements\n\nThe performance should be able to handle 1k-10k QPS of concurrent access in the future, with a latency of 300ms-2500ms.\n\nAdditionally, if you have any questions in the future, you can contact me or find some projects you can use from my repositories for dependencies. You can also search for repositories starting with \"storch\" in the Maven Central Repository. \nThere are three other original authors of Storch, who are very skilled programmers. They will give you a lot of inspiration. @sbrunk \nhttps://github.com/sbrunk\n\nAlso, join the Scala community on Discord. Many people are looking forward to your collaboration and participation. Now, the entire Scala 3 community is seeking breakthroughs in deep learning with Scala, as Scala has lagged behind compared to Python, Rust, and Mojo.\n\nLink to Scala Community Discussion\nhttps://contributors.scala-lang.org/t/compiling-scala-to-python-as-a-new-platform-for-data-engineering-and-ai/7129/32\n\nhttps://github.com/mullerhai/storch\nhttps://github.com/mullerhai/storch-safe-tensor\nhttps://github.com/mullerhai/storch-numpy\nhttps://github.com/mullerhai/storch-pickle\nhttps://github.com/mullerhai/storch-recommend\nhttps://github.com/mullerhai/storch-tutorial\n\n\n"
      },
      {
        "user": "mullerhai",
        "body": "> Hi [@mullerhai](https://github.com/mullerhai),\n> \n> I'm a new contributor to the Hugging Face community, and I came across your issue while exploring the codebase. I understand your frustration about the lack of Java/Scala support in Transformers. You're absolutely right that JVM languages are widely used in enterprise environments, and having native support would be valuable for many users. As someone who's just starting to contribute, I'd like to help move this forward and learn from your experience.\n> \n> I see you've mentioned existing PyTorch bindings like javacpp-pytorch and storch. To help us prioritize and implement this feature effectively, could you share:\n> \n> 1. In your current setup:\n>    \n>    * Are you using any of these existing bindings (javacpp-pytorch, storch) in production?\n>    * What specific limitations are you encountering with the current solutions?\n> 2. For your use case:\n>    \n>    * Which specific models are you most interested in using?\n>    * Do you need both training and inference capabilities?\n>    * Are you working with any specific JVM-based ML frameworks that we should ensure compatibility with?\n> 3. Regarding deployment:\n>    \n>    * Are you running this in a production environment?\n>    * What are your current performance requirements (throughput, latency, etc.)?\n> \n> As a new community member, I'm particularly interested in understanding the real-world needs of JVM users like yourself. Your experience with the current limitations would be invaluable in helping me and others shape this implementation effectively.\n> \n> Thank you for your patience and for helping me learn more about the community's needs!\n\nby the way , if you need , maybe you could  connect me  on discord ,and email  hai710459649@gmail.com  ,wechat mullerhelen"
      }
    ]
  },
  {
    "issue_number": 38130,
    "title": "eval_loss not found when training a peft model using trainer.py / losses not retrieved from base model where appropriate",
    "author": "kreil",
    "state": "open",
    "created_at": "2025-05-14T17:47:27Z",
    "updated_at": "2025-06-10T01:13:17Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\npip list |grep transf; python --version; uname -a\n\ntransformers              4.51.3\nPython 3.12.10\nLinux gap 6.11.10+bpo-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.11.10-1~bpo12+1 (2024-12-19) x86_64 GNU/Linux\n\n\n### Who can help?\n\n@zach-huggingface @SunMarc\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nTrain any peft model. I am training a LoRA adapter using the AutoModelForMaskedLM from ModernBERT.\n\n### Expected behavior\n\nTrainer and its most regular features should handle looking up the losses in the base model, which at several places in the code they apparently don't. So instead of finding `eval_loss` they only see the new losses added by `peft` such as `eval_steps_per_second`.\n\nOne such issue is described, including a fix, in #33420. This fix should be merged with the regular sources.\n\nThe second such issue, triggered by setting `load_best_model_at_end` to True, is described in the comments to #33420 (no fix yet).\n",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @benjaminbossan and @SunMarc "
      },
      {
        "user": "SunMarc",
        "body": "Hey @kreil, if you are share a reproducer, that would help us fix the bug. Happyy to review the PR if you have the solution also ! "
      },
      {
        "user": "artintel",
        "body": "I also had this problem and solved it according to [#33420](https://github.com/huggingface/transformers/issues/38130) hopefully this pr will be merge"
      }
    ]
  },
  {
    "issue_number": 33420,
    "title": "eval_loss not found when training a peft model using trainer.py",
    "author": "ChintanShahDS",
    "state": "closed",
    "created_at": "2024-09-11T04:48:31Z",
    "updated_at": "2025-06-10T01:12:49Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntransformers version: 4.43.3\r\nPython 3.10.12\r\nUbuntu\r\n\r\nIssue is with the trainer.py since it does not check the base_model for Peft cases to get the label information\r\n\r\nHave fixed in local but raising this to fix in the branch\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nRun a vision transformer training using Lora\n\n### Expected behavior\n\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/optimum/habana/transformers/trainer.py\", line 1341, in _save_checkpoint\r\n    metric_value = metrics[metric_to_check]\r\nKeyError: 'eval_loss'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/vision/run_image_classification.py\", line 510, in <module>\r\n    main()\r\n  File \"/home/vision/run_image_classification.py\", line 480, in main\r\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\r\n  File \"/usr/local/lib/python3.10/dist-packages/optimum/habana/transformers/trainer.py\", line 553, in train\r\n    return inner_training_loop(\r\n  File \"/usr/local/lib/python3.10/dist-packages/optimum/habana/transformers/trainer.py\", line 1052, in _inner_training_loop\r\n    self._maybe_log_save_evaluate(tr_loss, _grad_norm, model, trial, epoch, ignore_keys_for_eval)\r\n  File \"/usr/local/lib/python3.10/dist-packages/optimum/habana/transformers/trainer.py\", line 1269, in _maybe_log_save_evaluate\r\n    self._save_checkpoint(model, trial, metrics=metrics)\r\n  File \"/usr/local/lib/python3.10/dist-packages/optimum/habana/transformers/trainer.py\", line 1343, in _save_checkpoint\r\n    raise KeyError(\r\nKeyError: \"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch', 'memory_allocated (GB)', 'max_memory_allocated (GB)', 'total_memory_available (GB)']. Consider changing the `metric_for_best_model` via the TrainingArguments.\"\r\n",
    "comments": [
      {
        "user": "ChintanShahDS",
        "body": "Able to find the problem and the solution.\r\n\r\n        # Modified for peft related changes that were missing in the original version\r\n        if _is_peft_model(self.model):\r\n            if hasattr(self.model, \"get_base_model\"):\r\n                model_to_inspect = self.model.get_base_model()\r\n                default_label_names = find_labels(model_to_inspect.__class__)\r\n                self.can_return_loss = can_return_loss(model_to_inspect.__class__)\r\n        else:\r\n            default_label_names = find_labels(self.model.__class__)\r\n            self.can_return_loss = can_return_loss(self.model.__class__)\r\n\r\n        self.label_names = default_label_names if self.args.label_names is None else self.args.label_names\r\n        self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\r\n        # End of modification\r\n\r\nNeed to convert to PR so that can pull and make these changes"
      },
      {
        "user": "ChintanShahDS",
        "body": "The issue is fixed using the code that is updated on trainer and sent as part of pull request"
      },
      {
        "user": "JulioSanchezD",
        "body": "8 months later and I'm still facing this issue with the base library. This solution has proven to work for me by modifying the trainer.py script, now I can monitor the evaluation loss during training.  This solution should be merged."
      }
    ]
  },
  {
    "issue_number": 38708,
    "title": "Bert2D: A 2D-Word Embedding Model for Morphologically Rich Languages",
    "author": "yigit353",
    "state": "open",
    "created_at": "2025-06-09T23:58:41Z",
    "updated_at": "2025-06-10T00:01:31Z",
    "labels": [
      "New model"
    ],
    "body": "### Model description\n\n**Bert2D** is a novel transformer-based model that builds upon the `BertModel` architecture by introducing a two-dimensional word embedding system. This enhancement is specifically designed to improve performance on morphologically rich languages, such as Turkish, Finnish, and Hungarian. This model card describes the initial release, which includes the model implementation and a pretrained checkpoint for Turkish.\n\nThis work is based on the research outlined in the paper **\"Bert2D: A 2D-Word Embedding for Morphologically Rich Languages\"**, which has been accepted by IEEE and is available at: [https://ieeexplore.ieee.org/document/10542953](https://ieeexplore.ieee.org/document/10542953).\n\nA pretrained model for Turkish, `Bert2D-cased-Turkish-128K-WWM-NSW2`, is available on the Hugging Face Hub at: [https://huggingface.co/yigitbekir/Bert2D-cased-Turkish-128K-WWM-NSW2](https://huggingface.co/yigitbekir/Bert2D-cased-Turkish-128K-WWM-NSW2)\n\n## Model Description\n\nThe primary innovation of **Bert2D** is its use of a 2D positional embedding mechanism to better capture the complex morphological structures present in agglutinative languages. Unlike standard BERT models that use a 1D positional embedding, Bert2D employs a dual system:\n\n1.  **Whole-Word Positional Embeddings (1st Dimension):** This captures the absolute position of each word in a sequence.\n2.  **Sub-word Relative Positional Embeddings (2nd Dimension):** This encodes the relative position of sub-words within each word, allowing the model to distinguish between the beginning, middle, and end of a word's sub-tokens.\n\nThis two-dimensional approach provides a more nuanced representation of meaning by enabling the model to understand the relationships between words and their constituent morphemes. The model also incorporates **Whole Word Masking (WWM)**, a training technique where all sub-tokens corresponding to a single word are masked, encouraging the model to learn deeper contextual relationships.\n\n### Architectural Innovations\n\nThe key components introduced in this release are:\n\n* **`Bert2DModel`**: A new model class inheriting from `BertPreTrainedModel` that implements the 2D embedding logic. The core modifications are in the embeddings layer to accommodate the dual positional encoding.\n* **`Bert2DTokenizer` and `Bert2DTokenizerFast`**: Custom tokenizers compatible with the `Bert2D` model.\n* **Model Variants**: Includes all standard BERT architecture variants, such as:\n    * `Bert2DForMaskedLM`\n    * `Bert2DForSequenceClassification`\n    * `Bert2DForTokenClassification`\n    * `Bert2DForQuestionAnswering`\n\n### Configuration Parameters\n\nThe `Bert2DConfig` includes new parameters to manage the 2D embeddings:\n\n* `max_word_position_embeddings`: Defines the maximum number of words (not sub-tokens) the model can handle in a sequence. The default is `512`.\n* `max_intermediate_subword_position_embeddings`: Specifies the embedding value for intermediate sub-tokens within a word. For the `NSW2` strategy, this is set to `2`.\n\nThe 2D embeddings are summed with the token and segment embeddings before being passed to the Transformer layers. The parameter count is nearly identical to a standard BERT model; the `128K` in the checkpoint name refers to the vocabulary size.\n\n### Example Usage\n\nThe `Bert2D` model can be easily used with the `pipeline` API for tasks such as `fill-mask`.\n\n```python\nfrom transformers import pipeline\n\n# Initialize the fill-mask pipeline with the Bert2D model\nfill_mask_pipe = pipeline(\"fill-mask\", model=\"yigitbekir/Bert2D-cased-Turkish-128K-WWM-NSW2\")\n\n# Example usage\nmasked_sentence = \"Adamın mesleği [MASK] midir acaba?\"\npredictions = fill_mask_pipe(masked_sentence)\n\n# Print the top predictions\nfor prediction in predictions:\n    print(f\"Token: {prediction['token_str']}\")\n    print(f\"Sequence: {prediction['sequence']}\")\n    print(f\"Score: {prediction['score']:.4f}\")\n    print(\"-\" * 20)\n```\n\n**Predicted Output:**\n```\nToken: mühendis\nSequence: Adamın mesleği mühendis midir acaba?\nScore: 0.2393\n--------------------\nToken: doktor\nSequence: Adamın mesleği doktor midir acaba?\nScore: 0.1698\n--------------------\nToken: asker\nSequence: Adamın mesleği asker midir acaba?\nScore: 0.0537\n--------------------\nToken: memur\nSequence: Adamın mesleği memur midir acaba?\nScore: 0.0471\n--------------------\nToken: öğretmen\nSequence: Adamın mesleği öğretmen midir acaba?\nScore: 0.0463\n--------------------\n```\n\n### Fine-Tuning Considerations\n\nWhen fine-tuning a `Bert2D` model, it is crucial to use the model's specific configuration. The introduction of `max_word_position_embeddings` and `max_intermediate_subword_position_embeddings` means that standard BERT configuration files are not directly compatible. Ensure that you are using the `Bert2DConfig` and its associated parameters for optimal performance.\n\n### Motivation and Context\n\nTraditional NLP models often struggle with languages that have rich morphology, as the vast number of word forms for a single root makes it difficult for models with 1D positional embeddings to generalize effectively. The **Bert2D** architecture was developed to address this limitation, and initial experiments on Turkish have shown that it consistently outperforms strong monolingual models across a range of downstream tasks.\n\n### Future Work and Contributions\n\nThe developers are actively seeking contributions in the following areas:\n\n* **Pretraining on other languages:** Particularly other morphologically complex languages like Finnish, Hungarian, and Korean.\n* **Further architectural enhancements.**\n* **Downstream task fine-tuning and evaluation.**\n\n### Open source status\n\n- [x] The model implementation is available\n- [x] The model weights are available\n\n### Provide useful links for the implementation\n\nWeights: https://huggingface.co/yigitbekir/Bert2D-cased-Turkish-128K-WWM-NSW2\nArticle: [https://ieeexplore.ieee.org/document/10542953](https://ieeexplore.ieee.org/document/10542953)\nThe PR: #38707",
    "comments": []
  },
  {
    "issue_number": 38656,
    "title": "Potential Memory Leak or Caching in Fast Image Processor",
    "author": "yhyang201",
    "state": "open",
    "created_at": "2025-06-07T08:46:48Z",
    "updated_at": "2025-06-09T22:02:54Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nHi team,\n\nThank you for your great work on `transformers`!\n\nWhile using the `AutoProcessor` with `use_fast=True`, I noticed that there seems to be a memory leak or possibly some form of persistent caching when processing images. Even after deleting the processor and clearing the CUDA cache, approximately 600MB of GPU memory remains occupied.\n\nHere is a minimal reproducible example:\n\n```python\nfrom transformers import AutoProcessor\nfrom PIL import Image\nimport time\nimport torch\nimport requests\nfrom io import BytesIO\n\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n    use_fast=True,\n    trust_remote_code=False,\n    revision=None,\n)\n\nurl = \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\nresponse = requests.get(url)\nimages = [Image.open(BytesIO(response.content)).convert(\"RGB\")]\n\nresult = processor(\n    text=[\n        \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n        \"<|im_start|>user\\nWhat’s in this image?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\\n\"\n        \"<|im_start|>assistant\\n\"\n    ],\n    padding=True,\n    return_tensors=\"pt\",\n    images=images,\n    device=\"cuda\"\n)\n\ndel result\ndel processor\ntorch.cuda.empty_cache()\n\nprint(\"You can now use nvidia-smi to observe GPU memory usage, which is around 600MB.\")\nwhile True:\n    time.sleep(60)\n```\n\nI’d like to kindly ask:\n\n1. If this is due to caching, is there a way to control or disable the cache?\n2. If this is an unintended memory leak, would it be possible to investigate and potentially fix it?\n\nThanks again for your help and time!\n\nBest regards\n\n### Who can help?\n\ntokenizers: @ArthurZucker and @itazap\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nAs provided above.\n\n### Expected behavior\n\nIt would be great if caching could be made optional, or if there could be an option to avoid any GPU memory usage entirely.",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "cc @yonigozlan , maybe it's the fused means/std, couldn't see what else can be cached on a quick glance"
      },
      {
        "user": "yonigozlan",
        "body": "Hmm not sure either, I'll have a look. Thanks for raising the issue!"
      },
      {
        "user": "CatherineSue",
        "body": "Hi @yonigozlan , I did some debugging, it seems the GPU memory jumped after this line: https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils_fast.py#L469-L472\n\nI tried an image shape with `[3, 4096, 4096]`, and dtype `torch.unit8`, after this line, `nvidia-smi` showed a GPU memory around 600 MB\n\nNot sure it will help. Please ignore me if this is incorrect"
      }
    ]
  },
  {
    "issue_number": 35282,
    "title": "Qwen2vl support for GGUF",
    "author": "cheald",
    "state": "open",
    "created_at": "2024-12-15T20:31:26Z",
    "updated_at": "2025-06-09T20:48:23Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nllama.cpp recently added [support for Qwen2VL](https://github.com/ggerganov/llama.cpp/commit/ba1cb19cdd0d92e012e0f6e009e0620f854b6afd), which means that we can now quantize Qwen2VL models (and I've done so, successfully!) I'd like to be able to load quantized Qwen2VL models with AutoModelForVision2Seq; currently, transformers doesn't recognize qwen2vl as a valid architecture.\n\n### Motivation\n\nIt would be wonderful to be able to use quantized GGUF Qwen2VL models!\n\n### Your contribution\n\nI'm happy to work up the PR for this, if I can get some direction on where to start. I'm hacking through the code right now, but I don't know it well enough to be able to meaningfully dent the problem just yet.",
    "comments": [
      {
        "user": "VladOS95-cyber",
        "body": "Hi @cheald! You could check previous GGUF implementations for other models following this task https://github.com/huggingface/transformers/issues/33260. There is well described workflow and a lot of different PRs. If you will have any questions, feel free to ask."
      },
      {
        "user": "cheald",
        "body": "Thank you for the pointer! I'll see if I can cobble together a PR."
      },
      {
        "user": "helloHKTK",
        "body": "Does Transformers now support loading the Qwen2vl model in GGUF format?"
      }
    ]
  },
  {
    "issue_number": 38609,
    "title": "Wav2Vec2Processor/Wav2Vec2ProcessorWithLM documentation still refers to 'as_target_processor'",
    "author": "renet10",
    "state": "closed",
    "created_at": "2025-06-05T13:30:05Z",
    "updated_at": "2025-06-09T19:26:10Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n\"as_target_processor()\" is to be deprecated since using it will throw a warning:\n```\nUserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n```\nThis warning has been around since at least transformers 4.49, however the documentation still refers to the use of \"as_target_processor\" in Wav2Vec2Processor/Wav2Vec2ProcessorWithLM.\n\nWhile HuggingFace task recipes have been updated, many 3rd-party examples still use as_target_processor() and the documentation is confusing since you no longer need to call the processor twice (once outside the 'with' block and once inside), but can add both the speech array and the transcription text into one single call to processor.\n\nThis is important because tnt documentation still carries the impression that processor should be called twice, and in fact simply calling the processor twice with no \"with\" block leads to hard to understand errors.\n\n### Who can help?\n\n@stevhliu \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nSee \n1. https://huggingface.co/docs/transformers/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor\n2. https://huggingface.co/docs/transformers/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM\n\n\n### Expected behavior\n\nUpdated documentation to match the task recipes.",
    "comments": [
      {
        "user": "stevhliu",
        "body": "Thanks, would you like to open a PR to remove `as_target_processor` from the docstrings of the `call` and `pad` methods?"
      },
      {
        "user": "marcndo",
        "body": "Hi @renet10 @stevhliu! 👋\n\nI’d love to pick this up and make it my first contribution to the project. Could you please assign it to me if it's still available?\n\nThank you!"
      },
      {
        "user": "renet10",
        "body": "@marcndo I believe you simply create a pull req as per https://huggingface.co/docs/transformers/en/contributing.  I would do so but coding is not my first skill."
      }
    ]
  },
  {
    "issue_number": 38544,
    "title": "Paligemma model card needs update",
    "author": "punitvara",
    "state": "open",
    "created_at": "2025-06-03T06:55:14Z",
    "updated_at": "2025-06-09T17:13:03Z",
    "labels": [],
    "body": "Hi \n\nI found a minor problem with paligemma model card. How can I raise a PR to fix it ? I am first time contributor. I raised PR. Whom should I mention to review it ? \nhttps://huggingface.co/google/paligemma-3b-pt-896",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @punitvara, you can ping @stevhliu for model card updates."
      },
      {
        "user": "punitvara",
        "body": "Thanks @Rocketknight1 I sent PR on model card. Let me see if I can find him on discord. "
      },
      {
        "user": "stevhliu",
        "body": "Hi, feel free to ping me on the PR! :)"
      }
    ]
  },
  {
    "issue_number": 38623,
    "title": "RagRetriever retrieve function mismatch in signature and return,  unneeded requirement for faiss",
    "author": "Fiona-Waters",
    "state": "closed",
    "created_at": "2025-06-05T22:08:34Z",
    "updated_at": "2025-06-09T15:17:35Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nWhen creating a custom RagRetriever based on the RagRetriever Class it became apparent when using linting tools that the signature and returned items in the [retrieve](https://github.com/huggingface/transformers/blob/main/src/transformers/models/rag/retrieval_rag.py#L560-L580) function do not match.  \n```\n    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, List[dict]]:\n    .......\n        return retrieved_doc_embeds, doc_ids, self.index.get_doc_dicts(doc_ids)\n\n```\n\nIn using the RagRetriever functionality it also became clear that I needed to install faiss-cpu library, even though it is not required for the work that I am doing. Therefore it should be optional.\n\n```\nFile /opt/app-root/lib64/python3.11/site-packages/transformers/models/rag/retrieval_rag.py:400, in RagRetriever.__init__(self, config, question_encoder_tokenizer, generator_tokenizer, index, init_retrieval)\n    398 def __init__(self, config, question_encoder_tokenizer, generator_tokenizer, index=None, init_retrieval=True):\n    399     self._init_retrieval = init_retrieval\n--> 400     requires_backends(self, [\"datasets\", \"faiss\"])\n    401     super().__init__()\n    402     self.index = index or self._build_index(config)\n\nFile /opt/app-root/lib64/python3.11/site-packages/transformers/utils/import_utils.py:1871, in requires_backends(obj, backends)\n   1868         failed.append(msg.format(name))\n   1870 if failed:\n-> 1871     raise ImportError(\"\".join(failed))\n\nImportError: \nCustomRAGRetriever requires the faiss library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n```\n\nI will submit a PR that will hopefully fix these issues.\n\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nSteps to reproduce the errors:\n\nretrieve function signature mismatch with returned items:\n\nRun mypy linting on code inheriting the base RagRetriever class where your custom retrieve function signature mentions returning 3 items and you will receive an error that this does not match \n```\nerror: Signature of \"retrieve\" incompatible with supertype \"RagRetriever\"  [override]\n```\n\nfaiss dependency requirement:\nCreate a custom rag retriever and try to use it without importing faiss in your environment and you will get this error:\n```\nCustomRagRetriever requires the faiss library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo...\n```\nNote: I am not using faiss, as it is not required, therefore a ragretriever can be used without this dependency.\n\n### Expected behavior\n\nI would expect the function signature to match the number of items returned by the function.\nI would expect to be able to inherit from the RagRetriever without needing the faiss dependency.",
    "comments": [
      {
        "user": "Fiona-Waters",
        "body": "/assign"
      }
    ]
  },
  {
    "issue_number": 38693,
    "title": "Add MulT Model from “Multimodal Transformer for Unaligned Multimodal Language Sequences\" paper",
    "author": "yushi2006",
    "state": "closed",
    "created_at": "2025-06-09T09:26:37Z",
    "updated_at": "2025-06-09T15:12:02Z",
    "labels": [
      "New model"
    ],
    "body": "### Model description\n\nI’d like to contribute an implementation of the MulT model described in the paper:\n\n**Title:**   Multimodal Transformer for Unaligned Multimodal Language Sequences\n**Paper:** [https://arxiv.org/pdf/1906.00295v1](https://arxiv.org/pdf/1906.00295v1)\n\n### 💡 Motivation\n\nMulT is a powerful approach for multimodal learning, combining textual, visual, and audio data via direct cross-modal attention. It’s not yet available in `transformers`, and I think it would be a valuable addition for researchers and developers interested in multimodal tasks.\n\n### 🛠️ My Plan\n\nI am currently working on implementing MulT, following the paper’s details and aligning with the `transformers` library standards.\n\nHere’s a rough checklist of what I plan to do:\n- Model architecture and layers\n- Unit tests and example scripts\n- Documentation\n\nIt may take me some time because I'm new to open-source contributions.\n\nI’ll keep this issue updated with progress and would be happy to receive any suggestions!\n\n---",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @yushi2006, this paper is quite old! It's unlikely that model checkpoints from ~6 years ago will get a lot of users, and so we probably won't add them to the `transformers` library itself, because that requires the team at Hugging Face to take over maintenance for them. However, you can upload the checkpoints as [custom code models](https://huggingface.co/docs/transformers/en/custom_models) if you want!"
      },
      {
        "user": "yushi2006",
        "body": "> Hi [@yushi2006](https://github.com/yushi2006), this paper is quite old! It's unlikely that model checkpoints from ~6 years ago will get a lot of users, and so we probably won't add them to the `transformers` library itself, because that requires the team at Hugging Face to take over maintenance for them. However, you can upload the checkpoints as [custom code models](https://huggingface.co/docs/transformers/en/custom_models) if you want!\n\nHi [@Rocketknight1 ](https://github.com/Rocketknight1),\n\nThanks for the feedback! That makes sense — I understand that supporting older models can be a maintenance burden. I’ll definitely look into uploading the MulT implementation as a custom model on the Hub instead. \n\nI’ve learned a lot about the Transformers repo during this process, and I’d love to keep contributing to more relevant models in the future. If you have any suggestions for newer models or features that could be a good fit for the main repo, I’d really appreciate any pointers!\n\nThanks again for your time — it was super helpful. Cheers!\n"
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @yushi2006, if you want to try a PR to `transformers` itself, that's great! A good approach is to look on the [trending](https://huggingface.co/models) page for custom code models that are getting a lot of downloads, and if you see one you like that doesn't already have a PR to add it to the library, it's probably a good candidate! Open an issue and ping me or someone else on the team to check, and we'll tell you if we're interested in a PR for it."
      }
    ]
  },
  {
    "issue_number": 38692,
    "title": "CheckpointLoaderSimple ..... Error while deserializing header: InvalidHeaderDeserialization",
    "author": "saeedafm",
    "state": "open",
    "created_at": "2025-06-09T07:54:50Z",
    "updated_at": "2025-06-09T14:35:32Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nHi guys, I just downloaded flux models and then if I Run, I received the following error:\nCheckpointLoaderSimple ..... Error while deserializing header: InvalidHeaderDeserialization\n\n\n\n\n# ComfyUI Error Report\n## Error Details\n- **Node ID:** 3\n- **Node Type:** CheckpointLoaderSimple\n- **Exception Type:** safetensors_rust.SafetensorError\n- **Exception Message:** Error while deserializing header: InvalidHeaderDeserialization\n## Stack Trace\n```\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 345, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 220, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 192, in _map_node_over_list\n    process_inputs(input_dict, i)\n\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 181, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\nodes.py\", line 570, in load_checkpoint\n    out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\sd.py\", line 908, in load_checkpoint_guess_config\n    sd, metadata = comfy.utils.load_torch_file(ckpt_path, return_metadata=True)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\utils.py\", line 68, in load_torch_file\n    raise e\n\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\utils.py\", line 55, in load_torch_file\n    with safetensors.safe_open(ckpt, framework=\"pt\", device=device.type) as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n```\n## System Information\n- **ComfyUI Version:** 0.3.29\n- **Arguments:** ComfyUI\\main.py --windows-standalone-build\n- **OS:** nt\n- **Python Version:** 3.12.9 (tags/v3.12.9:fdb8142, Feb  4 2025, 15:27:58) [MSC v.1942 64 bit (AMD64)]\n- **Embedded Python:** true\n- **PyTorch Version:** 2.6.0+cu126\n## Devices\n\n- **Name:** cuda:0 NVIDIA GeForce RTX 4070 Laptop GPU : cudaMallocAsync\n  - **Type:** cuda\n  - **VRAM Total:** 8585216000\n  - **VRAM Free:** 7398752256\n  - **Torch VRAM Total:** 0\n  - **Torch VRAM Free:** 0\n\n## Logs\n```\n2025-06-09T11:03:11.343705 - [START] Security scan2025-06-09T11:03:11.343705 - \n2025-06-09T11:03:12.134459 - [DONE] Security scan2025-06-09T11:03:12.134459 - \n2025-06-09T11:03:12.228807 - ## ComfyUI-Manager: installing dependencies done.2025-06-09T11:03:12.228807 - \n2025-06-09T11:03:12.228807 - ** ComfyUI startup time:2025-06-09T11:03:12.228807 -  2025-06-09T11:03:12.228807 - 2025-06-09 11:03:12.2282025-06-09T11:03:12.228807 - \n2025-06-09T11:03:12.228807 - ** Platform:2025-06-09T11:03:12.228807 -  2025-06-09T11:03:12.228807 - Windows2025-06-09T11:03:12.228807 - \n2025-06-09T11:03:12.228807 - ** Python version:2025-06-09T11:03:12.228807 -  2025-06-09T11:03:12.228807 - 3.12.9 (tags/v3.12.9:fdb8142, Feb  4 2025, 15:27:58) [MSC v.1942 64 bit (AMD64)]2025-06-09T11:03:12.228807 - \n2025-06-09T11:03:12.228807 - ** Python executable:2025-06-09T11:03:12.228807 -  2025-06-09T11:03:12.228807 - D:\\Ai\\ComfyUI_windows_portable\\python_embeded\\python.exe2025-06-09T11:03:12.228807 - \n2025-06-09T11:03:12.228807 - ** ComfyUI Path:2025-06-09T11:03:12.228807 -  2025-06-09T11:03:12.228807 - D:\\Ai\\ComfyUI_windows_portable\\ComfyUI2025-06-09T11:03:12.228807 - \n2025-06-09T11:03:12.228807 - ** ComfyUI Base Folder Path:2025-06-09T11:03:12.228807 -  2025-06-09T11:03:12.228807 - D:\\Ai\\ComfyUI_windows_portable\\ComfyUI2025-06-09T11:03:12.228807 - \n2025-06-09T11:03:12.228807 - ** User directory:2025-06-09T11:03:12.228807 -  2025-06-09T11:03:12.228807 - D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\user2025-06-09T11:03:12.228807 - \n2025-06-09T11:03:12.228807 - ** ComfyUI-Manager config path:2025-06-09T11:03:12.228807 -  2025-06-09T11:03:12.228807 - D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\user\\default\\ComfyUI-Manager\\config.ini2025-06-09T11:03:12.228807 - \n2025-06-09T11:03:12.228807 - ** Log path:2025-06-09T11:03:12.228807 -  2025-06-09T11:03:12.228807 - D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\user\\comfyui.log2025-06-09T11:03:12.228807 - \n2025-06-09T11:03:13.171480 - \nPrestartup times for custom nodes:\n2025-06-09T11:03:13.171480 -    2.5 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\comfyui-manager\n2025-06-09T11:03:13.171480 - \n2025-06-09T11:03:17.015681 - Checkpoint files will always be loaded safely.\n2025-06-09T11:03:17.292874 - Total VRAM 8188 MB, total RAM 65229 MB\n2025-06-09T11:03:17.292874 - pytorch version: 2.6.0+cu126\n2025-06-09T11:03:17.292874 - Set vram state to: NORMAL_VRAM\n2025-06-09T11:03:17.292874 - Device: cuda:0 NVIDIA GeForce RTX 4070 Laptop GPU : cudaMallocAsync\n2025-06-09T11:03:20.563827 - Using pytorch attention\n2025-06-09T11:03:24.102716 - Python version: 3.12.9 (tags/v3.12.9:fdb8142, Feb  4 2025, 15:27:58) [MSC v.1942 64 bit (AMD64)]\n2025-06-09T11:03:24.102716 - ComfyUI version: 0.3.29\n2025-06-09T11:03:24.276988 - ComfyUI frontend version: 1.16.8\n2025-06-09T11:03:24.276988 - [Prompt Server] web root: D:\\Ai\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\comfyui_frontend_package\\static\n2025-06-09T11:03:25.400122 - Config Export Error2025-06-09T11:03:25.400122 - [Errno 2] No such file or directory: 'XXXHOST-PATHXXX\\\\PATH_CFG.json'2025-06-09T11:03:25.734006 - ComfyUI-GGUF: Partial torch compile only, consider updating pytorch\n2025-06-09T11:03:25.753031 - ### Loading: ComfyUI-Impact-Pack (V8.15.3)2025-06-09T11:03:25.753031 - \n2025-06-09T11:03:26.107634 - [Impact Pack] Wildcards loading done.2025-06-09T11:03:26.114681 - \n2025-06-09T11:03:26.122418 - ### Loading: ComfyUI-Manager (V3.32.8)\n2025-06-09T11:03:26.123251 - [ComfyUI-Manager] network_mode: public\n2025-06-09T11:03:26.241014 - ### ComfyUI Revision: 3347 [93292bc4] *DETACHED | Released on '2025-04-17'\n2025-06-09T11:03:26.716689 - \u001b[36;20m[D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\comfyui_controlnet_aux] | INFO -> Using ckpts path: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\comfyui_controlnet_aux\\ckpts\u001b[0m\n2025-06-09T11:03:26.716689 - \u001b[36;20m[D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\comfyui_controlnet_aux] | INFO -> Using symlinks: False\u001b[0m\n2025-06-09T11:03:26.716689 - \u001b[36;20m[D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\comfyui_controlnet_aux] | INFO -> Using ort providers: ['CUDAExecutionProvider', 'DirectMLExecutionProvider', 'OpenVINOExecutionProvider', 'ROCMExecutionProvider', 'CPUExecutionProvider', 'CoreMLExecutionProvider']\u001b[0m\n2025-06-09T11:03:27.017334 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json\n2025-06-09T11:03:27.113419 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json\n2025-06-09T11:03:27.135199 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json\n2025-06-09T11:03:27.192743 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/github-stats.json\n2025-06-09T11:03:27.716053 - [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json\n2025-06-09T11:03:28.970262 - [ComfyUI-Manager] An error occurred while fetching 'https://api.comfy.org/nodes?page=1&limit=30&comfyui_version=v0.3.29&form_factor=git-windows': Expecting value: line 2 column 1 (char 1)\n2025-06-09T11:03:28.970262 - Cannot connect to comfyregistry.2025-06-09T11:03:28.985997 - \n2025-06-09T11:03:29.001753 - FETCH DATA from: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json2025-06-09T11:03:29.001753 - 2025-06-09T11:03:29.055871 - D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\comfyui_controlnet_aux\\node_wrappers\\dwpose.py:26: UserWarning: DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly\n  warnings.warn(\"DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly\")\n2025-06-09T11:03:29.327544 -  [DONE]2025-06-09T11:03:29.327544 - \n2025-06-09T11:03:29.524929 - [ComfyUI-Manager] All startup tasks have been completed.\n2025-06-09T11:03:31.856243 - \u001b[34mWAS Node Suite: \u001b[0mOpenCV Python FFMPEG support is enabled\u001b[0m2025-06-09T11:03:31.860328 - \n2025-06-09T11:03:31.865483 - \u001b[34mWAS Node Suite \u001b[93mWarning: \u001b[0m`ffmpeg_bin_path` is not set in `D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\was-node-suite-comfyui\\was_suite_config.json` config file. Will attempt to use system ffmpeg binaries if available.\u001b[0m2025-06-09T11:03:31.869930 - \n2025-06-09T11:03:32.316900 - \u001b[34mWAS Node Suite: \u001b[0mFinished.\u001b[0m \u001b[32mLoaded\u001b[0m \u001b[0m220\u001b[0m \u001b[32mnodes successfully.\u001b[0m2025-06-09T11:03:32.316900 - \n2025-06-09T11:03:32.316900 - \n\t\u001b[3m\u001b[93m\"The only limit to our realization of tomorrow will be our doubts of today.\"\u001b[0m\u001b[3m - Franklin D. Roosevelt\u001b[0m\n2025-06-09T11:03:32.316900 - \n2025-06-09T11:03:32.316900 - \nImport times for custom nodes:\n2025-06-09T11:03:32.316900 -    0.0 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\websocket_image_save.py\n2025-06-09T11:03:32.316900 -    0.0 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-ImageGallery-ED\n2025-06-09T11:03:32.316900 -    0.0 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\Cup-ClipBoard\n2025-06-09T11:03:32.316900 -    0.0 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-CUP\n2025-06-09T11:03:32.316900 -    0.0 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI_experiments\n2025-06-09T11:03:32.316900 -    0.0 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-Advanced-ControlNet-main\n2025-06-09T11:03:32.316900 -    0.1 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-Tripo\n2025-06-09T11:03:32.316900 -    0.1 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-Custom-Scripts\n2025-06-09T11:03:32.316900 -    0.3 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\n2025-06-09T11:03:32.332849 -    0.4 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI_essentials-main\n2025-06-09T11:03:32.332849 -    0.4 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-Impact-Pack\n2025-06-09T11:03:32.332849 -    0.5 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\comfyui-manager\n2025-06-09T11:03:32.332849 -    1.3 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\OneButtonPrompt\n2025-06-09T11:03:32.332849 -    1.4 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\was-node-suite-comfyui\n2025-06-09T11:03:32.332849 -    2.6 seconds: D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\comfyui_controlnet_aux\n2025-06-09T11:03:32.332849 - \n2025-06-09T11:03:32.332849 - Starting server\n\n2025-06-09T11:03:32.332849 - To see the GUI go to: http://127.0.0.1:8188\n2025-06-09T11:03:37.650461 - got prompt\n2025-06-09T11:03:38.025734 - !!! Exception during processing !!! Error while deserializing header: InvalidHeaderDeserialization\n2025-06-09T11:03:38.034301 - Traceback (most recent call last):\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 345, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 220, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 192, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 181, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\nodes.py\", line 570, in load_checkpoint\n    out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\sd.py\", line 908, in load_checkpoint_guess_config\n    sd, metadata = comfy.utils.load_torch_file(ckpt_path, return_metadata=True)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\utils.py\", line 68, in load_torch_file\n    raise e\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\utils.py\", line 55, in load_torch_file\n    with safetensors.safe_open(ckpt, framework=\"pt\", device=device.type) as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nsafetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization\n\n2025-06-09T11:03:38.040510 - Prompt executed in 0.38 seconds\n2025-06-09T11:04:17.790751 - got prompt\n2025-06-09T11:04:17.821252 - !!! Exception during processing !!! Error while deserializing header: InvalidHeaderDeserialization\n2025-06-09T11:04:17.845292 - Traceback (most recent call last):\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 345, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 220, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 192, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 181, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\nodes.py\", line 570, in load_checkpoint\n    out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\sd.py\", line 908, in load_checkpoint_guess_config\n    sd, metadata = comfy.utils.load_torch_file(ckpt_path, return_metadata=True)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\utils.py\", line 68, in load_torch_file\n    raise e\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\utils.py\", line 55, in load_torch_file\n    with safetensors.safe_open(ckpt, framework=\"pt\", device=device.type) as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nsafetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization\n\n2025-06-09T11:04:17.871446 - Prompt executed in 0.07 seconds\n2025-06-09T11:21:30.198259 - got prompt\n2025-06-09T11:21:30.212261 - !!! Exception during processing !!! Error while deserializing header: InvalidHeaderDeserialization\n2025-06-09T11:21:30.214325 - Traceback (most recent call last):\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 345, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 220, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 192, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 181, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\nodes.py\", line 570, in load_checkpoint\n    out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\sd.py\", line 908, in load_checkpoint_guess_config\n    sd, metadata = comfy.utils.load_torch_file(ckpt_path, return_metadata=True)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\utils.py\", line 68, in load_torch_file\n    raise e\n  File \"D:\\Ai\\ComfyUI_windows_portable\\ComfyUI\\comfy\\utils.py\", line 55, in load_torch_file\n    with safetensors.safe_open(ckpt, framework=\"pt\", device=device.type) as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nsafetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization\n\n2025-06-09T11:21:30.218271 - Prompt executed in 0.01 seconds\n\n```\n## Attached Workflow\nPlease make sure that workflow does not contain any sensitive information such as API keys or passwords.\n```\n{\"id\":\"00000000-0000-0000-0000-000000000000\",\"revision\":0,\"last_node_id\":12,\"last_link_id\":19,\"nodes\":[{\"id\":9,\"type\":\"EmptyLatentImage\",\"pos\":[1117.3448486328125,362.3992919921875],\"size\":[315,106],\"flags\":{},\"order\":0,\"mode\":0,\"inputs\":[{\"localized_name\":\"width\",\"name\":\"width\",\"type\":\"INT\",\"widget\":{\"name\":\"width\"},\"link\":null},{\"localized_name\":\"height\",\"name\":\"height\",\"type\":\"INT\",\"widget\":{\"name\":\"height\"},\"link\":null},{\"localized_name\":\"batch_size\",\"name\":\"batch_size\",\"type\":\"INT\",\"widget\":{\"name\":\"batch_size\"},\"link\":null}],\"outputs\":[{\"localized_name\":\"LATENT\",\"name\":\"LATENT\",\"type\":\"LATENT\",\"links\":[9]}],\"properties\":{\"cnr_id\":\"comfy-core\",\"ver\":\"0.3.29\",\"Node name for S&R\":\"EmptyLatentImage\"},\"widgets_values\":[1024,1024,1]},{\"id\":5,\"type\":\"CLIPTextEncode\",\"pos\":[1097.3914794921875,174.45838928222656],\"size\":[385.0762023925781,88],\"flags\":{},\"order\":4,\"mode\":0,\"inputs\":[{\"localized_name\":\"clip\",\"name\":\"clip\",\"type\":\"CLIP\",\"link\":17},{\"localized_name\":\"text\",\"name\":\"text\",\"type\":\"STRING\",\"widget\":{\"name\":\"text\"},\"link\":null}],\"outputs\":[{\"localized_name\":\"CONDITIONING\",\"name\":\"CONDITIONING\",\"type\":\"CONDITIONING\",\"links\":[19]}],\"properties\":{\"cnr_id\":\"comfy-core\",\"ver\":\"0.3.29\",\"Node name for S&R\":\"CLIPTextEncode\"},\"widgets_values\":[\"\"],\"color\":\"#322\",\"bgcolor\":\"#533\"},{\"id\":4,\"type\":\"CLIPTextEncode\",\"pos\":[1095.3770751953125,-71.65526580810547],\"size\":[400,200],\"flags\":{},\"order\":3,\"mode\":0,\"inputs\":[{\"localized_name\":\"clip\",\"name\":\"clip\",\"type\":\"CLIP\",\"link\":16},{\"localized_name\":\"text\",\"name\":\"text\",\"type\":\"STRING\",\"widget\":{\"name\":\"text\"},\"link\":null}],\"outputs\":[{\"localized_name\":\"CONDITIONING\",\"name\":\"CONDITIONING\",\"type\":\"CONDITIONING\",\"links\":[18]}],\"properties\":{\"cnr_id\":\"comfy-core\",\"ver\":\"0.3.29\",\"Node name for S&R\":\"CLIPTextEncode\"},\"widgets_values\":[\"portrait of a woman\"],\"color\":\"#232\",\"bgcolor\":\"#353\"},{\"id\":10,\"type\":\"VAEDecode\",\"pos\":[2219.472900390625,74.85771179199219],\"size\":[210,46],\"flags\":{},\"order\":6,\"mode\":0,\"inputs\":[{\"localized_name\":\"samples\",\"name\":\"samples\",\"type\":\"LATENT\",\"link\":10},{\"localized_name\":\"vae\",\"name\":\"vae\",\"type\":\"VAE\",\"link\":12}],\"outputs\":[{\"localized_name\":\"IMAGE\",\"name\":\"IMAGE\",\"type\":\"IMAGE\",\"links\":[11]}],\"properties\":{\"cnr_id\":\"comfy-core\",\"ver\":\"0.3.29\",\"Node name for S&R\":\"VAEDecode\"},\"widgets_values\":[]},{\"id\":11,\"type\":\"PreviewImage\",\"pos\":[2477.2490234375,76.54154968261719],\"size\":[210,246],\"flags\":{},\"order\":7,\"mode\":0,\"inputs\":[{\"localized_name\":\"images\",\"name\":\"images\",\"type\":\"IMAGE\",\"link\":11}],\"outputs\":[],\"properties\":{\"cnr_id\":\"comfy-core\",\"ver\":\"0.3.29\",\"Node name for S&R\":\"PreviewImage\"},\"widgets_values\":[]},{\"id\":7,\"type\":\"KSampler\",\"pos\":[1767.0162353515625,-9.899660110473633],\"size\":[315,474],\"flags\":{},\"order\":5,\"mode\":0,\"inputs\":[{\"localized_name\":\"model\",\"name\":\"model\",\"type\":\"MODEL\",\"link\":15},{\"localized_name\":\"positive\",\"name\":\"positive\",\"type\":\"CONDITIONING\",\"link\":18},{\"localized_name\":\"negative\",\"name\":\"negative\",\"type\":\"CONDITIONING\",\"link\":19},{\"localized_name\":\"latent_image\",\"name\":\"latent_image\",\"type\":\"LATENT\",\"link\":9},{\"localized_name\":\"seed\",\"name\":\"seed\",\"type\":\"INT\",\"widget\":{\"name\":\"seed\"},\"link\":null},{\"localized_name\":\"steps\",\"name\":\"steps\",\"type\":\"INT\",\"widget\":{\"name\":\"steps\"},\"link\":null},{\"localized_name\":\"cfg\",\"name\":\"cfg\",\"type\":\"FLOAT\",\"widget\":{\"name\":\"cfg\"},\"link\":null},{\"localized_name\":\"sampler_name\",\"name\":\"sampler_name\",\"type\":\"COMBO\",\"widget\":{\"name\":\"sampler_name\"},\"link\":null},{\"localized_name\":\"scheduler\",\"name\":\"scheduler\",\"type\":\"COMBO\",\"widget\":{\"name\":\"scheduler\"},\"link\":null},{\"localized_name\":\"denoise\",\"name\":\"denoise\",\"type\":\"FLOAT\",\"widget\":{\"name\":\"denoise\"},\"link\":null}],\"outputs\":[{\"localized_name\":\"LATENT\",\"name\":\"LATENT\",\"type\":\"LATENT\",\"links\":[10]}],\"properties\":{\"cnr_id\":\"comfy-core\",\"ver\":\"0.3.29\",\"Node name for S&R\":\"KSampler\"},\"widgets_values\":[930925108629876,\"randomize\",20,1,\"euler\",\"beta\",1]},{\"id\":12,\"type\":\"LoraLoader\",\"pos\":[589.8250732421875,-32.128360748291016],\"size\":[315,126],\"flags\":{},\"order\":2,\"mode\":0,\"inputs\":[{\"localized_name\":\"model\",\"name\":\"model\",\"type\":\"MODEL\",\"link\":13},{\"localized_name\":\"clip\",\"name\":\"clip\",\"type\":\"CLIP\",\"link\":14},{\"localized_name\":\"lora_name\",\"name\":\"lora_name\",\"type\":\"COMBO\",\"widget\":{\"name\":\"lora_name\"},\"link\":null},{\"localized_name\":\"strength_model\",\"name\":\"strength_model\",\"type\":\"FLOAT\",\"widget\":{\"name\":\"strength_model\"},\"link\":null},{\"localized_name\":\"strength_clip\",\"name\":\"strength_clip\",\"type\":\"FLOAT\",\"widget\":{\"name\":\"strength_clip\"},\"link\":null}],\"outputs\":[{\"localized_name\":\"MODEL\",\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[15]},{\"localized_name\":\"CLIP\",\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":[16,17]}],\"properties\":{\"cnr_id\":\"comfy-core\",\"ver\":\"0.3.29\",\"Node name for S&R\":\"LoraLoader\"},\"widgets_values\":[\"Design\\\\papercut.safetensors\",1,1]},{\"id\":3,\"type\":\"CheckpointLoaderSimple\",\"pos\":[79.66006469726562,92.55416870117188],\"size\":[315,98],\"flags\":{},\"order\":1,\"mode\":0,\"inputs\":[{\"localized_name\":\"ckpt_name\",\"name\":\"ckpt_name\",\"type\":\"COMBO\",\"widget\":{\"name\":\"ckpt_name\"},\"link\":null}],\"outputs\":[{\"localized_name\":\"MODEL\",\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[13]},{\"localized_name\":\"CLIP\",\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":[14]},{\"localized_name\":\"VAE\",\"name\":\"VAE\",\"type\":\"VAE\",\"links\":[12]}],\"properties\":{\"cnr_id\":\"comfy-core\",\"ver\":\"0.3.29\",\"Node name for S&R\":\"CheckpointLoaderSimple\"},\"widgets_values\":[\"flux1-dev-fp8.safetensors\"]}],\"links\":[[9,9,0,7,3,\"LATENT\"],[10,7,0,10,0,\"LATENT\"],[11,10,0,11,0,\"IMAGE\"],[12,3,2,10,1,\"VAE\"],[13,3,0,12,0,\"MODEL\"],[14,3,1,12,1,\"CLIP\"],[15,12,0,7,0,\"MODEL\"],[16,12,1,4,0,\"CLIP\"],[17,12,1,5,0,\"CLIP\"],[18,4,0,7,1,\"CONDITIONING\"],[19,5,0,7,2,\"CONDITIONING\"]],\"groups\":[],\"config\":{},\"extra\":{\"ds\":{\"scale\":0.9229599817706885,\"offset\":[-35.688659187764245,196.63612505210955]},\"frontendVersion\":\"1.16.8\",\"reroutes\":[{\"id\":10,\"pos\":[1739.99658203125,715.8917846679688],\"linkIds\":[9]},{\"id\":11,\"parentId\":10,\"pos\":[1732.232666015625,484.27398681640625],\"linkIds\":[9]},{\"id\":12,\"pos\":[1338.0205078125,-364.29925537109375],\"linkIds\":[15]},{\"id\":13,\"parentId\":14,\"pos\":[2071.25439453125,874.9921264648438],\"linkIds\":[12]},{\"id\":14,\"pos\":[529.2798461914062,874.7973022460938],\"linkIds\":[12]}],\"linkExtensions\":[{\"id\":9,\"parentId\":11},{\"id\":12,\"parentId\":13},{\"id\":15,\"parentId\":12}]},\"version\":0.4}\n```\n\n## Additional Context\n(Please add any additional context or steps to reproduce the error here)\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nMy english is not very good please you fill it \nthank you so much\n\n### Expected behavior\n\nMy english is not very good please you fill it \nthank you so much",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @saeedafm, this seems like an issue with the checkpoint you're using. Can you paste a simple script that shows the issue?"
      }
    ]
  },
  {
    "issue_number": 38680,
    "title": "Add support for Orthogonal Residual Updates",
    "author": "BootsofLagrangian",
    "state": "open",
    "created_at": "2025-06-09T03:04:07Z",
    "updated_at": "2025-06-09T14:22:28Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nThis is a feature request to implement \"Orthogonal Residual Update,\" a novel mechanism for training deep neural networks proposed in the paper \"[Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks](https://arxiv.org/abs/2505.11881)\".\n\nThe core idea is to modify the standard additive residual connection, which is pivotal in modern architectures like Transformers and ResNets. Instead of a standard update, $$x_{n+1} = x_{n} + f( σ(x_{n}))$$, the proposed method decomposes the module's output $$f(σ(x_{n} ))$$ into two components: one parallel to the input stream $$x_{n} $$ and one orthogonal to it. The update is then performed using only the orthogonal component.\n\nThis method has been shown to improve generalization accuracy and training stability across diverse architectures (including Vision Transformers and ResNetV2) and datasets. The implementation is computationally efficient, adding only a minimal number of FLOPs ($$O(sd)$$ in a Transformer block) compared to the main modules.\n\nPaper: https://arxiv.org/abs/2505.11881\nOriginal Code: https://github.com/BootsofLagrangian/ortho-residual\n\n### Motivation\n\nThe motivation for this proposal is to address a potential inefficiency in standard residual connections. In current models, a module's learned transformation can predominantly scale or modulate the magnitude of the existing feature stream, potentially underutilizing the module's capacity for learning entirely new, complex features. This can lead to representational redundancy.\n\n\nBy explicitly encouraging modules to contribute novel information (i.e., new directional components orthogonal to the existing representation), the Orthogonal Residual Update mechanism provides a path toward more efficient and stable training, as well as better generalization. For instance, a ViT-B model with this update achieved a +4.3%p top-1 accuracy gain on ImageNet-1k.\n\nGiven that the principle of a linear residual stream is a core feature in most transformers models, integrating this mechanism as a configurable option could provide a significant and low-cost benefit to the community for a wide range of models and tasks.\n\n### Your contribution\n\nYes, I can help by submitting a Issue. I have read the `CONTRIBUTING.MD` guide.\n\nMy proposed contribution would include:\n\n- Implementing the core Orthogonal Residual Update logic in a reusable way that can be applied to different layers (e.g., Attention and MLP blocks).\n- Adding a first concrete implementation with the OrthoViT model, including `OrthoViTConfig`, `OrthoViTModel`, and `OrthoViTForImageClassification`.\n- Providing comprehensive tests for the new model and the orthogonal update logic to ensure correctness and integration with the library's testing framework.\n - Adding the necessary documentation for the new model, following the library's standards.\n - An OrthoViT model example : https://huggingface.co/BootsofLagrangian/ortho-vit-b-imagenet1k-hf",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "This is interesting, but it will be quite a large PR and I think we'd need to see more widespread use of this technique in the field before we add it to the `Trainer`! cc @sunmarc"
      }
    ]
  },
  {
    "issue_number": 38663,
    "title": "MODEL_FOR_MASK_GENERATION_MAPPING_NAMES variable is present twice in modeling_auto.py",
    "author": "sbucaille",
    "state": "closed",
    "created_at": "2025-06-07T20:57:42Z",
    "updated_at": "2025-06-09T13:40:47Z",
    "labels": [],
    "body": "Hi,\n\nWas looking around the Auto classes for some reason and noticed that ``MODEL_FOR_MASK_GENERATION_MAPPING_NAMES`` is present twice in ``modeling_auto.py``. Seems to come from this [PR](https://github.com/huggingface/transformers/pull/35147)\n\nSee here\nhttps://github.com/huggingface/transformers/blob/ebeec13609b537f9c760292354118c9d1d63f5a0/src/transformers/models/auto/modeling_auto.py#L1529C1-L1539C2\n\nI don't think it is intended, I'll make a PR.\n \nSteven",
    "comments": []
  },
  {
    "issue_number": 38667,
    "title": "TypeError: 'NoneType' object is not iterable in ESM when using DDP training",
    "author": "dbleyl",
    "state": "open",
    "created_at": "2025-06-08T01:35:54Z",
    "updated_at": "2025-06-09T13:31:57Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n2 machines:\n\n- `transformers` version: 4.52.4\n- Platform: Linux-6.14.10-arch1-1-x86_64-with-glibc2.41\n- Python version: 3.12.10\n- Huggingface_hub version: 0.32.4\n- Safetensors version: 0.5.3\n- Accelerate version: 1.7.0\n- Accelerate config:    - compute_environment: LOCAL_MACHINE\n        - distributed_type: MULTI_GPU\n        - mixed_precision: bf16\n        - use_cpu: False\n        - debug: False\n        - num_processes: 2\n        - machine_rank: 0\n        - num_machines: 1\n        - gpu_ids: all\n        - rdzv_backend: static\n        - same_network: True\n        - main_training_function: main\n        - enable_cpu_affinity: False\n        - downcast_bf16: no\n        - tpu_use_cluster: False\n        - tpu_use_sudo: False\n        - tpu_env: []\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.7.1+cu128 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: yes\n- Using GPU in script?: yes, via accelerate command.\n- GPU type: NVIDIA GeForce RTX 4060 Ti\n- `transformers` version: 4.52.4\n- Platform: Linux-6.14.10-arch1-1-x86_64-with-glibc2.41\n- Python version: 3.12.10\n- Huggingface_hub version: 0.32.4\n- Safetensors version: 0.5.3\n- Accelerate version: 1.7.0\n- Accelerate config:    - compute_environment: LOCAL_MACHINE\n        - distributed_type: MULTI_GPU\n        - mixed_precision: bf16\n        - use_cpu: False\n        - debug: False\n        - num_processes: 2\n        - machine_rank: 0\n        - num_machines: 1\n        - gpu_ids: all\n        - rdzv_backend: static\n        - same_network: True\n        - main_training_function: main\n        - enable_cpu_affinity: False\n        - downcast_bf16: no\n        - tpu_use_cluster: False\n        - tpu_use_sudo: False\n        - tpu_env: []\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.7.1+cu128 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: ddp via accelerate\n- Using GPU in script?: yes\n- GPU type: NVIDIA GeForce RTX 5090\n\n### Who can help?\n\n@MekkCyber @Cyrilvallez \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nJust configure accelerate and load an ESM model other than `EsmModel`, such as `EsmForMaskedLM` or `EsmForSequenceClassification` using the `.from_pretrained(...)` syntax.\n\nYou will see:\n\n```bash\ntransformers/modeling_utils.py\", line 6031, in caching_allocator_warmup\n[rank1]:     re.compile(\"|\".join([re.escape(plan) for plan in model._tp_plan]))\n[rank1]:                                                      ^^^^^^^^^^^^^^\n[rank1]: TypeError: 'NoneType' object is not iterable\n```\n\nThe issue is that `EsmModel` calls `post_init` in `__init__` but the other three classes do not.  This is the same problem in [Pull Request 37708](https://github.com/huggingface/transformers/pull/37708), which was rejected for proposing a check instead of fixing the model.\n\nI have prepared a PR for this and ready to submit it.  I believe it follows all the guidelines.  I'm opening this for tracking purposes.\n\n### Expected behavior\n\n`tp_plan` should be initialized by `post_init` and not raise the error reported.  Pull request is ready to be requested.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @dbleyl, yes, `post_init()` should be called at the end of `__init__()`. If you can make a PR to fix it, that'd be great!"
      }
    ]
  },
  {
    "issue_number": 38659,
    "title": "NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.",
    "author": "lucavlasblom",
    "state": "closed",
    "created_at": "2025-06-07T15:52:53Z",
    "updated_at": "2025-06-09T13:26:26Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.4\n- Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n- Python version: 3.11.13\n- Huggingface_hub version: 0.32.4\n- Safetensors version: 0.5.3\n- Accelerate version: 1.7.0\n- Accelerate config: \tnot found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0+cu124 (False)\n- Tensorflow version (GPU?): 2.18.0 (False)\n- Flax version (CPU?/GPU?/TPU?): 0.10.6 (cpu)\n- Jax version: 0.5.2\n- JaxLib version: 0.5.1\n- Using distributed or parallel set-up in script?: <fill in>\n\n### Who can help?\n\n@ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nfrom datasets import DatasetDict, load_dataset\ndata_files = {\n    \"train\": \"train.txt\",\n    \"validation\": \"dev.txt\",\n    \"test\": \"test.txt\"\n}\n\ndataset = load_dataset(\"text\", data_files=data_files, split={\n    \"train\": \"train\",\n    \"validation\": \"validation\",\n    \"test\": \"test\"\n})\n\n\nNotImplementedError                       Traceback (most recent call last)\n[<ipython-input-51-784f867eba96>](https://localhost:8080/#) in <cell line: 0>()\n      5 }\n      6 \n----> 7 dataset = load_dataset(\"conll2003\", data_files=data_files, split={\n      8     \"train\": \"train\",\n      9     \"validation\": \"validation\",\n\n1 frames\n[/usr/local/lib/python3.11/dist-packages/datasets/builder.py](https://localhost:8080/#) in as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\n   1171         is_local = not is_remote_filesystem(self._fs)\n   1172         if not is_local:\n-> 1173             raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")\n   1174         if not os.path.exists(self._output_dir):\n   1175             raise FileNotFoundError(\n\nNotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.\n\n### Expected behavior\n\nI expect load_dataset to load my train, evaluate and test set that are locally stored in google colab.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @lucavlasblom, can you ask this in the community support space? One of the engineers there should be able to help! https://huggingface.co/spaces/transformers-community/support"
      }
    ]
  },
  {
    "issue_number": 38650,
    "title": "Support of Qwen3 GGUF model",
    "author": "Auth0rM0rgan",
    "state": "open",
    "created_at": "2025-06-06T20:11:23Z",
    "updated_at": "2025-06-09T13:19:29Z",
    "labels": [],
    "body": "Hi, I am getting the following error when I want to use the GGUF model with Qwen3\n\"ValueError: GGUF model with architecture qwen3 is not supported yet.\"\n\nI have the latest transformers and gguf-0.17.0\n```\nself.tokenizer = AutoTokenizer.from_pretrained(model_name, gguf_file= \"Qwen3-0.6B-Q2_K_L.gguf\",use_fast=True)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = \"<pad>\"\n            self.tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n        self.tokenizer.padding_side = \"left\"\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            gguf_file = \"Qwen3-0.6B-Q2_K_L.gguf\",\n            pad_token_id=self.tokenizer.pad_token_id,\n            trust_remote_code=True,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n        )\n```\nHow can I use the gguf model of Qwen3 with transformers? Could you please add the support of it?\n\nThanks!",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "There is an open PR for this #38645"
      }
    ]
  },
  {
    "issue_number": 38066,
    "title": "`AutoModel.from_pretrained(...)` (with explicit `device_map` unset) fails under `with torch.device(\"meta\")` with PyTorch 2.6.0 and 2.7.0",
    "author": "vadimkantorov",
    "state": "open",
    "created_at": "2025-05-10T20:35:19Z",
    "updated_at": "2025-06-09T12:56:03Z",
    "labels": [],
    "body": "```python\n# from torch.nn.attention.flex_attention import BlockMask, flex_attention\nfrom transformers import AutoModel\nimport torch\n\nwith torch.device('meta'):\n    AutoModel.from_pretrained('Qwen/Qwen2.5-0.5B', trust_remote_code=True)\n````\n\nI found this code in the wild in https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/f6d1ec77ce2ce18f3d925a1014c9e4d6b4ad3072/orz/ppo/actors.py#L745-L746 (linked issue https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/issues/71)\n\nfails with:\n```\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n[<ipython-input-1-00ba4c43be18>](https://localhost:8080/#) in <cell line: 0>()\n      4 \n      5 with torch.device('meta'):\n----> 6     AutoModel.from_pretrained('Qwen/Qwen2.5-0.5B', trust_remote_code=True)\n\n6 frames\n[/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n    569             if model_class.config_class == config.sub_configs.get(\"text_config\", None):\n    570                 config = config.get_text_config()\n--> 571             return model_class.from_pretrained(\n    572                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n    573             )\n\n[/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py](https://localhost:8080/#) in _wrapper(*args, **kwargs)\n    277         old_dtype = torch.get_default_dtype()\n    278         try:\n--> 279             return func(*args, **kwargs)\n    280         finally:\n    281             torch.set_default_dtype(old_dtype)\n\n[/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\n   4397                 offload_index,\n   4398                 error_msgs,\n-> 4399             ) = cls._load_pretrained_model(\n   4400                 model,\n   4401                 state_dict,\n\n[/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py](https://localhost:8080/#) in _load_pretrained_model(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\n   4831             # Skip it with fsdp on ranks other than 0\n   4832             elif not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):\n-> 4833                 disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n   4834                     model_to_load,\n   4835                     state_dict,\n\n[/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py](https://localhost:8080/#) in decorate_context(*args, **kwargs)\n    114     def decorate_context(*args, **kwargs):\n    115         with ctx_factory():\n--> 116             return func(*args, **kwargs)\n    117 \n    118     return decorate_context\n\n[/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py](https://localhost:8080/#) in _load_state_dict_into_meta_model(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\n    822                     param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n    823 \n--> 824                 _load_parameter_into_model(model, param_name, param.to(param_device))\n    825 \n    826             else:\n\n[/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py](https://localhost:8080/#) in __torch_function__(self, func, types, args, kwargs)\n    102         if func in _device_constructors() and kwargs.get('device') is None:\n    103             kwargs['device'] = self.device\n--> 104         return func(*args, **kwargs)\n    105 \n    106 # NB: This is directly called from C++ in torch/csrc/Device.cpp\n\nNotImplementedError: Cannot copy out of meta tensor; no data!\n```\n\nAlso, unless uncommenting the first line, it also fails on 2.6.0 with `RuntimeError: Tensor.item() cannot be called on meta tensors`: \n- https://github.com/pytorch/pytorch/issues/153330",
    "comments": [
      {
        "user": "vadimkantorov",
        "body": "Probably in the code\n\nhttps://github.com/huggingface/transformers/blob/716819b8309324302e00a3488a3c3d6faa427f79/src/transformers/modeling_utils.py#L833-L834\n\nthere `\"cpu\"` should be replaced  with `\"meta\"` even if it's rank0, if the default device type is changed to meta (because of https://github.com/pytorch/pytorch/issues/148874, this can be checked as `torch.empty(()).device == torch.device(\"meta\")`)\n\nIn all of modeling_utils.py there are 3-4 places where this needs to be fixed to not fail under `with torch.device(\"meta\")` (which forces the loaded params to actually be of device type `\"meta\"` which leads to not being able to copy from them to `param_device = \"cpu\"`)\n\n---\n\nI think the code I found in the wild does this as a smoke test, e.g. to force downloading the model weights, check that the model code works etc?"
      },
      {
        "user": "Rocketknight1",
        "body": "Hi, I'm not sure `with torch.device(\"meta\")` works cleanly with `from_pretrained()`! Try `device_map=\"meta\"` in the call instead, but note that this doesn't actually load any weights, since tensors on the meta device store no data, which somewhat defeats the purpose of `from_pretrained`"
      },
      {
        "user": "vadimkantorov",
        "body": "It actually worked after my fixes to ~three lines. The only problems came from usage of default `cpu`. I can submit a draft PR if you'd like to take a look.\n\nI think it's a valid way for smoke tests / getting HF to download everything and check that sizes in the checkpoint are matching the meta tensors in the model etc. And supporting colloquial PyTorch's `with torch.device(\"meta\"):` (in addition to `device_map=\"meta\"`) would be good too, given that it's a small fix"
      }
    ]
  },
  {
    "issue_number": 36638,
    "title": "[BUG] Batch inference DDP + zero stage 3 = inference code hangs",
    "author": "ShengYun-Peng",
    "state": "closed",
    "created_at": "2025-03-11T03:20:47Z",
    "updated_at": "2025-06-09T08:03:53Z",
    "labels": [],
    "body": "https://github.com/deepspeedai/DeepSpeed/issues/7128\n\nI ran the batch inference code with deepspeed generation, not the vllm one. The code hangs while I set zero stage = 3. I created a minimal code snippet for you to debug the error. \n\n```python\nimport os\n\nimport torch\nimport torch.distributed as dist\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nimport deepspeed\n\n# Initialize distributed environment\ndef setup_distributed():\n    dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n    local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n    torch.cuda.set_device(local_rank)\n    return local_rank\n\n\ndef load_model(model_name=\"facebook/opt-1.3b\", local_rank=0):\n    # Ensure distributed environment is set up\n    if not dist.is_initialized():\n        dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n\n    world_size = dist.get_world_size()  # Number of GPUs available\n    torch.cuda.set_device(local_rank)  # Assign each process to a GPU\n\n    print(\n        f\"Loading model {model_name} on rank {local_rank}, using {world_size} GPUs for model parallelism\"\n    )\n\n    # Load model and tokenizer\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # ✅ DeepSpeed Inference config for Model Parallelism\n    ds_config = {\n        # \"replace_with_kernel_inject\": False,  # Enables optimized inference kernels\n        \"tensor_parallel\": {\"tp_size\": 1},  # Enables Model Parallelism\n        \"dtype\": \"bf16\"\n        if torch.cuda.is_bf16_supported()\n        else \"fp16\",  # Automatic dtype selection\n    }\n\n    # ✅ Initialize DeepSpeed for Model Parallel Inference\n    model = deepspeed.init_inference(model, config=ds_config)\n\n    return model, tokenizer\n\n\n# Perform inference with data parallelism\ndef batch_inference(model, tokenizer, prompts, local_rank):\n    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\n        f\"cuda:{local_rank}\"\n    )\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=150, synced_gpus=True)\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n\ndef main():\n    local_rank = setup_distributed()\n    model, tokenizer = load_model(local_rank=local_rank)\n\n    # Each GPU gets a different batch\n    global_batch = [\n        [\n            \"What is AI?\",\n            \"Explain deep learning.\",\n        ],  # Batch for GPU 0\n        [\n            \"Tell me a joke.\",\n            \"What is reinforcement learning? Tell me all the details\",\n        ],  # Batch for GPU 1\n    ]\n    prompts = global_batch[local_rank] if local_rank < len(global_batch) else []\n\n    print(f\"GPU {local_rank} prompts:\", prompts)\n    # Perform batch inference\n    results = batch_inference(model, tokenizer, prompts, local_rank)\n    print(f\"GPU {local_rank} results:\", results)\n\n    dist.barrier()  # Ensure all GPUs finish\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nRun the code with \n```bash\nNCCL_DEBUG=INFO NCCL_BLOCKING_WAIT=1 NCCL_ASYNC_ERROR_HANDLING=1 deepspeed --num_gpus 2 test_deepspeed.py\n```\n\nThe code should run without error because it's DDP. \nNow, if we change set \"tensor_parallel\": {\"tp_size\": 1} -> \"tensor_parallel\": {\"tp_size\": 2} and rerun the code. The code hangs forever. Note that the bug happens when DDP + TP are enabled.  ",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @muellerzr for deepspeed, @arthurzucker @cyrilvallez for TP changes"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "ShengYun-Peng",
        "body": "Any Updates on this?"
      }
    ]
  },
  {
    "issue_number": 38468,
    "title": "AssertionError: Torch not compiled with CUDA enabled when using device_map=\"auto\" in Ascend NPU",
    "author": "jiaqiw09",
    "state": "open",
    "created_at": "2025-05-29T08:08:15Z",
    "updated_at": "2025-06-09T06:33:26Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\nAscend NPU\ntransformers>=4.50.0\ntorch 2.1\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nWhen using device_map with Ascend NPU devices in transformers >=4.50.0, loading models fails with assertion errors. The issue occurs because the new loading implementation in _load_state_dict_into_meta_model doesn't properly handle integer device indices for NPU devices, whereas previous versions (<4.50.0) used accelerate.utils.set_module_tensor_to_device which correctly converts integer indices to device strings like \"npu:0\".\n\nOn an Ascend NPU system, attempt to load a model with device mapping:\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"gpt2\",\n    device_map=\"auto\",  # Or custom device_map with integer indices\n    torch_dtype=torch.float16\n)\n```\n\nObserve the failure with stack trace pointing to modeling_utils.py in _load_state_dict_into_meta_model\n\n### Expected behavior\n\nIn transformers <=4.49.0, device mapping used accelerate.utils.set_module_tensor_to_device  for various device types\n\n<img width=\"693\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4abec3a3-6eea-4782-bed7-e8d54116968e\" />\n\n\nIn transformers >=4.50.0, the new _load_state_dict_into_meta_model directly uses device values from device_map without converting integer indices to device-specific strings\n\nFor NPU devices, integer indices (like 0) are not automatically converted to proper device strings (\"npu:0\")",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @sunmarc"
      },
      {
        "user": "SunMarc",
        "body": "> In transformers >=4.50.0, the new _load_state_dict_into_meta_model directly uses device values from device_map without converting integer indices to device-specific strings\n\nThis might indeed be an issue. Would you like to fix this in transformers by adding this mapping @jiaqiw09 ? \n\n"
      },
      {
        "user": "jiaqiw09",
        "body": "> > In transformers >=4.50.0, the new _load_state_dict_into_meta_model directly uses device values from device_map without converting integer indices to device-specific strings\n> \n> This might indeed be an issue. Would you like to fix this in transformers by adding this mapping [@jiaqiw09](https://github.com/jiaqiw09) ?\n\n\n@SunMarc  Thanks for the suggestion. I checked the code, and I believe the most appropriate place to fix this is here:\n\n[https://github.com/huggingface/transformers/blob/main/src/transformers/modeling\\_utils.py#L4365](https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L4365)\n\n```python\n        # change device_map into a map if we passed an int, a str or a torch.device\n        if isinstance(device_map, torch.device):\n            device_map = {\"\": device_map}\n        elif isinstance(device_map, str) and device_map not in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]:\n            try:\n                device_map = {\"\": torch.device(device_map)}\n            except RuntimeError:\n                raise ValueError(\n                    \"When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or \"\n                    f\"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}.\"\n                )\n        elif isinstance(device_map, int):\n            if device_map < 0:\n                raise ValueError(\n                    \"You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' \"\n                )\n            else:\n                device_map = {\"\": device_map}\n```\n\nMy plan is to modify the `elif isinstance(device_map, int):` block, and convert the integer index into a proper device string before wrapping it into a dictionary. For example:\n\n```python\ndef get_current_device(device_index: Union[str, int]):\n    r\"\"\"\n    Return current device.\n    \"\"\"\n    state = PartialState()\n    device_type = state.device.type.lower()\n    device = f\"{device_type}:{device_index}\"\n    return device\n```\n\nI haven’t found a ready-to-use function in `Accelerate` that provides this functionality, so I think we may need to handle this manually. However, I'm aware that this could potentially touch a broader range of logic, so I'd appreciate any feedback or suggestions on the best way to integrate this safely."
      }
    ]
  },
  {
    "issue_number": 38312,
    "title": "LagKV for key-value compression",
    "author": "JoelSeniorLiang",
    "state": "open",
    "created_at": "2025-05-23T03:54:42Z",
    "updated_at": "2025-06-09T03:12:44Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nIntegrated the LagKV for KV compression.\n\nhttps://github.com/AI-Lab-China-Merchants-Bank/LagKV\n\n### Motivation\n\nRight now, the Cache utils is expanding rapidly. But, it still lack an efficient token eviction algorithm except SinkCache and SlidingWindowCache, which show limited performance in information retrieval tasks.\n\nLagKV provides a compression strategy which offers follow features:\n\n1. High accuracy performance in Needle-In-A-Haystack Passkey retrieval and other normal tasks.\n2. Training free: no finetune required.\n3. Warmup free: no pre warm up required, no additional parameters attached.\n4. Efficient: negligible computation overhead, Flash-Attention compatible.\n\n### Your contribution\n\nAuthor of LagKV.\n\nImplement a version of Cache Interface.",
    "comments": [
      {
        "user": "molbap",
        "body": "Hi @JoelSeniorLiang ! This looks very promising, do you plan to open a PR to add LagKV to `transformers`? We can review and help along the way. Seems you are inheriting from `DynamicCache` so the best way would be to add it to `cache_utils.py` and import it from there. What do you think? Also cc @zucchini-nlp 👀 "
      },
      {
        "user": "zucchini-nlp",
        "body": "Super cool! I think this can be another good addition as the custom generation technique imported from HF hub. We've been experimenting with this new workflow for generation feature requests, more info under [this comment](https://github.com/huggingface/transformers/pull/38055#issuecomment-2900553588)"
      },
      {
        "user": "gante",
        "body": "Building on top of @zucchini-nlp's comment: here is a [repo](https://huggingface.co/transformers-community/sink_cache) containing a new cache implementation with custom generation"
      }
    ]
  },
  {
    "issue_number": 36111,
    "title": "Add Deepseek-VL2",
    "author": "ArthurZucker",
    "state": "open",
    "created_at": "2025-02-10T09:35:34Z",
    "updated_at": "2025-06-09T00:09:16Z",
    "labels": [
      "New model"
    ],
    "body": "### Model description\n\nDeepseek-VL2: \n- [paper](https://arxiv.org/abs/2412.10302) \n- [code](https://github.com/deepseek-ai/DeepSeek-VL2)\n- [weights](https://huggingface.co/collections/deepseek-ai/deepseek-vl2-675c22accc456d3beb4613ab)\n\n### Open source status\n\n- [ ] The model implementation is available\n- [ ] The model weights are available\n\n### Provide useful links for the implementation\n\n_No response_",
    "comments": [
      {
        "user": "geetu040",
        "body": "Prerequisite is Deepseek-V2: https://github.com/huggingface/transformers/issues/35317"
      },
      {
        "user": "mariebbz",
        "body": "Hi, I’ve been assigned to add support for this model.\nI’ve started looking into it and will post updates here @ArthurZucker @jadechoghari"
      }
    ]
  },
  {
    "issue_number": 37524,
    "title": "A type error in the Template writing document",
    "author": "Vegetabledog-BUAA",
    "state": "closed",
    "created_at": "2025-04-15T03:11:24Z",
    "updated_at": "2025-06-08T08:02:46Z",
    "labels": [],
    "body": "https://huggingface.co/docs/transformers/main/en/chat_templating_writing\n\nIn the example template as shown below.\n{%- for message in messages %}\n    {{- '<|' + message['role'] + |>\\n' }}\n    {{- message['content'] + eos_token }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- endif %}\n\nA quote is missing in the template before the  |>\\n' which after the  message['role'].\n\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' }}\n    {{- message['content'] + eos_token }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- endif %}",
    "comments": [
      {
        "user": "julien-c",
        "body": "will move this issue to the transformers repo, ok?"
      },
      {
        "user": "julien-c",
        "body": "Would you want to try and open a PR to fix this, @Vegetabledog-BUAA?"
      },
      {
        "user": "Rocketknight1",
        "body": "Yes, this is an error. If you can open a PR to fix it, please do!"
      }
    ]
  },
  {
    "issue_number": 37857,
    "title": "ImageInput doesn't include JAX ndarray and TensorFlow tensor",
    "author": "arjunaskykok",
    "state": "closed",
    "created_at": "2025-04-29T10:23:02Z",
    "updated_at": "2025-06-08T08:02:33Z",
    "labels": [],
    "body": "In the [`image_utils.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py) file, we can see the following code:\n\n```python\nclass ImageType(ExplicitEnum):\n    PIL = \"pillow\"\n    TORCH = \"torch\"\n    NUMPY = \"numpy\"\n    TENSORFLOW = \"tensorflow\"\n    JAX = \"jax\"\n\n\ndef get_image_type(image):\n    if is_pil_image(image):\n        return ImageType.PIL\n    if is_torch_tensor(image):\n        return ImageType.TORCH\n    if is_numpy_array(image):\n        return ImageType.NUMPY\n    if is_tf_tensor(image):\n        return ImageType.TENSORFLOW\n    if is_jax_tensor(image):\n        return ImageType.JAX\n    raise ValueError(f\"Unrecognised image type {type(image)}\")\n\n\ndef is_valid_image(img):\n    return is_pil_image(img) or is_numpy_array(img) or is_torch_tensor(img) or is_tf_tensor(img) or is_jax_tensor(img)\n```\n\nIt supports PIL image, numpy ndarray, torch tensor, tensorflow tensor, JAX ndarray.\n\nBut `ImageInput` doesn't!\n\n```python\nImageInput = Union[\n    \"PIL.Image.Image\", np.ndarray, \"torch.Tensor\", list[\"PIL.Image.Image\"], list[np.ndarray], list[\"torch.Tensor\"]\n]  # noqa\n```\n\nI think it should be this way:\n\n```python\nImageInput = Union[\n    \"PIL.Image.Image\", np.ndarray, \"torch.Tensor\", \"tf.Tensor\", \"jax.numpy.ndarray\", list[\"PIL.Image.Image\"], list[np.ndarray], list[\"torch.Tensor\"], list[\"tf.Tensor\"], list[\"jax.numpy.ndarray\"]\n]  # noqa\n```\n\nWhat do you think?",
    "comments": [
      {
        "user": "arjunaskykok",
        "body": "`VideoInput` also suffers the same problem.\n\n```python\nVideoInput = Union[\n    list[\"PIL.Image.Image\"],\n    \"np.ndarray\",\n    \"torch.Tensor\",\n    list[\"np.ndarray\"],\n    list[\"torch.Tensor\"],\n    list[list[\"PIL.Image.Image\"]],\n    list[list[\"np.ndarray\"]],\n    list[list[\"torch.Tensor\"]],\n]  # noqa\n```"
      },
      {
        "user": "zucchini-nlp",
        "body": "AFAIK we will not be adding new models with Jax or TF, and since not every model supports them I don't think it is worth adding in `ImageInput`. cc @qubvel to verify"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37859,
    "title": "BUG: ModernBERT flash-attention2 incompatible on Ascend NPU",
    "author": "wakaka6",
    "state": "closed",
    "created_at": "2025-04-29T11:32:00Z",
    "updated_at": "2025-06-08T08:02:31Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.51.3\n- Platform: Linux-5.4.0-125-generic-aarch64-with-glibc2.35\n- Python version: 3.10.12\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: not installed\n- Accelerate config: not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.4.0 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using NPU in script?: <fill in>\n- NPU type: Ascend310P3\n- CANN version: 8.0.0\n\n### Who can help?\n\nAscend NPU: @ivarflakstad\nRelated PR #36696 by @FightingZhen \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nHere is the minimal reproducible code.\n```py\nimport torch\nimport torch_npu\nimport numpy as np\nfrom transformers import AutoTokenizer\nfrom transformers.models.modernbert.modeling_modernbert import ModernBertForSequenceClassification\n\n\nmodel = ModernBertForSequenceClassification.from_pretrained(\"answerdotai/ModernBERT-base\", torch_dtype=torch.float16).to(\"npu:0\")\n```\n\n\n\n\nException log\n\n```\nTraceback (most recent call last):\n  File \"/app/modernBERT/mini.py\", line 8, in <module>\n    model = ModernBertForSequenceClassification.from_pretrained(\"answerdotai/ModernBERT-base\", torch_dtype=torch.float16).to(\"npu:0\")\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 279, in _wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4342, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py\", line 1184, in __init__\n    self.model = ModernBertModel(config)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py\", line 862, in __init__\n    [ModernBertEncoderLayer(config, layer_id) for layer_id in range(config.num_hidden_layers)]\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py\", line 862, in <listcomp>\n    [ModernBertEncoderLayer(config, layer_id) for layer_id in range(config.num_hidden_layers)]\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py\", line 527, in __init__\n    self.attn = ModernBertAttention(config=config, layer_id=layer_id)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py\", line 479, in __init__\n    self.rotary_emb = ModernBertUnpaddedRotaryEmbedding(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/modernbert/modeling_modernbert.py\", line 165, in __init__\n    super().__init__(dim=dim, base=base, pos_idx_in_fp32=True, device=device, interleaved=False)\nTypeError: object.__init__() takes exactly one argument (the instance to initialize)\n[ERROR] 2025-04-29-19:20:46 (PID:610680, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception\n```\n\n## Bug description\n\nWhen the `from_pretrained` parameter attn_implementation is not specified, modernBERT is automatically set to the fa2 implementation and tested for fa2 availability.\nhttps://github.com/huggingface/transformers/blob/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src/transformers/models/modernbert/modeling_modernbert.py#L655-L673\n\nFor Ascend NPU will set `config._attn_ implementation` to fa2, which causes modernBERT's usability test on fa2 to pass. However, the modernBERT model definition is highly integrated with the `flash-attn` api, which is not adapted to the Ascend NPU, and this will eventually lead to an exception.\n\nhttps://github.com/huggingface/transformers/blob/a847d4aa6bd2279f5be235dc0fd862f58f7403d1/src/transformers/modeling_utils.py#L2262-L2271\n\n\n\n\n\n### Expected behavior\n\nworking",
    "comments": [
      {
        "user": "FightingZhen",
        "body": "@wakaka6 Hi, I have read your above description. \n\nThe support for Model `modernbert` is not included in the PR [#36696](https://github.com/huggingface/transformers/pull/36696), because it relies on a triton related api `apply_rotary` in package `flash-attn`, which is not supported on Ascend NPU yet. \n\nTherefore, logics about Flash Attention 2 in this model still keeps the original one, which judge the possibility of using Flash Attention 2 with the available of package `flash-attn`. This is the reason for why you encounter exception.\n\nYou can try to set `config._attn_implementation=sdpa` to run in SDPA mode, hope to solve your problem :)"
      },
      {
        "user": "wakaka6",
        "body": "> [@wakaka6](https://github.com/wakaka6) Hi, I have read your above description.\n> \n> The support for Model `modernbert` is not included in the PR [#36696](https://github.com/huggingface/transformers/pull/36696), because it relies on a triton related api `apply_rotary` in package `flash-attn`, which is not supported on Ascend NPU yet.\n> \n> Therefore, logics about Flash Attention 2 in this model still keeps the original one, which judge the possibility of using Flash Attention 2 with the available of package `flash-attn`. This is the reason for why you encounter exception.\n> \n> You can try to set `config._attn_implementation=sdpa` to run in SDPA mode, hope to solve your problem :)\n\nThanks for the solution, but unfortunately I tried it before and it seems that torch_npu compatibility isn't covered, which would throw up another problem. \n```\nbackend='inductor' raised.\nAssertionError: Device npu not supported\n```"
      },
      {
        "user": "FightingZhen",
        "body": "> > [@wakaka6](https://github.com/wakaka6) Hi, I have read your above description.\n> > The support for Model `modernbert` is not included in the PR [#36696](https://github.com/huggingface/transformers/pull/36696), because it relies on a triton related api `apply_rotary` in package `flash-attn`, which is not supported on Ascend NPU yet.\n> > Therefore, logics about Flash Attention 2 in this model still keeps the original one, which judge the possibility of using Flash Attention 2 with the available of package `flash-attn`. This is the reason for why you encounter exception.\n> > You can try to set `config._attn_implementation=sdpa` to run in SDPA mode, hope to solve your problem :)\n> \n> Thanks for the solution, but unfortunately I tried it before and it seems that torch_npu compatibility isn't covered, which would throw up another problem.\n> \n> ```\n> backend='inductor' raised.\n> AssertionError: Device npu not supported\n> ```\n\nCan you show more detail error messages? Through current shown message, it seems that this model heavily relies on Triton, which is not supported on Ascend NPU yet."
      }
    ]
  },
  {
    "issue_number": 37862,
    "title": "Llama2 can output scores normally, but Llama3 outputs full inf",
    "author": "Huangshuo621",
    "state": "closed",
    "created_at": "2025-04-29T12:45:48Z",
    "updated_at": "2025-06-08T08:02:29Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n\ntransformers:4.44.2\n\n`outputs = self.model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    max_new_tokens=max_length,\n    return_dict_in_generate=True,\n    output_scores=True,\n)\n\nprint(\"outputs.scores:\", outputs.scores)\n\n`\n\n我在生成内容时同时获取scores，并且在使用llama2-chat可以正常输出score，llama3-8b-Instruct却输出了全inf\nllama2:\n![Image](https://github.com/user-attachments/assets/e8b26895-08ab-4bba-93cc-1156b81bf9a7)\nllama3:\n\n![Image](https://github.com/user-attachments/assets/a41f46f7-3505-4048-827e-e6718b7de34f)\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n\ntransformers:4.44.2\n\n`outputs = self.model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    max_new_tokens=max_length,\n    return_dict_in_generate=True,\n    output_scores=True,\n)\n\nprint(\"outputs.scores:\", outputs.scores)\n\n### Expected behavior\n\nSolution ,I really need",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37883,
    "title": "ModernBert Tokenizer flag `is_split_into_words` not working",
    "author": "bablf",
    "state": "closed",
    "created_at": "2025-04-30T08:11:16Z",
    "updated_at": "2025-06-08T08:02:27Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nUsing the Tokenizer for ModernBERT does not work as expected when using the `is_split_into_words` flag. The Tokenizer does not insert spaces, like other tokenizers do. \n\nMinimal sample to reproduce: \n\n\n\n `transformers` version: 4.50.1\n- Platform: Linux-6.11.0-21-generic-x86_64-with-glibc2.39\n- Python version: 3.10.15\n- Huggingface_hub version: 0.26.0\n- Safetensors version: 0.4.5\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA GeForce RTX 3080 Ti Laptop GP  \n\n### Who can help?\n\n@ArthurZucker @itazap\n\n### Information\n\n- [x] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n``` \nfrom transformers import AutoTokenizer\ntk = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\") \ntk.tokenize([\"This\", \"is\", \"a\", \"test\"], is_split_into_words=True) \n```\nOutput: `['This', 'is', 'a', 'test']` which is missing the spaces\nExpected Output: `['This', 'Ġis', 'Ġa', 'Ġtest']`\n\nThis leads to different tokenization: \n``` tk.encode([\"This\", \"is\", \"a\", \"test\"], is_split_into_words=True) ``` \n[50281, 1552, 261, 66, 2566, 50282]\n```tk.encode(\"This is a test\")```\n[50281, 1552, 310, 247, 1071, 50282]\n\n`tk.tokenize(\"This is a test\")` and `tk.encode(\"This is a test\")` work as expected.\n\n### Expected behavior\n\nsee above",
    "comments": [
      {
        "user": "ArthurZucker",
        "body": "cc @itazap ! "
      },
      {
        "user": "itazap",
        "body": "Hey! Thanks for the clear reproducer 😊 Agreed that the outputs don't match. It works with passing `add_prefix_space` ! Let me know if the below fix works on your end\n```python\nAutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\", add_prefix_space=True) \n``` \n\n\nIf the behavior is expected to follow models such as Bart, it should throw an error if passing `is_split_into_words` but not `add_prefix_space` \n\nhttps://github.com/huggingface/transformers/blob/5b223bbc8c6ee319f9e32bd4046bbbda961b912e/src/transformers/models/bart/tokenization_bart_fast.py#L217-L221\n\nthis arg unforunately not in our test suite 😅 @ArthurZucker are we keeping this param in the future? if so I can add this Error to `PretrainedTokenizerFast` + a test"
      },
      {
        "user": "jgyasu",
        "body": "> this arg unforunately not in our test suite 😅 @ArthurZucker are we keeping this param in the future? if so I can add this Error to PretrainedTokenizerFast + a test\n\nHi @itazap @ArthurZucker if it is decided to add the test, can I please work on it? I have used HF a lot and wanted to start with contributing :)"
      }
    ]
  },
  {
    "issue_number": 37900,
    "title": "Error in input expansion for `generate` with `num_return_sequences` > 1 for multi-image inputs to `AutoModelForImageTextToText`",
    "author": "saujasv",
    "state": "closed",
    "created_at": "2025-04-30T18:59:49Z",
    "updated_at": "2025-06-08T08:02:25Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n```\n- `transformers` version: 4.51.3\n- Platform: Linux-5.14.0-427.40.1.el9_4.x86_64-x86_64-with-glibc2.34\n- Python version: 3.12.7\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.4.0\n- Accelerate config:    - compute_environment: LOCAL_MACHINE\n        - distributed_type: DEEPSPEED\n        - mixed_precision: bf16\n        - use_cpu: False\n        - debug: False\n        - num_processes: 2\n        - machine_rank: 0\n        - num_machines: 1\n        - rdzv_backend: static\n        - same_network: True\n        - main_training_function: main\n        - enable_cpu_affinity: False\n        - deepspeed_config: {'gradient_accumulation_steps': 4, 'offload_optimizer_device': 'cpu', 'offload_param_device': 'cpu', 'zero3_init_flag': False, 'zero3_save_16bit_model': True, 'zero_stage': 3}\n        - downcast_bf16: no\n        - tpu_use_cluster: False\n        - tpu_use_sudo: False\n        - tpu_env: []\n- DeepSpeed version: 0.15.1\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: No\n- Using GPU in script?: Yes\n- GPU type: NVIDIA L40S\n```\n\n### Who can help?\n@zucchini-nlp @amyeroberts @qubvel \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nI want to generate multiple responses to the same prompt with an image-text-to-text model. One straightforward way to do this is to use the `generate` function with `num_return_sequences` > 1 in the `GenerationConfig`. However, there appears to be an issue with this. I will use the example of the `google/gemma-3-12b-it` model to present the issue but anecdotally observed this with other models (`mistral-community/pixtral-12b`, `mistralai/Mistral-Small-3.1-24B-Base-2503`, etc.) but not sure which ones, or to what extend the specific model influences this issue.\n\nWhen using generate with `num_return_sequences` > 1, the inputs are first expanded and then passed to the sample function.\nhttps://github.com/huggingface/transformers/blob/86777b5e2f651d7f7c46db919beb13893743a5b5/src/transformers/generation/utils.py#L2486-L2492\n\nI suspect that the expansion for image inputs when there are multiple images present does not work as expected leading to this error. More details in reproduction/expected behavior.\n\nHere is a code snippet that reproduces the behavior in my setting:\n\n```python\nfrom transformers import AutoModelForImageTextToText, AutoProcessor\n\ngemma_processor = AutoProcessor.from_pretrained(\n    \"google/gemma-3-12b-it\", trust_remote_code=True\n)\ngemma_model = AutoModelForImageTextToText.from_pretrained(\n    \"google/gemma-3-12b-it\",\n    trust_remote_code=True,\n    attn_implementation=\"flash_attention_2\",\n    device_map=\"cuda:1\",\n    torch_dtype=\"bfloat16\",\n).eval()\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Generate a message referring to one of the images.\",\n            }\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"I will show  4 images labelled as A, B, C, D. I will then mention an image. Describe the image corresponding to the label. Your response should only contain the message. Your message does not need to be a full sentence. Your message should be a fluent description.\",\n            },\n            {\"type\": \"text\", \"text\": \"Round 1, \"},\n            {\"type\": \"text\", \"text\": \"\\nImage A: \"},\n            {\n                \"type\": \"image\",\n                \"url\": \"http://images.cocodataset.org/val2014/COCO_val2014_000000166401.jpg\",\n            },\n            {\"type\": \"text\", \"text\": \"\\nImage B: \"},\n            {\n                \"type\": \"image\",\n                \"url\": \"http://images.cocodataset.org/val2014/COCO_val2014_000000140076.jpg\",\n            },\n            {\"type\": \"text\", \"text\": \"\\nImage C: \"},\n            {\n                \"type\": \"image\",\n                \"url\": \"http://images.cocodataset.org/val2014/COCO_val2014_000000290477.jpg\",\n            },\n            {\"type\": \"text\", \"text\": \"\\nImage D: \"},\n            {\n                \"type\": \"image\",\n                \"url\": \"http://images.cocodataset.org/val2014/COCO_val2014_000000213224.jpg\",\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"Describe Image B. Generate only a message containing a description.\",\n            },\n        ],\n    },\n]\n\ninputs = gemma_processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n)\n\noutput_tokens = gemma_model.generate(\n    **inputs.to(gemma_model.device, gemma_model.dtype),\n    do_sample=True,\n    max_new_tokens=128,\n    temperature=1.0,\n    top_p=1.0,\n    num_return_sequences=8,\n    tokenizer=gemma_processor.tokenizer,\n)\noutputs = gemma_processor.batch_decode(\n    output_tokens[:, inputs.input_ids.shape[1]:], skip_special_tokens=True\n)\n```\n\nThis yields the following list for `outputs`:\n```\n['A luxurious bathroom space with dark cabinetry, a large countertop sink and mirror, a tiled accent wall, and a soaking tub in the corner.',\n 'A luxury bathroom with dark cabinetry, a stone countertop, and a large mirror illuminated by a modern light fixture; a potted plant and candles add decorative touches alongside a glimpse of a bathtub and a view out a window.',\n 'A vibrant patchwork quilt hangs above a dark wood dining table set with red placemats, adorned with a vase of yellow tulips and a figurine.',\n 'A dining room scene with a patchwork wall hanging, dark leather chairs, a wooden table set with red placemats, and vibrant tulips in a glass vase.',\n 'A lush, green foliage arrangement bursts from a metallic vase, resting on a vibrant purple cloth atop a wooden altar; flanked by tall candlesticks.',\n 'A vibrant, leafy arrangement sits in a decorative bronze vase, centered on a purple runner, flanked by tall candlesticks in a church setting.',\n 'A vibrant arrangement of lilies, carnations, and other blossoms overflowing from a clear glass vase, complemented by smaller vases of red flowers on a wooden table.',\n 'A vibrant flower arrangement in a clear glass vase, complemented by smaller vases with red blooms, all sitting on a wooden table.']\n```\n\nNote how the first two captions are for the first image, the second two for the second image, and so on. This should not be the case, the model is capable of describing the correct image. A description of how this can be determined is under expected behavior.\n\n### Expected behavior\n\nIf, instead of asking for 8 completions for 1 prompt, I ask for 1 completion each of 8 copies of the prompt, this issue is fixed. \n\n```python\ninputs = gemma_processor.apply_chat_template(\n    [messages for _ in range(8)],\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n)\n\noutput_tokens = gemma_model.generate(\n    **inputs.to(gemma_model.device, gemma_model.dtype),\n    do_sample=True,\n    max_new_tokens=128,\n    temperature=1.0,\n    top_p=1.0,\n    num_return_sequences=1,\n    tokenizer=gemma_processor.tokenizer,\n)\noutputs = gemma_processor.batch_decode(\n    output_tokens[:, inputs.input_ids.shape[1]:], skip_special_tokens=True\n)\n```\nyields the outputs\n```\n['A vibrant patchwork quilt adorns a stark white wall, centered above a dark wood dining table set with cheerful tulips.',\n 'A vibrant, patchwork textile hangs on a crisp white wall, complemented by a dark wooden chair and a table set with red placemats and a vase of tulips.',\n 'A vibrant, patchwork quilt dominates a white wall, framed by a dark wooden chair and table with red accents and a tulip arrangement.',\n 'A vibrant, intricately patched textile hangs on a white wall, complemented by a wooden chair and a dining table set with tulips and place settings.',\n 'A vibrant patchwork quilt dominates the wall above a dark wooden table set with black chairs and a vase of tulips.',\n 'A vibrant patchwork quilt adorns a white wall, centered above a dark wood dining table set with black chairs and a bouquet of tulips.',\n 'A vibrant patchwork textile hangs on a white wall, complemented by a dark wood dining table set with red placemats and a vase of tulips.',\n 'A vibrant patchwork wall hanging dominates, framed by a simple white wall, accented by a dark wooden chair and a table set with tulips.']\n```\nwhich is expected behavior.\n\nThis suggests a bug in how the inputs are expanded for generation.",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "@saujasv thanks for reporting! We've had similar issues with dataset iterations when training the model, because in multi-image case the first dimension doesn't reflect the batch size. I believe it is the same issue in `generate()`, didn't know it was affecting `num_return_sequences`. I will investigate and see how it can be fixed\n\ncc @yonigozlan one more case to take into account when refactoring IDEFICS image processors :)"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38258,
    "title": "AutoConfig has potential issue with composite config.",
    "author": "Tavish9",
    "state": "open",
    "created_at": "2025-05-21T10:09:31Z",
    "updated_at": "2025-06-08T07:30:00Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.0.dev0\n- Platform: Linux-5.10.134-008.12.kangaroo.al8.x86_64-x86_64-with-glibc2.35\n- Python version: 3.10.17\n- Huggingface_hub version: 0.31.1\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: 0.16.7\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA A100-SXM4-80GB\n\n\n### Who can help?\n\n@zucchini-nlp \n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nFor model which has composite config like `qwen2_5_vl`, `llava_onevision`\nhttps://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py#L293-L294\n\nThe following code would return `kwargs={'use_cache': True}`\n```python\nconfig, kwargs = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", return_unused_kwargs=True, use_cache=True)\n```\n\nSo, if we use `AutoModel` or `AutoModelForCausalLM`, etc to load these models, we would get\n```bash\nTypeError: Qwen2_5OmniThinkerForConditionalGeneration.__init__() got an unexpected keyword argument 'use_cache'\n```\n\nThis is due to the following code in \nhttps://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src/transformers/models/auto/auto_factory.py#L521-L538\n\nhttps://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src/transformers/models/auto/auto_factory.py#L568-L573\n\nthe kwargs are passed to `PretrainedModel.from_pretrained()`, and then passed to `__init__()`, which cause the error.\n\n### Expected behavior\n\npass `kwargs` to `text_config` if the model has composite config",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "Thanks for reporting this! Yes, it is a known issues and currently we expect users to overwrite text/vision config values by passing the config dict. It is quite annoying, and I have it in my todo list. Will work on that a bit later\n\n`config, kwargs = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", return_unused_kwargs=True, text_config=my_updated_text_config)`"
      },
      {
        "user": "Tavish9",
        "body": "okay, should i close this issue or just keep it linked with your pr?"
      },
      {
        "user": "zucchini-nlp",
        "body": "There is not PR yet, we can keep the issue open so it doesn't get lost :)"
      }
    ]
  },
  {
    "issue_number": 38040,
    "title": "Modernbert 3D attention mask",
    "author": "meetdoshi-iitb",
    "state": "open",
    "created_at": "2025-05-09T11:21:56Z",
    "updated_at": "2025-06-08T06:51:18Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nSupport request for passing a custom 3D attention mask to a modernbert model.\nCurrently it only supports 2D attention mask of bs, seq_len [modeling_modernbert.py#L859](https://github.com/huggingface/transformers/blob/774dc274ac966f4bccbcd90d55bba23f6cca37ae/src/transformers/models/modernbert/modeling_modernbert.py#L859)\nUnlike bert which supports 3D attention mask as well [modeling_bert.py#L988](https://github.com/huggingface/transformers/blob/774dc274ac966f4bccbcd90d55bba23f6cca37ae/src/transformers/models/bert/modeling_bert.py#L988)\n\n### Motivation\n\nLinked issues: [transformers/issues/27640](https://github.com/huggingface/transformers/issues/27640)\n\n### Your contribution\n\nNA",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @arthurzucker for text models / attention"
      },
      {
        "user": "bvantuan",
        "body": "Hi @meetdoshi-iitb @Rocketknight1 @ArthurZucker ! I'm interested in working on this feature and have opened a PR for your review."
      }
    ]
  },
  {
    "issue_number": 38662,
    "title": "Whisper models appear to be broken with Flash Attention 2",
    "author": "Anjum48",
    "state": "open",
    "created_at": "2025-06-07T20:52:22Z",
    "updated_at": "2025-06-07T21:13:52Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.4\n- Platform: Linux-6.8.0-60-generic-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- Huggingface_hub version: 0.32.4\n- Safetensors version: 0.4.5\n- Accelerate version: 1.7.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.7.1+cu128 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA GeForce RTX 5090\n\nflash-attn: Built from main\n\n### Who can help?\n\n@eustlb \n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nCode to reproduce:\n```python\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"distil-whisper/distil-large-v3.5\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id,\n    torch_dtype=torch_dtype,\n    low_cpu_mem_usage=True,\n    use_safetensors=True,\n    attn_implementation=\"flash_attention_2\",\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    max_new_tokens=128,\n    torch_dtype=torch_dtype,\n    device=device,\n    return_timestamps=True\n)\n\nresult = pipe(str(audio_path))\n```\n\nThe audio file is long (~2 hours). Note that the model runs fine if flash attention is disabled. Have tested with various Whisper models and get the same error. \n\nRolling back to 4.46.1 seems to run ok, but I'm not sure flash attn is actually working. Transcription on a 5090 for some models (e.g. openai/whisper-base) is significantly slower than a 3090, but this might be due to hardware optimisations on the flash attn side...\n\nError:\n```\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[13], line 28\n     15 processor = AutoProcessor.from_pretrained(model_id)\n     17 pipe = pipeline(\n     18     \"automatic-speech-recognition\",\n     19     model=model,\n   (...)\n     25     return_timestamps=True\n     26 )\n---> 28 result = pipe(str(audio_path))\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py:295, in AutomaticSpeechRecognitionPipeline.__call__(self, inputs, **kwargs)\n    234 def __call__(\n    235     self,\n    236     inputs: Union[np.ndarray, bytes, str],\n    237     **kwargs,\n    238 ):\n    239     \"\"\"\n    240     Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\n    241     documentation for more information.\n   (...)\n    293                 `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\n    294     \"\"\"\n--> 295     return super().__call__(inputs, **kwargs)\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1423, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n   1421     return self.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n   1422 elif self.framework == \"pt\" and isinstance(self, ChunkPipeline):\n-> 1423     return next(\n   1424         iter(\n   1425             self.get_iterator(\n   1426                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n   1427             )\n   1428         )\n   1429     )\n   1430 else:\n   1431     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124, in PipelineIterator.__next__(self)\n    121     return self.loader_batch_item()\n    123 # We're out of items within a batch\n--> 124 item = next(self.iterator)\n    125 processed = self.infer(item, **self.params)\n    126 # We now have a batch of \"inferred things\".\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:269, in PipelinePackIterator.__next__(self)\n    266             return accumulator\n    268 while not is_last:\n--> 269     processed = self.infer(next(self.iterator), **self.params)\n    270     if self.loader_batch_size is not None:\n    271         if isinstance(processed, torch.Tensor):\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1338, in Pipeline.forward(self, model_inputs, **forward_params)\n   1336     with inference_context():\n   1337         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)\n-> 1338         model_outputs = self._forward(model_inputs, **forward_params)\n   1339         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(\"cpu\"))\n   1340 else:\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py:527, in AutomaticSpeechRecognitionPipeline._forward(self, model_inputs, return_timestamps, **generate_kwargs)\n    524 if \"generation_config\" not in generate_kwargs:\n    525     generate_kwargs[\"generation_config\"] = self.generation_config\n--> 527 tokens = self.model.generate(\n    528     inputs=inputs,\n    529     attention_mask=attention_mask,\n    530     **generate_kwargs,\n    531 )\n    532 # whisper longform generation stores timestamps in \"segments\"\n    533 if return_timestamps == \"word\" and self.type == \"seq2seq_whisper\":\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:774, in WhisperGenerationMixin.generate(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\n    765             proc.set_begin_index(decoder_input_ids.shape[-1])\n    767 # 6.6 Run generate with fallback\n    768 (\n    769     seek_sequences,\n    770     seek_outputs,\n    771     should_skip,\n    772     do_condition_on_prev_tokens,\n    773     model_output_type,\n--> 774 ) = self.generate_with_fallback(\n    775     segment_input=segment_input,\n    776     decoder_input_ids=decoder_input_ids,\n    777     cur_bsz=cur_bsz,\n    778     batch_idx_map=batch_idx_map,\n    779     seek=seek,\n    780     num_segment_frames=num_segment_frames,\n    781     max_frames=max_frames,\n    782     temperatures=temperatures,\n    783     generation_config=generation_config,\n    784     logits_processor=logits_processor,\n    785     stopping_criteria=stopping_criteria,\n    786     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n    787     synced_gpus=synced_gpus,\n    788     return_token_timestamps=return_token_timestamps,\n    789     do_condition_on_prev_tokens=do_condition_on_prev_tokens,\n    790     is_shortform=is_shortform,\n    791     batch_size=batch_size,\n    792     attention_mask=attention_mask,\n    793     kwargs=kwargs,\n    794 )\n    796 # 6.7 In every generated sequence, split by timestamp tokens and extract segments\n    797 for i, seek_sequence in enumerate(seek_sequences):\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:950, in WhisperGenerationMixin.generate_with_fallback(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\n    945     if generate_kwargs.get(\"encoder_outputs\") is not None:\n    946         generate_kwargs[\"encoder_outputs\"] = F.pad(\n    947             generate_kwargs[\"encoder_outputs\"], (0, 0, 0, 0, 0, batch_size - cur_bsz), value=0\n    948         )\n--> 950 seek_outputs = super().generate(\n    951     segment_input,\n    952     generation_config=generation_config,\n    953     logits_processor=logits_processor,\n    954     stopping_criteria=stopping_criteria,\n    955     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n    956     synced_gpus=synced_gpus,\n    957     decoder_input_ids=decoder_input_ids,\n    958     attention_mask=attention_mask,\n    959     **generate_kwargs,\n    960 )\n    962 model_output_type = type(seek_outputs)\n    964 # post-process sequence tokens and outputs to be in list form\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)\n    113 @functools.wraps(func)\n    114 def decorate_context(*args, **kwargs):\n    115     with ctx_factory():\n--> 116         return func(*args, **kwargs)\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2616, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\n   2609     input_ids, model_kwargs = self._expand_inputs_for_generation(\n   2610         input_ids=input_ids,\n   2611         expand_size=generation_config.num_beams,\n   2612         is_encoder_decoder=self.config.is_encoder_decoder,\n   2613         **model_kwargs,\n   2614     )\n   2615     # 12. run beam sample\n-> 2616     result = self._beam_search(\n   2617         input_ids,\n   2618         logits_processor=prepared_logits_processor,\n   2619         stopping_criteria=prepared_stopping_criteria,\n   2620         generation_config=generation_config,\n   2621         synced_gpus=synced_gpus,\n   2622         **model_kwargs,\n   2623     )\n   2625 elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n   2626     # 11. prepare beam search scorer\n   2627     beam_scorer = BeamSearchScorer(\n   2628         batch_size=batch_size,\n   2629         num_beams=generation_config.num_beams,\n   (...)\n   2635         max_length=generation_config.max_length,\n   2636     )\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:4030, in GenerationMixin._beam_search(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\n   4027 model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n   4028 model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n-> 4030 model_outputs = self(**model_inputs, return_dict=True)\n   4032 # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n   4033 model_kwargs = self._update_model_kwargs_for_generation(\n   4034     model_outputs,\n   4035     model_kwargs,\n   4036     is_encoder_decoder=self.config.is_encoder_decoder,\n   4037 )\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-> 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:1694, in WhisperForConditionalGeneration.forward(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\n   1689     if decoder_input_ids is None and decoder_inputs_embeds is None:\n   1690         decoder_input_ids = shift_tokens_right(\n   1691             labels, self.config.pad_token_id, self.config.decoder_start_token_id\n   1692         )\n-> 1694 outputs = self.model(\n   1695     input_features,\n   1696     attention_mask=attention_mask,\n   1697     decoder_input_ids=decoder_input_ids,\n   1698     encoder_outputs=encoder_outputs,\n   1699     decoder_attention_mask=decoder_attention_mask,\n   1700     head_mask=head_mask,\n   1701     decoder_head_mask=decoder_head_mask,\n   1702     cross_attn_head_mask=cross_attn_head_mask,\n   1703     past_key_values=past_key_values,\n   1704     decoder_inputs_embeds=decoder_inputs_embeds,\n   1705     decoder_position_ids=decoder_position_ids,\n   1706     use_cache=use_cache,\n   1707     output_attentions=output_attentions,\n   1708     output_hidden_states=output_hidden_states,\n   1709     return_dict=return_dict,\n   1710     cache_position=cache_position,\n   1711 )\n   1712 lm_logits = self.proj_out(outputs[0])\n   1714 loss = None\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-> 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:1529, in WhisperModel.forward(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\n   1522     encoder_outputs = BaseModelOutput(\n   1523         last_hidden_state=encoder_outputs[0],\n   1524         hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n   1525         attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n   1526     )\n   1528 # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n-> 1529 decoder_outputs = self.decoder(\n   1530     input_ids=decoder_input_ids,\n   1531     attention_mask=decoder_attention_mask,\n   1532     encoder_hidden_states=encoder_outputs[0],\n   1533     head_mask=decoder_head_mask,\n   1534     cross_attn_head_mask=cross_attn_head_mask,\n   1535     past_key_values=past_key_values,\n   1536     inputs_embeds=decoder_inputs_embeds,\n   1537     position_ids=decoder_position_ids,\n   1538     use_cache=use_cache,\n   1539     output_attentions=output_attentions,\n   1540     output_hidden_states=output_hidden_states,\n   1541     return_dict=return_dict,\n   1542     cache_position=cache_position,\n   1543 )\n   1545 if not return_dict:\n   1546     return decoder_outputs + encoder_outputs\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-> 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:1188, in WhisperDecoder.forward(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\n   1174     layer_outputs = self._gradient_checkpointing_func(\n   1175         decoder_layer.__call__,\n   1176         hidden_states,\n   (...)\n   1185         cache_position,\n   1186     )\n   1187 else:\n-> 1188     layer_outputs = decoder_layer(\n   1189         hidden_states,\n   1190         attention_mask=causal_mask,\n   1191         encoder_hidden_states=encoder_hidden_states,\n   1192         layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n   1193         cross_attn_layer_head_mask=(\n   1194             cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n   1195         ),\n   1196         past_key_value=past_key_values if use_cache else None,\n   1197         output_attentions=output_attentions,\n   1198         use_cache=use_cache,\n   1199         cache_position=cache_position,\n   1200     )\n   1201 hidden_states = layer_outputs[0]\n   1203 if output_attentions:\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-> 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:727, in WhisperDecoderLayer.forward(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\n    725 residual = hidden_states\n    726 hidden_states = self.encoder_attn_layer_norm(hidden_states)\n--> 727 hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n    728     hidden_states=hidden_states,\n    729     key_value_states=encoder_hidden_states,\n    730     attention_mask=encoder_attention_mask,\n    731     layer_head_mask=cross_attn_layer_head_mask,\n    732     past_key_value=past_key_value,\n    733     output_attentions=output_attentions,\n    734 )\n    735 hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n    736 hidden_states = residual + hidden_states\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-> 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nFile ~/github/tapesearch/.venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:401, in WhisperFlashAttention2.forward(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position)\n    399     value_states = past_key_value.value_cache[self.layer_idx]\n    400 else:\n--> 401     key_states = self.k_proj(current_states).view(bsz, tgt_len, self.num_heads, self.head_dim)\n    402     value_states = self.v_proj(current_states).view(bsz, tgt_len, self.num_heads, self.head_dim)\n    403     key_states = key_states.transpose(1, 2).contiguous()\n\nRuntimeError: shape '[5, 3, 20, 64]' is invalid for input of size 9600000\n```\n\n\n### Expected behavior\n\nExpect the pipeline to run in the same way as when flash attention is disabled (but faster!)",
    "comments": []
  },
  {
    "issue_number": 36978,
    "title": "[Contributions Welcome] Add Fast Image Processors",
    "author": "yonigozlan",
    "state": "open",
    "created_at": "2025-03-25T20:03:27Z",
    "updated_at": "2025-06-07T20:53:43Z",
    "labels": [
      "Good First Issue",
      "Good Second Issue",
      "Vision",
      "contributions-welcome",
      "Processing"
    ],
    "body": "## Community contributions: Add Fast Image Processors\n\nFast image processors have been rolling out progressively for a while. Now that the [BaseImageProcessorFast](https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils_fast.py#L292), from which all fast image processors inherit, is in a more stable state, I'm opening this issue to encourage contributors to add fast image processors for models that still only have a \"slow\" image processor.\n\n### How to implement a Fast Image Processor\n\nThe core principle of fast image processors is to use `torch` and `torchvision` functions for image transformations instead of `PIL` or `numpy`. Among other performance benefits, this enables processing images on GPU, significantly improving inference speed.\n\nAnother key difference compared to slow image processors is that, unlike `BaseImageProcessor`, which provides only a minimal skeleton, `BaseImageProcessorFast` includes all the fundamental functionalities needed for a basic image processor. This allows optimizations made in BaseImageProcessorFast to propagate to its inherited classes. Additionally, most repetitive logic for image loading and argument handling is managed within BaseImageProcessorFast. Except in rare cases, inherited classes do not need to handle image loading, conversion, or retrieving arguments from class attributes in the call/preprocess function, this is all handled in `BaseImageProcessorFast`.\n\n#### Getting Started\n\nRun the following command:\n```bash\ntransformers-cli add-fast-image-processor --model-name model_name\n```\nwhere `model_name` is the name of the model (as found in its folder under `transformers/src/transformers/models`) for which you're adding the fast image processor.\n\nThis command will handle all necessary imports and generate a basic fast image processor, which will look similar to this example for Beit:\n\n```python\n# coding=utf-8\n# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fast Image processor class for Beit.\"\"\"\n\nfrom ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast\nfrom ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\nfrom ...utils import add_start_docstrings\n\n\n@add_start_docstrings(\n    \"Constructs a fast Beit image processor.\",\n    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n)\nclass BeitImageProcessorFast(BaseImageProcessorFast):\n    # This generated class can be used as a starting point for the fast image processor.\n    # if the image processor is only used for simple augmentations, such as resizing, center cropping, rescaling, or normalizing,\n    # only the default values should be set in the class.\n    # If the image processor requires more complex augmentations, methods from BaseImageProcessorFast can be overridden.\n    # In most cases, only the `_preprocess` method should be overridden.\n\n    # For an example of a fast image processor requiring more complex augmentations, see `LlavaNextImageProcessorFast`.\n\n    # Default values should be checked against the slow image processor\n    # None values left after checking can be removed\n    resample = PILImageResampling.BICUBIC\n    image_mean = IMAGENET_STANDARD_MEAN\n    image_std = IMAGENET_STANDARD_STD\n    size = {\"height\": 256, \"width\": 256}\n    default_to_square = None\n    crop_size = {\"height\": 224, \"width\": 224}\n    do_resize = True\n    do_center_crop = True\n    do_rescale = True\n    do_normalize = True\n    do_convert_rgb = None\n\n\n__all__ = [\"BeitImageProcessorFast\"]\n```\n\nAs explained in the generated file, if the image processor only performs basic augmentations such as resizing, center cropping, rescaling, and normalizing, the generated file might be sufficient for a working fast image processor. The class attributes, such as `resample` and `image_mean`, are automatically parsed from the slow image processor when running the script above. However, you should verify their correctness and check for any missing or incorrectly assigned values.\n\n### Customizing the Image Processor\n\nIf the image processor requires additional functionalities beyond the basic augmentations, you will need to override the `_preprocess` function in `BaseImageProcessorFast`. Check the `_preprocess` implementation in `BaseImageProcessorFast` for reference. Notably, it leverages `group_images_by_shape` and `reorder_images` to enable batch processing, significantly increasing processing speed, particularly on GPUs. If you create new image processing functions, ensure they support batch processing by utilizing `group_images_by_shape` and `reorder_images` where possible.\n\nIf your image processor requires additional kwargs not present in [`DefaultFastImageProcessorKwargs`](https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils_fast.py#L172), you must create a `ModelNameFastImageProcessorKwargs` class that inherits from `DefaultFastImageProcessorKwargs` and defines the new kwargs. Additionally, you should document the added kwargs in the class and the `preprocess` function using `add_start_docstrings`. (This documentation process may be simplified soon, but is necessary for now to get a correct documentation).\n\nFor an example of handling custom kwargs and documentation, refer to [LlavaNextImageProcessorFast](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llava_next/image_processing_llava_next_fast.py).\n\n### Important Notes\n\n- In nearly all cases, `_preprocess` is the only function in `BaseImageProcessorFast` that needs to be overridden.\n- The `_preprocess` function does not require default values for its arguments, as they are automatically derived from class attributes if not explicitly provided.\n- Even if `PIL` images or `numpy` arrays are passed to the image processor, the `images` argument in `_preprocess` will always be a list of tensors, with the channel dimension first.\n\n### Handling Edge Cases\n\n- **Nested Images:** If images are provided as nested lists (e.g., `[[image1, image2], [image3]]`), they will be flattened to `[image1, image2, image3]` by default before being passed to `_preprocess`. This behavior can be modified by overriding `_prepare_images_structure`, though flattening is generally recommended.\n- **Formatting Custom  Kwargs:** If any custom kwargs require formatting before `_preprocess`, override `_further_process_kwargs`.\n- **Validating Custom Kwargs:** If additional validation is needed for custom kwargs or existing ones, override `_validate_preprocess_kwargs`.\n\n### Testing\n\nIn the case where the model already has a `test_image_processing_model_name.py` file under `transformers/tests/models/model_name`, the script ran before should have imported the fast image processor to the file, and added it as a `fast_image_processing_class` class attribute to the `ModelNameImageProcessingTest` class.\nHowever this is not enough to get all the tests to run on the fast image processor. For all the test functions under `ModelNameImageProcessingTest`, you need to replace `image_processing = self.image_processing_class(**self.image_processor_dict)` with a loop over `self.image_processor_list`.\n\nFor example, the `test_image_processor_properties` test in `test_image_processing_beit.py` which looks like this:\n\n```python\n    def test_image_processor_properties(self):\n        image_processing = self.image_processing_class(**self.image_processor_dict)\n        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n        self.assertTrue(hasattr(image_processing, \"size\"))\n        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n        self.assertTrue(hasattr(image_processing, \"image_std\"))\n        self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n```\nshould be changed to this:\n\n```python\n    def test_image_processor_properties(self):\n        for image_processing_class in self.image_processor_list:\n            image_processing = image_processing_class(**self.image_processor_dict)\n            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n            self.assertTrue(hasattr(image_processing, \"size\"))\n            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n            self.assertTrue(hasattr(image_processing, \"image_std\"))\n            self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n```\n\nIn the case where no image processing test file is present, now is a great time to add one! You can have a look at the[ CLIP image processing test file](https://github.com/huggingface/transformers/blob/ad5d40de9c4d4899d5b79243f63e22c72e8b3669/tests/models/clip/test_image_processing_clip.py) to use as a simple starting point.\n\nDon't hesitate to add model-specific tests if you feel like there are some non-standard image processing techniques in the processor :).\n\nTo run the tests, use this command:\n```bash\nRUN_SLOW=1 python -m pytest tests/models/model_name/test_image_processing_model_name.py\n```\n\n\n### Choosing an Image Processor to Implement\n\nThe difficulty of implementing a fast image processor varies by model. If this is your first issue, consider starting with an easier one!\n\nHappy coding!\n\nHere is the list of fast image processors left to implement:\n- [x] BEiT -> https://github.com/huggingface/transformers/pull/37005\n- [x] BiT -> https://github.com/huggingface/transformers/pull/37180\n- [x] Blip\n- [x] BridgeTower -> https://github.com/huggingface/transformers/pull/37373\n- [ ] Chameleon -> https://github.com/huggingface/transformers/pull/37140\n- [x] Chinese-CLIP -> https://github.com/huggingface/transformers/pull/37012\n- [x] CLIP\n- [x] Conditional-DETR -> https://github.com/huggingface/transformers/pull/37071\n- [X] ConvNext\n- [X] Deformable-DETR \n- [x] Deit\n- [x] DepthPro\n- [x] ~~Deta~~ (deprecated)\n- [x] DETR\n- [x] Donut -> https://github.com/huggingface/transformers/pull/37081\n- [ ] DPT -> https://github.com/huggingface/transformers/pull/37481\n- [x] ~~EfficientFormer~~ (deprecated)\n- [x] EfficientNet -> https://github.com/huggingface/transformers/pull/37055\n- [x] Flava -> https://github.com/huggingface/transformers/pull/37135\n- [ ] Fuyu -> https://github.com/huggingface/transformers/pull/37410\n- [x] Gemma3\n- [ ] GLPN -> https://github.com/huggingface/transformers/pull/38461\n- [x] GotOcr2\n- [x] Grounding Dino -> https://github.com/huggingface/transformers/pull/37108\n- [ ] Idefics 2 -> https://github.com/huggingface/transformers/pull/37168\n- [ ] Idefics3 -> https://github.com/huggingface/transformers/pull/37045\n- [ ] ImageGPT -> https://github.com/huggingface/transformers/pull/37320\n- [x] LayoutLMv2 -> https://github.com/huggingface/transformers/pull/37203\n- [x] LayoutLMv3 -> https://github.com/huggingface/transformers/pull/37201\n- [x] LeViT -> https://github.com/huggingface/transformers/pull/37154\n- [x] LLava\n- [x] LLaVa-NeXT\n- [ ] LLaVa-NeXT-Video -> https://github.com/huggingface/transformers/pull/37297\n- [x] LLaVa-Onevision\n- [ ] Mask2Former -> https://github.com/huggingface/transformers/pull/35685\n- [ ] MaskFormer -> https://github.com/huggingface/transformers/pull/35685\n- [ ] MLlama -> https://github.com/huggingface/transformers/pull/37539\n- [x] MobileNetV1 -> https://github.com/huggingface/transformers/pull/37111\n- [x] MobileNetV2 -> https://github.com/huggingface/transformers/pull/37113\n- [ ] MobileViT -> https://github.com/huggingface/transformers/pull/37143\n- [ ] Nougat -> https://github.com/huggingface/transformers/pull/37661\n- [ ] OneFormer -> https://github.com/huggingface/transformers/pull/38343\n- [ ] OWLv2 -> https://github.com/huggingface/transformers/pull/37289\n- [x] OwlViT -> https://github.com/huggingface/transformers/pull/37164\n- [x] Perceiver -> https://github.com/huggingface/transformers/pull/37176\n- [ ] Pix2Struct -> https://github.com/huggingface/transformers/pull/37210\n- [x] Pixtral\n- [x] PoolFormer -> https://github.com/huggingface/transformers/pull/37182\n- [x] Pvt -> https://github.com/huggingface/transformers/pull/37204\n- [x] Qwen2-VL (Not standard as it also handles videos, don't use it as an example :) )\n- [x] RT-DETR\n- [ ] SAM -> https://github.com/huggingface/transformers/pull/36999\n- [ ] Segformer -> https://github.com/huggingface/transformers/pull/37024\n- [x] SigLIP\n- [x] SigLIP2\n- [ ] SuperPoint -> https://github.com/huggingface/transformers/pull/37804\n- [x] Swin2SR  -> https://github.com/huggingface/transformers/pull/37169\n- [x] ~~TVLT~~ (deprecated)\n- [ ] TVP\n- [ ] Video-LLaVA -> https://github.com/huggingface/transformers/pull/37023\n- [ ] VideoMAE -> https://github.com/huggingface/transformers/pull/37191\n- [ ] Vilt -> https://github.com/huggingface/transformers/pull/37304\n- [x] ViT\n- [x] ~~ViT hybrid~~ (deprecated)\n- [x] ViTMatte -> https://github.com/huggingface/transformers/pull/37616\n- [ ] VitPose -> https://github.com/huggingface/transformers/pull/38502\n- [ ] Vivit\n- [x] YOLOS -> https://github.com/huggingface/transformers/pull/37292\n- [x] ZoeDepth -> https://github.com/huggingface/transformers/pull/38515",
    "comments": [
      {
        "user": "MinJu-Ha",
        "body": "Hey! I'd like to work on this issue with **MobileViT** 😊"
      },
      {
        "user": "edgarriba",
        "body": "@yonigozlan have you considered adopting [`kornia`](https://github.com/kornia/kornia) for that ? we have been curating algorithms (+700 ops) already for several years in terms of image processing and low level vision using exclusively pytorch."
      },
      {
        "user": "Knight7561",
        "body": "I would love to pick one and start contributing. Good task for this week..!"
      }
    ]
  },
  {
    "issue_number": 26826,
    "title": "Pipelines should yield results incrementally even when their input is a list",
    "author": "uyhcire",
    "state": "closed",
    "created_at": "2023-10-16T04:06:23Z",
    "updated_at": "2025-06-07T19:20:12Z",
    "labels": [],
    "body": "### System Info\r\n\r\n- `transformers` version: 4.34.0\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.3\r\n- Safetensors version: 0.4.0\r\n- Accelerate version: not installed\r\n- Accelerate config: not found\r\n- PyTorch version (GPU?): 2.0.1+cu118 (False)\r\n- Tensorflow version (GPU?): 2.13.0 (False)\r\n- Flax version (CPU?/GPU?/TPU?): 0.7.4 (cpu)\r\n- Jax version: 0.4.16\r\n- JaxLib version: 0.4.16\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\r\n\r\n### Who can help?\r\n\r\n@nar\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n```\r\nfrom transformers import pipeline \r\n\r\npipe = pipeline(\"text-classification\")\r\n\r\ndef input_generator():\r\n    for _ in range(100000):\r\n        yield \"This restaurant is awesome\"\r\n\r\n# Finishes quickly\r\nfor result in pipe(input_generator()):\r\n    print(result)\r\n    break\r\n\r\n# Takes basically forever\r\nfor result in pipe([\"This restaurant is awesome\"] * 100000):\r\n    print(result)\r\n    break\r\n```\r\n\r\n### Expected behavior\r\n\r\nPipelines should yield results as soon as they are available, regardless of whether the inputs to be processed come in the form of a list or a generator.",
    "comments": [
      {
        "user": "ArthurZucker",
        "body": "Makes sense to me, however this changes the default behaviour, and does not take into account batching, which is what's recommended if you are running with a lot of inputs. \r\nAlso depending on the pipeline this might already be implemented ! \r\nI am not sure how @Narsil feels about it ? (he is off for now but this can wait, in the meantime feel free to open a PR to see what kind of changes are required for this!)"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "Gistix",
        "body": "This works for image-text-to-text pipeline even with batching so I don't see why it shouldn't work for text-generation or other pipelines."
      }
    ]
  },
  {
    "issue_number": 37736,
    "title": "ValueError: GGUF model with architecture deci is not supported yet.",
    "author": "iEddie-cmd",
    "state": "open",
    "created_at": "2025-04-24T08:29:20Z",
    "updated_at": "2025-06-06T22:18:04Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nValueError: GGUF model with architecture deci is not supported yet.\n\n### Motivation\n\nValueError: GGUF model with architecture deci is not supported yet. (Nvidia Nemotron 2025)\n\n### Your contribution\n\nI could help in maybe programming ",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @sunmarc @mekkcyber for gguf?"
      },
      {
        "user": "MekkCyber",
        "body": "Hi @iEddie-cmd, do you want to open a pr to add it ? "
      },
      {
        "user": "iEddie-cmd",
        "body": "@MekkCyber would love to, but no idea how :("
      }
    ]
  },
  {
    "issue_number": 38613,
    "title": "MDX Errors",
    "author": "rileyafox",
    "state": "closed",
    "created_at": "2025-06-05T14:19:45Z",
    "updated_at": "2025-06-06T20:12:36Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nUbuntu 24.04.2 LTS, CPython 3.11.12,  transformers==4.53.0.dev0\n\n\n@stevhliu  I'm trying to contribute to the model cards. I forked the latest transformers and I ran the scripts, from the home page and then I want to the documents page. I'm having issues with the doc builder. I keep receiving the errors \"ValueError: There was an error when converting docs/source/en/internal/generation_utils.md to the MDX format.\nUnable to find generation.TFGreedySearchEncoderDecoderOutput in transformers. Make sure the path to that object is correct.\" And Unable to find image_processing_utils_fast.BaseImageProcessorFast in transformers. Make sure the path to that object is correct.\n\nI ran the \" pip install -e \".[docs]\" and saw this after installing everything: \"warning: The package `transformers @ file://s` does not have an extra named `docs`\"\n\nI ran the doc builder and that ran as expected until I ran the doc-builder command \"doc-builder build transformers docs/source/en/ --build_dir ~/tmp/test-build\"\n\nIs there something that I'm misunderstanding? Is there a workaround for me to write the markdown of the card that I have been assigned without having to run those scripts instead, in the meantime.. Thank you!\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nRan install scripts on the Documents folder\n\n### Expected behavior\n\nTo generate the docs",
    "comments": [
      {
        "user": "stevhliu",
        "body": "Hmm, I'm not exactly sure what is causing this error!\n\nFor the model cards, you can just edit the `.md` file you're working on and then open a PR. Our `Build PR` test will build a live preview of your changes directly on the PR."
      },
      {
        "user": "rileyafox",
        "body": "Do I have to run a script for that does it auto generate the preview?Thank you so much for your response!"
      },
      {
        "user": "stevhliu",
        "body": "Nope, the test will automatically generate the preview so you don't need to run any scripts or install anything :)"
      }
    ]
  },
  {
    "issue_number": 38268,
    "title": "Group beam search with sampling?",
    "author": "adrian-valente",
    "state": "open",
    "created_at": "2025-05-21T18:08:59Z",
    "updated_at": "2025-06-06T18:11:13Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nIn the current generation code, group beam search is necessarily greedy. From a theoretical point of view, it is not very clear why that should be the case, since the diversity penalty is applied on the logits anyway, yielding a full distribution from which sampling can still be performed.\n\n### Motivation\n\nI think there is a reasonable use case for such a feature: diversity beam search is very useful in particular for modalities like biological sequences which increasingly use the transformers library, but I could see it be useful as well for natural language or code, to generate diverse paths without falling to the drawbacks of greedy generation. From a more abstract point of view it is also seemingly unjustified to allow sampling for standard beam search and not for diversity beam search.\n\n### Your contribution\n\nI am aware of the work in #30810 so don't want to disrupt but would be happy to look into it.",
    "comments": [
      {
        "user": "zucchini-nlp",
        "body": "cc @gante "
      },
      {
        "user": "gante",
        "body": "Hi @adrian-valente 👋 Thank you for opening this issue!\n\nWe've just merged support for custom generation code on the Hub (#36405), and we will move a few less popular generation techniques to code on the Hub. The idea is to improve the maintenance quality on a few select techniques, and normalize the use of advanced generation methods from the Hub.\n\nGroup beam search will be one of those techniques, and we will redirect calls to a Hub repo. If you're open to it, you can create a clone of the existing group beam search, but with sampling enabled, and we will redirect calls to your repo 🤗 Otherwise, when moving the code to the Hub, I will add this functionality.\n\nDocs: https://huggingface.co/docs/transformers/generation_strategies#custom-decoding-methods"
      },
      {
        "user": "adrian-valente",
        "body": "Amazing, thank you, will give it a try! That answers my issue, I will link my hub function here then if I publish it."
      }
    ]
  },
  {
    "issue_number": 35037,
    "title": "LlamaTokenizer being recognized as a bool",
    "author": "spgerlach",
    "state": "closed",
    "created_at": "2024-12-01T22:13:44Z",
    "updated_at": "2025-06-06T16:57:41Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nWhen initializing LlamaTokenizer from the Transformers library, the tokenizer is being recognized as a bool. This issue persists across different environments and Python versions.\r\n\r\nSteps to Reproduce:\r\nInstall the required libraries:\r\npip install transformers torch sentencepiece\r\n\r\nUse the following script to initialize the tokenizer:\r\nfrom transformers.models.llama import LlamaTokenizer\r\n\r\nmodel_path = \"C:/Users/spger/.llama/checkpoints/Llama3.1-70B\"\r\n\r\ntry:\r\n    tokenizer = LlamaTokenizer.from_pretrained(model_path, use_fast=True, legacy=False)\r\n    print(\"Tokenizer initialized successfully.\")\r\n    print(\"Tokenizer type:\", type(tokenizer))\r\nexcept Exception as e:\r\n    print(\"Error initializing tokenizer:\", e)\r\n\r\nObserved Output:\r\nThe tokenizer type is <class 'bool'> instead of the expected tokenizer class.\r\n\r\nSystem Info:\r\ntransformers version: 4.46.3\r\n\r\nPlatform: Windows-10-10.0.26100-SP0\r\n\r\nPython version: 3.11.9\r\n\r\nHuggingface_hub version: 0.26.3\r\n\r\nSafetensors version: 0.4.5\r\n\r\nAccelerate version: not installed\r\n\r\nAccelerate config: not found\r\n\r\nPyTorch version (GPU?): 2.5.1+cpu (False)\r\n\r\nTensorflow version (GPU?): not installed (NA)\r\n\r\nFlax version (CPU?/GPU?/TPU?): not installed (NA)\r\n\r\nJax version: not installed\r\n\r\nJaxLib version: not installed\r\n\r\nUsing distributed or parallel set-up in script?: No\r\n\r\nAdditional Details:\r\nOther tokenizers like AutoTokenizer for GPT-2 and BERT initialize correctly.\r\n\n\n### Who can help?\n\n@ArthurZucker @itazap\r\n\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nfrom transformers.models.llama import LlamaTokenizer\r\n\r\nmodel_path = \"C:/Users/spger/.llama/checkpoints/Llama3.1-70B\"\r\n\r\ntry:\r\n    tokenizer = LlamaTokenizer.from_pretrained(model_path, use_fast=True, legacy=False)\r\n    print(\"Tokenizer initialized successfully.\")\r\n    print(\"Tokenizer type:\", type(tokenizer))\r\nexcept Exception as e:\r\n    print(\"Error initializing tokenizer:\", e)\r\n\r\nSteps to reproduce the behavior:\r\n1. Install the required libraries:\r\n   ```bash\r\n   pip install transformers torch sentencepiece\r\n2. Run the provided script.\r\n3. Observe that the tokenizer is of type bool.\r\n\r\n\n\n### Expected behavior\n\n\r\n#### Expected behavior:\r\n```markdown\r\nI expect the `LlamaTokenizer` to be correctly initialized and recognized as a `LlamaTokenizer` object instead of a `bool`.\r\n",
    "comments": [
      {
        "user": "aka133",
        "body": "Hi, I'd love to help solve this if possible. New to fixing issues but it seems like:\r\n- Llama 3.1's tokenizer.model is a base64-encoded vocabulary file (126,784 tokens)\r\n- LlamaTokenizer expects a SentencePiece model file\r\n- The initialization silently fails and returns a bool instead of raising an error\r\n\r\nAlso, the Llama tokenizer script uses SPIECE_UNDERLINE to handle spaces, which is not applicable for LLaMA 3.1 as it doesn't use this token. I wasn't exactly sure how to fix this but wanted to point this out (should this be a separate issue?).\r\n\r\nIt seems like in order to use Llama 3.1 with the LlamaTokenizer the tokenizer.model needs to be converted from a Base64 vocabulary to a SentencePiece model. I drafted a PR that does that within the get_spm_processor method in the tokenization_llama.py file.\r\n\r\nThe changes added logic to decode each line of the vocabulary file from base64. This allows the tokenizer to handle files that are base64-encoded."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "buaali",
        "body": "The error still occurs in transformers 0.49.0, please fix it"
      }
    ]
  },
  {
    "issue_number": 35385,
    "title": "Support modernBERT for encoder-decoder models",
    "author": "Bachstelze",
    "state": "open",
    "created_at": "2024-12-21T12:53:54Z",
    "updated_at": "2025-06-06T16:34:22Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nThe docs state that the [EncoderDecoderModel](https://huggingface.co/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel) can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the encoder. Though [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base) isn't supported:\r\n```\r\nFile \"/content/syntax_transformer/data/../models/encoderDecoder.py\", line 40, in __init__\r\n    self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py\", line 538, in from_encoder_decoder_pretrained\r\n    decoder = AutoModelForCausalLM.from_pretrained(decoder_pretrained_model_name_or_path, **kwargs_decoder)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 567, in from_pretrained\r\n    raise ValueError(\r\nValueError: Unrecognized configuration class <class 'transformers.models.modernbert.configuration_modernbert.ModernBertConfig'> for this kind of AutoModel: AutoModelForCausalLM.\r\nModel type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, GitConfig, GlmConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig.\r\n```\n\n### Motivation\n\nModernBert has a better performance and a longer context length.\n\n### Your contribution\n\nHow is it possible to support monderBERT? It isn't that different from other BERT models.",
    "comments": [
      {
        "user": "NielsRogge",
        "body": "The reason ModernBERT isn't supported yet to be used as decoder is because it does not include a cross-attention module.\r\n\r\nWhen you use the `EncoderDecoderModel` class and want to initialize the weights of the decoder with those of a pre-trained encoder-only one (like ModernBERT), the modeling_xxx.py file needs to support cross-attention (and causal attention mask). This is supported in modeling_bert.py as can be seen [here](https://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src/transformers/models/bert/modeling_bert.py#L566). But for ModernBERT, explicit support for a `config.is_decoder` argument (and corresponding implementation) would need to be added."
      },
      {
        "user": "Bachstelze",
        "body": "To include a cross-attention module those other modules should be changed:\r\n```\r\nif self.add_cross_attention:\r\n   if not self.is_decoder:\r\n      raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\r\n   self.crossattention = ModernBertAttention(config)\r\n```\r\nin ModernBertEncoderLayer __init__(), better rename it to ModernBertLayer.\r\n\r\nIf ModernBertAttention is instantiated as a cross-attention module, the keys and values come from an encoder; the attention mask needs to be such that the encoder's padding tokens are not attended to.\r\n\r\nmodeling_modernbert.py is generated from modular_modernbert.py.\r\nHow can this generation be triggered?"
      },
      {
        "user": "rnbokade",
        "body": "I am trying to do TDAE Domain adaptation task on ModernBert and having same error. Any comments on this @AnswerDotAI ?"
      }
    ]
  },
  {
    "issue_number": 38588,
    "title": "Transformers fail to load deepseek-ai/DeepSeek-V3 with vllm",
    "author": "ajayvohra2005",
    "state": "open",
    "created_at": "2025-06-04T16:45:38Z",
    "updated_at": "2025-06-06T15:10:07Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n**transformers_version:** 4.51.2 \n**platform**: ubuntu 24.04\n**python_version**:  3.12\n\n**Stacktrace:**\n\n```\nFailed to initialize Python stub: TypeError: unsupported operand type(s) for /: 'PosixPath' and 'NoneType'\n\nAt:\n/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py(411): get_cached_module_file \n/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py(558): get_class_from_dynamic_module\n/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py(1123): from_pretrained/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py(315): get_config\\n  /usr/local/lib/python3.12/dist-packages/vllm/config.py(423): __init__\n/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py(1042): create_model_config  /usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py(1154): create_engine_config\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py(166): build_async_engine_client_from_engine_args\n/usr/lib/python3.12/contextlib.py(211): __aenter__\n/opt/tritonserver/backends/vllm/model.py(289): _run_llm_engine\n/usr/lib/python3.12/asyncio/events.py(103): _run\n/usr/lib/python3.12/asyncio/base_events.py(1988): _run_once\n/usr/lib/python3.12/asyncio/base_events.py(649): run_forever\n/usr/lib/python3.12/asyncio/base_events.py(687): run_until_complete\n/usr/lib/python3.12/asyncio/runners.py(126): run\n/usr/lib/python3.12/asyncio/runners.py(193): run\n/usr/lib/python3.12/threading.py(1014): run\n/usr/lib/python3.12/threading.py(1077): _bootstrap_inner\n/usr/lib/python3.12/threading.py(1030): _bootstrap\"\n```\n\n**Root cause analysis:**\n\nIn the code below, `get_cached_module_file` is always being called with `_commit_hash=None` , leading to the error, as the called function, `get_cached_module_file`, does not protect against `_commit_hash` being `None`.  Maybe set `_commit_hash=revision` in `get_cached_module_file`, when `_commit_hash` is passed as `None`.\n\n```\ndef get_class_from_dynamic_module(\n    class_reference: str,\n    pretrained_model_name_or_path: Union[str, os.PathLike],\n    cache_dir: Optional[Union[str, os.PathLike]] = None,\n    force_download: bool = False,\n    resume_download: Optional[bool] = None,\n    proxies: Optional[dict[str, str]] = None,\n    token: Optional[Union[bool, str]] = None,\n    revision: Optional[str] = None,\n    local_files_only: bool = False,\n    repo_type: Optional[str] = None,\n    code_revision: Optional[str] = None,\n    **kwargs,\n) -> type:\n    \"\"\"\n    Extracts a class from a module file, present in the local folder or repository of a model.\n\n    <Tip warning={true}>\n\n    Calling this function will execute the code in the module file found locally or downloaded from the Hub. It should\n    therefore only be called on trusted repos.\n\n    </Tip>\n\n\n\n    Args:\n        class_reference (`str`):\n            The full name of the class to load, including its module and optionally its repo.\n        pretrained_model_name_or_path (`str` or `os.PathLike`):\n            This can be either:\n\n            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n              huggingface.co.\n            - a path to a *directory* containing a configuration file saved using the\n              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n\n            This is used when `class_reference` does not specify another repo.\n        module_file (`str`):\n            The name of the module file containing the class to look for.\n        class_name (`str`):\n            The name of the class to import in the module.\n        cache_dir (`str` or `os.PathLike`, *optional*):\n            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n            cache should not be used.\n        force_download (`bool`, *optional*, defaults to `False`):\n            Whether or not to force to (re-)download the configuration files and override the cached versions if they\n            exist.\n        resume_download:\n            Deprecated and ignored. All downloads are now resumed by default when possible.\n            Will be removed in v5 of Transformers.\n        proxies (`Dict[str, str]`, *optional*):\n            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n        token (`str` or `bool`, *optional*):\n            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n            when running `huggingface-cli login` (stored in `~/.huggingface`).\n        revision (`str`, *optional*, defaults to `\"main\"`):\n            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n            identifier allowed by git.\n        local_files_only (`bool`, *optional*, defaults to `False`):\n            If `True`, will only try to load the tokenizer configuration from local files.\n        repo_type (`str`, *optional*):\n            Specify the repo type (useful when downloading from a space for instance).\n        code_revision (`str`, *optional*, defaults to `\"main\"`):\n            The specific revision to use for the code on the Hub, if the code leaves in a different repository than the\n            rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for\n            storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n\n    <Tip>\n\n    Passing `token=True` is required when you want to use a private model.\n\n    </Tip>\n\n    Returns:\n        `typing.Type`: The class, dynamically imported from the module.\n\n    Examples:\n\n    ```python\n    # Download module `modeling.py` from huggingface.co and cache then extract the class `MyBertModel` from this\n    # module.\n    cls = get_class_from_dynamic_module(\"modeling.MyBertModel\", \"sgugger/my-bert-model\")\n\n    # Download module `modeling.py` from a given repo and cache then extract the class `MyBertModel` from this\n    # module.\n    cls = get_class_from_dynamic_module(\"sgugger/my-bert-model--modeling.MyBertModel\", \"sgugger/another-bert-model\")\n    ```\"\"\"\n    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n    if use_auth_token is not None:\n        warnings.warn(\n            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n            FutureWarning,\n        )\n        if token is not None:\n            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n        token = use_auth_token\n\n    # Catch the name of the repo if it's specified in `class_reference`\n    if \"--\" in class_reference:\n        repo_id, class_reference = class_reference.split(\"--\")\n    else:\n        repo_id = pretrained_model_name_or_path\n    module_file, class_name = class_reference.split(\".\")\n\n    if code_revision is None and pretrained_model_name_or_path == repo_id:\n        code_revision = revision\n    # And lastly we get the class inside our newly created module\n    final_module = get_cached_module_file(\n        repo_id,\n        module_file + \".py\",\n        cache_dir=cache_dir,\n        force_download=force_download,\n        resume_download=resume_download,\n        proxies=proxies,\n        token=token,\n        revision=code_revision,\n        local_files_only=local_files_only,\n        repo_type=repo_type,  \n    )\n    return get_class_in_module(class_name, final_module, force_reload=force_download)\n```\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nRun `deepseek-ai/DeepSeek-V3` model with vLLM, with loading of model directly from hub.\n\n### Expected behavior\n\nNo transformers library crash.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @ajayvohra2005, can you give us a sample of the code you ran to get this? We generally don't see this error in `transformers`, so I think it might be an issue either in `vLLM` or your code"
      },
      {
        "user": "ajayvohra2005",
        "body": "I don't have any code to share. It is a straight use of vLLM. You can try to load this model with vLLM using openai server, or triton inference server. The reason you may not see this error is because code takes very different pathways depending on how the model is being loaded. \n\nHowever, as you may see in the code, there is no check against `_commit_hash` being None, if the code path way vLLM takes is exercised, and it will always cause this error."
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @ajayvohra2005, I'm still not sure I see the bug, and other vLLM users haven't reported it! When I inspect our code, it seems like instances of `revision=None` or `_commit_hash=None` are correctly caught and default to the `main` branch of the model repo.\n\nCan you give me a minimal reproducer script or something we can run to see the error?"
      }
    ]
  },
  {
    "issue_number": 38601,
    "title": "Contribute to Transformers on windows natively without WSL",
    "author": "muha33ad",
    "state": "open",
    "created_at": "2025-06-05T04:14:12Z",
    "updated_at": "2025-06-06T14:42:42Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n### System info\nOS: Windows 11\nPython: 3.13.3 and 3.10\nGit: 2.49.0\nCMake: 4.0.2\nMsys64:  Pacman v6.1.0 - libalpm v14.0.0\nPip: 25.1.1 \nSetuptools: 80.9.0\nVisual studio C++ build tools\n\n### NOTE: I followed the steps here [Contribute to 🤗 Transformers](https://huggingface.co/docs/transformers/en/contributing) and for sure system info already existed before following but let me walk through again for additional info.\n1- Forked the repo.\n2- Cloned it\n3- cd transformers (so made sure I am in the right path which is the root for the repo)\n3- switched to my own branch\n4- made a python virtual environment using python 3.10 then activated it \n5- made sure transformers ain't installed inside it\n6- installed PyTorch\n7- Ran this command `pip install -e \".[dev]\"`\n\n\n### NOTE: I tried making requirements.txt and using this command `pip install -r requirements.txt` but I got no output and I tried installing onnx with pip which happened successfully then Ran this command `pip install -e \".[dev]\"` but nothing changed\n\n### NOTE 6/6/2025: I tried uv instead of python venv, nothing worked. I tried deleting everything including system info and install everything from the beginning, nothing worked still. I made a requiremets.txt from what is in setup.py and installed it and tried to run `pip install -e \".[dev]\"` but same issues again, nothing worked\n\n```\n  error: subprocess-exited-with-error\n\n  × python setup.py egg_info did not run successfully.\n  │ exit code: 1\n  ╰─> [11 lines of output]\n      ...\\setup.py:36: DeprecationWarning: Use shutil.which instead of find_executable\n        CMAKE = find_executable('cmake3') or find_executable('cmake')\n      ...\\setup.py:37: DeprecationWarning: Use shutil.which instead of find_executable\n        MAKE = find_executable('make')\n      fatal: not a git repository (or any of the parent directories): .git\n      Traceback (most recent call last):\n        File \"<string>\", line 2, in <module>\n        File \"<pip-setuptools-caller>\", line 35, in <module>\n        File \"...\\setup.py\", line 318, in <module>\n          raise FileNotFoundError(\"Unable to find \" + requirements_file)\n      FileNotFoundError: Unable to find requirements.txt\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n```\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n`pip install -e \".[dev]\"`\n\n### Expected behavior\n\nBeing able to install transformers for contributing with no issue",
    "comments": [
      {
        "user": "John6666cat",
        "body": "Related?:\nSimilar error (not similar issue): https://github.com/autogluon/autogluon/issues/4620\nSimilar issue (not similar error): https://discuss.huggingface.co/t/transformers-repo-install-error/158047 to https://huggingface.co/spaces/transformers-community/support/discussions/12"
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @MuhammadEssameldeen, can you post this one on the community support space? Someone from the community or one of the Hugging Face engineers should be able to help there: https://huggingface.co/spaces/transformers-community/support"
      }
    ]
  },
  {
    "issue_number": 38071,
    "title": "transformers showing decoder model architecture detected so padding should be left",
    "author": "sleepingcat4",
    "state": "open",
    "created_at": "2025-05-11T19:45:23Z",
    "updated_at": "2025-06-06T11:38:29Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nI had implemented some batching both using pipeline and model.generate method and for Qwen3 and Llama4 models it shows decoder only model detected so padding should mentioned in left. Which is weird as I inspected the model output and it was totally fine. \n\nHowever when I mentioned padding to be left, Qwen3 class returned an error. it is very weird. Can this waring/bug be fixed?\n\n### Who can help?\n\n@ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nTo reproduce please use my below code:\n\n```\ndef load_pipeline(model_name: str):\n    pipe = pipeline(\n        \"text-generation\",\n        model=model_name,\n        model_kwargs={\"torch_dtype\": torch.bfloat16, \"attn_implementation\": \"flash_attention_2\"},\n        device_map=\"auto\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    return pipe, tokenizer\n\nsystem_prompt = (\n    \"Think step by step to answer the following question.\\n\"\n    \"Return the answer for the MC (Multiple Choice) question at the end of the response after a separator #####.\\n\"\n)\n\ndef zero_eval(df, pipe, tokenizer, agieval=False, batch_size=4):\n    full_prompts = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        question = row[\"question\"]\n        if agieval:\n            choices_str = row[\"answer\"]\n        else:\n            choices = eval(row[\"options\"])\n            choices_str = \"\\n\".join([f\"({chr(65+i)}) {c}\" for i, c in enumerate(choices)])\n        user_prompt = f\"---\\nQ: {question}\\nChoices:\\n{choices_str}\\nA:\"\n        full_prompts.append(user_prompt)\n\n    batch_messages = [[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": prompt}\n    ] for prompt in full_prompts]\n\n    prompts = [\n        tokenizer.apply_chat_template(msg, tokenize=False, enable_thinking=False, add_generation_prompt=True)\n        for msg in batch_messages\n    ]\n\n    outputs = []\n    for i in tqdm(range(0, len(prompts), batch_size)):\n        batch = prompts[i:i+batch_size]\n        result = pipe(batch, batch_size=batch_size, max_new_tokens=2035)\n        outputs.extend([r[0][\"generated_text\"] for r in result])\n\n    df[\"full_prompt\"] = full_prompts\n    df[\"model_output\"] = outputs\n\n    model_name = pipe.model.config.name_or_path.split(\"/\")[-1].replace(\":\", \"_\")\n    timestamp = datetime.now().strftime(\"%H-%M_%Y-%m-%d\")\n    folder_name = model_name\n    os.makedirs(folder_name, exist_ok=True)\n    file_name = f\"{model_name}_{timestamp}.csv\"\n    file_path = os.path.join(folder_name, file_name)\n\n    df.to_csv(file_path, index=False)\n```\n\n### Expected behavior\n\nWhen you will run the code, the warning will be shown for Qwen3 models and Llama4 models.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @gante as well maybe, since I think that warning is generation-related?"
      },
      {
        "user": "gante",
        "body": "Hi @sleepingcat4 👋 \n\nYes, the warning is thrown at generation-time: at text generation time, when padding is used, it should be left-padding (see [here](https://huggingface.co/docs/transformers/llm_tutorial#padding-side) for more details).\n\nCan you try passing the `attention_mask` from tokenization into the pipeline as well? In general, it's good practice always to pass the attention mask to the forward pass. If it doesn't work, let me know :) "
      },
      {
        "user": "sleepingcat4",
        "body": "@gante thanks but we didn't see any issues with our code in terms of what the model was generating. I have tried to do what the error told me but it produces garbage. Can you please try my code and see once?"
      }
    ]
  },
  {
    "issue_number": 31626,
    "title": "New `save_strategy` option called \"best\" to save when a new best performance is achieved.",
    "author": "seanswyi",
    "state": "closed",
    "created_at": "2024-06-26T04:04:29Z",
    "updated_at": "2025-06-06T11:33:26Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\r\n\r\nIntroduce a new option for the `save_strategy` argument called `\"best\"` which would save the model once a new best performance is achieved.\r\n\r\n### Motivation\r\n\r\nThe `save_strategy` argument was first introduced a few years ago in https://github.com/huggingface/transformers/pull/10286. Currently the supported options are `\"no\"`, `\"epoch\"`, and `\"steps\"`. I'm assuming that this is to match the [`IntervalStrategy`](https://github.com/huggingface/transformers/blob/0f67ba1d741d65b07d549daf4ee157609ce4f9c1/src/transformers/trainer_utils.py#L221:L224) that's used by evaluation as well.\r\n\r\nJudging by a conversation on a HuggingFace Discussion Forum topic, the best model is always kept by default and therefore if saving occurs at any time during the process then the best model is saved (ref: https://discuss.huggingface.co/t/save-only-best-model-in-trainer/8442). If the user deems that saving often is too burdensome then they may set `save_strategy = \"no\"` which would save the best model at the end of training.\r\n\r\nI believe that introducing some flexibility for saving would be beneficial so that users don't have to perform saving so often but also don't have to wait until the end for a checkpoint.\r\n\r\n### Your contribution\r\n\r\nIf this feature is deemed worth it by the core maintainers then I'd be willing to take this on myself and open a PR. There are some aspects that I believe might warrant further discussion (e.g., which metric should be used to determine \"best,\" how to override `IntervalStrategy`, etc.).",
    "comments": [
      {
        "user": "amyeroberts",
        "body": "cc @muellerzr @SunMarc "
      },
      {
        "user": "muellerzr",
        "body": "Sure, this seems reasonable. Feel free to open a PR!"
      },
      {
        "user": "rudolfwilliam",
        "body": "It seems like this has been implemented by now. Maybe close the issue?"
      }
    ]
  },
  {
    "issue_number": 36718,
    "title": "Add RoMa keypoint matcher",
    "author": "tcourat",
    "state": "open",
    "created_at": "2025-03-14T10:42:45Z",
    "updated_at": "2025-06-06T10:46:08Z",
    "labels": [
      "New model",
      "Vision"
    ],
    "body": "### Model description\n\nRoMa is a state of the art deep keypoint detector and matcher.  It falls in the \"fully dense\" matcher category, contrary to semi-dense matchers like LoFTR or Sparse matchers like Superpoint+SuperGlue. \n\nWhen you want the best performance when you do not have many limitation from speed or memory, then RoMa is giving among the best results.\n\nThe paper introduces several key contributions:\n* Feature Integration: Combines DINOv2’s coarse features with fine ConvNet features for robust, precise localization.\n* Transformer-Based Match Decoder: Predicts anchor probabilities instead of coordinates.\n* Improved Loss Formulation: Uses classification-based regression for global matches and robust regression for refinement.\n\n### Open source status\n\n- [x] The model implementation is available\n- [x] The model weights are available\n\n### Provide useful links for the implementation\n\nPaper : https://arxiv.org/abs/2305.15404\nCode : https://github.com/Parskatt/RoMa",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @qubvel @NielsRogge "
      },
      {
        "user": "qubvel",
        "body": "btw we have EfficientLoFTR matcher PR in progress \n - #36355 "
      },
      {
        "user": "henrikm11",
        "body": "As this appears to have been inactive for a while: Is this still open and if so in progress? Otherwise I'd be happy to pick this up if it's ok that it will most likely take me quite a while. @qubvel @Rocketknight1 "
      }
    ]
  },
  {
    "issue_number": 32035,
    "title": "Why MPS can never be used successfully?",
    "author": "AimoneAndex",
    "state": "closed",
    "created_at": "2024-07-18T03:23:37Z",
    "updated_at": "2025-06-06T09:30:26Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nDevice:Apple M3 Pro\r\nOS:macOS Sonoma 14.1\r\npackages:\r\ndatasets           2.20.1.dev0 \r\nevaluate           0.4.2 \r\nhuggingface-hub    0.23.5\r\ntokenizers         0.19.1\r\ntorch              2.5.0.dev20240717\r\ntorchaudio         2.4.0.dev20240717\r\ntorchvision        0.20.0.dev20240717\r\ntransformers       4.43.0.dev0  \n\n### Who can help?\n\n@ArthurZucker @muellerzr @ArthurZucker\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\n```python\r\nimport os\r\nfrom datasets import load_dataset\r\nfrom transformers import (\r\n    AutoTokenizer,\r\n    AutoModelForCausalLM,\r\n    DataCollatorForSeq2Seq,\r\n    TrainingArguments,\r\n    Trainer\r\n)\r\nimport torch\r\nfrom peft import LoraConfig, TaskType, get_peft_model\r\n\r\n# 设定路径\r\ncurrent_dir = os.getcwd()\r\nmodel_dir = os.path.join(current_dir, 'model', 'zh-7b')\r\nsave_dir = os.path.join(current_dir, 'model', 'zh-7b-saved')\r\ntarget_file_path = os.path.join(current_dir, 'dats.csv')\r\n\r\n# 加载数据集\r\ndataset = load_dataset(\"csv\", data_files=target_file_path, split=\"train\")\r\n\r\n# 加载分词器\r\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\r\ntokenizer.padding_side = \"right\"\r\ntokenizer.pad_token_id = 2\r\n\r\n# 数据预处理\r\ndef process_func(example):\r\n    MAX_LENGTH = 384\r\n    instruction = tokenizer(f\"Human: {example['instruction']}\\n{example['input']}\\n\\nAssistant: \", add_special_tokens=False)\r\n    response = tokenizer(example['output'], add_special_tokens=False)\r\n    input_ids = instruction['input_ids'] + response['input_ids'] + [tokenizer.eos_token_id]\r\n    attention_mask = instruction['attention_mask'] + response['attention_mask'] + [1]\r\n    labels = [-100] * len(instruction['input_ids']) + response['input_ids'] + [tokenizer.eos_token_id]\r\n    if len(input_ids) > MAX_LENGTH:\r\n        input_ids = input_ids[:MAX_LENGTH]\r\n        attention_mask = attention_mask[:MAX_LENGTH]\r\n        labels = labels[:MAX_LENGTH]\r\n    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\r\n\r\n# 预处理数据集\r\ntokenized_dataset = dataset.map(process_func, remove_columns=dataset.column_names)\r\n\r\n# 加载模型到 MPS 设备\r\ndevice = torch.device(\"mps\")\r\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, low_cpu_mem_usage=True, torch_dtype=torch.half, device_map=\"mps\")\r\n\r\n# 加载 LoRA 配置\r\nconfig = LoraConfig(task_type=TaskType.CAUSAL_LM)\r\nmodel = get_peft_model(model, config)\r\n\r\n# 设置训练参数\r\ntraining_args = TrainingArguments(\r\n    output_dir=save_dir,\r\n    per_device_train_batch_size=2,\r\n    gradient_accumulation_steps=8,\r\n    logging_steps=10,\r\n    num_train_epochs=1,\r\n)\r\n\r\n# 创建 Trainer\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=tokenized_dataset.select(range(3)),  # 限制训练数据集大小，仅用于测试\r\n    data_collator=DataCollatorForSeq2Seq(tokenizer, padding=True),\r\n)\r\n\r\n# 开始训练\r\ntrainer.train()\r\n\r\n# 保存模型\r\nmodel.save_pretrained(save_dir)\r\n```\r\nThen it prints:\r\nRuntimeError: Placeholder storage has not been allocated on MPS device!\n\n### Expected behavior\n\nTrain successfully.",
    "comments": [
      {
        "user": "jponf",
        "body": "@AimoneAndex I was looking for this too, luckily it has already been fixed, see #31812. It is a matter of waiting for a new release. In the meantime you can install the package from github to get the fix now:\r\n\r\n```console\r\npip install git+https://github.com/huggingface/transformers.git\r\n```\r\n\r\n@ArthurZucker could you tell us when is the next release planned? not being able to use MPS on mac devices is quite annoying 😥"
      },
      {
        "user": "ArthurZucker",
        "body": "Release will be this week! Sorry all for the troubles, I am also a mac user and this sucks! "
      },
      {
        "user": "AimoneAndex",
        "body": "> ```\r\n> pip install git+https://github.com/huggingface/transformers.git\r\n> ```\r\n> \r\n> @ArthurZucker could you tell us when is the next release planned? not being able to use MPS on mac devices is quite annoying 😥\r\n\r\nThank you!And I will update it as soon as the new version releases!"
      }
    ]
  },
  {
    "issue_number": 38571,
    "title": "Oneke not utilizing much from GPU(Nvidia L20)",
    "author": "zy1260957619",
    "state": "closed",
    "created_at": "2025-06-04T03:51:57Z",
    "updated_at": "2025-06-06T08:52:36Z",
    "labels": [],
    "body": "I have met the same issue as #18124  when I run [oneke model](https://huggingface.co/zjunlp/OneKE) with bnb_quant.\nthe relate package version as：\naccelerate=1.7.0、1.5.2、0.31.0\ntransformers=4.44.2、4.50.0\nbitsandbytes=0.43.3\n\nNvidia-smi 535.183.01\n\n\n\nbut use another device  NVIDIA A4000 NVIDIA-smi 528.89 and use awq_quanted model on Nvidia L20 do not occur this issue。",
    "comments": [
      {
        "user": "zy1260957619",
        "body": "I would like to seek some help and advice for this issue"
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @zy1260957619, I'm not sure this is a bug but if you ask in the [community support space](https://huggingface.co/spaces/transformers-community/support) one of the HF engineers there might be able to help!"
      }
    ]
  },
  {
    "issue_number": 36010,
    "title": "ImportError: cannot import name 'GenerationMixin' from 'transformers.generation'",
    "author": "sij411",
    "state": "closed",
    "created_at": "2025-02-03T08:40:41Z",
    "updated_at": "2025-06-06T08:14:20Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n\n- `transformers` version: 4.47.1\n- Platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.39\n- Python version: 3.11.11\n- Huggingface_hub version: 0.28.1\n- Safetensors version: 0.5.2\n- Accelerate version: not installed\n- Accelerate config: not found\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA GeForce RTX 3090\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nRunning  Usage example on this [repo](https://github.com/segment-any-text/wtpsplit)\n\nRun the codes on new minicoda virtual environmet only after `pip install wtpsplit` and `pip install torch`.\n\n`---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[2], line 3\n      1 from wtpsplit import SaT\n----> 3 sat = SaT(\"sat-3l\")\n      4 # optionally run on GPU for better performance\n      5 # also supports TPUs via e.g. sat.to(\"xla:0\"), in that case pass `pad_last_batch=True` to sat.split\n      6 sat.half().to(\"cuda\")\n\nFile ~/miniforge3/envs/sat/lib/python3.11/site-packages/wtpsplit/__init__.py:514, in SaT.__init__(self, model_name_or_model, from_pretrained_kwargs, ort_providers, ort_kwargs, style_or_domain, language, lora_path, hub_prefix)\n    511     except ModuleNotFoundError:\n    512         raise ValueError(\"Please install `torch` to use WtP with a PyTorch model.\")\n--> 514     import wtpsplit.models  # noqa\n    516     self.model = PyTorchWrapper(\n    517         AutoModelForTokenClassification.from_pretrained(\n    518             model_name_to_fetch, **(from_pretrained_kwargs or {})\n    519         )\n    520     )\n    521 # LoRA LOADING\n\nFile ~/miniforge3/envs/sat/lib/python3.11/site-packages/wtpsplit/models.py:13\n      8 from transformers import AutoModel, AutoModelForTokenClassification\n      9 from transformers.modeling_outputs import (\n     10     BaseModelOutputWithPoolingAndCrossAttentions,\n     11     BaseModelOutputWithPastAndCrossAttentions,\n     12 )\n---> 13 from transformers.modeling_utils import ModuleUtilsMixin\n     14 from transformers.models.bert.modeling_bert import BertEncoder, BertForTokenClassification, BertModel, BertPooler\n     15 from transformers.models.canine.modeling_canine import (\n     16     _PRIMES,\n     17     ACT2FN,\n   (...)\n     33     TokenClassifierOutput,\n     34 )\n\nFile ~/miniforge3/envs/sat/lib/python3.11/site-packages/transformers/modeling_utils.py:46\n     44 from .configuration_utils import PretrainedConfig\n     45 from .dynamic_module_utils import custom_object_save\n---> 46 from .generation import CompileConfig, GenerationConfig, GenerationMixin\n     47 from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n     48 from .loss.loss_utils import LOSS_MAPPING\n\nImportError: cannot import name 'GenerationMixin' from 'transformers.generation' (/home/bering-gpu-3/miniforge3/envs/sat/lib/python3.11/site-packages/transformers/generation/__init__.py)`\n\n### Expected behavior\n\nThere shouldn't be no ImportError and print `['This is a test ', 'This is another test']` ",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "BasisSun",
        "body": "I also encountered this issue. But it occurred when I imported the following package.\n`from transformers import LayoutLMv3ImageProcessor,AutoTokenizer,LayoutLMv3Processor`\nThe error message is:\n`ImportError: cannot import name 'GenerationMixin' from 'transformers.generation' (/home/healthcenter/workFiles/CondaEnV/Transformers_test/lib/python3.10/site-packages/transformers/generation/__init__.py)`"
      }
    ]
  },
  {
    "issue_number": 35588,
    "title": "flash_attention_2 2.7.2.post1 seems to crash when using `torch.compile` and `DataCollatorWithFlattening`",
    "author": "avishaiElmakies",
    "state": "closed",
    "created_at": "2025-01-09T14:16:52Z",
    "updated_at": "2025-06-06T08:05:21Z",
    "labels": [
      "bug",
      "Compilation"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.47.1\r\n- Platform: Linux-6.6.20-aufs-1-x86_64-with-glibc2.36\r\n- Python version: 3.11.2\r\n- Huggingface_hub version: 0.26.2\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: No\r\n- Using GPU in script?: yes\r\n- GPU type: NVIDIA RTX A5000\n\n### Who can help?\n\n@ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nupdate to latest flash attention version (as the time of writing 2.7.2). this should be torch.compile compatible as described in https://github.com/Dao-AILab/flash-attention\r\nload a model with fa2 (tested with opt and qwen)\r\nuse trainer with `DataCollatorWithFlattening` and train.\r\n\r\nthis causes a crash with the following stacktrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/slm_eval/train.py\", line 89, in main\r\n    trainer.train(resume_from_checkpoint=cfg.cont_training)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/trainer.py\", line 2164, in train\r\n    return inner_training_loop(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/trainer.py\", line 2524, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/slm_eval/trainer/slam_trainer.py\", line 71, in training_step\r\n    return super().training_step(model, inputs, num_items_in_batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/trainer.py\", line 3654, in training_step\r\n    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/trainer.py\", line 3708, in compute_loss\r\n    outputs = model(**inputs)\r\n              ^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 823, in forward\r\n    return model_forward(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 811, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/slm_eval/model/unit_lm.py\", line 118, in forward\r\n    def forward(self,\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1109, in forward\r\n    @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 895, in forward\r\n    layer_outputs = decoder_layer(\r\n                    ^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 584, in forward\r\n    def forward(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 364, in forward\r\n    def forward(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 419, in torch_dynamo_resume_in_forward_at_419\r\n    logger.warning_once(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py\", line 231, in _flash_attention_forward\r\n    def _flash_attention_forward(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py\", line 329, in torch_dynamo_resume_in__flash_attention_forward_at_329\r\n    max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1269, in __call__\r\n    return self._torchdynamo_orig_callable(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\r\n    result = self._inner_convert(\r\n             ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\r\n    return _compile(\r\n           ^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\r\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\r\n    return _compile_inner(code, one_graph, hooks, transform)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_utils_internal.py\", line 87, in wrapper_function\r\n    return function(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\r\n    out_code = transform_code_object(code, transform)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\r\n    tracer.run()\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\r\n    super().run()\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n    while self.step():\r\n          ^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n    self.dispatch_table[inst.opcode](self, inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n    return inner_fn(self, inst)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1680, in CALL_FUNCTION_EX\r\n    self.call_function(fn, argsvars.items, kwargsvars)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n    return super().call_function(tx, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 111, in call_function\r\n    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n    return cls.inline_call_(parent, func, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n    tracer.run()\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n    while self.step():\r\n          ^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n    self.dispatch_table[inst.opcode](self, inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n    return inner_fn(self, inst)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2279, in CALL\r\n    self._call(inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2273, in _call\r\n    self.call_function(fn, args, kwargs)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 1024, in call_function\r\n    return self.obj.call_method(tx, self.name, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 774, in call_method\r\n    return self.call_apply(tx, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 699, in call_apply\r\n    ).call_function(tx, args, kwargs)\r\n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2015, in call_function\r\n    (fwd_out, _), fwd_graph, fwd_freevars = speculate_subgraph(\r\n                                            ^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 462, in speculate_subgraph\r\n    output = f.call_function(tx, args, sub_kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n    return super().call_function(tx, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 111, in call_function\r\n    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n    return cls.inline_call_(parent, func, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n    tracer.run()\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n    while self.step():\r\n          ^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n    self.dispatch_table[inst.opcode](self, inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n    return inner_fn(self, inst)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2279, in CALL\r\n    self._call(inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2273, in _call\r\n    self.call_function(fn, args, kwargs)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/torch.py\", line 897, in call_function\r\n    tensor_variable = wrap_fx_proxy(\r\n                      ^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 2037, in wrap_fx_proxy\r\n    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 2124, in wrap_fx_proxy_cls\r\n    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2082, in get_fake_value\r\n    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2017, in get_fake_value\r\n    ret_val = wrap_fake_exception(\r\n              ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 1574, in wrap_fake_exception\r\n    return fn()\r\n           ^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2018, in <lambda>\r\n    lambda: run_node(tx.output, node, args, kwargs, nnmodule)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2150, in run_node\r\n    raise RuntimeError(make_error_message(e)).with_traceback(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2132, in run_node\r\n    return node.target(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_ops.py\", line 1116, in __call__\r\n    return self._op(*args, **(kwargs or {}))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntorch._dynamo.exc.TorchRuntimeError: Failed running call_function flash_attn._flash_attn_varlen_forward(*(FakeTensor(..., device='cuda:0', size=(s3, s4, s5), dtype=torch.float16,\r\n           grad_fn=<AsStridedBackward0>), FakeTensor(..., device='cuda:0', size=(s6, s7, s8), dtype=torch.float16,\r\n           grad_fn=<Error>), FakeTensor(..., device='cuda:0', size=(s9, s10, s11), dtype=torch.float16,\r\n           grad_fn=<Error>), FakeTensor(..., device='cuda:0', size=(s13,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(s13,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), 0.0, FloatPow(ToFloat(s5), -0.5)), **{'causal': True, 'window_size_left': -1, 'window_size_right': -1, 'softcap': 0.0, 'alibi_slopes': None, 'return_softmax': False, 'block_table': None}):\r\nflash_attn::_flash_attn_varlen_forward() Expected a value of type 'int' for argument 'max_seqlen_q' but instead found type 'FakeTensor'.\r\nPosition: 5\r\nValue: FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64)\r\nDeclaration: flash_attn::_flash_attn_varlen_forward(Tensor q, Tensor k, Tensor v, Tensor cu_seqlens_q, Tensor cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left=-1, SymInt window_size_right=-1, float softcap=0., Tensor? alibi_slopes=None, bool return_softmax=False, Tensor? block_table=None, Tensor? leftpad_k=None, Tensor? seqused_k=None) -> (Tensor, Tensor, Tensor, Tensor)\r\nCast error details: Unable to cast Python instance of type <class 'torch._subclasses.fake_tensor.FakeTensor'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)\r\n\r\nfrom user code:\r\n   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py\", line 346, in torch_dynamo_resume_in__flash_attention_forward_at_335\r\n    attn_output = flash_attn_varlen_func(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py\", line 1412, in flash_attn_varlen_func\r\n    return FlashAttnVarlenFunc.apply(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py\", line 901, in forward\r\n    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(\r\n\r\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    import torch._dynamo\r\n    torch._dynamo.config.suppress_errors = True\r\n``` \r\n\r\nthe code works fine when not using compile.\r\nthe code doesn't crash when using compile but **not** using `DataCollatorWithFlattening`.\r\nwhen using compile and **not** using `DataCollatorWithFlattening` I am getting the following graph break with qwen2.5\r\n```\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] Graph break from `Tensor.item()`, consider setting:\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] or:\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] to include these operations in the captured graph.\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] \r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] Graph break: from user code at:\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 823, in forward\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     return model_forward(*args, **kwargs)\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 811, in __call__\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     return convert_to_fp32(self.model_forward(*args, **kwargs))\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     return func(*args, **kwargs)\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/slm_eval/model/unit_lm.py\", line 138, in forward\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     outputs = self.lm(\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1165, in forward\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     outputs = self.model(\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 864, in forward\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     causal_mask = self._update_causal_mask(\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 943, in _update_causal_mask\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     if attention_mask is not None and 0.0 in attention_mask:\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] \r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] \r\n```\n\n### Expected behavior\n\nthe training shouldn't crash. ",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Maybe @muellerzr @SunMarc? Feel free to ping someone else if you think they're more appropriate"
      },
      {
        "user": "SunMarc",
        "body": "For the issue with qwen2, it is solved with https://github.com/huggingface/transformers/pull/35187. \r\nAs for DataCollatorWithFlattening, I don't have an answer yet without debugging. Looks like there is an issue with an arg. Can you share a minimal reproducer ? "
      },
      {
        "user": "avishaiElmakies",
        "body": "Hi @SunMarc \n\nthis seems like code that reproduces the issue. I think it is not directly related to `DataCollatorWithFlattening` and more related to the path that the model takes because of the position ids it gets. \n\n```\nfrom transformers import AutoModelForCausalLM,AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer(\"facebook/opt-125m\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-125m\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n)\nmodel = model.cuda()\nmodel = torch.compile(model)\noutputs = tokenizer([\"Text\",\"Second_text\"])\ninput_ids = []\nposition_ids = []\nfor item in outputs[\"input_ids\"]:\n    item = torch.as_tensor(item)\n    input_ids.append(item)\n    position_ids.append(torch.arange(item.shape[-1]))\ninput_ids = torch.cat(input_ids).unsqueeze(0).cuda()\nposition_ids = torch.cat(position_ids).unsqueeze(0).cuda()\nmodel(input_ids=input_ids,position_ids=position_ids) # crashes with same error\n```"
      }
    ]
  },
  {
    "issue_number": 37780,
    "title": "Support multimodal models in vLLM with transformers backend",
    "author": "zucchini-nlp",
    "state": "closed",
    "created_at": "2025-04-25T07:39:06Z",
    "updated_at": "2025-06-06T08:04:17Z",
    "labels": [
      "External",
      "VLM"
    ],
    "body": "This is a tracker issue since there are now too many PRs here and there, all making some sort of standardization to help vLLM support vision LLMs.\n\nWhat we need is:\n- [x] Identical naming for special multimodal tokens, same way as we have `config.pad_token_id` (https://github.com/huggingface/transformers/pull/37573)\n- [x] Add base model and nudge new models to follow LLM-like format. In other words, a base model holds everything without head and the `ConditionalGeneration` holds \"base model + head\" (https://github.com/huggingface/transformers/pull/37033)\n- [ ] Helper fn to obtain multimodal embeddings from the base model because each model has its own pre/post projection layers on top (https://github.com/huggingface/transformers/pull/37743)\n- [x] Clean up qwen models which are completely different from existing VLMs in structure (https://github.com/huggingface/transformers/pull/37268) and hopefully after all done it will be okay. Needs verification\n- [X] Support attention backends for all models by using new attn interface and correctly propagating `kwargs` . BTW, we also need to use `self.loss_fn` after this PR, to fix issues in Trainer with grad accum. But that is not vLLM related and comes in subsequent PR (https://github.com/huggingface/transformers/pull/37576)\n- [ ] Processors need helpers to calculate `num_multimodal_tokens` given input image sizes and to return `mm_token_type_ids` (https://github.com/huggingface/transformers/pull/37915)\n- [ ] All vision backbones should return `embeds` of shape `(bs, ...., dim)`. I found Pixtral doesn't do so, and will need to go over other models as well. To check if it is doable in vision encoder (BC breaking?) or we need to postprocess `embeds` in `_get_image_features`\n\n\ncc @hmellor so you can keep track\ncc @ArthurZucker @Cyrilvallez , I will be pinging you for reviews 😄 ",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "zucchini-nlp",
        "body": "Closing as complete!"
      }
    ]
  },
  {
    "issue_number": 36961,
    "title": "Gemma3: Cuda error: misaligned address",
    "author": "falkbene",
    "state": "closed",
    "created_at": "2025-03-25T13:34:52Z",
    "updated_at": "2025-06-06T08:04:00Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.50.0\n- Platform: Linux-5.15.148-tegra-aarch64-with-glibc2.35\n- Python version: 3.10.12\n- Huggingface_hub version: 0.29.3\n- Safetensors version: 0.5.3\n- Accelerate version: 1.5.2\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: Orin\n\n### Who can help?\n\n@ Rocketknight1 @ ArthurZucker\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nI am creating a `HuggingFacePipeline(pipeline=pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\",device=\"cuda\")) ` and are using it for async generation, it works for a few generations, but at some point I run into this error: \n```\n...\nRuntimeError: CUDA error: misaligned address\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n...\n  File \"/usr/lib/python3.10/queue.py\", line 179, in get\n    raise Empty\n_queue.Empty\n```\n\nIt works fine for all other models I have tried (e.g. Llama-3.2 1B/3B, gemma-2-2b-it, ...), but gemma 3 seems to cause this error. I stumbled across this issue: AUTOMATIC1111/stable-diffusion-webui#9954, but the cause seems to be a different one and the solutions did not resolve my issue. Temperature also seems to be normal ~45 C°. \n\nI am sending 20 consecutive responses to the model to benchmark it on my hardware, but only one at a time. \n\nWould appreciate any help on this :)\n\n### Expected behavior\n\nGeneration without errors just like the first few requests to the model and like any other model I tried.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @falkbene, can you give us a complete reproducer so we can verify the issue?"
      },
      {
        "user": "falkbene",
        "body": "Thank you for your response @Rocketknight1. You can try to run the following code. I am working on a Nvidia Jetson agx Orin.\n\n```\nimport asyncio\nfrom huggingface_hub import login\nfrom langchain_huggingface.llms import HuggingFacePipeline\nfrom langchain_core.prompts import PromptTemplate\nfrom transformers import AutoTokenizer, pipeline\n\nclass ChatServer:\n    def __init__(self, model_id: str):\n        self.model_name = model_id\n        self.login()\n        self.setup_model()\n        self.hf = self.configure_pipeline(150)\n        self.chain = self.prompt | self.hf.bind(skip_prompt=True)\n\n    def login(self):\n        login(token=\"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\")\n\n    def setup_model(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        #self.model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map=\"auto\")\n        self.template = \"\"\"Question: {question}\n\n        Answer: \"\"\"\n        self.prompt = PromptTemplate.from_template(self.template)\n\n    def configure_pipeline(self, max_tokens: int):\n        self.pipeline = pipeline(\"text-generation\", model=self.model_name, tokenizer=self.tokenizer, max_new_tokens=max_tokens, device=\"cuda\")\n        return HuggingFacePipeline(pipeline=self.pipeline)\n\nasync def main():\n    #model_id = \"meta-llama/Llama-3.2-1B\"\n    model_id = \"google/gemma-3-1b-it\"\n    prompt = \"\"\"\n    Randomly stream lines from the following text with 152 output tokens. Don't generate eos tokens:\n\nAnd make Time's spoils despised every where.\nSince, seldom coming, in the long year set,\nThat use is not forbidden usury,\nAnd gives thy pen both skill and argument.\nWhere art thou, Muse, that thou forget'st so long\nSometime too hot the eye of heaven shines,\nAs tender nurse her babe from faring ill.\nFor blunting the fine point of seldom pleasure.\nIn thee thy summer, ere thou be distill'd:\nTherefore are feasts so solemn and so rare,\nSince mind at first in character was done!\nSo is the time that keeps you as my chest,\nWhich in thy breast doth live, as thine in me:\nAnd summer's lease hath all too short a date:\nThen look I death my days should expiate.\nBut thy eternal summer shall not fade\nHath been before, how are our brains beguiled,\nOr whether revolution be the same.\nWith beauty's treasure, ere it be self-kill'd.\nSo long as youth and thou are of one date;\nIf there be nothing new, but that which is\nPresume not on thy heart when mine is slain;\nReturn, forgetful Muse, and straight redeem\nBeing had, to triumph, being lack'd, to hope.\nTo be death's conquest and make worms thine heir.\nBut when in thee time's furrows I behold,\nTen times thyself were happier than thou art,\nBearing thy heart, which I will keep so chary\nAnd every fair from fair sometime declines,\nShall I compare thee to a summer's day?\nThe which he will not every hour survey,\nNor lose possession of that fair thou owest;\nEven of five hundred courses of the sun,\nAnd often is his gold complexion dimm'd;\nIn gentle numbers time so idly spent;\nTo speak of that which gives thee all thy might?\nBy chance or nature's changing course untrimm'd;\nThou art \n    \"\"\"\n\n    chat_server = ChatServer(model_id)\n\n    for i in range(20):\n        \n        generated_output = chat_server.chain.astream(prompt)\n        \n        async for chunk in generated_output:\n            print(chunk,sep=\" \",end=\"\",flush=True)\n        \n        print(\"\\n\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nYou can try both models, Llama works for me, gemma3 causes the error in the first loop. "
      },
      {
        "user": "Rocketknight1",
        "body": "Hi @falkbene, can you reproduce the error without `langchain`? We don't actually maintain that, and I haven't seen that kind of CUDA error in `transformers` before! For example, if you just run the pipeline you get from the Hugging Face `pipeline()` function, does it also fail in the same way?"
      }
    ]
  },
  {
    "issue_number": 37823,
    "title": "Decoder Attention Mask  is not passed to the VisionEncoderDecoderModel during training!!",
    "author": "AhmadM-DL",
    "state": "closed",
    "created_at": "2025-04-28T08:25:54Z",
    "updated_at": "2025-06-06T08:03:05Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nlatest transformers\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI am training a VisionEncoderDecoderModel.\nI noticed that while the modeling code shifts the \"labels\" to right to generate \"decoder_input_ids\".\nIt doesn't generate the \"decoder_attnetion_mask\" by default.\nHere a snippet from the `vision_encoder_decoder_modeling` file:\n\nhttps://github.com/huggingface/transformers/blob/816b37010cb6fd963433c6c5681b18be6475592e/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L612\n\n### Expected behavior\n\nThe default behavior should be to generate the mask on the go.\nAside form this how we can pass decoder attention mask?\n\nI am using the Seq2Seq trainer.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @AhmadM-DL, the decoder model will usually generate the attention mask internally if `None` is passed"
      },
      {
        "user": "AhmadM-DL",
        "body": "Are you sure. I tried to trace the code but was not successful.\r\n\r\nI am doubtful of that as when I am training the VisionEncoderDecoderModel I end up with a warning message that they strongly recommend using an attention mask!\r\n________________________________\r\nFrom: Matt ***@***.***>\r\nSent: Monday, April 28, 2025 2:01:07 PM\r\nTo: huggingface/transformers ***@***.***>\r\nCc: AhmadM-DL ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [huggingface/transformers] Decoder Attention Mask is not passed to the VisionEncoderDecoderModel during training!! (Issue #37823)\r\n\r\n[https://avatars.githubusercontent.com/u/12866554?s=20&v=4]Rocketknight1 left a comment (huggingface/transformers#37823)<https://github.com/huggingface/transformers/issues/37823#issuecomment-2834871730>\r\n\r\nHi @AhmadM-DL<https://github.com/AhmadM-DL>, the decoder model will usually generate the attention mask internally if None is passed\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/huggingface/transformers/issues/37823#issuecomment-2834871730>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AMQXU6HVWX2OEIF3U6TANJT23YC7HAVCNFSM6AAAAAB37ZV3FSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDQMZUHA3TCNZTGA>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n"
      },
      {
        "user": "Rocketknight1",
        "body": "Can you paste the code you used?"
      }
    ]
  },
  {
    "issue_number": 37851,
    "title": "AttentionMaskVisualizer hard-code sliding_window to 5 in transformers code.",
    "author": "MilkClouds",
    "state": "closed",
    "created_at": "2025-04-29T07:54:21Z",
    "updated_at": "2025-06-06T08:03:00Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\ntitle is all. you may check it in code in https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src/transformers/utils/attention_visualizer.py#L143.\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nCode itself has a clear bug\n\n### Expected behavior\n\nsliding window must not be set by visualizer.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @ArthurZucker do you know why the visualizer sets attributes on the text config like that?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38484,
    "title": "Clarification on per_device_train_batch_size in Trainer",
    "author": "KeshavSingh29",
    "state": "open",
    "created_at": "2025-05-30T02:17:12Z",
    "updated_at": "2025-06-06T01:36:03Z",
    "labels": [
      "Good First Issue",
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.1\n- Platform: Linux-5.15.0-1061-nvidia-x86_64-with-glibc2.35\n- Python version: 3.10.16\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.2\n- Accelerate version: 1.7.0\n- Accelerate config:    - compute_environment: LOCAL_MACHINE\n        - distributed_type: FSDP\n        - mixed_precision: bf16\n        - use_cpu: False\n        - debug: False\n        - num_processes: 8\n        - machine_rank: 0\n        - num_machines: 1\n        - main_process_ip: 10.3.0.43\n        - main_process_port: 5678\n        - rdzv_backend: static\n        - same_network: True\n        - main_training_function: main\n        - enable_cpu_affinity: False\n        - fsdp_config: {'fsdp_activation_checkpointing': True, 'fsdp_auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_offload_params': False, 'fsdp_reshard_after_forward': True, 'fsdp_state_dict_type': 'SHARDED_STATE_DICT', 'fsdp_version': 2}\n        - downcast_bf16: no\n        - tpu_use_cluster: False\n        - tpu_use_sudo: False\n        - tpu_env: []\n- DeepSpeed version: 0.15.3\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA H100 80GB HBM3\n\n### Who can help?\n@zach-huggingface @muellerzr  @SunMarc can you please help. \n\n### Brief summary: \n- Trying to train LLM using custom collator and iterable dataset with accelerate (FSDP) \n- Setup is multiGPU on single node (8 GPUs)\n- Need to calculate max_steps parameter prior due to iterable dataset \n\nMy understanding was : \n- Per device means per gpu, so if my `per_device_train_batch_size` is 64 and i have 8 gpus , effective batch size should be 512 \n- Extending that to no. tokens processed per step ->  sequence_len (2048), total tokens per step should be 512*2048 ~ 1M tokens (assuming grad_acc step is 1)\n\nProblem: \n- Training a dummy LLM model from scratch using 10M tokens \n- According to my setup (explained above), it should take 10 steps approx to finish the training.\n- However it takes exactly 8x more steps (only possible if the per device batch size is actually spread across all gpus equally) \n\nNote: \n- I have no padding involved as all data is concatenated to be equal to sequence_len i.e. 2048 \n- I have no sliding_window or chunking when doing this test. Chunk size and stride is set to sequence_len\n- Additionally I logged the tokens seen by my custom collator at each step \n```python\nbatch_size: 64, max_len: 2048                                                                                                                                                                                                     \nInput shape: torch.Size([64, 2048])                                                                                                                                                                                               \n[Step] Tokens this step: 131072     \n```\n\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nWith the same setup, train any LLM using one node. \n\nSharing my data collator code for reference: \n\n```python\nclass CustomDataset(IterableDataset):\n    \"\"\"\n    Custom Dataset class for GPT model\n    Input:\n        data_files: list of all the files used for training\n            Example:\n                {\n                \"train\": [\"file1\",\"file2\" ...],\n                \"validation\": [\"file1\",\"file2\" ...],\n                }\n        split: dataset split (train or val)\n        chunk_len: Max len of sequence a model can handle\n        stride: analogus to window size\n        tokenizer: a tiktoken based tokenizer\n    \"\"\"\n\n    def __init__(\n        self,\n        data_files: dict,\n        split: str,\n        chunk_len: int,\n        stride: int,\n        tokenizer,\n    ):\n        self.data = load_dataset(\n            'json',\n            data_files=data_files,\n            streaming=True,\n            split=split,\n        )\n        self.chunk_len = chunk_len\n        self.stride = stride\n        self.tokenizer = tokenizer\n\n    def __iter__(self):\n        # add sequences to buffer -> less padding tokens\n        buffer = []\n        last_file = None\n        for example in self.data:\n            current_file = example.get('file_name')\n\n            if current_file is not None and current_file != last_file:\n                logger.info(f'Processing file: {current_file}')\n                last_file = current_file\n\n            sequence_ids = example['token_ids']\n\n            # Inject BOS and EOS\n            buffer.append(self.tokenizer.bos_id)\n            buffer.extend(sequence_ids)\n            buffer.append(self.tokenizer.eos_id)\n\n            while len(buffer) >= self.chunk_len:\n                chunk = buffer[:self.chunk_len]\n                buffer = buffer[self.stride:]  # slide the window\n                yield {'input_ids': torch.tensor(chunk, dtype=torch.long)}\n\n\nclass PretrainCollator:\n    \"\"\"\n    Collator for variable-length pretraining sequences.\n    Pads to the batch’s max length, builds attention masks,\n    and uses `ignore_index` for label padding.\n    \"\"\"\n\n    def __init__(self, tokenizer, ignore_index: int = -100):\n        self.tokenizer = tokenizer\n        self.ignore_index = ignore_index\n        self.total_seen_samples = 0\n        self.total_tokens_seen = 0\n\n    def __call__(self, batch: list[dict[str, Tensor]]) -> dict[str, Tensor]:\n        self.total_seen_samples += len(batch)\n        self.total_tokens_seen += sum(len(item['input_ids']) for item in batch)\n\n        # 1) collect all input-id sequences\n        sequences: list[Tensor] = [item['input_ids'] for item in batch]\n\n        # 2) pad inputs (pad with pad_id) and labels (pad with ignore_index)\n        inputs_padded = pad_sequence(\n            sequences, batch_first=True, padding_value=self.tokenizer.pad_id,\n        )\n        labels_padded = pad_sequence(\n            sequences, batch_first=True, padding_value=self.ignore_index,\n        )\n\n        # 3) build attention mask (1 for real tokens, 0 for padding)\n        attention_mask = (inputs_padded != self.tokenizer.pad_id).long()\n\n        return {\n            'input_ids': inputs_padded,\n            'attention_mask': attention_mask,\n            'labels': labels_padded,\n        }\n```\n\n### Expected behavior\n\nThe training should finish in 10 steps. ",
    "comments": [
      {
        "user": "SunMarc",
        "body": "Will have a look soon ! Tagging it as first good issue in case someone have time to look into that before me "
      },
      {
        "user": "KeshavSingh29",
        "body": "@SunMarc I figured that in accelerator_config of trainer, `split_batches` was set to `True`. So it makes sense that the batches per device was actually split across all the processes. \nIn that case, it would be nice to have a log to show the user if they use `per_device_train_batch_size` and `split_batches=True` together.\nElse it becomes a mystery whats happening. \n\nIf okay, i will make a PR / close this issue"
      },
      {
        "user": "SunMarc",
        "body": "Nice that you manage to find the issue @KeshavSingh29  ! Please make a PR for that, that would be super nice  "
      }
    ]
  },
  {
    "issue_number": 38625,
    "title": "Remove 'as_target_processor' from docstring of Wav2Vec2Processor* methods",
    "author": "renet10",
    "state": "open",
    "created_at": "2025-06-05T22:22:49Z",
    "updated_at": "2025-06-05T22:25:02Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nCompanion report to #38609 to modify the docstring within the code for Wav2Vec2Processor and Wav2Vec2ProcessorWithLM\n\n### Who can help?\n\n@eustlb \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nNone.  Refer to #38609 for details.\n\n### Expected behavior\n\nDocstring should not refer to as_target_processor so users will migrate to new method.",
    "comments": [
      {
        "user": "renet10",
        "body": "Opened by mistake, misunderstood a comment in #38609"
      }
    ]
  },
  {
    "issue_number": 38593,
    "title": "Consider Deprecating Sigopt from Hyperparameter Search",
    "author": "ParagEkbote",
    "state": "closed",
    "created_at": "2025-06-04T19:09:58Z",
    "updated_at": "2025-06-05T21:07:24Z",
    "labels": [],
    "body": "The [Sigopt](https://github.com/sigopt/sigopt-server) project is in public archive mode. I think the current backend integration provided by [hyperparameter_search](https://huggingface.co/docs/transformers/en/hpo_train) can be deprecated since the package will not provide continued security or bugfix support. Could you please let me know your thoughts?\n\ncc: @SunMarc ",
    "comments": [
      {
        "user": "SunMarc",
        "body": "Thanks for the report @ParagEkbote ! I don't think we need to remove the integration as this might break the workflow of our users. Instead, maybe we can put in the docs that sigopt was archived and that we recommend using another backend integration instead. "
      }
    ]
  },
  {
    "issue_number": 38416,
    "title": "Model implmenetation using Liger Kernel layers",
    "author": "helloworld1",
    "state": "open",
    "created_at": "2025-05-27T23:18:13Z",
    "updated_at": "2025-06-05T20:45:37Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nIn the new and existing model implementation,check if Liger Kernel and `use_liger_kernel` flag is on is installed and provide an alternative and more efficient efficient layer implementation. This moves the monkey-patch process in Liger-Kernel to transformes library.\n\n```\nself.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps) if not is_liger_kernel_enabled() else LigerLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n```\n\n\n### Motivation\n\nTransformer is changing model implementation between releases. Liger kernel monkey patch process has ran into compatiblity issues multiple times.\nExample: https://github.com/linkedin/Liger-Kernel/issues/723\n\nI would like to make Liger Kernel as part of transformers model implementation to ensure the stability of efficient layers being used. This will also give modeller ability to use efficient kernels during modelling.\n\n### Your contribution\n\nI can help PR the change",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "This could be a good opportunity to add it to [huggingface/kernels](https://github.com/huggingface/kernels), maybe? cc @Narsil @mekkcyber"
      },
      {
        "user": "MekkCyber",
        "body": "Hi @helloworld1! Yes I totally agree, we are planning to do the same thing using the `kernels` library. I already started adding some `liger kernels` here : https://huggingface.co/kernels-community/liger_kernels"
      },
      {
        "user": "helloworld1",
        "body": "Excellent work @MekkCyber ! Currently your code copied the kernels from liger kernel. Since Liger Kernel is evolving, it would be best to depend on liger-kernel library in HF kernels. We can work on the Liger kernel side change to make it easy for HF kernels to consume the kernel / layers."
      }
    ]
  },
  {
    "issue_number": 20179,
    "title": "🌐 [i18n-KO] Translating docs to Korean",
    "author": "wonhyeongseo",
    "state": "open",
    "created_at": "2022-11-12T06:55:39Z",
    "updated_at": "2025-06-05T20:04:00Z",
    "labels": [
      "Documentation",
      "WIP"
    ],
    "body": "Hi!\r\n\r\nLet's bring the documentation to all the Korean-speaking community 🌏  (currently 9 out of 77 complete)\r\n\r\nWould you want to translate? Please follow the 🤗 [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.\r\n\r\nSome notes:\r\n\r\n* Please translate using an informal tone (imagine you are talking with a friend about transformers 🤗).\r\n* Please translate in a gender-neutral way.\r\n* Add your translations to the folder called `ko` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).\r\n* Register your translation in `ko/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).\r\n* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @ArthurZucker, @sgugger and @eunseojo for review.\r\n* 🙋 If you'd like others to help you with the translation, you can also post in the 🤗 [forums](https://discuss.huggingface.co/).\r\n* With the [HuggingFace Documentation l10n](https://pseudo-lab.com/HuggingFace-0558662add4949558f6b4c4d526547da) initiative of [Pseudo Lab](https://pseudo-lab.com/), full translation will be done even faster. 🎉 Please give us your support! Cheers to our team 👍@0525hhgus, @KIHOON71, @gabrielwithappy, @jungnerd, @sim-so, @HanNayeoniee, @wonhyeongseo\r\n\r\n안녕하세요!\r\n\r\n한국어를 사용하는 모두가 기술 문서를 읽을 수 있게 해보아요 🌏 (현재 77개 문서 중 9개 완료)\r\n\r\n번역에 참여하고 싶으신가요? 🤗 [번역 가이드](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md)를 먼저 읽어보시기 바랍니다. 끝 부분에 번역해야할 파일들이 나열되어 있습니다. 작업하고 계신 파일이 있다면 여기에 간단히 알려주세요. 중복되지 않도록 `작업중`으로 표시해둘게요.\r\n\r\n참고 사항:\r\n* 기술 문서이지만 (친구에게 설명 듣듯이) 쉽게 읽히면 좋겠습니다. __존댓말__ 로 써주시면 감사하겠습니다.\r\n* 성별은 일부 언어(스페인어, 프랑스어 등)에만 적용되는 사항으로, 한국어의 경우 번역기를 사용하신 후 문장 기호와 조사 등이 알맞는지 확인해주시기 바랍니다.\r\n* [소스 폴더](https://github.com/huggingface/transformers/tree/main/docs/source) 아래 `ko` 폴더에 번역본을 넣어주세요.\r\n* 목차(`ko/_toctree.yml`)도 함께 업데이트해주세요. [영어 목차](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml)와 순서가 동일해야 합니다.\r\n* 모두 마치셨다면, 기록이 원활하도록 PR을 여실 때 현재 이슈(`#20179`)를 내용에 넣어주시기 바랍니다. 리뷰 요청은 @ArthurZucker님, @sgugger님, @eunseojo님께 요청해주세요.\r\n*  🙋 커뮤니티에 마음껏 홍보해주시기 바랍니다! 🤗 [포럼](https://discuss.huggingface.co/)에 올리셔도 좋아요.\r\n* [가짜연구소](https://pseudo-lab.com/)의 [이니셔티브](https://pseudo-lab.com/HuggingFace-0558662add4949558f6b4c4d526547da)로 번역이 더욱 빠르게 진행될 예정입니다. 🎉 많은 응원 부탁드려요! 우리팀 화이팅 👍 \r\n  @0525hhgus, @KIHOON71, @gabrielwithappy, @jungnerd, @sim-so, @HanNayeoniee, @wonhyeongseo\r\n\r\n## GET STARTED\r\n\r\n- [x] 🤗 Transformers https://github.com/huggingface/transformers/pull/20180\r\n- [x] Quick tour https://github.com/huggingface/transformers/pull/20946\r\n- [x] Installation https://github.com/huggingface/transformers/pull/20948\r\n\r\n## TUTORIAL\r\n- [x] Pipelines for inference https://github.com/huggingface/transformers/pull/22508\r\n- [x] Load pretrained instances with an AutoClass https://github.com/huggingface/transformers/pull/22533\r\n- [x] Preprocess https://github.com/huggingface/transformers/pull/22578\r\n- [x] Fine-tune a pretrained model https://github.com/huggingface/transformers/pull/22670\r\n- [x] Train with a script https://github.com/huggingface/transformers/pull/22793\r\n- [x] Distributed training with 🤗 Accelerate https://github.com/huggingface/transformers/pull/22830\r\n- [x] Load and train adapters with 🤗 PEFT https://github.com/huggingface/transformers/pull/25706\r\n- [x] Share a model\r\n- [x] Agents https://github.com/huggingface/transformers/pull/24881\r\n- [x] Generation with LLMs https://github.com/huggingface/transformers/pull/25791\r\n\r\n## TASK GUIDES\r\n\r\n### NATURAL LANGUAGE PROCESSING\r\n- [x] Text classification https://github.com/huggingface/transformers/pull/22655\r\n- [x] Token classification https://github.com/huggingface/transformers/pull/22945\r\n- [x] Question answering\r\n- [x] Causal language modeling\r\n- [x] Masked language modeling https://github.com/huggingface/transformers/pull/22838\r\n- [x] Translation https://github.com/huggingface/transformers/pull/22805\r\n- [x] Summarization https://github.com/huggingface/transformers/pull/22783\r\n- [x] Multiple choice\r\n\r\n### AUDIO\r\n- [x] Audio classification https://github.com/huggingface/transformers/pull/26200\r\n- [x] Automatic speech recognition\r\n\r\n### COMPUTER VISION\r\n- [x] Image classification\r\n- [x] Semantic segmentation https://github.com/huggingface/transformers/pull/26515\r\n- [x] Video classification\r\n- [x] Object detection\r\n- [x] Zero-shot object detection\r\n- [x] Zero-shot image classification\r\n- [x] Depth estimation\r\n\r\n### MULTIMODAL\r\n- [x] Image captioning\r\n- [x] Document Question Answering\r\n- [x] Visual Question Answering https://github.com/huggingface/transformers/pull/25679\r\n- [ ] Text to speech\r\n\r\n### GENERATION\r\n- [ ] Customize the generation strategy\r\n\r\n## DEVELOPER GUIDES\r\n- [x] Use tokenizers from 🤗 Tokenizers https://github.com/huggingface/transformers/pull/22956\r\n- [x] Inference for multilingual models\r\n- [x] Create a custom architecture https://github.com/huggingface/transformers/pull/22754\r\n- [x] Sharing custom models https://github.com/huggingface/transformers/pull/22534\r\n- [x] Run training on Amazon SageMaker https://github.com/huggingface/transformers/pull/22509\r\n- [x] Export to ONNX https://github.com/huggingface/transformers/pull/22806\r\n- [x] Export to TFLite\r\n- [x] Export to TorchScript\r\n- [ ] Benchmarks\r\n- [ ] Notebooks with examples\r\n- [x] Community resources https://github.com/huggingface/transformers/pull/25674\r\n- [x] Custom Tools and Prompts\r\n- [x] Troubleshoot\r\n\r\n## PERFORMANCE AND SCALABILITY\r\n- [x] Overview\r\n\r\n### EFFICIENT TRAINING TECHNIQUES\r\n- [ ] Training on one GPU https://github.com/huggingface/transformers/pull/25250\r\n- [x] Training on many GPUs https://github.com/huggingface/transformers/pull/26244\r\n- [x] Training on CPU https://github.com/huggingface/transformers/pull/24911\r\n- [x] Training on many CPUs https://github.com/huggingface/transformers/pull/24923\r\n- [ ] Training on TPUs\r\n- [x] Training on TPU with TensorFlow\r\n- [ ] Training on Specialized Hardware\r\n- [x] Custom hardware for training https://github.com/huggingface/transformers/pull/24966\r\n- [x] Hyperparameter Search using Trainer API\r\n\r\n### OPTIMIZING INFERENCE\r\n- [x] Inference on CPU https://github.com/huggingface/transformers/pull/24920\r\n- [x] Inference on one GPU https://github.com/huggingface/transformers/pull/24978\r\n- [x] Inference on many GPUs https://github.com/huggingface/transformers/pull/24943\r\n- [ ] Inference on Specialized Hardware\r\n- [x] Instantiating a big model https://github.com/huggingface/transformers/pull/26245\r\n- [x] Debugging https://github.com/huggingface/transformers/pull/26246\r\n- [x] XLA Integration for TensorFlow Models https://github.com/huggingface/transformers/pull/24904\r\n- [ ] Optimize inference using `torch.compile`\r\n\r\n### CONTRIBUTE\r\n- [x] How to contribute to transformers? https://github.com/huggingface/transformers/pull/25877\r\n- [x] How to add a model to 🤗 Transformers? https://github.com/huggingface/transformers/pull/24957\r\n- [x] How to convert a 🤗 Transformers model to TensorFlow? https://github.com/huggingface/transformers/pull/25017\r\n- [x] How to add a pipeline to 🤗 Transformers? https://github.com/huggingface/transformers/pull/25498\r\n- [x] Testing https://github.com/huggingface/transformers/pull/24900\r\n- [ ] Checks on a Pull Request\r\n\r\n### CONCEPTUAL GUIDES\r\n- [x] Philosophy https://github.com/huggingface/transformers/pull/25010\r\n- [ ] Glossary\r\n- [x] What 🤗 Transformers can do\r\n- [x] How 🤗 Transformers solve tasks https://github.com/huggingface/transformers/pull/23844\r\n- [x] The Transformer model family https://github.com/huggingface/transformers/pull/24625\r\n- [x] Summary of the tokenizers https://github.com/huggingface/transformers/pull/26243\r\n- [x] Attention mechanisms\r\n- [x] Padding and truncation https://github.com/huggingface/transformers/pull/23823\r\n- [x] BERTology https://github.com/huggingface/transformers/pull/23968\r\n- [x] Perplexity of fixed-length models https://github.com/huggingface/transformers/pull/23850\r\n- [x] Pipelines for webserver inference https://github.com/huggingface/transformers/pull/24828\r\n- [x] Model training anatomy https://github.com/huggingface/transformers/pull/25755\r\n\r\n<details>\r\n<summary>\r\n\r\n## Other relevant PRs along the way\r\n\r\n</summary>\r\n\r\n- Enable easy Table of Contents editing https://github.com/huggingface/transformers/pull/22581\r\n- Added forgotten internal English anchors for `sagemaker.mdx` https://github.com/huggingface/transformers/pull/22549\r\n- Fixed anchor links for `auto_class`, `training` https://github.com/huggingface/transformers/pull/22796\r\n- Update ToC from upstream https://github.com/huggingface/transformers/pull/23112\r\n\r\n</details>",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "wonhyeongseo",
        "body": "Hello @sgugger, may you please add the `WIP` tag to this issue? Thank you so much."
      },
      {
        "user": "wonhyeongseo",
        "body": "For contributors and PseudoLab team members, please see a PR template [gist](https://gist.github.com/wonhyeongseo/af2a8855264bb494212f81e8b8173b9a) ([raw](https://gist.githubusercontent.com/wonhyeongseo/af2a8855264bb494212f81e8b8173b9a/raw/4a50fca630c4f6188cea658bdba98704d9a3e979/pr-template.md)) that could ease your first PR experience.\r\n@0525hhgus, @KIHOON71, @gabrielwithappy, @jungnerd, @sim-so, @HanNayeoniee, @wonhyeongseo"
      }
    ]
  },
  {
    "issue_number": 38271,
    "title": "Attention refactor in #35235 adds a `__getitem__` into the forward pass, which causes errors with torch dynamo.",
    "author": "i-colbert",
    "state": "open",
    "created_at": "2025-05-21T20:57:40Z",
    "updated_at": "2025-06-05T14:41:02Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.51.3\n- Platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.31\n- Python version: 3.12.8\n- Huggingface_hub version: 0.31.1\n- Safetensors version: 0.5.2\n- Accelerate version: 1.3.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.4.0+rocm6.1 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: N/A\n- Using GPU in script?: NA\n- GPU type: AMD Instinct MI210\n\n### Who can help?\n\n@ArthurZucker, error seems to be introduced with #35235 \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nYou can reproduce the error with:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", torch_dtype=torch.bfloat16)\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n\ndata = tokenizer(\"Once upon a time, \", return_tensors='pt')\ninputs = {\n    'input_ids': data.input_ids,\n    'attention_mask': torch.ones_like(data.input_ids)}\nmodel.config.use_cache = False\nwith torch.no_grad():\n    model(**inputs)  # verify forward pass works\n    model, guards = torch._dynamo.export(model)(**inputs)\n    model(**inputs)  # verify forward pass works\n```\n\nand the error received looks like\n```shell\nTraceback (most recent call last):\n  File \"/home/repro.py\", line 17, in <module>\n    model, guards = torch._dynamo.export(model)(**inputs)\n  # ... removed for brevity...\ntorch._dynamo.exc.Unsupported: call_method GetAttrVariable(UserDefinedObjectVariable(AttentionInterface), _global_mapping) __getitem__ (ConstantVariable(),) {}\n\nfrom user code:\n  # ...removed for brevity... \n  File \"/site-packages/transformers/models/llama/modeling_llama.py\", line 274, in forward\n    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n  File \"/site-packages/transformers/modeling_utils.py\", line 5841, in __getitem__\n    return self._global_mapping[key]\n```\n\n### Expected behavior\n\nExpected behavior is no errors during the dynamo export.\n\nA possible fix is\n\n```python\ndef forward(self, ...):\n  # ...ignored for brevity...\n  if self.attention_interface is None:\n      self.attention_interface: Callable = eager_attention_forward\n      if self.config._attn_implementation != \"eager\":\n          if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n              logger.warning_once(\n                  \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n                  'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n              )\n          else:\n              self.attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n\n  attn_output, attn_weights = self.attention_interface(\n      self,\n      query_states,\n      key_states,\n      value_states,\n      attention_mask,\n      dropout=0.0 if not self.training else self.attention_dropout,\n      scaling=self.scaling,\n      **kwargs,\n  )\n```\n\nwhere `self.attention_interface = None` in the init",
    "comments": [
      {
        "user": "ArthurZucker",
        "body": "cc @Cyrilvallez we need to either register the op, or we clarify `torch.export` vs `torch._dynamo.export` but not super super important IMO @i-colbert as `torch._dynamo` is private"
      },
      {
        "user": "i-colbert",
        "body": "Thanks for the reply. This is infrastructure that we leverage to get modular fx graphs within Brevitas. I'll loop in @Giuseppe5 and @nickfraser for further comment as it was suggested by the PyTorch folks to use this."
      }
    ]
  },
  {
    "issue_number": 38559,
    "title": "Possible Typo in \"Mask2FormerLoss\"",
    "author": "NiccoloCavagnero",
    "state": "open",
    "created_at": "2025-06-03T12:27:08Z",
    "updated_at": "2025-06-05T13:36:57Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nNone\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nNone\n\n### Expected behavior\n\nHi everyone, \n\nI think there might be a typo within the **Mask2FormerLoss** class in **modeling_mask2former.py**, line **506**.\nIn particular, when the matcher is defined, the **class_weight** is not passed to the matcher, where there is an hardcoded **1.0**.\n\nCurrent definition:\n```\nself.matcher = Mask2FormerHungarianMatcher(\n            cost_class=1.0,\n            cost_dice=config.dice_weight,\n            cost_mask=config.mask_weight,\n            num_points=self.num_points,\n        )\n```\n\nIt should be changed to:\n```\nself.matcher = Mask2FormerHungarianMatcher(\n            cost_class=config.class_weight,\n            cost_dice=config.dice_weight,\n            cost_mask=config.mask_weight,\n            num_points=self.num_points,\n        )\n```\n\nBest, \nNiccolò Cavagnero",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "@NielsRogge you reviewed the original PR, can you take a look?"
      },
      {
        "user": "NielsRogge",
        "body": "Hi @NiccoloCavagnero thanks for spotting this, feel free to submit a PR!"
      },
      {
        "user": "yaswanth19",
        "body": "@NielsRogge @NiccoloCavagnero , I can fix that in EoMT PR and we can then directly import the `Mask2FormerLoss`"
      }
    ]
  },
  {
    "issue_number": 38602,
    "title": "Failed to full fine tuning code5p 2B",
    "author": "latuan1",
    "state": "open",
    "created_at": "2025-06-05T04:34:17Z",
    "updated_at": "2025-06-05T13:20:15Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nI got assertion error while saving CodeT5+ 2B:\nFile \"/root/anaconda3/envs/train-model/lib/python3.11/site-packages/transformers/trainer.py\", line 2245, in train\nreturn inner_training_loop(\n^^^^^^^^^^^^^^^^^^^^\nFile \"/root/anaconda3/envs/train-model/lib/python3.11/site-packages/transformers/trainer.py\", line 2627, in _inner_training_loop\nself._maybe_log_save_evaluate(\nFile \"/root/anaconda3/envs/train-model/lib/python3.11/site-packages/transformers/trainer.py\", line 3103, in _maybe_log_save_evaluate\nself._save_checkpoint(model, trial)\nFile \"/root/anaconda3/envs/train-model/lib/python3.11/site-packages/transformers/trainer.py\", line 3200, in _save_checkpoint\nself.save_model(output_dir, _internal_call=True)\nFile \"/root/anaconda3/envs/train-model/lib/python3.11/site-packages/transformers/trainer.py\", line 3902, in save_model\nself._save(output_dir)\nFile \"/root/anaconda3/envs/train-model/lib/python3.11/site-packages/transformers/trainer.py\", line 4006, in _save\nself.model.save_pretrained(\nFile \"/root/anaconda3/envs/train-model/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3337, in save_pretrained\nmisplaced_generation_parameters = model_to_save.config._get_non_default_generation_parameters()\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/root/anaconda3/envs/train-model/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 1079, in _get_non_default_generation_parameters\ndefault_config = self.class()\n^^^^^^^^^^^^^^^^\nFile \"/root/.cache/huggingface/modules/transformers_modules/Salesforce/codet5p-2b/0083d4d638746e6c9ee3dbd504e6dd68738e3c87/configuration_codet5p.py\", line 78, in init\n\"encoder\" in kwargs and \"decoder\" in kwargs\nAssertionError: Config has to be initialized with encoder and decoder config.\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nhttps://www.kaggle.com/code/latuan1st/testllm\n\n### Expected behavior\n\nThis checkpoint will be saved",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @latuan1, I confirmed the bug but this is a custom code model, which means the cause is likely some interaction between their outdated code and modern transformers.\n\nTo help us resolve this, can you try older versions of `transformers` to see if it works there? If so, a `git bisect` to identify the exact commit causing the issue would be very helpful!"
      },
      {
        "user": "latuan1",
        "body": "@Rocketknight1 Thank you"
      }
    ]
  },
  {
    "issue_number": 38600,
    "title": "MambaInnerFnBackward",
    "author": "HN-BB",
    "state": "open",
    "created_at": "2025-06-05T02:17:03Z",
    "updated_at": "2025-06-05T12:33:35Z",
    "labels": [],
    "body": "Hello, I'm using MambaConfig and MambaMixer from transformers.\n\n`from transformers.models.mamba.configuration_mamba import MambaConfig\nfrom transformers.models.mamba.modeling_mamba import MambaMixer`\n\nWhen I train my model with mamba, the error shown below occurs.\n`  File \"/root/.cache/pypoetry/virtualenvs/reverie-retrieval-VsnhxLU2-py3.8/lib/python3.8/site-packages/torch/_tensor.py\", line 522, in backward\n    torch.autograd.backward(\n  File \"/root/.cache/pypoetry/virtualenvs/reverie-retrieval-VsnhxLU2-py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 266, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: Function MambaInnerFnBackward returned an invalid gradient at index 6 - got [] but expected shape compatible with [1024]`\n\nI applyed #37 to transformers/kernels/falcon_mamba/selective_scan_with_ln_interface.py because it has MambaInnerFnBackward class.\n`dB_proj_bias = dB if not ctx.B_proj_bias_is_None else None`\n`dC_proj_bias = dC if not ctx.C_proj_bias_is_None else None`\n`dout_proj_bias = dout.sum(dim=(1)) if not ctx.out_proj_bias_is_None else None`\n\nHowever, I have the same error.",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hmmn, we'll need a reproducer script to figure this one out! It seems like the forward pass ran correctly but the backward is returning an empty tensor instead of the gradients, which means this bug probably won't be too easy to resolve"
      }
    ]
  },
  {
    "issue_number": 38538,
    "title": "Hidden states are different for model() and model.generate()",
    "author": "okihnjo",
    "state": "closed",
    "created_at": "2025-06-02T21:32:00Z",
    "updated_at": "2025-06-05T11:39:09Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nModel: Qwen/Qwen2.5-0.5B-Instruct, BitsAndBytesConfig(load_in_8bit=True)\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nWhen doing model.generate() and accessing hidden_states, I access the first element, which consists again of the number of layers.\nAccessing an element again gives something like (batch_size, tokens, embedding_size) -> similar to model() when accessing the hidden_states.\n\nI always thought that\n ```python\ngenerated_hidden_states[0][layer][:,-1,:],\nhidden_states[layer][:,-1,:],\n```\nshould be equal, both have the same shape and I thought both represent the same thing, but when looking at the cosine similarity between these vectors, I get different results\n\n![Image](https://github.com/user-attachments/assets/ac7221ca-a5d0-4e19-aa07-f6fec65add81)\n\nDoes model.generate() do stuff that is not present in model() or vice versa ? \n\nFor reproducing: \n```python\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\ndef load_model(model_name):\n    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        \n    )\n    model.config.use_cache = False\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n\n  \n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n    return model, tokenizer\n\nmodel, tokenizer = load_model(model_name)\nmodel_inputs = tokenizer(\n    [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n).to(\"cuda\")\noutput = model(**model_inputs, output_hidden_states=True)\nmy_hidden_states = output.hidden_states\ngenerated_ids = model.generate(**model_inputs, output_hidden_states=True,return_dict_in_generate=True, max_new_tokens=13, output_scores=True)\nF.cosine_similarity(my_hidden_states[-1][0,-1,:], generated_ids[\"hidden_states\"][0][-1][0,-1,:], dim=0)\n# results in different values\n\n### Expected behavior\n\n\nShould have equal values.",
    "comments": [
      {
        "user": "akshathmangudi",
        "body": "Hi @okihnjo , I hope you're doing good. \n\nI took some time to reproduce your script and verify few things. On a architectural note, calling `model()` and `model.generate()` _do_ mean different things. Calling `model` passes over the entire input sequence in one forward pass while `model.generate()` performs incremental decoding, so it computes a forward pass, per generated token. \n\nI also took some time to run your reproduction script, and the discrepancy could be quantization-induced. This was the output I got when I ran your reproduction script: \n```python\ntensor(0.9980, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward1>)\n```\n\n0.9980 is very very close to 1. To make sure, I ran it with two more tests, one without quantization and one with Mistral/7B-Instruct-v0.1, these were the results I got respectively: \n\nOutput for **test 1**: \n```python\nCosine similarity (no quant): 1.000000\n```\n\nOutput for **test 2**: \n```python\ntensor(0.9985, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward1>)\n```\n\nThis discrepancy doesn't seem to be limited to just the Qwen family, but other models too. \n\nCodes I used for non-quantized version: \n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport torch.nn.functional as F\nfrom huggingface_hub import login\nimport os\n\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\ndef load_model_no_q(model_name):\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float32,\n        device_map=\"auto\"\n    )\n    model.config.use_cache = False\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    return model, tokenizer\n\nmodel, tokenizer = load_model_no_q(model_name)\n\nmodel_inputs = tokenizer(\n    [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n).to(model.device)\n\nwith torch.no_grad():\n    output = model(**model_inputs, output_hidden_states=True)\n    my_hidden_states = output.hidden_states\n\n    generated_ids = model.generate(\n        **model_inputs,\n        output_hidden_states=True,\n        return_dict_in_generate=True,\n        max_new_tokens=13,\n        output_scores=True\n    )\n\ncos_sim = F.cosine_similarity(\n    my_hidden_states[-1][0, -1, :],\n    generated_ids[\"hidden_states\"][0][-1][0, -1, :],\n    dim=0\n)\n\nprint(f\"Cosine similarity (no quant): {cos_sim.item():.6f}\")\n```\n\nIf you think I have misunderstood your problem, kindly let me know :) "
      },
      {
        "user": "okihnjo",
        "body": "Hey, I am good, hope you are doing well too!\n\nThanks for your response! When running this for a lot of queries, the cosine similarity tends to become worse (still in 0.9x region though) - I think I generally struggle with for activation/embedding analysis, I am not quite sure which method to call.\nI know that ``model.generate()`` is for token generation, however I thought the first element represents the same as when doing simply ``model()``. I want my model to do open text generation, but I want to analyze the query embedding (not the generated tokens but basically the last token of my input to the model), would you recommend doing one ``model()``call for hidden_states analysis and one additional ``model.generate()``call in order to get the text ? So two calls in one batch loop so to say?"
      },
      {
        "user": "akshathmangudi",
        "body": "Thanks for your response!\n\nThat seems to make the most sense. For your use-case where you want to analyze the last token of your input to the model, it would be wise to: \n1. first call `model()` with `output_hidden_states=True` to extract the final hidden state of the prompt. \n2. Then we pass `input_ids`, `attention_mask`to `model.generate()` for actual text output. \n\nSo, step 1 for analysis and step 2 for generation. \n\nI have provided some code which may help you: \n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch.nn.functional as F\n\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\nmodel.config.use_cache = False\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\ninputs = tokenizer([\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True).to(model.device)\n\n\nwith torch.no_grad():\n    output = model(**inputs, output_hidden_states=True)\n    hidden_states = output.hidden_states\n    last_hidden = hidden_states[-1]\n\n\n    input_lengths = (inputs['attention_mask'] != 0).sum(dim=1)\n    last_token_hiddens = torch.stack([\n        last_hidden[i, input_lengths[i] - 1, :] for i in range(len(input_lengths))\n    ])\n\nwith torch.no_grad():\n    gen_output = model.generate(\n        **inputs,\n        return_dict_in_generate=True,\n        output_scores=True,\n        max_new_tokens=20\n    )\n    generated_texts = tokenizer.batch_decode(gen_output.sequences, skip_special_tokens=True)\n\ncos_sim = F.cosine_similarity(last_token_hiddens[0], last_token_hiddens[1], dim=0)\nprint(f\"Cosine similarity between inputs: {cos_sim.item():.6f}\")\n```\n\nWhere the output for this will be: \n```python\nCosine similarity between inputs: -0.142544\n```\n\nThis actually makes sense because they both represent very different contexts semantically. So yes, running two calls where we do `model()` for analysis and `model.generate()` for generation will decouple the two tasks cleanly. "
      }
    ]
  },
  {
    "issue_number": 35308,
    "title": "Multi-GPU training crashes with IterableDataset and different length input (e.g. Next token prediction)",
    "author": "avishaiElmakies",
    "state": "open",
    "created_at": "2024-12-17T11:03:19Z",
    "updated_at": "2025-06-05T09:23:46Z",
    "labels": [
      "Good Second Issue",
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.46.3\r\n- Platform: Linux-6.6.20-aufs-1-x86_64-with-glibc2.36\r\n- Python version: 3.11.2\r\n- Huggingface_hub version: 0.26.1\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: yes, DDP. multi-gpu\r\n- Using GPU in script?: yes\r\n- GPU type: NVIDIA RTX A5000\n\n### Who can help?\n\n@muellerz @SunMarc \n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nI am using Trainer with hf `IterableDataset`. using multi-gpu training with at least 2 gpus.\r\nI found out that i need `--accelerator_config {\"split_batches\":true}` for it work with varying length input (e.g. NTP prediction). I got this from https://github.com/huggingface/transformers/issues/26548\r\n\r\nI am still having crashes in my case.\r\nfrom what I understand the problem arises because my dataset has a reminder when considering the batch size.\r\nI am getting the following stacktrace:\r\n\r\n```\r\nrank1]: Traceback (most recent call last):\r\n[rank1]:   File \"/cs/labs/oabend/avishai.elma/src/train_multimodal_multistream.py\", line 249, in <module>\r\n[rank1]:     main()\r\n[rank1]:   File \"/cs/labs/oabend/avishai.elma/src/train_multimodal_multistream.py\", line 235, in main\r\n[rank1]:     trainer.train()\r\n[rank1]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py\", line 2123, in train\r\n[rank1]:     return inner_training_loop(\r\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py\", line 2427, in _inner_training_loop\r\n[rank1]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)\r\n[rank1]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py\", line 5045, in get_batch_samples\r\n[rank1]:     batch_samples += [next(epoch_iterator)]\r\n[rank1]:                       ^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/data_loader.py\", line 842, in __iter__\r\n[rank1]:     batch = concatenate([batch, first_batch], dim=0)\r\n[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 621, in concatenate\r\n[rank1]:     return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})\r\n[rank1]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 621, in <dictcomp>\r\n[rank1]:     return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})\r\n[rank1]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 624, in concatenate\r\n[rank1]:     return torch.cat(data, dim=dim)\r\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank1]: RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 1008 but got size 1000 for tensor number 1 in the list.\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/cs/labs/oabend/avishai.elma/src/train_multimodal_multistream.py\", line 249, in <module>\r\n[rank0]:     main()\r\n[rank0]:   File \"/cs/labs/oabend/avishai.elma/src/train_multimodal_multistream.py\", line 235, in main\r\n[rank0]:     trainer.train()\r\n[rank0]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py\", line 2123, in train\r\n[rank0]:     return inner_training_loop(\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py\", line 2427, in _inner_training_loop\r\n[rank0]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)\r\n[rank0]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/transformers/trainer.py\", line 5045, in get_batch_samples\r\n[rank0]:     batch_samples += [next(epoch_iterator)]\r\n[rank0]:                       ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/data_loader.py\", line 842, in __iter__\r\n[rank0]:     batch = concatenate([batch, first_batch], dim=0)\r\n[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 621, in concatenate\r\n[rank0]:     return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})\r\n[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 621, in <dictcomp>\r\n[rank0]:     return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})\r\n[rank0]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 624, in concatenate\r\n[rank0]:     return torch.cat(data, dim=dim)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 1008 but got size 1000 for tensor number 1 in the list.\r\n  0%|                                                                                                                                                                                                                  | 3/43040 [01:02<248:43:41, 20.81s/it]\r\n[rank0]:[W1217 12:41:08.453747481 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\r\nW1217 12:41:24.654000 1980229 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1980456 closing signal SIGTERM\r\nE1217 12:41:24.771000 1980229 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 1980457) of binary: /cs/labs/oabend/avishai.elma/.lab_env_v3/bin/python\r\nTraceback (most recent call last):\r\n  File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\r\n    return f(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\r\n    run(args)\r\n  File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\r\n    elastic_launch(\r\n  File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/.lab_env_v3/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n============================================================\r\ntrain_multimodal_multistream.py FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2024-12-17_12:41:24\r\n  host      : binky-03.cs.huji.ac.il\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : 1 (pid: 1980457)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n```\r\n\r\nfrom what i understand this is because of those lines\r\nhttps://github.com/huggingface/accelerate/blob/200c9eb7833cfa505907f6f224ebf5a275aa6d92/src/accelerate/data_loader.py#L840-L844\r\n\r\nso i noticed that droping the remeinder might fix the issue. but it didn't help. this is probably because dataloader_drop_last doesn't get used when using `IterableDataset` \r\n\r\nhttps://github.com/huggingface/transformers/blob/5d7739f15a6e50de416977fe2cc9cb516d67edda/src/transformers/trainer.py#L997-L1009\r\n\r\n\n\n### Expected behavior\n\nMulti-gpu training should not crash when using iterable dataset",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @muellerzr @SunMarc "
      },
      {
        "user": "SunMarc",
        "body": "Can you try to choose a batch_size that doesn't end up with a  a remainder with your dataset to make sure that this is the actual issue ? "
      },
      {
        "user": "avishaiElmakies",
        "body": "I have tried it with dataset that doesn't have reminder (by using take()). It seemed to have worked fine. "
      }
    ]
  },
  {
    "issue_number": 35838,
    "title": "forward() got an unexpected keyword argument 'num_items_in_batch'",
    "author": "Bachstelze",
    "state": "closed",
    "created_at": "2025-01-22T12:09:35Z",
    "updated_at": "2025-06-05T09:02:21Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nNew versions can't train encoder-decoder models.\nRelated issue and pull request: https://github.com/huggingface/transformers/issues/34575\nSystem-Info:\n- `transformers` version: 4.48.1\n- Platform: Linux-6.8.0-36-generic-x86_64-with-glibc2.39\n- Python version: 3.12.8\n- Huggingface_hub version: 0.24.6\n- Safetensors version: 0.4.5\n- Accelerate version: 1.2.1\n- Accelerate config:    not found\n- PyTorch version (GPU?): 2.4.1+cu121 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: No\n- Using GPU in script?: Yes\n- GPU type: Tesla V100-PCIE-32GB\n\n\n```\nTraceback (most recent call last):\n  File \"/home/hilsenbek/workspace/thesis/syntax_transformer/training/train_cross_attention.py\", line 110, in <module>\n    trainer.train()\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/transformers/trainer.py\", line 2171, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/transformers/trainer.py\", line 2531, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/transformers/trainer.py\", line 3675, in training_step\n    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/transformers/trainer.py\", line 3731, in compute_loss\n    outputs = model(**inputs)\n              ^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 433, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 823, in forward\n    return model_forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 811, in __call__\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py\", line 603, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hilsenbek/.conda/envs/harness/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: RobertaModel.forward() got an unexpected keyword argument 'num_items_in_batch'\n```\n\n### Who can help?\n\n@ArthurZucker \n@gheinrich\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nfollow the blog https://huggingface.co/blog/encoder-decoder\n\n### Expected behavior\n\nWork as in old transformer versions",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "This seems related to the `trainer` changes - cc @muellerzr @SunMarc "
      },
      {
        "user": "shubhamjain0594",
        "body": "Getting same error for the Gemma Model."
      },
      {
        "user": "SilverSoldier",
        "body": "Same for bloom which is marking unexpected arguments as deprecated and throws `ValueError: Got unexpected arguments: {'num_items_in_batch': 5120}`.\n\nSeems to be these [3 lines](https://github.com/huggingface/transformers/blob/2c3a44f9a769e98597d62ecdc7383785318be5a2/src/transformers/trainer.py#L3744) causing the problem:\n```\nloss_kwargs[\"num_items_in_batch\"] = num_items_in_batch\ninputs = {**inputs, **loss_kwargs}\noutputs = model(**inputs)\n```"
      }
    ]
  },
  {
    "issue_number": 37818,
    "title": "Return type is not `List[...]`?",
    "author": "ChengLyu",
    "state": "closed",
    "created_at": "2025-04-28T00:11:06Z",
    "updated_at": "2025-06-05T08:02:26Z",
    "labels": [],
    "body": "Trying to understand this line: https://github.com/huggingface/transformers/blob/main/src/transformers/cache_utils.py#L385\n\nThe return type is a `Tuple`, which contradicts line 379 claiming the return type is a `List`?",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @ChengLyu, that type hint is probably wrong! Feel free to open a PR to fix it."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37820,
    "title": "When will transformers==4.51.4 be released?",
    "author": "FightingZhen",
    "state": "closed",
    "created_at": "2025-04-28T06:57:27Z",
    "updated_at": "2025-06-05T08:02:24Z",
    "labels": [],
    "body": "As title, when will transformers==4.51.4 be released? A static version is required :)",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 38590,
    "title": "\"facebook/opt-125m\" gives wrong results",
    "author": "BoyuanFeng",
    "state": "closed",
    "created_at": "2025-06-04T18:15:07Z",
    "updated_at": "2025-06-05T03:32:19Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.3\n- Platform: Linux-6.4.3-0_fbk15_hardened_2630_gf27365f948db-x86_64-with-glibc2.34\n- Python version: 3.10.16\n- Huggingface_hub version: 0.32.2\n- Safetensors version: 0.5.3\n- Accelerate version: not installed\n- Accelerate config: not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.8.0a0+gitef4d573 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA H100\n\n### Who can help?\n\n@Rocketknight1\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nWe will get the following wrong output when we run the same code. This wrong prediction happens on all prompts I have tried.\n\n![Image](https://github.com/user-attachments/assets/cdb486bb-436c-4f4d-96eb-5eb51895dff5)\n\n\n### Expected behavior\n\nAccording to [doc](https://huggingface.co/facebook/opt-125m), we expect:\n\n<img width=\"722\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/12e36dad-3b06-4e1a-b13e-1f5d283d7a6d\" />",
    "comments": [
      {
        "user": "BoyuanFeng",
        "body": "Update: the issue does not happen on pytorch-2.7.0 stable. it happens only on pytorch nightly\n\n![Image](https://github.com/user-attachments/assets/6199eeb2-c188-43d6-85b1-b682977d2e5e)"
      },
      {
        "user": "ydshieh",
        "body": "Hi @BoyuanFeng In this case, opening an issue on torch github would make sense :-) as this looks like some regression on torch?\n\n"
      },
      {
        "user": "BoyuanFeng",
        "body": "the issue comes from transformers==4.52.3. It disappeared with transformers==4.52.4. It's actually not related to pytorch version."
      }
    ]
  },
  {
    "issue_number": 38189,
    "title": "Qwen2.5-VL using ascend NPU with flash-attention-2 raises error",
    "author": "llan-ml",
    "state": "closed",
    "created_at": "2025-05-18T06:49:25Z",
    "updated_at": "2025-06-04T15:54:31Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.0.dev0\n- Platform: Linux-4.19.90-vhulk2211.3.0.h1543.eulerosv2r10.aarch64-aarch64-with-glibc2.31\n- Python version: 3.10.5\n- Huggingface_hub version: 0.30.2\n- Safetensors version: 0.5.3\n- Accelerate version: 1.6.0\n- Accelerate config:    not found\n- DeepSpeed version: 0.16.7\n- PyTorch version (GPU?): 2.3.1 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using NPU in script?: <fill in>\n- NPU type: Ascend910B4\n- CANN version: 8.0.0\n\n### Who can help?\n\n@FightingZhen \n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nreproduction script:\n```\nimport os\n\nos.environ[\"NPU_VISIBLE_DEVICES\"]=\"0\"\nos.environ[\"ASCEND_RT_VISIBLE_DEVICES\"]=\"0\"\n\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"/cache/Qwen2.5-VL-7B-Instruct/\", torch_dtype=\"auto\", device_map=\"auto\", attn_implementation=\"flash_attention_2\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\nmin_pixels = 256*28*28\nmax_pixels = 1280*28*28\nprocessor = AutoProcessor.from_pretrained(\"/cache/Qwen2.5-VL-7B-Instruct/\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///home/ma-user/work/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"npu\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\n```\n\nlog:\n```\n/usr/local/python3.10.5/lib/python3.10/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.\n  warnings.warn(f\"Warning: The {path} owner does not match the current user.\")\n/usr/local/python3.10.5/lib/python3.10/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.0/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.\n  warnings.warn(f\"Warning: The {path} owner does not match the current user.\")\n[W compiler_depend.ts:615] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.03it/s]\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n[W compiler_depend.ts:51] Warning: CAUTION: The operator 'aten::isin.Tensor_Tensor_out' is not currently supported on the NPU backend and will fall back to run on the CPU. This may have performance implications. (function npu_cpu_fallback)\nTraceback (most recent call last):\n  File \"/home/ma-user/work/test_qwen25vl.py\", line 59, in <module>\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/ma-user/work/transformers/src/transformers/generation/utils.py\", line 2592, in generate\n    result = self._sample(\n  File \"/home/ma-user/work/transformers/src/transformers/generation/utils.py\", line 3552, in _sample\n    outputs = self(**model_inputs, return_dict=True)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ma-user/work/transformers/src/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/home/ma-user/work/transformers/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1907, in forward\n    outputs = self.model(\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ma-user/work/transformers/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1660, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n  File \"/home/ma-user/work/transformers/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1613, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ma-user/work/transformers/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 530, in forward\n    hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ma-user/work/transformers/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 341, in forward\n    hidden_states = hidden_states + self.attn(\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ma-user/work/transformers/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 189, in forward\n    q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)\n  File \"/home/ma-user/work/transformers/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 156, in apply_rotary_pos_emb_flashatt\n    q_embed = apply_rotary_emb(q.float(), cos.float(), sin.float()).type_as(q)\n  File \"/usr/local/python3.10.5/lib/python3.10/site-packages/torch/_ops.py\", line 854, in __call__\n    return self_._op(*args, **(kwargs or {}))\nRuntimeError: The size of tensor a (40) must match the size of tensor b (80) at non-singleton dimension 3\n[ERROR] 2025-05-18-14:48:37 (PID:123684, Device:0, RankID:-1) ERR99999 UNKNOWN application exception\n```\n\n### Expected behavior\n\nNo error",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @ivarflakstad (sorry)"
      },
      {
        "user": "FightingZhen",
        "body": "@llan-ml API `apply_rotary_emb` on Ascend NPU does not support the situation when q shape not equal to cos/sin shape, you can use `cos.unsqueeze(0).unsqueeze(2).float()` instead, same operation on tensor `sin`, please try it. I also plan to commit this code into `main` branch soon."
      },
      {
        "user": "llan-ml",
        "body": "@FightingZhen `RuntimeError: The size of tensor a (40) must match the size of tensor b (80) at non-singleton dimension 3`\nAccording to the raised error, besides to the shapes of `q` and `cos`, the hidden size dims are also different (40 vs 80)."
      }
    ]
  },
  {
    "issue_number": 29786,
    "title": "Community contribution: enabling `device_map=\"auto\"` support for more vision and multimodal models",
    "author": "amyeroberts",
    "state": "closed",
    "created_at": "2024-03-21T17:13:59Z",
    "updated_at": "2025-06-04T15:42:41Z",
    "labels": [
      "Good Second Issue"
    ],
    "body": "### Feature request\r\n\r\n# Feature Request \r\n\r\n`transformers` models can be easily loaded across multiple devices using `device_map=\"auto\"`. This will automatically allocate weights across available devices e.g. GPUs and offload any weights onto CPU, then disk as necessary. This is useful when doing inference with large models. \r\n\r\nTo enable this, `_no_split_modules` has to be defined in the model's pretrained model class e.g. [like here for LLaMa](https://github.com/huggingface/transformers/blob/fadb053379b3ef24c4ec8e6d7d58555af21f58db/src/transformers/models/llama/modeling_llama.py#L793). This defines layers which should not be split across devices, and should contain as few layers as possible.\r\n\r\n### Steps to add\r\n* Pick a model to work on and open a PR - comment on this issue to say which model you're working on\r\n* Define `_no_split_modules` in the PreTrainedModel subclass. Try with `_no_split_modules = []` first\r\n* Enable testing\r\n    * Ensure the following tests are not skipped for the model: `test_disk_offload_bin`, `test_disk_offload_safetensors`, `test_cpu_offload`, `test_model_parallelism`, `test_model_parallel_beam_search`\r\n    * Run the tests in a multi-gpu environment `pytest tests/models/{MODEL_NAME}/test_modeling_{MODEL_NAME}.py -vv -k \"offload or parallelism\"`\r\n\r\n## Models\r\n- [ ] [Align](https://github.com/huggingface/transformers/blob/main/src/transformers/models/align/modeling_align.py)\r\n- [ ] [Altclip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py)\r\n- [x] [Beit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/beit/modeling_beit.py) #30379\r\n- [ ] [Bit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bit/modeling_bit.py)\r\n- [ ] [Blip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/modeling_blip.py)\r\n- [ ] [Chinese_clip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/chinese_clip/modeling_chinese_clip.py)\r\n- [x] [Convnext](https://github.com/huggingface/transformers/blob/main/src/transformers/models/convnext/modeling_convnext.py) #30207\r\n- [x] [Convnextv2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py) #30207\r\n- [x] [Cvt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/cvt/modeling_cvt.py) #30207\r\n- [ ] [Data2vec](https://github.com/huggingface/transformers/blob/main/src/transformers/models/data2vec/modeling_data2vec_vision.py)\r\n- [ ] [Depth_anything](https://github.com/huggingface/transformers/blob/main/src/transformers/models/depth_anything/modeling_depth_anything.py)\r\n- [ ] [Dinat](https://github.com/huggingface/transformers/blob/main/src/transformers/models/dinat/modeling_dinat.py)\r\n- [ ] [Dinov2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/dinov2/modeling_dinov2.py)\r\n- [ ] [Donut](https://github.com/huggingface/transformers/blob/main/src/transformers/models/donut/modeling_donut_swin.py)\r\n- [ ] [Dpt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpt/modeling_dpt.py)\r\n- [ ] [Efficientformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/efficientformer/modeling_efficientformer.py)\r\n- [x] [Efficientnet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/efficientnet/modeling_efficientnet.py) #29989\r\n- [ ] [Flava](https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py)\r\n- [ ] [Focalnet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/focalnet/modeling_focalnet.py) #30207\r\n- [ ] [Git](https://github.com/huggingface/transformers/blob/main/src/transformers/models/git/modeling_git.py)\r\n- [x] [Glpn](https://github.com/huggingface/transformers/blob/main/src/transformers/models/glpn/modeling_glpn.py) #30207\r\n- [ ] [Groupvit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/groupvit/modeling_groupvit.py)\r\n- [x] [Imagegpt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/imagegpt/modeling_imagegpt.py) #30207\r\n- [ ] [Layoutlmv3](https://github.com/huggingface/transformers/blob/main/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py)\r\n- [x] [Levit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/levit/modeling_levit.py) #30207\r\n- [ ] [Mask2former](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mask2former/modeling_mask2former.py)\r\n- [ ] [Maskformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/maskformer/modeling_maskformer.py)\r\n- [ ] [Maskformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/maskformer/modeling_maskformer_swin.py)\r\n- [x] [Mgp_str](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mgp_str/modeling_mgp_str.py) #30207\r\n- [x] [Mobilenet_v1](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py) #30207\r\n- [x] [Mobilenet_v2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py) #30207\r\n- [x] [Mobilevit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mobilevit/modeling_mobilevit.py) #30207\r\n- [ ] [Mobilevitv2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py)\r\n- [ ] [Nat](https://github.com/huggingface/transformers/blob/main/src/transformers/models/nat/modeling_nat.py)\r\n- [ ] [Oneformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/oneformer/modeling_oneformer.py)\r\n- [ ] [Perceiver](https://github.com/huggingface/transformers/blob/main/src/transformers/models/perceiver/modeling_perceiver.py)\r\n- [x] [Poolformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/poolformer/modeling_poolformer.py) #30207\r\n- [ ] [Pvt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/pvt/modeling_pvt.py)\r\n- [x] [Regnet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/regnet/modeling_regnet.py) #30207\r\n- [ ] [Resnet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/resnet/modeling_resnet.py) #30207\r\n- [x] [Sam](https://github.com/huggingface/transformers/blob/main/src/transformers/models/sam/modeling_sam.py) #30207\r\n- [ ] [Segformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/segformer/modeling_segformer.py)\r\n- [x] [Swiftformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swiftformer/modeling_swiftformer.py) #30207\r\n- [x] [Swin](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swin/modeling_swin.py) #30207\r\n- [ ] [Swin2sr](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swin2sr/modeling_swin2sr.py)\r\n- [x] [Swinv2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/swinv2/modeling_swinv2.py) #30207\r\n- [ ] [Timesformer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/timesformer/modeling_timesformer.py)\r\n- [ ] [Timm_backbone](https://github.com/huggingface/transformers/blob/main/src/transformers/models/timm_backbone/modeling_timm_backbone.py)\r\n- [x] [Trocr](https://github.com/huggingface/transformers/blob/main/src/transformers/models/trocr/modeling_trocr.py) #30207\r\n- [ ] [Tvlt](https://github.com/huggingface/transformers/blob/main/src/transformers/models/tvlt/modeling_tvlt.py)\r\n- [ ] [Tvp](https://github.com/huggingface/transformers/blob/main/src/transformers/models/tvp/modeling_tvp.py)\r\n- [x] [Upernet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/upernet/modeling_upernet.py) #30207\r\n- [ ] [Videomae](https://github.com/huggingface/transformers/blob/main/src/transformers/models/videomae/modeling_videomae.py)\r\n- [ ] [Vit_mae](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit_mae/modeling_vit_mae.py)\r\n- [ ] [Vit_msn](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit_msn/modeling_vit_msn.py)\r\n- [x] [Vitmatte](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vitmatte/modeling_vitmatte.py) #30379\r\n- [x] [Vivit](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vivit/modeling_vivit.py) #30379\r\n- [ ] [X_clip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/x_clip/modeling_x_clip.py)\r\n- [x] [Yolos](https://github.com/huggingface/transformers/blob/main/src/transformers/models/yolos/modeling_yolos.py) #30207\r\n\r\n### Motivation\r\n\r\nEnable a powerful HF feature for all of our vision models\r\n\r\n### Your contribution\r\n\r\nPing me for review 🤗 ",
    "comments": [
      {
        "user": "jla524",
        "body": "I'm working on [Resnet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/resnet/modeling_resnet.py)\r\n\r\nedit: I'm running into a strange issue, where the tests would pass on one system and fail on another. I'm going to close to PR for now and investigate further."
      },
      {
        "user": "tnnandi",
        "body": "BERT is not included in the above list of models. Does it mean that \"device_map='auto'\" is available for BERT models in any upcoming version of HF transformers? I still see the message BertForSequenceClassification does not support `device_map='auto'` with transformers 4.39.3."
      },
      {
        "user": "amyeroberts",
        "body": "Hi @tnnandi, the list above is just for vision models that I got from a simple grep and filtering. `device_map=\"auto\"` isn't yet enabled for BERT, c.f. #25296. If you or anyone in the community would like to add it, we'd be happy to review a PR."
      }
    ]
  },
  {
    "issue_number": 35739,
    "title": "Audio-Classification pipeline function_to_apply ignores initialized values (possibly generalizes to other classification pipelines)",
    "author": "wilke0818",
    "state": "closed",
    "created_at": "2025-01-16T20:27:00Z",
    "updated_at": "2025-06-04T15:28:08Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.48.0\n- Platform: macOS-14.6-arm64-arm-64bit\n- Python version: 3.12.4\n- Huggingface_hub version: 0.27.1\n- Safetensors version: 0.5.2\n- Accelerate version: 1.2.1\n- Accelerate config: \tnot found\n- PyTorch version (GPU?): 2.5.1 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: None/NA\n\n### Who can help?\n\n@Rocketknight1 \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n```python\nfrom transformers import pipeline\nimport torch\nimport numpy as np\n\nmodel_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n#model_name = 'pollner/distilhubert-finetuned-ravdess'\ntop_k = 5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nclassification_pipeline = pipeline(\n                \"audio-classification\",\n                model=model_name,\n                top_k=top_k, \n                function_to_apply='none',\n                device=device,\n            )\n\n# dummy signal\nsampling_rate = 16000\nsignal = np.zeros((sampling_rate), dtype=np.float32)\nprint('No call parameter should match passing none:')\nprint(classification_pipeline(signal))\nprint('Call parameter with none:')\nprint(classification_pipeline(signal, function_to_apply='none'))\nprint('Call parameter with softmax which matches no parameter:')\nprint(classification_pipeline(signal, function_to_apply='softmax'))\nprint('Call parameter with sigmoid for show:')\nprint(classification_pipeline(signal, function_to_apply='sigmoid'))\n```\n\n### Expected behavior\n\nI will note that this behavior could make sense, but should probably be noted somewhere if it is the intended behavior. I assume this is not intended behavior however because in [this line](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py#L1320) we are only overwriting initialized parameters if they were sent with the call function, theoretically, but [this line](https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py#L1320) in `_sanitize_parameters` returns a default value that will always overwrite the value from initialization.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "Rocketknight1",
        "body": "Doing some bookkeeping: This was resolved in https://github.com/huggingface/transformers/pull/35771"
      }
    ]
  },
  {
    "issue_number": 29113,
    "title": "ValueError: lags cannot go further than history length, found lag 37 while history length is only 16",
    "author": "nikhilajoshy",
    "state": "closed",
    "created_at": "2024-02-19T16:51:14Z",
    "updated_at": "2025-06-04T12:43:04Z",
    "labels": [],
    "body": "### System Info\n\n- `transformers` version: 4.37.2\r\n- Platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.7\r\n- Huggingface_hub version: 0.20.3\r\n- Safetensors version: 0.4.2\r\n- Accelerate version: 0.27.2\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.2.0+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n`model = TimeSeriesTransformerModel.from_pretrained(\r\n    \"models/time-series-transformer-tourism-monthly\"\r\n)\r\n\r\n\r\noutputs = model(\r\n    past_values=past_values,\r\n    past_time_features=past_time_features,\r\n    past_observed_mask=past_observed_mask,\r\n    # static_categorical_features=static_categorical_features\"],\r\n    # static_real_features=static_real_features\"],\r\n    future_values=future_values,\r\n    future_time_features=future_time_features,\r\n)\r\n`\n\n### Expected behavior\n\nSize of the arguments passed to model is below:\r\npast values torch.Size([502, 8])\r\npast_time_features torch.Size([502, 8, 1])\r\npast_observed_mask torch.Size([502, 8])\r\nfuture_values torch.Size([502, 8])\r\nfuture_time_features torch.Size([502, 8, 1])",
    "comments": [
      {
        "user": "amyeroberts",
        "body": "cc @kashif @NielsRogge "
      },
      {
        "user": "kashif",
        "body": "@nikhilajoshy  can you kindly paste in some more verbose error?"
      },
      {
        "user": "nikhilajoshy",
        "body": "@kashif \r\n```\r\noutputs = model(\r\n              ^^^^^^\r\n  File \"/home/nikhila/encdec_venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nikhila/encdec_venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nikhila/encdec_venv/lib/python3.11/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py\", line 1384, in forward\r\n    transformer_inputs, loc, scale, static_feat = self.create_network_inputs(\r\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nikhila/encdec_venv/lib/python3.11/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py\", line 1304, in create_network_inputs\r\n    lagged_sequence = self.get_lagged_subsequences(sequence=inputs, subsequences_length=subsequences_length)\r\n```"
      }
    ]
  },
  {
    "issue_number": 37187,
    "title": "allow custom head_dim for qwen2_moe",
    "author": "bzantium",
    "state": "closed",
    "created_at": "2025-04-02T00:29:26Z",
    "updated_at": "2025-06-04T12:27:31Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nallow to use custom head_dim for qwen2_moe. (qwen2, qwen3, qwen3_moe models are possible currently.)\n\n### Motivation\n\nI want to use train model with custom head_dim:\nnot `self.head_dim = config.hidden_size // config.num_attention_heads`\nbut `self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)`\n\n### Your contribution\n\nslightly change modeling code to allow custom head_dim.",
    "comments": []
  },
  {
    "issue_number": 38576,
    "title": "A local variable 'image_seq_length' leading to UnboundLocalError: cannot access local variable 'image_seq_length' where it is not associated with a value",
    "author": "IceGiraffe",
    "state": "closed",
    "created_at": "2025-06-04T09:06:04Z",
    "updated_at": "2025-06-04T12:20:33Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\n- `transformers` version: 4.52.3\n- Platform: Linux-5.15.0-125-generic-x86_64-with-glibc2.35\n- Python version: 3.12.2\n- Huggingface_hub version: 0.32.2\n- Safetensors version: 0.5.3\n- Accelerate version: 0.26.0\n- Accelerate config:    not found\n- DeepSpeed version: not installed\n- PyTorch version (GPU?): 2.6.0+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA GeForce RTX 4090\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nThe code snippet is as follows:\nfrom transformers.utils.attention_visualizer import AttentionMaskVisualizer\n\nvisualizer = AttentionMaskVisualizer(\"meta-llama/Llama-2-7b-hf\")\nvisualizer(\"Plants create energy through a process known as\")\n\nIn the Class AttentionMaskVisualizer, a local variable in the first branch (lines 181-201), 'image_seq_length,' is passed to the function (line 232). However, in the text case, the branch will not be executed, and it will lead to UnboundLocalError: cannot access local variable 'image_seq_length' where it is not associated with a value.\n\n### Expected behavior\n\nNone",
    "comments": []
  },
  {
    "issue_number": 28005,
    "title": "Open to contribution: adding `torch.nn.functional.scaled_dot_product_attention` support for more architectures",
    "author": "fxmarty",
    "state": "closed",
    "created_at": "2023-12-13T12:35:52Z",
    "updated_at": "2025-06-04T12:05:27Z",
    "labels": [
      "contributions-welcome"
    ],
    "body": "### Feature request\n\nIn [`Transformers 4.36`](https://github.com/huggingface/transformers/releases/tag/v4.36.0), we started adding native support of [torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA), enabled by default in Transformers: https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention\r\n\r\nSDPA allows to dispatch to memory-efficient attention, flash attention on supported GPUs (currently NVIDIA-only), and even on [Intel CPUs](https://pytorch.org/blog/new-features-for-ai/#flash-attention-based-scaled-dot-product-algorithm-for-cpu).\r\n\r\nFor the record, here's a benchmark on some currently supported models:\r\n\r\n**[Training benchmark](https://gist.github.com/fxmarty/7e75cc3942d6974e4849093ebea0a331), run on A100-SXM4-80GB.**\r\n\r\n| Model     | Batch size | Sequence length | Time per batch (`\"eager\"`, s) | Time per batch (`\"sdpa\"`, s) | **Speedup** | Peak memory (`\"eager\"`, MB) | Peak memory (`\"sdpa\"`, MB) | **Memory savings**    |\r\n|-----------|------------|-----------------|-------------------------------|------------------------------|-------------|-----------------------------|----------------------------|-----------------------|\r\n| llama2 7b | 4          | 1024            | 1.065                         | 0.90                         | **19.4%**   | 73878.28                    | 45977.81                   | **60.7%**             |\r\n| llama2 7b | 4          | 2048            | OOM                           | 1.87                         | /           | OOM                         | 78394.58                   | **SDPA does not OOM** |\r\n| llama2 7b | 1          | 2048            | 0.64                          | 0.48                         | **32.0%**   | 55557.01                    | 29795.63                   | **86.4%**             |\r\n| llama2 7b | 1          | 3072            | OOM                           | 0.75                         | /           | OOM                         | 37916.08                   | **SDPA does not OOM** |\r\n| llama2 7b | 1          | 4096            | OOM                           | 1.03                         | /           | OOM                         | 46028.14                   | **SDPA does not OOM** |\r\n| llama2 7b | 2          | 4096            | OOM                           | 2.05                         | /           | OOM                         | 78428.14                   | **SDPA does not OOM** |\r\n\r\n**[Inference benchmark](https://gist.github.com/fxmarty/5113e4304fbdd38c9c3702ce44683f6a), run on A100-SXM4-80GB.**\r\n\r\n| Model            | Batch size | Prompt length | Num new tokens | Per token latency `\"eager\"` (ms) | Per token latency `\"sdpa\"` (ms) | **Speedup** |\r\n|------------------|------------|---------------|----------------|----------------------------------|---------------------------------|-------------|\r\n| llama2 13b       | 1          | 1024          | 1 (prefill)    | 178.66                           | 159.36                          | **12.11%**  |\r\n| llama2 13b       | 1          | 100           | 100            | 40.35                            | 37.62                           | **7.28%**   |\r\n| llama2 13b       | 8          | 100           | 100            | 40.55                            | 38.06                           | **6.53%**   |\r\n| Whisper v3 large | 1          | /             | 62             | 20.05                            | 18.90                           | **6.10%**   |\r\n| Whisper v3 large | 8          | /             | 77             | 25.42                            | 24.77                           | **2.59%**   |\r\n| Whisper v3 large | 16         | /             | 77             | 28.51                            | 26.32                           | **8.34%**   |\r\n\r\nPreviously, we had a partial support of SDPA in [Optimum BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview) but we are now looking to slowly deprecate it in favor of upstream support of SDPA directly in Transformers.\r\n\r\nHere are the architectures for which support has been requested:\r\n- [ ] Codegen (https://github.com/huggingface/optimum/issues/1050)\r\n- [ ] LLAVA (https://github.com/huggingface/optimum/issues/1592)\r\n- [ ] Marian (https://github.com/huggingface/optimum/issues/1142)\r\n- [x] Mistral (https://github.com/huggingface/optimum/issues/1553)\r\n- [ ] LongT5 (https://github.com/huggingface/optimum/issues/1506)\r\n- [ ] ViT (https://github.com/huggingface/optimum/issues/1553)\r\n\r\nThe integration could take inspiration from https://github.com/huggingface/optimum/blob/main/optimum/bettertransformer/models/decoder_models.py & https://github.com/huggingface/optimum/blob/main/optimum/bettertransformer/models/attention.py\n\n### Motivation\n\nFaster training & inference, lower memory requirement\n\n### Your contribution\n\nI may work on some at some point, but contributions are most welcome.\r\n\r\nYou should refer to https://github.com/huggingface/transformers/pull/26572 to add the support of SDPA for a model, roughly following these steps:\r\n* Create a `XxxSdpaAttention` class inheriting from `XxxAttention` and implement the attention logic using SDPA\r\n* Use `_prepare_4d_causal_attention_mask_for_sdpa` instead of `_prepare_4d_causal_attention_mask` for SDPA\r\n* Use `_prepare_4d_attention_mask_for_sdpa` instead of `_prepare_4d_attention_mask` for SDPA\r\n* Add `_supports_sdpa = True` to `XxxPreTrainedModel`\r\n* Add `\"sdpa\"` key to `XXX_ATTENTION_CLASSES` in the model modeling file",
    "comments": [
      {
        "user": "ENate",
        "body": "Hi @fxmarty I can take a look at this issue. Of I can ask questions if necessary. Or has anyone taken it already?"
      },
      {
        "user": "davidan5",
        "body": "does someone know if longT5 and all T5 models are blocked by bias support in flash attention ?\r\n\r\nhttps://github.com/Dao-AILab/flash-attention/pull/617"
      },
      {
        "user": "ENate",
        "body": "Hi @davidan5 are you working on the implementation?"
      }
    ]
  },
  {
    "issue_number": 38573,
    "title": "Request to add the small-doge model",
    "author": "wubingheng111",
    "state": "open",
    "created_at": "2025-06-04T06:37:38Z",
    "updated_at": "2025-06-04T11:57:06Z",
    "labels": [
      "New model"
    ],
    "body": "### Model description\n\nRequest to add the small-doge model\nthe model url: https://github.com/SmallDoges/small-doge\n\n### Open source status\n\n- [x] The model implementation is available\n- [x] The model weights are available\n\n### Provide useful links for the implementation\n\n_No response_",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @wubingheng111, those models are already available as custom code models! We generally only add models to the main library when they have a lot of usage, because once they're in the main library Hugging Face needs to take responsibility for testing them in the CI and maintaining the code."
      }
    ]
  },
  {
    "issue_number": 38344,
    "title": "[Tests] Testing for ALBERT is quite slow",
    "author": "saqlain2204",
    "state": "closed",
    "created_at": "2025-05-24T18:03:20Z",
    "updated_at": "2025-06-04T10:46:42Z",
    "labels": [],
    "body": "I've noticed that the tests for the ALBERT model are running quite slowly. Would it be possible to reduce the model size to speed up the testing process? I’d be happy to implement this change if needed!\n\nThanks",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "Hi @saqlain2204, if you can improve the test runtime without losing test coverage, we'd be happy to review that PR!"
      },
      {
        "user": "saqlain2204",
        "body": "Hi @Rocketknight1 ,\nCan you please review the PR and let me know if there is something more to add on to it?\nThanks!"
      },
      {
        "user": "saqlain2204",
        "body": "Can we try to optimise other testing models as well?\n"
      }
    ]
  },
  {
    "issue_number": 38524,
    "title": "404 Client Error when accessing https://router.huggingface.co/nebius/v1/chat/completions endpoint",
    "author": "indrawi15",
    "state": "closed",
    "created_at": "2025-06-02T07:45:52Z",
    "updated_at": "2025-06-04T09:08:06Z",
    "labels": [
      "Feature request"
    ],
    "body": "### Feature request\n\nHello Hugging Face Team,\n\nI encountered a 404 Client Error when trying to access the following API endpoint:\n\n404 Client Error: Not Found for url: https://router.huggingface.co/nebius/v1/chat/completions\n(Request ID: Root=1-683d55ae-4365e822229e0a423f164d56;0912aa19-4d00-4575-b250-5e23c4163bcb)\n\n\n### Motivation\n\nI'm trying to use the nebius chat completion model via the Hugging Face API, but I consistently get a 404 error when accessing the endpoint https://router.huggingface.co/nebius/v1/chat/completions. This prevents me from integrating the model into my application and disrupts my workflow. It’s unclear whether the endpoint has changed or if there is a bug in the API routing. Clarification or a fix would help me and other users relying on this model.\n\n\n\n### Your contribution\n\nI’m currently unable to submit a pull request or code fix, but I’m happy to provide more details or test any solutions you suggest",
    "comments": [
      {
        "user": "hanouticelina",
        "body": "Hi @indrawi15,\nSorry about the inconvenience and thanks for reporting. The URL you mentioned (https://router.huggingface.co/nebius/v1/chat/completions) is correct, and I’m not able to reproduce the 404 on my side. Which model are you trying to use? also, could you share the exact request you're sending, specifically the headers (without your token, of course :) ) and the payload? That would help us investigate further."
      },
      {
        "user": "indrawi15",
        "body": "Hi! Thanks for the response 🙏\n\nI'm using the model: `google/gemma-2-27b-it`.\n\nHere's the request info:\n\n**Endpoint:**\n`https://router.huggingface.co/nebius/v1/chat/completions`\n\n**Headers:**\n```http\nAuthorization: Bearer <my_token>\nContent-Type: application/json\nPayload:\n\njson\nSalin\nEdit\n{\n  \"model\": \"google/gemma-2-27b-it\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Data cuaca:\\n- Temperatur Rata-rata: 25.0°C\\n- Kelembaban Rata-rata: 80%\\n- Prediksi Curah Hujan: 20.5 mm\\nBerikan analisis cuaca berdasarkan data di atas...\"\n    }\n  ],\n  \"max_tokens\": 300,\n  \"temperature\": 0.5\n}\nI'm calling this using huggingface_hub.InferenceClient with provider=\"nebius\" in a Hugging Face Space. Let me know if you need anything else!\n\nyaml\nSalin\nEdit\n\n![Image](https://github.com/user-attachments/assets/7af5a5e4-e182-49db-94a1-9d050ed0a1ea)"
      },
      {
        "user": "diogoalvesderesende",
        "body": "I am facing the same issue with `microsoft/Phi-3.5-mini-instruct`\n\n"
      }
    ]
  },
  {
    "issue_number": 34446,
    "title": "Beit image classification have different results compared from versions prior to 4.43.0",
    "author": "KaraKaraWitch",
    "state": "open",
    "created_at": "2024-10-27T11:15:38Z",
    "updated_at": "2025-06-04T08:24:28Z",
    "labels": [
      "bug",
      "Vision"
    ],
    "body": "### System Info\r\n\r\n- `transformers` version: 4.43.0\r\n- Platform: Windows-10-10.0.19045-SP0\r\n- Python version: 3.10.9\r\n- Huggingface_hub version: 0.26.1\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 0.23.0\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 1.13.1+cu117 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: No\r\n- Using GPU in script?: Yes\r\n- GPU type: NVIDIA GeForce RTX 3060 Ti\r\n\r\n### Who can help?\r\n\r\n@amyeroberts\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nGiven the following image:\r\n![image](https://github.com/user-attachments/assets/4c4cf99d-e5fc-40ff-adaf-13d3c1b3d337)\r\n\r\nRunning the following pipeline for versions prior to `4.43.0` (4.42.4)\r\n```py\r\nfrom PIL import Image\r\nfrom transformers import pipeline\r\nimport transformers\r\n\r\npipeline_aesthetic = pipeline(\r\n    \"image-classification\", \"cafeai/cafe_aesthetic\", device=0\r\n)\r\nwith Image.open(\"F:\\\\Downloads\\\\Tower.jpg\") as img:\r\n    predictions = pipeline_aesthetic(img, top_k=2)\r\n    predict_keyed = {}\r\n    for p in predictions:\r\n        # print(type(p))\r\n        if not isinstance(p, dict):\r\n            raise Exception(\"Prediction value is missing?\")\r\n        predict_keyed[p[\"label\"]] = p[\"score\"]\r\n    print(predict_keyed,transformers.__version__)\r\n```\r\n\r\nFor 4.42.4, it returns:\r\n```\r\n{'aesthetic': 0.651885986328125, 'not_aesthetic': 0.3481140434741974} 4.42.4\r\n```\r\nFor 4.43.0:\r\n```\r\n{'aesthetic': 0.43069663643836975, 'not_aesthetic': 0.2877475321292877} 4.43.0\r\n```\r\n\r\n### Expected behavior\r\n\r\nExpected results from 4.42.4 instead of 4.43.0.\r\n\r\n\r\n### Addn Notes.\r\n\r\nI narrowed it down to this commit being the cause: https://github.com/huggingface/transformers/blob/06fd7972acbc6a5e9cd75b4d482583c060ac2ed0/src/transformers/models/beit/modeling_beit.py but unsure where exactly it is changed.",
    "comments": [
      {
        "user": "LysandreJik",
        "body": "cc @molbap @qubvel "
      },
      {
        "user": "molbap",
        "body": "Hi @KaraKaraWitch, thanks! looking into it. I have a hunch though, in your setup, can you run the pipeline with an input image resized to the model's default expected size? It might be due to the interpolation of positional embeddings acting up"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  },
  {
    "issue_number": 37183,
    "title": "TapasTokenizer  Produces All Zero token_type_ids Even with Tutorial Data",
    "author": "optionsraghu",
    "state": "closed",
    "created_at": "2025-04-01T18:49:36Z",
    "updated_at": "2025-06-04T08:03:02Z",
    "labels": [],
    "body": "Dear Hugging Face Transformers Team,\n\nI am encountering a persistent issue with TapasTokenizer (version 4.50.3) where it consistently produces token_type_ids filled with zeros, even when tokenizing a Pandas DataFrame directly recreated from the first table example in the official fine-tuning tutorial for TAPAS on SQA by Niels Rogge (https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb).\n\nSubject: TapasTokenizer Produces All Zero token_type_ids Even with Tutorial Data\n\nDear Hugging Face Transformers Team,\n\nI am encountering a persistent issue with TapasTokenizer (version 4.50.3) where it consistently produces token_type_ids filled with zeros, even when tokenizing a Pandas DataFrame directly recreated from the first table example in the official fine-tuning tutorial for TAPAS on SQA by Niels Rogge (https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb). 1  \n1. github.com\n\ngithub.com\n\nI have tried various steps, including:\n\n    Using both google/tapas-base and google/tapas-base-finetuned-wtq tokenizers.\n    Ensuring the Pandas DataFrame is entirely of string type.\n    Explicitly passing headers.\n    Loading table data from a list of dictionaries and a CSV file.\n    Testing with a minimal, simple string-based DataFrame (which did produce non-zero token_type_ids).\n\nThe issue persists specifically with DataFrames that have the structure and content similar to the tutorial example and my own  data.\n\nI am using Python [Your Python Version] on [Your Operating System] with transformers version 4.50.3 .\n\nI would appreciate any insights or assistance in resolving this issue. I can provide the code snippets I've been using for testing.\n\nThank you for your time and effort.\nSincerely,\nRaghu",
    "comments": [
      {
        "user": "Rocketknight1",
        "body": "cc @NielsRogge "
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "user": "NielsRogge",
        "body": "Could you provide a short reproducible Python code snippet?"
      }
    ]
  },
  {
    "issue_number": 37789,
    "title": "Whisper chunking algorithm increases WER",
    "author": "asusdisciple",
    "state": "closed",
    "created_at": "2025-04-25T13:25:21Z",
    "updated_at": "2025-06-04T08:02:32Z",
    "labels": [
      "bug"
    ],
    "body": "### System Info\n\nSo I made a few experiments with whisper and seamless m4tv2 on the fleurs (concatenated files to 5min samples) dataset. I used the batching functionality by setting `chunk_length_s` to 30s and as is turns out the WER increases by **20%** over all languages compared to long form transcription (sequentially going through each file). Do you have the same behaviour? Is this a bug or expected to happen because of the chunking? 20% seems to be far too much from my point of view.\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nJust use the default pipeline implementation of whisper on files which are a few minutes long. Its far worse when chunking is enabled.\n\n### Expected behavior\n\nI would expect the same transcription quality or maybe a few % less but 20% is far from that.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ]
  }
]