[
  {
    "issue_number": 11954,
    "title": "Add dask.tokenize to API docs",
    "author": "djhoese",
    "state": "open",
    "created_at": "2025-05-16T18:21:04Z",
    "updated_at": "2025-06-16T14:58:50Z",
    "labels": [
      "needs triage"
    ],
    "body": "I would like to use intersphinx to link to the `tokenize` function, but it isn't documented.\n\n```bash\n$ python -m sphinx.ext.intersphinx \"https://docs.dask.org/en/latest/objects.inv\" | grep \"tokenize\"\n    dask.typing.DaskCollection.__dask_tokenize__                                         : custom-collections.html#dask.typing.DaskCollection.__dask_tokenize__\n```\n\nSide question: How are the API docs in `generated/` generated? The READMEs and the developer guide all say to just run \"make html\", but I can't find the thing actually creating \"generated/\".",
    "comments": [
      {
        "user": "djhoese",
        "body": "I can probably fix this myself if there is anyone who can point me to instructions or more information about how the API docs are generated. "
      }
    ]
  },
  {
    "issue_number": 11854,
    "title": "Perform all graph optimizations for all collection types",
    "author": "djhoese",
    "state": "open",
    "created_at": "2025-03-31T01:53:11Z",
    "updated_at": "2025-06-16T14:54:43Z",
    "labels": [
      "needs triage"
    ],
    "body": "Discussion originally started here: https://dask.discourse.group/t/optimal-graph-optimization-when-mixing-dask-objects/3867\n\nShort summary is that there are cases where a single graph consists of tasks from different types and they may not be optimally optimized. For example, a series of Array operations ending in a Delayed task will be optimized as a Delayed task and miss all the possible Array graph optimizations. This gets even more complicated when `da.store` is involved.\n\nI don't have the larger context for the discussion in issues like #11458 to understand it and the help I got on discourse lead to filing this issue for more maintainers eyes. The main question is, can all collection optimizations always be performed during optimizations? In my real world use case, in a simple data case, I see a 35+% improvement in execution time by forcing Array optimizations on a graph that ends in a Delayed object.\n\nHere is a basic example showing the issue:\n\n```python\nimport dask\nimport dask.array as da\n\n@dask.delayed(pure=True)\ndef delayed_func(arr1):\n    return arr1 + 1\n\nstart = da.zeros((2, 2), chunks=1)\nsubarr1 = start[:1]\nsubarr2 = subarr1[:, :1]\ndelay_result = delayed_func(subarr2)\n\nassert len(delay_result.dask.keys()) == 9  # zeros * 4 -> getitem * 3 -> finalize -> delayed_func\nif True:\n    # current dask\n    assert len(dask.optimize(delay_result)[0].dask.keys()) == 5  # zeros * 1 -> getitem * 2 -> finalize -> delayed_func\nelse:\n    # if Arrays were detected in Delayed graphs:\n    assert len(dask.optimize(delay_result)[0].dask.keys()) == 2  # finalize-getitem-zeros -> delayed_func\n# use array optimize instead of delayed\nwith dask.config.set(delayed_optimize=da.optimize):\n    assert len(dask.optimize(delay_result)[0].dask.keys()) == 2  # finalize-getitem-zeros -> delayed_func\n```\n\nRelated:\n\n* https://github.com/dask/dask/issues/8380\n* https://github.com/dask/dask/pull/9732\n* Maybe https://github.com/dask/dask/issues/6732?\n* https://github.com/dask/dask/issues/11458",
    "comments": [
      {
        "user": "fjetter",
        "body": "> This gets even more complicated when da.store is involved.\n\nFYI I intend to change that in https://github.com/dask/dask/pull/11844 which would preserve the array type and avoid many of those problems"
      },
      {
        "user": "fjetter",
        "body": "Also note that we're working on a new array backend which will change how optimization will be performed. The new backend will also be using what we call an Expression, just like the DataFrame backend. Once this is implemented, the optimizations will be done according to the collections type as you're suggesting here. However, the transition from DataFrame/Array->Delayed will always be awkward and I'd discourage from doing so.\n_Typically_ there is an appropriate API to do what you want to do instead, e.g. [`dask.array.reduction`](https://docs.dask.org/en/latest/generated/dask.array.reduction.html#dask-array-reduction) could be used to throw all chunks of an array (or slice of array) into one function. It's a little less ergonomic, of course.\nJust to be clear: The conversion / passing to a Delayed will (most likely) not go away it will just not always be very smooth."
      },
      {
        "user": "djhoese",
        "body": "My library (satpy) has too common caes that I can think of:\n\n1. An algorithm which is not \"dask-friendly\" and must look at the entire input array. The result is an Array. We currently do this by Array -> Delayed -> Array.from_delayed. I'm not sure this fits the `reduction` pattern, so would you recommend maybe rechunking to one giant chunk, map_blocks'ing, then rechunking back to the original chunk size?\n2. An ending Delayed function typically to save an Array to disk. The result is either a filename or None. It sounds like you are saying `reduction` is best for this case?\n\nCould you explain in more detail why the Array -> Delayed is discouraged? Especially when APIs like `Array.from_delayed` exist?\n\nThe #11844 PR seems exactly like what I've been wanting regarding `da.store`. Thanks for working on it."
      }
    ]
  },
  {
    "issue_number": 11983,
    "title": "JAX as a possible dask array backend?",
    "author": "dpanici",
    "state": "open",
    "created_at": "2025-06-11T21:13:27Z",
    "updated_at": "2025-06-16T14:05:29Z",
    "labels": [
      "needs triage"
    ],
    "body": "Hi,\n\nI am interested in using dask as a vehicle for parallelization of an existing CPU/GPU code which is written using JAX. I am pretty naive/new to dask, but I see that there is some documentation on use for GPUs/ a [blog post](https://blog.dask.org/2023/02/02/easy-cpu-gpu) where the backend for array creation can be cupy, for example. Is there something similar planned/available for JAX?\n\nThanks!",
    "comments": [
      {
        "user": "dbalabka",
        "body": "@dpanici, if I undestand you right, you would like to perform distributed training with JAX. \n\nBase on my exprience with [T5X](https://github.com/google-research/t5x) for training on TPUs with JAX and Dask for distributed data processing and model training, I would said Dask can help to provision wokers in the cloud and then you can join them using [jax.distributed.initilize](https://docs.jax.dev/en/latest/_autosummary/jax.distributed.initialize.html#jax.distributed.initialize). Here is an official example working with GPUs distributed across multiple hosts:\nhttps://docs.jax.dev/en/latest/multi_process.html#gpu-example"
      }
    ]
  },
  {
    "issue_number": 11992,
    "title": "Grouping by unknown, high-cardinality categorical columns can lead to OOM",
    "author": "dbalabka",
    "state": "open",
    "created_at": "2025-06-16T06:55:56Z",
    "updated_at": "2025-06-16T13:37:40Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\nThe following code leads to exponential memory usage exposure if categorical columns are `unknown`:\n```python\nsample_data = (\n        sample_data\n        .loc[:, ['category1', 'category2', 'category3', 'price']]\n        .groupby(['category1', 'category2', 'category3'])\n        .sum()\n        .reset_index()\n        .persist()\n)\n\ndask.base.wait(sample_data)\n```\n\n![Image](https://github.com/user-attachments/assets/e8fcb66e-14f8-4b7a-9c46-73e5d1748693)\n\nThe workaround is to `categorize()` columns and convert into integer:\n```python\nsample_data= data.categorize(columns=categorical_cols)\nfor col in categorical_cols:\n    sample_data[f\"{col}_int\"] = sample_data[col].cat.codes.astype(\"uint16[pyarrow]\")\n\nsample_data= sample_data.persist()\ndask.base.wait(sample_data)\n```\n\n\n**Minimal Complete Verifiable Example**:\n\nNotebook:\n[test_oom_latest.zip](https://github.com/user-attachments/files/20758110/test_oom_latest.zip)\n\n\n**Environment**:\n\n- Dask version: 2025.5.1\n- Python version: 3.12\n- Operating System: WSL, Ubuntu 22.04\n- Install method (conda, pip, source): poetry\n- All deps:\n```toml\ndependencies = [\n    \"bokeh>=3.7.3\",\n    \"dask[distributed]>=2025.5.1,<2026.0.0\",\n    \"jupyterlab>=4.4.3\",\n    \"matplotlib>=3.10.3\",\n    \"memray>=1.17.2\",\n    \"numpy>=2.3.0\",\n    \"pandas>=2.3.0\",\n    \"pyarrow (>=20.0.0,<21.0.0)\",\n]\n```\n",
    "comments": []
  },
  {
    "issue_number": 11283,
    "title": "Issues with da.linalg.inv using CuPy-backed Dask arrays",
    "author": "DanielBajac",
    "state": "open",
    "created_at": "2024-08-07T20:31:34Z",
    "updated_at": "2025-06-16T02:13:21Z",
    "labels": [
      "array",
      "needs attention",
      "gpu"
    ],
    "body": "Hi everyone,\r\n\r\nI'm encountering an issue while trying to compute the inverse of a matrix using Dask arrays with CuPy backend. \r\nMy goal with the code is to performe inverse of larger than memory arrays with Dask, using cupy as backend\r\n\r\nHere's a simplified version of my code:\r\n\r\n```\r\nimport numpy as np\r\nimport dask.array as da\r\nimport cupy as cp\r\n\r\na = np.random.rand(40, 40)\r\ninv = np.linalg.inv(a)\r\na_d = da.from_array(a, chunks='auto')\r\na_d = a_d.map_blocks(cp.asarray)\r\ninv_d = da.linalg.inv(a_d).compute()\r\nprint(np.allclose(inv, inv_d))\r\n\r\n```\r\n\r\nthe error message is\r\n\r\n  File \"/users/dbajac/micromamba/envs/pyscf_env/lib/python3.12/site-packages/dask/array/linalg.py\", line 1282, in inv\r\n    return solve(a, eye(a.shape[0], chunks=a.chunks[0][0]))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/users/dbajac/micromamba/envs/pyscf_env/lib/python3.12/site-packages/dask/array/linalg.py\", line 1263, in solve\r\n    uy = solve_triangular(l, b, lower=True)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/users/dbajac/micromamba/envs/pyscf_env/lib/python3.12/site-packages/dask/array/linalg.py\", line 1198, in solve_triangular\r\n    res = _solve_triangular_lower(\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/users/dbajac/micromamba/envs/pyscf_env/lib/python3.12/site-packages/dask/array/linalg.py\", line 970, in _solve_triangular_lower\r\n    return solve_triangular_safe(a, b, lower=True)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/users/dbajac/micromamba/envs/pyscf_env/lib/python3.12/site-packages/dask/array/utils.py\", line 571, in solve_triangular_safe\r\n    return scipy_linalg_safe(\"solve_triangular\", a, b, lower=lower)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/users/dbajac/micromamba/envs/pyscf_env/lib/python3.12/site-packages/dask/array/utils.py\", line 567, in scipy_linalg_safe\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/users/dbajac/micromamba/envs/pyscf_env/lib/python3.12/site-packages/cupyx/scipy/linalg/_solve_triangular.py\", line 43, in solve_triangular\r\n    _util._assert_cupy_array(a, b)\r\n  File \"/users/dbajac/micromamba/envs/pyscf_env/lib/python3.12/site-packages/cupy/linalg/_util.py\", line 20, in _assert_cupy_array\r\n    raise linalg.LinAlgError(\r\nnumpy.linalg.LinAlgError: cupy.linalg only supports cupy.ndarray\r\n\r\n\r\n**Environment**: micromamba\r\n\r\n- Dask version: 2024.8.0\r\n- Python version: 3.12.4\r\n- Cupy version: 13.2.0\r\n- Operating System: Debian\r\n- Install method (conda, pip, source): conda\r\n\r\nThanks a lot in advance\r\n\r\nBest, \r\nDaniel.\r\n",
    "comments": [
      {
        "user": "quasiben",
        "body": "Thanks for the report.  I think this issue comes from `eye` not being \"gpu safe\" and a numpy array is included\r\n\r\nhttps://github.com/dask/dask/blob/ed5f68897b3a097f7c5ec1a9ec13ce49c112a544/dask/array/linalg.py#L1282\r\n\r\nMy guess is that a fix wouldn't be too hard...are you interested in exploring a solution ?\r\n\r\nI'll also point out that this is has been a long standing issue: https://github.com/dask/dask/issues/4899  -- though with the various NEPs now long in I suspect it shouldn't be too challenging.  If you can, can you tell us more about the larger workload you are running ?\r\n\r\nfyi, here's a smaller reproducer:\r\n\r\n```python\r\nimport dask\r\nimport dask.array as da\r\n\r\ndask.config.set({\"array.backend\": \"cupy\"})\r\ndata = da.random.random((40,40))\r\n\r\nda.linalg.inv(data).compute()\r\n```"
      }
    ]
  },
  {
    "issue_number": 11280,
    "title": "Dask Client slows down non-dask code (thus dask-distributed code as well)",
    "author": "stavoltafunzia",
    "state": "open",
    "created_at": "2024-08-07T14:38:40Z",
    "updated_at": "2025-06-16T02:13:21Z",
    "labels": [
      "needs info",
      "needs attention"
    ],
    "body": "I recently found that creating a `dask.distributed.Client` instance slows down the execution of the remaining code, even if Dask is not used at all.\r\n\r\nAn example to reproduce is given below (it basically executes some math-heavy calculation on a separate process).\r\n\r\n```\r\nimport sys\r\nimport multiprocessing\r\nimport time\r\nfrom concurrent.futures import ProcessPoolExecutor\r\nfrom dask.distributed import Client\r\nimport numpy as np\r\nfrom scipy import optimize\r\n\r\ndef fn1(x, *, epsi=1e-2):\r\n    return 0.5 * (x + np.sqrt(x ** 2 + epsi ** 2))\r\n\r\n\r\ndef fn1_d1(x, *, epsi=1e-2):\r\n    return 0.5 * (1 + x / np.sqrt(x ** 2 + epsi ** 2))\r\n\r\n\r\ndef fn1_d2(x, *, epsi=1e-2):\r\n    return 0.5 * 1 / np.sqrt(x ** 2 + epsi ** 2) - x ** 2 * (x ** 2 + epsi ** 2) ** (-3 / 2)\r\n\r\ndef fn2(x, y, *, epsi=1e-2):\r\n    return (fn1(x, epsi=epsi) - fn1(y, epsi=epsi)) ** 2\r\n\r\n\r\ndef fn2_d1(x, y, *, epsi=1e-2):\r\n    return 2 * (fn1(x, epsi=epsi) - fn1(y, epsi=epsi)) * fn1_d1(x, epsi=epsi)\r\n\r\n\r\ndef fn2_d2(x, y, *, epsi=1e-2):\r\n    return 2 * fn1_d1(x, epsi=epsi)**2 + 2 * (fn1(x, epsi=epsi) - fn1(y, epsi=epsi)) * fn1_d2(x, epsi=epsi)\r\n\r\ndef job(A, B):\r\n    # A, B = args\r\n    \r\n    alpha = 1e-5 / A.shape[0]\r\n    x0 = np.zeros(A.shape[1])\r\n    last_x = [x0, np.matmul(A, x0)]\r\n    \r\n    def get_w(x):\r\n        if np.all(x == last_x[0]):\r\n            return last_x[1]\r\n        last_x[0] = x\r\n        last_x[1] = np.matmul(A, x0)\r\n        return last_x[1]\r\n    \r\n    def target(x):\r\n        w = get_w(x)\r\n        return alpha * np.sum(x**2) + np.sum(fn2(w, B))\r\n    \r\n    def gradient(x):\r\n        w = get_w(x)\r\n        return 2 * alpha * x + np.matmul(fn2_d1(w, B), A)\r\n\r\n    def hessian(x):\r\n        w = get_w(x)\r\n        return 2 * alpha * np.eye(x.shape[0]) + np.matmul(A.T, A * fn2_d2(w, B).reshape((-1, 1)))\r\n    \r\n    for _ in range(30):\r\n        optimize.minimize(target, x0, args=(),\r\n            method='Newton-CG', jac=gradient, hess=hessian,\r\n            hessp=None)\r\n\r\n    \r\nif __name__ == '__main__':\r\n    \r\n    n_jobs = 1\r\n    np.random.seed(10)\r\n    a = np.random.rand(500000, 10)\r\n    b = np.matmul(a, np.random.rand(a.shape[1], 1) + 1).reshape((-1))\r\n    b = b > np.mean(b)\r\n\r\n    if len(sys.argv) > 1:\r\n        print(\"Creating client\")\r\n        client = Client(processes=True)  # Executing this line slows down the execution by roughly 10 to 20%\r\n\r\n    ctx = multiprocessing.get_context('forkserver')\r\n    executor = ProcessPoolExecutor(mp_context=ctx)\r\n    \r\n    start = time.time()\r\n    futs = [executor.submit(job, a, b) for fn_ in range(n_jobs)]\r\n    res = [f.result() for f in futs]\r\n    end = time.time()\r\n    print(\"Took {:.3f} minutes\".format((end-start)/60))\r\n\r\n```\r\n\r\nThe code above can be executed with and without extra argument. When an extra argument is provided, the line `client = Client(processes=True)` gets executed and this causes the remaining part to run, on my machine, 20% slower.\r\n\r\nConsequence of this issue (which is actually how I found it after few days of reducing down to minimal example) is that whenever similar tasks are distributed to the cluster, there is a baseline 20% performance overhead (per thread) which is highly undesirable.\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.7.1\r\n- Python version: 3.11.9\r\n- Operating System: Debian 12\r\n- Install method (conda, pip, source): pip\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Thanks for your report.\r\n\r\nDask sets the variables OMP_NUM_THREADS, MKL_NUM_THREADS and OPENBLAS_NUM_THREADS to 1 when a client is created. Can you try unsetting the 3 to see if that speeds your stuff back up?\r\n\r\nthis speeds things up for me"
      },
      {
        "user": "stavoltafunzia",
        "body": "I don't have my PC available right now, will check tomorrow. But I'm pretty sure that such environment variables are not the root cause. I was already using OPENBLAS_NUM_THREADS=1 on my parent process (I do use openblas numpy). Moreover I monitored CPU utilisation, and I always see 1 core at 100% (same number as the variable n_jobs, even when changing it), regardless of the Client instance.\r\n\r\nI don't know if it gives any hint, but what I did notice it's the following: monitoring CPU usage with \"htop\", when the Client object is not instantiated the CPU utilisation bar is almost entirely green (i.e. load with normal priority), but when the Client object is instantiated I clearly see the bar having a noticeable fraction of red color (i.e. kernel thread). I do suspect the otlverhead is caused by some system call, possibly some locking mechanism. But I have no clue why it would be happening."
      },
      {
        "user": "stavoltafunzia",
        "body": "I tested for the environment variables OMP_NUM_THREADS, MKL_NUM_THREADS and OPENBLAS_NUM_THREADS, setting all of them to 1 (even when Dask Client is not instantiated), didn't make a difference. What written above still holds."
      }
    ]
  },
  {
    "issue_number": 11295,
    "title": "Error in addition of dask dataframe and array when reading from parquet",
    "author": "kbbat",
    "state": "open",
    "created_at": "2024-08-09T21:07:01Z",
    "updated_at": "2025-06-16T02:13:19Z",
    "labels": [
      "needs attention",
      "dask-expr",
      "array-expr"
    ],
    "body": "\r\n<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\nI have encountered a very particular set of circumstances in my workflow that causes an error when adding a dask DataFrame to a dask Array that are derived from the same data that has been read from parquet.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport math\r\nimport numpy as np\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\nfrom dask.distributed import Client, LocalCluster\r\n\r\ncluster = LocalCluster()\r\nclient = Client(cluster)\r\n\r\n# Generate dask Dataframe and save to parquet\r\ndf = pd.DataFrame(np.random.random((100,3)), columns=['x','y','z'])\r\nddf = dd.from_pandas(df, npartitions=100)\r\nddf.to_parquet('/path/to/data')\r\n\r\n# Load data from parquet\r\nddf_loaded = dd.read_parquet('/path/to/data')\r\nddf_sub = ddf_loaded[['x','y']]                # Error only occurs if a subset of the columns are selected\r\nddf_sub.columns = [0,1]                        # Error only occurs if columns have been renamed\r\nsub_array = ddf_sub.to_dask_array()            # sub_array only has 50 chunks instead of 100 like ddf_sub\r\nddf_persisted = client.persist(ddf_sub)        # converting ddf_persisted to dask array will have 100 chunks\r\n(ddf_persisted+sub_array).compute()            # Error only occurs if ddf_sub is persisted, ddf_sub+sub_array works\r\n```\r\nGenerates the following error:\r\n```python\r\nValueError                                Traceback (most recent call last)\r\nCell In[4], line 24\r\n     20 dask_sub_array = ddf_sub.to_dask_array()\r\n     21 ddf_persisted = client.persist(ddf_sub)\r\n---> 22 (ddf_persisted+dask_sub_array).compute()\r\n\r\nFile [~/miniconda3/envs/dask/lib/python3.11/site-packages/dask_expr/_collection.py:162](http://localhost:8888/lab/tree/test/miniconda3/envs/dask/lib/python3.11/site-packages/dask_expr/_collection.py#line=161), in _wrap_expr_op(self, other, op)\r\n    160     other = other.expr\r\n    161 elif isinstance(other, da.Array):\r\n--> 162     other = from_dask_array(\r\n    163         other, index=self.index.to_legacy_dataframe(), columns=self.columns\r\n    164     )\r\n    165     if self.ndim == 1 and len(self.columns):\r\n    166         other = other[self.columns[0]]\r\n\r\nFile [~/miniconda3/envs/dask/lib/python3.11/site-packages/dask_expr/_collection.py:5098](http://localhost:8888/lab/tree/test/miniconda3/envs/dask/lib/python3.11/site-packages/dask_expr/_collection.py#line=5097), in from_dask_array(x, columns, index, meta)\r\n   5096 if columns is not None and isinstance(columns, list) and not len(columns):\r\n   5097     columns = None\r\n-> 5098 df = from_dask_array(x, columns=columns, index=index, meta=meta)\r\n   5099 return from_legacy_dataframe(df, optimize=True)\r\n\r\nFile [~/miniconda3/envs/dask/lib/python3.11/site-packages/dask/dataframe/io/io.py:448](http://localhost:8888/lab/tree/test/miniconda3/envs/dask/lib/python3.11/site-packages/dask/dataframe/io/io.py#line=447), in from_dask_array(x, columns, index, meta)\r\n    443 if index.npartitions != x.numblocks[0]:\r\n    444     msg = (\r\n    445         \"The index and array have different numbers of blocks. \"\r\n    446         \"({} != {})\".format(index.npartitions, x.numblocks[0])\r\n    447     )\r\n--> 448     raise ValueError(msg)\r\n    449 divisions = index.divisions\r\n    450 graph_dependencies.append(index)\r\n\r\nValueError: The index and array have different numbers of blocks. (100 != 50)\r\n```\r\n\r\n**Environment**:\r\n\r\n- Dask version:  2024.8.0\r\n- Python version: 3.11.8\r\n- Operating System: Ubuntu 22.04\r\n- Install method (conda, pip, source): conda\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Thanks for your report. parquet automatically reorders the number of partitions, round tripping through arrays is currently not what I would advise to do. You can set ``optimize=False`` in the to_dask_array call to make this error go awayy"
      },
      {
        "user": "kbbat",
        "body": "Thank you for your reply, the  ```optimize``` flag seems to be missing from the API documentation for ```to_dask_array```. The issue in my workflow actually arises because I apply a function using ```map_partitions``` to ```ddf_sub``` that returns an array. I can coerce the function return to a pandas dataframe so that the output of ```map_partitions``` is a dask dataframe with the same number of partitions, but leaving it natively as an array gave better performance - perhaps simply because the number of partitions was being optimized. I will either convert to a dask array with ```optimize=False``` and use ```map_blocks``` or return a pandas dataframe in ```map_partitions``` going forward. Feel free to close with any additional tips or best practices you wish to add, and thanks again for resolving my issue."
      },
      {
        "user": "phofl",
        "body": "Thanks for the context, that helps me understand this better.\r\n\r\nnot calling persist would also fix stuff btw, the optimizer will then take both branches into account and will return the same number of partitions for both "
      }
    ]
  },
  {
    "issue_number": 11307,
    "title": "Out of memory ",
    "author": "dbalabka",
    "state": "open",
    "created_at": "2024-08-13T13:34:44Z",
    "updated_at": "2025-06-16T02:13:18Z",
    "labels": [
      "dataframe",
      "needs attention",
      "dask-expr"
    ],
    "body": "**Describe the issue**:\r\nIt looks like `dask-expr` optimizes the query wrongly. Adding the extra `persist()` fixes the OOM memory:\r\n```text\r\n---------------------------------------------------------------------------\r\nMemoryError                               Traceback (most recent call last)\r\nCell In[5], line 100\r\n     87 display(\r\n     88     train_df,\r\n     89     test_df,\r\n     90     sku_groups_df,\r\n     91 )\r\n     93 display(\r\n     94     train_df.head(),\r\n     95     test_df.head(),\r\n     96     sku_groups_df.head(),\r\n     97 )\r\n     99 display(\r\n--> 100     *dask.compute(\r\n    101         sku_groups_df['group'].value_counts(),\r\n    102         train_df['group'].value_counts(),\r\n    103         test_df['group'].value_counts(),\r\n    104     )\r\n    105 )\r\n    107 print(train_df.dtypes)\r\n    108 print(test_df.dtypes)\r\n\r\nFile ~/src/.venv/lib/python3.10/site-packages/dask/base.py:661, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    658     postcomputes.append(x.__dask_postcompute__())\r\n    660 with shorten_traceback():\r\n--> 661     results = schedule(dsk, keys, **kwargs)\r\n    663 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/src/.venv/lib/python3.10/site-packages/distributed/client.py:2234, in Client._gather(self, futures, errors, direct, local_worker)\r\n   2232         exc = CancelledError(key)\r\n   2233     else:\r\n-> 2234         raise exception.with_traceback(traceback)\r\n   2235     raise exc\r\n   2236 if errors == \"skip\":\r\n\r\nMemoryError: Task ('repartitionsize-a141886386f8cc3cbacc3d585efdbc07', 470) has 22.35 GiB worth of input dependencies, but worker tls://10.164.0.41:40427 has memory_limit set to 15.64 GiB.\r\n```\r\n\r\nWorking example:\r\n```python\r\nimport dask\r\nimport dask.dataframe as dd\r\n\r\ngroups_df = (\r\n    dd\r\n    .read_parquet(\r\n        f'gs://groups.parquet/*',\r\n        columns=['ItemID', 'group'],\r\n    )\r\n    .astype({'group': 'category'})\r\n    .repartition(partition_size='100MB')\r\n    .persist()\r\n)\r\n\r\ndef read_dataset(path: str) -> dd.DataFrame:\r\n    df = (\r\n        dd.read_parquet(\r\n            path,\r\n            # We have to request existing cols before renaming\r\n            columns=['ItemID', 'A_n/a', 'AT_n/a'],\r\n        )\r\n        .rename(columns={'A_n/a': 'A_na', 'AT_n/a': 'AT_na' })\r\n        # This fixes weired issue with dask-expr. I faced with OOM\r\n        .persist()\r\n    )\r\n\r\n    return (\r\n        df\r\n        .merge(\r\n            groups_df,\r\n            on='ID',\r\n            how='left'\r\n        )\r\n        .query('group in @groups_to_include',\r\n               local_dict={'groups_to_include': ['group1', 'group2', 'group3', 'group4', 'group4']})\r\n        .repartition(partition_size='100MB')\r\n    )\r\n\r\n\r\ntrain_df = read_dataset(f'gs://train.parquet/*')\r\ntest_df = read_dataset(f'gs://test.parquet/*')\r\n\r\ntrain_df.pprint()\r\n\r\ntrain_df, test_df = dask.persist(train_df, test_df)\r\n\r\ndisplay(\r\n    train_df,\r\n    test_df,\r\n    groups_df,\r\n)\r\n\r\ndisplay(\r\n    train_df.head(),\r\n    test_df.head(),\r\n    groups_df.head(),\r\n)\r\n\r\ndisplay(\r\n    *dask.compute(\r\n        groups_df['sku_group'].value_counts(),\r\n        train_df['sku_group'].value_counts(),\r\n        test_df['sku_group'].value_counts(),\r\n    )\r\n)\r\n\r\n```\r\n`pprint`:\r\n```text\r\nRepartition: partition_size='100MB'None\r\n  Query: _expr='group in @groups_to_include' expr_kwargs={'local_dict': {'groups_to_include': ['group1', 'group2', 'group3', 'group4', 'group4']}}\r\n    Merge: how='left' left_on='ItemID' right_on='ItemID'\r\n      FromGraph: layer={('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 0): <Future: pending, key: ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 0)>, ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 1): <Future: pending, key: ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 1)>, ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 2): <Future: pending, key: ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 2)>, ..., ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 941): <Future: finished, type: pandas.core.frame.DataFrame, key: ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 941)>, ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 942): <Future: finished, type: pandas.core.frame.DataFrame, key: ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 942)>} _meta='<pandas>' divisions=(None, None, ..., None, None) keys=[('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 0), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 1), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 2), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 3), ..., ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 934), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 935), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 936), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 937), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 938), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 939), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 940), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 941), ('read_parquet-operation-fe85241c2cbf3f4e3c47c8ce4d1bb145', 942)] name_prefix='read_parquet-operation'\r\n      FromGraph: layer={('repartitionsize-a7f9918679644d0763b0995e4cc22887', 0): <Future: finished, type: pandas.core.frame.DataFrame, key: ('repartitionsize-a7f9918679644d0763b0995e4cc22887', 0)>} _meta='<pandas>' divisions=(None, None) keys=[('repartitionsize-a7f9918679644d0763b0995e4cc22887', 0)] name_prefix='repartitionsize'\r\n```\r\nAn example that leads to OOM:\r\n```python\r\n# ...\r\n        .rename(columns={'A_n/a': 'A_na', 'AT_n/a': 'AT_na' })\r\n        # Commeting out this line leads to OOM\r\n        #.persist()\r\n#...\r\n```\r\n`pprint`:\r\n```text\r\nRepartition: partition_size='100MB'None\r\n  Query: _expr='sku_group in @sku_groups_to_include' expr_kwargs={'local_dict': {'sku_groups_to_include': ['no_sales', 'very_intermittent', 'intermittent', 'frequent', 'very_frequent']}}\r\n    Merge: how='left' left_on='InventoryID' right_on='InventoryID'\r\n      RenameFrame: columns={'A_n/a': 'A_na', 'AT_n/a': 'AT_na', ... }\r\n        ReadParquetFSSpec: path='gs://ecentria-ai-team-352717-demand-forecasting/ai-459/data/ecentria/features/train_features.parquet/*' columns=['CN', 'lag1', 'A_n/a', 'AT_n/a', ...] kwargs={'dtype_backend': None}\r\n      FromGraph: layer={('repartitionsize-a7f9918679644d0763b0995e4cc22887', 0): <Future: finished, type: pandas.core.frame.DataFrame, key: ('repartitionsize-a7f9918679644d0763b0995e4cc22887', 0)>} _meta='<pandas>' divisions=(None, None) keys=[('repartitionsize-a7f9918679644d0763b0995e4cc22887', 0)] name_prefix='repartitionsize'\r\n```\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\nTBD\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n\r\n- Dask version: \r\n```\r\ndask = \"2024.5.2\" \r\ndask-expr = \"1.1.2\"\r\n```\r\n- Python version: 3.10\r\n- Operating System: WSL\r\n- Install method (conda, pip, source): poetry\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Can you say a bit more about the size of your parquet files, worker specs, ...\r\n\r\ndask-expr fuses multiple parquet files to a single partition until we reach 75MB in memory size per partition, this might be relevant here.\r\n\r\nQuery blocks the optimiser, so not much should happen here tbh.\r\n\r\nIs your pprint result optimised? if not, please post df.optimize(fuse=False).pprint()"
      },
      {
        "user": "dbalabka",
        "body": "@phofl , thanks for quick reply.\r\n\r\n> Can you say a bit more about the size of your parquet files, worker specs, ...\r\n\r\nWe're utilizing 12 `n2-highmem-2` instances, 2 vCPUs and 16GB of RAM per each.\r\n\r\n* `train.parquet/*` - 2827 files of ~5MB each\r\n* `test.parquet/*` - 429 files of ~4MB each\r\n* `groups.parquet` - single 5.3 MB file\r\n\r\n> Is your pprint result optimised?\r\n\r\nI believe, yes, because of default behaviour:\r\n```\r\n...\r\ntrain_df.pprint()\r\n...\r\n```\r\n\r\nI'll provide unoptimized outputs shortly. Currently, I'm working on reproducing the problem with the latest versions of `dask-expr` and `distributed`. "
      },
      {
        "user": "phofl",
        "body": "No, pprint is not optimising your query \r\n\r\nCan you try setting\r\n\r\n```\r\ndask.config.set({\"dataframe.parquet.minimum-partition-size\": 1})\r\n```\r\n\r\nthat will disable the fusion of partitions in read_parquet"
      }
    ]
  },
  {
    "issue_number": 11308,
    "title": "Unexpected Behavior When Using `dask.delayed` with `xarray` to Load a Chunked Dataset",
    "author": "Eis-ba-er",
    "state": "open",
    "created_at": "2024-08-13T13:38:02Z",
    "updated_at": "2025-06-16T02:13:17Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "### **Description:**\r\nI'm experiencing unexpected behavior when using `dask.delayed` to process a chunked dataset loaded with `xarray`. The goal is to retrieve a specific subset of data based on latitude, longitude, and a given timestamp for analysis.\r\n\r\nThe function works as expected when `dask.delayed` is not used, returning the correct subset immediately. However, when I introduce `dask.delayed`, the function triggers excessive computations, leading to severe performance degradation.\r\n\r\n### Expected Behavior:\r\n- **Without** `dask.delayed`: The function retrieves the specified data subset promptly. \r\n- **With** `dask.delayed`: The function should only compute the specified data subset (latitude, longitude, and timestamp) without loading or processing unnecessary data.\r\n\r\n### Actual Behavior:\r\n- **Without** `dask.delayed`: The function behaves correctly, retrieving and returning the specified data subset quickly. \r\n- **With** `dask.delayed`: Instead of processing only the requested data subset, `dask` appears to initiate a computation for the entire dataset. The Dask dashboard shows approximately 8000 function calls to `load_grid_data()`, suggesting that all chunks (both timesteps and ensemble members) are being processed.\r\n\r\n### Additionally:\r\n- **Loading Dataset to RAM:** If I load the entire dataset into RAM first, then call the delayed method, everything works as expected. The computation speed is the same as without `dask.delayed`.\r\n- **Lazy Loading Dataset:** When using lazy loading (typically necessary for large datasets), this issue occurs as described. The excessive computations are triggered when using the `dask.delayed` decorator.\r\n\r\n### Steps to Reproduce:\r\n**1. Create a Toy Dataset:**\r\n   - Generate a random dataset with dimensions for ensemble members, time steps, latitude, and longitude.\r\n   - Save the dataset to disk using `xarray`.\r\n\r\n**2. Load the Dataset with `xarray`:**\r\n    - Load the dataset with `xarray`, specifying chunks for the `step` and `ensemble_nr` dimensions.\r\n     - Alternatively, specify chunks for `latitude` and `longitude`.\r\n\r\n**3. Apply `dask.delayed`:**\r\n    - Define a function to load a specific grid cell's data for a given latitude, longitude, and timestamp.\r\n    - Apply `dask.delayed` to this function and observe the behavior.\r\n\r\n **4.  Compare the Performance:**\r\n        - Observe the difference in behavior with and without `dask.delayed`. Without `dask.delayed`, the function runs as expected.  With `dask.delayed`, excessive computations are triggered, leading to performance issues.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport xarray as xr\r\nimport dask.delayed\r\nimport dask\r\nimport numpy as np\r\nfrom dask.distributed import Client, LocalCluster\r\n\r\n# Initialize Dask cluster\r\ncluster = LocalCluster(n_workers=2, threads_per_worker=1)\r\nclient = Client(cluster)\r\nclient.scheduler_info()\r\n\r\n# Create sample dataset\r\nlat = np.arange(-5, 5, 0.5)\r\nlon = np.arange(0, 5, 0.5)\r\nensemble = np.arange(0, 100, 1)\r\nstart = np.datetime64('2022-01-01T00:00:00', 'ns')\r\nend = start + np.timedelta64(40, 'D')\r\nstep = np.arange(start, end, np.timedelta64(12, 'h'))\r\ntemperature = np.random.randint(20, 35, size=(len(ensemble), len(step), len(lat), len(lon)))\r\n\r\n# Save dataset to disk\r\nxr.Dataset(\r\n    coords={\r\n        'ensemble_nr': ensemble,\r\n        'step': step,\r\n        'latitude': lat,\r\n        'longitude': lon\r\n    },\r\n    data_vars={\r\n        'temperature': (['ensemble_nr', 'step', 'latitude', 'longitude'], temperature)\r\n    }\r\n).to_netcdf('test_data.nc')\r\n\r\n# Load the dataset with xarray\r\ntest_data = xr.open_dataset('test_data.nc', chunks={'step': 1, 'ensemble_nr': 1})\r\n\r\n@dask.delayed  # Remove this decorator to see the correct behavior\r\ndef load_grid_data(data, lat, lon, time_step):\r\n    grid_data = data.sel(latitude=lat, longitude=lon, step=time_step).temperature.values\r\n    return grid_data\r\n\r\n# Specify grid cell to load\r\ndata_grid = load_grid_data(data=test_data, lat=0, lon=0, time_step=start)\r\n\r\n# Trigger computation\r\ndata_grid.compute()\r\n\r\n```\r\n### Additional Information:\r\n- **Chunking by Latitude and Longitude:**  When chunking by `latitude` and `longitude`, the issue scales further as the number of grid cells increases. In global projections, this could lead to processing hundreds of thousands of grid cells, causing excessive memory usage and potentially crashing the process on systems with limited resources (e.g., HPC environments).\r\n- **Loading Entire Dataset to RAM:** If the entire dataset is loaded into RAM before applying the `dask.delayed` method, the computation proceeds correctly and at expected speeds, similar to when `dask.delayed` is not used.\r\n- **Lazy Loading:** The problem arises when using lazy loading (which is typically necessary for large datasets), where the excessive computation behavior is observed as described.\r\n\r\n### Request for Assistance:\r\n- I would appreciate insights into why `dask.delayed` is causing these excessive computations and any recommendations for modifying the code to avoid this issue.\r\n-  If this issue should be posted in a different repository, please advise accordingly.\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.7.0\r\n- Xarray version: 2024.6.0\r\n- Python version: 3.12.4\r\n- Operating System: Linux\r\n- Install method (conda, pip, source): conda-forge via mamba\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Hi, thanks for your report. You are doing nested dask computation here which is generally not a good idea. Why would you use delayed here? xarray is lazy as well, so nothing will happen until you call compute anyway. \r\n\r\nThe pattern you are using here creates one task group to open the files and then another one to actually select the data, these 2 tasks are completely disentangled from each other and dask can not see which parts of your dataset you actually need, because this information is hidden in delayed.\r\n\r\nIf you remove delayed, we can build a proper dask graph and restrict the execution to the parts that we actually need since we have this information beforehand.\r\n\r\nTLDR: Don't use delayed like this, just stay with plain xarray."
      },
      {
        "user": "Eis-ba-er",
        "body": "I thought that by using the delayed argument, it would be possible to create a task graph. The main idea is similar to the example in the [Dask delayed tutorial](https://docs.dask.org/en/stable/delayed-best-practices.html) (**Break up computations into many pieces**), where in the first step, data is loaded using delayed. After this, the data is analyzed in some way, and the result of this analysis is stored for each grid cell. Since the data is in a latitude and longitude grid, the idea was to parallelize the analysis. Once all the grid cells have been analyzed, the data will be saved in a NetCDF file with latitude, longitude, time-step and information about the performed analysis.\r\n\r\nThe goal was to create a function that would automatically compute all the necessary data and perform the desired analysis in a consistent manner. If this works, the plan is to hand the task graph to a Dask cluster on an HPC system to perform the analysis much faster.\r\n\r\nI’m searching for a fast way to perform such an analysis using a combination of Dask and xarray. When I run similar code using concurrent.futures, there’s a lot of unused core time. My goal is to reduce this inefficiency as much as possible, and that’s exactly where I thought dask.delayed could be the right tool."
      },
      {
        "user": "phofl",
        "body": "> I thought that by using the delayed argument, it would be possible to create a task graph.\r\n\r\nYes, this is generally true. But Xarray is doing all of this for you without you having to use delayed, they integrate with Dask directly. You can just remove futures / delayed and all of those when you work with chunked xarrays."
      }
    ]
  },
  {
    "issue_number": 11336,
    "title": "An inconsistency between the documentation of `dask.array.percentile` and code implementation",
    "author": "ParsifalXu",
    "state": "open",
    "created_at": "2024-08-21T07:07:55Z",
    "updated_at": "2025-06-16T02:13:16Z",
    "labels": [
      "array",
      "documentation",
      "needs attention"
    ],
    "body": "**Describe the issue**:\r\n\r\nAs mentioned in the parameter `method ` in the documentation of [`dask.array.percentile`](https://docs.dask.org/en/stable/generated/dask.array.percentile.html?highlight=percentile):\r\n\r\n> **method{‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}, optional**\r\nThe interpolation method to use when the desired percentile lies between two data points i < j. **Only valid for internal_method='dask'.**\r\n\r\nHowever, Corresponding part in the source code:\r\n```python\r\nif (\r\n    internal_method == \"tdigest\"\r\n    and method == \"linear\"\r\n    and (np.issubdtype(dtype, np.floating) or np.issubdtype(dtype, np.integer))\r\n):\r\n    from dask.utils import import_required\r\n    import_required(\r\n        \"crick\", \"crick is a required dependency for using the t-digest method.\"\r\n    )\r\n    name = \"percentile_tdigest_chunk-\" + token\r\n    dsk = {\r\n        (name, i): (_tdigest_chunk, key) for i, key in enumerate(a.__dask_keys__())\r\n    }\r\n    name2 = \"percentile_tdigest-\" + token\r\n    dsk2 = {(name2, 0): (_percentiles_from_tdigest, q, sorted(dsk))}\r\n```\r\nApparently, method is only valid for internal_method='dask', but also valid for internal_method='tdigest'. \r\n\r\nMaybe you can check it and improve the documentation.\r\n\r\n\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Yep this could use some clarification, are you interested in submitting a PR?"
      },
      {
        "user": "ParsifalXu",
        "body": "> Yep this could use some clarification, are you interested in submitting a PR?\r\n\r\nSure, I'm glad to.\r\n"
      }
    ]
  },
  {
    "issue_number": 11333,
    "title": "order: Run ordering test on distributed cluster and compare against local ordering",
    "author": "phofl",
    "state": "open",
    "created_at": "2024-08-20T10:39:30Z",
    "updated_at": "2025-06-16T02:13:16Z",
    "labels": [
      "needs attention",
      "dask-order"
    ],
    "body": "We've recently found a bug in how the distributed scheduler calls the ordering algorithm that caused us to use a bad ordering for common xarray map reduce patterns.\r\n\r\nSee https://github.com/dask/distributed/pull/8818 for the fix\r\n\r\nWe should check the order on the scheduler and compare it to a local ordering call to ensure that this doesn't happen again\r\n\r\nAn initial stab at this failed because equivalent nodes are scheduled somewhat randomly which makes a comparison hard\r\n\r\nSee https://github.com/dask/dask/pull/11310 which was reverted\r\n",
    "comments": []
  },
  {
    "issue_number": 11339,
    "title": "Suggesting updates on the doc of `dask.dataframe.read_sql_query`",
    "author": "ParsifalXu",
    "state": "open",
    "created_at": "2024-08-21T09:51:20Z",
    "updated_at": "2025-06-16T02:13:15Z",
    "labels": [
      "dataframe",
      "documentation",
      "needs attention"
    ],
    "body": "**Describe the issue**:\r\nThere is an incorrect description in the document of [`dask.dataframe.read_sql_query`](https://docs.dask.org/en/stable/generated/dask.dataframe.read_sql_query.html#dask.dataframe.read_sql_query). As mentioned in the description of the parameter `divisions`:\r\n\r\n>**divisions: sequence**\r\nValues of the index column to split the table by. If given, this will override `npartitions` and `bytes_per_chunk`.\r\n\r\nHowever, corresponding part in the code:\r\n```python\r\nif divisions is None:\r\n    if limits is None:\r\n        # calculate max and min for given index\r\n        q = sa.sql.select(\r\n            sa.sql.func.max(index), sa.sql.func.min(index)\r\n        ).select_from(sql.subquery())\r\n        minmax = pd.read_sql(q, engine)\r\n        maxi, mini = minmax.iloc[0]\r\n        dtype = minmax.dtypes[\"max_1\"]\r\n    else:\r\n        mini, maxi = limits\r\n        dtype = pd.Series(limits).dtype\r\n\r\n    if npartitions is None:\r\n        q = sa.sql.select(sa.sql.func.count(index)).select_from(sql.subquery())\r\n        count = pd.read_sql(q, engine)[\"count_1\"][0]\r\n        npartitions = (\r\n            int(round(count * bytes_per_row / parse_bytes(bytes_per_chunk))) or 1\r\n        )\r\n    if dtype.kind == \"M\":\r\n        divisions = methods.tolist(\r\n            pd.date_range(\r\n                start=mini,\r\n                end=maxi,\r\n                freq=\"%is\" % ((maxi - mini).total_seconds() / npartitions),\r\n            )\r\n        )\r\n        divisions[0] = mini\r\n        divisions[-1] = maxi\r\n    elif dtype.kind in [\"i\", \"u\", \"f\"]:\r\n        divisions = np.linspace(mini, maxi, npartitions + 1, dtype=dtype).tolist()\r\n    else:\r\n        raise TypeError(\r\n            'Provided index column is of type \"{}\".  If divisions is not provided the '\r\n            \"index column type must be numeric or datetime.\".format(dtype)\r\n        )\r\n```\r\n\r\nApparently, the correct description should be \"If **not** given, then this will override `npartition`\". Meanwhile, there doesn't seem to be any place to override `bytes_per_chunk`.\r\n\r\nCould you check it ?",
    "comments": [
      {
        "user": "phofl",
        "body": "Investigations are welcome!"
      },
      {
        "user": "benrutter",
        "body": "Hey both - thought I'd have a brief investigate, the logic on overriding `npartitions` looks sound (i.e. matching the docs) to me. If you *don't* give a `divisions` keyword, it's evaluate to `None`, so that whole `if divisions is None:` statement won't be evaluated.\r\n\r\nAfter which the return is this:\r\n\r\n```python\r\nreturn from_delayed(parts, meta, divisions=divisions)\r\n```\r\n(i.e. it uses divisions and ignores npartitions)\r\n\r\n\r\nIf you **don't** give a `divisions` argument, that loop will get executed, and specifically one of these two branches will run:\r\n\r\n```python\r\n        if dtype.kind == \"M\":\r\n            divisions = methods.tolist(\r\n                pd.date_range(\r\n                    start=mini,\r\n                    end=maxi,\r\n                    freq=\"%is\" % ((maxi - mini).total_seconds() / npartitions),\r\n                )\r\n            )\r\n            divisions[0] = mini\r\n            divisions[-1] = maxi\r\n        elif dtype.kind in [\"i\", \"u\", \"f\"]:\r\n            divisions = np.linspace(mini, maxi, npartitions + 1, dtype=dtype).tolist()\r\n```\r\nEssentially divisions is being set based on some logic making use of the `npartitions` keyword.\r\n\r\nLater on the return is based on the *now overwritten* divisions:\r\n```python\r\nreturn from_delayed(parts, meta, divisions=divisions)\r\n```\r\n\r\nWhich might look like it's using divisions, but it's using divisions the code has set using npartitions.\r\n\r\nSo in short:\r\n- If you give `divisions` that will be used (and `npartitions` will be ignored)\r\n- If you don't give `divisions` dask will set divisions based on the `npartitions` keyword.\r\n\r\nSame with `bytes_per_chunk` - that's only getting used on the branch executed if `divisions` is `None` (the \"overwrite\" logic is essentially, none of that branch getting executed)\r\n\r\n@ParsifalXu does that make sense from your point of view? (assuming it does and you're not seeing an issue, I think this issue is ok to close?)"
      }
    ]
  },
  {
    "issue_number": 11341,
    "title": "Improve how normalize_chunks selects chunk sizes if auto is given",
    "author": "phofl",
    "state": "open",
    "created_at": "2024-08-22T12:13:35Z",
    "updated_at": "2025-06-16T02:13:14Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "Specifying auto for a dimension when new chunks are selected like in rechunking allows us to choose a chunk size that we think is a good fit and efficient. \r\n\r\nThis is for now an issue to collect thoughts from others and feedback, I am pretty sure that I haven't found all suboptimal corner cases yet.\r\n\r\n\r\nAssume we have the following array:\r\n\r\n```\r\nimport dask.array as da\r\n\r\narr = da.random.random((5000, 300, 372), chunks=(600, 150, 172))\r\n```\r\n\r\nTriggering a Rechunk operation to -1 and auto for dimensions 2 and 3 results in the following chunks:\r\n\r\n```\r\nx = arr.rechunk((-1, \"auto\", \"auto\"))\r\n```\r\n\r\n<img width=\"747\" alt=\"Screenshot 2024-08-20 at 15 34 19\" src=\"https://github.com/user-attachments/assets/31e5c070-fc01-40be-af63-21fa06e457f5\">\r\n\r\n```normalize_chunks`` targets the 128MiB that is the default for auto pretty aggressively and creates a significantly more complex task graph than necessary if we would compromise a little bit on the in memory size of the chunks.\r\n\r\nChoosing 50 and (58, 57, 57) per input chunk would allows us to split every chunk into 3 pieces along every axis and thus avoid including the neighbouring chunks alongside this axis.\r\n\r\nI am pretty sure that there are better examples where this makes things very complex, but this should illustrate the general issue.\r\n\r\nThe proposal is to compromise a little bit on the chunk size side of things to create a simpler topology if possible.\r\n\r\n",
    "comments": [
      {
        "user": "davidhassell",
        "body": "Hi,\r\n\r\nSounds like good idea to me.\r\n\r\nI presume such a suggested change would only apply if the `previous_chunks` keyword to `normalize_chunks` was used - is that the idea? I.e.  `normalize_chunks(..., previous_chunks=None)` would still give the current \"strict\" auto behaviour.\r\n\r\nDavid"
      },
      {
        "user": "phofl",
        "body": "Yes, that is my current idea, but I haven't looked into the previous_chunks=None case very closely yet. I don't see a reason why this should be affected though. I mostly care about cases where we can ensure that the graph executes better and we need information about the previous chunks for that."
      },
      {
        "user": "djhoese",
        "body": "@phofl I'm having trouble adjusting some code in one of my libraries to meet the changes discussed here and I'm not sure I followed the above discussion. Could you help?\r\n\r\nI end up making a call like this to `normalize_chunks`:\r\n\r\n```python\r\ndask.array.core.normalize_chunks((\"auto\", -1), shape=(120, 5400), dtype=np.float32, previous_chunks=(40, -1))\r\n```\r\n\r\nIt used to produce:\r\n\r\n```\r\n((120,), (5400,))\r\n```\r\n\r\nBut now with current `main` I get:\r\n\r\n```\r\nFile ~/miniforge3/envs/satpy_py312_unstable/lib/python3.12/site-packages/dask/array/core.py:3171, in normalize_chunks(chunks, shape, limit, dtype, previous_chunks)\r\n   3168 chunks = tuple(\"auto\" if isinstance(c, str) and c != \"auto\" else c for c in chunks)\r\n   3170 if any(c == \"auto\" for c in chunks):\r\n-> 3171     chunks = auto_chunks(chunks, shape, limit, dtype, previous_chunks)\r\n   3173 if shape is not None:\r\n   3174     chunks = tuple(c if c not in {None, -1} else s for c, s in zip(chunks, shape))\r\n\r\nFile ~/miniforge3/envs/satpy_py312_unstable/lib/python3.12/site-packages/dask/array/core.py:3322, in auto_chunks(chunks, shape, limit, dtype, previous_chunks)\r\n   3320 for i, s in enumerate(shape):\r\n   3321     chunk_frequencies = frequencies(previous_chunks[i])\r\n-> 3322     mode, count = max(chunk_frequencies.items(), key=lambda kv: kv[1])\r\n   3323     if mode > 1 and count >= len(previous_chunks[i]) / 2:\r\n   3324         ideal_shape.append(mode)\r\n\r\nValueError: max() iterable argument is empty\r\n```\r\n\r\nWhat should I be doing? The inputs to `normalize_chunks` come from various places and its been a while since I've looked at this code so I may be missing something obvious, but like I said I didn't really understand the discussion above."
      }
    ]
  },
  {
    "issue_number": 2629,
    "title": "Implementation of scipy.integrate.simps",
    "author": "Joshuaalbert",
    "state": "open",
    "created_at": "2017-08-26T01:00:18Z",
    "updated_at": "2025-06-13T23:55:17Z",
    "labels": [
      "good first issue",
      "array"
    ],
    "body": "Are there plans to implement `scipy.integrate.simps`? \r\nIt is a pretty useful method that would easily gain from chunking (when integration is done down one axis of a multi-N array).",
    "comments": [
      {
        "user": "mrocklin",
        "body": "Is this something that you're interested in implementing @Joshuaalbert ?"
      },
      {
        "user": "Joshuaalbert",
        "body": "Yes, I would like to. Is there a developers guide I should read first?\n\nOn 2017-08-26 08:47, Matthew Rocklin wrote:\n> Is this something that you're interested in implementing @Joshuaalbert\n> [1] ?\n> \n> --\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub [2], or mute the\n> thread [3].\n> \n> \t*\n> \n> Links:\n> ------\n> [1] https://github.com/joshuaalbert\n> [2] https://github.com/dask/dask/issues/2629#issuecomment-325124147\n> [3]\n> https://github.com/notifications/unsubscribe-auth/AOHv-LN7biOizmP-tkrGUsMWfmGNLr38ks5scBP4gaJpZM4PDTAL\n\n"
      },
      {
        "user": "mrocklin",
        "body": "I apologize for the long delay here.  http://dask.pydata.org/en/latest/develop.html"
      }
    ]
  },
  {
    "issue_number": 1439,
    "title": "Dask doesn't play well with interrupts",
    "author": "alanhdu",
    "state": "closed",
    "created_at": "2016-08-03T07:30:43Z",
    "updated_at": "2025-06-13T12:26:11Z",
    "labels": [
      "scheduler"
    ],
    "body": "This is on `Python 3.5.2`, `dask 0.10.1`, and `jupyter-notebook 4.2.1`, all installed via conda on a 64-bit Ubuntu machine.\n\nIf you run the following piece of code:\n\n``` Python\nimport json\n\nfrom dask.diagnostics import ProgressBar\nimport dask.bag as db\n\nj = json.dumps({\"a\": 1, \"b\": 1})\n\nfor i in range(8):\n    data = [j for _ in range(10 ** i)]\n    bag = db.from_sequence(data, npartitions=4).map(json.loads)\n    with ProgressBar():\n        db.zip(bag.pluck(\"a\"), bag.pluck('b')).count().compute()\n```\n\nAnd try to do a keyboard interrupt in the middle, sometimes dask will flip out and just refuse to exit (no matter how many `CTRL-C`s and `CTRL-D`s you press. See https://asciinema.org/a/2yqxbhn1patwenwy024m2316l for a video.\n\nThis is especially annoying on a Jupyter notebook. Sometimes, dask will mysteriously keep printing out the progress bar from the last session, even after restarting the kernel (!).\n\n_EDIT_: When this happens, dask will also leave around `python` processes still running. See https://asciinema.org/a/8x1liiyuiw7910mugj9tscher for an example (particularly the extra `python` process at the end).\n",
    "comments": [
      {
        "user": "alanhdu",
        "body": "Notably, I can only reproduce this in the python repl (and ipython). When I run it as a script (`python test.py`), 3 `KeyboardInterrupt`s are enough to exit cleanly without leaving phantom python processes anywhere (see https://asciinema.org/a/eitz5acubx6ot7a9cmm1sdz2l)\n"
      },
      {
        "user": "alanhdu",
        "body": "...contrary to my previous assumption, this doesn't have anything to do with the progressbar. I just thought it did because the phantom progressbar on a jupyter notebook was the most obvious indicator something was wrong.\n\nEverything above can be replicated without the `with ProgressBar():` line.\n\nSorry for the noise.\n"
      },
      {
        "user": "rmax",
        "body": "I noticed that behavior when using `dask` in a jupyter notebook: the progressbar running in background after a stop and also orphan processes even after killing all kernels.\n\nFWIW, I used `ProgressBar().register()` at the beginning of the notebook.\n"
      }
    ]
  },
  {
    "issue_number": 11990,
    "title": "AssertionError during compute for groupby",
    "author": "dbalabka",
    "state": "open",
    "created_at": "2025-06-13T07:07:10Z",
    "updated_at": "2025-06-13T07:12:16Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nFor LocalCluster the following assertion error appears that isn't self descriptive. I have faced this issue few times. \n```\nTraceback (most recent call last):\n  File \"/home/dmitrybalabka/src/test_cli.py\", line 42, in <module>\n    test_filter_first_empty_rows_basic()\n  File \"/home/dmitrybalabka/src/test_cli.py\", line 38, in test_filter_first_empty_rows_basic\n    result_ddf.compute().sort_values(['ID', 'time']).reset_index(drop=True)\n    ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/.venv/lib/python3.11/site-packages/dask_expr/_collection.py\", line 479, in compute\n    out = out.optimize(fuse=fuse)\n          ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/.venv/lib/python3.11/site-packages/dask_expr/_collection.py\", line 594, in optimize\n    return new_collection(self.expr.optimize(fuse=fuse))\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/.venv/lib/python3.11/site-packages/dask_expr/_expr.py\", line 93, in optimize\n    return optimize(self, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/.venv/lib/python3.11/site-packages/dask_expr/_expr.py\", line 3118, in optimize\n    return optimize_until(expr, stage)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/.venv/lib/python3.11/site-packages/dask_expr/_expr.py\", line 3089, in optimize_until\n    expr = optimize_blockwise_fusion(expr)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/.venv/lib/python3.11/site-packages/dask_expr/_expr.py\", line 3286, in optimize_blockwise_fusion\n    expr, done = _fusion_pass(expr)\n                 ^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/.venv/lib/python3.11/site-packages/dask_expr/_expr.py\", line 3251, in _fusion_pass\n    dep.npartitions == root.npartitions or next._broadcast_dep(dep)\n    ^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/.venv/lib/python3.11/site-packages/dask_expr/_expr.py\", line 397, in npartitions\n    return len(self.divisions) - 1\n               ^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/.pyenv/versions/3.11.7/lib/python3.11/functools.py\", line 1001, in __get__\n    val = self.func(instance)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/.venv/lib/python3.11/site-packages/dask_expr/_expr.py\", line 381, in divisions\n    return tuple(self._divisions())\n                 ^^^^^^^^^^^^^^^^^\n  File \"/home/dmitrybalabka/src/venv/lib/python3.11/site-packages/dask_expr/_expr.py\", line 529, in _divisions\n    assert arg.divisions == dependencies[0].divisions\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n```\n\n**Minimal Complete Verifiable Example**:\n\n```shell\npython test.py\n```\n```python\nimport numpy as np\nimport dask.dataframe as dd\nimport pandas as pd\nfrom datetime import datetime\n\ndef filter_first_empty_rows(\n    data: dd.DataFrame,\n    col: str,\n    time_col: str,\n    id_col: str,\n    train_till: datetime,\n) -> dd.DataFrame:\n    \n    def _filter_group(df: pd.DataFrame) -> pd.DataFrame:\n        first_date = df.loc[~df[col].isna(), time_col].min()\n        mask = (df[time_col] >= first_date) | (df[time_col] > train_till)\n        return df[mask]\n\n    return data.groupby(id_col, group_keys=False).apply(_filter_group, meta=data).reset_index(drop=True)\n\n\ndef test_filter_first_empty_rows_basic():\n    # Create a dataframe with two IDs:\n    # For ID 1: first non-null price is on 2021-01-02, so any row before that should be dropped;\n    # For ID 2: all price values are NaN so only rows after train_till are kept.\n    data = pd.DataFrame({\n        'ID': [1, 1, 1, 2, 2, 2],\n        'time': pd.to_datetime([\n            '2021-01-01', '2021-01-02', '2021-01-03',\n            '2021-01-01', '2021-01-02', '2021-01-03'\n        ]),\n        'price': [np.nan, 100, np.nan, np.nan, np.nan, np.nan]\n    })\n    ddf = dd.from_pandas(data, npartitions=1)\n    train_till = datetime(2021, 1, 2)\n    \n    result_ddf = filter_first_empty_rows(ddf, 'price', 'time', 'ID', train_till)\n    result_ddf.compute().sort_values(['ID', 'time']).reset_index(drop=True)\n    \n\nif __name__ == \"__main__\":\n    test_filter_first_empty_rows_basic()\n    print(\"Test passed!\")\n```\n\n**Environment**:\n\n- Dask version: 2024.12.1\n- Python version: 3.11\n- Operating System: WSL\n- Install method (conda, pip, source): poetry\n",
    "comments": [
      {
        "user": "dbalabka",
        "body": "The issue is quiet silly. If I pass `meta` param in `apply()` function as following `meta=data._meta` instead of `meta=data` problem dissapears:\n```\nreturn data.groupby(id_col, group_keys=False).apply(_filter_group, meta=data._meta).reset_index(drop=True)\n```"
      }
    ]
  },
  {
    "issue_number": 11986,
    "title": "da.quantile: AttributeError: 'generator' object has no attribute 'ndim'",
    "author": "m-albert",
    "state": "closed",
    "created_at": "2025-06-12T13:30:27Z",
    "updated_at": "2025-06-12T18:43:13Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nI get `AttributeError: 'generator' object has no attribute 'ndim'` when running `da.quantile` on dask arrays.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\nx=da.ones(5)\nda.quantile(x, 0.5)\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.5.1\n- Python version: 3.11\n- Operating System: macOS\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "Thanks for the report. Easy-ish workaround for now is to use `axis=0`:\n\n```python\nIn [3]: da.quantile(da.arange(5), 0.5, axis=0).compute()\nOut[3]: np.float64(2.0)\n```\n\nI'll have a PR up shortly (the builtin `any` is being shadowed by `dask.array.reductions.any` in the same module)"
      },
      {
        "user": "m-albert",
        "body": "Hey @TomAugspurger, thanks for your quick response 🙏\n\nI was just working on this and have a PR ready referencing `builtins.any` in a few lines and adding a quick test.\n\nIn case you're not working on this yet I'd open the PR."
      },
      {
        "user": "TomAugspurger",
        "body": "Great thanks! Feel free to ping me when you open it."
      }
    ]
  },
  {
    "issue_number": 11989,
    "title": "da.percentile and np.percentile can have different output shapes",
    "author": "m-albert",
    "state": "open",
    "created_at": "2025-06-12T16:02:34Z",
    "updated_at": "2025-06-12T16:05:48Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nIt seems that `da.percentile` and `np.percentile` return arrays of different shapes. Also confusingly `da.percentile` and `da.nanpercentile` differ in their outputs (and implementations).\n\nFound in the context of https://github.com/dask/dask/pull/11988.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\nimport numpy as np\n\nx_np = np.ones(3)\nx_da = da.from_array(x_np)\n\nnp_percentile_result = np.percentile(x_np, 50) # np.array(1.)\nnp_nanpercentile_result = np.nanpercentile(x_np, 50) # np.array(1.)\nda_percentile_result = da.percentile(x_da, 50).compute() # np.array(1.)\nda_nanpercentile_result = da.nanpercentile(x_da, 50).compute() # np.array([1.])\n\n# ok\nassert np_percentile_result.shape == np_nanpercentile_result.shape\n\n# ok\nassert np_percentile_result.shape == da_nanpercentile_result.shape\n\n# fails\nassert np_percentile_result.shape == da.percentile_result.shape\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.5.1\n",
    "comments": []
  },
  {
    "issue_number": 11985,
    "title": "`dask.tensordot` brings different results with `numpy.tensordot`",
    "author": "apiqwe",
    "state": "open",
    "created_at": "2025-06-12T09:16:40Z",
    "updated_at": "2025-06-12T09:16:49Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nI found that `dask.tensordot` will bring different results with `numpy.tensordot` as follow.\nI think this may be a bug in `dask`.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.array as da\n\nprint(np.tensordot(np.ones((2,2),bool),np.ones((2,2),bool),1))\nprint(da.tensordot(da.ones((2, 2), bool), da.ones((2, 2), bool), 1).compute())\n```\nOutput:\n```bash\n[[ True  True]\n [ True  True]]\n[[2. 2.]\n [2. 2.]]\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.10.0 \n- Operating System: Linux 6.11.0-25-generic  Ubuntu 24.04.1 x86_64  GNU/Linux\n- Install method (conda, pip, source): pip\n- Numpy version: 2.2.4\n\n",
    "comments": []
  },
  {
    "issue_number": 11984,
    "title": "`dask.take` brings different results with `numpy.take`",
    "author": "apiqwe",
    "state": "open",
    "created_at": "2025-06-12T09:14:25Z",
    "updated_at": "2025-06-12T09:14:36Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nI found that `dask.take` will bring different results with `numpy.take` as follow.\nI think this may be a bug in `dask`.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.array as da\n\nprint(np.take([10,20],[True,False]))\nprint(da.take(da.array([10, 20]), da.array([True, False])).compute())\n```\nOutput:\n```bash\n[20 10]\n[10]\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.10.0 \n- Operating System: Linux 6.11.0-25-generic  Ubuntu 24.04.1 x86_64  GNU/Linux\n- Install method (conda, pip, source): pip\n- Numpy version: 2.2.4\n\n",
    "comments": []
  },
  {
    "issue_number": 11968,
    "title": "`dask.array.left_shift` brings different results with `numpy.left_shift`",
    "author": "apiqwe",
    "state": "open",
    "created_at": "2025-05-27T07:07:30Z",
    "updated_at": "2025-06-11T23:06:45Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nI found that `dask.array.left_shift` will bring different results with `numpy.left_shift` as follow.\nI think this may be a bug in `dask`.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.array as da\n\nprint(np.left_shift(5,2,where=np.array([1,0],dtype=bool)))\nprint(da.left_shift(5, 2, where=da.array([1, 0], dtype=bool)).compute())\n```\nOutput:\n```\n[                 20 8768622988392019495]\n[20  0]\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.10.0 \n- Operating System: Linux 6.11.0-25-generic  Ubuntu 24.04.1 x86_64  GNU/Linux\n- Install method (conda, pip, source): pip\n- Numpy version: 2.2.4\n",
    "comments": [
      {
        "user": "zenocross",
        "body": "This doesn't seem to be a Dask bug. \n\n[Numpy Left Shift Docs](https://numpy.org/doc/stable/reference/generated/numpy.left_shift.html)\nThis appears to be an issue with using numpy left_shift w/ the where parameter specified. According to the numpy docs you'll have to set an out var to allocate the output correctly. \n\nnp.left_shift(5, 2, where=np.array([1,0], dtype=bool), out=np.zeros(2, dtype=int))"
      }
    ]
  },
  {
    "issue_number": 11967,
    "title": "`dask.array.zeros` brings error results",
    "author": "apiqwe",
    "state": "open",
    "created_at": "2025-05-27T07:03:42Z",
    "updated_at": "2025-06-11T22:31:24Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nI found that `dask.array.zeros` will bring error results as follow.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.array as da\n\nprint(np.zeros(5, like=np.array([1,2])))\nprint(da.zeros(5, like=da.array([1, 2])).compute())\n```\nOutput:\n```\n[0. 0. 0. 0. 0.]\n<function _broadcast_trick_inner at 0x70c3caa3c280>\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.10.0 \n- Operating System: Linux 6.11.0-25-generic  Ubuntu 24.04.1 x86_64  GNU/Linux\n- Install method (conda, pip, source): pip\n- Numpy version: 2.2.4\n\n",
    "comments": [
      {
        "user": "zenocross",
        "body": "I'm pretty sure you're expected to do this instead for the zeros implementation to convert the numpy array to a dask array first. \nda.zeros(5, dtype=da.array([1, 2]).dtype).compute()\n\nThough I believe the optimal approach would be to just use zeros_like\nda.zeros_like(da.array([1, 2]), shape=5).compute()"
      }
    ]
  },
  {
    "issue_number": 11850,
    "title": "`dask.array.from_array` Loads Entire `memmap` into Memory",
    "author": "izkgao",
    "state": "open",
    "created_at": "2025-03-26T05:07:54Z",
    "updated_at": "2025-06-10T07:59:59Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\n---\n\nI encountered an issue when using `dask.array.from_array` with a NumPy `memmap` in Dask version 2025.2.0. Instead of lazily loading the file as expected, the function loads the entire array into memory.  \n\nThis issue was previously reported in #11152 and fixed in version 2024.6.0 via #11161. However, after testing different versions, I found that the problem reappeared in 2024.8.1 and persists in 2025.3.0.  \n\nUpon investigating the changes, I found this line in [[#11320](https://github.com/dask/dask/pull/11320/files#r1719649338)](https://github.com/dask/dask/pull/11320/files#r1719649338):  \n\n```python\nhash_buffer_hex(np.ascontiguousarray(mm))\n```  \n\nWhile `np.ascontiguousarray(mm)` does not trigger file loading, `hash_buffer_hex` does, which likely causes the issue.  \n\n\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\nimport numpy as np\ndata = np.memmap(file, dtype=dtype, mode='r', offset=offset, shape=shape)\ndarr = da.from_array(data)\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: after 2024.8.1 (included)\n- Python version: 3.12\n- Operating System: Linux\n- Install method (conda, pip, source): conda",
    "comments": [
      {
        "user": "izkgao",
        "body": "I realized that I should have used [this example](https://docs.dask.org/en/latest/array-creation.html#memory-mapping) from the documentation instead of the code above. The documented example works as expected in Dask version 2025.3.0, avoiding loading the entire array into memory. To scale this approach for higher dimensions, I made the necessary modifications:\n\n```python\nimport numpy as np\nimport dask\nimport dask.array as da\nfrom itertools import product\n\n\n@dask.delayed\ndef mmap_load_chunk(filename, shape, dtype, offset, sl):\n    '''\n    Memory map the given file with overall shape and dtype and return a slice\n    specified by :code:`sl`.\n    Parameters\n    ----------\n    filename : str\n    shape : tuple\n        Total shape of the data in the file\n    dtype:\n        NumPy dtype of the data in the file\n    offset : int\n        Skip :code:`offset` bytes from the beginning of the file.\n    sl:\n        Object that can be used for indexing or slicing a NumPy array to\n        extract a chunk\n    Returns\n    -------\n    numpy.memmap or numpy.ndarray\n        View into memory map created by indexing with :code:`sl`,\n        or NumPy ndarray in case no view can be created using :code:`sl`.\n    '''\n    data = np.memmap(filename, mode='r', shape=shape, dtype=dtype, offset=offset)\n    return data[sl]\n\n\ndef mmap_dask_array(filename, shape, dtype, offset=0, chunks=\"auto\"):\n    '''\n    Create a Dask array from raw binary data in :code:`filename`\n    by memory mapping.\n    This method creates a dask array with the same chunking pattern as\n    da.ones(shape, dtype=dtype), and works for arrays of any dimensionality.\n    \n    Parameters\n    ----------\n    filename : str\n    shape : tuple\n        Total shape of the data in the file\n    dtype:\n        NumPy dtype of the data in the file\n    offset : int, optional\n        Skip :code:`offset` bytes from the beginning of the file.\n    chunks : int, tuple\n        How to chunk the array\n\n    Returns\n    -------\n    dask.array.Array\n        Dask array matching :code:`shape` and :code:`dtype`, backed by\n        memory-mapped chunks with the same chunking as da.ones(shape, dtype=dtype).\n    '''\n    # Create a sample array to get the default chunking\n    sample_array = da.empty(shape, dtype=dtype, chunks=chunks)\n    chunks = sample_array.chunks\n    \n    # Special case for 0-dimensional arrays\n    if len(shape) == 0:\n        delayed_chunk = mmap_load_chunk(filename, shape, dtype, offset, ())\n        return da.from_delayed(delayed_chunk, shape=(), dtype=dtype)\n    \n    # Calculate the chunk indices for each dimension\n    chunk_indices = []\n    for dim_chunks in chunks:\n        indices = []\n        start = 0\n        for size in dim_chunks:\n            indices.append((start, start + size))\n            start += size\n        chunk_indices.append(indices)\n    \n    # Function to build a block at specific chunk indices\n    def build_block(idx_tuple):\n        # Create slices from the chunk indices\n        slices = tuple(slice(start, stop) for start, stop in idx_tuple)\n        \n        # Calculate the shape of this chunk\n        chunk_shape = tuple(stop - start for start, stop in idx_tuple)\n        \n        # Create a delayed chunk\n        delayed_chunk = mmap_load_chunk(filename, shape, dtype, offset, slices)\n        \n        # Create a dask array from the delayed chunk\n        return da.from_delayed(delayed_chunk, shape=chunk_shape, dtype=dtype)\n    \n    # Create a nested list structure that matches the dimensionality of chunks\n    # This is a recursive function to handle any number of dimensions\n    def create_nested_blocks(dimension=0, indices=()):\n        if dimension == len(chunks):\n            # We've reached the right depth, build the block\n            return build_block(indices)\n        else:\n            # Create a list of blocks for this dimension\n            blocks_in_dim = []\n            for idx in chunk_indices[dimension]:\n                new_indices = indices + (idx,)\n                blocks_in_dim.append(create_nested_blocks(dimension + 1, new_indices))\n            return blocks_in_dim\n    \n    # Create the nested structure of blocks\n    nested_blocks = create_nested_blocks()\n    \n    # Combine all blocks using da.block\n    return da.block(nested_blocks)\n```\n\nI believe this issue can be closed. It would also be beneficial to update the example code in the documentation to reflect these modifications. Thank you."
      },
      {
        "user": "fjetter",
        "body": "Are you interested in contributing to the documentation to improve this section?"
      },
      {
        "user": "izkgao",
        "body": "Sorry for getting back late.\nYes, I am interested in doing that. I will try to modify the documentation and create a PR."
      }
    ]
  },
  {
    "issue_number": 4368,
    "title": "Sort dask array",
    "author": "Pierre-Bartet",
    "state": "open",
    "created_at": "2019-01-10T13:41:00Z",
    "updated_at": "2025-06-09T22:24:29Z",
    "labels": [
      "array"
    ],
    "body": "Unless I missed it, there is no way to sort or argsort dask arrays, is it a current work in progress ?\r\n",
    "comments": [
      {
        "user": "mrocklin",
        "body": "I don't know of any current work on this, but it would be in scope if\nanyone wants to start work on it.\n\nOn Thu, Jan 10, 2019 at 5:41 AM Pierre-Bartet <notifications@github.com>\nwrote:\n\n> Unless I missed it, there is no way to sort or argsort dask arrays, is it\n> a current work in progress ?\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/dask/dask/issues/4368>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AASszFoITk3JhTA4Fti9L8LcoDD8BJ5zks5vB0LtgaJpZM4Z5hQY>\n> .\n>\n"
      },
      {
        "user": "shoyer",
        "body": "The easy version of this would be to sort along single dimensions only when they use a single chunk, and otherwise error. The hard version would be to support sorting even in the multiple chunk case. I'd be glad to see either of these in dask.array."
      },
      {
        "user": "Pierre-Bartet",
        "body": "Isn't it done somewhere in dask.dataframe.set_index ? I'll have a look."
      }
    ]
  },
  {
    "issue_number": 11980,
    "title": "`dask.array.choose()` fails when the choice is trivial",
    "author": "rossjjennings",
    "state": "open",
    "created_at": "2025-06-09T03:53:49Z",
    "updated_at": "2025-06-09T17:03:11Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nIf `dask.array.choose()` is used in the trivial case where there is only one choice, it returns `NotImplemented`. This behavior differs from Numpy's version, which keeps working just fine in this case (see example below).\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.array as da\n\nindices = np.array([0, 0, 0, 0])\nchoices = (np.array([10., 20., 30., 40.]),)\nprint(np.choose(indices, choices))\n\nindices = da.array([0, 0, 0, 0])\nchoices = (da.array([10., 20., 30., 40.]),)\nprint(da.choose(indices, choices))\n```\nThe Numpy portion prints the correct result (`[10., 20., 30., 40.]`), but the Dask portion prints `NotImplemented`.\n\n**Anything else we need to know?**:\nThis example may seem useless, but this originally came up in a context where the number of choices is dynamic, and it is important to retain the correct behavior when there is only one choice.\n\n**Environment**:\n\n- Dask version: 2025.5.1\n- Python version: Python 3.12\n- Operating System: Ubuntu 24.04\n- Install method (conda, pip, source): conda\n",
    "comments": [
      {
        "user": "EduardAkhmetshin",
        "body": "I've looked into this issue and here is what I found:\n`NotImplemented` error is thrown by dtype inference method:\nhttps://github.com/dask/dask/blob/720efd2c7b1224b91775042119e906df3c18101c/dask/array/core.py#L4959-L4978\nThis code creates empty (uninitialized) arrays of the same shape as the function arguments and calls the function to infer the output dtype. However, in this case, the function can raise an error if the indices argument contains a (random) value that's bigger than the size of choices. See the description of `mode` argument in here: https://numpy.org/doc/2.1/reference/generated/numpy.choose.html \nThere are a few ways to fix this. One is to infer the common dtype for choices, which would be equal to the output dtype. I tried using `numpy.common_type`, but I got an error about it not being supported in Dask. I don't know how to fix that.\nAnother way is to pass another function to `apply_infer_dtype` specifically for type inference. I can open a PR with that change in a short time. Please let me know if it makes sense or there is a better solution. Thanks."
      }
    ]
  },
  {
    "issue_number": 11832,
    "title": "ValueError: Index contains duplicate entries when using unstack() after groupby().sum() on Dask DataFrame with categorical column",
    "author": "DixQ",
    "state": "open",
    "created_at": "2025-03-18T14:23:19Z",
    "updated_at": "2025-06-09T08:12:35Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nI encountered an issue while performing a groupby aggregation followed by unstack() on a Dask DataFrame that has categorical column. The operation results in a ValueError stating that the `Index contains duplicate entries, cannot reshape.`\nIf reduce number of partitions or change column to string the step can be finished successfully. Here is example to reproduce the issue\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nfrom dask.distributed import Client\n# Start a local Dask cluster\nclient = Client()\n\n# Define parameters\nnum_rows = 45\nnum_partitions = 20\nmodels = ['A', 'B', 'C']\nstart_date = datetime(2025, 3, 1)\nnum_days = 45\n\n# Generate date range\ndates = [start_date + timedelta(days=i) for i in range(num_days)]\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'Date': np.tile(dates, len(models)), \n    'Model': np.repeat(models, num_days), \n    'Sales': np.random.randint(0, 101, num_days * len(models)) \n})\nddf = (\n    dd\n    .from_pandas(df, npartitions=num_partitions)\n    .astype({\n        \"Date\": 'timestamp[ns][pyarrow]',\n        \"Model\": 'string[pyarrow]',\n        \"Sales\": 'int64[pyarrow]',\n    })\n)\n\n# Categorize column\nddf = ddf.categorize('Model')\n\n# Error\nddf.loc[:, ['Date', 'Sales', 'Model']].groupby(['Date', 'Model']).sum('Sales').compute().sort_values('Date').unstack()\n```\n```\nIndex contains duplicate entries, cannot reshape.\n```\n\nNo error if dtype of Model is changed to string:\n```\nddf.loc[:, ['Date', 'Sales', 'Model']].astype({\"Model\": \"string\"}).groupby(['Date', 'Model']).sum('Sales').compute().sort_values('Date').unstack()\n```\n\nNo error if # of partitions is reduced to 10\n```\nddf.loc[:, ['Date', 'Sales', 'Model']].repartition(npartitions=10).groupby(['Date', 'Model']).sum('Sales').compute().sort_values('Date').unstack()\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2024.12.1\n- Python version: 3.11.10 (main, Dec 15 2024, 15:28:55) [GCC 11.4.0]\n- Operating System: Linux 5.15.153.1-microsoft-standard-WSL2\n- Install method (conda, pip, source): poetry \n",
    "comments": [
      {
        "user": "dbalabka",
        "body": "@DixQ, I looks like that the problem might be related to how Dask operates with categorical types with too many partitions. Here is the problem that I have face with #11830"
      }
    ]
  },
  {
    "issue_number": 11342,
    "title": "Better chunk size value for chunks=auto setting",
    "author": "phofl",
    "state": "open",
    "created_at": "2024-08-22T12:16:42Z",
    "updated_at": "2025-06-09T02:13:27Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "The current default is 128MiB, this seems rather small for a NumPy array, most operations are fairly short and thus are straining the scheduler.\r\n\r\nI would like to collect feedback on raising this value to around 250-500Mib. Advantages are:\r\n\r\n- longer task durations, using the resources more efficient at larger scale\r\n- smaller task graphs\r\n\r\nThe downside is obviously that the average memory use would increase, so this doesn't seem to be a good choice for every infrastructure. A good compromise would be to take information from the client into account and choose this value based on the size of the workers, but this is probably something we wouldn't want to do before we have array-expr",
    "comments": []
  },
  {
    "issue_number": 11346,
    "title": "KilledWorker (exceeded 95% memory budget) with new optimizer",
    "author": "noreentry",
    "state": "open",
    "created_at": "2024-08-23T22:43:40Z",
    "updated_at": "2025-06-09T02:13:26Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "There is a problem running this query (optimization?) with dask version\r\n```\r\npython                    3.12.3          hab00c5b_0_cpython    conda-forge\r\ndask                      2024.8.1           pyhd8ed1ab_0    conda-forge\r\ndask-core                 2024.8.1           pyhd8ed1ab_0    conda-forge\r\ndask-expr                 1.1.11             pyhd8ed1ab_0    conda-forge\r\n```\r\nGet multiple\r\n`distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44453 (pid=1024943) exceeded 95% memory budget. Restarting...`\r\nerrors and\r\n```KilledWorker: Attempted to run task ('decomposablegroupbyaggregation-tree-a55b66ddda118a5af05205196d365d02', 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:44853. Inspecting worker logs is often a good next step to diagnose what went wrong.```\r\nfinally\r\n\r\nThe query runs without errors with dask version\r\n```\r\npython                    3.11.8          hab00c5b_0_cpython    conda-forge\r\ndask                      2024.2.1           pyhd8ed1ab_0    conda-forge\r\ndask-core                 2024.2.1           pyhd8ed1ab_1    conda-forge\r\n```\r\n\r\nHere is the code\r\n```python\r\nimport dask.dataframe as dd\r\nimport dask.distributed\r\nimport os\r\n\r\npath1  = '/tmp/bad001/'\r\ntmpdir = '/tmp/'\r\n\r\nwith dask.distributed.Client(protocol='tcp://', n_workers=10, threads_per_worker=2, memory_limit='30GB', processes=True, local_directory=os.path.join(tmpdir, '.dask'), dashboard_address=':8787'):\r\n    df1 = dd.read_parquet(path1, calculate_divisions=True)\r\n    print(df1.head(5))\r\n    df2 = df1.assign(ts1=lambda df: df.index).assign(ts2=lambda df: (df['ts1'] - 1) // 1000 * 1000 + 1000).groupby('ts2', sort=True).aggregate({\r\n        'flgs': 'max', 'val1': ['last', 'max', 'min'], 'val2': ['last', 'max', 'min'], 'val3': 'max', 'val4': 'min', 'val5': 'max', 'val6': 'min', 'ts1': 'last'\r\n    }, split_out=len(df1.divisions) // 10)\r\n    df2.columns = ['_'.join(cn) for cn in df2.columns]\r\n    df2 = df2.assign(ts0=lambda df: df.index).assign(ts0=lambda df: df['ts0'] - 1000) \\\r\n        .assign(flgs_t0=0).assign(flgs_t1=lambda df: df['flgs_t0'].shift(1).fillna(1).astype('int64'), flgs_t2=lambda df: df['flgs_t0'].shift(-1).fillna(2).astype('int64')) \\\r\n        .assign(flgs0=lambda df: df['flgs_max'] + df['flgs_t1'] + df['flgs_t2']).drop(columns=['flgs_max', 'flgs_t0', 'flgs_t1', 'flgs_t2']) \\\r\n        .assign(flgs1=lambda df: df['flgs0'].shift( 1).fillna(1).astype('int64').clip(upper=1) * 1,\r\n                flgs2=lambda df: df['flgs0'].shift(-1).fillna(1).astype('int64').clip(upper=1) * 2)\r\n    df2 = df2.loc[(df2['flgs0'] == 0) & ((df2['flgs1'] == 0) | (df2['flgs2'] == 0))]\\\r\n        .assign(flgs=lambda df: df['flgs1'] + df['flgs2'])\\\r\n        .drop(columns=['flgs0', 'flgs1', 'flgs2'])\r\n    df2.to_parquet(os.path.join(tmpdir, 'badout'), overwrite=True, compression='gzip')\r\n```\r\n\r\nand dataset is at\r\nhttps://drive.google.com/file/d/11lGko9pGsUTPvg6Zb-jHAsuAJWSK1WGN/view?usp=sharing\r\n\r\nThe server has more than 300GB RAM and 18 cores. ubuntu24, python 3.11/3.12, conda-forge",
    "comments": [
      {
        "user": "phofl",
        "body": "Hi, thanks for your report.\r\n\r\nQuick question about your cluster spec, your workers seem small, I would recommend providing a bit more memory, i.e. 4 GB per thread is what works well\r\n\r\nI strongly suspect that this is because of the groupby aggregation. Setting split_out only has an effect if you don't specify sort in the groupby call. Could you try removing the sort to see if things run through?"
      },
      {
        "user": "noreentry",
        "body": "1. with memory_limit='30GB' it has 15Gb per thread and still runs out of memory.\r\n2. removing `sort=True` in `groupby('ts2', sort=True).aggregate({` solved the problem (but `sort=True` is default for groupby in docs?)\r\nwith memlimit of 15Gb per thread doesn't the planner have enough memory to spill tasks to disk?"
      },
      {
        "user": "phofl",
        "body": "> with memory_limit='30GB' it has 15Gb per thread and still runs out of memory.\r\n\r\nSorry that's on me, I should have read the docs. I assumed it was for the whole cluster....\r\n\r\n> with memlimit of 15Gb per thread doesn't the planner have enough memory to spill tasks to disk?\r\n\r\nWe pull the whole DataFrame into a single partition for ``sort=True``, this is a pandas DataFrame that we have to hold in memory. We could spill, if you would have more than one partition.\r\n\r\n> but sort=True is default for groupby in docs?\r\n\r\nThis is one of the inconsistencies with pandas docs that are mentioned unfortunately. We should probably create our own doctoring here instead of importing from pandas. Dask treats sort basically as not given. We will sort if we can, but specifying split_out takes precedence. "
      }
    ]
  },
  {
    "issue_number": 11349,
    "title": "map_overlap passes wrong block_info[:]['array-location']",
    "author": "bnavigator",
    "state": "open",
    "created_at": "2024-08-27T13:55:32Z",
    "updated_at": "2025-06-09T02:13:25Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\n`dask.array.map_overlap()` reports the wrong position in the full array to the mapping function. It should report the indices of the original array not after the addition of the overlaps.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> from dask import array as da\r\n>>> x = da.from_array(np.arange(1000), chunks=100)\r\n>>> x.map_overlap(lambda b, block_info=None: np.array([[b[0], b[-1]]]).T,\r\n...               depth=5, dtype=int, chunks=(2,1), new_axis=0)\r\ndask.array<_trim, shape=(2, 10), dtype=int64, chunksize=(2, 1), chunktype=numpy.ndarray>\r\n>>> _.compute()\r\narray([[  0,  95, 195, 295, 395, 495, 595, 695, 795, 895],\r\n       [104, 204, 304, 404, 504, 604, 704, 804, 904, 999]])\r\n>>> x.map_overlap(lambda b, block_info=None: np.array(block_info[0]['array-location']).T,\r\n...               depth=5, dtype=int, chunks=(2,1), new_axis=0)\r\ndask.array<_trim, shape=(2, 10), dtype=int64, chunksize=(2, 1), chunktype=numpy.ndarray>\r\n>>> _.compute()\r\narray([[   0,  105,  215,  325,  435,  545,  655,  765,  875,  985],\r\n       [ 105,  215,  325,  435,  545,  655,  765,  875,  985, 1090]])\r\n```\r\n\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.8.1\r\n- Python version: 3.11.9\r\n- Operating System: Linux\r\n- Install method (conda, pip, source): pip\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Thx! Yes this should change, the fix is not trivial unfortunately "
      },
      {
        "user": "rlebourd",
        "body": "I ran into this issue as well. It is very unfortunate that we cannot depend on the `”array-location”` field within `block_info[None]`. \r\n\r\nAs far as I can tell, the `”chunk-location”` field within `block_info[None]` does, however, correctly identify the relative index of the chunk in the original, chunked array. Using the `”chunk-location”` field within `block_info[None]`, the `chunks` field of the Dask array passed to `map_overlap`, the depth parameter passed to `map_overlap`, and the dimensions of the chunk received by the callback function invoked by `map_overlap`, it seems to me that you can reliably compute the chunk’s position in the original array.\r\n\r\nI’m not deeply familiar with the codebase, but that is a workaround for people who run into this issue until this problem is fixed."
      }
    ]
  },
  {
    "issue_number": 11379,
    "title": "different `run_spec` between consecutive calls to `update_graph` | zarr-formatted xarray",
    "author": "templiert",
    "state": "open",
    "created_at": "2024-09-07T11:39:19Z",
    "updated_at": "2025-06-09T02:13:24Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\r\n\r\nI originally reported the issue in [pydata/xarray#9325](https://github.com/pydata/xarray/issues/9325).\r\nIt contained an MCVE.\r\n#11320 fixed the MCVE.\r\nNevertheless, the warning still occurs in my application despite the fix. I failed to produce a new MCVE.\r\n\r\nIn the meantime, another user @josephnowak reported seeing the warning and [produced an MCVE here](https://github.com/dask/dask/issues/9888#issuecomment-2308529565). I am pasting their MCVE below (minor edit: add `if name==main`) and the warning I am also receiving from their MCVE.\r\n\r\nI am also pasting below some of the warnings I get in my application I described [here](https://github.com/pydata/xarray/issues/9325).\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport dask.array as da\r\nimport xarray as xr\r\nfrom dask.distributed import LocalCluster\r\n\r\nbase_path = r\"YOUR_PATH\"\r\nsizes = [(\"a\", 3), (\"b\", 5), (\"c\", 2)]\r\n\r\n\r\ndef update_dataset(factor):\r\n    coords = {dim: list(range(size)) for dim, size in sizes}\r\n    dataset = xr.Dataset(\r\n        {\r\n            \"data\": xr.DataArray(\r\n                da.ones(shape=[size for _, size in sizes], chunks=(1, 1, 1)) * factor,\r\n                coords=coords,\r\n                dims=[dim for dim, _ in sizes],\r\n            )\r\n        }\r\n    )\r\n    path = f\"{base_path}/test_zarr_{factor}\"\r\n    dataset.to_zarr(path, mode=\"w\", compute=True)\r\n    return xr.open_zarr(path)\r\n\r\n\r\ndef calculate1(x, y):\r\n    path = f\"{base_path}/test_zarr_calculate1\"\r\n    result = x * y\r\n    result.to_zarr(path, mode=\"w\", compute=True)\r\n    return result\r\n\r\n\r\ndef calculate2(x, y):\r\n    path = f\"{base_path}/test_zarr_calculate2\"\r\n    result = x / y\r\n    result.to_zarr(path, mode=\"w\", compute=True)\r\n    return result\r\n\r\n\r\ndag = {\r\n    \"update_arr1\": (update_dataset, 2),\r\n    \"update_arr2\": (update_dataset, 4),\r\n    \"calculate1\": (calculate1, \"update_arr1\", \"update_arr2\"),\r\n    \"calculate2\": (calculate2, \"update_arr1\", \"update_arr2\"),\r\n    \"report\": (lambda x, y: True, \"calculate1\", \"calculate2\"),\r\n}\r\n\r\nif __name__ == \"__main__\":\r\n    with (\r\n        LocalCluster(n_workers=1, threads_per_worker=2, memory_limit=\"1GB\") as cluster,\r\n        cluster.get_client() as client,\r\n    ):\r\n        print(client.get(dag, \"report\"))\r\n```\r\n```\r\n2024-09-07 07:33:33,550 - distributed.scheduler - WARNING - Detected different `run_spec` for key 'original-open_dataset-data-82387e3302b96f07dd1529a08cfc15cc' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888.\r\nDebugging information\r\n---------------------\r\nold task state: memory\r\nold run_spec: (<function execute_task at 0x0000017031100360>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x000001703201F0C0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nnew run_spec: (<function execute_task at 0x0000017031100360>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x00000170323CF940>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nold token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('0e80b1b9015435fc78813424225ab2c7da970d04', ['2da998e53403d195a27e3e251b85adb7ecb9e29d'])]), ('dict', [])])\r\nnew token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('0e80b1b9015435fc78813424225ab2c7da970d04', ['0fb83d27412f7e3214aba2b6abf7d9cefc09c289'])]), ('dict', [])])\r\nold dependencies: set()\r\nnew dependencies: set()\r\n```\r\n\r\n\r\n\r\n**Warnings issued in my application**:\r\n```\r\nW 2024-09-07T04:33:48.918 Detected different `run_spec` for key 'original-open_dataset-acquisition-44879678a7f125ab1acff32de4b947c0' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \r\nDebugging information\r\n---------------------\r\nold task state: memory\r\nold run_spec: (<function execute_task at 0x0000029830442AC0>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000029A10D10FC0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nnew run_spec: (<function execute_task at 0x0000029830442AC0>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000029A2AED38C0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nold token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('15e75053801e08bb0cd151dba7ac2fb5dc343a6d', ['50d1e79c594e6c82ff30aeb53ddf38a90c9824ce'])]), ('dict', [])])\r\nnew token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('15e75053801e08bb0cd151dba7ac2fb5dc343a6d', ['352fe8cc616453bca361249ea6865b4d7109a97e'])]), ('dict', [])])\r\nold dependencies: set()\r\nnew dependencies: set()\r\n | distributed.scheduler | 11008 | MainProcess | IO loop | 1908 | scheduler | _generate_taskstates\r\n```\r\n```\r\nW 2024-09-07T04:45:06.421 Detected different `run_spec` for key 'original-open_dataset-acquisition-44879678a7f125ab1acff32de4b947c0' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \r\nDebugging information\r\n---------------------\r\nold task state: memory\r\nold run_spec: (<function execute_task at 0x0000029830442AC0>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x00000299FD616F80>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nnew run_spec: (<function execute_task at 0x0000029830442AC0>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000029A540579C0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nold token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('15e75053801e08bb0cd151dba7ac2fb5dc343a6d', ['c2312e2b3d5bebf54b5bde3347f81853e1bb66bd'])]), ('dict', [])])\r\nnew token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('15e75053801e08bb0cd151dba7ac2fb5dc343a6d', ['05fe405753166f125559e7c9ac558654f107c7e9'])]), ('dict', [])])\r\nold dependencies: set()\r\nnew dependencies: set()\r\n | distributed.scheduler | 11008 | MainProcess | IO loop | 1908 | scheduler | _generate_taskstates\r\n```\r\n```\r\nW 2024-09-07T04:45:18.310 Detected different `run_spec` for key 'original-open_dataset-acquisition-44879678a7f125ab1acff32de4b947c0' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \r\nDebugging information\r\n---------------------\r\nold task state: processing\r\nold run_spec: (<function execute_task at 0x0000029830442AC0>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000029A3C6B35C0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nnew run_spec: (<function execute_task at 0x0000029830442AC0>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000029A4A1FAF80>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nold token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('15e75053801e08bb0cd151dba7ac2fb5dc343a6d', ['4463581d1a8573381d3195764b1227ac3ec68b53'])]), ('dict', [])])\r\nnew token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('15e75053801e08bb0cd151dba7ac2fb5dc343a6d', ['b56d64bfbaab02736547d3ab138de7043f40f014'])]), ('dict', [])])\r\nold dependencies: set()\r\nnew dependencies: set()\r\n | distributed.scheduler | 11008 | MainProcess | IO loop | 1908 | scheduler | _generate_taskstates\r\n```\r\n```\r\nW 2024-09-07T04:45:19.398 Detected different `run_spec` for key 'original-open_dataset-shift_scan_y-394294ffc4e900ad0bec40754ed15105' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \r\nDebugging information\r\n---------------------\r\nold task state: processing\r\nold run_spec: (<function execute_task at 0x0000029830442AC0>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000029A8A990300>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nnew run_spec: (<function execute_task at 0x0000029830442AC0>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000029A79FEC8C0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\r\nold token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('a973c0a209aecb771394c43a62210df34e1df1a8', ['7279f27f69d107b5c1e3760b3c3dd70deecb08ab'])]), ('dict', [])])\r\nnew token: ('tuple', [('913ceb5b5beb463a9010ec0790bc30002ca34164', []), ('tuple', [('a973c0a209aecb771394c43a62210df34e1df1a8', ['0aa60933f90adb707f898d201ff9ec9abf6cb156'])]), ('dict', [])])\r\nold dependencies: set()\r\nnew dependencies: set()\r\n | distributed.scheduler | 11008 | MainProcess | IO loop | 1908 | scheduler | _generate_taskstates\r\n```\r\n**Environment**:\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:24:51) [MSC v.1940 64 bit (AMD64)]\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 85 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: ('English_United States', '1252')\r\nlibhdf5: None\r\nlibnetcdf: None\r\n\r\nxarray: 2024.7.0\r\npandas: 2.2.2\r\nnumpy: 2.1.0\r\nscipy: 1.14.1\r\nnetCDF4: None\r\npydap: None\r\nh5netcdf: None\r\nh5py: None\r\nzarr: 2.18.2\r\ncftime: None\r\nnc_time_axis: None\r\niris: None\r\nbottleneck: None\r\ndask: 2024.8.2\r\ndistributed: 2024.8.2\r\nmatplotlib: 3.9.2\r\ncartopy: None\r\nseaborn: None\r\nnumbagg: None\r\nfsspec: 2024.6.1\r\ncupy: None\r\npint: None\r\nsparse: None\r\nflox: None\r\nnumpy_groupies: None\r\nsetuptools: 72.2.0\r\npip: 24.2\r\nconda: None\r\npytest: None\r\nmypy: None\r\nIPython: 8.27.0\r\nsphinx: None\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Any chance you could try with the newest dask release? The MCVE doesn't warn for me anymore"
      },
      {
        "user": "josephnowak",
        "body": "I ran it again on my machine with Dask 2024.11.2 and it is raising a different warning, but I think it has the same meaning, should I try with the last changes on the main branch?\r\n\r\nWarning:\r\n```python\r\n2024-11-15 12:04:06,724 - distributed.scheduler - WARNING - Detected different `run_spec` for key 'original-open_dataset-data-d392cf7bb482f272dcf2f1a65576c2d3' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888.\r\nDebugging information\r\n---------------------\r\nold task state: memory\r\nold run_spec: DataNode(original-open_dataset-data-d392cf7bb482f272dcf2f1a65576c2d3, type=<class 'xarray.core.indexing.ImplicitToExplicitIndexingAdapter'>, ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x000001E7076EA4C0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))))\r\nnew run_spec: DataNode(original-open_dataset-data-d392cf7bb482f272dcf2f1a65576c2d3, type=<class 'xarray.core.indexing.ImplicitToExplicitIndexingAdapter'>, ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x000001E7076606C0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))))\r\nold token: ('DataNode', 'c35c3f3f439e6bc4e2da9b6439d34374')\r\nnew token: ('DataNode', '86fc9560a5f940e5325aef5de1f393be')\r\nold dependencies: set()\r\nnew dependencies: set()\r\n\r\n```\r\nEnvironment:\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:29:11) [MSC v.1935 64 bit (AMD64)]\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: ('Spanish_Venezuela', '1252')\r\nlibhdf5: None\r\nlibnetcdf: None\r\n\r\nxarray: 2024.10.0\r\npandas: 2.2.2\r\nnumpy: 1.26.4\r\nscipy: 1.11.4\r\nnetCDF4: None\r\npydap: None\r\nh5netcdf: None\r\nh5py: None\r\nzarr: 2.18.3\r\ncftime: None\r\nnc_time_axis: None\r\niris: None\r\nbottleneck: 1.3.8\r\ndask: 2024.11.2\r\ndistributed: 2024.11.2\r\nmatplotlib: 3.9.2\r\ncartopy: None\r\nseaborn: None\r\nnumbagg: 0.8.1\r\nfsspec: 2024.6.1\r\ncupy: None\r\npint: None\r\nsparse: 0.15.4\r\nflox: 0.9.10\r\nnumpy_groupies: 0.11.2\r\nsetuptools: 75.1.0\r\npip: 23.3.1\r\nconda: None\r\npytest: 8.2.1\r\nmypy: None\r\nIPython: 8.22.2\r\n"
      },
      {
        "user": "templiert",
        "body": "thanks for checking @phofl. Unfortunately I also just updated to 2024.11.2 and there are still similar warning messages in my application, see below.\r\n\r\n```\r\nW 2024-11-15T20:35:45.768 Detected different `run_spec` for key 'original-open_dataset-scale_similarity-c2d92a88cf66aad512261ac617a202f4' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \r\nDebugging information\r\n---------------------\r\nold task state: processing\r\nold run_spec: DataNode(original-open_dataset-scale_similarity-c2d92a88cf66aad512261ac617a202f4, type=<class 'xarray.core.indexing.ImplicitToExplicitIndexingAdapter'>, ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000028D478962C0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))))\r\nnew run_spec: DataNode(original-open_dataset-scale_similarity-c2d92a88cf66aad512261ac617a202f4, type=<class 'xarray.core.indexing.ImplicitToExplicitIndexingAdapter'>, ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000028CE992B800>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))))\r\nold token: ('DataNode', 'fd746a16272eb390411aa40d0e2e2d40')\r\nnew token: ('DataNode', 'aff4b98ac7b2012eba77eee97754e772')\r\nold dependencies: set()\r\nnew dependencies: set()\r\n | distributed.scheduler | 14632 | MainProcess | IO loop | 16156 | scheduler | _generate_taskstates\r\nW 2024-11-15T20:35:45.791 Detected different `run_spec` for key 'original-open_dataset-scale_affine_x-c2d92a88cf66aad512261ac617a202f4' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \r\nDebugging information\r\n---------------------\r\nold task state: processing\r\nold run_spec: DataNode(original-open_dataset-scale_affine_x-c2d92a88cf66aad512261ac617a202f4, type=<class 'xarray.core.indexing.ImplicitToExplicitIndexingAdapter'>, ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000028CE6F530C0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))))\r\nnew run_spec: DataNode(original-open_dataset-scale_affine_x-c2d92a88cf66aad512261ac617a202f4, type=<class 'xarray.core.indexing.ImplicitToExplicitIndexingAdapter'>, ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x0000028CE71BA940>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))))\r\nold token: ('DataNode', '8bab236cd2a560d61e6869e986733227')\r\nnew token: ('DataNode', '981493789e656583419978ddac4b1931')\r\nold dependencies: set()\r\nnew dependencies: set()\r\n | distributed.scheduler | 14632 | MainProcess | IO loop | 16156 | scheduler | _generate_taskstates\r\n```\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:24:51) [MSC v.1940 64 bit (AMD64)]\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 85 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: ('English_United States', '1252')\r\nlibhdf5: None\r\nlibnetcdf: None\r\n\r\nxarray: 2024.10.0\r\npandas: 2.2.2\r\nnumpy: 2.1.0\r\nscipy: 1.14.1\r\nnetCDF4: None\r\npydap: None\r\nh5netcdf: None\r\nh5py: None\r\nzarr: 2.18.2\r\ncftime: None\r\nnc_time_axis: None\r\niris: None\r\nbottleneck: 1.4.2\r\ndask: 2024.11.2\r\ndistributed: 2024.11.2\r\nmatplotlib: 3.9.2\r\ncartopy: None\r\nseaborn: None\r\nnumbagg: None\r\nfsspec: 2024.6.1\r\ncupy: None\r\npint: None\r\nsparse: None\r\nflox: None\r\nnumpy_groupies: None\r\nsetuptools: 72.2.0\r\npip: 24.2\r\nconda: None\r\npytest: None\r\nmypy: None\r\nIPython: 8.27.0\r\nsphinx: None\r\n```"
      }
    ]
  },
  {
    "issue_number": 11978,
    "title": "warn_dtype_mismatch() prints confusing message showing the same type as mismatch",
    "author": "NordMike",
    "state": "open",
    "created_at": "2025-06-06T00:27:27Z",
    "updated_at": "2025-06-07T12:20:06Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nDuring merging of dask dataframes you can get a warning which states you have a type mismatch showing the same type:\n```\n./venv/lib/python3.11/site-packages/dask/dataframe/multi.py:169: UserWarning: Merging dataframes with merge column data type mismatches: \n+---------------------------------+------------+-------------+\n| Merge columns                   | left dtype | right dtype |\n+---------------------------------+------------+-------------+\n| ('Bank Name Last Part', 'City') | string     | string      |\n+---------------------------------+------------+-------------+\nCast dtypes explicitly to avoid unexpected results.\n```\n\nIt advises to cast columns explicitly. But the left column is the result of `astype('string')` and the right column is got after `read_csv('file.csv', dtype='string')`.\n\nIf we check `ddf.dtypes` we notice that types aren't completely the same. They are `string[pyarrow]` and `string[python]`.\n\nAre they compatible? I suppose if they are this warning shouldn't be printed and if they are not the warning should print full types instead of just `string`.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\n\nfor lib in [pd, dd]:\n    print(f'= lib: {lib.__name__} =')\n\n    cities = lib.read_csv('banklist.csv', dtype='string')[['City', 'ST']].drop_duplicates()\n    banks = lib.read_csv('banklist.csv', dtype='string')[['Bank Name', 'Acquiring Institution']]\n\n    banks['Bank Name Last Part'] = banks['Bank Name'].str.split(' ').str[0].astype('string')\n\n    print('= cities =', cities.dtypes, sep='\\n')\n    print('= banks =', banks.dtypes, sep='\\n')\n\n    banks_with_geo = banks.merge(cities, left_on='Bank Name Last Part', right_on='City', how='left')\n\n    print('= banks_with_geo =', banks_with_geo.dtypes, sep='\\n')\n\n    print()\n```\n\n**Anything else we need to know?**:\nThere was a similar bug with this function already #5437.\n\nExample data: [banklist.csv](https://github.com/pandas-dev/pandas/blob/main/pandas/tests/io/data/csv/banklist.csv).\n\nMCVE output:\n```\n= lib: pandas =\n= cities =\nCity    string[python]\nST      string[python]\ndtype: object\n= banks =\nBank Name                string[python]\nAcquiring Institution    string[python]\nBank Name Last Part      string[python]\ndtype: object\n= banks_with_geo =\nBank Name                string[python]\nAcquiring Institution    string[python]\nBank Name Last Part      string[python]\nCity                     string[python]\nST                       string[python]\ndtype: object\n\n= lib: dask.dataframe =\n= cities =\nCity    string[pyarrow]\nST      string[pyarrow]\ndtype: object\n= banks =\nBank Name                string[pyarrow]\nAcquiring Institution    string[pyarrow]\nBank Name Last Part       string[python]\ndtype: object\n./venv/lib/python3.11/site-packages/dask/dataframe/multi.py:169: UserWarning: Merging dataframes with merge column data type mismatches:                                                         \n+---------------------------------+------------+-------------+\n| Merge columns                   | left dtype | right dtype |\n+---------------------------------+------------+-------------+\n| ('Bank Name Last Part', 'City') | string     | string      |\n+---------------------------------+------------+-------------+\nCast dtypes explicitly to avoid unexpected results.\n  warnings.warn(\n= banks_with_geo =\nBank Name                string[pyarrow]\nAcquiring Institution    string[pyarrow]\nBank Name Last Part               object\nCity                     string[pyarrow]\nST                       string[pyarrow]\ndtype: object\n```\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.11.5\n- Operating System: Linux\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "> Are they compatible? I suppose if they are this warning shouldn't be printed and if they are not the warning should print full types instead of just string.\n\n`string[pyarrrow]` and `string[python]` are two different data types in pandas. They can't be placed in the same column, so the warning is correct.\n\nDask doesn't control the string formatting of that data type. You could maybe ask on the pandas issue tracker whether they'd consider changing the repr, but I'm guessing that was done for a reason.\n\nBTW, I think you can configure dask and pandas to always use `string[pyarrow]` and avoid this warning."
      }
    ]
  },
  {
    "issue_number": 11975,
    "title": "Fix dangerous/bad design in apply",
    "author": "acampove",
    "state": "open",
    "created_at": "2025-06-03T17:25:56Z",
    "updated_at": "2025-06-06T16:47:37Z",
    "labels": [
      "needs triage"
    ],
    "body": "So I have code that looks like:\n\n```python\n        ddf = self._ddf.dropna()\n        ddf = ddf.repartition(partition_size=partition_size)\n        ddf = self._apply_selection(ddf=ddf)\n\n        # This meta thing MUST be here\n        # Or else dask will try to \"guess\"\n        # the structure by plugging in data\n        # that will make the code rise exception\n        meta = {\n            'row': 'f8',\n            'col': 'f8',\n            'are': 'f8',\n            'eng': 'f8',\n            'npv': 'i8',\n            'blk': 'i8',\n            'mu' : 'f8'\n        }\n\n        ddf = ddf.apply(\n                PreProcessor.build_features,\n                meta = meta,\n                axis = 1)\n\n```\n\nNaively, given that I am not an expert, I was not using meta. What the code up there would do is calling `apply_selection` and remove certain data. Then it would plug certain data to be processed `build_features` would run checks and if those checks fail, would raise an exception, telling me how the data looks like. Otherwise it would just run.\n\nThat function was raising exceptions and showing me rows filled with 1s. I had no idea what was happening or where those rows were coming from. \n\nI would just make `meta` mandatory, instead of injecting fake data that might break the users code, causing an endless debugging session.\n\nCheers.",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "Did you get a warning similar to this from `.apply`?\n\n```\n/Users/toaugspurger/gh/dask/dask/dask/dataframe/dask_expr/_collection.py:3243: UserWarning:\nYou did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\nTo provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the apply function that you are using.\n  Before: .apply(func)\n  After:  .apply(func, meta={'A': 'int64'})\n\n  warnings.warn(meta_warning(meta))\n```\n\nThat's no perfect, but has struck an OK balance historically. See https://docs.dask.org/en/stable/dataframe-design.html#metadata for more."
      },
      {
        "user": "acampove",
        "body": "> Did you get a warning similar to this from `.apply`?\n> \n> ```\n> /Users/toaugspurger/gh/dask/dask/dask/dataframe/dask_expr/_collection.py:3243: UserWarning:\n> You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n> To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the apply function that you are using.\n>   Before: .apply(func)\n>   After:  .apply(func, meta={'A': 'int64'})\n> \n>   warnings.warn(meta_warning(meta))\n> ```\n> \n> That's no perfect, but has struck an OK balance historically. See https://docs.dask.org/en/stable/dataframe-design.html#metadata for more.\n\nI get it, however some of us live by the principle of:\n\n```\nif there is a hole in the ground, do not put a warning sign, fill the hole.\n```"
      },
      {
        "user": "TomAugspurger",
        "body": "You can elevate errors to warnings if you prefer:\n\n```python\nimport warnings\n\nwarnings.filterwarnings(\"error\", category=UserWarning, module=\"dask\")\n```\n\nWe can leave this open for a bit to see if there's appetite for making `meta` required (perhaps as an option). IMO it's not worth the churn on users."
      }
    ]
  },
  {
    "issue_number": 11877,
    "title": "⚠️ Upstream CI failed ⚠️",
    "author": "github-actions[bot]",
    "state": "closed",
    "created_at": "2025-04-09T02:02:20Z",
    "updated_at": "2025-06-05T21:48:43Z",
    "labels": [
      "upstream"
    ],
    "body": "[Workflow Run URL](https://github.com/dask/dask/actions/runs/15471457494)\n<details><summary>Python 3.12 Test Summary</summary>\n\n```\ndask/dataframe/tests/test_groupby.py::test_groupby_numeric_only_false_cov_corr[disk-cov]: TypeError: DataFrame contains columns with dtype datetime64 or timedelta64, which are not supported for cov.\ndask/dataframe/tests/test_groupby.py::test_groupby_numeric_only_false_cov_corr[tasks-cov]: TypeError: DataFrame contains columns with dtype datetime64 or timedelta64, which are not supported for cov.\ndask/dataframe/tests/test_dataframe.py::test_cov_corr_mixed[True]: TypeError: DataFrame contains columns with dtype datetime64 or timedelta64, which are not supported for cov.\n```\n\n</details>\n",
    "comments": [
      {
        "user": "jrbourbeau",
        "body": "Closed via https://github.com/dask/dask/pull/11977 "
      }
    ]
  },
  {
    "issue_number": 11964,
    "title": "Recommended way to enforce method update after modification",
    "author": "apatlpo",
    "state": "closed",
    "created_at": "2025-05-23T14:12:23Z",
    "updated_at": "2025-06-05T15:46:20Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nI am wondering what is the recommended way to pass a modified method to map_partitions.\nThis question was initially [posted in discourse](https://dask.discourse.group/t/recommended-way-to-enforce-method-update-after-modification/3952) and I was recommended to post in the dask issue tracker.\n\n**Minimal Complete Verifiable Example**:\n\n```\ndf = pd.DataFrame({'x': [1, 2, 3, 4, 5],\n                   'y': [1., 2., 3., 4., 5.]})\nddf = dd.from_pandas(df, npartitions=2)\n\ndef myadd(df, a, b=1):\n    return df.x + df.y + a + b\nres = ddf.map_partitions(myadd, 1, b=2)\nres.compute()\n```\noutput:\n```\n0     5.0\n1     7.0\n2     9.0\n3    11.0\n4    13.0\ndtype: float64\n```\nUpdating `myadd` according to\n```\ndef myadd(df, a, b=1):\n    return df.x + df.y + a + b + 1 \nres = ddf.map_partitions(myadd, 1, b=2)\nres.compute()\n```\noutputs:\n```\n0     5.0\n1     7.0\n2     9.0\n3    11.0\n4    13.0\ndtype: float64\n```\nSo the update of myadd was not passed around and the result is the same.\nDefining the method as static seems to work on the other hand:\n```\n@staticmethod\ndef myadd(df, a, b=1):\n    return df.x + df.y + a + b + 1 \nres = ddf.map_partitions(myadd, 1, b=2)\nres.compute()\n```\noutputs:\n```\n0     6.0\n1     8.0\n2    10.0\n3    12.0\n4    14.0\ndtype: float64\n```\n\nIs this the recommended way to do this?\n\nI am fairly convinced older dask versions were more robust and provided the \"correct\" output without the static definition (@guillaumeeb verified this is the case 2024.12.1, see [discourse post](https://dask.discourse.group/t/recommended-way-to-enforce-method-update-after-modification/3952)).\nIn any case, I did not find any mention of this in the dask documention nor on the web.\nMore visibility of this behavior would be welcomed to my opinion.\n\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.10\n- Operating System: \"SUSE Linux Enterprise Server 12 SP1\"\n- Install method (conda, pip, source): conda\n",
    "comments": []
  },
  {
    "issue_number": 11972,
    "title": "Array API linalg extension missing functions ",
    "author": "purepani",
    "state": "open",
    "created_at": "2025-06-01T19:34:47Z",
    "updated_at": "2025-06-01T19:35:00Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n\nRight now, `dask.array` implements a majority of the as ready API spec correctly. However, the `linalg` extension is missing quite a few functions from the spec which makes it non-compliant. I've not double-checked this, but a potentially non comprehensive list can be found here: https://data-apis.org/array-api-compat/supported-array-libraries.html#dask\n\nHaving the spec be partially implemented causes some problems, in that, as an array API library consumer, I'm unable to just check for `has_attr(xp, 'linalg')`. My understanding is that some of these functions are difficult to write in dask, so I'm not quite sure what the best way forward is here, aside from implementing the functions, or getting rid of the linalg module entirely. \n\n",
    "comments": []
  },
  {
    "issue_number": 11671,
    "title": "to_sql method returns \"AttributeError: 'Engine' object has no attribute 'cursor'\" whenever pandas>2.0.0",
    "author": "fabclmnt",
    "state": "open",
    "created_at": "2025-01-14T20:18:01Z",
    "updated_at": "2025-05-30T09:11:37Z",
    "labels": [
      "dataframe",
      "needs attention",
      "bug"
    ],
    "body": "**Describe the issue**:\r\nWhenever using DASK to_sql method (https://docs.dask.org/en/stable/generated/dask.dataframe.to_sql.html), the following error is shown\r\n![image](https://github.com/user-attachments/assets/81daf888-4884-4b5f-b5d9-07d453220243)\r\n\r\nThis error is related with the latest changes from pandas v2, where the **to_sql** method no longer expects a cursor but rather a connection. \r\n\r\nI believe that to fix this issue the change should be done in the method **_to_sql_chunk** [from this file](https://github.com/dask/dask/blob/main/dask/dataframe/io/sql.py)\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\nname='test'\r\nschema_name = 'schema_name'\r\nif_exists = 'replace'\r\nuri = 'add here database uri'\r\n\r\ndf = pd.DataFrame([ {'i':i, 's':str(i)*2 } for i in range(4) ])\r\nddf = dd.from_pandas(df, npartitions=2)\r\n\r\nddf.to_sql(name=name, uri=uri,\r\n                   schema=schema_name, index=False,\r\n                   if_exists=if_exists)\r\n```\r\n\r\n**Anything else we need to know?**:\r\n- This error is happening for any type of RDBMS (SQL Server, PostGreSQL Server, MySQL)\r\n\r\n**Environment**: \r\n\r\n- Dask version: 2024.12.1\r\n- Python version: 3.12\r\n- Operating System: MacOS Sonoma\r\n- Install method (conda, pip, source): pip\r\n- pandas version: 2.2.3\r\n",
    "comments": [
      {
        "user": "LennaHammer",
        "body": "It seems that this bug is due to the breaking change of SqlAlchemy 2. Maybe it needs to wait for Pandas to fix it(https://github.com/pandas-dev/pandas/issues/60684)."
      }
    ]
  },
  {
    "issue_number": 11969,
    "title": "groupby apply using scheduler='threads' runs like with scheduler='synchronous'",
    "author": "vianneylan",
    "state": "open",
    "created_at": "2025-05-28T10:00:10Z",
    "updated_at": "2025-05-28T11:03:09Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\nWhen running parallel groupby.apply with a GIL-releasing custom function, the computation is done group by group sequentially, regardless of scheduler being set to 'threads' or to 'synchronous' in compute()\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nfrom time import sleep\n\n# Create a Dask DataFrame\nnrow = 100000\ndf = dd.from_pandas(pd.DataFrame({'A': range(nrow), 'B': np.random.choice([0,1,2,3], nrow)}))\n\n# Define a custom function to apply, that releases the GIL (sleep fcn)\ndef custom_function(group):\n    for i in range(5):\n        print(f\"  iteration {i} of group {group.name}\")\n        sleep(0.5)\n    return group['A'].mean()\n\n# Group by column 'A' and apply the custom function\nresult = df.groupby('B').apply(custom_function, meta=('A', 'float'))\n\nprint('Run using scheduler=synchronous')\ncomputed_result = result.compute(scheduler='synchronous')\nprint('Run using scheduler=threads')\ncomputed_result = result.compute(scheduler='threads')\n\n```\nOutput:\n```\nRun using scheduler=synchronous\n  iteration 0 of group 0\n  iteration 1 of group 0\n  iteration 2 of group 0\n  iteration 3 of group 0\n  iteration 4 of group 0\n  iteration 0 of group 1\n  iteration 1 of group 1\n  iteration 2 of group 1\n  iteration 3 of group 1\n  iteration 4 of group 1\n  iteration 0 of group 2\n  iteration 1 of group 2\n  iteration 2 of group 2\n  iteration 3 of group 2\n  iteration 4 of group 2\n  iteration 0 of group 3\n  iteration 1 of group 3\n  iteration 2 of group 3\n  iteration 3 of group 3\n  iteration 4 of group 3\nRun using scheduler=threads\n  iteration 0 of group 0\n  iteration 1 of group 0\n  iteration 2 of group 0\n  iteration 3 of group 0\n  iteration 4 of group 0\n  iteration 0 of group 1\n  iteration 1 of group 1\n  iteration 2 of group 1\n  iteration 3 of group 1\n  iteration 4 of group 1\n  iteration 0 of group 2\n  iteration 1 of group 2\n  iteration 2 of group 2\n  iteration 3 of group 2\n  iteration 4 of group 2\n  iteration 0 of group 3\n  iteration 1 of group 3\n  iteration 2 of group 3\n  iteration 3 of group 3\n  iteration 4 of group 3\n```\n\n**Anything else we need to know?**:\n\nIn the above MCVE, I would expect the logs to be kinda shuffled in the 'scheduler=threads' part, because the fcn releases the GIL during time.sleep() and thus the GIL could be allocated to a different thread to execute cusom fcn on other groups\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.11\n- Operating System: windows 10\n- Install method (conda, pip, source): conda\n",
    "comments": [
      {
        "user": "vianneylan",
        "body": "Note that I also tried to setup LocalCluster with 5 workers and 1 thread per worker and use it instead of relying on default compute(), but same result."
      }
    ]
  },
  {
    "issue_number": 11963,
    "title": "Multiple independent `map_overlap` operations on same input with different window sizes produce incorrect prev_part sizes",
    "author": "neumannjan",
    "state": "open",
    "created_at": "2025-05-23T09:42:47Z",
    "updated_at": "2025-05-26T14:51:53Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nWhenever I run multiple `map_overlap`-type operations (includes also `rolling`, `shift`, etc.) on the same input with different window sizes (i.e., different `before` parameter), I get an error that \"Partition size is less than overlapping window size\", no matter the actual partitioning.\n\nThe error is thrown in the `_combined_parts` function, but sometimes it is even the case that `prev_part.shape[0]` is *greater than* the `before` parameter, so this is clearly not an issue of partitioning.\n\n**Minimal Complete Verifiable Example**:\n\nThe following should give you the error. You should even see a case where `prev_part.shape[0] > before`. If you don't see that, play with how many of the parallel operations you run. Sometimes I reproduce a case where `prev_part.shape[0] < before`, sometimes the other.\n\n```python\nimport dask\nimport numpy as np\nimport dask.dataframe as dd\nimport pandas as pd\nfrom dask.diagnostics import ProgressBar\n\ndf = dd.from_pandas(pd.DataFrame({\n    'close_price': np.random.sample((2_000_000,))\n}), chunksize=1_000_000)\n\n# out1 = df['close_price'] - df['close_price'].shift(14)\n# out2 = df['close_price'] - df['close_price'].shift(100)\n# out3 = df['close_price'] - df['close_price'].shift(1)\n\nout1 = df['close_price'] - df['close_price'].map_overlap(lambda x: x, before=14, after=0)\nout3 = df['close_price'] - df['close_price'].map_overlap(lambda x: x, before=1, after=0)\n\n# dask.visualize(out1, out3, filename='dask.svg', engine='ipycytoscape')\n\nwith ProgressBar():\n    df1, df3 = dask.compute(out1, out3, scheduler='single-threaded')\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.5.1\n- Python version: 3.11.11\n- Operating System: Ubuntu 24.04\n- Install method (conda, pip, source): poetry",
    "comments": [
      {
        "user": "faulaire",
        "body": "Hi,\n\nUnit tests are missing and patch may not be perfect yet but I opened a PR to address this issue : https://github.com/dask/dask/pull/11965/files\n\nRegards,\n\nFabien\n"
      },
      {
        "user": "faulaire",
        "body": "Hi,\n\nDev is done, happy to get any feedback from contributors.\n\nRegards,\n\nFabien"
      },
      {
        "user": "neumannjan",
        "body": "I am not a contributor, but I tested this also manually with my own experiments, and it seems to work just fine. The resulting data also matches exactly for both Dask and pure Pandas. I'm using `shift`, `map_overlap`, `rolling`, with various window sizes."
      }
    ]
  },
  {
    "issue_number": 11778,
    "title": "Support for sharding when storing dask arrays to zarr",
    "author": "biprateep",
    "state": "open",
    "created_at": "2025-02-25T03:13:16Z",
    "updated_at": "2025-05-23T20:30:51Z",
    "labels": [
      "array",
      "io",
      "needs attention",
      "enhancement"
    ],
    "body": "<!-- Please do a quick search of existing issues to make sure that this has not been asked before. -->\nHi,\n\nWith the release of zarr-python 3, the feature to store multiple chunks in a single storage object (i.e. [sharding](https://zarr.readthedocs.io/en/stable/user-guide/performance.html#sharding)) was introduced. Since, Dask has started to support zarr-python 3, I would request the option to specify the the shards in the `to_zarr()` function be introduced. This will be a very helpful feature for people trying to store large arrays on regular file systems while needing fast random reads. ",
    "comments": [
      {
        "user": "phofl",
        "body": "Contributions are welcome in this area!"
      },
      {
        "user": "ivirshup",
        "body": "What's the desired implementation look like here?\n\nIf it's 1 dask chunk -> 1 zarr shard I think this is pretty straight forward, and is what I believe xarray has implemented right now. If it's 1 dask chunk -> 1 zarr chunk, then I think it becomes more complicated since you'll need a lock per shard/ a more complicated write algorithm."
      }
    ]
  },
  {
    "issue_number": 11900,
    "title": "IndexError: tuple index out of range with dask delayed and dask 2025.4.0",
    "author": "joshua-gould",
    "state": "closed",
    "created_at": "2025-04-23T15:30:26Z",
    "updated_at": "2025-05-22T13:26:39Z",
    "labels": [
      "needs triage"
    ],
    "body": "I don't yet have a minimal reproducible example to share, but our internal code that worked with versions of dask before version `2025.4.0` is now failing with the error below:  \n\n\n```\n  File \"/opt/homebrew/envs/ops/lib/python3.12/site-packages/dask/base.py\", line 683, in compute\n    return repack(results)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/envs/ops/lib/python3.12/site-packages/dask/base.py\", line 526, in repack\n    return simple_get(dsk, out)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/envs/ops/lib/python3.12/site-packages/dask/core.py\", line 109, in get\n    result = execute_graph(dsk2, cache, keys=set(flatten([out])))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/envs/ops/lib/python3.12/site-packages/dask/_task_spec.py\", line 1030, in execute_graph\n    cache[key] = node(cache)\n                 ^^^^^^^^^^^\n  File \"/opt/homebrew/envs/ops/lib/python3.12/site-packages/dask/_task_spec.py\", line 758, in __call__\n    return self.func(*new_argspec)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: tuple index out of range\n```",
    "comments": [
      {
        "user": "joshua-gould",
        "body": "Closing as I can't create a minimal reproducible example."
      }
    ]
  },
  {
    "issue_number": 11961,
    "title": "map_overlap does not trim properly with new_axis when input array has more than 2 dimensions",
    "author": "joshua-gould",
    "state": "open",
    "created_at": "2025-05-21T13:46:37Z",
    "updated_at": "2025-05-21T13:46:53Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\nimport numpy as np\n\n\ndef process_chunk(x):\n    return np.ones(shape=(2,) + x.shape)\n\n\n# fails with 3d array\na = da.random.random((4, 100, 100)).rechunk((4, 10, 10))\nresult_shape = da.map_overlap(\n    process_chunk,\n    a,\n    new_axis=0,\n    depth={0: 0, 1: 2, 2: 2},\n    trim=True,\n    boundary=\"none\",\n).compute().shape\nassert result_shape[1:] == a.shape, f\"{result_shape} != {a.shape}\"\n\n# works with 2d array\na = da.random.random((100, 100)).rechunk((10, 10))\nresult_shape = da.map_overlap(\n    process_chunk,\n    a,\n    new_axis=0,\n    depth={0: 2, 1: 2},\n    trim=True,\n    boundary=\"none\",\n).compute().shape\nassert result_shape[1:] == a.shape, f\"{result_shape} != {a.shape}\"\n\n```\n\n\n**Environment**:\n\n- Dask version: 2025.5.1\n- Python version: 3.12\n- Operating System: Linux\n- Install method: pip\n",
    "comments": []
  },
  {
    "issue_number": 11960,
    "title": "Issue during the closing of xarray open with dask backend",
    "author": "LouisPauchet",
    "state": "open",
    "created_at": "2025-05-21T11:30:39Z",
    "updated_at": "2025-05-21T11:33:30Z",
    "labels": [
      "needs triage"
    ],
    "body": "Hello,\n\nI didn't manage to create an example to reproduce the bug in few lines, but the complete is example is available on GitHub : https://github.com/MET-OM/metocean-api and https://github.com/MET-OM/metocean-api/actions/runs/15142723801.\n\nWhen having the test processing, they randomly pass and randomly fail at the closing of dataset open in a with statement.\n\n```python\nimport xarray as xr\nfrom dask.diagnostics import ProgressBar\n\ntempfiles = [...] # list of NetCDF4 files\n\nwith xr.open_mfdataset(tempfiles, parallel=True, engine=\"netcdf4\") as ds, ProgressBar():\n            ds.load()\n            if save_nc:\n                \n                # Save the unaltered structure\n                ds = ds.sel({\"time\": slice(ts.start_time, ts.end_time)})\n                save_to_netcdf(ds, ts.datafile.replace(\".csv\", \".nc\"))\n\n            df = self.create_dataframe(\n                ds=ds,\n                lon_near=lon_near,\n                lat_near=lat_near,\n                outfile=ts.datafile,\n                start_time=ts.start_time,\n                end_time=ts.end_time,\n                save_csv=save_csv,\n                **flatten_dims,\n            )\n\ndef save_to_netcdf(ds, outfile):\n    remove_if_datafile_exists(outfile)\n    ds.to_netcdf(outfile)\n    print(f\"NetCDF file created at {outfile}\")\n```\n\nHere are the logs when running the same test command 'pytest -v --full-trace  >>log4.txt'.\n\n[log4.txt](https://github.com/user-attachments/files/20366734/log4.txt)\n\nI absolutly don't know what is happening and it's maybe not really a bug but a not complete understanding of what is happening.\n\n**Environment**:\n\n- Dask version: 2025.5.1\n- xarray version: 2025.4.0\n- Python version: 3.13.3\n- Operating System: Ubuntu 24.04.2 LTS / Windows (both same behaviour)\n- Install method (conda, pip, source): conda forge\n\nThank you very much for your help",
    "comments": []
  },
  {
    "issue_number": 11958,
    "title": "in-place ops broken after reindexing (2025.5.0 regression)",
    "author": "ilia-kats",
    "state": "closed",
    "created_at": "2025-05-20T11:25:44Z",
    "updated_at": "2025-05-21T00:50:10Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nAfter reindexing a dask array, in-place subtraction raises a shape mismathch error. This is a regression in 2025.5.0, it works in 2025.4.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.array as da\n\ntest = np.random.rand(10, 2)\ndtest = da.from_array(test)\ndtest = dtest[:, [0, 1, -1]]\nidx = np.asarray([1,1,1,1,1, 0, 0, 0, 0, 0], dtype=bool)\ndtest[idx, :] -= np.arange(3, dtype=dtest.dtype)\ndtest.compute()\n```\n\nraises\n```python\nValueError                                Traceback (most recent call last)\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/slicing.py:2088, in setitem(x, v, indices)\n   2087 try:\n-> 2088     x[tuple(indices)] = v\n   2089 except ValueError as e:\n\nValueError: shape mismatch: value array of shape (5,3) could not be broadcast to indexing result of shape (5,1)\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nCell In[44], line 9\n      7 idx = np.asarray([1,1,1,1,1, 0, 0, 0, 0, 0], dtype=bool)\n      8 dtest[idx, :] -= np.arange(3, dtype=dtest.dtype)\n----> 9 dtest.compute()\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/base.py:373, in DaskMethodsMixin.compute(self, **kwargs)\n    349 def compute(self, **kwargs):\n    350     \"\"\"Compute this dask collection\n    351 \n    352     This turns a lazy Dask collection into its in-memory equivalent.\n   (...)    371     dask.compute\n    372     \"\"\"\n--> 373     (result,) = compute(self, traverse=False, **kwargs)\n    374     return result\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/base.py:681, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    678     expr = expr.optimize()\n    679     keys = list(flatten(expr.__dask_keys__()))\n--> 681     results = schedule(expr, keys, **kwargs)\n    683 return repack(results)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/slicing.py:2090, in setitem(x, v, indices)\n   2088     x[tuple(indices)] = v\n   2089 except ValueError as e:\n-> 2090     raise ValueError(\n   2091         \"shape mismatch: value array could not be broadcast to indexing result\"\n   2092     ) from e\n   2094 return x\n\nValueError: shape mismatch: value array could not be broadcast to indexing result\n```\n\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.5.0\n- Python version: 3.11.2\n- Operating System: Debian 12\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "jrbourbeau",
        "body": "Thanks @ilia-kats I'm able to reproduce the issue. It looks like https://github.com/dask/dask/pull/11947 fixes this. We'll look to get a release out soon "
      },
      {
        "user": "jrbourbeau",
        "body": "The 2025.5.1 with a fix for this is out now. Closing but feel free to reopen as needed. "
      }
    ]
  },
  {
    "issue_number": 11959,
    "title": "Array indexing broken in 2025.5.0",
    "author": "djhoese",
    "state": "closed",
    "created_at": "2025-05-20T16:20:56Z",
    "updated_at": "2025-05-20T16:44:53Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nCertain indexing situations seem to not return the equivalent result that numpy produces.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.array as da\n\na = np.zeros((5, 2), dtype=np.float32)\nb = da.from_array(a)\n# dask.array<array, shape=(5, 2), dtype=float32, chunksize=(5, 2), chunktype=numpy.ndarray>\n\na[(0, [0, 1])]\n# array([0., 0.], dtype=float32)\n\nb[(0, [0, 1])]\n# dask.array<array, shape=(5, 2), dtype=float32, chunksize=(5, 2), chunktype=numpy.ndarray>\n\n```\n\nConfusingly, this doesn't happen with the default float64:\n\n```python\na = np.zeros((4, 5))\nb = da.from_array(a)\n# dask.array<array, shape=(4, 5), dtype=float64, chunksize=(4, 5), chunktype=numpy.ndarray>\n\na[(0, [0, 1])]\n# array([0., 0.])\n\nb[(0, [0, 1])]\n# dask.array<getitem, shape=(2,), dtype=float64, chunksize=(2,), chunktype=numpy.ndarray>\n```\n\n**Anything else we need to know?**:\n\nI haven't narrowed down what combination of indexes and dtypes this works or does not work with. I do have tests failing with the scalar index being second.\n\nMay be related to https://github.com/dask/dask/issues/11958.\n\n**Environment**:\n\n- Dask version: 2025.5.0\n- Python version: 3.12\n- Operating System: Ubuntu/PopOS\n- Install method (conda, pip, source): conda/conda-forge\n",
    "comments": [
      {
        "user": "jrbourbeau",
        "body": "Thanks @djhoese -- I think https://github.com/dask/dask/pull/11947 (already in `main`) fixes this. Can you confirm? "
      },
      {
        "user": "djhoese",
        "body": "Yep, that looks fixed. Closed by #11947."
      },
      {
        "user": "jrbourbeau",
        "body": "Sweet, thanks for confirming. We're planning to push out a new release later today (xref https://github.com/dask/community/issues/421) "
      }
    ]
  },
  {
    "issue_number": 11942,
    "title": "Concatenated Frame Parquet Write Failure (Lost Dependencies)",
    "author": "McBains",
    "state": "open",
    "created_at": "2025-05-09T23:03:15Z",
    "updated_at": "2025-05-20T15:15:51Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nThe function below used to be able concatenate datasets that all had the same division without having to do ```reset_index```/```set_index``` due to unknown divisions (Parquet metadata files are unavailable). Since Dask 2024.x concatenating frames with unknown divisions is not supported, I re-introduce the divisions using ```reset_index```/```set_index```.\n\nIn debug, I was able to confirm that divisions are set before ```df``` is appended to ```dfs```. When calling ```to_parquet```, however, for some reason it fails with the following stack trace (partial):\n```\nFile \"C:\\Program Files\\Python310\\lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py\", line 3312, in to_parquet\n    return to_parquet(self, path, **kwargs)\n  File \"C:\\Program Files\\Python310\\lib\\site-packages\\dask\\dataframe\\dask_expr\\io\\parquet.py\", line 661, in to_parquet\n    out = out.compute(**compute_kwargs)\n  File \"C:\\Program Files\\Python310\\lib\\site-packages\\dask\\base.py\", line 373, in compute\n    (result,) = compute(self, traverse=False, **kwargs)\n  File \"C:\\Program Files\\Python310\\lib\\site-packages\\dask\\base.py\", line 681, in compute\n    results = schedule(expr, keys, **kwargs)\n  File \"C:\\Program Files\\Python310\\lib\\site-packages\\distributed\\client.py\", line 2403, in _gather\n    raise exception.with_traceback(traceback)\ndistributed.client.FutureCancelledError: ('toparquetbarrier-834ac4c9412f13d8dec40a4fd430c4ae', 0) cancelled for reason: lost dependencies.\n```\n\nThe frame which it attempts to concatenate contains ~1.1M rows x ~40K columns. Dataset is split into 4 folders. Each folder  contains 1137 partitions with 1000 rows/partition and ~10K distinct columns; the data in each folder is partitioned using the same divisions. Even stranger, I have cases when ```None``` was passed as ```divisions``` and concatenation worked. \n\nLogging this as a bug given that the same code used to work before divisions were required and now it doesn't.\n\n**Minimal Complete Verifiable Example**:\n```python\ndef merge(prefix_vars: list[str], target_vars: list[str], dirs: dict[str, dict[str, list[str]]], divisions: list | tuple):\n\t'''\n\tMerges datasets column-wise.\n\n\tParameters\n\t----------\n\tprefix_vars: list[str]\n\t\tList of prefix variables to include in output.\n\n\ttarget_vars: list[str]\n\t\tTarget variables from frame to include in output.\n\n\tdirs: dict[str, dict[str, list[str]]]\n\t\tMap of output file to map of inputs and the variables within them.\n\n\tdivisions: list | tuple\n\t\tDivisions to use for repartitioning data when merging.\n\t'''\n\ttargets = {*target_vars}\n\n\tfor k, v in dirs.items():\n\t\tdfs = []\n\t\tprefix = [*prefix_vars]\n\n\t\tfor inp, vars in v.items():\n\t\t\tvars = targets.intersection(vars)\n\n\t\t\tif not vars:\n\t\t\t\tcontinue\n\n\t\t\tvars = [*prefix, *sorted(vars)]\n\n\t\t\tdf = dask.dataframe.read_parquet(inp, columns=vars)\n\n\t\t\tif divisions is not None:\n\t\t\t\tdf_idx = df.index.name\n\t\t\t\tdf = df.reset_index(drop=False)\n\t\t\t\tdf = df.set_index(df_idx, sorted=True, divisions=divisions)\n\n\t\t\tdfs.append(df)\n\t\t\tprefix.clear()\n\n\t\tdf = dask.dataframe.concat(dfs, axis=1)\n\t\tdf.to_parquet(k, write_index=True, write_metadata_file=False, name_function=lambda x: f\"Partition-{x}.parquet\")\n```\n\n**Anything else we need to know?**:\nIf it's worth anything, I am using PyArrow ```20.0.0``` and Pandas ```2.2.3```.\n\n**Environment**:\n- Dask version: ```2025.4.1```\n- Python version: ```3.10.4```\n- Operating System: ```Microsoft Windows [Version 10.0.19045.5737]```\n- Install method (conda, pip, source): ```pip```\n",
    "comments": [
      {
        "user": "McBains",
        "body": "Issue is even weirder. When ```sorted=True``` is replaced with ```sort=False```, the concatenation works despite divisions not being set. However, it breaks when I include a particular column in ```prefix_vars```; the column in question is a string. When it does \"work\", the divisions are not respected; it seemingly halves the number of partitions. It goes from 1000 rows/partition to 2000 rows/partition."
      }
    ]
  },
  {
    "issue_number": 11951,
    "title": "DataFrame.loc[:stop] raises ValueError when using set_index() with datetime divisions",
    "author": "faulaire",
    "state": "closed",
    "created_at": "2025-05-15T15:15:29Z",
    "updated_at": "2025-05-20T11:08:47Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nWhen constructing a dask.dataframe from a pandas.DataFrame and setting a datetime index using .set_index() along with explicitly specified divisions, attempting to slice with .loc[:stop] results in a ValueError.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport datetime\nimport dask.dataframe\nimport pandas as pd\n\nstart, stop = datetime.datetime(2019, 1, 1), datetime.datetime(2021, 1, 1)\n\ndivisions = (start, stop)\ndask_dataframe = dask.dataframe.from_pandas(pd.DataFrame(divisions, columns=['divisions']),\n                                            npartitions=1).set_index('divisions', divisions=divisions)\n\ndask_dataframe.compute() # OK\ndask_dataframe.loc[:stop].compute() # Raise ValueError: Can not use loc on DataFrame without known divisions\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.5.0\n- Python version: 3.13.2\n- Operating System: Windows\n- Install method (conda, pip, source): conda\n",
    "comments": [
      {
        "user": "phoebecd",
        "body": "Hi, it’s unclear what exactly you want your program’s output to look like. In the code you provided, you created a Dask dataframe with only two rows: the first row has the value of start, and the second row has the value of stop. For `dask_dataframe.compute()`, it does not matter how many rows your dataframe has. However, for `dask_dataframe.loc[:stop].compute()`, you are attempting to slice the dataframe. If your dataframe contains only two index values (as seen here), Dask cannot guarantee correct slicing, so it outputs a ValueError.\n\nHere is a snippet of code that I wrote to demonstrate how to use `dask_dataframe.loc[:stop]` without errors. This code creates a dataframe with divisions, then calls dask_dataframe.compute() to compute the whole dataframe, and finally uses `dask_dataframe.loc[:stop].compute()` to compute the rows from the start up to and including the value of stop.\n\n```python\ndata_start, data_stop = datetime.datetime(2018, 1, 1), datetime.datetime(2022, 1, 1)\nstart, stop = datetime.datetime(2019, 1, 1), datetime.datetime(2021, 1, 1)\n\ndates = pd.date_range(data_start, data_stop, freq='D')\ndf = pd.DataFrame({'value': range(len(dates))}, index=dates)\ndf.index.name = 'divisions'\ndask_dataframe = dask.dataframe.from_pandas(df, npartitions=1)\n\ndask_dataframe.compute()\ndask_dataframe.loc[:stop].compute()\n```\n\n\n\n"
      },
      {
        "user": "faulaire",
        "body": "Hi, I just wanted to create a minimal dask dataframe with specific divisions. The provided implementation was working before dask_expr.\nI agree that your solution is better and I'll use it to address this case. \n\nThank you !\n\n"
      }
    ]
  },
  {
    "issue_number": 11957,
    "title": "Documentation lists that dask.dataframe.merge has how=\"cross\" option, but using this option throws an error",
    "author": "morleytj",
    "state": "open",
    "created_at": "2025-05-19T23:41:28Z",
    "updated_at": "2025-05-19T23:41:41Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nUsing dask.dataframe.merge, there is an error if you use the option how=\"cross\". It throws the following error:  \n```ValueError: dask.dataframe.merge does not support how='cross'.Options are: ('left', 'right', 'outer', 'inner', 'leftsemi').```\n\nHowever, in the documentation it explicitly mentions that cross merge is implemented, located here: https://docs.dask.org/en/stable/generated/dask.dataframe.merge.html\n\nIt is unclear whether the documentation should be modified to remove this mention or the code should be modified to add this functionality(Assuming it was removed at some point). This was tested using dask==2025.5.0. Thanks!\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport sys\n\nddf1 = dd.read_parquet(sys.argv[1])\nddf2 = dd.read_parquet(sys.argv[2])\n\ndd.merge(ddf1, ddf2, how='cross')\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.5.0\n- Python version: 3.13.2\n- Operating System: CentOS 9\n- Install method (conda, pip, source): pip\n",
    "comments": []
  },
  {
    "issue_number": 11938,
    "title": "Can not select parquet nested columns with dataframe.read_parquet",
    "author": "hombit",
    "state": "open",
    "created_at": "2025-05-08T20:54:46Z",
    "updated_at": "2025-05-19T20:10:58Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nIt looks like Dask Dataframe doesn't support nested column selection for parquet files, supported by both `pandas` and `pyarrow`.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nfrom tempfile import NamedTemporaryFile\n\nimport dask.dataframe as dd\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nwith NamedTemporaryFile(suffix=\".parquet\") as f:\n    table = pa.table({\"a\": pa.array([{\"b\": 1, \"c\": 1.0}, {\"b\": 2, \"c\": 1.0}])})\n    pq.write_table(table, f.name)\n    _df = pd.read_parquet(f.name, columns=[\"a.b\"])  # works\n    _ddf = dd.read_parquet(f.name, columns=[\"a.b\"])\n```\n\n```\nFile lib/python3.12/site-packages/dask/backends.py:151, in CreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\n    149     raise e\n    150 else:\n--> 151     raise exc from e\n\nKeyError: 'An error occurred while calling the read_parquet method registered to the pandas backend.\\nOriginal Message: \"None of [Index([\\'a.b\\'], dtype=\\'object\\')] are in the [columns]\"'\n```\n\n**Anything else we need to know?**:\n\nIt prevents us to provide efficient struct-column field selection in [LSDB](https://github.com/astronomy-commons/lsdb) project.\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.12.10\n- Operating System: macOS 15.4.1\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "phoebecd",
        "body": "Yes, that is correct. Dask dataframe does not support nested column selection for parquet files, but `pandas` and `pyarrow` does. Dask’s `read_parquet` only supports selecting top-level columns. If you specify 'a.b' as a column, Dask looks for a column named 'a.b' at the top level of the dataset. Since the nested field `b` exists inside the struct column `a` (not as a top-level column), Dask does not recognize it and returns an error.\n\nBesides using pandas to select parque nested fields, you can also use PyArrow. Here is how you can select  nested fields using PyArrow:\n```python\n_ddf = pq.read_table(f.name, columns=[\"a.b\"])\n```\n\n"
      },
      {
        "user": "hombit",
        "body": "@phoebecd Thank you for the comment. I believe I’ve highlighted that this is possible with both pyarrow and pandas, but not with Dask DataFrames. I also believe this difference in behavior is not documented in Dask and is likely unexpected by users."
      }
    ]
  },
  {
    "issue_number": 11929,
    "title": "dd.concat([]).reset_index().set_index() raises IndexError",
    "author": "noreentry",
    "state": "open",
    "created_at": "2025-05-02T23:30:05Z",
    "updated_at": "2025-05-19T12:36:28Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Environment**:\n\n- Dask version:\n```\ndask                      2025.4.1           pyhd8ed1ab_0    conda-forge\ndask-core                 2025.4.1           pyhd8ed1ab_0    conda-forge\ndask-expr                 2.0.0              pyhd8ed1ab_0    conda-forge\n```\n\n- Python version: `Python 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0] on linux`\n- Operating System: `Description:  Ubuntu 24.04.2 LTS`\n- Install method (conda, pip, source): `conda`\n\n**Describe the issue**:\nset_index() raises IndexError exception when is called on concatenated df\n```\ndf1 = pd.DataFrame({'trid': [0, 1], 'ts': [100, 101]}).set_index('trid')\ndf1 = dd.from_pandas(df1, npartitions=1)\ndf2 = pd.DataFrame({'trid': [2, 3], 'ts': [102, 103]}).set_index('trid')\ndf2 = dd.from_pandas(df2, npartitions=1)\ndd.concat([df1, df2], axis=0).reset_index().set_index('trid')\n```\n\n```\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/IPython/core/formatters.py:770, in PlainTextFormatter.__call__(self, obj)\n    763 stream = StringIO()\n    764 printer = pretty.RepresentationPrinter(stream, self.verbose,\n    765     self.max_width, self.newline,\n    766     max_seq_length=self.max_seq_length,\n    767     singleton_pprinters=self.singleton_printers,\n    768     type_pprinters=self.type_printers,\n    769     deferred_pprinters=self.deferred_printers)\n--> 770 printer.pretty(obj)\n    771 printer.flush()\n    772 return stream.getvalue()\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/IPython/lib/pretty.py:411, in RepresentationPrinter.pretty(self, obj)\n    400                         return meth(obj, self, cycle)\n    401                 if (\n    402                     cls is not object\n    403                     # check if cls defines __repr__\n   (...)    409                     and callable(_safe_getattr(cls, \"__repr__\", None))\n    410                 ):\n--> 411                     return _repr_pprint(obj, self, cycle)\n    413     return _default_pprint(obj, self, cycle)\n    414 finally:\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/IPython/lib/pretty.py:786, in _repr_pprint(obj, p, cycle)\n    784 \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\n    785 # Find newlines and replace them with p.break_()\n--> 786 output = repr(obj)\n    787 lines = output.splitlines()\n    788 with p.group():\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:429, in FrameBase.__repr__(self)\n    428     def __repr__(self):\n--> 429         data = self._repr_data().to_string(max_rows=5)\n    430         _str_fmt = \"\"\"Dask {klass} Structure:\n    431 {data}\n    432 Dask Name: {name}, {n_expr}\n    433 Expr={expr}\"\"\"\n    434         if not isinstance(self, Series) and not len(self.columns):\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4063, in DataFrame._repr_data(self)\n   4061 def _repr_data(self):\n   4062     meta = self._meta\n-> 4063     index = self._repr_divisions\n   4064     cols = meta.columns\n   4065     if len(cols) == 0:\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:2616, in FrameBase._repr_divisions(self)\n   2614 @property\n   2615 def _repr_divisions(self):\n-> 2616     name = f\"npartitions={self.npartitions}\"\n   2617     if self.known_divisions:\n   2618         divisions = pd.Index(self.divisions, name=name)\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:352, in FrameBase.npartitions(self)\n    349 @property\n    350 def npartitions(self):\n    351     \"\"\"Return number of partitions\"\"\"\n--> 352     return self.expr.npartitions\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_shuffle.py:803, in BaseSetIndexSortValues.npartitions(self)\n    801 @property\n    802 def npartitions(self):\n--> 803     return self.operand(\"npartitions\") or len(self._divisions()) - 1\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_shuffle.py:786, in BaseSetIndexSortValues._divisions(self)\n    779 if (\n    780     is_index_like(self._divisions_column._meta)\n    781     and self._divisions_column.known_divisions\n    782     and self._divisions_column.npartitions == self.frame.npartitions\n    783 ):\n    784     return self.other.divisions\n--> 786 divisions, mins, maxes, presorted = _get_divisions(\n    787     self.frame,\n    788     self._divisions_column,\n    789     self._npartitions_input,\n    790     self.ascending,\n    791     upsample=self.upsample,\n    792 )\n    793 if presorted and len(mins) == self._npartitions_input:\n    794     divisions = mins.copy() + [maxes[-1]]\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_shuffle.py:1333, in _get_divisions(frame, other, npartitions, ascending, partition_size, upsample)\n   1331 if key in divisions_lru:\n   1332     return divisions_lru[key]\n-> 1333 result = _calculate_divisions(\n   1334     frame, other, npartitions, ascending, partition_size, upsample\n   1335 )\n   1336 divisions_lru[key] = result\n   1337 return result\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_shuffle.py:1357, in _calculate_divisions(frame, other, npartitions, ascending, partition_size, upsample)\n   1354     other = new_collection(other).cat.as_ordered()._expr\n   1356 try:\n-> 1357     divisions, mins, maxes = compute(\n   1358         new_collection(RepartitionQuantiles(other, npartitions, upsample=upsample)),\n   1359         new_collection(other).map_partitions(M.min),\n   1360         new_collection(other).map_partitions(M.max),\n   1361     )\n   1362 except TypeError as e:\n   1363     # When there are nulls and a column is non-numeric, a TypeError is sometimes raised as a result of\n   1364     # 1) computing mins/maxes above, 2) every null being switched to NaN, and 3) NaN being a float.\n   1365     # Also, Pandas ExtensionDtypes may cause TypeErrors when dealing with special nulls such as pd.NaT or pd.NA.\n   1366     # If this happens, we hint the user about eliminating nulls beforehand.\n   1367     if not pd.api.types.is_numeric_dtype(other._meta.dtype):\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/base.py:678, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    659 expr = FinalizeCompute(expr)\n    661 with shorten_traceback():\n    662     # The high level optimize will have to be called client side (for now)\n    663     # The optimize can internally trigger already a computation\n   (...)    675     # change the graph submission to a handshake which introduces all sorts\n    676     # of concurrency control issues)\n--> 678     expr = expr.optimize()\n    679     keys = list(flatten(expr.__dask_keys__()))\n    681     results = schedule(expr, keys, **kwargs)\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/_expr.py:416, in Expr.optimize(self, fuse)\n    413 def optimize(self, fuse: bool = False) -> Expr:\n    414     stage: OptimizerStage = \"fused\" if fuse else \"simplified-physical\"\n--> 416     return optimize_until(self, stage)\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/_expr.py:900, in optimize_until(expr, stage)\n    897     return result\n    899 # Simplify\n--> 900 expr = result.simplify()\n    901 if stage == \"simplified-logical\":\n    902     return expr\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/_expr.py:426, in Expr.simplify(self)\n    424 while True:\n    425     dependents = collect_dependents(expr)\n--> 426     new = expr.simplify_once(dependents=dependents, simplified={})\n    427     if new._name == expr._name:\n    428         break\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/_expr.py:396, in Expr.simplify_once(self, dependents, simplified)\n    393 if isinstance(operand, Expr):\n    394     # Bandaid for now, waiting for Singleton\n    395     dependents[operand._name].append(weakref.ref(expr))\n--> 396     new = operand.simplify_once(\n    397         dependents=dependents, simplified=simplified\n    398     )\n    399     simplified[operand._name] = new\n    400     if new._name != operand._name:\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/_expr.py:396, in Expr.simplify_once(self, dependents, simplified)\n    393 if isinstance(operand, Expr):\n    394     # Bandaid for now, waiting for Singleton\n    395     dependents[operand._name].append(weakref.ref(expr))\n--> 396     new = operand.simplify_once(\n    397         dependents=dependents, simplified=simplified\n    398     )\n    399     simplified[operand._name] = new\n    400     if new._name != operand._name:\n\n    [... skipping similar frames: Expr.simplify_once at line 396 (2 times)]\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/_expr.py:396, in Expr.simplify_once(self, dependents, simplified)\n    393 if isinstance(operand, Expr):\n    394     # Bandaid for now, waiting for Singleton\n    395     dependents[operand._name].append(weakref.ref(expr))\n--> 396     new = operand.simplify_once(\n    397         dependents=dependents, simplified=simplified\n    398     )\n    399     simplified[operand._name] = new\n    400     if new._name != operand._name:\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/_expr.py:379, in Expr.simplify_once(self, dependents, simplified)\n    377 # Allow children to simplify their parents\n    378 for child in expr.dependencies():\n--> 379     out = child._simplify_up(expr, dependents)\n    380     if out is None:\n    381         out = expr\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_concat.py:280, in Concat._simplify_up(self, parent, dependents)\n    260 frames = [\n    261     (\n    262         frame[cols]\n   (...)    268     if len(cols) > 0\n    269 ]\n    270 result = type(self)(\n    271     self.join,\n    272     self.ignore_order,\n   (...)    277     *frames,\n    278 )\n--> 280 if result.columns == _convert_to_list(parent.operand(\"columns\")):\n    281     if result.ndim == parent.ndim:\n    282         return result\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py:451, in Expr.columns(self)\n    448 @property\n    449 def columns(self) -> list:\n    450     try:\n--> 451         return list(self._meta.columns)\n    452     except AttributeError:\n    453         if self.ndim == 1:\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/functools.py:998, in cached_property.__get__(self, instance, owner)\n    996 val = cache.get(self.attrname, _NOT_FOUND)\n    997 if val is _NOT_FOUND:\n--> 998     val = self.func(instance)\n    999     try:\n   1000         cache[self.attrname] = val\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dask_expr/_concat.py:62, in Concat._meta(self)\n     58 @functools.cached_property\n     59 def _meta(self):\n     60     # ignore DataFrame without columns to avoid dtype upcasting\n     61     return make_meta(\n---> 62         methods.concat(\n     63             [\n     64                 meta_nonempty(df._meta)\n     65                 for df in self._frames\n     66                 if df.ndim < 2 or len(df._meta.columns) > 0\n     67             ],\n     68             join=self.join,\n     69             filter_warning=False,\n     70             axis=self.axis,\n     71             ignore_order=self.ignore_order,\n     72             **self._kwargs,\n     73         )\n     74     )\n\nFile ~/miniconda3/envs/py312forge/lib/python3.12/site-packages/dask/dataframe/dispatch.py:67, in concat(dfs, axis, join, uniform, filter_warning, ignore_index, **kwargs)\n     65     return dfs[0]\n     66 else:\n---> 67     func = concat_dispatch.dispatch(type(dfs[0]))\n     68     return func(\n     69         dfs,\n     70         axis=axis,\n   (...)     75         **kwargs,\n     76     )\n\nIndexError: list index out of range\n```\n",
    "comments": [
      {
        "user": "Tunahanyrd",
        "body": "I tried to reproduce this issue on my system:\n\n- OS: Arch Linux (kernel 6.14.6-zen1-1-zen)\n- Dask: 2025.5.0 (installed via pip)\n- Pandas: 2.2.3 (pip)\n- Python: 3.10.16\n- GPU: NVIDIA GeForce RTX 4060 Max-Q\n\nWith the code sample provided in this issue, **no error occurs** and everything works as expected.\n\nIt may be that this issue was fixed in Dask 2025.5.0, or it could be related to the difference between conda-forge and pip installations, or OS differences.\n\n@noreentry Could you please try with Dask 2025.5.0 and check if the error is still present on your side?\n\nLet me know if you need my full environment details or if you want me to try other scenarios!"
      }
    ]
  },
  {
    "issue_number": 11884,
    "title": "`.str.split` coerces `dtype` to `object`",
    "author": "noahblakesmith",
    "state": "open",
    "created_at": "2025-04-11T07:23:36Z",
    "updated_at": "2025-05-19T10:58:33Z",
    "labels": [
      "good first issue",
      "dataframe"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nThe `.str.split` method coerces `dtype` from `string` to `object`. This behavior is also inconsistent with `pandas` (if that matters).\n\n**Minimal complete verifiable example**:\n\nCode\n```python\nimport dask.dataframe as dd\nimport pandas as pd\n\ndata = {\"c\": [\"a,b,c\", \"d,e,f\", \"g,h,i\"]}\n\n# `pandas`\ndf = pd.DataFrame(data, dtype=\"string[pyarrow]\")\nprint(df.dtypes)\nprint(df[\"c\"].str.split(\",\", n=1, expand=True).dtypes)\n\n# `dask`\nddf = dd.from_pandas(df)\nprint(ddf.dtypes)\nprint(ddf[\"c\"].str.split(\",\", n=1, expand=True).dtypes)\n```\n\nOutput\n```\nc    string[pyarrow]\ndtype: object\n0    string[pyarrow]\n1    string[pyarrow]\ndtype: object\nc    string[pyarrow]\ndtype: object\n0    object\n1    object\ndtype: object\n```\n\n**Environment**:\n\n- Dask version: 2025.3.0\n- Python version: 3.10.16\n- Operating System: Ubuntu 24.04.2 LTS\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "Note that after computing, the dtypes are correct:\n\n```python\nIn [6]: ddf[\"c\"].str.split(\",\", n=1, expand=True).compute().dtypes\nOut[6]: \n0    string[pyarrow]\n1    string[pyarrow]\ndtype: object\n```\n\nso it's just the `_meta` after `.str.split`. https://github.com/dask/dask/blob/0fa5e18d511c49f1a9cd5f98c675a9f6cd2fc02f/dask/dataframe/dask_expr/_str_accessor.py#L186-L189 looks a bit suspicious. I haven't stepped through it, but it seems like we're doing `pd.Series(['list', 'of', 'strings'])` which pandas will infer as `object`. Maybe we need to pass a dtype there? Mind taking a look?"
      },
      {
        "user": "Tunahanyrd",
        "body": "Hey! I’ve prepared a fix for this issue and confirmed that it passes tests (including preservation of `string[pyarrow]` dtype). I’ll open a PR shortly"
      }
    ]
  },
  {
    "issue_number": 11686,
    "title": "`map_blocks` shape and chunk calculations are wrong when combining chunk aligned but different `ndim` arrays",
    "author": "tasansal",
    "state": "open",
    "created_at": "2025-01-21T01:27:54Z",
    "updated_at": "2025-05-19T02:12:53Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nWhen doing operations on chunk-aligned 3D and 2D arrays (I have a more complicated case here, but for simplicity, let's say we want to add the sum of the last axis of the 3D to the 2D array) in `map_blocks`, the output shape and chunks are inferred AND miscalculated. Equivalent to `array_3d.sum(axis=-1) + array_2d`.\n\nIn the example below, I omitted the actual operation I mentioned above because the inputs sent to the function by `map_blocks` are also the incorrect size and operation fails.\n\nChanging it into:\n```python\ndef create_2d_from_both(block_3d, block_2d):\n    sum_3d = block_3d.sum(axis=-1)\n    return block_2d + sum_3d\n```\n\nerrors out \n\n```python\nCell In[55], line 18, in create_2d_from_both(block_3d, block_2d)\n     16 def create_2d_from_both(block_3d, block_2d):\n     17     sum_3d = block_3d.sum(axis=-1)\n---> 18     return block_2d + sum_3d\n\nValueError: operands could not be broadcast together with shapes (1,3) (1,2) \n```\n\nIn example below the expected result (without sum):\n```python\narray([[0, 1, 2],\n       [3, 4, 5]])\n```\n\nbut it returns:\n```python\narray([[0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5]])\n```\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\n\n# Define the shapes\nshape_3d = (2, 3, 4)  # 3D array shape\nshape_2d = (2, 3)     # 2D array shape\n\n# Explicit chunk definitions\nchunks_3d = (1, 2, 3)\nchunks_2d = (1, 2)\n\n# Create Dask arrays with specified chunks\narray_3d = da.arange(24).reshape(shape_3d).rechunk(chunks_3d)\narray_2d = da.arange(6).reshape(shape_2d).rechunk(chunks_2d)\n\n# Define a function that reduces the 5D array along its last axis\ndef create_2d_from_both(block_3d, block_2d):\n    # Do something with block_3d and block_2d\n    # That generates something exactly like block_2d\n    return block_2d\n\n# Use map_blocks with explicit chunk control\nresult = da.map_blocks(\n    create_2d_from_both,\n    array_3d,\n    array_2d,\n    dtype=array_2d.dtype,\n    chunks=chunks_2d,\n    drop_axis=[2],  # losing last axis\n)\n\n# Compute and verify the result\ncomputed_result = result.compute(scheduler=\"single-threaded\")\nprint(\"Before compute (inferred) shape:\", result.shape, \"chunks:\", result.chunks)\nprint(\"Computed result shape:\", computed_result.shape, \"chunks:\", result.chunks)\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.1.0\n- Python version: 3.12\n- Operating System: Mac OS\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "tasansal",
        "body": "Looking at shapes within the mapped function:\n\n```python\ndef create_2d_from_both(block_3d, block_2d):\n    print(block_3d.shape)\n    print(block_2d.shape)\n    return block_2d\n```\n\nwe get:\n```python\n(1, 1, 4)\n(1, 3)\n(1, 1, 4)\n(1, 3)\n(1, 2, 4)\n(1, 3)\n(1, 2, 4)\n(1, 3)\n```\n\nSo for some reason the last axis of `array_2d` (dimension=1 overall) seems to be concatenated too? According to `drop_axis` documentation, it should only concatenate the `drop_axis` dimension, which is two?"
      },
      {
        "user": "tasansal",
        "body": "Adding a dummy dimension to the 2D array, as a bandaid solution, works as expected. The extra dimension has to be dropped within the mapped function as well.\n\n```diff\ndef create_2d_from_both(block_3d, block_2d):\n    # Do something with block_3d and block_2d\n    # That generates something exactly like block_2d\n-    return block_2d\n+    return block_2d[..., 0]\n\nresult = da.map_blocks(\n    create_2d_from_both,\n    array_3d,\n+    array_2d[..., None]\n-    array_2d,\n-    dtype=array_2d.dtype,\n-    chunks=chunks_2d,\n    drop_axis=[2],  # losing last axis\n)\n```"
      },
      {
        "user": "fjetter",
        "body": "Thanks for the reproducer. I can reproduce this behavior.\n\ncc @phofl "
      }
    ]
  },
  {
    "issue_number": 11934,
    "title": "map_overlap does not support overlapping depth larger than input array",
    "author": "jdemaria",
    "state": "open",
    "created_at": "2025-05-06T13:52:49Z",
    "updated_at": "2025-05-16T17:08:09Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nmap_overlap does not support overlapping depth larger than input array: it is the normal behavior or a bug? Because I don't understand why this case is not allowed.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\nda.zeros((2,5)).map_overlap(lambda x:x, depth=3)\n```\nFull Traceback:\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \".../lib/python3.11/site-packages/dask/array/core.py\", line 2702, in map_overlap\n    return map_overlap(\n           ^^^^^^^^^^^^\n  File \".../lib/python3.11/site-packages/dask/array/overlap.py\", line 717, in map_overlap\n    args = [\n           ^\n  File \".../lib/python3.11/site-packages/dask/array/overlap.py\", line 718, in <listcomp>\n    overlap(x, depth=d, boundary=b, allow_rechunk=allow_rechunk)\n  File \".../lib/python3.11/site-packages/dask/array/overlap.py\", line 427, in overlap\n    new_chunks = tuple(\n                 ^^^^^^\n  File \".../lib/python3.11/site-packages/dask/array/overlap.py\", line 428, in <genexpr>\n    ensure_minimum_chunksize(size, c) for size, c in zip(depths, x.chunks)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../lib/python3.11/site-packages/dask/array/overlap.py\", line 356, in ensure_minimum_chunksize\n    raise ValueError(\nValueError: The overlapping depth 3 is larger than your array 2.\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2024.5.2\n- Python version: Python 3.11.12 | packaged by conda-forge | (main, Apr 10 2025, 22:23:25) [GCC 13.3.0] on linux\n- Operating System: Rocky 9.5\n- Install method (conda, pip, source): Micromamba forge then pip inside\n",
    "comments": [
      {
        "user": "guillaumeeb",
        "body": "Hi @jdemaria, do you have an example where this might be needed?\n\nIf you need a bigger overlap in column axe, you can always use:\n```python\nda.zeros((2,5)).map_overlap(lambda x:x, depth=(1,3))\n```\n\nI don't see how an overlap bigger than your array will be useful."
      },
      {
        "user": "jdemaria",
        "body": "Hi @guillaumeeb, thx for your answer, an exemple: I have a processor which can process big images but also small subsets, and this processing uses a fixed overlap and I want to be able to run it even if the subset in rows is smaller than the needed overlap. In my case I want a small subset to have a small fast run for debug, but I think that the use case of applying some algo which need overlap on data smaller than overlap is a real use case.\n\nIn fact I don't understand why we should have a constraint on that? What's is the reason? except maybe for internal technical dask constraints/limitations?"
      }
    ]
  },
  {
    "issue_number": 11937,
    "title": "`dask.array.eye` brings error results",
    "author": "apiqwe",
    "state": "open",
    "created_at": "2025-05-08T10:16:32Z",
    "updated_at": "2025-05-16T16:35:35Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nI found that `dask.array.eye` will bring error results as follow.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.array as da\n\nprint(np.eye(2, 3))\nprint(da.eye(2, 3).compute())\n```\nOutput:\n```bash\n[[1. 0. 0.]\n [0. 1. 0.]]\n[[1. 0.]\n [0. 1.]]\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.10.0 \n- Operating System: Linux 6.11.0-25-generic  Ubuntu 24.04.1 x86_64  GNU/Linux\n- Install method (conda, pip, source): pip\n- Numpy version: 2.2.4\n",
    "comments": [
      {
        "user": "guillaumeeb",
        "body": "Hi @apiqwe,\n\nActually, there is a more important problem. \n\nFirst, your code is wrong, if you don't use named kwargs, the second argument of dask eye function is the chunksize. So no effect on the number of columns. This might not be a good thing for people coming from Numpy... I guess chunks kwarg should be at the end.\n\nAnyway, there is an error when using appropriate code, **only if M is > than N**:\n```python\nprint(da.eye(2, M=3).compute())\n```\nResults in\n```\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[47], line 1\n----> 1 print(da.eye(4, M=53).compute())\n\nFile /work/scratch/env/eynardbg/.conda/envs/pangeo3.12/lib/python3.12/site-packages/dask/base.py:372, in DaskMethodsMixin.compute(self, **kwargs)\n    348 def compute(self, **kwargs):\n    349     \"\"\"Compute this dask collection\n    350 \n    351     This turns a lazy Dask collection into its in-memory equivalent.\n   (...)\n    370     dask.compute\n    371     \"\"\"\n--> 372     (result,) = compute(self, traverse=False, **kwargs)\n    373     return result\n\nFile /work/scratch/env/eynardbg/.conda/envs/pangeo3.12/lib/python3.12/site-packages/dask/base.py:660, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    657     postcomputes.append(x.__dask_postcompute__())\n    659 with shorten_traceback():\n--> 660     results = schedule(dsk, keys, **kwargs)\n    662 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile /work/scratch/env/eynardbg/.conda/envs/pangeo3.12/lib/python3.12/site-packages/dask/local.py:309, in nested_get(ind, coll)\n    307     return tuple(nested_get(i, coll) for i in ind)\n    308 else:\n--> 309     return coll[ind]\n\nKeyError: ('eye-945bdca49ce141e5ee7ec08dd7cfca35', 0, 1)\n```"
      }
    ]
  },
  {
    "issue_number": 11950,
    "title": "Document dataclass traversal",
    "author": "gsakkis",
    "state": "open",
    "created_at": "2025-05-14T22:44:23Z",
    "updated_at": "2025-05-14T22:44:33Z",
    "labels": [
      "needs triage"
    ],
    "body": "After a few hours trying to figure out why two very similar cases behave differently, I discovered that dask [traverses dataclasses](https://github.com/dask/dask/pull/4165) (at least [some](https://github.com/dask/dask/issues/9370) of them). AFAICT this isn't documented.\n\nDataclass usage is often a implementation detail. Refactoring a regular class to dataclass or vice versa may lead to hard to diagnose bugs or behavior changes when using dask.\n\nThere is an open issue for [deprecating](https://github.com/dask/dask/issues/9363) this behavior but in the meantime I would strongly recommend mentioning it in the docs at least. ",
    "comments": []
  },
  {
    "issue_number": 11949,
    "title": "Delayed function raises not implemented for dask dataframes passed as kwargs",
    "author": "aaravind100",
    "state": "open",
    "created_at": "2025-05-14T20:17:07Z",
    "updated_at": "2025-05-14T20:17:18Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nDelayed function raises not implemented error when parameters are passed as kwargs as opposed to positional args for dask dataframes.\n\nThis behavior is occurring from v2025.4.0+, works in v2025.3.0. Have not tested in older versions.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nfrom typing import Any\n\nimport dask\nimport pandas as pd\nfrom dask import dataframe as dd\nfrom dask import array as da\n\nprint(dask.__version__)\n\n\n@dask.delayed\ndef foo(a) -> Any:\n    return a\n\n\nn = 1\narr = [1, 2, 3]\ndf = pd.DataFrame({\"a\": [1, 2, 3]})\nddf = dd.from_pandas(df, npartitions=1)\ndarr = da.from_array([1, 2, 3])\n\n\nprint(f\"{foo(n)=}\")  # <- returns Delayed('foo-...') object\nprint(f\"{foo(a=n)=}\")  # <- returns Delayed('foo-...') object\nprint(f\"{foo(arr)=}\")  # <- returns Delayed('foo-...') object\nprint(f\"{foo(a=arr)=}\")  # <- returns Delayed('foo-...') object\nprint(f\"{foo(darr)=}\")  # <- returns Delayed('foo-...') object\nprint(f\"{foo(a=darr)=}\")  # <- returns Delayed('foo-...') object\nprint(f\"{foo(df)=}\")  # <- returns Delayed('foo-...') object\nprint(f\"{foo(a=df)=}\")  # <- returns Delayed('foo-...') object\nprint(f\"{foo(ddf)=}\")  # <- returns Delayed('foo-...') object\nprint(f\"{foo(a=ddf)=}\")  # <- raises NotImplementedError in 2025.4.0+\n```\n\n**Anything else we need to know?**:\n<details>\n  <summary>traceback</summary>\n\n  ```sh\n  Traceback (most recent call last):\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/scraps.py\", line 32, in <module>\n      print(f\"{foo(a=ddf)=}\")  # <- raises NotImplementedError in 2025.4.0+\n               ~~~^^^^^^^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/delayed.py\", line 846, in __call__\n      return call_function(\n          self._obj, self._key, args, kwargs, pure=self._pure, nout=self._nout\n      )\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/delayed.py\", line 816, in call_function\n      dask_kwargs, collections2 = unpack_collections(kwargs)\n                                  ~~~~~~~~~~~~~~~~~~^^^^^^^^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/delayed.py\", line 222, in unpack_collections\n      args, collections = unpack_collections(\n                          ~~~~~~~~~~~~~~~~~~^\n          [[k, v] for k, v in expr.items()], _return_collections=False\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      )\n      ^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/delayed.py\", line 206, in unpack_collections\n      args, collections = utils.unzip(\n                          ~~~~~~~~~~~^\n          (unpack_collections(e, _return_collections=False) for e in expr), 2\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      )\n      ^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/utils.py\", line 2298, in unzip\n      out = list(zip(*ls))\n                 ~~~^^^^^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/delayed.py\", line 207, in <genexpr>\n      (unpack_collections(e, _return_collections=False) for e in expr), 2\n       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/delayed.py\", line 206, in unpack_collections\n      args, collections = utils.unzip(\n                          ~~~~~~~~~~~^\n          (unpack_collections(e, _return_collections=False) for e in expr), 2\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      )\n      ^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/utils.py\", line 2298, in unzip\n      out = list(zip(*ls))\n                 ~~~^^^^^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/delayed.py\", line 207, in <genexpr>\n      (unpack_collections(e, _return_collections=False) for e in expr), 2\n       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/delayed.py\", line 193, in unpack_collections\n      (name,) = expr.__dask_keys__()\n                ~~~~~~~~~~~~~~~~~~^^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/dataframe/dask_expr/_expr.py\", line 94, in __dask_keys__\n      return [(self._name, i) for i in range(self.npartitions)]\n                                             ^^^^^^^^^^^^^^^^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/dataframe/dask_expr/_expr.py\", line 446, in npartitions\n      return len(self.divisions) - 1\n                 ^^^^^^^^^^^^^^\n    File \"/home/aaravind/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/functools.py\", line 1026, in __get__\n      val = self.func(instance)\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/dataframe/dask_expr/_expr.py\", line 431, in divisions\n      return tuple(self._divisions())\n                   ~~~~~~~~~~~~~~~^^\n    File \"/home/aaravind/Documents/Projects/PyProjects/scraps/.venv/lib/python3.13/site-packages/dask/dataframe/dask_expr/_expr.py\", line 434, in _divisions\n      raise NotImplementedError()\n  NotImplementedError\n  ```\n</details>\n\n**Environment**:\n\n- Dask version: 2025.4.0+\n- Python version: 3.13.3\n- Operating System: Ubuntu 24.04.2 LTS (WSL)\n- Install method (conda, pip, source): pip(uv)\n",
    "comments": []
  },
  {
    "issue_number": 3530,
    "title": "Memory leak with Numpy arrays and the threaded scheduler",
    "author": "mrocklin",
    "state": "open",
    "created_at": "2018-05-24T18:14:06Z",
    "updated_at": "2025-05-14T16:23:01Z",
    "labels": [],
    "body": "This example leaks around 500MB of memory on my machine when using the threaded scheduler, and almost no memory when using the single-threaded scheduler:\r\n\r\n```python\r\nimport dask.array as da\r\nx = da.ones((2e4, 2e4), chunks=(2e4, 100))\r\ny = x.rechunk((100, 2e4))\r\nz = y.rechunk((2e4, 100))\r\n\r\nimport psutil\r\nproc = psutil.Process()\r\n\r\nfrom distributed.utils import format_bytes\r\nprint(format_bytes(proc.memory_info().rss))\r\n# 80MB\r\n\r\nfrom dask.diagnostics import ProgressBar\r\nProgressBar().register()\r\n\r\n# z.sum().compute(scheduler='single-threaded')  # This doesn't cause problems\r\nz.sum().compute(scheduler='threads')  # This leaks around 500MB of memory\r\n\r\nprint(format_bytes(proc.memory_info().rss))\r\n# 500-600MB\r\n```\r\n\r\n[Notebook](https://gist.github.com/a32b2ba79dc51ff6f0632104ac23ae40)\r\n\r\nThis doesn't happen when I run it with the single-threaded scheduler.  \r\n\r\nCalling `gc.collect()` doesn't help.  Allocating a new large numpy array afterwards also doesn't take up the leaked memory,  the number just climbs.  Looking at the objects that Python knows about shows that there isn't much around:\r\n\r\n```python\r\nfrom pympler import muppy\r\nall_objects = muppy.get_objects()\r\n\r\nfrom pympler import summary\r\nsum1 = summary.summarize(all_objects)\r\nsummary.print_(sum1)                          \r\n```\r\n\r\n```\r\n                                      types |   # objects |   total size\r\n=========================================== | =========== | ============\r\n                                <class 'str |       60517 |      8.75 MB\r\n                               <class 'dict |       11991 |      5.30 MB\r\n                               <class 'code |       19697 |      2.72 MB\r\n                               <class 'type |        2228 |      2.24 MB\r\n                              <class 'tuple |       16142 |      1.04 MB\r\n                                <class 'set |        2285 |    858.84 KB\r\n                               <class 'list |        7284 |    738.09 KB\r\n                            <class 'weakref |        4412 |    344.69 KB\r\n                        <class 'abc.ABCMeta |         261 |    263.54 KB\r\n                        function (__init__) |        1378 |    183.02 KB\r\n  <class 'traitlets.traitlets.MetaHasTraits |         180 |    175.45 KB\r\n                 <class 'wrapper_descriptor |        2240 |    175.00 KB\r\n                                <class 'int |        5584 |    168.92 KB\r\n                  <class 'getset_descriptor |        2389 |    167.98 KB\r\n            <class 'collections.OrderedDict |         292 |    141.00 KB\r\n```\r\n\r\nThe local schedulers don't have any persistent state.  My next step is to reproduce with the standard concurrent.futures module, but I thought I'd put this up early in case people have suggestions.\r\n\r\ncc @shoyer @pitrou @njsmith",
    "comments": [
      {
        "user": "pitrou",
        "body": "See discussion in https://bugs.python.org/issue33444"
      },
      {
        "user": "mrocklin",
        "body": "Hrm, interesting.  I set `export MALLOC_ARENA_MAX=1`, then start a jupyter notebook and work from there.\r\n\r\nSometimes I get no leak, sometimes I get a much bigger leak at 1.4GB."
      },
      {
        "user": "mrocklin",
        "body": "cc @stefanv and @mattip"
      }
    ]
  },
  {
    "issue_number": 11735,
    "title": "Array indexing is 2-3x slower on 2024.12.0",
    "author": "slevang",
    "state": "closed",
    "created_at": "2025-02-07T17:45:43Z",
    "updated_at": "2025-05-14T15:11:46Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nArray indexing can be 2-3x slower on `2024.12.0`, at least in certain cases when dealing with many chunks. I've narrowed it down to #11548 as the source of the change.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\nimport numpy as np\n\nx = da.ones((1000000,), chunks=(1,))\n%timeit x[np.arange(x.size)]\n```\n\n```\nBefore #11548\n884 ms ± 29.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nAfter\n2.14 s ± 15.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```",
    "comments": [
      {
        "user": "phofl",
        "body": "Can you try this on the newest version? "
      },
      {
        "user": "slevang",
        "body": "I get 1.91s, so there must have been some other marginal improvements but still a significant regression."
      },
      {
        "user": "slevang",
        "body": "Profile indicates this is all time creating the `Task` class in `take`:\n\n![Image](https://github.com/user-attachments/assets/ac35d77b-0257-472c-ba85-ed0728fccec0)"
      }
    ]
  },
  {
    "issue_number": 11753,
    "title": "`ValueError: cannot broadcast shape` in `Array.__setitem__` with boolean mask",
    "author": "TomAugspurger",
    "state": "closed",
    "created_at": "2025-02-17T16:01:52Z",
    "updated_at": "2025-05-13T11:45:13Z",
    "labels": [
      "array",
      "needs attention",
      "bug"
    ],
    "body": "**Describe the issue**:\n\nAfter https://github.com/dask/dask/pull/11728, `Array.__setitem__` with a boolean mass fails with a `ValueError`\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\n\n\na = da.random.uniform(size=10)\nmask = a > 0.5\na[mask] = a\n```\n\nthat raises:\n\n```pytb\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[3], line 6\n      4 a = da.random.uniform(size=10)\n      5 mask = a > 0.5\n----> 6 a[mask] = a\n\nFile ~/gh/dask/dask/dask/array/core.py:1973, in Array.__setitem__(self, key, value)\n   1967 # If value has ndim > 0, they must be broadcastable to self.shape[idx].\n   1968 # This raises when the bool mask causes the size to become unknown,\n   1969 # e.g. this is valid in numpy but raises here:\n   1970 # x = da.array([1,2,3])\n   1971 # x[da.array([True, True, False])] = [4, 5]\n   1972 if value.ndim:\n-> 1973     value = broadcast_to(value, self[key].shape)\n   1975 y = where(key, value, self)\n   1976 # FIXME does any backend allow mixed ops vs. numpy?\n   1977 # If yes, is it wise to let them change the meta?\n\nFile ~/gh/dask/dask/dask/array/core.py:5241, in broadcast_to(x, shape, chunks, meta)\n   5237 ndim_new = len(shape) - x.ndim\n   5238 if ndim_new < 0 or any(\n   5239     new != old for new, old in zip(shape[ndim_new:], x.shape) if old != 1\n   5240 ):\n-> 5241     raise ValueError(f\"cannot broadcast shape {x.shape} to shape {shape}\")\n   5243 if chunks is None:\n   5244     chunks = tuple((s,) for s in shape[:ndim_new]) + tuple(\n   5245         bd if old > 1 else (new,)\n   5246         for bd, old, new in zip(x.chunks, x.shape, shape[ndim_new:])\n   5247     )\n\nValueError: cannot broadcast shape (10,) to shape (nan,)\n```\n\n**Anything else we need to know?**:\n\nThat snippet worked prior to #11728. https://github.com/dask/dask-ml/issues/1012 is a downstream failure in dask-ml from this.\n\n**Environment**:\n\n- Dask version:\n- Python version:\n- Operating System:\n- Install method (conda, pip, source):\n",
    "comments": [
      {
        "user": "phofl",
        "body": "@crusaderky as well"
      },
      {
        "user": "crusaderky",
        "body": "To me this looks like dask-ml is abusing accidental, incorrect, undocumented, and untested behaviour of dask <2025.2.0.\n\nYour reproducer fails on numpy, and rightfully so, as you are trying to call `__setitem__` on an array with size ~5 with a value of size 10.\n\nOn dask 2025.1.0, `a[mask] = b` _accidentally_ behaves like `a[mask] = b[mask]` does on numpy, due to lack of validation. `a[mask] = b[mask]` has always failed on Dask (and I recognize that's a problem).\n\nI would suggest making `a[mask] = b[mask]` work in Dask instead of restoring the previous behaviour. In other words, if both lhs and rhs have the same nan chunks, assume they have been sliced by the same mask and delay shape checks until compute time. This would be generically useful through a variety of use cases.\n\nIn the meantime, dask-ml should use `da.where` instead (which is what `da.Array.__setitem__` calls internally) unless there is a use case about applying the same functions to numpy arrays, which would take a performance hit?\n"
      },
      {
        "user": "TomAugspurger",
        "body": "@crusaderky sorry, I trimmed my minimal example too far. It was supposed to be `a[mask] = a[mask]`. I see now that works when the size of the last dimension is `1`.\n\n```\nimport dask.array as da\n\n\na = da.random.uniform(size=1)\nmask = a > 0.5\na[mask] = a[mask]\n```\n\nI'll look later whether that's just an artifact of the test, or whether it's a behavior dask-ml (and so probably others) were relying on."
      }
    ]
  },
  {
    "issue_number": 11941,
    "title": "Deprecation of HLG?",
    "author": "martindurant",
    "state": "open",
    "created_at": "2025-05-09T14:33:02Z",
    "updated_at": "2025-05-13T09:39:01Z",
    "labels": [
      "needs triage"
    ],
    "body": "dask-expr has been merged into the main code, and increasingly replaced legacy HLG for dataframe and array. As of April, all HLGs are now wrapped in Expressions.\n\n- https://docs.dask.org/en/latest/expr-system-internals.html#legacy-highlevelgraph-hlg-support suggests HLGs remain supported\n- other discussion suggests they are deprecated (but there are no depr warnings).\n\nQuestions: \n- can we rely on HLGs existing into the future? \n- is there a path to migrating HLG code to Expr?",
    "comments": [
      {
        "user": "fjetter",
        "body": "HLGs are still supported but will likely see even less attention than they've had over the last few years. I expect that once dask arrays are backed by HLGs, they will vanish entirely. They'd still be required for bags and delayed but I don't consider it worth keeping HLGs around for just these interfaces, considering that they are barely using any features HLGs offer.\nHowever, I can't tell when or even if the array expression work will conclude. It's been dragging for a few months now and nobody is actively working on it right now. Therefore, HLGs are likely going to stick around for a while longer.\n \nStill, I strongly discourage using them for any new projects and there is no migration path. If Expr are not an option directly, old school low level graphs always will be supported.\n\nThe only project I am aware of that is implementing their own `Layer`s is `dask-awkward`. If that is indeed true and the dask core project no longer needs them, dask-awkward will likely have to start vendoring some code.\n\n---\n\nI suspect this question came up in the context of https://github.com/dask-contrib/dask-awkward/pull/588 / https://github.com/dask/dask/issues/11927 and I want to point out that these compatibility issues do not stem from HLGs themselves or the Expr interface but are rather originating from a combination of other changes. Primarily it stems from us always adding a task to the graph that will concatenate end results (i.e. postcompute). Your custom optimizer is changing the graph topology by removing entire computation branches which is not reflected in the postcompute task."
      },
      {
        "user": "martindurant",
        "body": "Quick searching suggests hlg/layer direct usage in the following:\n \n- dask-sql, e.g., https://github.com/dask-contrib/dask-sql/blob/775b56fb8f99a5e061bc9c9fc86a035554489578/dask_sql/physical/utils/filter.py#L344\n- dask-histogram (which does not require dask-awkward)\n- maybe (not thoroughly checked)\n  - flox\n  - cudf\n  - xarray\n\nDo I understand that delayed, bag and submit/map are to disappear, then? Or will the user APIs still work, but without HLGs?"
      },
      {
        "user": "dcherian",
        "body": "flox and xarray only nominally use HLGs. Both those projects create explicit dict graphs and just wrap them up in a Layer"
      }
    ]
  },
  {
    "issue_number": 5229,
    "title": "Dask hangs when running certain tasks depending on number of nodes ",
    "author": "ivirshup",
    "state": "closed",
    "created_at": "2019-08-06T10:50:59Z",
    "updated_at": "2025-05-13T09:06:55Z",
    "labels": [],
    "body": "I'm sure there's a more succinct title, but it's the best I can explain it. Dask just seems to hang if I run UMAP multiple times.\r\n\r\nI've been able to trigger this behavior by running UMAP with different parameters in parallel using dask. Some of the computations may complete before the process hangs. I think this is dependent on the number of computations and workers.\r\n\r\n```python\r\nfrom umap import UMAP\r\nimport dask\r\nfrom dask.distributed import Client\r\nimport numpy as np\r\nfrom functools import reduce\r\n\r\ndef run_umap(data, n_neighbors, min_dist) -> np.ndarray:\r\n    return UMAP(n_neighbors=n_neighbors, min_dist=min_dist).fit_transform(data)\r\n\r\ndef sum_all(x):\r\n    return reduce(np.add, x)\r\n\r\nclient = Client()  # This also occurs with the threaded schedueler\r\n# dask.config.set(scheduler='threads')\r\n\r\nN = 10 # Some may complete, seems like fewer complete when N is larger\r\ndata = np.random.random_sample((100, 100))\r\ncomps = []\r\n\r\nfor n in np.linspace(0, 1, N):\r\n    comps.append(dask.delayed(run_umap)(data, 15, n))\r\n\r\njob = dask.delayed(sum_all)(comps)\r\n\r\njob.compute()\r\n```\r\n\r\nAfter some of the tasks complete (sometimes none), the task graph have some nodes stuck at \"processing\":\r\n\r\n![bokeh_plot](https://user-images.githubusercontent.com/8238804/62531056-04ab3100-b885-11e9-984b-f9a10bea0731.png)\r\n\r\nVersion info: `python==3.7.3 dask==2.2.0 distributed==2.2.0 umap-learn==0.3.9 numba==0.45.1`\r\n\r\nMy actual use case involves just using the graph building parts of UMAP (similar to pynndescent) which still triggers this behavior but takes more setup. My suspicion is that it has something to do with how UMAP heavy use of numba, or I've done something terribly wrong. I'd definitely appreciate any tips or suggestions for what to try when debugging. Thanks for any help!",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "I notice that the tasks finish when the workers are started with one thread per worker (`Client(threads_per_worker=1`) Do you know how if UMAP interacts well with multiple threads?"
      },
      {
        "user": "jrbourbeau",
        "body": "UMAP utilizes `@numba.njit(parallel=True)` throughout its codebase. Numba and Dask aren't currently able to share threadpools, so using multi-threaded Numba and multi-threaded Dask can lead to the type of hanging you're seeing @ivirshup. See #5132 for another example. I'd recommend either using single-threaded Dask (as @TomAugspurger pointed out), or using single-threaded Numba "
      },
      {
        "user": "ivirshup",
        "body": "Thanks! Setting the `threads_per_worker=1` worked great! Should I also expect potential halting with any library that might multithread its operations?\r\n\r\nI'd like to try telling numba to use a single thread in each of the worker processes. I think I've got that working by creating a script containing `import os; os.environ[\"NUMBA_NUM_THREADS\"] = \"1\"` and passing that to `Client(preload=[\"path/to/script.py\"])`. Is there a way I could just pass a dict of environment variables?"
      }
    ]
  },
  {
    "issue_number": 11768,
    "title": "querying df.compute(concatenate=True)",
    "author": "martindurant",
    "state": "open",
    "created_at": "2025-02-20T19:50:49Z",
    "updated_at": "2025-05-12T02:11:33Z",
    "labels": [
      "needs triage"
    ],
    "body": "https://github.com/dask/dask-expr/pull/1138 introduced the `concatenate` kwargs to dask-dataframe compute operations, and defaulted to True (a change in behaviour). This is now the default in core dask following the merger of expr into the main repo.\n\nI am concerned that the linked PR did not provide any rationale for the change, nor document under what circumstances it should *not* be used.\n\n>             Concatenating enables more powerful optimizations but it also incurs additional\n>             data transfer cost. Generally, it should be enabled.\n\nI suggest the following contraindications:\n- worker memory limits are generally much more strict than in the client, so concatenating in-cluster can crash the specific worker and make the workflow unrunnable\n- the concatenation task cannot begin until all of its inputs are ready, whereas the client can download each partition as it completes, so in the straggler case, concatenate=True will tend to be slower\n\nI can see the option being useful in the case that:\n- there are a large number of small partitions in the output, and we expect the inter-worker latency to be much more favourable than the client-worker latency\n\nI can see the option making no difference in the case that:\n- the number of partitions is small compared to the total volume of data in the output, but there is no worker memory issue\n\ncf https://github.com/dask/community/issues/411",
    "comments": [
      {
        "user": "fjetter",
        "body": "This is a bit more complex topic than the community issue makes it look like. Right now, the behavior is one of three different kinds depending on what API and collection type you are using.\n\n1. If you're using the _method_, `DataFrame.compute` the behavior is as you are describing. It concatenates results and is fetching it remotely. This behavior is unique to `DataFrame`s since other collections are just falling back to 2.)\n2. If you're using the top-level function `dask.compute` it is **skipping** expressions optimizations entirely and is fetching individual partitions and concatenates them on the client\n3. If you're using `Client.compute` it also skips the expressions optimization but is concatenating results on the cluster.\n\nFixing this goes way beyond a keyword argument. I'm currently working on https://github.com/dask/dask/pull/11736 which will fix all of this but my current approach will hardcode this to concatenation on the cluster. The reason for this is that it is actually much easier to implement consistently. I'm open to adding this feature back in, in a follow up PR but for now I care much more about consistency (and the HLG/expr pickle change that is the actual intention of that PR). \n\nBelow a couple of comments about your assumptions\n\n---\n\n> worker memory limits are generally much more strict than in the client, so concatenating in-cluster can crash the specific worker and make the workflow unrunnable\n\nI doubt this is true. My notebook has 16GB of RAM (M1 Mac Book) and I rarely run computations on machines with less. I think this isn't untypical. I also see Coiled users rarely running on smaller workers.\n\nBesides, the default behavior of clients is to fetch data via the scheduler (and it's not trivial to change that for most users), i.e. in most situations where the memory on a single worker is not sufficient, the most likely outcome is that the scheduler runs OOM before the client. The scheduler dying is pretty much the worst situation because we cannot provide proper exceptions to the users in this case. However, if the concatentation happens on the worker, the scheduler is currently able to abort the finalize task and is raising a proper exception if we know that the data doesn't fit.\n\nThe only marginal benefit from client side concatenation is that we safe ourselves one cluster internal network hop but most applications will not care about this.\n\n> the concatenation task cannot begin until all of its inputs are ready, whereas the client can download each partition as it completes, so in the straggler case, concatenate=True will tend to be slower\n\nWhile technically true, this behavior isn't implemented. The only argument for speed that is currently true is that there is one more cluster internal network bump."
      },
      {
        "user": "fjetter",
        "body": "TLDR with https://github.com/dask/dask/pull/11736 I am proposing to hard code this to \"always concat on cluster\""
      },
      {
        "user": "martindurant",
        "body": "I believe that the following proves that the client is able to download partitions as they become available, at least via delayed. Note that concatenate=True makes no difference for this flow, perhaps because of the use of delayed\n```python\nclient = Client()\n\nclass deserme:\n    def __reduce__(self):\n        import time\n        import os\n        print(os.getppid(), time.time(), \"deser!\")\n        return (lambda: 1), ()\n\ndef part():\n    import time\n    time.sleep(random.randint(1, 5))\n    return pd.DataFrame({\"a\": [deserme()]})\n\ndf = dd.from_delayed([dask.delayed(part, pure=False)() for _ in range(10)])\ndf.compute(concatenate=False);\n\n# produces\n99299 1740494986.1009972 deser!\n99299 1740494986.101203 deser!\n99299 1740494986.1013372 deser!\n99299 1740494986.102061 deser!\n99299 1740494986.102284 deser!\n99299 1740494986.102438 deser!\n99299 1740494987.1030989 deser!\n99299 1740494987.103334 deser!\n99299 1740494987.1034791 deser!\n```\n(note that some objects arrive before others)"
      }
    ]
  },
  {
    "issue_number": 11936,
    "title": "`dask.array.array` brings different results with `numpy.array`",
    "author": "apiqwe",
    "state": "open",
    "created_at": "2025-05-07T17:05:55Z",
    "updated_at": "2025-05-09T10:25:05Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nI found that `dask.array.array` will bring different results with `numpy.array` as follow.\nI think this may be a bug in `dask`.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.array as da\n\nprint(np.array([(1,2),(3,4)],dtype=[('x','i4'),('y','i4')]))\nprint(da.array([(1,2),(3,4)],dtype=[('x','i4'),('y','i4')]).compute())\n```\nOutput:\n```bash\n[(1, 2) (3, 4)]\n[[(1, 1) (2, 2)]\n [(3, 3) (4, 4)]]\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.10.0 \n- Operating System: Linux 6.11.0-25-generic  Ubuntu 24.04.1 x86_64  GNU/Linux\n- Install method (conda, pip, source): pip\n- Numpy version: 2.2.4\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "I can confirm this behavior. I'm not entirely sure who's at fault. I'm not very familiar with this dtype format but to me this looks like a numpy bug (but not where you'd expect).\n\nThe `da.array` constructor internally first generates an ordinary array and then applies the dtype using `astype`. That's arguably suboptimal behavior but does the trick. However, the `astype` is just forwarding to numpy itself and numpy does this...\n\n```python\n>>> import numpy as np\n>>> np.array([(1,2),(3,4)]).astype([('x','i4'),('y','i4')])\n\narray([[(1, 1), (2, 2)],\n       [(3, 3), (4, 4)]], dtype=[('x', '<i4'), ('y', '<i4')])\n```\n\nwhich is exactly what dask returns.\n"
      },
      {
        "user": "apiqwe",
        "body": "> I can confirm this behavior. I'm not entirely sure who's at fault. I'm not very familiar with this dtype format but to me this looks like a numpy bug (but not where you'd expect).\n> \n> The `da.array` constructor internally first generates an ordinary array and then applies the dtype using `astype`. That's arguably suboptimal behavior but does the trick. However, the `astype` is just forwarding to numpy itself and numpy does this...\n> \n> >>> import numpy as np\n> >>> np.array([(1,2),(3,4)]).astype([('x','i4'),('y','i4')])\n> \n> array([[(1, 1), (2, 2)],\n>        [(3, 3), (4, 4)]], dtype=[('x', '<i4'), ('y', '<i4')])\n> which is exactly what dask returns.\n\nThanks for your reply. How about I take an issue to `numpy`?"
      },
      {
        "user": "apiqwe",
        "body": "> I can confirm this behavior. I'm not entirely sure who's at fault. I'm not very familiar with this dtype format but to me this looks like a numpy bug (but not where you'd expect).\n> \n> The `da.array` constructor internally first generates an ordinary array and then applies the dtype using `astype`. That's arguably suboptimal behavior but does the trick. However, the `astype` is just forwarding to numpy itself and numpy does this...\n> \n> >>> import numpy as np\n> >>> np.array([(1,2),(3,4)]).astype([('x','i4'),('y','i4')])\n> \n> array([[(1, 1), (2, 2)],\n>        [(3, 3), (4, 4)]], dtype=[('x', '<i4'), ('y', '<i4')])\n> which is exactly what dask returns.\n\nI have token an [issue](https://github.com/numpy/numpy/issues/28924) to `numpy`, and their developers believed that `numpy`'s results are correct."
      }
    ]
  },
  {
    "issue_number": 11940,
    "title": "to_parquet() throws \"ArrowInvalid: Invalid column index to set field.\"",
    "author": "dbalabka",
    "state": "open",
    "created_at": "2025-05-09T10:16:24Z",
    "updated_at": "2025-05-09T10:21:03Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\nWhen we writing data frame into parquet we face with the following issue:\n```\nFile ~/src/.venv/lib/python3.11/site-packages/dask_expr/_collection.py:3331, in DataFrame.to_parquet(self, path, **kwargs)\n   3328 def to_parquet(self, path, **kwargs):\n   3329     from dask_expr.io.parquet import to_parquet\n-> 3331     return to_parquet(self, path, **kwargs)\n\nFile ~/src/.venv/lib/python3.11/site-packages/dask_expr/io/parquet.py:595, in to_parquet(df, path, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, engine, **kwargs)\n    586     raise ValueError(\n    587         \"User-defined key/value metadata (custom_metadata) can not \"\n    588         \"contain a b'pandas' key.  This key is reserved by Pandas, \"\n    589         \"and overwriting the corresponding value can render the \"\n    590         \"entire dataset unreadable.\"\n    591     )\n    593 # Engine-specific initialization steps to write the dataset.\n    594 # Possibly create parquet metadata, and load existing stuff if appending\n--> 595 i_offset, fmd, metadata_file_exists, extra_write_kwargs = engine.initialize_write(\n    596     df,\n    597     fs,\n    598     path,\n    599     append=append,\n    600     ignore_divisions=ignore_divisions,\n    601     partition_on=partition_on,\n    602     division_info=division_info,\n    603     index_cols=index_cols,\n    604     schema=schema,\n    605     custom_metadata=custom_metadata,\n    606     **kwargs,\n    607 )\n    609 # By default we only write a metadata file when appending if one already\n    610 # exists\n    611 if append and write_metadata_file is None:\n\nFile ~/src/.venv/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:736, in ArrowDatasetEngine.initialize_write(cls, df, fs, path, append, partition_on, ignore_divisions, division_info, schema, index_cols, **kwargs)\n    734             i = inferred_schema.get_field_index(name)\n    735             j = schema.get_field_index(name)\n--> 736             inferred_schema = inferred_schema.set(i, schema.field(j))\n    737     schema = inferred_schema\n    739 # Check that target directory exists\n\nFile ~/src/.venv/lib/python3.11/site-packages/pyarrow/types.pxi:3047, in pyarrow.lib.Schema.set()\n\nFile ~/src/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:154, in pyarrow.lib.pyarrow_internal_check_status()\n\nFile ~/src/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:91, in pyarrow.lib.check_status()\n\nArrowInvalid: Invalid column index to set field.\n```\n\n**Minimal Complete Verifiable Example**:\n\n```python\nTBD\n```\n\n**Environment**:\n```\ndask = \"2024.12.1\"\npyarrow = \"14.0.2\"\npandas = \"2.2.3\"\n```\n\n- Python version: 3.11\n- Operating System: WSL, Ubuntu \n- Install method (conda, pip, source): poetry\n",
    "comments": [
      {
        "user": "dbalabka",
        "body": "Related PyArrow PR:\nhttps://github.com/apache/arrow/pull/11877\n\n\nhttps://github.com/apache/arrow/blob/e2a5b4e90243c8b677218fa2f7d8cb483fb3a840/cpp/src/arrow/type.cc#L2423-L2425"
      }
    ]
  },
  {
    "issue_number": 11939,
    "title": "dask compatibility issues(TypeError)",
    "author": "wbs-cz",
    "state": "closed",
    "created_at": "2025-05-09T02:08:51Z",
    "updated_at": "2025-05-09T09:03:20Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\nWhen I was using dask2025.4.1, the following error occurred:\nTypeError: descriptor '__call__' for 'type' objects doesn't apply to a 'property' object\n\n**Minimal Complete Verifiable Example**:\n# test_dask.py\nimport dask.dataframe as dd\nprint(\"Dask import success!\")\nimport pandas as pd\n\ndf = dd.from_pandas(pd.DataFrame({'a': [1,2,3]}), npartitions=1)\nprint(df.compute())\n\n\n**Anything else we need to know?**:\n\nSomeone suggested that I change to the pandas version, but my pandas version is 2.2.3, which meets the requirements of dask. Exception. The version of dask_ml I'm using is 2025.1.0.\n\n**Environment**:Pycharm 2024.3\n\n- Dask version:2025.4.1\n- Python version:3.12.8\n- Operating System:\n- Install method (conda, pip, source):pip\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "I cannot reproduce an issue with the code you provided. Can you please double check that your reproducer is indeed showing this issue and can you please share the entire exception message and traceback?"
      },
      {
        "user": "wbs-cz",
        "body": "@fjetter Thank you for your answer. I think there is a problem with the configuration of my virtual environment. The original Traceback is as follows:\nTraceback (most recent call last):\nFile \"C:\\Users\\hasee\\Desktop\\Digital_twin\\Digital_twin\\company_data\\machine_learing\\test_dask.py\", line 1, in <module>\nmport dask.dataframe as dd\nAfter I uninstalled and reinstalled dask, dask_ml and pandas, the code could run normally."
      }
    ]
  },
  {
    "issue_number": 11928,
    "title": "`FrameBase.to_dask_array` raises error when `lengths=True`",
    "author": "jessegrabowski",
    "state": "closed",
    "created_at": "2025-05-02T09:29:53Z",
    "updated_at": "2025-05-08T14:34:36Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nThe `FrameBase.to_dask_array` raises `TypeError: 'int' object is not iterable` when called with `lengths=True`\n\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport dask.dataframe as dd\n\ndata = dd.from_array(np.random.normal(size=(10_000, 2)))\ndata.to_dask_array(lengths=True)\n```\n\n<details>\n\n<summary>Full Traceback</summary>\n\n```bash\nTypeError                                 Traceback (most recent call last)\nCell In[16], line 6\n      4 data = np.random.normal(size=(10_000, 2))\n      5 data = dd.from_array(data)\n----> 6 data.to_dask_array(lengths=True)\n\nFile ~/mambaforge/envs/pymc-statespace/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:1405, in FrameBase.to_dask_array(self, lengths, meta, optimize, **optimize_kwargs)\n   1379 \"\"\"Convert a dask DataFrame to a dask array.\n   1380\n   1381 Parameters\n   (...)   1402 A Dask Array\n   1403 \"\"\"\n   1404 if lengths is True:\n-> 1405     lengths = tuple(self.map_partitions(len, enforce_metadata=False).compute())\n   1407 arr = self.values\n   1409 chunks = self._validate_chunks(arr, lengths)\n\nTypeError: 'int' object is not iterable\n\n```\n\n</details>\n\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.12.10\n- Operating System: Ubuntu (via WSL)\n- Install method (conda, pip, source): conda\n",
    "comments": [
      {
        "user": "officialankan",
        "body": "hey!  is this solved if I just update or has it not been included in a release yet?"
      },
      {
        "user": "jrbourbeau",
        "body": "Not released yet but the fix is on `main` if you want to use it "
      }
    ]
  },
  {
    "issue_number": 11932,
    "title": "Scheduler hangs with filtered and multi-partitioned parquet",
    "author": "rsundqvist",
    "state": "closed",
    "created_at": "2025-05-06T09:38:42Z",
    "updated_at": "2025-05-08T13:31:50Z",
    "labels": [
      "parquet",
      "regression"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nUnder some circumstances, the scheduler will hang forever when computing partitioned parquet data with more than one partition column.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport os\nimport shutil\n\nimport pandas as pd\nfrom dask import dataframe as dd, __version__ as DASK_VERSION\nfrom distributed import Client, __version__ as DISTRIBUTED_VERSION\n\n\ndef create_date(path: str) -> None:\n    if os.path.isdir(path):\n        shutil.rmtree(path, ignore_errors=True)\n    os.mkdir(path)\n\n    ts = pd.date_range(\"2019-08-01\", \"2019-09-04\", periods=1000, tz=\"utc\").round(\"s\")\n    df = ts.to_frame(index=False, name=\"timestamp\")\n    df[\"date\"] = df[\"timestamp\"].dt.date\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n    df.to_parquet(\n        path,\n        # NOTE: Works with just one partition column.\n        partition_cols=[\"date\", \"hour\"],\n    )\n\n\ndef main():\n    # NOTE: Works on dask/distributed <= 2023.3.0\n    # NOTE: Fails on dask/distributed >= 2023.4.0\n    print(f\"       {DASK_VERSION=}\")\n    print(f\"{DISTRIBUTED_VERSION=}\")\n\n    path = \"./data/\"\n    create_date(path)\n\n    # NOTE: Works without a client.\n    client = Client(n_workers=2, threads_per_worker=1)\n    print(f\"{client=}\")\n    print(f\"{client.dashboard_link=}\")\n\n    # NOTE: Works without any filters.\n    filters = [(\"date\", \"=\", \"2019-09-01\")]\n    ddf = dd.read_parquet(path, filters=filters)\n    # repr(ddf)  # NOTE: Works if calling repr on graph first.\n    print(f\"{len(ddf)=:}\")  # Hangs forever - no activity in dashboard\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Anything else we need to know?**:\n* Works on `2025.3.0` and before (we want at least `2025.4.1` due to #11898)\n* Works if you call `repr()` on the graph first.\n* Works without filters.\n* Works with just one partition column.\n* Works without a client.\n* Works in Pandas.\n\n**Environment**:\n- Dask version: 2025.4.1\n- Python version: 3.11.12\n- Operating System: Ubuntu 24.04.2 LTS\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "Working on a fix in https://github.com/dask/dask/pull/11933"
      }
    ]
  },
  {
    "issue_number": 11913,
    "title": "Slow graph computation time introduced by PR 11736 and magically fixed by PR 11904 locally but not fixed with a distributed Client",
    "author": "ikrommyd",
    "state": "open",
    "created_at": "2025-04-25T18:07:03Z",
    "updated_at": "2025-05-05T12:55:41Z",
    "labels": [
      "needs triage"
    ],
    "body": "I don't have a simple reproducible example since I noticed this in a physics workflow involving dask-awkward and dask-histogram but the first PR after version 2025.3.0 https://github.com/dask/dask/pull/11736 changed how a particular graph is optimized (visually appears for the better) but also made its computation very very slow.\nThe slowdown is magically fixed by https://github.com/dask/dask/pull/11904 and is fine in the `main` branch in general while the \"new\" optimization is kept. I'm attaching the graphs below:\n\n[unoptimized_old.pdf](https://github.com/user-attachments/files/19913926/unoptimized_old.pdf)\n[unoptimized_new.pdf](https://github.com/user-attachments/files/19913929/unoptimized_new.pdf)\n[optimized_old.pdf](https://github.com/user-attachments/files/19913928/optimized_old.pdf)\n[optimized_new.pdf](https://github.com/user-attachments/files/19913927/optimized_new.pdf)\n\nI'm wrapping my compute call like this:\n```py\n        with ProgressBar():\n            (out,) = dask.compute(to_compute, scheduler=\"threads\")\n```\nAnd I also noticed that I see a lot more progress bars when the computation is slow versus 2 which was always the normal one. I don't know if that tells you anything, I'm just putting it out there.\nSlow execution:\n```shell\nINFO     Computing the task graph                                                                                                                                                                                         run_analysis.py:222\n[########################################] | 100% Completed | 106.72 ms\n[########################################] | 100% Completed | 216.65 ms\n[########################################] | 100% Completed | 106.66 ms\n[########################################] | 100% Completed | 106.73 ms\n[########################################] | 100% Completed | 106.94 ms\n[########################################] | 100% Completed | 218.73 ms\n[########################################] | 100% Completed | 106.79 ms\n[########################################] | 100% Completed | 106.90 ms\n[########################################] | 100% Completed | 105.17 ms\n[########################################] | 100% Completed | 105.12 ms\n[########################################] | 100% Completed | 106.73 ms\n[########################################] | 100% Completed | 106.54 ms\n[########################################] | 100% Completed | 106.73 ms\n[########################################] | 100% Completed | 106.67 ms\n[########################################] | 100% Completed | 219.44 ms\n[########################################] | 100% Completed | 106.73 ms\n[########################################] | 100% Completed | 14.28 s\nINFO     Finished the E/Gamma Tag and Probe workflow  \n```\nFast execution:\n```\nINFO     Computing the task graph                                                                                                                                                                                         run_analysis.py:222\n[########################################] | 100% Completed | 269.26 ms\n[########################################] | 100% Completed | 2.74 ss\nINFO     Finished the E/Gamma Tag and Probe workflow   \n```\nThe last progress bar shows the difference in speed ~3 vs ~15 seconds\n",
    "comments": [
      {
        "user": "ikrommyd",
        "body": "An update, the slowdown appears to disappear with the `threads` scheduler but when I use a `distributed` `Client` like `from distributed import Client; client=Client()`, then the slowdown still remains and on the dask dashboard I'm seeing the task names that appear in the old optimized graph and not the new one that `dask.visualize` reports me. It's like it's optimizing differently when `distributed` is in the way."
      },
      {
        "user": "fjetter",
        "body": "A reproducing example would be helpful"
      },
      {
        "user": "ikrommyd",
        "body": "> A reproducing example would be helpful\n\nI don't know if it's possible without using `dask-awkward` and `dask-histogram` which is why I didn't attempt it yet."
      }
    ]
  },
  {
    "issue_number": 11927,
    "title": "CI tests using dask v2025.4.0 (or newer) hang indefinitely",
    "author": "pfackeldey",
    "state": "open",
    "created_at": "2025-04-30T13:42:16Z",
    "updated_at": "2025-05-05T09:21:50Z",
    "labels": [
      "needs triage"
    ],
    "body": " > Our CI tests hang indefinitely over in https://github.com/dask-contrib/dask-awkward and https://github.com/scikit-hep/uproot5 since this PR (found this out using `git bisect`). This issue seems to be only present for running tests with `pytest`, there's no problem (as far as we can tell) with just running dask code otherwise. (If I understand it correctly, it seems to be stuck forever in `queue.get()` for the `get_sync` scheduler, but I'm not very sure about this.)\n\nDo you have an idea what could cause this @fjetter & @hendrikmakait?\nThanks for your help in advance 🙏 \n\ncc @martindurant & @lgray\n\n_Originally posted by @pfackeldey in https://github.com/dask/dask/issues/11859#issuecomment-2828709361_            ",
    "comments": [
      {
        "user": "fjetter",
        "body": "without a reproducer I cannot help, unfortunately."
      },
      {
        "user": "pfackeldey",
        "body": "Hi @fjetter,\nthe following is the most minimal producer I could build (for now):\n\n### Setup:\n```shell\nuv venv\nsource .venv/bin/activate\nuv pip install dask-awkward pytest\nuv pip install --upgrade dask==2025.4.0\n\npytest test.py  # hangs indefinitely \n```\n\n### `test.py`\n\n```python\nimport json\nimport fsspec\nimport pytest\nimport awkward as ak\nimport dask_awkward as dak\nfrom dask_awkward.lib.testutils import assert_eq, awkward_xy_points\n\n\n@pytest.fixture(scope=\"session\")\ndef ndjson_points1(tmp_path_factory: pytest.TempPathFactory) -> str:\n    array = awkward_xy_points()\n    fname = tmp_path_factory.mktemp(\"data\") / \"points_ndjson1.json\"\n    with fsspec.open(fname, \"w\") as f:\n        for entry in array.tolist():\n            print(json.dumps({\"points\": entry}), file=f)\n    return str(fname)\n\n\n@pytest.fixture(scope=\"session\")\ndef daa_p1(ndjson_points1: str) -> dak.Array:\n    return dak.from_json([ndjson_points1] * 3)\n\n\n@pytest.fixture(scope=\"session\")\ndef caa_p1(ndjson_points1: str) -> ak.Array:\n    with open(ndjson_points1) as f:\n        lines = [json.loads(line) for line in f]\n    return ak.Array(lines * 3)\n\n\ndef test(\n    daa_p1: dak.Array,\n    caa_p1: ak.Array,\n) -> None:\n    assert_eq(daa_p1, caa_p1)\n```\n\nIf you install dask v2025.3.0 (`uv pip install --upgrade dask==2025.3.0`), there's no problem.\n\nI suspect that this issue is independent of dask-awkward, but I couldn't write a reproducer yet without it. (I'll post a smaller reproducer once I have one with pure dask and pytest.)"
      },
      {
        "user": "fjetter",
        "body": "I believe the uproot5 problem also goes back to this line. If that issue persists, please point me to a fail case to investigate"
      }
    ]
  },
  {
    "issue_number": 11894,
    "title": "customized nunique aggregation not work in newer dask version",
    "author": "lyzf",
    "state": "open",
    "created_at": "2025-04-18T02:33:07Z",
    "updated_at": "2025-05-05T07:28:45Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:   customized nunique aggregation  not work in  dask 2025.3.0， 2024.8.2.  \nbut work in 2023.11.0.\nnunique = dd.Aggregation(\n    name=\"nunique\",\n    chunk=lambda s: s.apply(lambda x: list(set(x.dropna()))),\n    agg=lambda s0: s0.sum(),\n    finalize=lambda s1: s1.apply(lambda final: len(set(final))),\n)\n\nwhen data is small and there is no shuffle, it seems worked, but get error when data is big and need shuffle.\nI test another version nunique code also get error.\nnunique = dd.Aggregation(\n    name=\"nunique\",\n    chunk=lambda s: s.apply(lambda x: list(set(x))),\n    agg=lambda s0: s0.obj.groupby(level=list(range(s0.obj.index.nlevels))).sum(),\n    finalize=lambda s1: s1.apply(lambda final: len(set(final))),\n)\n\nplease help me out, thanks.\n\n**Minimal Complete Verifiable Example**:\n\n```python\n# Put your MCVE code here\nimport geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import Point\nimport json\nfrom shapely.strtree import STRtree\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nimport dask\nimport logging\n\n\n# Define the custom nunique aggregation\nnunique = dd.Aggregation(\n    name=\"nunique\",\n    chunk=lambda s: s.apply(lambda x: list(set(x.dropna()))),\n    agg=lambda s0: s0.sum(),\n    finalize=lambda s1: s1.apply(lambda final: len(set(final))),\n)\n\ndef main():\n    # Create a sample Dask DataFrame\n    # df = pd.DataFrame({\n    #     'group': ['A', 'A', 'B', 'B', 'B', 'C', 'C'],\n    #     'value': [1, 2, 2, 3, np.nan, 4, 4]\n    # })\n    # ddf = dd.from_pandas(df, npartitions=2)\n\n    n_rows = 100_000\n    df = pd.DataFrame({\n        'group': np.random.choice(['A', 'B', 'C', 'D', 'E'], size=n_rows, p=[0.5, 0.2, 0.1, 0.1, 0.1]),\n        'value': np.random.randint(0, 100, size=n_rows)  \n    })\n    ddf = dd.from_pandas(df, npartitions=16) \n\n    # Test the aggregation\n    result = ddf.groupby('group').agg({'value': nunique}).compute()\n\n    print(\"Original DataFrame:\")\n    print(df)\n    print(\"\\nGrouped nunique counts:\")\n    print(result)\n\n\nif __name__ == \"__main__\": \n    \n    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    dask.config.set({\"dataframe.convert-string\": False})\n    from dask.distributed import LocalCluster\n    \n    \n    \n    cluster = LocalCluster(n_workers = 4, threads_per_worker=1,memory_limit = None)          \n    client = cluster.get_client()\n    print(client)\n\n\n    \n    main()\n```\n\n**Anything else we need to know?**:\n04/18/2025 02:22:05 Shuffle 7b2f73ba56fa061f759b764f7adcbf70 initialized by task ('shuffle-transfer-7b2f73ba56fa061f759b764f7adcbf70', 3) executed on worker tcp://127.0.0.1:34491\n2025-04-18 02:22:06,293 - distributed.worker - ERROR - Compute Failed\nKey:       ('operation-_call_with_list_arg-7326a409d87117c7463d08749d6f3915', 0)\nState:     executing\nTask:  <Task ('operation-_call_with_list_arg-7326a409d87117c7463d08749d6f3915', 0) _execute_subgraph(...)>\nException: \"ValueError('operands could not be broadcast together with shapes (99,) (100,) ')\"\nTraceback: '  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_reductions.py\", line 107, in _call_with_list_arg\\n    return func(list(args), **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_groupby.py\", line 531, in aggregate\\n    return _agg_finalize(_concat(inputs), **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 1046, in _agg_finalize\\n    df = _groupby_apply_funcs(\\n         ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 997, in _groupby_apply_funcs\\n    r = func(grouped, **func_kwargs)\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 1088, in _apply_func_to_columns\\n    return func(*columns)\\n           ^^^^^^^^^^^^^^\\n  File \"/app/mydask10_9addchangsuo/test20250417_v2.py\", line 17, in <lambda>\\n    agg=lambda s0: s0.sum(),\\n                   ^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\", line 3146, in sum\\n    result = self._agg_general(\\n             ^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\", line 1906, in _agg_general\\n    result = self._cython_agg_general(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\", line 1998, in _cython_agg_general\\n    new_mgr = data.grouped_reduce(array_func)\\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/internals/base.py\", line 367, in grouped_reduce\\n    res = func(arr)\\n          ^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\", line 1973, in array_func\\n    result = self._grouper._cython_operation(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/ops.py\", line 831, in _cython_operation\\n    return cy_op.cython_operation(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/ops.py\", line 550, in cython_operation\\n    return self._cython_op_ndim_compat(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/ops.py\", line 329, in _cython_op_ndim_compat\\n    res = self._call_cython_op(\\n          ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/ops.py\", line 418, in _call_cython_op\\n    func(\\n  File \"groupby.pyx\", line 725, in pandas._libs.groupby.group_sum\\n'\n\n04/18/2025 02:22:06 Shuffle 7b2f73ba56fa061f759b764f7adcbf70 deactivated due to stimulus 'task-erred-1744942926.294215'\nTraceback (most recent call last):\n  File \"/app/mydask10_9addchangsuo/test20250417_v2.py\", line 59, in <module>\n    main()\n  File \"/app/mydask10_9addchangsuo/test20250417_v2.py\", line 37, in main\n    result = ddf.groupby('group').agg({'value': nunique}).compute()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py\", line 491, in compute\n    return DaskMethodsMixin.compute(out, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/base.py\", line 370, in compute\n    (result,) = compute(self, traverse=False, **kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/base.py\", line 656, in compute\n    results = schedule(dsk, keys, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_reductions.py\", line 107, in _call_with_list_arg\n    return func(list(args), **kwargs)\n  ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_groupby.py\", line 531, in aggregate\n    return _agg_finalize(_concat(inputs), **kwargs)\n  ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 1046, in _agg_finalize\n    df = _groupby_apply_funcs(\n      ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 997, in _groupby_apply_funcs\n    r = func(grouped, **func_kwargs)\n  ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 1088, in _apply_func_to_columns\n    return func(*columns)\n      ^^^^^^^^^^^^^^^^^\n  File \"/app/mydask10_9addchangsuo/test20250417_v2.py\", line 17, in <lambda>\n    agg=lambda s0: s0.sum(),\n      ^^^^^^^^^^^^^^^^^\n  File \"groupby.pyx\", line 725, in pandas._libs.groupby.group_sum\nValueError: operands could not be broadcast together with shapes (99,) (100,)\n\n/////////////////////////////////////////////\n04/18/2025 02:28:16 Shuffle e88bbe7f0037aa10efec52469d3244bf deactivated due to stimulus 'task-finished-1744943296.1277819'\n2025-04-18 02:28:16,134 - distributed.worker - ERROR - Compute Failed\nKey:       ('operation-_call_with_list_arg-a4a282a40642ad63ac547ccd9ccb771b', 0)\nState:     executing\nTask:  <Task ('operation-_call_with_list_arg-a4a282a40642ad63ac547ccd9ccb771b', 0) _execute_subgraph(...)>\nException: \"ValueError('operands could not be broadcast together with shapes (100,) (99,) ')\"\nTraceback: '  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_reductions.py\", line 107, in _call_with_list_arg\\n    return func(list(args), **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_groupby.py\", line 531, in aggregate\\n    return _agg_finalize(_concat(inputs), **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 1046, in _agg_finalize\\n    df = _groupby_apply_funcs(\\n         ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 997, in _groupby_apply_funcs\\n    r = func(grouped, **func_kwargs)\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 1088, in _apply_func_to_columns\\n    return func(*columns)\\n           ^^^^^^^^^^^^^^\\n  File \"/app/mydask10_9addchangsuo/test20250417_v2.py\", line 24, in <lambda>\\n    agg=lambda s0: s0.obj.groupby(level=list(range(s0.obj.index.nlevels))).sum(),\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\", line 3146, in sum\\n    result = self._agg_general(\\n             ^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\", line 1906, in _agg_general\\n    result = self._cython_agg_general(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\", line 1998, in _cython_agg_general\\n    new_mgr = data.grouped_reduce(array_func)\\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/internals/base.py\", line 367, in grouped_reduce\\n    res = func(arr)\\n          ^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\", line 1973, in array_func\\n    result = self._grouper._cython_operation(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/ops.py\", line 831, in _cython_operation\\n    return cy_op.cython_operation(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/ops.py\", line 550, in cython_operation\\n    return self._cython_op_ndim_compat(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/ops.py\", line 329, in _cython_op_ndim_compat\\n    res = self._call_cython_op(\\n          ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/site-packages/pandas/core/groupby/ops.py\", line 418, in _call_cython_op\\n    func(\\n  File \"groupby.pyx\", line 725, in pandas._libs.groupby.group_sum\\n'\n\nTraceback (most recent call last):\n  File \"/app/mydask10_9addchangsuo/test20250417_v2.py\", line 66, in <module>\n    main()\n  File \"/app/mydask10_9addchangsuo/test20250417_v2.py\", line 44, in main\n    result = ddf.groupby('group').agg({'value': nunique}).compute()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py\", line 491, in compute\n    return DaskMethodsMixin.compute(out, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/base.py\", line 370, in compute\n    (result,) = compute(self, traverse=False, **kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/base.py\", line 656, in compute\n    results = schedule(dsk, keys, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_reductions.py\", line 107, in _call_with_list_arg\n    return func(list(args), **kwargs)\n  ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_groupby.py\", line 531, in aggregate\n    return _agg_finalize(_concat(inputs), **kwargs)\n  ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 1046, in _agg_finalize\n    df = _groupby_apply_funcs(\n      ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 997, in _groupby_apply_funcs\n    r = func(grouped, **func_kwargs)\n  ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/dask/dataframe/groupby.py\", line 1088, in _apply_func_to_columns\n    return func(*columns)\n      ^^^^^^^^^^^^^^^^^\n  File \"/app/mydask10_9addchangsuo/test20250417_v2.py\", line 24, in <lambda>\n    agg=lambda s0: s0.obj.groupby(level=list(range(s0.obj.index.nlevels))).sum(),\n      ^^^^^^^^^^^^^^^^^\n  File \"groupby.pyx\", line 725, in pandas._libs.groupby.group_sum\nValueError: operands could not be broadcast together with shapes (100,) (99,)\n\n\n\n**Environment**:\n\n- Dask version:\n- Python version:\n- Operating System:\n- Install method (conda, pip, source):\n",
    "comments": [
      {
        "user": "guillaumeeb",
        "body": "Not really useful for now, but I can confirm that the reproducer works without using a LocalCluster, and fails when declaring one.\n\nHowever @lyzf, is there a reason why you are not using the built-in nunique aggregation proposed in Dask?\nIT might just also be a bug in your custom aggregation.\n\nhttps://docs.dask.org/en/latest/generated/dask.dataframe.api.SeriesGroupBy.nunique.html.\n\n```python\nresult = ddf.groupby('group').value.nunique().compute()\n```"
      },
      {
        "user": "lyzf",
        "body": "Thanks for your help. \nBecause  I usually need to groupby on many columns,  and agg on many columns, like code below.\n```\nresult = validddf4g.groupby(zhengtigrid_groupcolumns, sort=False, dropna=False).agg(\n{\n        'SS_dgtd':{\n                'avg_dgtd':'mean',\n                'sum_dgtd':'sum',\n                },\n        'SID':{\n                'user_num': nunique,\n                },\n        'TS':{\n            'item_num' : 'count',\n        },\n).compute()\n```\n\nI also test built-in series nunique aggregation, but also find an unacceptable bug. \nalthough I specified dropna = False, records be dropped if group keys contain NA values.\n```\nresult = validddf4g.groupby(zhengtigrid_groupcolumns, sort=False, dropna=False).SID.nunique().compute()\n\n```"
      },
      {
        "user": "qltwis",
        "body": "Try this:\n\n```python\nnunique = dd.groupby.Aggregation(\n    name=\"nunique\",\n    chunk=lambda s: s.apply(np.array),\n    agg=lambda s: s.apply(np.array).apply(np.concatenate),\n    finalize=lambda s: s.apply(np.unique).apply(len),\n)\n```"
      }
    ]
  },
  {
    "issue_number": 11816,
    "title": "Add support of delayed objects in map_partitions",
    "author": "faulaire",
    "state": "closed",
    "created_at": "2025-03-06T14:23:14Z",
    "updated_at": "2025-05-05T06:37:01Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "Hello,\n\nLegacy implementation of map_partitions supported Delayed and Scalar objects as arguments.\n\n\"\"\nargs, kwargs :\n        Arguments and keywords to pass to the function.  At least one of the\n        args should be a Dask.dataframe. Arguments and keywords may contain\n        **``Scalar``, ``Delayed``** or regular python objects. DataFrame-like args\n        (both dask and pandas) will be repartitioned to align (if necessary)\n        before applying the function (see ``align_dataframes`` to control).\n\"\"\"\n\nThis is not the case anymore with daskexpr. Is there any particular reason ?\n\nThis code was fine with legacy implementation:\n\n```\nimport dask\nfrom dask.dataframe import from_pandas\nimport pandas as pd\n\n@dask.delayed\ndef delayed_input():\n    return \"Hello World !\"\n\ndef func(df, delayed_obj):\n    assert delayed_obj == \"Hello World !\"\n    return df\n\ndf = pd.Series([1., 2., 3.])\ndd = from_pandas(df, npartitions=2)\n\ndd2 = dd.map_partitions(func, delayed_input(), meta=df.head(0))\ndf2 = dd2.compute()\n```\n\nWould it be possible to support it in daskexpr ?\n\nThanks\n\n",
    "comments": [
      {
        "user": "guillaumeeb",
        "body": "As also answered [on Discourse](https://dask.discourse.group/t/delayed-argument-in-map-partition/3847/4), I am able to reproduce this issue.\n\nMoreover, trying to modify the function as below also raise an exception. \n```\ndef func(df, delayed_obj):\n    delayed_obj.compute()\n    return df\n```\n\nIt looks there is something wrong, and probably not just in the documentation."
      },
      {
        "user": "faulaire",
        "body": "Fixed by https://github.com/dask/dask/pull/11907"
      }
    ]
  },
  {
    "issue_number": 11853,
    "title": "Xarray resampling seems worse now",
    "author": "dcherian",
    "state": "open",
    "created_at": "2025-03-29T03:24:32Z",
    "updated_at": "2025-05-05T02:11:00Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nLarge-ish resampling problems appear to be broken on latest dask. \n\nwith `use_flox=True` the graph builds quickly but is slow to start computing. There seems to be a lot of overhead per array. With 1 data variable, I get a warning about a 60MB grapḣ. This scales with number of data vairables `n_data_vars` . The real dataset has 59 and doesn't ever compute.\n\n- ~I think I understand this one. I'm embedding many copies of the same data there similar to what was fixed in https://github.com/pydata/xarray/pull/9658~\n\nwith `use_flox=False`, the graph is incredibly slow to build. i didn't wait for it to construct, so don't know if that would have computed.\n- this seems to be an issue with slicing based on a py-spy profile\n\n**Minimal Complete Verifiable Example**:\n\n```python\n# Put your MCVE code here\n%load_ext watermark\nimport flox\nimport dask\nimport xarray as xr\nimport distributed\n%watermark -iv\n\n\nimport dask.array\nimport xarray as xr\n\nn_data_vars = 1 # really 59\ndims = (\"mid_date\", \"y\", \"x\")\nshape = (47892, 833, 833)\n\nds = xr.Dataset()\nds.coords[\"mid_date\"] = xr.date_range(\"1985-09-11\", \"2024-10-29\", periods=shape[0])\n\nfor i in range(n_data_vars):\n    ds[f\"var_{i}\"] = (dims, dask.array.full(shape, fill_value=i, chunks=(20000, 10, 10)))\nds\n\n\nwith xr.set_options(use_flox=False):  # toggle True & False\n    resampled = ds.resample(mid_date=\"3ME\").mean()\n\nresampled.compute()\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Xarray 2025.3.0\n- Dask version: 2025.3.0\n- Python version: 3.13\n- Operating System:\n- Install method (conda, pip, source):\n",
    "comments": [
      {
        "user": "e-marshall",
        "body": "I tried this also downgrading to xr 2024.10.0, dask 2025.1.0, python 3.11.3 and ran into the same issue, which makes me wonder if its being caused by something else? I'll troubleshoot more tomorrow and update"
      },
      {
        "user": "dcherian",
        "body": "OK my initial observations were because of `dask.array.random.random` which results in a huge (size-wise) graph after pickling.\n\nNow i have rewritten to use `dask.array.full` (updated the example), this is better but still creating huge (size-wise) graphs for `use_flox=True`: \n\nQ: Why the factor of 6 here?\n<img width=\"468\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/28931a37-3804-49eb-8379-16c7b5e240ed\" />\n\n\nI reduced `n_data_vars` to 3, and it does eventually start computing, which is quite smooth and nice. \n\nSome perhaps actionable questions:\n1. How do we make this scale for large number of data variables? \n2. The slicing code seems to have regressed, in that it is now much slower to generate a graph"
      },
      {
        "user": "fjetter",
        "body": "This has been introduced by https://github.com/dask/dask/pull/11736 This change was supposed to improve client to scheduler submission time but I noticed that for some workloads it had the opposite effects, particularly sliced `random` arrays since those embed huge matrices to initialize the random state"
      }
    ]
  },
  {
    "issue_number": 11864,
    "title": "Writing Zarr array fails when turning off compression",
    "author": "marberi",
    "state": "open",
    "created_at": "2025-04-02T15:51:24Z",
    "updated_at": "2025-05-05T02:10:59Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "For bench marking a storage system, I wanted to turn off the compression\nwhen writing a Zarr file. Generating random number seemed to have a\nsignificant overhead (when generating writing 40TB/s) and storing a constant\nresulted in the compression kicking in. When attempting to turn off the\ncompression it fails for not having written the JSON:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/data/incaem/scratch_nvme/eriksen/benchmark/myarray_v2.zarr/zarr.json'\n\nwhich seems to be a Dask error. You can reproduce the error with:\n\n```\nimport dask.array as da\n\nA = da.random.random((3_000,3_000), chunks=(100,100))\n\n# Write an array.\npath = '/data/incaem/scratch_nvme/eriksen/benchmark/myarray_v2.zarr'\nA.to_zarr(path, storage_options={\"compressor\": None})\n```\n\nThe problem happens each time the code being run and for different\narray sizes. Creating the directory first does not solve the issue.\n\n**Environment**:\n\n- Dask version: 2025.3.0\n- Python version: 3.13.2\n- Operating System: AlmaLinux release 9.3 (Shamrock Pampas Cat)\n- Install method (conda, pip, source): Conda\n",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "I think the issue is that `auto_mkdir` isn't being set on the fsspec LocalFileSystem, and so an intermediate directory isn't being created.\n\nThis works with\n\n```\nIn [3]: import dask.array as da\n   ...:\n   ...: A = da.random.random((3_000,3_000), chunks=(100,100))\n   ...:\n   ...: # Write an array.\n   ...: path = '/tmp/yarray_v2.zarr'\n   ...: A.to_zarr(path, storage_options={\"compressor\": None, \"auto_mkdir\": True})\n```\n\nI haven't debugged where that's not getting set. It could be in dask, zarr, or fsspec. It'd be nice if you could debug this to see where it's being dropped."
      },
      {
        "user": "guillaumeeb",
        "body": "Hi @marberi, are you sure you are using Zarr v2?"
      }
    ]
  },
  {
    "issue_number": 11865,
    "title": "Unable to add DaskLightGBMRanker predictions as column to existing dask dataframe",
    "author": "dwaipayanseal",
    "state": "open",
    "created_at": "2025-04-02T18:29:08Z",
    "updated_at": "2025-05-05T02:10:58Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "**Describe the issue**:\nAfter getting the predictions from DaskLGBMRanker model (as a dask.array.core.Array), I'm trying to add it as a column to the X_ddf dask dataframe (type = dask.dataframe.dask_expr._collection.DataFrame). When I try to do head() operation on X_ddf dataframe, I get a P2P error like below -\n`RuntimeError: P2P b4f701e48bbcadc4d506a234fc3d903d failed during unpack phase`\n\nApologies if I've not followed mvce as I'm posting a bug issue on github for the first time.\n\n```python\nfrom dask.distributed import Client, LocalCluster\n\n[dask.p2p.error.20250402.txt](https://github.com/user-attachments/files/19573618/dask.p2p.error.20250402.txt)\n\nimport lightgbm.dask as dlgbm\n\nn_customers = 100\nn_ranks_per_customer = 12\nn_features = 20\nn_partitions = 10\n\ntotal_rows = n_customers * n_ranks_per_customer\nrows_per_partition = total_rows // n_partitions \nfeature_cols = [f'feature_{i}' for i in range(n_features)]\nall_cols = ['customer_id', 'rank'] + feature_cols\n\n'''\nincluding the ddf creation part also, just to be sure if I'm getting the dummy data correctly\n'''\nmeta_dtypes = {\n    'customer_id': 'int64',\n    'rank': 'int32',\n}\nfor col in feature_cols:\n    meta_dtypes[col] = 'float64'\n\n@dask.delayed\ndef create_partition(partition_index, total_partitions, n_cust, n_rnk, n_feat, column_names, dtype_map):\n    rows_per_part = (n_cust * n_rnk) // total_partitions\n    customers_per_part = rows_per_part // n_rnk \n    start_customer_id = partition_index * customers_per_part\n    end_customer_id = start_customer_id + customers_per_part\n    customer_ids = np.repeat(np.arange(start_customer_id, end_customer_id), n_rnk)\n    ranks = np.tile(np.arange(1, n_rnk + 1), customers_per_part)\n    feature_data = np.random.rand(rows_per_part, n_feat)\n    data_dict = {\n        'customer_id': customer_ids,\n        'rank': ranks\n    }\n    feature_column_names = [f for f in column_names if f.startswith('feature_')]\n    for i, col_name in enumerate(feature_column_names):\n         data_dict[col_name] = feature_data[:, i]\n    df_part = pd.DataFrame(data_dict)\n    df_part = df_part.astype(dtype_map)[column_names]\n    return df_part\n\ndelayed_partitions = [\n    create_partition(i, n_partitions, n_customers, n_ranks_per_customer, n_features, all_cols, meta_dtypes)\n    for i in range(n_partitions)\n]\nddf = dd.from_delayed(delayed_partitions, meta=meta_dtypes, verify_meta=True)\n\nmax_rank = n_ranks_per_customer # For relevance calculation\ndef add_relevance(df, max_rank_val):\n    df['relevance'] = max_rank_val - df['rank'] + 1\n    return df.astype({'relevance': 'int32'}) # Ensure consistent dtype\n\nmeta_with_relevance = meta_dtypes.copy()\nmeta_with_relevance['relevance'] = 'int32'\nddf = ddf.map_partitions(add_relevance, max_rank, meta=meta_with_relevance)\n\nX = ddf[feature_cols]\ny = ddf['relevance']\n\ncluster = LocalCluster(n_workers=4, threads_per_worker=2, memory_limit='2GB')\nclient = Client(cluster)\n\ndranker = dlgbm.DaskLGBMRanker(\n    objective='lambdarank', \n    metric='ndcg',          \n    n_estimators=50,       \n    learning_rate=0.05,\n    num_leaves=31,\n    random_state=42,\n    verbosity = 1,\n    n_jobs=-1               \n)\ndranker.fit(X, y, group=group_dask)\nX_reset = X.reset_index(drop=True)\npredictions_reset = dranker.predict(X_reset)\npredictions_series_reset = dd.from_dask_array(predictions_reset, index=X_reset.index)\nX_with_predictions_reset = X_reset.assign(prediction_score=predictions_series_reset)\nX_with_predictions_reset.head()\n\n```\n\n**Anything else we need to know?**:\ntried with 2025.2.0 ealier. thought it could be a version issue, so upgraded it to 2025.3.0\n\n**Environment**:\n\n- Dask version: 2025.3.0 \n- Python version: Python 3.11.11\n- Operating System: Linux default 5.10.234-225.910.amzn2.x86_64\n- Install method (conda, pip, source): python -m pip install --upgrade \"dask[complete]\"\n",
    "comments": []
  },
  {
    "issue_number": 7652,
    "title": "Sparsely blocked/chunked arrays",
    "author": "system123",
    "state": "open",
    "created_at": "2021-05-13T07:19:15Z",
    "updated_at": "2025-05-04T23:23:12Z",
    "labels": [
      "array",
      "highlevelgraph",
      "needs attention"
    ],
    "body": "I have an application whereby I am manually forming a dask array using `blocks` as each of the chunks in the final array need to go through a specific transform to ensure the correct alignment in the final dask array. Furthermore, not all the chunks in the array which is formed contain useful information, but some are just needed to ensure spatial alignment between their neighboring chunks, thus forming a Dask array with sparse blocks/chunks.\r\n\r\nCurrently I am using masked arrays to mask out entire chunks, however, this is inefficient as those blocks are still included in the computational graph and are still evaluated to some degree. Ultimately it would be nice to have the concept of chunk/block masks which let dask know that the evaluation of any operation on an entire chunk will result in a NaN or empty result and can thus be skipped entirely without additional computation. \r\n\r\nThe application generally has more `empty` blocks/chunks that those with valid data and thus the computational graph grows very large with a lot of unneeded operations, having a way to prevent this would be nice.\r\n\r\nA small example of how I currently handle this:\r\n```\r\nimport numpy as np\r\nimport dask.array as da\r\n\r\narr0 = da.from_array(np.arange(1, 26).reshape(5,5), chunks=(5, 5))\r\narr1 = da.from_array(np.arange(25, 50).reshape(5,5), chunks=(5, 5))\r\n# This chunk is empty/blank/invalid but is needed to align the other chunks\r\narr2 = da.from_array(np.full((5,5), np.nan), chunks=(5, 5)) \r\narr3 = da.from_array(np.arange(75, 100).reshape(5,5), chunks=(5, 5))\r\n\r\na = da.block([[arr0, arr1],[arr2, arr3]])\r\nb = da.ma.masked_invalid(a)\r\nc = b.min().compute()\r\n```\r\n\r\nWhat would be nice to have:\r\n```\r\nimport numpy as np\r\nimport dask.array as da\r\n\r\narr0 = da.from_array(np.arange(1, 26).reshape(5,5), chunks=(5, 5))\r\narr1 = da.from_array(np.arange(25, 50).reshape(5,5), chunks=(5, 5))\r\narr3 = da.from_array(np.arange(75, 100).reshape(5,5), chunks=(5, 5))\r\n\r\na = da.sparse_block([[arr0, arr1],[None, arr3]])\r\nb = a.min().compute()\r\n```",
    "comments": [
      {
        "user": "GenevieveBuckley",
        "body": "Perhaps what's needed here is better culling of tasks for masked arrays?\r\n"
      },
      {
        "user": "system123",
        "body": "That would probably be the easier approach, just check if the entire chunk is masked and if so then cull it."
      },
      {
        "user": "jsignell",
        "body": "This seems vaguely similar to your work in #7655 right @GenevieveBuckley?"
      }
    ]
  },
  {
    "issue_number": 11930,
    "title": "dask.array `_array_expr_enabled` issues with LSP's",
    "author": "leaver2000",
    "state": "open",
    "created_at": "2025-05-03T20:22:29Z",
    "updated_at": "2025-05-03T20:22:41Z",
    "labels": [
      "needs triage"
    ],
    "body": "The import structure in the `dask.array.__init__.py` (because try: except blocks) causes issues with Pylance.\n\n![Image](https://github.com/user-attachments/assets/918016c5-71d5-4d6f-830d-6c63fb4dfb71)\n\nA simple solution can be acomplished in the following form, using `typing.TYPE_CHECKING`\n\n![Image](https://github.com/user-attachments/assets/965a24b9-c299-4318-a5ea-82ca6475cd0b)\n\nWhich resolves the issue and at runtime `not TYPE_CHECKING` will always be true.\n\nWhich results in expected IDE behavior\n\n![Image](https://github.com/user-attachments/assets/f2e49a00-37e6-4cb2-9aaa-e517c38f5469)",
    "comments": []
  },
  {
    "issue_number": 11901,
    "title": "Better handling or erroring when computing non-dask collections (xarray computes duplicated)",
    "author": "djhoese",
    "state": "closed",
    "created_at": "2025-04-23T17:39:09Z",
    "updated_at": "2025-05-01T17:42:07Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nI'm not sure if this is a bug, by design, or needs better warnings or documentation. The main question is, what should `dask.compute` do when given a non-dask collection. Specifically I keep stepping on my own toes by passing `xarray` objects (ex. `DataArray`).\n\nAccording to https://docs.dask.org/en/latest/api.html#dask.compute, the inputs can be:\n\n> Any number of objects. If it is a dask object, it’s computed and the result is returned. By default, python builtin collections are also traversed to look for dask objects (for more information see the traverse keyword). Non-dask arguments are passed through unchanged.\n\nSo if a `xr.DataArray` is considered a \"dask\" object then it should be optimized, computed, and returned as a DataArray (this is mostly true now, see below). If it is a non-dask object then it should be passed unchanged and uncomputed. If this confuses `dask.compute` then I would like a warning or an error.\n\nMy test below runs two variations of the same operation. Two objects are computed that share some dask tasks. If they are just dask arrays then everything is optimized as expected and the shared task is only executed once for each chunk. If DataArrays are passed then the shared task(s) are run once for each chunk of *each* DataArray.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask\nimport dask.array as da\nimport numpy as np\nimport pytest\nimport xarray as xr\n\n\n@pytest.mark.parametrize(\"wrap_xarray\", [False, True])\ndef test_shared_tasks(wrap_xarray):\n    total_calls = 0\n\n    def my_func(a: np.ndarray) -> np.ndarray:\n        nonlocal total_calls\n        total_calls += 1\n        return a\n    \n    in1 = da.zeros((5, 5), chunks=2)\n    res = da.map_blocks(my_func, in1,\n                        meta=np.array((), dtype=in1.dtype),\n                        dtype=in1.dtype)\n    out1 = res + 1\n    out2 = res + 2\n    if wrap_xarray:\n        out1 = xr.DataArray(out1)\n        out2 = xr.DataArray(out2)\n        \n    comp_res = dask.compute(out1, out2)\n    \n    if wrap_xarray:\n        assert isinstance(comp_res[0], xr.DataArray)\n        assert isinstance(comp_res[0].data, np.ndarray)\n    else:\n        assert isinstance(comp_res[0], np.ndarray)\n    assert total_calls == in1.blocks.size  # fails for xarray with 2x calls\n\n\nif __name__ == \"__main__\":\n    import pytest\n    pytest.main()\n```\n\n**Anything else we need to know?**:\n\nIn past versions of dask this was inconsistent behavior (like over the last year or 2) and I just never had time to track it down because it was so seldomly encountered. The recent changes to dask in the 2025.4.0 release made this consistently fail.\n\n**Environment**:\n\n- Dask version: 2025.4.0\n- Python version: 3.12\n- Operating System: Ubuntu/PopOS\n- Install method (conda, pip, source): conda with conda-forge\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "Yeah, this is a bug. I'll look into it.\n\nI agree that this has been a bit inconsistent in the past. My recent work in this area is trying to make this more predictable"
      },
      {
        "user": "djhoese",
        "body": "> My recent work in this area is trying to make this more predictable\n\nAnd I can't express how happy I am that someone besides me, who focuses on dask, is tackling these issues. I only ever had time to workaround and describe the issues, not suggest large sweeping refactors."
      },
      {
        "user": "djhoese",
        "body": "@fjetter I've run into another case like this issue where things are computed when they shouldn't be. I haven't gotten it down to a minimal example yet, but it does involve a custom graph. If I'm creating a custom graph involve arrays, should I be using some of these newer expression (or other) classes instead of tuples?\n\nEdit: I misread my own tests. It isn't the custom graph path that is failing, but a map_blocks + delayed function operating on arrays. I'll update or file a new issue if I figure out what's going on."
      }
    ]
  },
  {
    "issue_number": 11910,
    "title": "`TypeError` in `dask.delayed(func)(obj=obj)` with `Future` from dask failing to substitute concrete value for future",
    "author": "TomAugspurger",
    "state": "closed",
    "created_at": "2025-04-25T16:23:14Z",
    "updated_at": "2025-04-30T09:27:19Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nOriginally reported by @Thomas-Z in https://github.com/dask/dask/issues/11908#issuecomment-2828102248, dask fails to substitute the concrete value for a `Future` when executing a task graph that involves passing a `Future` as a keyword to a delayed function:\n\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask\nimport distributed\n\nclient = distributed.Client()\n\n\ndef my_task_z(obj: int):\n    return obj + 1\n\n\nobj = client.scatter(3)\n\nr = dask.delayed(my_task_z)(obj=obj)\nr.compute()\n```\n\nFails with\n\n```pytb\n2025-04-24 17:34:35,983 - distributed.worker - ERROR - Compute Failed\nKey:       my_task_z-b497bc40-886b-4caa-884d-4e4e76c0ade2\nState:     executing\nTask:  <Task 'my_task_z-b497bc40-886b-4caa-884d-4e4e76c0ade2' _apply2(..., ...)>\nException: 'TypeError(\"unsupported operand type(s) for +: \\'Future\\' and \\'int\\'\")'\nTraceback: '  File \"/.../conda/envs/ong313_local/lib/python3.13/site-packages/dask/delayed.py\", line 651, in _apply2\\n    return func(*args, **dask_kwargs)\\n  File \"/tmp/ipykernel_3344879/1975028682.py\", line 9, in my_task_z\\n'\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 15\n     12 obj = client.scatter(3)\n     14 r = dask.delayed(my_task_z)(obj=obj)\n---> 15 r.compute()\n\nFile /.../conda/envs/ong313_local/lib/python3.13/site-packages/dask/base.py:373, in DaskMethodsMixin.compute(self, **kwargs)\n    349 def compute(self, **kwargs):\n    350     \"\"\"Compute this dask collection\n    351 \n    352     This turns a lazy Dask collection into its in-memory equivalent.\n   (...)    371     dask.compute\n    372     \"\"\"\n--> 373     (result,) = compute(self, traverse=False, **kwargs)\n    374     return result\n\nFile /.../conda/envs/ong313_local/lib/python3.13/site-packages/dask/base.py:681, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    678     expr = expr.optimize()\n    679     keys = list(flatten(expr.__dask_keys__()))\n--> 681     results = schedule(expr, keys, **kwargs)\n    683 return repack(results)\n\nFile /.../conda/envs/ong313_local/lib/python3.13/site-packages/dask/delayed.py:651, in _apply2()\n    650 def _apply2(func, *args, dask_kwargs):\n--> 651     return func(*args, **dask_kwargs)\n\nCell In[1], line 9, in my_task_z()\n      8 def my_task_z(obj: int):\n----> 9     return obj + 1\n\nTypeError: unsupported operand type(s) for +: 'Future' and 'int'\n```\n\n**Anything else we need to know?**:\n\nThis only happens if the Future is passed as a keyword.\n\nI have an unsatisfactory fix locally that I'll propose as a PR.\n\n**Environment**:\n\n- Dask version:\n- Python version:\n- Operating System:\n- Install method (conda, pip, source):\n",
    "comments": []
  },
  {
    "issue_number": 11908,
    "title": "`delayed(Future)` fails with dask==2025.4.0",
    "author": "TomAugspurger",
    "state": "closed",
    "created_at": "2025-04-24T15:31:41Z",
    "updated_at": "2025-04-30T09:27:19Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nPreviously, you could wrap a `Future` in `dask.delayed`. Computing the result of the delayed would get the actual result of the future.\n\n\n**Minimal Complete Verifiable Example**:\n\n```python\n❯ uv run --with ipython --with dask==2025.2.0 --with distributed==2025.2.0 ipython\nIn [1]: import dask; from distributed import Client\n\nIn [2]: with Client(n_workers=1) as client:\n   ...:     fut = client.scatter(1)\n   ...:     d = dask.delayed(fut)\n   ...:     print(d.compute())\n   ...:\n1\n```\n\nWith 2025.4.0, that raises an error:\n\n```python\n❯ uv run --with ipython --with dask==2025.4.0 --with distributed==2025.4.0 ipython\nIn [1]: import dask; from distributed import Client\n\nIn [2]: with Client(n_workers=1) as client:\n   ...:     fut = client.scatter(1)\n   ...:     d = dask.delayed(fut)\n   ...:     print(d.compute())\n   ...:\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[2], line 4\n      2 fut = client.scatter(1)\n      3 d = dask.delayed(fut)\n----> 4 print(d.compute())\n\nFile ~/.cache/uv/archive-v0/IYrH2n33IZyyTrTFTRIKM/lib/python3.13/site-packages/distributed/client.py:592, in Future.__str__(self)\n    591 def __str__(self):\n--> 592     return repr(self)\n\nFile ~/.cache/uv/archive-v0/IYrH2n33IZyyTrTFTRIKM/lib/python3.13/site-packages/distributed/client.py:595, in Future.__repr__(self)\n    594 def __repr__(self):\n--> 595     if self.type:\n    596         return (\n    597             f\"<Future: {self.status}, type: {typename(self.type)}, key: {self.key}>\"\n    598         )\n    599     else:\n\nFile ~/.cache/uv/archive-v0/IYrH2n33IZyyTrTFTRIKM/lib/python3.13/site-packages/distributed/client.py:557, in Future.type(self)\n    554 @property\n    555 def type(self):\n    556     \"\"\"Returns the type\"\"\"\n--> 557     return self._state.type\n\nAttributeError: 'NoneType' object has no attribute 'type'\n```\n\n**Anything else we need to know?**:\n\nSimilar in spirit to https://github.com/dask/dask/pull/11907.\n\nI'm not entirely sure that this is a good idea, but we had some APIs relying on it (https://github.com/rapidsai/cuml/pull/6576).\n\n**Environment**:\n\n- Dask version:\n- Python version:\n- Operating System:\n- Install method (conda, pip, source):\n",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "Potential workaround for anyone running into this: setting `result = dask.delayed(fut, name=fut.key)` appears to fix this. I'll see if we can use that in `delayed` automatically."
      },
      {
        "user": "Thomas-Z",
        "body": "Probably related.\nThe following code does not work anymore:\n```python\nimport dask\nimport distributed\n\nclient = distributed.Client()\n\n\ndef my_task_z(obj: int):\n    return obj + 1\n\n\nobj = client.scatter(3)\n\nr = dask.delayed(my_task_z)(obj=obj)\nr.compute()\n```\n\nIt generates this error:\n```python\n2025-04-24 17:34:35,983 - distributed.worker - ERROR - Compute Failed\nKey:       my_task_z-b497bc40-886b-4caa-884d-4e4e76c0ade2\nState:     executing\nTask:  <Task 'my_task_z-b497bc40-886b-4caa-884d-4e4e76c0ade2' _apply2(..., ...)>\nException: 'TypeError(\"unsupported operand type(s) for +: \\'Future\\' and \\'int\\'\")'\nTraceback: '  File \"/.../conda/envs/ong313_local/lib/python3.13/site-packages/dask/delayed.py\", line 651, in _apply2\\n    return func(*args, **dask_kwargs)\\n  File \"/tmp/ipykernel_3344879/1975028682.py\", line 9, in my_task_z\\n'\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 15\n     12 obj = client.scatter(3)\n     14 r = dask.delayed(my_task_z)(obj=obj)\n---> 15 r.compute()\n\nFile /.../conda/envs/ong313_local/lib/python3.13/site-packages/dask/base.py:373, in DaskMethodsMixin.compute(self, **kwargs)\n    349 def compute(self, **kwargs):\n    350     \"\"\"Compute this dask collection\n    351 \n    352     This turns a lazy Dask collection into its in-memory equivalent.\n   (...)    371     dask.compute\n    372     \"\"\"\n--> 373     (result,) = compute(self, traverse=False, **kwargs)\n    374     return result\n\nFile /.../conda/envs/ong313_local/lib/python3.13/site-packages/dask/base.py:681, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    678     expr = expr.optimize()\n    679     keys = list(flatten(expr.__dask_keys__()))\n--> 681     results = schedule(expr, keys, **kwargs)\n    683 return repack(results)\n\nFile /.../conda/envs/ong313_local/lib/python3.13/site-packages/dask/delayed.py:651, in _apply2()\n    650 def _apply2(func, *args, dask_kwargs):\n--> 651     return func(*args, **dask_kwargs)\n\nCell In[1], line 9, in my_task_z()\n      8 def my_task_z(obj: int):\n----> 9     return obj + 1\n\nTypeError: unsupported operand type(s) for +: 'Future' and 'int'\n```"
      },
      {
        "user": "TomAugspurger",
        "body": "Mmm unfortunately my fix of setting the `name` of the delayed object to the `future.key` doesn't fix example.\n\nI'll take a closer look this afternoon and will open a separate issue if it requires a bigger fix."
      }
    ]
  },
  {
    "issue_number": 11924,
    "title": "Remove pip pinning for readthedocs build",
    "author": "fjetter",
    "state": "open",
    "created_at": "2025-04-29T13:42:25Z",
    "updated_at": "2025-04-29T13:42:37Z",
    "labels": [
      "needs triage"
    ],
    "body": "See https://github.com/dask/dask/pull/11923 and https://github.com/dask/dask/issues/11920",
    "comments": []
  },
  {
    "issue_number": 11920,
    "title": "Docs build failing",
    "author": "jrbourbeau",
    "state": "closed",
    "created_at": "2025-04-28T14:45:46Z",
    "updated_at": "2025-04-29T13:42:28Z",
    "labels": [
      "documentation",
      "tests"
    ],
    "body": "Here's an example build that's failing https://app.readthedocs.org/projects/dask/builds/27995978/ \n\nThis is the traceback we've got (there's an issue when installing `numpydoc`):\n\n```\n\nINFO: pip is still looking at multiple versions of numpydoc to determine which version is compatible with other requirements. This could take a while.\n--\n242 | Downloading numpydoc-0.7.0.tar.gz (19 kB)\n243 | Preparing metadata (setup.py): started\n244 | Preparing metadata (setup.py): finished with status 'done'\n245 | Downloading numpydoc-0.6.0.zip (39 kB)\n246 | Preparing metadata (setup.py): started\n247 | Preparing metadata (setup.py): finished with status 'done'\n248 | Downloading numpydoc-0.5.tar.gz (32 kB)\n249 | Preparing metadata (setup.py): started\n250 | Preparing metadata (setup.py): finished with status 'done'\n251 | Downloading numpydoc-0.4.tar.gz (28 kB)\n252 | Preparing metadata (setup.py): started\n253 | Preparing metadata (setup.py): finished with status 'error'\n254 | error: subprocess-exited-with-error\n255 |  \n256 | × python setup.py egg_info did not run successfully.\n257 | │ exit code: 1\n258 | ╰─> [1 lines of output]\n259 | error in numpydoc setup command: \"values of 'package_data' dict\" must be of type <tuple[str, ...] \\| list[str]> (got 'tests')\n260 | [end of output]\n261 |  \n262 | note: This error originates from a subprocess, and is likely not a problem with pip.\n263 | error: metadata-generation-failed\n264 |  \n265 | × Encountered error while generating package metadata.\n266 | ╰─> See above for output.\n267 |  \n268 | note: This is an issue with the package mentioned above, not pip.\n269 | hint: See above for details.\n```",
    "comments": [
      {
        "user": "fjetter",
        "body": "I looked into this a bit and the problems appears to be that pip is having issues solving the environment and is trying **ancient** versions. I suspect those versions were just not forward compatible with the build process. It's possible to avoid this for numpydoc by adding a conservative lower bound, e.g. `numpydoc>1` but then something else fails.\n\nI tried moving to conda but can't get a solve, primarily due to https://github.com/dask/dask-sphinx-theme/issues/68"
      },
      {
        "user": "fjetter",
        "body": "Looks like this is due to the latest pip version 25.1. `pip==25.0.1` still produces a working but corrupt environment."
      }
    ]
  },
  {
    "issue_number": 11922,
    "title": "dask.array with query planning misses materialization dunder methods",
    "author": "crusaderky",
    "state": "open",
    "created_at": "2025-04-29T09:53:57Z",
    "updated_at": "2025-04-29T09:55:43Z",
    "labels": [
      "needs triage"
    ],
    "body": "dask 2025.4.1\n\n`dask.array._array_expr.Array` lacks `__array__`, `__bool__`, `__int__`, `__float__`, `__complex__`, and `__index__` methods.\nI am unsure if this is a deliberate change (_it would make sense if it were_, but I cannot find documentation); regardless the user-facing error is obscure. In the case of `np.asarray`, the function doesn't crash but instead returns a numpy array of dask arrays.\n\n```python\n>>> import dask\n>>> dask.__version__\n'2025.4.1'\n>>> dask.config.set({\"array.query-planning\": True})\n>>> import dask.array as da\n>>> import numpy as np\n>>> import operator\n>>> a = da.asarray(1)\n>>> int(a)\nTypeError: int() argument must be a string, a bytes-like object or a real number, not 'Array'\n>>> float(a)\nTypeError: float() argument must be a string or a real number, not 'Array'\n>>> operator.index(a)\nTypeError: 'Array' object cannot be interpreted as an integer\n>>> bool(a.any())\nTypeError: len() of unsized object\n>>> np.asarray(a)\narray(<dask.array._array_expr._collection.Array object at 0x7ac0aeaedde0>,\n      dtype=object)\n```\n\n# Expected behaviour\nEither\n- these functions continue auto-materializing the graph like they do without query planning, with all the pros and the cons involved, or\n- they raise a very clear error message that they are no longer meant to work.\n\nI think that the former is a much healthier option. Disabling auto-materialization would be a BIG change (although I would 100% agree with it), so it should be definitely decoupled from the transition to query planning, with a substantial deprecation cycle too.\n\nCC @fjetter ",
    "comments": []
  },
  {
    "issue_number": 11921,
    "title": "`dask.core.store` always return `tuple`",
    "author": "emmaai",
    "state": "open",
    "created_at": "2025-04-29T03:13:05Z",
    "updated_at": "2025-04-29T07:59:30Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nNot sure if it's an intentional change and only a docs issue,  that `dask.core.store` only returns `tuple` now no matter what `returned_stored` is set.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\nimport dask\nimport numpy as np\n\nx = da.zeros([5, 5, 5])\ny = da.ones([5, 5, 5])\ndset1 = dask.delayed(np.zeros(x.shape))\ndset2 = dask.delayed(np.zeros(y.shape))\nfut = da.core.store([x,y], [dset1, dset2], compute=False, return_stored=False)\nprint(fut)\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.4.1\n- Python version: 3.12\n- Operating System: Debian\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "The documentation wasn't updated but that's intended after https://github.com/dask/dask/pull/11844\n\n\nTo compute, use `dask.compute(fut)`"
      }
    ]
  },
  {
    "issue_number": 11918,
    "title": "Windows tests failing with 127",
    "author": "fjetter",
    "state": "closed",
    "created_at": "2025-04-28T12:13:56Z",
    "updated_at": "2025-04-29T07:56:19Z",
    "labels": [
      "needs triage"
    ],
    "body": "All windows test are currently broken without even running CI\n\nFor example: https://github.com/dask/dask/actions/runs/14701358898/job/41251358583",
    "comments": []
  },
  {
    "issue_number": 11897,
    "title": "Computing a DataFrame column produced by map_partitions raises an assertion error",
    "author": "rsundqvist",
    "state": "closed",
    "created_at": "2025-04-22T08:47:01Z",
    "updated_at": "2025-04-28T13:14:54Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nCalling `compute` on a ddf column produced by `map_partitions` raises an assertion error.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport pandas as pd\nimport dask.dataframe as dd\n\npath = \"parquet-data\"\n# import shutil; shutil.rmtree(path, ignore_errors=True)\ndata = {\"country\": [\"se\", \"dk\", \"no\"], \"value\": [0.1, 0.2, 0.3]}\npd.DataFrame(data).to_parquet(path, partition_cols=[\"country\"])\ndf = dd.read_parquet(path)\n\ny_pred = df.map_partitions(len)\ny_pred.compute()  # This works\ndf[\"y_pred\"] = y_pred\nassert \"y_pred\" in df.compute()  # This works\ndf[\"y_pred\"].compute()  # This crashes\n```\n\nThis causes an assertion to fail:\n\n```python\n  File \"<sys.exec_prefix>/lib/python3.11/site-packages/dask/dataframe/dask_expr/_expr.py\", line 580, in _divisions\n    assert arg.divisions == dependencies[0].divisions\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n```\n\n**Anything else we need to know?**:\nCuriously, this seems to work if you do another `map_partitions` first, even if it doesn't change the frame.\n```python\ndf[\"y_pred\"] = df.map_partitions(len)\ndf = df.map_partitions(lambda part: part)  # comment this line = crash\ndf[\"y_pred\"].compute()  # This works now\n```\n\nThis adds a node `MapPartitions(Assign)` when inspecting the graph with `visualize()`, which doesn't feel like it should be a solution.\n\n**Environment**:\n\n- Dask version: 2025.3.0\n- Python version: 3.11.12\n- Operating System: Ubuntu 24.04.2 LTS\n- Install method (conda, pip, source): pip\n\nThe results seem to be the same on a few other version",
    "comments": []
  },
  {
    "issue_number": 11916,
    "title": "Difference in *display* of masked array depending if using `compute()` or not",
    "author": "habi",
    "state": "open",
    "created_at": "2025-04-28T10:10:34Z",
    "updated_at": "2025-04-28T10:51:54Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nI'm computing Otsu thresholds on *many* tomographic datasets.\nFor quality control I want to display the thresholded data on top of the original (with `matplotlib.pyplot`).\nThis can be easily done with a masked array, but I've run into a *display* issue when \"converting\" my code from `nump` to `dask`.\n\nWhen I do not use `compute()`, the masked array is not displayed as I'd have expected it to do, and is *not* masked.\n\n![Image](https://github.com/user-attachments/assets/4640b1b6-6080-4920-89d4-bea3a20f9928)\n\nI'm not sure if this is really an issue, or just me displaying dask-masked arrays wrongly.\n\n**Minimal Complete Verifiable Example**:\n\nA minimized example showing the issue is found here: https://gist.github.com/habi/27085cb8a3a096a91dee43972202d6ea\n\n**Environment**:\n\n- Dask version: 2025.3.0\n- Python version: 3.10.16\n- Operating System: Linux Mint 22\n- Install method (conda, pip, source): `conda`\n",
    "comments": [
      {
        "user": "habi",
        "body": "Here's a direct [Binder link](https://mybinder.org/v2/gist/habi/27085cb8a3a096a91dee43972202d6ea/HEAD), where only the first line needs to be uncommented and run, too."
      }
    ]
  },
  {
    "issue_number": 11758,
    "title": "Document that `Series.unique` order differs from Pandas",
    "author": "hoxbro",
    "state": "open",
    "created_at": "2025-02-18T12:53:45Z",
    "updated_at": "2025-04-28T02:09:44Z",
    "labels": [
      "dataframe",
      "documentation",
      "needs attention"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nRunning  `.unique` on a Series differs from the output from pandas. I'm unsure if this is a bug or just a consequence of the move to query planning. If so, it would be nice to mention it in the [docstring](https://docs.dask.org/en/stable/generated/dask.dataframe.Series.unique.html).\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\n\ndf = pd.DataFrame([[1, 2], [3, 4], [5, 6]], columns=['x', 'y'])\nddf = dd.from_pandas(df, npartitions=2)\n\nddf.x.unique().compute()  # 3, 1, 5\ndf.x.unique()  # 1, 3, 5\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.1.0\n- Python version: 3.12\n- Operating System: Linux\n- Install method (conda, pip, source): conda\n",
    "comments": [
      {
        "user": "maximlt",
        "body": "Note I think this is a behavior change, the previous dataframe implementation preserved data ordering as far as I can see. cc https://github.com/holoviz/hvplot/pull/1499"
      },
      {
        "user": "phofl",
        "body": "This is expected. We are triggering a shuffle under the hood to avoid overloading a single worker. And a shuffle won't preserve the input order.\n\nI'll label this as a documentation issue"
      },
      {
        "user": "rjzamora",
        "body": "Just a quick note that the \"tree-reduction\" version of unique (only good for low-cardinality data) will preserve  the original order. E.g.\n\n```python\nddf.x.unique(split_out=1).compute()  # 1, 3, 5\ndf.x.unique()  # 1, 3, 5\n```"
      }
    ]
  },
  {
    "issue_number": 11785,
    "title": "Inconsistent behavior of __bool__ between dask.Array and dask.DataFrame",
    "author": "ilia-kats",
    "state": "open",
    "created_at": "2025-02-26T13:00:04Z",
    "updated_at": "2025-04-28T02:09:42Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nCasting a scalar `dask.Array` to a boolean (e.g. in a conditional statement) automatically runs compute(), which is very convenient when passing dask Arrays to 3rd party code that is not aware of dask, but works with array-api-compat or similar. On the other hand, a `dask.DataFrame` raises a `TypeError`.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport pandas as pd\nimport dask.array as da\nimport dask.dataframe as dd\n\narr = np.random.rand(10,5)\ndf = pd.DataFrame({\"a\": [0,1,2,3,4], \"b\": [\"A\", \"B\", \"C\", \"D\", \"E\"]})\ndarr = da.from_array(arr)\nddf = dd.from_pandas(df)\n\nbool(darr.shape[0] == 10) # succeeds\nbool(ddf.shape[0] == 5) # raises TypeError\n```\nresults in\n```\nTypeError                                 Traceback (most recent call last)\nCell In[28], line 12\n      9 ddf = dd.from_pandas(df)\n     11 bool(darr.shape[0] == 10) # succeeds\n---> 12 bool(ddf.shape[0] == 5) # raises ValueError\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:4789, in Scalar.__bool__(self)\n   4788 def __bool__(self):\n-> 4789     raise TypeError(\n   4790         f\"Trying to convert {self} to a boolean value. Because Dask objects are \"\n   4791         \"lazily evaluated, they cannot be converted to a boolean value or used \"\n   4792         \"in boolean conditions like if statements. Try calling .compute() to \"\n   4793         \"force computation prior to converting to a boolean value or using in \"\n   4794         \"a conditional statement.\"\n   4795     )\n\nTypeError: Trying to convert <dask_expr.expr.Scalar: expr=df.size() // 2 == 5, dtype=bool> to a boolean value. Because Dask objects are lazily evaluated, they cannot be converted to a boolean value or used in boolean conditions like if statements. Try calling .compute() to force computation prior to converting to a boolean value or using in a conditional statement.\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: 3.11\n- Operating System: Debian 12\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "phofl",
        "body": "I think the only direction we would consider here is to make Dask Array raise as well. We are moving away from implicit computes since they can be ridiculously expensive. Not sure if we want to change anything here though"
      }
    ]
  },
  {
    "issue_number": 11789,
    "title": "A new way of \"dask on ray\"",
    "author": "wingkitlee0",
    "state": "open",
    "created_at": "2025-02-27T04:08:28Z",
    "updated_at": "2025-04-28T02:09:41Z",
    "labels": [
      "documentation",
      "discussion",
      "needs attention"
    ],
    "body": "There was a time that `dask` can be run as Ray tasks on a Ray cluster. That functionality was implemented long time ago using some private functions (which were removed/changed in recent refactoring effort in dask).\n\nJust curious how difficult is it to write a scheduler that could run Dask on Ray (again)? Am I understand correctly that this will be reimplementing `dask.distributed` using Ray core?\n\nI think there are some interests: https://github.com/ray-project/ray/issues/48689 \n\n\n\n",
    "comments": [
      {
        "user": "rjzamora",
        "body": "Hi @wingkitlee0 - \"Dask on ray\" is definitely a cool feature, but the developments/updates would need to live in the Ray repository. What you need on the dask side is clear documentation on the new task specification used in `dask`/`distributed`. \n\nMy understanding is that the task specification is still in flux for the time being (I'm not sure if @fjetter has a clear timeframe in mind)."
      },
      {
        "user": "fjetter",
        "body": "I'm not familiar with Ray and can't tell what's necessary to make this work. If the failure is due to the removal of an internal API this is hardly related to the Task specification and while that may be subject to change I doubt that's what's blocking here. Without more information I cannot help."
      },
      {
        "user": "wingkitlee0",
        "body": "Thanks for the responses.\n\nJust to clarify, this is not an \"issue\" about the broken dask-on-ray developed long time ago; so the ask isn't about fixing it. \n\nThe question is more about \"Was Dask designed to run on different distributed frameworks?\". If so, what is the proper way to do it?\n\nConcretely how does one (Ray developers) implement something new that can schedule and run dask tasks via other mechanisms. It seems to me that those refactoring work was working toward a more flexible system. As an example, one could in principle implement something with multiprocessing Pool + some queue to run dask on a single node. Is there some new APIs that allow this? what does a task specification specify? \n\n> due to the removal of an internal API \n\nWhile this is true, I do not see anyone has the interest to rescue it. Even if it's fixable, it may not be following the intended API boundaries set by Dask developers. "
      }
    ]
  },
  {
    "issue_number": 11809,
    "title": "module 'dask.dataframe.multi' has no attribute 'concat'",
    "author": "Leo-AO-99",
    "state": "closed",
    "created_at": "2025-03-05T01:50:56Z",
    "updated_at": "2025-04-25T18:54:31Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nI am trying to run the first examle code from https://xgboost.readthedocs.io/en/stable/tutorials/dask.html\n\nand get following error\n\n```shell\nTraceback (most recent call last):\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 244, in dconcat\n    return concat(value)\n  ^^^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/compat.py\", line 164, in concat\n    raise TypeError(\"Unknown type.\")\n      ^^^^^^^^^^^^^^^^^\nTypeError: Unknown type.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/liao/LeoGit/NEU_courses/a.py\", line 20, in <module>\n    output = dxgb.train(\n             ^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/core.py\", line 726, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 1088, in train\n    return client.sync(\n           ^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/distributed/utils.py\", line 363, in sync\n    return sync(\n           ^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/distributed/utils.py\", line 439, in sync\n    raise error\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/distributed/utils.py\", line 413, in f\n    result = yield future\n             ^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/tornado/gen.py\", line 766, in run\n    value = future.result()\n            ^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 1024, in _train_async\n    result = await map_worker_partitions(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 549, in map_worker_partitions\n    result = await client.compute(fut).result()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/distributed/client.py\", line 409, in _result\n    raise exc.with_traceback(tb)\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 533, in <lambda>\n    lambda *args, **kwargs: [func(*args, **kwargs)],\n^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 979, in dispatched_train\n    Xy, evals = _get_dmatrices(\n^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 921, in _get_dmatrices\n    Xy = _dmatrix_from_list_of_parts(**train_ref, nthread=n_threads)\n      ^^^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 828, in _dmatrix_from_list_of_parts\n    return _create_dmatrix(**kwargs)\n      ^^^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 810, in _create_dmatrix\n    v = concat_or_none(value)\n  ^^^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 805, in concat_or_none\n    return dconcat(data)\n  ^^^^^^^^^^^^^^^^^\n  File \"/Users/liao/miniconda3/envs/boost3/lib/python3.11/site-packages/xgboost/dask/__init__.py\", line 246, in dconcat\n    return dd.multi.concat(list(value), axis=0)\n  ^^^^^^^^^^^^^^^^^\nAttributeError: module 'dask.dataframe.multi' has no attribute 'concat'\n```\n\n**Minimal Complete Verifiable Example**:\n\n```python\nfrom xgboost import dask as dxgb\n\nimport dask.array as da\nimport dask.distributed\n\nif __name__ == \"__main__\":\n    cluster = dask.distributed.LocalCluster()\n    client = dask.distributed.Client(cluster)\n\n    # X and y must be Dask dataframes or arrays\n    num_obs = 1e5\n    num_features = 20\n    X = da.random.random(size=(num_obs, num_features), chunks=(1000, num_features))\n    y = da.random.random(size=(num_obs, 1), chunks=(1000, 1))\n\n    dtrain = dxgb.DaskDMatrix(client, X, y)\n    # or\n    # dtrain = dxgb.DaskQuantileDMatrix(client, X, y)\n\n    output = dxgb.train(\n        client,\n        {\"verbosity\": 2, \"tree_method\": \"hist\", \"objective\": \"reg:squarederror\"},\n        dtrain,\n        num_boost_round=4,\n        evals=[(dtrain, \"train\")],\n    )\n```\n\n**Anything else we need to know?**:\n\nI create a new conda env, and only install neccessary packages\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: 3.11\n- Operating System: mac M3\n- Install method (conda, pip, source): conda\n- xgboost version: 2.1.4\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Hi, thanks for your report. This is an issue on the xgboost side, but has already been fixed on their latest development version and will be in the next release. You could ping them to see if they would include that in a patch release, but I don't know their policy and if that's an option"
      },
      {
        "user": "hakan-77",
        "body": "Hi @phofl we are still facing this issue, do you know which XGBoost issue/PR has resolved this?\n\nI see on XGBoost repo open issues. I'm sharing because we had to pin Dask to <24.12 as well.\nhttps://github.com/dmlc/xgboost/issues/10994\nhttps://github.com/dmlc/xgboost/issues/11324\n"
      },
      {
        "user": "TomAugspurger",
        "body": "@hakan-77 you're seeing the specific issue about `AttributeError: module 'dask.dataframe.multi' has no attribute 'concat'`? The linked xgboost issues are hitting different failures."
      }
    ]
  },
  {
    "issue_number": 11909,
    "title": "dask with zarr as file i/o slower in v3 vs v2 pakcage",
    "author": "ilan-gold",
    "state": "open",
    "created_at": "2025-04-25T08:55:53Z",
    "updated_at": "2025-04-25T17:12:39Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n~~There is an order of magnitude difference (at least 2X, see below) between zarr v2 and v3, and not in the direction you would expect.  v3 is much slower.~~\n\nZarr v3 is somehwat (maybe much) slower than zarr v2 the package (see https://github.com/dask/dask/issues/11909#issuecomment-2829813155)\n\n**Minimal Complete Verifiable Example**:\n\nFor zarr v3:\n```python\nimport dask.array as da\nimport zarr\n\nda.random.random((10_000, 10_000)).to_zarr(\"./foo.zarr\", zarr_format=2)\narr = da.from_zarr('./foo.zarr')\n%timeit arr[...].compute()\n```\n\nFor zarr v2:\n```python\nimport dask.array as da\n\narr = da.from_zarr('./foo.zarr')\n%timeit arr[...].compute()\n```\n\n**Anything else we need to know?**:\n\n\n**Environment**:\n\n- Dask version: `main`  branch as of writing i.e., c68e71d204b38cf6500090f48f99daa3884463f8\n- Python version: 3.12\n- Operating System: Mac\n- Install method (conda, pip, source): `uv pip`\n",
    "comments": [
      {
        "user": "Intron7",
        "body": "I see the same behavior on 2 different Linux based systems."
      },
      {
        "user": "ilan-gold",
        "body": "Update - it seems relying on the `uv pip install -e .` by memory is not a good idea.  Once I upgraded my local install, the problem got better but is still present to some degree:\n\nFor zarr v3: `443 ms ± 8.99 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)`\nFor zarr v2: `365 ms ± 12.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)`"
      },
      {
        "user": "LDeakin",
        "body": "Could be the switch to zstd by default?"
      }
    ]
  },
  {
    "issue_number": 11902,
    "title": "dask.delayed calls crash if a collection containing P2PBarrierTask as a dependency is passed in as a parameter",
    "author": "roflmaoqwertyqaz",
    "state": "closed",
    "created_at": "2025-04-24T00:08:10Z",
    "updated_at": "2025-04-25T15:10:04Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nIn the latest Dask version (2025.4.0), calling `dask.delayed` with a parameter that is a dask collection containing `P2PBarrierTask`  in its graph call crashes with the following error:\n`TypeError: P2PBarrierTask.__init__() missing 1 required keyword-only argument: 'spec'`\nThis error does not occur if the collection is explicitly converted to a delayed using `to_delayed()`. It is also worth noting that this error does not happen after calling `compute()`, but it happens when creating the delayed task in `dask.delayed()`\n\nI dug into the source code and used a debugger to investigate, and I think I found the root cause, but I will leave it to you all to decide on the best course of action. This regression happened after the changes made by #11881. It seems that the `Task.substitute` calls in `ProhibitReuse.__dask_graph__` do not copy the `spec` kwarg for `P2PBarrierTask` because `P2PBarrierTask` (defined in the [dask/distributed](https://github.com/dask/distributed/blob/main/distributed/shuffle/_core.py) repo) does not pass that argument when calling `super()`. Because of that, `Task.kwargs` field used in `substitute` is an empty dict, so the `P2PBarrierTask` constructor call fails.\n\nCurrently I am working around this issue by using the previous release of dask (2025.3.0), since this issue is arising in a library I am using.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask\nimport dask.distributed as dd\nimport dask.dataframe as ddf\nimport pandas as pd\n\ndef delayed_func(x):\n    return x\n\nif __name__ == '__main__':\n    client = dd.Client(n_workers=8, threads_per_worker=1)\n    test_df = ddf.from_dict(\n        {\n            'partition': [0, 1, 2, 3, 0, 1, 2, 3],\n            'value': [1, 2, 3, 4, 5, 6, 7, 8]\n        },\n        npartitions=2\n    )\n    #running set_index for force p2p shuffle\n    part_df = test_df.set_index(\n        'partition',\n        divisions=[0, 1, 2, 3, 3],\n        shuffle_method='p2p'\n    )\n\n    #works with dask.delayed\n    outs = part_df.to_delayed()\n    delay_test1 = dask.delayed(\n        delayed_func\n    )(outs)\n    print(delay_test1.compute())\n\n    # does not work with direct input of other dask collection, even if\n    # collection is only 1 partition.\n    delay_test2 = dask.delayed(\n        delayed_func\n    )(part_df.repartition(npartitions=1))\n    print(delay_test2.compute())\n    client.close()\n\n```\n\nOutput in dask 2025.4.0:\n```\n2025-04-23 18:37:20,175 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 53adee7987ab7a6f999d30a134cfdf8c initialized by task ('shuffle-transfer-53adee7987ab7a6f999d30a134cfdf8c', 0) executed on worker tcp://127.0.0.1:8953\n2025-04-23 18:37:20,457 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 53adee7987ab7a6f999d30a134cfdf8c deactivated due to stimulus 'task-finished-1745451440.4560237'\n[           value\npartition\n0              1\n0              5,            value\npartition\n1              2\n1              6,            value\npartition\n2              3\n2              7,            value\npartition\n3              4\n3              8]\nTraceback (most recent call last):\n  File \"C:\\Users\\{obfuscated}\\bug_test.py\", line 34, in <module>\n    delay_test2 = dask.delayed(\n                  ^^^^^^^^^^^^^\n  File \"C:\\Users\\{obfuscated}\\venv\\Lib\\site-packages\\dask\\delayed.py\", line 846, in __call__\n    return call_function(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\{obfuscated}\\venv\\Lib\\site-packages\\dask\\delayed.py\", line 813, in call_function\n    args2, collections = unzip(map(unpack_collections, args), 2)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\{obfuscated}\\venv\\Lib\\site-packages\\dask\\utils.py\", line 2298, in unzip\n    out = list(zip(*ls))\n               ^^^^^^^^\n  File \"C:\\Users\\{obfuscated}\\venv\\Lib\\site-packages\\dask\\delayed.py\", line 181, in unpack_collections\n    dsk = finalized.__dask_graph__()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\{obfuscated}\\venv\\Lib\\site-packages\\dask\\_expr.py\", line 1356, in __dask_graph__\n    dsk.pop(old_key).substitute(subs),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\{obfuscated}\\venv\\Lib\\site-packages\\dask\\_task_spec.py\", line 796, in substitute\n    return type(self)(\n           ^^^^^^^^^^^\nTypeError: P2PBarrierTask.__init__() missing 1 required keyword-only argument: 'spec'\n```\n\nOutput in dask 2025.3.0 (expected behavior):\n```\n2025-04-23 18:42:40,518 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b70c7311b9f2e5da0cfb421c0509c85f initialized by task ('shuffle-transfer-b70c7311b9f2e5da0cfb421c0509c85f', 0) executed on worker tcp://127.0.0.1:9165\n2025-04-23 18:42:40,863 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b70c7311b9f2e5da0cfb421c0509c85f deactivated due to stimulus 'task-finished-1745451760.8599823'\n[           value\npartition\n0              1\n0              5,            value\npartition\n1              2\n1              6,            value\npartition\n2              3\n2              7,            value\npartition\n3              4\n3              8]\n2025-04-23 18:42:40,915 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b70c7311b9f2e5da0cfb421c0509c85f initialized by task ('shuffle-transfer-b70c7311b9f2e5da0cfb421c0509c85f', 0) executed on worker tcp://127.0.0.1:9165\n2025-04-23 18:42:40,992 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b70c7311b9f2e5da0cfb421c0509c85f deactivated due to stimulus 'task-finished-1745451760.9919975'\n           value\npartition\n0              1\n0              5\n1              2\n1              6\n2              3\n2              7\n3              4\n3              8\n```\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.4.0\n- Python version: 3.12.6\n- Operating System: Observed on both Windows and Ubuntu Linux\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "Thanks for the bug report. I'll have a look shortly"
      },
      {
        "user": "jrbourbeau",
        "body": "Closed via https://github.com/dask/dask/pull/11906"
      },
      {
        "user": "roflmaoqwertyqaz",
        "body": "Thanks everyone, that was quick!"
      }
    ]
  },
  {
    "issue_number": 11880,
    "title": "Reductions on scipy.sparse arrays fail",
    "author": "ilia-kats",
    "state": "open",
    "created_at": "2025-04-09T10:10:46Z",
    "updated_at": "2025-04-25T13:58:29Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nRunning any reduction on a Dask Array backed by a scipy.sparse array fails due to the sparse array not supporting the `keepdims` argument.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nfrom scipy.sparse import csr_array\nimport dask.array as da\n\nrow = np.array([0, 0, 1, 2, 2, 2])\ncol = np.array([0, 2, 2, 0, 1, 2])\ndata = np.array([1, 2, 3, 4, 5, 6])\n\ntest = csr_array((data, (row, col)), shape=(3, 3))\ndatest = da.from_array(test)\n\nnp.sum(datest)\n```\nresults in\n```python\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/core.py:1811, in Array.__array_function__(self, func, types, args, kwargs)\n   1808 if has_keyword(da_func, \"like\"):\n   1809     kwargs[\"like\"] = self\n-> 1811 return da_func(*args, **kwargs)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/reductions.py:70, in sum(a, axis, dtype, keepdims, split_every, out)\n     68 if dtype is None:\n     69     dtype = getattr(np.zeros(1, dtype=a.dtype).sum(), \"dtype\", object)\n---> 70 result = reduction(\n     71     a,\n     72     chunk.sum,\n     73     chunk.sum,\n     74     axis=axis,\n     75     keepdims=keepdims,\n     76     dtype=dtype,\n     77     split_every=split_every,\n     78     out=out,\n     79 )\n     80 return result\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/_reductions_generic.py:173, in reduction(x, chunk, aggregate, axis, keepdims, dtype, split_every, combine, name, out, concatenate, output_size, meta, weights)\n    170     args += (wgt, inds)\n    172 # The dtype of `tmp` doesn't actually matter, and may be incorrect.\n--> 173 tmp = blockwise(\n    174     chunk, inds, *args, axis=axis, keepdims=True, token=name, dtype=dtype or float\n    175 )\n    176 tmp._chunks = tuple(\n    177     (output_size,) * len(c) if i in axis else c for i, c in enumerate(tmp.chunks)\n    178 )\n    180 if meta is None and hasattr(x, \"_meta\"):\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/blockwise.py:304, in blockwise(func, out_ind, name, token, dtype, adjust_chunks, new_axes, align_arrays, concatenate, meta, *args, **kwargs)\n    301 if meta is None:\n    302     from dask.array.utils import compute_meta\n--> 304     meta = compute_meta(func, dtype, *args[::2], **kwargs)\n    305 return new_da_object(graph, out, chunks, meta=meta, dtype=dtype)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/utils.py:141, in compute_meta(func, _dtype, *args, **kwargs)\n    139     if has_keyword(func, \"computing_meta\"):\n    140         kwargs_meta[\"computing_meta\"] = True\n--> 141     meta = func(*args_meta, **kwargs_meta)\n    142 except TypeError as e:\n    143     if any(\n    144         s in str(e)\n    145         for s in [\n   (...)\n    149         ]\n    150     ):\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:2389, in sum(a, axis, dtype, out, keepdims, initial, where)\n   2386         return out\n   2387     return res\n-> 2389 return _wrapreduction(\n   2390     a, np.add, 'sum', axis, dtype, out,\n   2391     keepdims=keepdims, initial=initial, where=where\n   2392 )\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:82, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)\n     78 else:\n     79     # This branch is needed for reductions like any which don't\n     80     # support a dtype.\n     81     if dtype is not None:\n---> 82         return reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\n     83     else:\n     84         return reduction(axis=axis, out=out, **passkwargs)\n\nTypeError: _cs_matrix.sum() got an unexpected keyword argument 'keepdims'\n```\n\nRunning `np.nanvar` yields a different exception, but as far as I can tell the root cause is still the `keepdims` argument:\n```python\nnp.nanvar(datest)\n\nTypeError                                 Traceback (most recent call last)\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/_reductions_generic.py:322, in partial_reduce(func, x, split_every, keepdims, dtype, name, reduced_meta)\n    321 try:\n--> 322     meta = func(reduced_meta, computing_meta=True)\n    323 # no meta keyword argument exists for func, and it isn't required\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/reductions.py:530, in moment_agg(pairs, order, ddof, dtype, sum, axis, computing_meta, **kwargs)\n    529 ns = _concatenate2(ns, axes=axis)\n--> 530 n = ns.sum(axis=axis, **keepdim_kw)\n    532 if computing_meta:\n\nTypeError: _cs_matrix.sum() got an unexpected keyword argument 'keepdims'\n\nDuring handling of the above exception, another exception occurred:\n\nIndexError                                Traceback (most recent call last)\nCell In[74], line 1\n----> 1 np.nanvar(datest)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/core.py:1811, in Array.__array_function__(self, func, types, args, kwargs)\n   1808 if has_keyword(da_func, \"like\"):\n   1809     kwargs[\"like\"] = self\n-> 1811 return da_func(*args, **kwargs)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/reductions.py:671, in nanvar(a, axis, dtype, keepdims, ddof, split_every, out)\n    667     dt = getattr(np.var(np.ones(shape=(1,), dtype=a.dtype)), \"dtype\", object)\n    669 implicit_complex_dtype = dtype is None and np.iscomplexobj(a)\n--> 671 return reduction(\n    672     a,\n    673     partial(\n    674         moment_chunk,\n    675         sum=chunk.nansum,\n    676         numel=nannumel,\n    677         implicit_complex_dtype=implicit_complex_dtype,\n    678     ),\n    679     partial(moment_agg, sum=np.nansum, ddof=ddof),\n    680     axis=axis,\n    681     keepdims=keepdims,\n    682     dtype=dt,\n    683     split_every=split_every,\n    684     combine=partial(moment_combine, sum=np.nansum),\n    685     out=out,\n    686     concatenate=False,\n    687 )\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/_reductions_generic.py:194, in reduction(x, chunk, aggregate, axis, keepdims, dtype, split_every, combine, name, out, concatenate, output_size, meta, weights)\n    191 else:\n    192     reduced_meta = None\n--> 194 result = _tree_reduce(\n    195     tmp,\n    196     aggregate,\n    197     axis,\n    198     keepdims,\n    199     dtype,\n    200     split_every,\n    201     combine,\n    202     name=name,\n    203     concatenate=concatenate,\n    204     reduced_meta=reduced_meta,\n    205 )\n    206 if keepdims and output_size != 1:\n    207     result._chunks = tuple(\n    208         (output_size,) if i in axis else c for i, c in enumerate(tmp.chunks)\n    209     )\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/_reductions_generic.py:262, in _tree_reduce(x, aggregate, axis, keepdims, dtype, split_every, combine, name, concatenate, reduced_meta)\n    260 if concatenate:\n    261     func = compose(func, partial(_concatenate2, axes=sorted(axis)))\n--> 262 return partial_reduce(\n    263     func,\n    264     x,\n    265     split_every,\n    266     keepdims=keepdims,\n    267     dtype=dtype,\n    268     name=(name or funcname(aggregate)) + \"-aggregate\",\n    269     reduced_meta=reduced_meta,\n    270 )\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/_reductions_generic.py:326, in partial_reduce(func, x, split_every, keepdims, dtype, name, reduced_meta)\n    324 except TypeError:\n    325     try:\n--> 326         meta = func(reduced_meta)\n    327     except ValueError as e:\n    328         # min/max functions have no identity, don't apply function to meta\n    329         if \"zero-size array to reduction operation\" in str(e):\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/reductions.py:528, in moment_agg(pairs, order, ddof, dtype, sum, axis, computing_meta, **kwargs)\n    525 keepdim_kw[\"keepdims\"] = True\n    526 keepdim_kw[\"dtype\"] = None\n--> 528 ns = deepmap(lambda pair: pair[\"n\"], pairs) if not computing_meta else pairs\n    529 ns = _concatenate2(ns, axes=axis)\n    530 n = ns.sum(axis=axis, **keepdim_kw)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/utils.py:283, in deepmap(func, *seqs)\n    272 \"\"\"Apply function inside nested lists\n    273 \n    274 >>> inc = lambda x: x + 1\n   (...)\n    280 [[11, 22], [33, 44]]\n    281 \"\"\"\n    282 if isinstance(seqs[0], (list, Iterator)):\n--> 283     return [deepmap(func, *items) for items in zip(*seqs)]\n    284 else:\n    285     return func(*seqs)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/utils.py:283, in <listcomp>(.0)\n    272 \"\"\"Apply function inside nested lists\n    273 \n    274 >>> inc = lambda x: x + 1\n   (...)\n    280 [[11, 22], [33, 44]]\n    281 \"\"\"\n    282 if isinstance(seqs[0], (list, Iterator)):\n--> 283     return [deepmap(func, *items) for items in zip(*seqs)]\n    284 else:\n    285     return func(*seqs)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/utils.py:285, in deepmap(func, *seqs)\n    283     return [deepmap(func, *items) for items in zip(*seqs)]\n    284 else:\n--> 285     return func(*seqs)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/dask/array/reductions.py:528, in moment_agg.<locals>.<lambda>(pair)\n    525 keepdim_kw[\"keepdims\"] = True\n    526 keepdim_kw[\"dtype\"] = None\n--> 528 ns = deepmap(lambda pair: pair[\"n\"], pairs) if not computing_meta else pairs\n    529 ns = _concatenate2(ns, axes=axis)\n    530 n = ns.sum(axis=axis, **keepdim_kw)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/scipy/sparse/_csr.py:24, in _csr_base.__getitem__(self, key)\n     22 def __getitem__(self, key):\n     23     if self.ndim == 2:\n---> 24         return super().__getitem__(key)\n     26     if isinstance(key, tuple) and len(key) == 1:\n     27         key = key[0]\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/scipy/sparse/_index.py:52, in IndexMixin.__getitem__(self, key)\n     51 def __getitem__(self, key):\n---> 52     row, col = self._validate_indices(key)\n     54     # Dispatch to specialized methods.\n     55     if isinstance(row, INT_TYPES):\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/scipy/sparse/_index.py:186, in IndexMixin._validate_indices(self, key)\n    184     row = _validate_bool_idx(bool_row, M, \"row\")\n    185 elif not isinstance(row, slice):\n--> 186     row = self._asindices(row, M)\n    188 if isintlike(col):\n    189     col = int(col)\n\nFile /data/ilia/envs/famo/lib/python3.11/site-packages/scipy/sparse/_index.py:212, in IndexMixin._asindices(self, idx, length)\n    209     raise IndexError('invalid index') from e\n    211 if x.ndim not in (1, 2):\n--> 212     raise IndexError('Index dimension must be 1 or 2')\n    214 if x.size == 0:\n    215     return x\n\nIndexError: Index dimension must be 1 or 2\n```\n\n**Environment**:\n\n- Dask version: 2025.03.0\n- Python version: 3.11.2\n- Operating System: Debian 12\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "ilan-gold",
        "body": "@ilia-kats `dask` does not have a supported sum function for sparse, but we do: https://github.com/scverse/fast-array-utils/blob/main/src/fast_array_utils/stats/_sum.py.  The package is on pypi.  Sorry, just commented because I noticed your name and I was here for different reasons.  Maybe the dask maintainers (@phofl ?) would be interested in a contribution?"
      },
      {
        "user": "ilia-kats",
        "body": "@ilan-gold I actually need `nanvar`, `nanmean`, and `nanmin`, not `sum`. I used `sum` in the example here because the traceback is simpler and easier to follow while still having the same root cause."
      },
      {
        "user": "ilan-gold",
        "body": "If we don't hear back here and/or you want to contribute them quickly, you could open an issue in `fast-array-utils` and see what Phil says."
      }
    ]
  },
  {
    "issue_number": 11905,
    "title": "Memory leak when using map_partitions on method",
    "author": "jtilly",
    "state": "open",
    "created_at": "2025-04-24T11:41:14Z",
    "updated_at": "2025-04-24T14:08:52Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nWhen I pass a method of an object to `map_partitions`, I observe a memory leak. The object seems to get copied for each partition of the data (which makes sense), but it doesn't get cleaned up until the very end, which makes me run out of memory.\n\nIs there a way to clean this up more efficiently? This may not be a bug, I might just not be \"holding it right\". I'm observing this behavior with all recent versions of dask.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nfrom dask.distributed import LocalCluster\nfrom distributed import Client\n\n\nclass Nothing:\n    def __init__(self, x):\n        self.x = x\n\n    def compute(self, df: pd.DataFrame):\n        # do nothing\n        return None\n\n\nif __name__ == \"__main__\":\n    # I'm making ddf tiny for the purposes of having a clean reproducer\n    N = 10_000\n    ddf = dd.from_pandas(pd.DataFrame({\"a\": range(N)}), npartitions=1_000)\n\n    # Large-ish data frame that is an attribute of Nothing\n    x = pd.DataFrame({\"a\": [str(x) for x in range(10_000)]})\n    nothing = Nothing(x=x)\n\n    with LocalCluster(n_workers=1) as cluster:\n        with Client(cluster) as client:\n            out = ddf.map_partitions(nothing.compute, meta=pd.Series([None]))\n            out.compute()\n\n```\n\n`mprof run -C` produces the following graph:\n\n![Image](https://github.com/user-attachments/assets/d8439eb3-d796-4f5b-ab2d-9334d1ce9ed2)\n\nWhen I add `del self.x` to `compute(...)`, I get a much nicer picture:\n\n![Image](https://github.com/user-attachments/assets/e84907bf-8d8a-459b-b5af-af2e70f01693)\n \n\nThe example is a bit contrived but it mirrors the real world problem reasonably well:\n- I have an embarrassingly parallel problem\n- I have an object that takes some time to instantiate\n- I want to apply a method of that object to all my partitions\n- I can work around the problem by manually serializing / deserializing the object, but that looks/feels a bit clunky \n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.3.0\n- Python version: 3.12.9\n- Operating System: ubuntu\n- Install method (conda, pip, source): conda\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "The problem you're facing here is that `nothing.compute` is a bound method, i.e. that callable keeps a reference to `Nothing` which keeps a reference to your `DataFrame` alive. Apart from the memory blowup, this also causes this \"thing\" to be quite expensive to serialize. You'll be much better off if you can convert this method to a `staticmethod` or a global function instead.\n\nThe memory blowup then stems from the fact that we can not deduplicate those objects across interpreters, i.e. every task will carry a new dataframe."
      },
      {
        "user": "fjetter",
        "body": "So instead, you should try to do it like...\n\n\n```python\n\nclass Nothing:\n    def __init__(self, x):\n        self.x = x\n\n    @staticmethod\n    def compute(x, df):\n        # do nothing\n        return None\n\n...\n\nout = ddf.map_partitions(Nothing.compute, nothing.x, meta=pd.Series([None]))\n```\n\nNote, though, that whatever you are passing to the function is likely going to be replicated. If this is a lot of data, wrap it in a `delayed` \n\n```python\nx = delayed(nothing.x)\nout = ddf.map_partitions(Nothing.compute, x, meta=pd.Series([None]))\n```\n\nthis way it is only sent to the cluster once and we pass references around"
      },
      {
        "user": "jtilly",
        "body": "Thanks for your quick response. That makes perfect sense, but I was somehow hoping that the workers would clean up the `nothing` object once they're done.\n\nI'm still keeping `x` in memory many times with `staticmethod`, `delayed` or a global function. Creating `x` inside `compute` (obviously) works and may be the cleanest solution.\n\nAm I missing anything? Should I expect any of this to \"just work\"?\n\n### staticmethod \n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nfrom dask.distributed import LocalCluster\nfrom distributed import Client\n\n\nclass Nothing:\n    def __init__(self, x):\n        self.x = x\n\n    @staticmethod\n    def compute(x, df: pd.DataFrame):\n        return None\n\n\nif __name__ == \"__main__\":\n    # I'm making ddf tiny for the purposes of having a clean reproducer\n    N = 10_000\n    ddf = dd.from_pandas(pd.DataFrame({\"a\": range(N)}), npartitions=1_000)\n\n    # Large-ish data frame that is an attribute of Nothing\n    x = pd.DataFrame({\"a\": [str(x) for x in range(10_000)]})\n    nothing = Nothing(x=x)\n\n    with LocalCluster(n_workers=1) as cluster:\n        with Client(cluster) as client:\n            out = ddf.map_partitions(Nothing.compute, nothing.x, meta=pd.Series([None]))\n            out.compute()\n```\n\n![Image](https://github.com/user-attachments/assets/a7bea3f2-292b-4a13-8124-a8ffd3cc16d3)\n\n### global function\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nfrom dask.distributed import LocalCluster\nfrom distributed import Client\n\ndef compute(df, x):\n    return None\n\n\nif __name__ == \"__main__\":\n    # I'm making ddf tiny for the purposes of having a clean reproducer\n    N = 10_000\n    ddf = dd.from_pandas(pd.DataFrame({\"a\": range(N)}), npartitions=1_000)\n    x = pd.DataFrame({\"a\": [str(x) for x in range(10_000)]})\n\n    with LocalCluster(n_workers=1) as cluster:\n        with Client(cluster) as client:\n            out = ddf.map_partitions(compute, x, meta=pd.Series([None]))\n            out.compute()\n```\n\n![Image](https://github.com/user-attachments/assets/649a26ea-9589-4889-a1a0-e183504b95b0)\n\n### delayed\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nfrom dask.distributed import LocalCluster\nfrom distributed import Client\nfrom dask import delayed\n\n\nclass Nothing:\n    def __init__(self, x):\n        self.x = x\n\n    @staticmethod\n    def compute(x, df: pd.DataFrame):\n        return None\n\n\nif __name__ == \"__main__\":\n    # I'm making ddf tiny for the purposes of having a clean reproducer\n    N = 10_000\n    ddf = dd.from_pandas(pd.DataFrame({\"a\": range(N)}), npartitions=1_000)\n\n    # Large-ish data frame that is an attribute of Nothing\n    x = pd.DataFrame({\"a\": [str(x) for x in range(10_000)]})\n    nothing = Nothing(x=x)\n\n    with LocalCluster(n_workers=1) as cluster:\n        with Client(cluster) as client:\n            x = delayed(nothing.x)\n            out = ddf.map_partitions(Nothing.compute, x, meta=pd.Series([None]))\n            out.compute()\n```\n\n![Image](https://github.com/user-attachments/assets/9a1e306e-9b39-419d-9a7c-02a0eab58471)\n\n### create `x` inside compute\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nfrom dask.distributed import LocalCluster\nfrom distributed import Client\n\ndef create_x():\n    return pd.DataFrame({\"a\": [str(x) for x in range(10_000)]})\n\ndef compute(df):\n    x = create_x()\n    return None\n\n\nif __name__ == \"__main__\":\n    # I'm making ddf tiny for the purposes of having a clean reproducer\n    N = 10_000\n    ddf = dd.from_pandas(pd.DataFrame({\"a\": range(N)}), npartitions=1_000)\n\n    with LocalCluster(n_workers=1) as cluster:\n        with Client(cluster) as client:\n            out = ddf.map_partitions(compute, meta=pd.Series([None]))\n            out.compute()\n```\n\nI can also speed this up by caching `create_x`.\n\n![Image](https://github.com/user-attachments/assets/bb9e9a72-513d-4c31-81fb-017569c30f2b)"
      }
    ]
  },
  {
    "issue_number": 11895,
    "title": "xarray upstream test failure",
    "author": "dcherian",
    "state": "closed",
    "created_at": "2025-04-18T04:31:56Z",
    "updated_at": "2025-04-22T22:09:21Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\n\nOne of the xarray tests is failing with upstream dask\n\nhttps://github.com/pydata/xarray/issues/10189#issue-2959201140\n\n```\nxarray/tests/test_dask.py::test_dask_layers_and_dependencies: AssertionError: assert False\n +  where False = <built-in method issuperset of set object at 0x7fae17a7f680>({'xarray-bar-f54588d9011da546160d9d0ff474270a': set(), 'xarray-foo-f54588d9011da546160d9d0ff474270a': set()})\n +    where <built-in method issuperset of set object at 0x7fae17a7f680> = {'Dataset-441a8e39-2e36-4113-8414-d20c6946292a', 'finalize-hlgfinalizecompute-125dcea4c3b340e7b147f3974e7ee789-6500cce611d24d4ebb3b0447c0efbc0b'}.issuperset\n +      where {'Dataset-441a8e39-2e36-4113-8414-d20c6946292a', 'finalize-hlgfinalizecompute-125dcea4c3b340e7b147f3974e7ee789-6500cce611d24d4ebb3b0447c0efbc0b'} = set({'Dataset-441a8e39-2e36-4113-8414-d20c6946292a': {'finalize-hlgfinalizecompute-125dcea4c3b340e7b147f3974e7ee789-6500cc...0447c0efbc0b'}, 'finalize-hlgfinalizecompute-125dcea4c3b340e7b147f3974e7ee789-6500cce611d24d4ebb3b0447c0efbc0b': set()})\n +        where {'Dataset-441a8e39-2e36-4113-8414-d20c6946292a': {'finalize-hlgfinalizecompute-125dcea4c3b340e7b147f3974e7ee789-6500cc...0447c0efbc0b'}, 'finalize-hlgfinalizecompute-125dcea4c3b340e7b147f3974e7ee789-6500cce611d24d4ebb3b0447c0efbc0b': set()} = HighLevelGraph with 2 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7fae17a4aed0>\\n 0. finalize-hlgfinalizecompute-125dcea4c3b340e7b147f3974e7ee789-6500cce611d24d4ebb3b0447c0efbc0b\\n 1. Dataset-441a8e39-2e36-4113-8414-d20c6946292a\\n.dependencies\n +          where HighLevelGraph with 2 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7fae17a4aed0>\\n 0. finalize-hlgfinalizecompute-125dcea4c3b340e7b147f3974e7ee789-6500cce611d24d4ebb3b0447c0efbc0b\\n 1. Dataset-441a8e39-2e36-4113-8414-d20c6946292a\\n = __dask_graph__()\n +            where __dask_graph__ = Delayed('Dataset-441a8e39-2e36-4113-8414-d20c6946292a').__dask_graph__\n +    and   {'xarray-bar-f54588d9011da546160d9d0ff474270a': set(), 'xarray-foo-f54588d9011da546160d9d0ff474270a': set()} = HighLevelGraph with 2 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7fadf08207a0>\\n 0. xarray-bar-f54588d9011da546160d9d0ff474270a\\n 1. xarray-foo-f54588d9011da546160d9d0ff474270a\\n.dependencies\n +      where HighLevelGraph with 2 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7fadf08207a0>\\n 0. xarray-bar-f54588d9011da546160d9d0ff474270a\\n 1. xarray-foo-f54588d9011da546160d9d0ff474270a\\n = __dask_graph__()\n +        where __dask_graph__ = <xarray.Dataset> Size: 80B\\nDimensions:  (x: 5)\\nDimensions without coordinates: x\\nData variables:\\n    foo      (x) int64 40B dask.array<chunksize=(5,), meta=np.ndarray>\\n    bar      (x) int64 40B dask.array<chunksize=(5,), meta=np.ndarray>.__dask_graph__\n\n```\n",
    "comments": [
      {
        "user": "jrbourbeau",
        "body": "Marking as closed via https://github.com/pydata/xarray/pull/10242 -- thanks for flagging @dcherian "
      }
    ]
  },
  {
    "issue_number": 11815,
    "title": "Index Query Hangs",
    "author": "mscanlon-exos",
    "state": "open",
    "created_at": "2025-03-05T14:31:58Z",
    "updated_at": "2025-04-21T02:10:00Z",
    "labels": [
      "dataframe",
      "needs attention",
      "bug"
    ],
    "body": "**Describe the issue**:\nAfter setting index to timestamp, some loc based query works but string based querying causes the operation to hang unless we call optimize first\n\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nimport random\n\ndef test_df() -> dd.DataFrame:\n    dfs = []\n    start_date = '2024-01-01'\n    end_date = '2024-01-31'\n    for num_rows in [2, 5, 10]:\n        df = pd.DataFrame(\n            {\n                'timestamp': pd.to_datetime(\n                    pd.date_range(start_date, end_date, periods=num_rows),\n                ),\n                'value1': random.choices(range(-20, 20), k=num_rows),\n                'value2': random.choices(range(-1000, 1000), k=num_rows),\n            },\n        )\n        dfs.append(\n            dd.from_pandas(df, npartitions=1),\n        )\n    return dd.concat(dfs)\n\ndf = test_df()\ndf = df.set_index('timestamp', npartitions=df.npartitions)\n\n# df = df.optimize()\ndf.loc[df.index > '2024-01-15'].compute() \n\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: 3.10\n- Operating System: Mac Os and Linux Tested\n- Install method (conda, pip, source): Pip\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Thanks!\n\nCould you created your dataframe in a way that we can reproduce this?"
      },
      {
        "user": "mscanlon-exos",
        "body": "Im sorry i thought i included it. Updated"
      },
      {
        "user": "rjzamora",
        "body": "I haven't investigated yet, but I can reproduce the hang locally."
      }
    ]
  },
  {
    "issue_number": 11830,
    "title": "Infer schema does not work properly for categorical types with high cardinality values",
    "author": "dbalabka",
    "state": "open",
    "created_at": "2025-03-17T17:46:08Z",
    "updated_at": "2025-04-21T02:09:56Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "Using categorical type with high cardinality values that do not fit into `int8` leads to error:\n```\nValueError: Failed to convert partition to expected pyarrow schema:\n    `ArrowInvalid('Integer value 575 not in range: -128 to 127', 'Conversion failed for column ProductName with type category')`\n```\n\nIt happens because `schema=infer` utilizes DataFrame that does not contain complete list of categorical values:\nhttps://github.com/dask/dask/blob/df372bdcdac8e7a7ba7416501bec4dff85e2b650/dask/dataframe/io/parquet/arrow.py#L655-L659\n\n![Image](https://github.com/user-attachments/assets/de86d899-152f-4cdf-80ba-97f8d5549dbe)\n\n> @bryanwweber, In my case, the problem with categorical values is that partitions contain different sets of values, leading to a schema mismatch during the Parque conversion. \n> \n> The problem is that one partition has `dictionary<values=string, indices=int8, ordered=0>` type but another has `dictionary<values=large_string, indices=int16, ordered=0>`. I assume it should happen quite often with high-cardinality categorical columns that do not fit into `int8`.\n> \n> I believe it can be a workaround by setting `int16` explicitly like `df.to_parquet('does_not_work', schema={\"name\": pa.dictionary(\"int16\", \"string\")})`.\n> \n> Here is a minimal reproducible example:\n> ```python\n> import pandas as pd\n> import numpy as np\n> import pyarrow as pa\n> import dask.dataframe as dd\n> \n> names = [ 'name']\n> address = names\n> \n> df1 = (\n>   dd\n>   .from_pandas(pd.DataFrame({'name':np.tile([f'A{i}' for i in range(1, 250)], 10)}), npartitions=1)\n> )\n> \n> df2 = (\n>   dd\n>   .from_pandas(pd.DataFrame({'name':np.tile([f'A{i}' for i in range(251, 501)], 10)}), npartitions=1)\n> )\n> \n> df = dd.concat([df1, df2]).astype('category')\n> \n> df.to_parquet('does_not_work')\n> ```\n> ```\n> ValueError: Failed to convert partition to expected pyarrow schema:\n>     `ArrowInvalid('Integer value 172 not in range: -128 to 127', 'Conversion failed for column name with type category')`\n> \n> Expected partition schema:\n>     name: dictionary<values=string, indices=int8, ordered=0>\n>     __null_dask_index__: int64\n> \n> Received partition schema:\n>     name: dictionary<values=large_string, indices=int16, ordered=0>\n>     __null_dask_index__: int64\n> \n> This error *may* be resolved by passing in schema information for\n> the mismatched column(s) using the `schema` keyword in `to_parquet`.\n> ``` \n\n _Originally posted by @dbalabka in [#6587](https://github.com/dask/dask/issues/6587#issuecomment-2717049419)_\n\n**Environment**:\n\n- Dask version: 2024.12.1\n- Python version: 3.11\n- Operating System: WSL, Ubuntu\n- Install method (conda, pip, source): poetry",
    "comments": []
  },
  {
    "issue_number": 11831,
    "title": "Track unknown shapes",
    "author": "crusaderky",
    "state": "open",
    "created_at": "2025-03-18T11:39:57Z",
    "updated_at": "2025-04-21T02:09:55Z",
    "labels": [
      "array",
      "discussion",
      "needs attention",
      "p2",
      "feature"
    ],
    "body": "I would like to suggest a new feature that would make it possible to run `a[mask] = b[mask]`, where a, b, and mask are all Dask arrays.\n\nThis is currently failing and it's a limitation that hurts Dask usability severely, particularly when it's meant to be use interchangeably with other array backends. Namely, there is ongoing work in scipy to make this happen and I'm routinely hitting this issue. Functions in array_api_extra `apply_where` and `at` contain sophisticated workarounds just for this Dask limitation.\n\nThe idea is to start tracking unknown shapes with a signature. `dask.array.Array.shape` would change from a tuple to a tuple subclass, with an extra attribute `key: tuple[int | str, ...]`. `key` will be the same as the shape whenever the shape is fully defined, e.g. all dimensions are `int`.\n\nWhenever an operation creates or changes a dimension to become unknown, e.g. `__getitem__` with a dask boolean mask, or `unique`, it populates the key on the matching dimension `tokenize`. Whenever another function alters  the unknown dimension, it uses `tokenize(new function, previous key)` to update the key.\n\nThis would allow `a[mask] = b[mask]` to no longer test `self.shape == other.shape` (where `self = a[mask]` and `other = b[mask]`, but it would instead test `self.shape.key == other.shape.key`.\n\nLikewise, many other operations like `da.broadcast_arrays` etc. would be allowed to work with NaN shapes.\nBecause the keys are contained in the shape itself and not in the array, `da.broadcast_shapes` would benefit from it as well.\n\n## Out of scope\nIt is in theory possible to observe that some chains of operations will produce the same shape. For example, `da.unique(a)` and `da.unique(a + 1)` produce the same shape, but `da.unique(da.abs(a))` doesn't. \nThis would be very complicated logic for a relatively niche benefit, so I suggest to skip it.\n\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "We've done a similar thing with DataFrames where we're tracking shuffle operations. That's much easier to do once the expression system is live"
      }
    ]
  },
  {
    "issue_number": 11833,
    "title": "FutureCancelledError when applying map_partitions on un-aligned dataframes",
    "author": "mlemainque",
    "state": "open",
    "created_at": "2025-03-18T17:03:36Z",
    "updated_at": "2025-04-21T02:09:52Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nThe new implementation of `map_partitions` does not have the same behavior as the original one when dealing with multiple un-aligned dataframes.\n\nI understand that this is not implemented yet (see [here](https://github.com/dask/dask/blob/df372bdcdac8e7a7ba7416501bec4dff85e2b650/dask/dataframe/dask_expr/_collection.py#L6172)), but in the meantime how can we manually make sure partitions are aligned even when dataframes are not sharing the same index values at all ? I tried to manually use `expr.MapPartitions` with `align_dataframes=True` but it does not work either\n\n**Minimal Complete Verifiable Example**:\n\nThe below code works fine with `dask 2024.2.0` but does not with `dask 2025.2.0`.\n\n```python\nimport pandas as pd\nimport dask.dataframe as dd\nimport distributed\n\nwith distributed.LocalCluster() as cluster:\n  with distributed.Client(cluster) as client:\n\n    df = pd.DataFrame(range(100), columns=[\"col\"]).set_index(\"col\", drop=False)\n\n    # Build two dataframes with disjointed partitions\n    ddf1 = dd.from_pandas(df[df[\"col\"] % 2 == 0], npartitions=100)\n    ddf2 = dd.from_pandas(df[df[\"col\"] % 3 == 0], npartitions=100)\n\n    def mul(df1, df2):\n      return df1.assign(res=df1[\"col\"] * df2[\"col\"])\n\n    ddf = ddf1.map_partitions(mul, ddf2)\n    print(client.compute(ddf).result())\n```\n\nIt will raise the below exception which is unexpected:\n\n```\nFutureCancelledError: finalize-xxxxx cancelled for reason: unknown.\n```\n\nNote that by reducing the number of partitions, it will end up working probably because there is no need to prune partitions anymore...\n\n**Anything else we need to know?**:\n\nI need to zip & map partitions of multiple big dataframes (100M rows and 1k partitions) to apply some complex pandas logic: a merge is not what I'm looking for.\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: 3.11.11\n- Operating System: Linux (google colab)\n- Install method (conda, pip, source): pip\n",
    "comments": []
  },
  {
    "issue_number": 3641,
    "title": "Investigate using da.pad in da.overlap",
    "author": "jakirkham",
    "state": "open",
    "created_at": "2018-06-19T18:57:09Z",
    "updated_at": "2025-04-19T02:23:18Z",
    "labels": [
      "array"
    ],
    "body": "The `da.overlap` module was written to handle `map_overlap` and all of its intricacies. It provides some support for adding padding to edge chunks. Though it only supports a few modes of padding, but not nearly as many as `da.pad` supports. Given that `da.pad` now exists, it makes sense to just use this for performing padding in `da.overlap` to provide these same options to `map_overlap`. Also should help simplify some of the code in `da.overlap`.",
    "comments": [
      {
        "user": "jakirkham",
        "body": "As a side note, `map_overlap` allows handling different boundaries with different modes of padding (e.g. reflection on one axis and constant on another). Not aware of any use cases where this is needed. We may consider deprecating this functionality to make this transition simpler."
      },
      {
        "user": "jakirkham",
        "body": "Any thoughts on this, @mrocklin?"
      },
      {
        "user": "mrocklin",
        "body": "I suspect that the modes used were built to support scikit-image, but I don't recall exactly."
      }
    ]
  },
  {
    "issue_number": 8570,
    "title": "Culling massive Blockwise graphs is very slow, not constant-time",
    "author": "gjoseph92",
    "state": "open",
    "created_at": "2022-01-14T18:03:17Z",
    "updated_at": "2025-04-17T15:57:11Z",
    "labels": [
      "highlevelgraph",
      "needs attention"
    ],
    "body": "**tl;dr: Could `get_all_external_keys` and `get_output_keys` be avoided in HLG culling?**\r\n\r\nIn some workflows, it can be desirable to create a dask Array/DataFrame structure representing some full-size, enormous dataset, then immediately use slicing to sub-select out only a tiny part of it, then work with that. Now that we can use Blockwise for IO (xref https://github.com/dask/dask/pull/7417), this is an especially appealing pattern, because it should be constant-time to construct the massive graph, since nothing has to be materialized.\r\n\r\nI had hoped it would also be linear-time to cull this massive graph, but it appears currently that it's not.\r\n\r\nHere's an example where I'm trying to create an xarray representing the [Landsat-8](https://planetarycomputer.microsoft.com/dataset/landsat-8-c2-l2) collection at full resolution over the entire continental US. This is a 10PB, 136-million-chunk array that involves 1.3 billion data loading tasks. Here I'm using https://github.com/gjoseph92/stackstac/pull/116 (so the data loading graph is fully blockwise) and https://github.com/dask/dask/pull/8560 (so we know `fuse_roots` isn't materializing the graph unnecessarily).\r\n\r\n![Screen Shot 2022-01-14 at 10 37 54 AM](https://user-images.githubusercontent.com/3309802/149560244-8b8bec05-f3e5-4427-a079-3714677e77a7.png)\r\n\r\nThen I'm sub-selecting a single chunk out of those 136 million. Based on what I know of the graph, this should cull down to 4 tasks.\r\n\r\n![Screen Shot 2022-01-14 at 10 37 59 AM](https://user-images.githubusercontent.com/3309802/149560279-e40a3d55-aedf-449b-b931-a9f2fd2a1645.png)\r\n\r\n<details><summary>Here's the HLG for reference</summary>\r\n\r\nYou can see the first layer is materialized with ~100,000 tasks, but the _big_, 100-million-task one is Blockwise. (https://github.com/dask/dask/issues/8497 sure would be nice here!)\r\n\r\n![Screen Shot 2022-01-14 at 10 30 55 AM](https://user-images.githubusercontent.com/3309802/149559272-8f5e0f13-863e-49ac-8482-5876cd2b9617.png)\r\n\r\n</details>\r\n\r\nBut when I try to optimize this graph, I see memory usage shoot up until it crashes the kernel on the 32GB machine. Interrupting the kernel after a few seconds makes it pretty clear what's going on: `HighLevelGraph.cull` is calling `get_all_external_keys`, which is forcing the generation of all 1.3 billion keys (or 136 million keys? not sure).\r\n\r\n![Screen Shot 2022-01-14 at 10 23 19 AM](https://user-images.githubusercontent.com/3309802/149558357-d981f103-cea9-44bb-b44e-ed7070003b77.png)\r\n\r\nEven if it didn't call `get_all_external_keys`, I see that `HighLevelGraph.cull` is still calling `get_output_keys` on every layer. For reference: https://github.com/dask/dask/blob/358a5e367eedc8f2963651071a72f43e0ac4f887/dask/highlevelgraph.py#L944-L970\r\n\r\nWhy is it necessary for HLG.cull to ask the layer for all its keys, intersect them itself, then pass that back into the layer? And why is it necessary for cull functions to take `all_hlg_keys`?\r\n\r\nI had thought the interface would be simply: HLG.cull tells each layer the necessary output keys; the layer figures out the rest on its own. If it needs to generate all its keys internally and do that intersection, fine, but for layers that don't need to do this, shouldn't the optimization be available?\r\n\r\n@rjzamora @madsbk why does culling work this way? Would it be possible to write Blockwise culling without this, in a way that's truly linear-time to only the number of final keys?\r\n\r\ncc @ian-r-rose @TomAugspurger ",
    "comments": [
      {
        "user": "madsbk",
        "body": "> Why is it necessary for HLG.cull to ask the layer for all its keys, intersect them itself, then pass that back into the layer? \r\n\r\nI don't think this is necessary, we did it to make the implementation of `layer.cull()` simpler. \r\n\r\n> And why is it necessary for cull functions to take `all_hlg_keys`?\r\n\r\nThe cull functions use `all_hlg_keys` to identify keys.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "gjoseph92",
        "body": "So would it be possible to rewrite the culling process to not require any layers to generate all their keys?"
      },
      {
        "user": "martindurant",
        "body": "Totally agree with @gjoseph92 , I was under the impression that there were two major benefits of HLGs in general:\r\n- smaller transfer size, since we do something similar to using a generator/comprehension rather than a concrete set of keys and tasks\r\n- that you can select/update from a large graph at no cost, effectively changing the start/end of the comprehension implied in the first bullet.\r\n- (possibilities for higher-level optimisations)\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 11879,
    "title": "`xarray` store disables blockwise fusion if `compute=False`",
    "author": "fjetter",
    "state": "closed",
    "created_at": "2025-04-09T09:33:47Z",
    "updated_at": "2025-04-16T08:16:12Z",
    "labels": [
      "needs triage"
    ],
    "body": "https://github.com/dask/dask/pull/11844 introduced a regression for xarray users for all `store` APIs if `compute=False`. \n\n`da.store` previously called optimize explicitly on the input arrays causing them to be optimized every single time regardless of what would happen next.\nHowever, if `compute=False` xarray is wrapping the result of da.store again in a delayed object (see [here](https://github.com/pydata/xarray/blob/dd446d7d9c5f208cedc18b4b02fcf380a5ba7217/xarray/backends/api.py#L2249)) which effectively disables optimization.\n\n**This change has not been released yet. This will be considered a release blocker.**\n\nReproducer\n\n```python\n\n@pytest.mark.parametrize(\"compute\", [True, False])\ndef test_xarray_blockwise_fusion_store(compute):\n    def custom_scheduler_get(dsk, keys, expected, **kwargs):\n        dsk = dsk.__dask_graph__()\n        assert (\n            len(dsk) == expected\n        ), f\"False number of tasks got {len(dsk)} but expected {expected}\"\n        return [42 for _ in keys]\n\n    # First test that this mocking stuff works as expecged\n    with pytest.raises(AssertionError, match=\"False number of tasks\"):\n        scheduler = partial(custom_scheduler_get, expected=42)\n        dask.compute(da.ones(10), scheduler=scheduler)\n\n    coord = da.arange(8, chunks=-1)\n    data = da.random.random((8, 8), chunks=-1) + 1\n    x = xr.DataArray(data, coords={\"x\": coord, \"y\": coord}, dims=[\"x\", \"y\"])\n\n    y = ((x + 1) * 2) / 2 - 1\n\n    # Everything fused into one compute task\n    # one finalize Alias\n    expected = 2\n    scheduler = partial(custom_scheduler_get, expected=expected)\n    dask.compute(y, scheduler=scheduler)\n\n    with tmpdir() as dirname:\n        if compute:\n            with dask.config.set(scheduler=scheduler):\n                y.to_zarr(dirname, compute=True)\n        else:\n            # There's a delayed finalize store smashed on top which won't be fused by\n            # default\n            expected += 1\n            scheduler = partial(custom_scheduler_get, expected=expected)\n            stored = y.to_zarr(dirname, compute=False)\n            dask.compute(stored, scheduler=scheduler)\n\n\n```\n\nthis is somewhat related to https://github.com/dask/dask/issues/11854\n\n\nI will attempt to address this by changing the delayed interface such that optimizations are indeed called on all collections that are passed to the delayed object. This would be consistent to how we're handling `Expr` objects already and I don't see an obvious reason why this should be any different for traditional HLG objects.\nIf that fix fails, I will contribute something to xarray upstream to ensure they will be able to cope with this appropriately.\n\ncc @dcherian for visibility",
    "comments": []
  },
  {
    "issue_number": 2812,
    "title": "List optional cytoolz dependency",
    "author": "jakirkham",
    "state": "open",
    "created_at": "2017-10-23T19:47:44Z",
    "updated_at": "2025-04-16T06:24:29Z",
    "labels": [
      "good first issue"
    ],
    "body": "Would be nice if somehow `cytoolz` was noted as an optional dependency in `setup.py`. Not totally sure the right place to add it though.",
    "comments": [
      {
        "user": "mrocklin",
        "body": "Any update here @jakirkham ?  It looks like this is stale.  Should we close it?"
      },
      {
        "user": "dhirschfeld",
        "body": "As someone responsible for specc'ing up and deploying an analytics environment I think this is a pretty important issue. If `cytoolz` isn't in your environment your code will just run slower... and you'll never know that it could be faster \r\n\r\nTh problem for me is discoverability - the default install isn't the fastest and it's hard for a non-developer to know what they need to install if they want the best performance.\r\n\r\nThat said, it doesn't affect me since I use `conda` and both [AnacondaRecipes](https://github.com/AnacondaRecipes/dask-feedstock/blob/master/recipe/meta.yaml#L20) and [conda-forge](https://github.com/conda-forge/dask-feedstock/blob/master/recipe/meta.yaml#L19) have a hard-dependency on `cytoolz`.\r\n\r\nIf `cytoolz` isn't a hard dependency then what would be very useful is just and `optional-requirements.txt` file which would make it much more discoverable."
      },
      {
        "user": "dhirschfeld",
        "body": "...or just add a `'perf'` section to [`extra_requires`](https://github.com/dask/dask/blob/master/setup.py#L10-L17) - e.g.\r\n```python\r\nextras_require = {\r\n  'array': ['numpy >= 1.11.0', 'toolz >= 0.7.3'],\r\n  'bag': ['cloudpickle >= 0.2.1', 'toolz >= 0.7.3', 'partd >= 0.3.8'],\r\n  'dataframe': ['numpy >= 1.11.0', 'pandas >= 0.19.0', 'toolz >= 0.7.3',\r\n                'partd >= 0.3.8', 'cloudpickle >= 0.2.1'],\r\n  'distributed': ['distributed >= 1.22'],\r\n  'delayed': ['toolz >= 0.7.3'],\r\n  'perf': ['cytoolz >= 0.7.3'],\r\n}\r\n```"
      }
    ]
  },
  {
    "issue_number": 11838,
    "title": "The link in the readme doesnt go to the documentation",
    "author": "a-holm",
    "state": "open",
    "created_at": "2025-03-21T10:02:52Z",
    "updated_at": "2025-04-14T19:13:58Z",
    "labels": [
      "documentation"
    ],
    "body": "It should go here: https://docs.dask.org/en/stable/ since that is the documentation.",
    "comments": [
      {
        "user": "jrbourbeau",
        "body": "Thanks @a-holm -- that makes sense to me. Would you like to open a PR updating the link? "
      },
      {
        "user": "a-holm",
        "body": "> Thanks [@a-holm](https://github.com/a-holm) -- that makes sense to me. Would you like to open a PR updating the link?\n\nYes I will"
      },
      {
        "user": "a-holm",
        "body": "@jrbourbeau I did some other changes as well, included what python versions are supported, I had to look a long time to find it, so probably someone else who also find use for it."
      }
    ]
  },
  {
    "issue_number": 11885,
    "title": "`AttributeError: '_ExprSequence' object has no attribute '_projection_columns'` in `persist`",
    "author": "TomAugspurger",
    "state": "closed",
    "created_at": "2025-04-11T20:38:18Z",
    "updated_at": "2025-04-14T14:19:03Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nWhile simplifying the expression graph, we hit an `AttributeError` in `determine_column_projection` when persisting multiple dataframes / series where the columns overlap:\n\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport pandas as pd\nimport dask.dataframe as dd\n\ndf = pd.DataFrame({\"A\": [1, 2, 3, 4], \"B\": [0, 0, 1, 1]})\nddf = dd.from_pandas(df, npartitions=2)\n\nX = ddf[[\"A\", \"B\"]]\ny = ddf[[\"B\"]]\n\ndask.persist(X, y)\n```\n\nThat raises with\n\n```pytb\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py:796](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py#line=795), in Expr.__getattr__(self, key)\n    795 try:\n--> 796     return object.__getattribute__(self, key)\n    797 except AttributeError as err:\n\nAttributeError: '_ExprSequence' object has no attribute '_projection_columns'\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\nCell In[9], line 10\n      7 X = ddf[[\"A\", \"B\"]]\n      8 y = ddf[[\"B\"]]\n---> 10 dask.persist(X, y)\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/base.py:1012](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/base.py#line=1011), in persist(traverse, optimize_graph, scheduler, *args, **kwargs)\n   1009                 return repack(results)\n   1011 expr = collections_to_expr(collections, optimize_graph)\n-> 1012 expr = expr.optimize()\n   1013 keys, postpersists = [], []\n   1014 for a in collections:\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py:414](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py#line=413), in Expr.optimize(self, fuse)\n    411 def optimize(self, fuse: bool = False) -> Expr:\n    412     stage: OptimizerStage = \"fused\" if fuse else \"simplified-physical\"\n--> 414     return optimize_until(self, stage)\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py:894](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py#line=893), in optimize_until(expr, stage)\n    891     return result\n    893 # Simplify\n--> 894 expr = result.simplify()\n    895 if stage == \"simplified-logical\":\n    896     return expr\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py:424](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py#line=423), in Expr.simplify(self)\n    422 while True:\n    423     dependents = collect_dependents(expr)\n--> 424     new = expr.simplify_once(dependents=dependents, simplified={})\n    425     if new._name == expr._name:\n    426         break\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py:394](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py#line=393), in Expr.simplify_once(self, dependents, simplified)\n    391 if isinstance(operand, Expr):\n    392     # Bandaid for now, waiting for Singleton\n    393     dependents[operand._name].append(weakref.ref(expr))\n--> 394     new = operand.simplify_once(\n    395         dependents=dependents, simplified=simplified\n    396     )\n    397     simplified[operand._name] = new\n    398     if new._name != operand._name:\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py:377](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py#line=376), in Expr.simplify_once(self, dependents, simplified)\n    375 # Allow children to simplify their parents\n    376 for child in expr.dependencies():\n--> 377     out = child._simplify_up(expr, dependents)\n    378     if out is None:\n    379         out = expr\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/io/io.py:497](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/io/io.py#line=496), in FromPandas._simplify_up(self, parent, dependents)\n    494         return Literal(sum(_lengths))\n    496 if isinstance(parent, Projection):\n--> 497     return super()._simplify_up(parent, dependents)\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/io/io.py:82](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/io/io.py#line=81), in BlockwiseIO._simplify_up(self, parent, dependents)\n     75 if (\n     76     self._absorb_projections\n     77     and isinstance(parent, Projection)\n     78     and is_dataframe_like(self._meta)\n     79 ):\n     80     # Column projection\n     81     parent_columns = parent.operand(\"columns\")\n---> 82     proposed_columns = determine_column_projection(self, parent, dependents)\n     83     proposed_columns = _convert_to_list(proposed_columns)\n     84     proposed_columns = [col for col in self.columns if col in proposed_columns]\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py:3820](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py#line=3819), in determine_column_projection(expr, parent, dependents, additional_columns)\n   3817         continue\n   3818     seen.add(p._name)\n-> 3820     column_union.extend(p._projection_columns)\n   3822 if additional_columns is not None:\n   3823     column_union.extend(flatten(additional_columns, container=list))\n\nFile [~/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py:814](http://localhost:8888/home/nfs/gh/rapidsai/dask-upstream-testing/.venv/lib/python3.12/site-packages/dask/_expr.py#line=813), in Expr.__getattr__(self, key)\n    811     idx = _parameters.index(key)\n    812     return self.operands[idx]\n--> 814 raise AttributeError(\n    815     f\"{err}\\n\\n\"\n    816     \"This often means that you are attempting to use an unsupported \"\n    817     f\"API function..\"\n    818 )\n\nAttributeError: '_ExprSequence' object has no attribute '_projection_columns'\n\nThis often means that you are attempting to use an unsupported API function..\n```\n\nAnd a few values there:\n\n```\nipdb>  pp type(p)\n<class 'dask._expr._ExprSequence'>\nipdb>  pp column_union\n['B']\nipdb>  pp p._name\n'_exprsequence-ed2ba73b3c54379215ddeccaf6bf0cc0'\n```\n\nI haven't looked too closely\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: `2025.3.0+19.g0fa5e18d5`\n- Python version:\n- Operating System:\n- Install method (conda, pip, source):\n",
    "comments": []
  },
  {
    "issue_number": 11798,
    "title": "topk: silently ignores additional axes and reports incorrect shape when axis tuple is provided",
    "author": "Huite",
    "state": "open",
    "created_at": "2025-02-27T19:44:52Z",
    "updated_at": "2025-04-14T02:08:37Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\n`topk` accepts tuples for the axis arugment. This suggests to me that it'll handle multiple axes. However, internally, it takes the first member of the tuple and discards the other axes:\n\nhttps://github.com/dask/dask/blob/2109bb767fad41f1f4c93055c00dc3cce78d9a73/dask/array/chunk.py#L173\n\nThis also seems to result in an inconsistency between the dask array and the computed numpy array.\n\nThe docstring is clear though, it only mentions an integer axis argument: https://docs.dask.org/en/latest/generated/dask.array.topk.html\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\n\na = da.ones((10, 20))\nb = da.topk(a, k=1, axis=(0, 1))\nprint(b.shape)  # prints (1, 1)\nprint(b.compute().shape)  #  prints (1, 20)!\n```\n\nI would argue that both shapes are wrong; the `topk` example above should return an array of shape `(1,)` in this case. As a sanity check, I checked the behavior of the quantile function, since it's conceptually similar with `q` determining the number of return entries instead of topk's `k` argument. It does accept tuples (but it doesn't support aggregation over chunks):\n\n```python\na = da.ones((10, 20))\nb = da.quantile(a, q=[1], axis=(0, 1))\nprint(b.shape)  # prints (1,)\nprint(b.compute().shape)  # prints (1,)\n```\n\n(There's another subtlety in that a scalar `q=1` results in a scalar return value for `quantile`, but I think I'd prefer to keep a 1D array for `k=1` in `topk` and `argtopk`.)\n\n`argtopk` simply fails when a tuple is provided:\n\n```python\na = da.ones((10, 20))\nb = da.argtopk(a, k=2, axis=(0, 1))\n```\n\nWith:\n\n```python\nFile z:\\src\\xarray\\.pixi\\envs\\complete\\Lib\\site-packages\\dask\\array\\reductions.py:1753, in argtopk(a, k, axis, split_every)\n   1750 axis = validate_axis(axis, a.ndim)\n   1752 # Generate nodes where every chunk is a tuple of (a, original index of a)\n-> 1753 idx = arange(a.shape[axis], chunks=(a.chunks[axis],), dtype=np.intp)\n   1754 idx = idx[tuple(slice(None) if i == axis else np.newaxis for i in range(a.ndim))]\n   1755 a_plus_idx = a.map_blocks(chunk.argtopk_preprocess, idx, dtype=object)\n\nTypeError: tuple indices must be integers or slices, not tuple\n```\n\nWhich is more in line with my expectations, and the error is clear, but that this specific error is raised seems a happy accident rather than a conscious choice.\n\nOne could also argue that multiple axes support would be convenient (but probably complicated in combination with chunking).\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: 3.12.3\n- Operating System: Windows\n- Install method (conda, pip, source): conda\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Would you be interested in opening a PR?"
      },
      {
        "user": "Huite",
        "body": "Sure; I think we'd want a TypeError along the lines of `TypeError: axis must be integer, not {type(axis).__name__}`?"
      },
      {
        "user": "phofl",
        "body": "Yep, that sounds good to me\r\n\r\nOn Wed, Mar 5, 2025 at 2:22 PM Huite ***@***.***> wrote:\r\n\r\n> Sure; I think we'd want a TypeError along the lines of TypeError: axis\r\n> must be integer, not {type(axis).__name__}?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/dask/dask/issues/11798#issuecomment-2700918319>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOYQZGDN7JCS5EQWKNMUVST2S33C5AVCNFSM6AAAAABYAVCO2WVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDOMBQHEYTQMZRHE>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n> [image: Huite]*Huite* left a comment (dask/dask#11798)\r\n> <https://github.com/dask/dask/issues/11798#issuecomment-2700918319>\r\n>\r\n> Sure; I think we'd want a TypeError along the lines of TypeError: axis\r\n> must be integer, not {type(axis).__name__}?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/dask/dask/issues/11798#issuecomment-2700918319>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AOYQZGDN7JCS5EQWKNMUVST2S33C5AVCNFSM6AAAAABYAVCO2WVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDOMBQHEYTQMZRHE>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      }
    ]
  },
  {
    "issue_number": 11800,
    "title": "`Array.__setitem__` fails on size 0",
    "author": "crusaderky",
    "state": "open",
    "created_at": "2025-02-28T16:52:57Z",
    "updated_at": "2025-04-14T02:08:35Z",
    "labels": [
      "array",
      "needs attention",
      "bug"
    ],
    "body": "As of Dask 2025.2.0:\n```python\n>>> import dask.array as da\n>>> a = da.zeros((0, 2))\n>>> b = da.ones((0, 2))\n>>> a[:, :] = b\nValueError: shape mismatch: value array of shape (0, 2) could not be broadcast to indexing result of shape (0, 2)\n```\n### Impact\nThis edge case by itself is unimportant; however it has a magnified impact in `array_api_tests` as it forces *all* tests for `__setitem__` to be XFAILed due to how hypothesis testing works.",
    "comments": []
  },
  {
    "issue_number": 11806,
    "title": "groupby-transform-cumsum returns results which look wrong",
    "author": "MarcoGorelli",
    "state": "open",
    "created_at": "2025-03-04T12:17:05Z",
    "updated_at": "2025-04-14T02:08:34Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nimport narwhals as nw\ndf = dd.from_pandas(\n    pd.DataFrame(\n        {\n            'a': ['a', 'a', 'b', 'b', 'b'],\n            'b': [1,2,3,5,3],\n            'c': [5,4,3,2,1],\n            'g': [1,1,1,1,1],\n            'i': [0,1,2,3,4],\n        }\n    ), npartitions=2\n).sort_values('c')\nprint(df.compute())\nprint(df.assign(b_cumsum = df.groupby('g')['b'].transform('cumsum')).compute().sort_values('c'))\n```\nThis prints:\n```\n   a  b  c  g  i\n4  b  3  1  1  4\n3  b  5  2  1  3\n2  b  3  3  1  2\n1  a  2  4  1  1\n0  a  1  5  1  0\n/home/marcogorelli/polars-api-compat-dev/t.py:38: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n  Before: .apply(func)\n  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n  or:     .apply(func, meta=('x', 'f8'))            for series result\n  print(df.assign(b_cumsum = df.groupby('g')['b'].transform('cumsum')).compute().sort_values('c'))\n   a  b  c  g  i  b_cumsum\n4  b  3  1  1  4         9\n3  b  5  2  1  3        14\n2  b  3  3  1  2         3\n1  a  2  4  1  1         5\n0  a  1  5  1  0         6\n```\n\nAside from the unclear warning, I'd have expected the output to be\n```\n   a  b  c  g  i  b_cumsum\n4  b  3  1  1  4         3\n3  b  5  2  1  3         8\n2  b  3  3  1  2         11\n1  a  2  4  1  1         14\n0  a  1  5  1  0         15\n```\nOr am I misunderstanding how `transform('cumsum')` works in dask?\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.02\n- Python version: 3.12\n- Operating System: linux\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Thanks! This is a very annoying case that I am not sure we do much about unfortunately. The transform call triggers a shuffle under the hood and the default shuffle algorithm if you don't have a dedicated Dask cluster running is not preserving the order of the DataFrame when shuffling (very annoying)\n\nMaybe using the tasks based algorithm by default if no cluster is around?\n\nadding \n\n```\nfrom distributed import Client\n\nclient = Client()\n```\n\nwill solve that problem fwiw"
      },
      {
        "user": "phofl",
        "body": "We might change the default shuffle algorithm, that will get rid of that problem"
      }
    ]
  },
  {
    "issue_number": 11825,
    "title": "P2PShuffle RuntimeError P2P {id} failed during transfer phase when groupby apply to_bag",
    "author": "IsisChameleon",
    "state": "open",
    "created_at": "2025-03-12T03:20:19Z",
    "updated_at": "2025-04-14T02:08:28Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "\n**Describe the issue**:\n\ngroupby.apply().to_bag() causes a Runtime error when using P2PShuffle (the default) with a distributed dask client.\n\nExpected:\n- when computing the bag, there is no error\nActual:\n- when computing the bag, \nThrows AssertionError on:    assert isinstance(barrier_task_spec, P2PBarrierTask)\n```\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 196, in _retrieve_spec\n    assert isinstance(barrier_task_spec, P2PBarrierTask)\n  ^^^^^^^^^^^^^^^^^\nAssertionError\n...\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_core.py\", line 531, in handle_transfer_errors\n    raise RuntimeError(f\"P2P {id} failed during transfer phase\") from e\n```\nTo unblock:\n- adding a persist() before the to_bag()\n- switch to Task shuffle\n\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.config\nimport dask.dataframe as dd\nimport pandas as pd\nfrom dask.distributed import Client\n\n\ndef main():\n    # ----------------------------------------------------------------\n    # Set up data\n    # ------------------------------------------------------------\n    simple_data = pd.DataFrame(\n        {\n            \"foo\": [\n                \"1\",\n                \"1\",\n                \"2\",\n                \"2\",\n            ],\n            \"bar\": [\"1\", \"2\", \"3\", \"4\"],\n        }\n    )\n\n    # ----------------------------------------------------------------\n    # Set up Dask\n    # ----------------------------------------------------------------\n\n    dask.config.set(scheduler=\"distributed\")\n    # dask.config.set({\"dataframe.shuffle.method\": \"tasks\"}) # adding this line will also fix the exception\n    client = Client()\n\n    simple_ddf = dd.from_pandas(simple_data)\n\n    # ----------------------------------------------------------------\n    # groupby()  then apply() then to_bag()\n    # ----------------------------------------------------------------\n\n    grouped_ddf = simple_ddf.groupby(\"foo\")\n\n    def worker(grouped_df):\n        print(f\"Name: {grouped_df.name}, content:\\n {grouped_df}\")\n        return object()\n\n    results_bag = (\n        grouped_ddf.apply(worker, meta=(\"result\", \"object\"))\n        # Add persist() to avoid the P2P Shuffle problem (or else use tasks shuffle)\n        # .persist()\n        .to_bag()\n    )\n    print(\"Results:\", results_bag.compute())\n\n    client.close()\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n**Anything else we need to know?**:\n\n**Script Expected behaviour:**\n. results_bag should compute\n\n**Actual Behaviour:**\nThrows AssertionError on:    assert isinstance(barrier_task_spec, P2PBarrierTask)\n\n**How to avoid the undesired behaviour:**\n(1) insert persist() before the to_bag()\nOR\n(2) change dask dataframe shuffle to Tasks dask.config.set({\"dataframe.shuffle.method\": \"tasks\"}) (edited) \n\"\"\"\n\n**Environment**:\n\n- Dask version:\ndask: Version: 2025.1.0\ndistributed: Version: 2025.1.0\n- Python version: Python 3.12.7\n- Operating System: Debian GNU/Linux 11 (bullseye)\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "IsisChameleon",
        "body": "Full stack trace from MCVE:\n```\n2025-03-12 03:29:45,159 - distributed.core - ERROR - Exception while handling op shuffle_get_or_create\nTraceback (most recent call last):\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 229, in get_or_create\n    run_spec = self._get(shuffle_id, worker)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 190, in _get\n    state = self.active_shuffles[id]\n            ~~~~~~~~~~~~~~~~~~~~^^^^\nKeyError: 'c5fe05526a8a8f3e888fab31fb453a54'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/core.py\", line 832, in _handle_comm\n    result = handler(**msg)\n             ^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 234, in get_or_create\n    run_spec = self._create(shuffle_id, key, worker)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 205, in _create\n    spec = self._retrieve_spec(shuffle_id)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 196, in _retrieve_spec\n    assert isinstance(barrier_task_spec, P2PBarrierTask)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n2025-03-12 03:29:45,237 - distributed.worker - ERROR - Compute Failed\nKey:       ('frompandas-operation-shuffle-transfer-c5fe05526a8a8f3e888fab31fb453a54', 0)\nState:     executing\nTask:  <Task ('frompandas-operation-shuffle-transfer-c5fe05526a8a8f3e888fab31fb453a54', 0) _execute_subgraph(...)>\nException: \"RuntimeError('P2P c5fe05526a8a8f3e888fab31fb453a54 failed during transfer phase')\"\nTraceback: '  File \"/home/vscode/.local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_shuffle.py\", line 548, in _shuffle_transfer\\n    return shuffle_transfer(\\n           ^^^^^^^^^^^^^^^^^\\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_shuffle.py\", line 56, in shuffle_transfer\\n    with handle_transfer_errors(id):\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 158, in __exit__\\n    self.gen.throw(value)\\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_core.py\", line 531, in handle_transfer_errors\\n    raise RuntimeError(f\"P2P {id} failed during transfer phase\") from e\\n'\n\nTraceback (most recent call last):\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 229, in get_or_create\n    run_spec = self._get(shuffle_id, worker)\n^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 190, in _get\n    state = self.active_shuffles[id]\n  ^^^^^^^^^^^^^^^^^\nKeyError: 'c5fe05526a8a8f3e888fab31fb453a54'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_core.py\", line 523, in handle_transfer_errors\n    yield\n  ^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_shuffle.py\", line 57, in shuffle_transfer\n    return get_worker_plugin().add_partition(\n  ^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_worker_plugin.py\", line 348, in add_partition\n    shuffle_run = self.get_or_create_shuffle(id)\n  ^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_worker_plugin.py\", line 411, in get_or_create_shuffle\n    return sync(\n  ^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/utils.py\", line 439, in sync\n    raise error\n  ^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/utils.py\", line 413, in f\n    result = yield future\n^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/tornado/gen.py\", line 766, in run\n    value = future.result()\n^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_worker_plugin.py\", line 145, in get_or_create\n    shuffle_run = await self._refresh(\n^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_worker_plugin.py\", line 222, in _refresh\n    result = await self._fetch(shuffle_id=shuffle_id, key=key)\n  ^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_worker_plugin.py\", line 190, in _fetch\n    response = await self._plugin.worker.scheduler.shuffle_get_or_create(\n^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/core.py\", line 1259, in send_recv_from_rpc\n    return await send_recv(comm=comm, op=key, **kwargs)\n^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/core.py\", line 1043, in send_recv\n    raise exc.with_traceback(tb)\n^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/core.py\", line 832, in _handle_comm\n    result = handler(**msg)\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 234, in get_or_create\n    run_spec = self._create(shuffle_id, key, worker)\n^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 205, in _create\n    spec = self._retrieve_spec(shuffle_id)\n  ^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_scheduler_plugin.py\", line 196, in _retrieve_spec\n    assert isinstance(barrier_task_spec, P2PBarrierTask)\n  ^^^^^^^^^^^^^^^^^\nAssertionError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/workspaces/operator/temp/test_scripts/test.py\", line 55, in <module>\n    main()\n  File \"/workspaces/operator/temp/test_scripts/test.py\", line 49, in main\n    print(\"Results:\", results_bag.compute())\n                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/dask/base.py\", line 374, in compute\n    (result,) = compute(self, traverse=False, **kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/dask/base.py\", line 662, in compute\n    results = schedule(dsk, keys, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_shuffle.py\", line 548, in _shuffle_transfer\n    return shuffle_transfer(\n      ^^^^^^^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_shuffle.py\", line 56, in shuffle_transfer\n    with handle_transfer_errors(id):\n      ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n^^^^^^^^^^^\n  File \"/home/vscode/.local/lib/python3.12/site-packages/distributed/shuffle/_core.py\", line 531, in handle_transfer_errors\n    raise RuntimeError(f\"P2P {id} failed during transfer phase\") from e\n  ^^^^^^^^^^^^^^^^^\nRuntimeError: P2P c5fe05526a8a8f3e888fab31fb453a54 failed during transfer phase\n```"
      }
    ]
  },
  {
    "issue_number": 11827,
    "title": "Optimize removes the divisions when only one division set",
    "author": "mscanlon-exos",
    "state": "open",
    "created_at": "2025-03-12T17:03:40Z",
    "updated_at": "2025-04-14T02:08:27Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "**Describe the issue**:\n When a dataframe has only one partition after set_index(), the optimize() function removes the divisions.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\nimport random\n\ndf = pd.DataFrame({'value1': random.choices(range(-20, 20), k=20), 'value2': random.choices(range(0, 50), k=20)})\ndf['instrument_id'] = 'a'\ndf = dd.from_pandas(df, npartitions=1)\n\nprint(df.set_index('instrument_id', divisions=('a','a')).divisions)\n# ('a', 'a')\nprint(df.set_index('instrument_id', divisions=('a','a')).optimize().divisions)\n# (None, None)\n\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version:3.10\n- Operating System: OSx\n- Install method (conda, pip, source): pip\n",
    "comments": []
  },
  {
    "issue_number": 11829,
    "title": "map_partitions: FrameBase as a keyword argument doesn't work",
    "author": "faulaire",
    "state": "open",
    "created_at": "2025-03-14T09:37:58Z",
    "updated_at": "2025-04-14T02:08:26Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nAccording to documentation of map_partitions, Arguments and keywords may contain FrameBase.\nIt doesn't work with keywords arguments.\n\nIn the provided example, type of df2 is dask.dataframe.dask_expr._collection.Series\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport pandas as pd\nfrom dask.dataframe import from_pandas\n\ndf = pd.Series([1., 2., 3.])\ndd = from_pandas(df, npartitions=2)\ndd_2 = from_pandas(df, npartitions=2)\n\ndef func(df, df2):\n    assert isinstance(df, pd.Series)\n    assert isinstance(df2, pd.Series)\n    return df\n\ndd_part = dd.map_partitions(func, df2=dd_2)\ndd_part.compute()\n```\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: 3.12.8\n- Operating System: RHEL7\n- Install method (conda, pip, source): pip\n",
    "comments": []
  },
  {
    "issue_number": 11882,
    "title": "IndexError for indexing Dask array with boolean numpy array",
    "author": "brendan-m-murphy",
    "state": "closed",
    "created_at": "2025-04-09T19:40:38Z",
    "updated_at": "2025-04-11T21:03:24Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nDropping NaNs from a `xr.Dataset` results in a `IndexError` from dask, from an out-of-bounds index in the last chunk. The original traceback is below.\n\nThe MCVE below only uses numpy and dask. In that case, the traceback is:\n\n```\nCell In[269], line 15\n     13 # random mask\n     14 mask = rng.binomial(1, 0.5, magic_number).astype(bool)\n---> 15 arr[mask].compute()  # raises ValueError from IndexError\n\nFile /user/work/bm13805/.inv_venv_sep2024/lib/python3.12/site-packages/dask/base.py:372, in DaskMethodsMixin.compute(self, **kwargs)\n    348 def compute(self, **kwargs):\n    349     \"\"\"Compute this dask collection\n    350 \n    351     This turns a lazy Dask collection into its in-memory equivalent.\n   (...)\n    370     dask.compute\n    371     \"\"\"\n--> 372     (result,) = compute(self, traverse=False, **kwargs)\n    373     return result\n\nFile /user/work/bm13805/.inv_venv_sep2024/lib/python3.12/site-packages/dask/base.py:660, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    657     postcomputes.append(x.__dask_postcompute__())\n    659 with shorten_traceback():\n--> 660     results = schedule(dsk, keys, **kwargs)\n    662 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile /user/work/bm13805/.inv_venv_sep2024/lib/python3.12/site-packages/dask/array/_shuffle.py:315, in _getitem(obj, index)\n    314 def _getitem(obj, index):\n--> 315     return getitem(obj, index[1])\n\nFile /user/work/bm13805/.inv_venv_sep2024/lib/python3.12/site-packages/dask/array/chunk.py:423, in getitem(obj, index)\n    421     result = obj[index]\n    422 except IndexError as e:\n--> 423     raise ValueError(\n    424         \"Array chunk size or shape is unknown. \"\n    425         \"Possible solution with x.compute_chunk_sizes()\"\n    426     ) from e\n    428 try:\n    429     if not result.flags.owndata and obj.size >= 2 * result.size:\n\nValueError: Array chunk size or shape is unknown. Possible solution with x.compute_chunk_sizes()\n```\n\n**Minimal Complete Verifiable Example**:\n\nThe \"magic numbers\" in this example seem important (I don't know why). The choice of rng seed isn't really important; if you try enough random masks with the \"magic\" array shape and chunk size, this error will occur.\n\n```python\nimport dask\nimport numpy as np\n\nmagic_number = 1692  #  from real data, not sure why this number causes problem\nmagic_chunk_size = 255  #  from real data, again, not sure of the significance, but e.g. 200 and 256 don't raise an error\n\narr = dask.array.from_array(np.ones(magic_number), chunks=magic_chunk_size)\n\n# make rng and advance to state where first random mask causes problems\nrng = np.random.default_rng(seed=123)\nrng.bit_generator.state = {'bit_generator': 'PCG64', 'state': {'state': 72993724299885113979891102826434028481, 'inc': 17686443629577124697969402389330893883}, 'has_uint32': 0, 'uinteger': 0}\n\n# random mask\nmask = rng.binomial(1, 0.5, magic_number).astype(bool)\narr[mask].compute()  # raises ValueError from IndexError\n```\n\n**Anything else we need to know?**:\n\nThis error arose when using `xr.Dataset.dropna`. It only occurs sporadically with some of the data we use. (There might be some kind of pattern, but it happens with at least two distinct datasets.)\n\nDebugging `xr.Dataset.dropna` gets as far as creating a boolean numpy array `mask`, then calls `ds.isel({dim: mask})`. I think this `isel` call actually results in `arr[(mask,)]` instead of `arr[mask]`, but the result is the same.\n\nThis happened in someone else's environment and I was able to reproduce it in mine. The MVCE is a simplified version of the data that caused their problem.\n\nThis is their original traceback:\n\n```\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nFile ~/.local/lib/python3.12/site-packages/dask/array/chunk.py:421, in getitem(obj, index)\n    420 try:\n--> 421     result = obj[index]\n    422 except IndexError as e:\n\nIndexError: index 229 is out of bounds for axis 1 with size 162\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nCell In[37], line 1\n----> 1 fp_data[\"ZEP\"][\"H\"].compute()\n\nFile /user/work/qq24644/miniconda3/envs/openghg_inv/lib/python3.12/site-packages/xarray/core/dataarray.py:1206, in DataArray.compute(self, **kwargs)\n   1181 \"\"\"Manually trigger loading of this array's data from disk or a\n   1182 remote source into memory and return a new array.\n   1183\n   (...)\n   1203 dask.compute\n   1204 \"\"\"\n   1205 new = self.copy(deep=False)\n-> 1206 return new.load(**kwargs)\n...\n    426     ) from e\n    428 try:\n    429     if not result.flags.owndata and obj.size >= 2 * result.size:\n\nValueError: Array chunk size or shape is unknown. Possible solution with x.compute_chunk_sizes()\n```\n\nIf you convert `mask` to a dask array, then it seems to work.\n\n**Environment**:\n\n- Dask version: 2024.12.0\n- Python version: 3.12.3\n- Operating System: Rocky Linux 8.9\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "brendan-m-murphy",
        "body": "Actually this seems to be fixed in dask 2025.1.0 (so possibly has the same cause as https://github.com/dask/dask/issues/11614)."
      }
    ]
  },
  {
    "issue_number": 11843,
    "title": "ImportError: cannot import name 'quote' from 'dask.base' (/opt/conda/lib/python3.11/site-packages/dask/base.py)",
    "author": "jwagemann",
    "state": "closed",
    "created_at": "2025-03-23T13:55:13Z",
    "updated_at": "2025-04-10T13:57:34Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\nIt seems that the latest dask version (`2025.3.0`) leads to an ImportError when importing the `odc-stac` library (version 0.3.11).\n\nThe error that is thrown is the following:\n`ImportError: cannot import name 'quote' from 'dask.base' (/opt/conda/lib/python3.11/site-packages/dask/base.py)`\n\nThe error disappeared when I downgraded dask to version 2025.2.0.\n\n\n**Environment**:\n\n- Dask version: 2025.3.0\n- Python version: 3.11\n- Operating System: MACOS\n- Install method (conda, pip, source): conda\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "`quote` is defined in `dask.core` (and has been for 10 years) and not `base`. I suspect that the `dask.base` import was only working accidentally due to an import that was now removed.\n\nI suggest to replace your imports. That should be forwards and backwards compatible"
      }
    ]
  },
  {
    "issue_number": 11883,
    "title": "Fancy indexing of a complex array emits a ComplexWarning",
    "author": "ev-br",
    "state": "open",
    "created_at": "2025-04-10T13:37:31Z",
    "updated_at": "2025-04-10T13:37:46Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nIndexing a complex-valued array with an integer array emits a numpy `ComplexWarning` even if the indexing result is correct.\n\n**Minimal Complete Verifiable Example**:\n\n```python\n\n>>> import dask.array as da\n>>> a = da.asarray([1.+2j, 2.+3j])\n>>> a[[1, 0]]      # no warning\ndask.array<getitem, shape=(2,), dtype=complex128, chunksize=(2,), chunktype=numpy.ndarray>\n>>> a[da.asarray([1, 0])]\na[da.asarray([1, 0])]    # warning\n/home/br/miniforge3/envs/scipy-dev/lib/python3.12/site-packages/dask/array/utils.py:104: ComplexWarning: Casting complex values to real discards the imaginary part\n  meta = meta.astype(dtype)\ndask.array<slice_with_int_dask_array_aggregate, shape=(2,), dtype=complex128, chunksize=(2,), chunktype=numpy.ndarray>\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.3.0\n- Python version: 3.12\n- Operating System: linux\n- Install method (conda, pip, source): pip-installed Dask in a conda env\n",
    "comments": []
  },
  {
    "issue_number": 11878,
    "title": "Failure with Function with Large Data Attributes with SLURM",
    "author": "SDykeUKHSA",
    "state": "open",
    "created_at": "2025-04-09T07:40:08Z",
    "updated_at": "2025-04-09T07:40:26Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:  I am trying to parallelise via SLURM, some existing code involving functions with large arrays of data embedded in them as attributes but my code is failing with a 'signal only works in main thread of the main interpreter' error. Problem seems to occurs only with data above a certain size and with the latest version of Jobqueue. \n\n**Minimal Complete Verifiable Example**:\n\n```python\n#!/usr/bin/env python\n# coding: utf-8\n\n# Minimial example of faliure of client when passed object with large attribute\nfrom dask.distributed import Client\nfrom dask_jobqueue import SLURMCluster\n\n# Define simple test class with one parameter which this is set as attribute, trivial\n# method and method running client and scattering method to it\nclass Test():\n    def __init__(self, x):\n        self.x = x\n\n    def wrapper_func(self, x):\n        return 3.4 \n\n    def run(self):  \n        # Setup SLURM cluster client      \n        cluster = SLURMCluster(queue='icelake',\n                        cores=1,                                   # cores per job\n                        memory='4 GiB',                            # mem per job\n                        processes=1,                               # how many processes\n                        local_directory='/tmp',                    # for file spilling\n                        walltime='10',                             # worker jobs walltime\n                        )\n        cluster.scale(jobs=4)\n        client = Client(cluster)\n\n        # Define trivial function to be applied to class method\n        def f(func):\n            return 1\n\n        # Scatter class method and apply function to it\n        wrapper = client.scatter(self.wrapper_func)\n        final = client.submit(f, wrapper).result()\n\n        return final\n\n# Try test on list\nlist_len = 5236000 # Fails with this length of list but not if set slightly lower e.g 5235000 \ntest_instance = Test([0 for i in range(list_len)])\nresult = test_instance.run()\nprint(f\"result with list of {list_len} =\", result)\n```\n\n**Anything else we need to know?**: Problem seems to be with version 0.90 of Jobqueue as does not appear to occur with version 0.8.5.\n\nError messages are:\n\n[error.txt](https://github.com/user-attachments/files/19662567/error.txt)\n\n**Environment**:\n\n- Dask version:  2035.3.0. \n- Dask Jobqueue version: 0.9.0\n- Python version: 3.13.2\n- Operating System: CentOS Linux 7 (Core)\n- Install method (conda, pip, source): micromamba\n",
    "comments": []
  },
  {
    "issue_number": 11874,
    "title": "`DataFrame.compute()` does not preserve columns name attribute",
    "author": "FBruzzesi",
    "state": "open",
    "created_at": "2025-04-08T16:45:05Z",
    "updated_at": "2025-04-08T22:03:20Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nTriggering a compute does not preserve the dataframe column index name\n\n```python\nimport pandas as pd\nimport dask.dataframe as dd\n\ndata = {\"a\": [1,2], \"b\": [3,4]}\ndf_pd = pd.DataFrame(data)\ndf_pd.columns.name = \"foo\"\n\ndf = dd.from_pandas(df_pd)\ndf.columns.name = \"bar\"\n\nprint(df.columns.name)\nbar\n\nprint(df.compute())\nfoo  a  b\n0    1  3\n1    2  4\n\n# expected output would be:\n# bar  a  b\n# 0    1  3\n# 1    2  4\n\n\ndf = dd.from_dict(data, npartitions=1)\ndf.columns.name = \"bar\"\n\nprint(df.columns.name)\nbar\n\nprint(df.compute())\n   a  b\n0  1  3\n1  2  4\n\n# expected output would be:\n# bar  a  b\n# 0    1  3\n# 1    2  4\n```\n**Additional info**:\n\nSpotted in Narwhals 😎\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: python 3.12\n- Operating System:  Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n- Install method (conda, pip, source): pip\n",
    "comments": []
  },
  {
    "issue_number": 11876,
    "title": "map_partitions: Allow specifying which columns are used for column projection",
    "author": "smcguire-cmu",
    "state": "open",
    "created_at": "2025-04-08T20:23:21Z",
    "updated_at": "2025-04-08T20:23:34Z",
    "labels": [
      "needs triage"
    ],
    "body": "As discussed in https://github.com/dask/community/issues/417#issuecomment-2776224755, it would be useful to allow specifying which columns are used by the ufunc passed to a `map_partitions` call. This would let the column projection in dask expressions to work through the map_partitions call instead of using all columns.\n\nAn example API of what this could look like:\n```\nddf.map_partitions(ufunc, required_columns=[\"col1\", \"col2\"])\n```\n\nFor an example of how this would be used, our use case is that we're building an astronomy framework on top of dask, and we'd like to define our own functions using map partitions that still allows our users to get the benefits of column projection.\n\nAn example in our use case where this would be helpful for our users:\n```\ncatalog.cone_search(ra, dec, radius)[\"magnitude\"].compute()\n```\nWhere we would define our `cone_search` function using map_partitions and specify which columns are needed.\n\nThank you!",
    "comments": []
  },
  {
    "issue_number": 11870,
    "title": "⚠️ Upstream CI failed ⚠️",
    "author": "github-actions[bot]",
    "state": "closed",
    "created_at": "2025-04-04T09:46:50Z",
    "updated_at": "2025-04-07T15:04:12Z",
    "labels": [
      "upstream"
    ],
    "body": "[Workflow Run URL](https://github.com/dask/dask/actions/runs/14298910258)\n<details><summary>Python 3.12 Test Summary</summary>\n\n```\ndask/dataframe/dask_expr/tests/test_reductions.py::test_cov_corr: [XPASS(strict)] https://github.com/dask/dask/issues/11858\ndask/dataframe/tests/test_dataframe.py::test_cov_corr_mixed[True]: [XPASS(strict)] https://github.com/dask/dask/issues/11858\n```\n\n</details>\n",
    "comments": []
  },
  {
    "issue_number": 11858,
    "title": "Unskip cov / corr tests with pandas 3.x",
    "author": "TomAugspurger",
    "state": "closed",
    "created_at": "2025-04-01T21:10:57Z",
    "updated_at": "2025-04-07T15:04:11Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\n\nIn https://github.com/dask/dask/pull/11857, we were addressing a CI failure on `main` with pandas-dev (3.x), flagged in https://github.com/dask/dask/issues/11852. It [looks like](https://github.com/pandas-dev/pandas/pull/61154#issuecomment-2770392415) pandas will need some changes, so for now we'll xfail these tests.\n\nThat PR will xfail the tests with pandas 3.x. This issue is to track un-xfailing those.",
    "comments": []
  },
  {
    "issue_number": 11641,
    "title": "order: combining different xarray variables followed by a reduction orders very inefficiently",
    "author": "phofl",
    "state": "open",
    "created_at": "2025-01-07T15:20:19Z",
    "updated_at": "2025-04-07T02:07:01Z",
    "labels": [
      "array",
      "needs attention",
      "dask-order"
    ],
    "body": "Lets look at the following example:\r\n\r\n```\r\nimport xarray as xr\r\nimport dask.array as da\r\n\r\nsize = 50\r\nds = xr.Dataset(\r\n    dict(\r\n        u=(\r\n            [\"time\", \"j\", \"i\"],\r\n            da.random.random((size, 20, 20), chunks=(10, -1, -1)),\r\n        ),\r\n        v=(\r\n            [\"time\", \"j\", \"i\"],\r\n            da.random.random((size, 20, 20), chunks=(10, -1, -1)),\r\n        ),\r\n        w=(\r\n            [\"time\", \"j\", \"i\"],\r\n            da.random.random((size, 20, 20), chunks=(10, -1, -1)),\r\n        ),\r\n    )\r\n)\r\n\r\nds[\"uv\"] = ds.u * ds.v\r\nds[\"vw\"] = ds.v * ds.w\r\n\r\nds = ds.fillna(199)\r\n```\r\n\r\nWe are combining u and v and then v and w. Not having a reduction after that step generally works fine:\r\n\r\n<img width=\"1321\" alt=\"Screenshot 2025-01-07 at 16 21 32\" src=\"https://github.com/user-attachments/assets/d72431d8-44bf-4bb1-b335-99480128c453\" />\r\n\r\n\r\nThe individual chunks in one array are independent of all other chunks, so we can process chunk by chunk for all data arrays. \r\n\r\nAdding a reduction after these cross dependencies makes things go sideways:\r\n\r\nAdd:\r\n\r\n```\r\nds = ds.count()\r\n```\r\n\r\n\r\nThe ordering algorithm eagerly processes a complete tree reduction for the first variable ``uv`` before touching anything from ``vw``. This means that the data array ``v`` is loaded completely into memory when the first tree reduction is finished before we are tackling the ``vw`` and thus we can't release any chunk from ``v``.\r\n\r\n\r\n<img width=\"1511\" alt=\"Screenshot 2025-01-07 at 16 21 12\" src=\"https://github.com/user-attachments/assets/977a78f1-a11c-4ecd-b468-392a2c7f9c98\" />\r\n\r\n\r\nI am not sure what a good solution here would look like. Ideally, the ordering algorithm would know that the ``v`` chunks are a lot larger than the reduced chunks of the ``uv`` combination and thus prefer processing ``v`` before starting with a new chunk of ``uv``.\r\n\r\nAlternatively, we could load ``v`` twice, i.e. drop the v chunks after they are added to ``uv``.\r\n\r\nThis is the pattern that kills https://github.com/coiled/benchmarks/blob/main/tests/geospatial/workloads/atmospheric_circulation.py\r\n\r\ntask graph:\r\n\r\n```\r\nfrom dask.base import collections_to_dsk\r\n\r\ndsk = collections_to_dsk([ds.uv.data, ds.vw.data], optimize_graph=True)\r\n```\r\n\r\ncc @fjetter ",
    "comments": [
      {
        "user": "dcherian",
        "body": "I thought this was fixed: https://github.com/pangeo-data/distributed-array-examples/issues/2 xref https://github.com/pydata/xarray/issues/6709"
      },
      {
        "user": "phofl",
        "body": "I think this is still true, but it only works for a single combined variable, as soon as you have 2 things go sideways :(\r\n\r\nthis is the ordering for your first example:\r\n\r\n<img width=\"1327\" alt=\"Screenshot 2025-01-07 at 16 36 14\" src=\"https://github.com/user-attachments/assets/2ec23c38-77d9-4b08-8e77-5491fe5101a4\" />\r\n\r\nAnd that looks as expected\r\n\r\n"
      },
      {
        "user": "fjetter",
        "body": "> I am not sure what a good solution here would look like. Ideally, the ordering algorithm would know that the v chunks are a lot larger than the reduced chunks of the uv combination and thus prefer processing v before starting with a new chunk of uv.\r\n\r\nA thing we could play with here is to set strategic targets (i.e. get_target) on intermediate nodes, e.g. instead of only considering leaf_nodes we may consider \"first reducers\" or other things. I don't know a straight forward way of controlling this (i.e. picking between this strategy and another one, maybe size of hull??). "
      }
    ]
  },
  {
    "issue_number": 11872,
    "title": "da.cumsum changes chunking of 1D array when not specifying axis",
    "author": "wokast",
    "state": "open",
    "created_at": "2025-04-06T13:19:06Z",
    "updated_at": "2025-04-06T13:19:17Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nWhen applying da.cumsum to a chunked one-dimensional array without specifying the axis, the chunk size is changed to values that can be as small as 2. For large arrays, such a small chunk size then results in extreme slowdowns when calling compute(), making cumsum effectively unusable. The problem did not occur when specifying `axis=0`.\n\nThe expected behavior would be that the chunk size is unchanged, and that calling cumsum on a 1D array with or without specifying the axis yields identical results. \n\n\n**Minimal Complete Verifiable Example**:\n\n```python\nz = da.zeros(10000, chunks=5000)\nprint(z.chunks, z.shape)\nprint(z)\n\nc = da.cumsum(z, axis=None)\nprint(c)\nprint(len(c.chunks[0]), np.min(c.chunks[0]), np.max(c.chunks[0]))\n\n\nc = da.cumsum(z, axis=0)\nprint(c)\nprint(len(c.chunks[0]), np.min(c.chunks[0]), np.max(c.chunks[0]))\n\n\nz = da.zeros(15000, chunks=5000)\nprint(z.chunks, z.shape)\nprint(z)\n\n\nc = da.cumsum(z, axis=None)\nprint(c)\nprint(len(c.chunks[0]), np.min(c.chunks[0]), np.max(c.chunks[0]))\n```\n\nOutput:\n\n```\n((5000, 5000),) (10000,)\ndask.array<zeros_like, shape=(10000,), dtype=float64, chunksize=(5000,), chunktype=numpy.ndarray>\ndask.array<cumsum, shape=(10000,), dtype=float64, chunksize=(2,), chunktype=numpy.ndarray>\n5000 2 2\ndask.array<cumsum, shape=(10000,), dtype=float64, chunksize=(5000,), chunktype=numpy.ndarray>\n2 5000 5000\n((5000, 5000, 5000),) (15000,)\ndask.array<zeros_like, shape=(15000,), dtype=float64, chunksize=(5000,), chunktype=numpy.ndarray>\ndask.array<cumsum, shape=(15000,), dtype=float64, chunksize=(3,), chunktype=numpy.ndarray>\n5000 3 3\n```\n\n\n**Anything else we need to know?**:\n\nIn the example above it seems like the faulty chunk size is the previous number of chunks, but I did not try to verify whether this the case in general. \n\n\n**Environment**:\n\n- Dask version: 2024.11.2\n- Python version: 3.13.2\n- Operating System: Linux (Fedora 41)\n- Install method (conda, pip, source): pip\n",
    "comments": []
  },
  {
    "issue_number": 8380,
    "title": "da.store loses dependency information",
    "author": "djhoese",
    "state": "closed",
    "created_at": "2021-11-15T18:10:00Z",
    "updated_at": "2025-04-04T09:21:12Z",
    "labels": [
      "array",
      "needs attention",
      "bug"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**What happened**:\r\n\r\nI'm trying to take the result from a `map_blocks` function and store one slice of the resulting array in one zarr array and another slice in another array. I set `compute=False` on the `da.store` calls so that I can compute them together later and avoid computing the array multiple times. However, it seems the map blocks function is called for each store operation. From what I can tell the dependency graph is getting lost.\r\n\r\n**What you expected to happen**:\r\n\r\nTwo `da.store` calls with shared tasks on their dask graphs should share computations when computed at the same time.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport dask\r\nimport dask.array as da\r\nimport numpy as np\r\n\r\nTOTAL_CALLS = 0\r\n\r\n\r\ndef shared_task2(arr1):\r\n    global TOTAL_CALLS\r\n    TOTAL_CALLS += 1\r\n    return np.stack([arr1 + 1, arr1 + 2])\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    start = da.zeros((2, 2), chunks=1)\r\n    src = da.map_blocks(shared_task2, start, dtype=start.dtype,\r\n                        meta=np.array((), dtype=start.dtype),\r\n                        new_axis=[0],\r\n                        chunks=(2,) + start.chunks)\r\n    target1 = np.zeros((2, 2))\r\n    target2 = np.zeros((2, 2))\r\n\r\n    with dask.config.set(schedulers='single-threaded'):\r\n        store_res1 = da.store(src[0], target1, compute=False)\r\n        store_res2 = da.store(src[1], target2, compute=False)\r\n        da.compute(store_res1, store_res2)\r\n    # one call per chunk\r\n    assert TOTAL_CALLS == start.blocks.size\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nAs mentioned above, I'm actually trying to use `to_zarr` to save some dask arrays. I want to write them to zarr and then get the resulting loaded zarr arrays as results, but I couldn't find a way to do that with the combination of `return_stored` and `compute=False` as you need to compute to write to the zarr arrays, but that then returns the resulting numpy arrays.\r\n\r\nAlso note that my map_blocks function is returning the `np.stack` because it is actually returning two results. \r\n\r\n**Environment**:\r\n\r\n- Dask version: 2021.11.1\r\n- Python version: 3.9\r\n- Operating System: Ubuntu/PopOS\r\n- Install method (conda, pip, source): conda-forge\r\n",
    "comments": [
      {
        "user": "djhoese",
        "body": "Also note this does not happen if the map_blocks function is a single result or if I don't do `src[0]` and instead store the entire 3D array twice. Something about the `__getitem__` task seems to be the problem."
      },
      {
        "user": "djhoese",
        "body": "Ok new find, if I call `da.store` with a list of sources and targets then it works as expected. So I guess the new question is how can I do `to_zarr` for multiple sources and targets?"
      },
      {
        "user": "djhoese",
        "body": "Found an ugly workaround, change the bottom portion of the script to turn fuse.active off:\r\n\r\n```python\r\n    with dask.config.set(schedulers='single-threaded'):\r\n        with dask.config.set({\"optimization.fuse.active\": False}):\r\n            store_res1 = da.store(src[0], target1, compute=False)\r\n            store_res2 = da.store(src[1], target2, compute=False)\r\n        da.compute(store_res1, store_res2)\r\n    # one call per chunk\r\n    assert TOTAL_CALLS == start.blocks.size\r\n```\r\n\r\nThis stops `da.store` from fusing the tasks and causing the later `da.compute` to not realize that the two sources are from the same dependencies."
      }
    ]
  },
  {
    "issue_number": 11862,
    "title": "tokenizing a pandas RangeIndex realizes the whole index in to memory",
    "author": "dcherian",
    "state": "closed",
    "created_at": "2025-04-02T14:21:06Z",
    "updated_at": "2025-04-03T11:04:56Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\ntokenizing a rangeindex seems to realize the whole vector in to memory. this is useless, the token should be determined by `start`, `stop`, and `step`\n\n**Minimal Complete Verifiable Example**:\n\nThis will grab as much memory as it can and then crash\n\n```python\nimport pandas as pd\n\ndask.base.tokenize(pd.RangeIndex(start=0, stop=14662360160, step=1))\n```\n\n**Anything else we need to know?**:\n\nSeems fixed by\n```python\n@normalize_token.register(pd.RangeIndex)\ndef normalize_range_index(x):\n    return normalize_token(type(x)), x.start, x.stop, x.step, x.dtype, x.name\n```\n\n**Environment**:\n\n- Dask version: 2025.3.0\n- Python version: 3.13\n- Operating System: macos\n- Install method (conda, pip, source): conda\n",
    "comments": []
  },
  {
    "issue_number": 11680,
    "title": "test_set_index_head_nlargest_string failure due to uncaught warning",
    "author": "QuLogic",
    "state": "closed",
    "created_at": "2025-01-19T02:53:41Z",
    "updated_at": "2025-04-03T05:33:48Z",
    "labels": [
      "tests",
      "needs attention",
      "dask-expr"
    ],
    "body": "**Describe the issue**:\nI am getting the following failure with 2025.1.0 due to a `UserWarning`:\n```pytb\n_____________________ test_set_index_head_nlargest_string ______________________\n[gw0] linux -- Python 3.13.1 /usr/bin/python3\n\npdf =      x   y    z\n0    0   0   a0\n1    1   1   a1\n2    2   2   a2\n3    3   3   a3\n4    4   4   a4\n..  ..  ..  ...\n95  15  95  a15\n96  16  96  a16\n97  17  97  a17\n98  18  98  a18\n99  19  99  a19\n\n[100 rows x 3 columns]\n\n    @pytest.mark.skipif(\n        not pyarrow_strings_enabled() or not PANDAS_GE_220,\n        reason=\"doesn't work without arrow\",\n    )\n    @xfail_gpu(\"cudf udf support\")\n    def test_set_index_head_nlargest_string(pdf):\n        pdf[\"z\"] = \"a\" + pdf.x.map(str)\n        df = from_pandas(pdf, npartitions=5)\n        print(df.dtypes)\n    \n        a = df.set_index(\"z\").head(10, compute=False)\n>       assert_eq(a, pdf.set_index(\"z\").sort_index().head(10))\n\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/dataframe/dask_expr/tests/test_shuffle.py:552: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/dataframe/dask_expr/tests/_util.py:44: in assert_eq\n    return dd_assert_eq(a, b, *args, **kwargs)\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/dataframe/utils.py:521: in assert_eq\n    assert_divisions(a, scheduler=scheduler)\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/dataframe/utils.py:599: in assert_divisions\n    results = get(ddf.dask, ddf.__dask_keys__())\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/local.py:562: in get_sync\n    return get_async(\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/local.py:505: in get_async\n    for key, res_info, failed in queue_get(queue).result():\n/usr/lib64/python3.13/concurrent/futures/_base.py:449: in result\n    return self.__get_result()\n/usr/lib64/python3.13/concurrent/futures/_base.py:401: in __get_result\n    raise self._exception\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/local.py:547: in submit\n    fut.set_result(fn(*args, **kwargs))\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/local.py:243: in batch_execute_tasks\n    return [execute_task(*a) for a in it]\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/local.py:234: in execute_task\n    result = pack_exception(e, dumps)\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/local.py:229: in execute_task\n    result = task(data)\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/_task_spec.py:741: in __call__\n    return self.func(*new_argspec)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ndf =     x   y\nz        \na0  0  80\na0  0  60\na0  0  20\na0  0   0\na0  0  40\nn = 10\n\n    def safe_head(df, n):\n        r = M.head(df, n)\n        if len(r) != n:\n>           warnings.warn(\n                f\"Insufficient elements for `head`. {n} elements requested, only {len(r)} \"\n                \"elements available. Try passing larger `npartitions` to `head`.\"\n            )\nE           UserWarning: Insufficient elements for `head`. 10 elements requested, only 5 elements available. Try passing larger `npartitions` to `head`.\n\n../../BUILDROOT/usr/lib/python3.13/site-packages/dask/dataframe/core.py:375: UserWarning\n----------------------------- Captured stdout call -----------------------------\nx              int64\ny              int64\nz    string[pyarrow]\ndtype: object\n```\nThis was not a problem with separate `dask-expr`.\n\n**Minimal Complete Verifiable Example**:\n\n```\npytest dask/dataframe/dask_expr/tests/test_shuffle.py -k test_set_index_head_nlargest_string\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.1.0\n- Python version: 3.13.1\n- Operating System: Fedora Rawhide\n- Install method (conda, pip, source): source\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Can you investigate this (or try to reproduce on a more common platform)?"
      },
      {
        "user": "QuLogic",
        "body": "This seems to have been fixed in 2025.3.0 somehow."
      }
    ]
  },
  {
    "issue_number": 11852,
    "title": "⚠️ Upstream CI failed ⚠️",
    "author": "github-actions[bot]",
    "state": "closed",
    "created_at": "2025-03-27T02:02:23Z",
    "updated_at": "2025-04-02T07:51:26Z",
    "labels": [
      "upstream"
    ],
    "body": "[Workflow Run URL](https://github.com/dask/dask/actions/runs/14209570707)\n<details><summary>Python 3.12 Test Summary</summary>\n\n```\ndask/dataframe/dask_expr/tests/test_reductions.py::test_cov_corr: assert False\n +  where False = <function allclose at 0x7f02b876cf30>(0.9993568336425479, np.float64(0.9999999999999999))\n +    where <function allclose at 0x7f02b876cf30> = np.allclose\ndask/dataframe/tests/test_dataframe.py::test_cov_dataframe[None]: AssertionError: DataFrame.iloc[:, 0] (column name=\"A\") are different\n\nDataFrame.iloc[:, 0] (column name=\"A\") values are different (25.0 %)\n[index]: [A, B, C, D]\n[left]:  [0.6298093599638032, -0.17570324281442068, 0.024978709251443937, 0.056073743070522694]\n[right]: [0.6647336130715106, -0.17570324281442068, 0.024978709251443947, 0.05607374307052267]\nAt positional index 0, first diff: 0.6298093599638032 != 0.6647336130715106\ndask/dataframe/tests/test_dataframe.py::test_cov_dataframe[True]: AssertionError: DataFrame.iloc[:, 0] (column name=\"A\") are different\n\nDataFrame.iloc[:, 0] (column name=\"A\") values are different (25.0 %)\n[index]: [A, B, C, D]\n[left]:  [0.47975541259478033, -0.06757555216884337, 0.11066479154661632, -0.035735628379362214]\n[right]: [0.49440581521969224, -0.06757555216884334, 0.1106647915466163, -0.03573562837936219]\nAt positional index 0, first diff: 0.47975541259478033 != 0.49440581521969224\ndask/dataframe/tests/test_dataframe.py::test_cov_dataframe[False]: AssertionError: DataFrame.iloc[:, 0] (column name=\"A\") are different\n\nDataFrame.iloc[:, 0] (column name=\"A\") values are different (25.0 %)\n[index]: [A, B, C, D]\n[left]:  [0.7115066186807248, -0.12793917313101483, 0.3821013511974603, 0.09072764952403418]\n[right]: [0.7256291626301037, -0.1279391731310148, 0.38210135119746025, 0.09072764952403418]\nAt positional index 0, first diff: 0.7115066186807248 != 0.7256291626301037\ndask/dataframe/tests/test_dataframe.py::test_corr: AssertionError: DataFrame.iloc[:, 0] (column name=\"A\") are different\n\nDataFrame.iloc[:, 0] (column name=\"A\") values are different (25.0 %)\n[index]: [A, B, C, D]\n[left]:  [0.9873726130711754, 0.25819210604810516, -0.14123242485127427, 0.21591868057531036]\n[right]: [1.0, 0.2581921060481051, -0.14123242485127424, 0.2159186805753103]\nAt positional index 0, first diff: 0.9873726130711754 != 1.0\ndask/dataframe/tests/test_dataframe.py::test_cov_corr_mixed[True]: AssertionError: DataFrame.iloc[:, 0] (column name=\"ints\") are different\n\nDataFrame.iloc[:, 0] (column name=\"ints\") values are different (87.5 %)\n[index]: [bools, constant, float_nans, floats, hardbools, int_categorical, int_nans, ints]\n[left]:  [-0.004270498938900094, nan, 0.009928791719452879, -0.002744784466901676, -0.004270498938900094, -0.0016391736906910806, -0.01120330955766828, 0.014025321593959729]\n[right]: [-0.014841292424075076, nan, 0.04881956980504242, -0.0006995397361007903, -0.014841292424075076, -0.01151195719496988, 0.01745940165519685, 1.0]\nAt positional index 0, first diff: -0.004270498938900094 != -0.014841292424075076\n```\n\n</details>\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "Our correlation results are a bit off. More than they should be.\n\n\n@TomAugspurger any chance you know what could've caused this?"
      },
      {
        "user": "TomAugspurger",
        "body": "https://github.com/pandas-dev/pandas/pull/61154 looks plausible. I'll look into that."
      }
    ]
  },
  {
    "issue_number": 10238,
    "title": "dask.array.store: support for distributed locks",
    "author": "ahnsws",
    "state": "closed",
    "created_at": "2023-04-28T23:55:36Z",
    "updated_at": "2025-04-01T11:05:26Z",
    "labels": [
      "array",
      "needs attention",
      "needs triage"
    ],
    "body": "**Describe the issue**:\r\nCalling `dask.array.store` with a `Lock`, a `distributed.Client` context, and a zarr target throws an error.\r\n\r\nPlease let me know if this should be filed elsewhere, with zarr or distributed.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport dask.array as da\r\nimport zarr\r\nfrom distributed import Client\r\n\r\n\r\ndef run_succeeds1():\r\n    z_arr: zarr.Array = zarr.zeros(shape=(5, 5))\r\n    d_arr: da.Array = da.from_zarr(z_arr)\r\n    z = zarr.create(shape=d_arr.shape)\r\n    with Client():\r\n        return da.store(d_arr, z, lock=False)\r\n\r\n\r\ndef run_succeeds2():\r\n    z_arr: zarr.Array = zarr.zeros(shape=(5, 5))\r\n    d_arr: da.Array = da.from_zarr(z_arr)\r\n    z = zarr.create(shape=d_arr.shape)\r\n    return da.store(d_arr, z, lock=True)\r\n\r\n\r\ndef run_fails():\r\n    z_arr: zarr.Array = zarr.zeros(shape=(5, 5))\r\n    d_arr: da.Array = da.from_zarr(z_arr)\r\n    z = zarr.create(shape=d_arr.shape)\r\n    with Client():\r\n        return da.store(d_arr, z, lock=True)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run_succeeds1()\r\n    run_succeeds2()\r\n    # run_fails()\r\n\r\n```\r\n\r\n**Anything else we need to know?**:\r\nThe full stacktrace I get when I run `run_fails` is:\r\n\r\n```\r\n2023-04-28 19:51:54,465 - distributed.protocol.pickle - ERROR - Failed to serialize <ToPickle: HighLevelGraph with 1 layers.\r\n<dask.highlevelgraph.HighLevelGraph object at 0x7fb099e26560>\r\n 0. 140396455233728\r\n>.\r\nTraceback (most recent call last):\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 63, in dumps\r\n    result = pickle.dumps(x, **dump_kwargs)\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 68, in dumps\r\n    pickler.dump(x)\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 81, in dumps\r\n    result = cloudpickle.dumps(x, **dump_kwargs)\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\r\n    cp.dump(obj)\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py\", line 632, in dump\r\n    return Pickler.dump(self, obj)\r\nTypeError: cannot pickle '_thread.lock' object\r\nTraceback (most recent call last):\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 63, in dumps\r\n    result = pickle.dumps(x, **dump_kwargs)\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 68, in dumps\r\n    pickler.dump(x)\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 350, in serialize\r\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 73, in pickle_dumps\r\n    frames[0] = pickle.dumps(\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 81, in dumps\r\n    result = cloudpickle.dumps(x, **dump_kwargs)\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\r\n    cp.dump(obj)\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py\", line 632, in dump\r\n    return Pickler.dump(self, obj)\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/titanium/PycharmProjects/debug-ome-zarr/run.py\", line 32, in <module>\r\n    run_fails()\r\n  File \"/home/titanium/PycharmProjects/debug-ome-zarr/run.py\", line 26, in run_fails\r\n    return da.store(d_arr, z, lock=True)\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/dask/array/core.py\", line 1237, in store\r\n    compute_as_if_collection(Array, store_dsk, map_keys, **kwargs)\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/dask/base.py\", line 341, in compute_as_if_collection\r\n    return schedule(dsk2, keys, **kwargs)\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/client.py\", line 3204, in get\r\n    futures = self._graph_to_futures(\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/client.py\", line 3103, in _graph_to_futures\r\n    header, frames = serialize(ToPickle(dsk), on_error=\"raise\")\r\n  File \"/home/titanium/.cache/pypoetry/virtualenvs/debug-ome-zarr-XhBiG45F-py3.10/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 372, in serialize\r\n    raise TypeError(msg, str(x)[:10000]) from exc\r\nTypeError: ('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 1 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7fb099e26560>\\n 0. 140396455233728\\n>')\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n**Environment**:\r\n- Dask version: 2023.4.1\r\n- Python version: 3.10.6\r\n- Operating System: ubuntu 22.04\r\n- Install method (conda, pip, source): poetry\r\n",
    "comments": [
      {
        "user": "j-bennet",
        "body": "cc @crusaderky"
      },
      {
        "user": "crusaderky",
        "body": "lock=True works exclusively with the threaded scheduler. It will work with neither multiprocessing nor distributed. At the very least, the documentation of da.store should clarify that.\r\n\r\nIt looks like you can pass [`lock=distributed.Lock()`](https://distributed.dask.org/en/stable/api.html#distributed.Lock) instead to fix the problem. However, it works only fortuitously - I can't find any unit tests for it.\r\n\r\nThis said - from my understanding, zarr shouldn't even need locks? At which point passing the lock parameter should be invalid. Could somebody more knowledgeable of zarr chime in? "
      },
      {
        "user": "d-v-b",
        "body": "> This said - from my understanding, zarr shouldn't even need locks? At which point passing the lock parameter should be invalid. Could somebody more knowledgeable of zarr chime in?\r\n\r\nIn principle, one could use locks in zarr to guard access to individual chunks. There's a [synchronization api](https://zarr.readthedocs.io/en/stable/api/sync.html), but I've never used it because I do most of my work on a file system that doesn't support file locking.\r\n\r\nAs I understand it, in da.store, dask [locks the entire array-like object](https://github.com/dask/dask/blob/260e633c0ec709d283f3463aef391cece6febba3/dask/array/core.py#L4376), which is almost certainly the wrong behavior for a zarr array. But I could be wrong about what dask is doing here."
      }
    ]
  },
  {
    "issue_number": 4349,
    "title": "TypeError with `store` and distributed scheduler",
    "author": "rainwoodman",
    "state": "closed",
    "created_at": "2019-01-02T23:28:26Z",
    "updated_at": "2025-04-01T11:05:26Z",
    "labels": [
      "io"
    ],
    "body": "Here is a stack trace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/protocol/pickle.py\", line 38, in dumps\r\n    result = pickle.dumps(x, protocol=pickle.HIGHEST_PROTOCOL)\r\nTypeError: can't pickle _thread.lock objects\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pdb.py\", line 1667, in main\r\n    pdb._runscript(mainpyfile)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pdb.py\", line 1548, in _runscript\r\n    self.run(statement)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/bdb.py\", line 434, in run\r\n    exec(cmd, globals, locals)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/global/project/projectdirs/m3127/lightcones/hod/SAM_GP18.py\", line 114, in <module>\r\n    main()\r\n  File \"/global/project/projectdirs/m3127/lightcones/hod/SAM_GP18.py\", line 108, in main\r\n    [column], dataset='ELG-SAM_GP18', header=None)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/nbodykit/base/catalog.py\", line 640, in save\r\n    array.store(_ColumnWrapper(bb), regions=(slice(offset, offset + len(array)),))\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/dask/array/core.py\", line 1131, in store\r\n    r = store([self], [target], **kwargs)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/dask/array/core.py\", line 866, in store\r\n    result.compute(**kwargs)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/dask/base.py\", line 156, in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/dask/base.py\", line 397, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/client.py\", line 2306, in get\r\n    actors=actors,\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/client.py\", line 2249, in _graph_to_futures\r\n    'tasks': valmap(dumps_task, dsk3),\r\n  File \"cytoolz/dicttoolz.pyx\", line 165, in cytoolz.dicttoolz.valmap\r\n  File \"cytoolz/dicttoolz.pyx\", line 190, in cytoolz.dicttoolz.valmap\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/worker.py\", line 2765, in dumps_task\r\n    'args': warn_dumps(task[1:])}\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/worker.py\", line 2774, in warn_dumps\r\n    b = dumps(obj)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/distributed/protocol/pickle.py\", line 51, in dumps\r\n    return cloudpickle.dumps(x, protocol=pickle.HIGHEST_PROTOCOL)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/cloudpickle/cloudpickle.py\", line 931, in dumps\r\n    cp.dump(obj)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/site-packages/cloudpickle/cloudpickle.py\", line 284, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pickle.py\", line 409, in dump\r\n    self.save(obj)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pickle.py\", line 751, in save_tuple\r\n    save(element)\r\n  File \"/global/homes/y/yfeng1/.conda/envs/bccp/lib/python3.6/pickle.py\", line 496, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle _thread.lock objects\r\n```\r\n\r\nThe full line that triggered this error was:\r\n\r\n```\r\n              with ff.create(dataset, dtype, size, Nfile) as bb:\r\n\r\n                    # ensure only the first dimension is chunked\r\n                    # because bigfile only support writing with slices in first dimension.\r\n                    rechunk = dict([(ind, -1) for ind in range(1, array.ndim)])\r\n                    array = array.rechunk(rechunk)\r\n\r\n                    # lock=False to avoid dask from pickling the lock with the object.\r\n                    array.store(_ColumnWrapper(bb), regions=(slice(offset, offset + len(array)),))\r\n```\r\n\r\nIf I add `lock=False` to the argument there is no longer such error. Some digging in the stack frames I realized with the default value of lock=True, a `_thread.lock` object is created for the target object to ensure no two workers attempt to write at the same time.\r\n\r\nThis form of locking is dubious in a cluster environment. Even if implemented correctly (e.g. a lock via the scheduler), the very wide lock also means the entire IO operation will be serialized, and severely limit the IO throughput. These caveats are not stated in the documentation of dask.Array.store.\r\n\r\nFurthermore, when locking is disabled, it is difficult to find a storage backend that correctly implements non-overlapping concurrent write operations of arbitrary striding. Even if one exists, it can hardly be efficient. It is much easier to find some that works properly for a contiguous striding (as simple as POSIX files). It may be beneficial to have a way to inform some guarantee on the kind of strides the `store` function triggers on the target object. In the example above, I used rechunk to rechunk the array; but currently there is no guarantee store will not decide to write the array in sequence of `[:, 0]`, `[:, 1]`, ...\r\n",
    "comments": [
      {
        "user": "jakirkham",
        "body": "Yeah this issue has been raised a few times. Personally would agree with you that the default for locking here no longer makes sense. Distributed has some lock objects that are friendly to use in a cluster environment. Maybe we could use one of those?\n\nThe counterargument to this has usually been we want users to think about whether what they are doing makes sense in a distributed environment before choosing that lock. However I suspect most users today (like yourself) have a pretty good handle on what the differences are and we should pick a default that does not cause unnecessary confusion."
      },
      {
        "user": "rainwoodman",
        "body": "A simple mention of this in the documentation could be the simplest solution to the locking issue in the short term, without changing the behavior.\r\n\r\nThe second issue about guaranteeing the strides of the emitted write operations is a bit more complex, and I think may be more important down the road. "
      }
    ]
  },
  {
    "issue_number": 10074,
    "title": "`da.store` is dropping all annotations even if low level fusion is deactivated",
    "author": "fjetter",
    "state": "closed",
    "created_at": "2023-03-15T18:11:42Z",
    "updated_at": "2025-04-01T11:05:25Z",
    "labels": [
      "array",
      "needs attention",
      "bug"
    ],
    "body": "## Problem\r\n\r\n`da.store` is dropping all annotations from the input graph even if low level fusion is deactivated\r\n\r\n## Example\r\n\r\n```python\r\nimport dask\r\nimport dask.array as da\r\nimport numpy as np\r\n\r\n# Low level fusion is on by default for arrays which is known to strip annotations\r\n# Opting out doesn't help. When using `store` there is no way to preserve annotatoins\r\nwith dask.config.set({\"optimization.fuse.active\": False}):\r\n    with dask.annotate(retries=4):\r\n        a = da.ones((4, 4), chunks=(2, 2))\r\n\r\n    assert any(l.annotations for l in a.dask.layers.values())\r\n    st = da.store([a], [_DevNull()], lock=False, compute=False)\r\n\r\n    # Boom\r\n    assert any(l.annotations for l in st.dask.layers.values())\r\n```\r\n\r\n## Why are we dropping these?\r\n\r\nLooking at the code, we're actually leaving HLGs behind at this specific point. We're only extracting the keys but are not actually preserving the meta information that is attached to layers\r\n \r\nhttps://github.com/dask/dask/blob/890de74381d89910097cd2e9409196db4ab5b5d9/dask/array/core.py#L1201-L1211\r\n\r\n## Why does this matter\r\n\r\nThere is the general theme of users being able to control retries, priorities and resources with this. However, internally we are also relying on annotations to enable P2P shuffling or in this specific case rechunking.\r\n\r\nA user reported this problem already https://github.com/dask/distributed/issues/7599#issuecomment-1458116131 and this is actually a major obstacle for adoption of P2P rechunking since `da.store` is very frequently used see also https://github.com/dask/dask/issues/9381.\r\n\r\nhttps://github.com/dask/dask/issues/9381 in fact already proposes to migrate `da.store` to `Blockwise` which would be one option to fix this problem but the attempt ran into issues over in https://github.com/dask/dask/pull/9382\r\n\r\nI do not care if this is using blockwise or not but I would like annotations to be preserved to enable usage of P2P rechunking.\r\n\r\n\r\n## Related tickets\r\n- https://github.com/dask/distributed/issues/7599\r\n- https://github.com/dask/dask/issues/9541\r\n- https://github.com/dask/dask/issues/9381",
    "comments": [
      {
        "user": "fjetter",
        "body": "@jakirkham I noticed you were the last one to contribute significant changes to this function. This is currently blocking us on the P2P rechunking front and I'm having a hard time wrapping my head around all of it. We don't need this to be blockwise for P2P but we do need to preserve annotations.\r\n\r\nIs this something you can take care of?"
      },
      {
        "user": "d-v-b",
        "body": "@fjetter has there been any update on this? I would love to use p2p reshuffling but my use case involves storing data as the output. Since the underlying computation being mapped in `da.store` is fairly simple, could you describe or sketch a solution that would preserve annotations (which I know nothing about), so that I could implement this myself?"
      },
      {
        "user": "mrocklin",
        "body": "@quasiben I don't suppose you or your team have any bandwidth to help out here?\r\n\r\n@d-v-b the challenge here is that `da.store` acts very differently from most other Dask IO functions.  It grabs the raw graph, manipulates it, and generally doesn't handle annotations (which shuffling uses).  This should probably be cleaned up, ideally by someone who understands dask.array well, and dask.array.store in particular.  Historically @jakirkham understood this code best, but I believe he's spending most of his time elsewhere currently.  If you or others around you have any interest in helping out here that would be very welcome."
      }
    ]
  },
  {
    "issue_number": 9381,
    "title": "Use Blockwise in `da.store`",
    "author": "gjoseph92",
    "state": "closed",
    "created_at": "2022-08-12T18:09:02Z",
    "updated_at": "2025-04-01T11:05:25Z",
    "labels": [
      "array",
      "highlevelgraph",
      "needs attention",
      "enhancement"
    ],
    "body": "`da.store` is basically the main method for writing arrays out of dask; things like `to_zarr` use `da.store` internally, and downstream projects like xarray use it extensively. Currently, it produces low-level graphs. These graphs would be very simple to convert to Blockwise: https://github.com/dask/dask/blob/8b95f983c232c1bd628e9cba0695d3ef229d290b/dask/array/core.py#L4430-L4433\r\n\r\nWe're just mapping a function over every block, which is the simplest case for Blockwise.\r\n\r\nThis would make graph generation (if not submission, everything gets materialized right now anyway) a lot faster for large array-writing operations, and allow blockwise fusion onto input tasks.\r\n\r\ncc @rjzamora @ian-r-rose",
    "comments": [
      {
        "user": "jrbourbeau",
        "body": "+1 -- nice suggestion @gjoseph92 "
      }
    ]
  },
  {
    "issue_number": 11851,
    "title": "`ddof` argument in `Rolling.std`",
    "author": "MarcoGorelli",
    "state": "closed",
    "created_at": "2025-03-26T14:11:26Z",
    "updated_at": "2025-04-01T11:03:46Z",
    "labels": [
      "dataframe",
      "dask-expr"
    ],
    "body": "```python\n\nIn [12]: df = dd.from_pandas(pd.DataFrame({'a':[1,2,3],'b':[4,5,6]}))\n\nIn [13]: df['a'].rolling(2).std(1)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[13], line 1\n----> 1 df['a'].rolling(2).std(1)\n\nTypeError: Rolling.std() takes 1 positional argument but 2 were given\n```\n\n```\nIn [15]: dask.__version__\nOut[15]: '2025.3.0'\n```\n\npandas has this arg",
    "comments": []
  },
  {
    "issue_number": 11842,
    "title": "⚠️ Upstream CI failed ⚠️",
    "author": "github-actions[bot]",
    "state": "closed",
    "created_at": "2025-03-23T02:01:10Z",
    "updated_at": "2025-03-25T14:34:13Z",
    "labels": [
      "upstream"
    ],
    "body": "[Workflow Run URL](https://github.com/dask/dask/actions/runs/14025315686)\n<details><summary>Python 3.12 Test Summary</summary>\n\n```\ndask/dataframe/tests/test_dataframe.py::test_describe_numeric[tdigest-test_values0]: AssertionError: DataFrame are different\n\nDataFrame shape mismatch\n[left]:  (7, 2)\n[right]: (8, 2)\ndask/dataframe/tests/test_dataframe.py::test_describe_numeric[dask-test_values1]: AssertionError: DataFrame are different\n\nDataFrame shape mismatch\n[left]:  (7, 2)\n[right]: (8, 2)\ndask/dataframe/tests/test_dataframe.py::test_describe[include8-None-percentiles8-None]: AssertionError: DataFrame are different\n\nDataFrame shape mismatch\n[left]:  (11, 4)\n[right]: (10, 4)\n```\n\n</details>\n",
    "comments": [
      {
        "user": "jrbourbeau",
        "body": "Closing after https://github.com/dask/dask/pull/11847. The CI job will reopen if upstream is still failing (I think it might be fixed) "
      }
    ]
  },
  {
    "issue_number": 8058,
    "title": "[Discussion] Improve Parquet-Metadata Processing in read_parquet",
    "author": "rjzamora",
    "state": "closed",
    "created_at": "2021-08-17T23:09:04Z",
    "updated_at": "2025-03-24T13:30:35Z",
    "labels": [
      "dataframe",
      "io",
      "discussion",
      "parquet",
      "needs attention"
    ],
    "body": "\r\n## Current Metadata-Related Challenges in `read_parquet`\r\n\r\nThe current approach to parquet-metadata handling in Dask-Dataframe has been causing pain for many users recently. The problem is especially true for large-scale IO from cloud-based file systems.  The origin of this pain seems to be the historical decision to rely on a shared _metadata file.\r\n\r\n### Why we use _metadata\r\n\r\nThe historical reason for our adoption of the _metadata file is simple: The usual convention in Dask is to only use the client process to construct a graph when a public collection-related API is used. There are certainly exceptions to this (e.g. `set_index`), but it is typically true that non-compute/persist API calls will only execute on the client.  Therefore, in order to avoid the slow process of opening and processing footer metadata for every file in a parquet dataset on the client, we have encouraged the construction of a single/global _metadata file at write time.  This is why Dask's `to_parquet` implementation will construct and write this file by default (see the `write_metadata_file` argument).\r\n\r\n### Why _metadata can be a problem\r\n\r\nAlthough most parquet users will likely benefit from writing/reading a shared _metadata file, there are clearly cases where this approach breaks down. As illustrated in issues like #8031 and #8027, there are large-scale IO scenarios in which a single metadata file can be too large to write and/or read on a single process.  Given that the entire purpose of Dask-Dataframe is to enable/accelerate large-scale tabular-data processing, I feel that we should treat these cases seriously.\r\n\r\nAs I will present below, my short-term suggestion to the _metadata problem is two-fold:\r\n\r\n1. Make it possible to ignore the existence of a shared _metadata file when it is present (Note that this step is already implemented in #8034)\r\n2. Refactor the `Engine.read_metadata` implementation internals (especially for \"pyarrow-datsaet\" and \"fastparquet\") to allow `parts`/`statistics` to be collected in parallel, and to avoid the unnecessary/intermediate step of constructing a proxy global-metadata object when the _metadata file is missing and/or needs to be ignored.\r\n\r\n\r\n## The Current Organization of `read_parquet`\r\n\r\n\r\n### Core-Engine Interface\r\n\r\nTo understand the proposed short-term _metadata solution, it is useful to have a rough understanding of \"core\"-\"engine\" interface that is currently used within Dask-Dataframe's `read_parquet` API.  When the user calls `read_parquet`, they are either explicitly or implicitly specifying a backend engine. That engine is then expected to produce three critical pieces of information when `engine.read_metadata` is called:\r\n\r\n1. `meta`: The metadata (an empty `pandas.DataFrame` object) for the output DataFrame collection\r\n2. `parts`: The `list` of information needed to produce the output-DataFrame partitions at IO time.  After `parts` is finalized, each if its elements will be used for a distinct call to `engine.read_partition` to produce a single output-`pandas.DataFrame` partition.  For this reason, generating `parts` is effectively the same as materializing the task graph.\r\n3. `statistics`: A list of statistics for each element of `parts`. This list is required to calculate output divisions, aggregate files by `chunksize`, and/or apply filters (except for \"pyarrow-dataset\").\r\n\r\n**Outline of the existing read_parquet algorithm:**\r\n\r\n```python\r\ndef read_parquet(paths, filters, engine, ...):\r\n    ...\r\n    # ENGINE logic to calculate meta, parts & statistics.\r\n    #\r\n    # This is where the engine is expected to produce output\r\n    # `meta`, and the `parts` & `statistics` lists-\r\n    (\r\n        meta,\r\n        parts,\r\n        statistics,\r\n        ...\r\n    ) = engine.read_metadata(paths, filters, ...)\r\n\r\n    # CORE logic to apply filters and calculate divisions\r\n    (\r\n        parts,\r\n        divisions,\r\n        ...\r\n    ) = process_statistics(parts, statistics, filters, ...)\r\n\r\n    # CORE logic to define Layer/DataFrame\r\n    layer = DataFrameIOLayer(parts, ...)\r\n    ...\r\n    return new_dd_object(...)\r\n```\r\n\r\n\r\nAlthough the best long-term `read_parquet` API will likely need to break the single `read_metadata` API into 2-3 distinct functions, the short-term proposal will start by leaving the surface area of this function as is.  Instead of modifying the \"public\" Engine API, I suggest that we focus on a smaller internal refactor of `read_metadata`.\r\n\r\n\r\n### Engine-Specific Logic: Collecting Metadata and `parts`/`statistics`\r\n\r\nIn order to understand how we must modify the existing `Engine.read_metadata` implementations, it is useful to describe how these functions are currently designed.  The general approach comprises three steps:\r\n\r\n1. Use engine-specific logic to construct a global parquet-metadata object (or a global-metadata \"proxy\")\r\n2. Use collected metadata/schema information to define the output DataFrame `meta`\r\n3. Use a mix of shared and engine-specific logic to convert `metadata` to `parts` and `statistics`\r\n\r\n**Outline of the existing ArrowDatasetEngine.read_metadata implementation:**\r\n\r\n```python\r\nclass ArrowDatasetEngine:\r\n\r\n    @classmethod\r\n    def read_metadata(cls, paths, filters, ...):\r\n\t\r\n        # Collect a `metadata` structure. For \"pyarrow-legacy\",\r\n        # this is a `pyarrow.parquet.FileMetaData` object. For\r\n        # \"pyarrow-dataset\", this is a list of dataset fragments.\r\n        metadata, ... = cls._gather_metadata(paths, filters, ...)\r\n\r\n        # Use the pyarrow.dataset schema to construct the `meta`\r\n        # of the output DataFrame collection\r\n        meta, ... = cls._generate_dd_meta(schema, ...)\r\n\t\t\r\n        # Use the `metadata` object (list of fragments) to construct\r\n        # coupled `parts` and `statistics` lists\r\n        parts, statistics, ... = cls._construct_parts(metadata, ...)\r\n\t\t\r\n        return meta, statistics, parts\r\n\t\t\r\n    @classmethod\r\n    def _gather_metadata(cls, paths, ...):\r\n\t\r\n        # Create pyarrow.dataset object\r\n        ds = pa_dataset.dataset(paths, ...)\r\n\t\t\r\n        # Collect filtered list of dataset \"fragments\".\r\n        # Call this list of fragments the \"metadata\"\r\n        # (this is NOT a formal \"parquet-metadata\" object)\r\n        metadata = _collect_pyarrow_dataset_frags(ds, filters, ...)\r\n\r\n        return schema, metadata, ...\r\n\t\t\r\n    @classmethod\r\n    def _construct_parts(cls, metadata, ...):\r\n\t\r\n        # Here we use a combination of engine-specific and\r\n        # shared logic to construct coupled `parts` and `statistics`\r\n        parts, statistics = <messy-logic>(metadata, ...)\r\n\r\n        return parts, statistics, ...\r\n```\r\n\r\nThe exact details of these steps depend on the specific engine, but the general algorithm is pretty much the same for both \"pyarrow\" and \"fastparquet\". This general algorithm works well in many cases, but it has the following drawbacks:\r\n\r\n1. The metadata/metadata-proxy object construction does not yet allow the user to opt out of _metadata processing.\r\n2. The metadata/metadata-proxy object is only collected in parallel for the \"arrow-legacy\" API (not for \"pyarrow\" or \"fastparquet\")\r\n3. Even when the metadata object is collected in parallel for \"pyarrow-legacy\", it is still reduced into a single object and then processed in serial on the client.  This is clearly not the most efficient way to produce the final `parts`/`statistics` lists.\r\n4. (**more of a future problem**) The `meta` is not constructed until after a metadata or metadata-proxy object has been constructed.  This is not a problem yet, but is likely to become one when it is time to implement an abstract-expression API for read_parquet. \r\n\r\n\r\n## Short-Term Proposal\r\n\r\n### Make _metadata Processing Optional\r\n\r\nI propose that we allow the user to specify that a global _metadata file should be ignored by Dask. This first step is already implemented in #8034, where a new `ignore_metadata_file=` kwarg has been added to the public `read_parquet` API. Please feel free to provide specific feedback in that PR.\r\n\r\n### Refactor `*Engine.read_metadata`\r\n\r\n**Outline of the PROPOSED `ArrowDatasetEngine.read_metadat`a implementation**:\r\n\r\n```python\r\nclass ArrowDatasetEngine:\r\n    @classmethod\r\n    def read_metadata(cls, paths, filters, ignore_metadata_file, ...):\r\n\r\n        # Stage 1: Use a combination of engine specific logic and shared\r\n        # fsspec utilities to construct an engine-specific `datset_info` dictionary.\r\n        dataset_info = cls._collect_dataset_info(paths, ignore_metadata_file, ...)\r\n\r\n        # Stage 2: Use information in `dataset_info` (like schema) to define\r\n        # the `meta` for the output DataFrame collection.\r\n        meta, ... = cls._generate_dd_meta(dataset_info, ...)\r\n\t\t\r\n        # Stage 3: Use information in `dataset_info` to directly construct\r\n        # `parts` and `statistics`\r\n        parts, statistics = cls._make_partition_plan(dataset_info, meta, filters, ...)\r\n\t\t\r\n        return meta, statistics, parts\r\n```\r\n\r\n#### Stage-1 Details\r\n\r\nThe purpose of this stage is to do as little work as possible to populate a dictionary of high-level information about the dataset in question.  This \"high-level\" information will most likely include the schema, the paths, the file-system, and the discovered hive partitions.  Although our first pass at this should not try to do anything particularly clever here, we may eventually want to use this space to discover the paths/hive-partitions in parallel. We can also leverage #9051 (`categorical_partitions`) to avoid the need to discover all files up front (since we may not need full categorical dtypes for the schema).\r\n\r\nAssuming that we should keep stage 1 simple for now, the new `_collect_dataset_info` functions will effectively pull out the existing logic in `*engine.read_metadata` used to define a `pyarrow.dataset`/`ParquetDataset`/`ParquetFile` objects.  These objects can be stored, along with other important information, in the output `dataset_info` dictionary. Note that the only critical detail in this stage is that we must support the `ignore_metadata_file=True` option.\r\n\r\n\r\n#### Stage-2 Details\r\n\r\nThere is not much (if any) work to be done here.  The existing logic in the various `_generate_dd_meta` implementations can be reused.\r\n\r\n#### Stage-3 Details\r\n\r\nThis stage will correspond to the lion's share of the pain required for this work. While the short-term plan for Stage-1 and Stage-2 are to effectively move around existing code into simple functions with clear objectives, Stage-3 will require us to implement a new algorithm to (optionally) construct `parts`/`statistics` in parallel.  I expect the exact details here to be pretty engine-specific.  That is, \"pyarrow-datset\" will (optionally) parallelize over the processing of file fragments into `parts`/`statistics`, while \"fastparquet\" will need to parallelize over paths a bit differently.  However, I do expect all engine to follow a similar \"parallelize over file-path/object\" approach.  In the case that the _metadata file exists (and the user has not asked to ignore it), we should avoid constructing `parts`/`statistics` on the workers.\r\n\r\n\r\n**ROUGH illustration of the PROPOSED `ArrowDatasetEngine._make_partition_plan` implementation**:\r\n\r\n```python\r\n@classmethod\r\ndef _make_partition_plan(cls, dataset_info, meta, filters, split_row_groups, ...):\r\n\r\n    parts, statistics = [], []\r\n\t\r\n    # (OPTIONALLY) DASK-PARALLELIZE THIS LOOP:\r\n    for file_object in all_file_objects:\r\n        part, part_stats = cls._collect_file_parts(file_object, ...)\r\n\t\r\n    return parts, statistics\r\n```",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "Thanks for writing this up Rick. I'm still digesting it, but do you think this \"allow parts/statistics to be collected in parallel\" would help with dask-geopandas, where they have to (re)-open each partition to read some metadata out of the partition's schema? (https://github.com/geopandas/dask-geopandas/blob/99fa1a865156649d66c9ae4fa6fe119b6be6206c/dask_geopandas/io/parquet.py#L22-L37)"
      },
      {
        "user": "rjzamora",
        "body": ">do you think this \"allow parts/statistics to be collected in parallel\" would help with dask-geopandas, where they have to (re)-open each partition to read some metadata out of the partition's schema?\r\n\r\nThanks for sharing this application @TomAugspurger ! This is similar to how NVTabular needs to re-read the metadata to calculate the length of a dataset more efficiently (although our solution solution is quite a bit uglier than yours).\r\n\r\nGenerally speaking, the short-term proposal here does not solve your metadata problem by itself.  However, the refactor does provide an opportunity to expose an API for collecting custom metadata/statistics during the original pass."
      },
      {
        "user": "mrocklin",
        "body": "Like @TomAugspurger , I read through this, and everything made sense as I was reading it, but now I'm having difficulty packing it into a small enough space in my brain to reason about it effectively :)\r\n\r\nI'll add two notes here for context:\r\n\r\n1.  Whenever we call `compute` in dask collections code we break the ability to operate asynchronously.  This isn't a big deal for users (most of the time) but it can be annoying for testing.  \r\n\r\n    One solution to this would be to add an async `_read_parquet` method that `read_parquet` then called directly.  This would bring in the dask/distributed async system into dask/dask though, which we probably don't want to do.\r\n\r\n    My guess is that we just accept the loss of async for dask.dataframe and move on, but I wanted to bring this up so that we're at least intentional about that choice.\r\n\r\n2.  For simplicity's and consistency's sakes, I'll propose an option that we *always* read statistics from row groups and ignore the `_metadata` file.  Would this improve the state of the world?  Degrade it?  "
      }
    ]
  },
  {
    "issue_number": 11669,
    "title": "Slicing problem that can occur when shuffle algorthm for 'take' is applied",
    "author": "davidhassell",
    "state": "open",
    "created_at": "2025-01-14T14:29:21Z",
    "updated_at": "2025-03-24T02:06:12Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "Hi,\r\n\r\nThe change that introduced the shuffle algorithm for `take` in slicing (introduced by PR https://github.com/dask/dask/pull/11267 in `2024.08.2`, no associated issue) seems like a useful optimisation - thanks. However I'd really like to have the option of turning it off!\r\n\r\nThe problem I have is with the concatenation step in `concatenate_arrays` (https://github.com/dask/dask/blob/main/dask/array/_shuffle.py#L318), which is called by `_shuffle` (https://github.com/dask/dask/blob/main/dask/array/_shuffle.py#L289). This causes problems with objects that implement the `__array_function__` protocol, but not for `np.concatenation`. For these we get (where `NetCDF4Array` is my array-like data object being sliced):\r\n```python\r\nFile ~/dask/dask/array/_shuffle.py:323, in concatenate_arrays(arrs, sorter, axis)\r\n    318 def concatenate_arrays(arrs, sorter, axis):\r\n    319    return take_lookup(\r\n--> 320         concatenate_lookup.dispatch(type(arrs[0]))(arrs, axis=axis),\r\n    321         np.argsort(sorter[1]),\r\n    322         axis=axis,\r\n    323     )\r\n\r\nTypeError: no implementation found for 'numpy.concatenate' on types that implement __array_function__: [<class 'NetCDF4Array'>]\r\n```\r\n\r\nIt is essential in one of my workflows **[*]** that my `NetCDFArray` array-like objects in the graph do not get converted to a numpy arrays through the slicing process - i.e. after an arbitrary number of computed `__getitem__`s, I still need a `NetCDF4Array` object in the graph, and not a numpy array. ( `NetCDFArray.__getitem__` does _not_ return a numpy array, rather another `NetCDF4Array` object with a lazy slice, and I manually add a \"convert to numpy\" layer to the graph _if I need to_.) \r\n\r\nI can't implement a `np.concatenate` `__array_function__` protocol that allows `np.concatenate` to return a new `NetCDFArray` object, because that would be like re-implementing Dask (!).\r\n\r\nUn-implementing the `__array_function__` protocol is not an option because, even though it makes the error go away and the graph executes, I end up with numpy arrays in my chunks, rather than the `NetCDF4Array` objects that I need.\r\n\r\nI have pinned to `2024.07.1` to get round this, but that's not going to be sustainable for ever. I can't see how the shuffle algorithm can get around doing a concatenation, so the only option I can see is the option to disable shuffling during `take` and revert to the older behaviour ... Am I missing a trick, though?\r\n\r\nMany thanks for your help,\r\nDavid\r\n\r\n----\r\n\r\n**[*]** The workflow is using Dask to manage \"active storage reductions\": essentially getting the remote server to carry out reduction calculations where the data actually is and returning the small reduced result, rather than bringing the data across the network and performing the reductions locally. To do this, the array-like data in the Dask graph must, up until the reduction graph layer, remain as a virtual array-like object that knows how to get the data (i.e. my `NetCDFArray`), rather than as an object that actually contains the data locally (i.e. a numpy array). Prior to the reduction layer, we don't allow operations that would change the data _values_ relative to those in the remote file (policed in our library outside of Dask) - but we must allow slicing, since real-life reductions are so often not required on the whole of a file's data.\r\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "Thank you for taking the time to write this issue.\n\nIt is not trivial, maybe not even possible, to introduce a toggle into the current algorithm. A toggle like this would therefore require us to revive the old (and flawed) algorithm just for this specific use case. That is quite a burden in terms of code complexity and maintenance."
      },
      {
        "user": "bnlawrence",
        "body": "As we noted, we appreciate the optimisations, but we think this is actually a rather major change that has gone in without any discussion or warning. I am sure we will not be the only research community that makes heavy use of Dask who have relied on the fact that the entity in the graph only has to quack like a numpy array and not actually be one.  Lots of things have broken, many of which might be fixable, but we currently expect our only long term solution would be to give up on Dask (i.e. fork it), or give up on being able to have a proper separation of concerns between reductions done _in_ the Dask graph, or before it (by for example, NICs that can do reductions, or as we have, physical storage that understands reductions).\n\nIt would be helpful if you could expand on what the complexity and maintenance issues would be, and whether or not if we could find maintainable solutions, you would accept a pull request."
      },
      {
        "user": "fjetter",
        "body": "> It would be helpful if you could expand on what the complexity and maintenance issues would be, and whether or not if we could find maintainable solutions, you would accept a pull request.\n\nThe algorithm is highly nontrivial and I'm not comfortable owning and maintaining this algorithm in two different versions, especially if the old one is known to be very problematic. If a proposed PR would suggest something like this I doubt we'd accept it.\n\n\nThis is a bit of an odd case. We support both standard numpy arrays and arrays that do properly implement the `__array_function__` protocol. I don't think supporting pseudo arrays like your version that chooses to only implement a partial API is in scope for the project.\n\nI don't understand your use case sufficiently well and I don't understand why you have to put lazy arrays inside of a dask array (which itself is also lazy) to make a recommendation. Do you see a way to implement concat? Is it possible for you to rely on dask being lazy? Would you feel comfortable owning a full implementation of take yourself if we offered a way to redirect to your library?"
      }
    ]
  },
  {
    "issue_number": 11679,
    "title": "dask shuffle pyarrow.lib.ArrowTypeError: struct fields don't match or are in the wrong orders",
    "author": "MikeChenfu",
    "state": "open",
    "created_at": "2025-01-17T22:27:22Z",
    "updated_at": "2025-03-24T02:06:10Z",
    "labels": [
      "dataframe",
      "needs attention",
      "bug",
      "dask-expr"
    ],
    "body": "Hello, I met a problem when I shuffle the data among 160 dask partitions.\nI got the error when each partition contains 200 samples. But the error is gone when it contains 400 samples or more. I really appreciate it if someone can help me.\n```bash\npyarrow.lib.ArrowTypeError: struct fields don't match or are in the wrong orders  Input fields: struct<image_url: struct<url: string>, text: string, type: string> output fields: struct<text: string, type: string, image_url: struct<url: string>>\n```\n\n\n**Environment**:\n\n- Dask version:  '2024.12.1'\n- Python version: '3.10'\n",
    "comments": []
  },
  {
    "issue_number": 11839,
    "title": "⚠️ Upstream CI failed ⚠️",
    "author": "github-actions[bot]",
    "state": "closed",
    "created_at": "2025-03-21T10:55:12Z",
    "updated_at": "2025-03-21T12:34:27Z",
    "labels": [
      "upstream"
    ],
    "body": "[Workflow Run URL](https://github.com/dask/dask/actions/runs/13989672683)\n<details><summary>Python 3.12 Test Summary</summary>\n\n```\ndask/bytes/tests/test_http.py::test_parquet[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/bytes/tests/test_s3.py::test_parquet[pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/bytes/tests/test_s3.py::test_parquet[pyarrow-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/bytes/tests/test_s3.py::test_parquet_append[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/bytes/tests/test_s3.py::test_parquet_wstoragepars[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_distributed.py::test_io_fusion_merge: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_simplify[<lambda>-<lambda>0]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_simplify[<lambda>-<lambda>1]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_simplify[<lambda>-<lambda>2]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_simplify[<lambda>-<lambda>3]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_simplify[<lambda>-<lambda>4]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_io_fusion[parquet]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_io_fusion_blockwise: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_repartition_io_fusion_blockwise: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_io_fusion_merge: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_io_fusion_zero: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_io_culling[parquet]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_parquet_complex_filters: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_combine_similar[parquet-read_parquet-ReadParquet]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_io.py::test_combine_similar_no_projection_on_one_branch: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_parquet_len_empty_dir: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_to_parquet[True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_to_parquet[False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_pyarrow_filesystem: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_pyarrow_filesystem_dtype_backend[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_pyarrow_filesystem_dtype_backend[numpy_nullable]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_pyarrow_filesystem_dtype_backend[None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_pyarrow_filesystem_types_mapper[None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_pyarrow_filesystem_types_mapper[<lambda>]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_index_only_from_parquet: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_timestamp_divisions: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/dask_expr/io/tests/test_parquet.py::test_read_parquet_index_projection: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_local[pyarrow-pyarrow-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_local[pyarrow-pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_empty[pyarrow-pyarrow-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_empty[pyarrow-pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_simple[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_delayed_no_metadata[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_glob[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_calculate_divisions_false[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_list[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_columns_auto_index[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_columns_index[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_nonsense_column[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_columns_no_index[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_calculate_divisions_no_index[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_columns_index_with_multi_index[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_no_index[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_series[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_names[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_from_pandas[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_nullable_dtypes: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_use_nullable_dtypes_with_types_mapper[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_categorical[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_append[pyarrow-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_append[pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_append_create[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_append_with_partition: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on_cats[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on_cats_pyarrow[False-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on_cats_pyarrow[False-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on_cats_pyarrow[True-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on_cats_pyarrow[True-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_parallel_metadata[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on_cats_2[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_append_wo_index[pyarrow-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_append_wo_index[pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_append_known_divisions_to_unknown_divisions_works[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filter_with_struct_column: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_ordering[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_parquet_custom_columns[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_categories[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_empty_partition[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_timestamp_index[pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_timestamp_index[pyarrow-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_schema_inference[infer-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_schema_inference[infer-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_schema_inference[complex-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_schema_inference[complex-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_schema_mismatch_explicit_schema_none: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on_duplicates[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on_string[aa]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partition_on_string[partition_on1]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filters_categorical[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filters[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filters_v0[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filtering_pyarrow_dataset[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filters_file_list[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_filter_divisions: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_to_parquet_lazy[pyarrow-threads]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_to_parquet_lazy[pyarrow-processes]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_parquet_select_cats[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_columns_name[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_writing_parquet_with_compression[pyarrow-None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_writing_parquet_with_compression[pyarrow-gzip]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_writing_parquet_with_compression[pyarrow-snappy]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_no_metadata[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_writing_parquet_with_kwargs[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_to_parquet_with_get[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_select_partitioned_column[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_arrow_partitioning: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_append_cat_fp[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df0]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df1]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df2]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df3]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df4]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df5]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df6]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df7]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df8]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df9]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df10]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df11]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df12]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df13]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df14]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df15]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df16]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df17]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df18]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df19]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_arrow[df20]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_datasets_timeseries[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pathlib_path[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_glob_no_meta[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_glob_yes_meta[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_dir_nometa[pyarrow-pyarrow-True-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_dir_nometa[pyarrow-pyarrow-True-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_dir_nometa[pyarrow-pyarrow-False-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_dir_nometa[pyarrow-pyarrow-False-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_statistics_nometa[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_timeseries_nulls_in_schema[pyarrow-infer]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_timeseries_nulls_in_schema[pyarrow-None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_graph_size_pyarrow[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_getitem_optimization_multi[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_row_groups[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_row_groups_int[pyarrow-True-1]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_row_groups_int[pyarrow-True-12]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_row_groups_int[pyarrow-False-1]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_row_groups_int[pyarrow-False-12]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_row_groups_int_aggregate_files[pyarrow-8]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_row_groups_int_aggregate_files[pyarrow-25]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filter_nulls[pyarrow-True-filters1-<lambda>-2]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filter_nulls[pyarrow-False-filters1-<lambda>-2]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filter_isna[True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filter_isna[False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_row_groups_filter[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_optimize_getitem_and_nonblockwise[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_optimize_and_not[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_empty[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_files[4096-None-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_files[4096-None-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_files[4096-a-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_files[4096-a-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_files[1MiB-None-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_files[1MiB-None-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_files[1MiB-a-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_files[1MiB-a-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_aggregate_files[a]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_aggregate_files[b]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_blocksize[pyarrow-None-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_blocksize[pyarrow-None-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_blocksize[pyarrow-1024-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_blocksize[pyarrow-1024-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_blocksize[pyarrow-4096-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_blocksize[pyarrow-4096-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_blocksize[pyarrow-1MiB-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_split_adaptive_blocksize[pyarrow-1MiB-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_blocksize[pyarrow-default-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_blocksize[pyarrow-default-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_blocksize[pyarrow-512-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_blocksize[pyarrow-512-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_blocksize[pyarrow-1024-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_blocksize[pyarrow-1024-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_blocksize[pyarrow-1MiB-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_blocksize[pyarrow-1MiB-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_pandas_blocksize[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filter_nonpartition_columns[pyarrow-pyarrow-None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_filter_nonpartition_columns[pyarrow-pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pandas_metadata_nullable_pyarrow: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pandas_timestamp_overflow_pyarrow: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_arrow_to_pandas[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partitioned_column_overlap[pyarrow-write_cols1]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partitioned_no_pandas_metadata[pyarrow-write_cols0]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partitioned_no_pandas_metadata[pyarrow-write_cols1]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_directory_partitioning: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_partitioned_preserve_index[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_from_pandas_preserve_none_index[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_multi_partition_none_index_false[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_from_pandas_preserve_none_rangeindex[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_divisions_with_null_partition[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_dataset_simple[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_dataset_partitioned[pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_dataset_partitioned[pyarrow-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_null_partition_pyarrow[None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_null_partition_pyarrow[processes]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_dataset_read_from_paths: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_dataset_filter_partitioned[True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_dataset_filter_partitioned[False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_pyarrow_dataset_filter_on_partitioned[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_create_metadata_file[None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_create_metadata_file[a]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_to_parquet_overwrite_adaptive_round_trip[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_to_parquet_overwrite_files_from_read_parquet_in_same_call_raises[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_dir_filter[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_decimal_dtype: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_date_dtype: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_rename_columns[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_custom_metadata[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_ignore_metadata_file[pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_ignore_metadata_file[pyarrow-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_ignore_metadata_file[pyarrow-None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_metadata_task_size[pyarrow-2-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_metadata_task_size[pyarrow-2-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_metadata_task_size[pyarrow-0-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_metadata_task_size[pyarrow-0-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_extra_file[pyarrow-b]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_extra_file[pyarrow-None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_unsupported_extension_file[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_unsupported_extension_dir[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_custom_filename[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_custom_filename_works_with_pyarrow_when_append_is_true: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_custom_filename_with_partition[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_roundtrip_partitioned_pyarrow_dataset[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_in_predicate_can_use_iterables[pyarrow-set]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_in_predicate_can_use_iterables[pyarrow-list]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_in_predicate_can_use_iterables[pyarrow-tuple]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_not_in_predicate[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_fsspec_to_parquet_filesystem_option: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_select_filtered_column[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_select_filtered_column_no_stats[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_parquet_convert_string[pyarrow-True]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_parquet_convert_string[pyarrow-False]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_parquet_convert_string_nullable_mapper[pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_dtype_backend[pyarrow-numpy_nullable]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_dtype_backend[pyarrow-pyarrow]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_parquet_preserve_categorical_column_dtype: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_dtype_backend_categoricals: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_non_categorical_partitioning_pyarrow[None]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_non_categorical_partitioning_pyarrow[filters1]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_read_parquet_lists_not_converting: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/dataframe/io/tests/test_parquet.py::test_parquet_string_roundtrip: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/tests/test_distributed.py::test_blockwise_dataframe_io[True-parquet]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/tests/test_distributed.py::test_blockwise_dataframe_io[False-parquet]: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\ndask/tests/test_distributed.py::test_client_compute_parquet: TypeError: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: unsupported operand type(s) for -: 'NoneType' and 'dict'\n```\n\n</details>\n",
    "comments": []
  },
  {
    "issue_number": 11834,
    "title": "head() does not rise a warning on empty result",
    "author": "dbalabka",
    "state": "closed",
    "created_at": "2025-03-20T16:12:09Z",
    "updated_at": "2025-03-20T16:29:42Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\nAccording to Dask `head()` method [documentation](https://docs.dask.org/en/latest/generated/dask.dataframe.DataFrame.head.html#:~:text=if%20there%20are%20fewer%20than%20n%20rows%20in%20the%20first%20npartitions%20a%20warning%20will%20be%20raised%20and%20any%20found%20rows%20returned):\n> If there are fewer than n rows in the first npartitions a warning will be raised and any found rows returned\n\nHowever, the warning isn't raised, leading to confusion and even incorrect conclusions when working with Dask in the Jupyter environment interactively.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nfrom dask.distributed import LocalCluster\nimport dask.dataframe as dd\nimport pandas as pd\n\ncluster = LocalCluster(n_workers=2)\nddf = dd.from_pandas(pd.DataFrame({\"a\": (['b']*1000) + (['c']*1000)})).shuffle('a').repartition(npartitions=2).shuffle('a')\nddf.map_partitions(lambda x: x.a.unique()).compute()\nprint(ddf.query('a == \"c\"').head())\n```\n\n```\nEmpty DataFrame\nColumns: [a]\nIndex: []\n```\nI'm expecting to see:\n```\n.venv/lib/python3.11/site-packages/dask/dataframe/core.py:8153: UserWarning: Insufficient elements for `head`. 5 elements requested, only 0 elements available. Try passing larger `npartitions` to `head`.\n  warnings.warn(\n```\n\nSearch over all partitions works as expected:\n```python\nprint(ddf.query('a == \"c\"').head(npartitions=-1))\n```\n```\n      a\n1000  c\n1001  c\n1002  c\n1003  c\n1004  c\n```\n\n**Environment**:\n\n- Dask version: 2024.12.1\n- Python version: 3.11\n- Operating System: WSL\n- Install method (conda, pip, source): poetry\n",
    "comments": [
      {
        "user": "dbalabka",
        "body": "I apologise for the noise. It is the Jupiter environmental issue."
      }
    ]
  },
  {
    "issue_number": 11824,
    "title": "read_parquet with empty columns gives incorrect partitions",
    "author": "bnaul",
    "state": "closed",
    "created_at": "2025-03-12T00:23:51Z",
    "updated_at": "2025-03-20T13:31:47Z",
    "labels": [
      "dataframe",
      "dask-expr"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nReading parquet files and either subsetting to empty columns or passing `columns=[]` results in inconsistent `map_partitions` behavior compared to with non-empty columns.\n\n\n**Minimal Complete Verifiable Example**:\n```python\nimport dask.dataframe as dd\n\n# Write some dummy data w/ 2 partitions\nddf = dd.from_pandas(pd.DataFrame({\"a\": range(12)}), npartitions=2)\nddf.to_parquet(\"/tmp/to_parquet/\")\n\n# Reload normally and it has 2 partitions; mapping gives 2 outputs\nreloaded = dd.read_parquet(\"/tmp/to_parquet\")\nprint(f\"{reloaded.map_partitions(len).compute()}\")\n# Output: \n# 0    6\n# 1    6\n# dtype: int64\n\n# Reload with empty columns, or reload and then subset to no columns, and suddenly mapping gives 1 output\nprint(f\"{reloaded[[]].map_partitions(len).compute()}\")\nrereloaded = dd.read_parquet(\"/tmp/to_parquet\", columns=[])\nprint(f\"{rereloaded.npartitions=}\")\nprint(f\"{rereloaded.map_partitions(len).compute()}\")\n# Output:\n# 0    12\n# dtype: int64\n# rereloaded.npartitions=2\n# 0    12\n# dtype: int64\n```\nEven though the number of partitions is still listed as 2, mapping gives just a single partition w/ all the rows.\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: `dask, version 2025.2.0`\n- Python version: `Python 3.10.16`\n- Operating System: MacOS\n- Install method (conda, pip, source): uv pip\n",
    "comments": [
      {
        "user": "rjzamora",
        "body": "Thanks for raising @bnaul !\n\nThe meaning of `DataFrame.npartitions` has changed a bit since `dask.dataframe` added query-planning optimizations. The `npartitions` attribute will only tell you the partition count **before** optimizations have been applied. In your example, the optimizer will automatically resize the partitions (described a bit [here](https://docs.dask.org/en/stable/dataframe-optimizer.html)) when `compute` is called.\n\nYou can explicitly check the partition count with `rereloaded.map_partitions(len).optimize().npartitions`, which should return `1` in this case.\n\nI actually don't think there is any easy way to \"turn off\" this optimization right now. You basically need to avoid the `dd.read_parquet` API. E.g.\n\n```python\nrereloaded = dd.from_map(pd.read_parquet, glob.glob(\"/tmp/to_parquet/*.parquet\"), columns=[])\n```\n\nI suppose we could add an easier way to disable this behavior if there is a need.\n"
      },
      {
        "user": "bnaul",
        "body": "Thanks @rjzamora, I haven't interacted with the optimizer much so this was unexpected behavior to me but sounds like it shouldn't be a surprise."
      }
    ]
  },
  {
    "issue_number": 11693,
    "title": "Performance regression from v2024.7.1 to 2024.8.0 when resampling in xarray",
    "author": "brendan-m-murphy",
    "state": "open",
    "created_at": "2025-01-23T12:06:41Z",
    "updated_at": "2025-03-17T02:05:08Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "**Describe the issue**:\n\nResampling and aggregating a dask array (via xarray) is much slower in dask 2024.8.0 than in 2024.7.1 if there are large gaps in the data.\n\nThis seems like more of an xarray issue, but the regression happens when changing the dask version, so I thought asking here might help figure out what changed in that release.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nda = xr.DataArray(np.arange(1000), coords={\"time\": pd.date_range(\"2025-01-01\", freq=\"1s\", periods=1000)})\n\n# add a big gap in the middle of the timeseries\nda2 = xr.concat([da, da.assign_coords(time=da.time + pd.Timedelta(\"3000D\"))], dim=\"time\")\n\n# this took about 2.4ms:\nda2.resample(time=\"1h\").sum()\n\n\nda2_chunked = da2.chunk(time=100)\n\n# this took about 550ms:\nda2_chunked.resample(time=\"1h\").sum()\n```\n\n**Anything else we need to know?**:\n\nWithout the gap, the slow down with dask is about 2x, which seem reasonable.\n\nWith dask 2024.7.1, in either case, the slow down with dask about 2x (i.e. the gap doesn't matter).\n\nThis is without `flox`. Using `flox` seemed to make the aggregation even slower (about 35s for a single run; using `%timeit` was taking a long time...)\n\nThe slowdown with `flox` is not present when using dask 2024.7.1\n\nIf it helps (for someone who knows dask and xarray well): the `DataArrayResample` object created seems to know that there are only two non-empty time periods. This is the repr of that object:\n\n```\n<DataArrayResample, grouped over 1 grouper(s), 72001 groups in total:\n    '__resample_dim__': 2/72001 groups present with labels 2025-01-01, 2033-03-20>\n```\n\nThere is a slightly more complicated example similar to this in our CI that is causing timeouts, so it taking minutes to run, when previously it took seconds at most.  I can try to simplify that if it will help.\n\nAlso, I can look at the dashboard or save a report if that helps.\n\n**Environment**:\n\n- Dask version: 2024.8.0\n- Python version: 3.10.14\n- Operating System: Mac OSX 15.2\n- Install method (conda, pip, source): pip\n\nThe same problem occurs on our CI, which uses Ubuntu and python versions 3.10, 3.11, and 3.12\n\nI ran the example using xarray 2025.1.1.\n",
    "comments": [
      {
        "user": "brendan-m-murphy",
        "body": "Some more info from dask `Profiler`:\n\nFor Dask 2024.7.1:\n\nWith flox:\n\n```\n                                                    mean duration (ms)  count  total (ms)  total (s)\ntask                                                                                          \n_simple_combine                                               0.800250      6    4.801498   0.004801\nconcatenate-getitem-groupby_sum-groupby_sum-sim...            0.035959      1    0.035959   0.000036\nconcatenate-groupby_sum-groupby_sum-simple-redu...            0.185208      1    0.185208   0.000185\ngetitem                                                       0.538771      2    1.077542   0.001078\ngroupby_sum-chunk-groupby-cohort-2508469cdfcccf...            0.657867     10    6.578666   0.006579\ngroupby_sum-chunk-groupby-cohort-e9db06258bae7e...            0.709146     10    7.091458   0.007091\nreindex_intermediates                                         2.178000     20   43.560001   0.043560\nsetitem                                                       0.790250      1    0.790250   0.000790\n```\n\nWithout flox:\n\n```\n                                                    mean duration (ms)  count  total (ms)  total (s)\ntask                                                                                          \nCompose                                                       0.198007      6    1.188043   0.001188\ngetitem                                                       0.167896      2    0.335792   0.000336\nstack-sum-aggregate-getitem-929a5114f7e8bee472a...            0.056230      2    0.112459   0.000112\nsubgraph_callable-1c14ef043d272b4d33d5168eab1988d8            0.324817     10    3.248167   0.003248\nsubgraph_callable-4d7fb342ae8c0f5cbd4d5c1712b9a18b            0.240696     10    2.406958   0.002407\nsubgraph_callable-a2a95171008037d8c2f54405988b486b            0.116479      2    0.232958   0.000233\nxarray-<this-array>-1a447f2e2ec9a2e5453ac89a071...            0.269150     20    5.383000   0.005383\n```\n\nFor Dask 2024.8.0:\n\nWith flox:\n```\n                                                mean duration (ms)  count  total (ms)  total (s)\ntask                                                                                                                                                                               \nconcatenate-getitem-groupby_sum-groupby_sum-sim...      0.174625      1      0.174625   0.000175\ngetitem-setitem-c30136337b5ac86fc5503eb249720ff6        0.177417      1      0.177417   0.000177\nsubgraph_callable-06c7a9c8158e083d11934318bef4b601      0.675333      1      0.675333   0.000675\ngroupby_sum-chunk-groupby-cohort-2508469cdfcccf...      0.170858     10      1.708582   0.001709\n_simple_combine                                         0.690493      6      4.142959   0.004143\nconcatenate-groupby_sum-simple-reduce-aggregate...     25.750250      1     25.750250   0.025750\ngroupby_sum-chunk-groupby-cohort-e9db06258bae7e...      3.102583     10     31.025834   0.031026\nreindex_intermediates                                   4.255640     20     85.112792   0.085113\ngetitem                                                 0.164064  72001  11812.760328  11.812760\nsetitem                                                 0.181715  71999  13083.275103  13.083275\n```\n\nWithout flox:\n```\n                                                mean duration (ms)  count  total (ms)  total (s)\ntask                                                                                          \nstack-sum-aggregate-getitem-929a5114f7e8bee472a...      0.104334      1      0.104334   0.000104\nCompose                                                 0.213201      6      1.279208   0.001279\nsubgraph_callable-4d7fb342ae8c0f5cbd4d5c1712b9a18b      0.238017     10      2.380166   0.002380\nsubgraph_callable-1c14ef043d272b4d33d5168eab1988d8      0.423021     10      4.230208   0.004230\nsum-aggregate-stack-9354e7003050af7a7be4d572ddb...     37.032542      1     37.032542   0.037033\nxarray-<this-array>-1a447f2e2ec9a2e5453ac89a071...      4.400025     20     88.000497   0.088000\ngetitem                                                 0.159698  72002  11498.570423  11.498570\nsubgraph_callable-971c29b2f9ffa0fac043d113ffdf681a      0.325062  72001  23404.788371  23.404788\n```\n\nFrom the `DataArrayResample` trace above, it looks like with dask 2024.8.0, all 72000 resampling periods are processed, while with dask 2024.7.1, only the two non-empty resampling periods are processed."
      },
      {
        "user": "phofl",
        "body": "Thanks for digging in here, there are a couple of things going on, what you found in your last post definitely looks like a bug. I'll have to check this real quick and then will get back to you"
      },
      {
        "user": "brendan-m-murphy",
        "body": "Hmm, so with dask 2024.8.0, `DataArrayGroupByBase._combine` returns a dask array with length 72001 and chunksize 1, while with dask 2024.7.1, it returns an array with chunk size 72000:\n\n2024.8.0:\n```\n> /Users/bm13805/Documents/openghg/.venv_new/lib/python3.10/site-packages/xarray/core/groupby.py(1539)_combine()\n   1537             combined = combined.assign_coords(self.encoded.coords)\n   1538         combined = self._maybe_unstack(combined)\n-> 1539         combined = self._maybe_restore_empty_groups(combined)\n   1540         return combined\n   1541 \n\nipdb> p combined\n<xarray.DataArray (__resample_dim__: 2)> Size: 16B\ndask.array<stack, shape=(2,), dtype=int64, chunksize=(1,), chunktype=numpy.ndarray>\nCoordinates:\n  * __resample_dim__  (__resample_dim__) datetime64[ns] 16B 2025-01-01 2033-0...\nipdb> n\n> /Users/bm13805/Documents/openghg/.venv_new/lib/python3.10/site-packages/xarray/core/groupby.py(1540)_combine()\n   1538         combined = self._maybe_unstack(combined)\n   1539         combined = self._maybe_restore_empty_groups(combined)\n-> 1540         return combined\n   1541 \n   1542     def reduce(\n\nipdb> p combined\n<xarray.DataArray (__resample_dim__: 72001)> Size: 576kB\ndask.array<where, shape=(72001,), dtype=float64, chunksize=(1,), chunktype=numpy.ndarray>\nCoordinates:\n  * __resample_dim__  (__resample_dim__) datetime64[ns] 576kB 2025-01-01 ... ...\n```\n\n2024.7.1:\n```\n> /Users/bm13805/Documents/openghg/.venv_new/lib/python3.10/site-packages/xarray/core/groupby.py(1539)_combine()\n   1537             combined = combined.assign_coords(self.encoded.coords)\n1  1538         combined = self._maybe_unstack(combined)\n-> 1539         combined = self._maybe_restore_empty_groups(combined)\n   1540         return combined\n   1541 \n\nipdb> p combined\n<xarray.DataArray (__resample_dim__: 2)> Size: 16B\ndask.array<stack, shape=(2,), dtype=int64, chunksize=(1,), chunktype=numpy.ndarray>\nCoordinates:\n  * __resample_dim__  (__resample_dim__) datetime64[ns] 16B 2025-01-01 2033-0...\nipdb> n\n> /Users/bm13805/Documents/openghg/.venv_new/lib/python3.10/site-packages/xarray/core/groupby.py(1540)_combine()\n1  1538         combined = self._maybe_unstack(combined)\n   1539         combined = self._maybe_restore_empty_groups(combined)\n-> 1540         return combined\n   1541 \n   1542     def reduce(\n\nipdb> p combined\n<xarray.DataArray (__resample_dim__: 72001)> Size: 576kB\ndask.array<where, shape=(72001,), dtype=float64, chunksize=(72000,), chunktype=numpy.ndarray>\nCoordinates:\n  * __resample_dim__  (__resample_dim__) datetime64[ns] 576kB 2025-01-01 ... ...\n```\n\nThe method `_maybe_restore_empty_groups` also took a few seconds to return with 2024.8.0, while it was instantaneous for 2024.7.1.\n\n(This is without `flox`.)"
      }
    ]
  },
  {
    "issue_number": 11708,
    "title": "Mypy error with latest version of _backends.py",
    "author": "777arc",
    "state": "open",
    "created_at": "2025-01-31T01:20:19Z",
    "updated_at": "2025-03-17T02:05:07Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "FYI mypy is giving the following error with the latest version of dask, which it didn't with a year-old version:\n`dask/dataframe/dask_expr/_backends.py:129: error: Condition can't be inferred, unable to merge overloads`\n\nFeel free to close if it's not of significance",
    "comments": [
      {
        "user": "phofl",
        "body": "our ci is green here, can you elaborate how this is happening?"
      }
    ]
  },
  {
    "issue_number": 11711,
    "title": "Add `device` and `mT` attributes to `Array` for Python array API standard compliance",
    "author": "mdhaber",
    "state": "open",
    "created_at": "2025-01-31T19:56:11Z",
    "updated_at": "2025-03-17T02:05:06Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "The array API standard [specifies](https://data-apis.org/array-api/latest/API_specification/array_object.html#attributes) that arrays will have `device` and `mT` attributes, but these don't seem to be attributes of Dask arrays yet. I'm aware of workarounds in the meantime, but please consider adding them.\n\nThis came up in the context of [`marray`](https://github.com/mdhaber/marray/), which adds mask support to array API features of any array API compliant backend.\n\nCrossref gh-8917.",
    "comments": []
  },
  {
    "issue_number": 11715,
    "title": "Core Dump Error When Calling Numba Functions with map_overlap()",
    "author": "yihang97",
    "state": "open",
    "created_at": "2025-02-05T11:13:41Z",
    "updated_at": "2025-03-17T02:05:05Z",
    "labels": [
      "dataframe",
      "needs attention",
      "bug"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nCalling numba wrapped functions with map_overlap will cause the core dump issue, if run in jupyter notebook then the kernel will die. This is associated with numpy structured array. Calling with naive numpy array seems fine. This issue is not occuring everytime, but re-run for 3-5 times should occurs, error info can be:\n```\nfree(): invalid pointer\nAborted (core dumped)\n```\nor segment fault. \n\nInterestingly if change map_overlap() to map_partitions() then the error won't occur. \n\n\n**Minimal Complete Verifiable Example**:\n\n\n```\nimport os\nimport dask\nfrom distributed import Client, LocalCluster\nimport numpy as np\nimport pandas as pd\nfrom numba import njit    \nfrom IPython import embed\nimport dask.dataframe as dd\n\n@njit(nogil=True)\ndef other_numba_func(array, idx, val):\n    array['Y'][idx] = val\n\n@njit(nogil=True)\ndef numba_func(arr, params, array, other_numba_func):\n    other_numba_func(array, 0, arr['Z'][0])\n    return array\n\ndef get_structured_array(num_rows):\n    dtype = [\n        ('X', np.float64),\n        ('A', np.float64),\n        ('B', np.float64),\n        ('Y', np.float64)\n    ]\n    ret = np.zeros(num_rows, dtype=dtype)\n    return ret\n\ndef call_numba_func(df: pd.DataFrame, **kwargs):\n    params = pd.DataFrame(kwargs, index=[0]).to_records()[0]\n    array = get_structured_array(df.shape[0])\n    array = numba_func(df.to_records(), params, array, other_numba_func)\n    df['X'] = array['X'].astype(np.float32)\n    return df\n\nif __name__ == '__main__':\n    client = Client(\n        LocalCluster(n_workers=1, threads_per_worker=8, dashboard_address=18787),\n        set_as_default=True\n    )\n    data = {\n        'Z': [i for i in range(1000)],\n    }\n    \n    df = pd.DataFrame(data)\n    ddf = dd.from_pandas(df, npartitions=2)\n    ddf.map_overlap(call_numba_func, 0, 0).head()\n    \n    client.shutdown()\n```\n\n**Environment**:\n\nThis is my environment:\n```\nPython 3.11.10 \ndask                      2025.1.0                 pypi_0    pypi\ndask-cloudprovider        2024.9.1                 pypi_0    pypi\ndask-expr                 1.1.20             pyhd8ed1ab_0    conda-forge\ndask-labextension         7.0.0              pyhd8ed1ab_0    conda-forge\nnumba                     0.61.0                   pypi_0    pypi\nnumpy                     2.1.0                    pypi_0    pypi\npandas                    2.2.3           py311h7db5c69_1    conda-forge\n\nDescription:    Ubuntu 24.04.1 LTS\nRelease:        24.04\nCodename:       noble\n```\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Thanks for the report. Investigations welcome here"
      },
      {
        "user": "guillaumeeb",
        "body": "Just to say, I was able to reproduce the problem, but had to add a loop for running the dataframe computation again and again:\n\n```python\nfor i in range(1000):\n    df = pd.DataFrame(data)\n    ddf = dd.from_pandas(df, npartitions=2)\n    ddf.map_overlap(call_numba_func, 0, 0).head()\n```\n\nNo hint on how to investigate this though."
      }
    ]
  },
  {
    "issue_number": 11716,
    "title": "dask.array.from_npy_stack doesn't support manually created chunks",
    "author": "adrian-gralak95",
    "state": "open",
    "created_at": "2025-02-05T16:24:46Z",
    "updated_at": "2025-03-17T02:05:04Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "I know this issue has been mentioned before ([here](https://stackoverflow.com/questions/50624710/dask-array-from-npy-stack-misses-info-file),  @martindurant , but seems nothing came out of it in the end. \n\nHappy to refactor into sth like this (if someone could comment if this looks good I'll issue a PR 😃 ) - I could also add some custom filename templates as well to make it more cross-compatible.\n\n```python\ndef _from_npy_stack_meta(dirname, dtype, chunks, axis,  mmap_mode=\"r\"):\n    \"\"\"Load dask array from stack of npy files\n    :param dirname: the with the chunks. They must be labeled {ix}.npy\n    :param dtype: the dtype of the arrays\n    :chunks the size of the chunk across each chunk. n-dim tuple where only the axis dimension is non-singleton. E.g. ((10, 10, 5), (10,), (5,)) for 3-dim dask array with 3 chunks, split along axis 0.\n    :axis: the axis along the chunks have been saved.\n    \"\"\"\n    name = \"from-npy-stack-%s\" % dirname\n    keys = list(product([name], *[range(len(c)) for c in chunks]))\n    values = [\n        (np.load, os.path.join(dirname, \"%d.npy\" % i), mmap_mode)\n        for i in range(len(chunks[axis]))\n    ]\n    dsk = dict(zip(keys, values))\n\n    return Array(dsk, name, chunks, dtype)\n\ndef from_npy_stack(dirname, mmap_mode=\"r\"):\n    \"\"\"Load dask array from stack of npy files\n\n    Parameters\n    ----------\n    dirname: string\n        Directory of .npy files\n    mmap_mode: (None or 'r')\n        Read data in memory map mode\n\n    See Also\n    --------\n    to_npy_stack\n    \"\"\"\n    with open(os.path.join(dirname, \"info\"), \"rb\") as f:\n        info = pickle.load(f)\n\n    return _from_npy_stack_meta(dirname, info[\"dtype\"], info[\"chunks\"], info[\"axis\"], mmap_mode)\n```\n\n\n\n",
    "comments": [
      {
        "user": "martindurant",
        "body": "I support trying to make this better, although I'm not sure how much use it gets.\n\nIt's worth mentioning that you are constructing all the tasks beforehand explicitly here (\"low-level graph\") and it should be possible to do this using a high-level Layer that instead encodes how to make the tasks across a set of inputs. That won't matter for a relatively small number of items, so any improvement would be appreciated."
      },
      {
        "user": "adrian-gralak95",
        "body": "right -  perhaps might be helpful to provide my use case, I am doing interop between numba chunk processor and dask, hence why I couldn't automatically write the chunks from dask. I understand if the use case is a bit obscure, but either way I'd be up for improving the info file documentation in case people want to replicate my work - I'll issue a PR for review with slightly polished up version of the above :)"
      }
    ]
  },
  {
    "issue_number": 11718,
    "title": "Processes scheduler runs `map_blocks` serially",
    "author": "ericpre",
    "state": "open",
    "created_at": "2025-02-05T20:11:06Z",
    "updated_at": "2025-03-17T02:05:03Z",
    "labels": [
      "scheduler",
      "needs attention",
      "bug"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nIt seems that the _processes_ scheduler doesn't run in parallel when using `map_blocks`.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nfrom time import sleep\n\nimport dask.array as da\nfrom dask.diagnostics import ProgressBar\n\n\ndef func(data):\n    sleep(10)\n    return data * 2\n\n\nif __name__ == '__main__':\n\n    # 4 chunks\n    array = da.ones((100, 100), chunks=(25, -1))\n    out = array.map_blocks(func, meta=array)\n\n    print(\"using threads scheduler\")\n    with ProgressBar():\n        out2 = out.compute(scheduler='threads')\n\n    print(\"using processes scheduler\")\n    with ProgressBar():\n        out3 = out.compute(scheduler='processes')\n```\nThe _threads_ and _processes_ scheduler will take about 10s and 40s, respectively. Howoever, in the following example (using dask delayed), both takes similar times: ~10-11s.\n```python\nfrom time import sleep\n\nimport dask\nimport dask.array as da\nfrom dask.diagnostics import ProgressBar\nimport numpy as np\n\n\ndef func(data):\n    sleep(10)\n    return data * 2\n\n\nif __name__ == '__main__':\n\n    # 4 task\n    delayed_out = [dask.delayed(func, pure=True)(data_) for data_ in [np.ones((100, 100))]*4]\n    arrays = [\n        da.from_delayed(delayed_out_, dtype=float, shape=(100, 100))\n        for delayed_out_ in delayed_out\n        ]\n    out = da.stack(arrays, axis=0)\n\n    print(\"using threads scheduler\")\n    with ProgressBar():\n        out2 = out.compute(scheduler='threads')\n\n    print(\"using processes scheduler\")\n    with ProgressBar():\n        out3 = out.compute(scheduler='processes')\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.1.0\n- Python version: 3.12\n- Operating System: windows 10\n- Install method (conda, pip, source): conda\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Thanks for the report. I would generally recommend spinning up a LocalCluster instead, the scheduler is significantly more powerful than the processes thing. Investigations are welcome nevertheless, but we don't have resources to dig in here"
      },
      {
        "user": "ericpre",
        "body": "Thank you @phofl for your recommendation. A quick try shows that a local cluster doesn't have the issue:\n\n```python\nfrom time import sleep, perf_counter\n\nimport dask.array as da\nfrom dask.distributed import Client, LocalCluster, progress\n\ndef func(data):\n    sleep(10)\n    return data * 2\n\n\nif __name__ == '__main__':\n\n    cluster = LocalCluster()\n    client = Client(cluster)\n\n    # 4 chunks\n    array = da.ones((100, 100), chunks=(25, -1))\n    out = array.map_blocks(func, meta=array)\n\n    print(\"using local cluster\")\n    t0 = perf_counter()\n    futures = client.compute(out)\n    progress(futures)\n    t1 = perf_counter()\n\n    time_multiprocessing_1 = t1 - t0\n    print(\"duration:\", time_multiprocessing_1)\n```"
      }
    ]
  },
  {
    "issue_number": 11730,
    "title": "cupy: `__setitem__` with int array index fails when chunked",
    "author": "crusaderky",
    "state": "open",
    "created_at": "2025-02-07T11:46:23Z",
    "updated_at": "2025-03-17T02:05:02Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "`x.__setitem__(idx, )`, where x is a dask array with cupy meta and idx is a dask array of integers, returns incorrect results when there are multiple chunks:\n\n```python\ndef test_setitem_1d():\n    x = cupy.arange(10)\n    idx = cupy.asarray([2, 3])\n    dx = da.from_array(x.copy(), chunks=(5,))\n\n    x[idx,] = -1\n    dx[da.from_array(idx)] = -1\n\n    assert_eq(x, dx)\n```\n```\nAssertionError: found values in 'a' and 'b' which differ by more than the allowed amount\nE           assert False\nE            +  where False = allclose(array([ 0,  1, -1, -1,  4,  5,  6,  7,  8,  9]), array([ 0,  1, -1, -1,  4, -1,  6,  7,  8,  9]), equal_nan=True, **{})\n```\nthe issue disappear if I set `chunks=-1`.\n\nNote that this is both upstream and downstream of #11728.",
    "comments": [
      {
        "user": "crusaderky",
        "body": "https://github.com/dask/dask/issues/11728 adds an XFAILed unit test for this"
      }
    ]
  },
  {
    "issue_number": 2516,
    "title": "to_parquet fails while to_csv works (distributed backend)",
    "author": "redst4r",
    "state": "closed",
    "created_at": "2017-07-10T18:56:58Z",
    "updated_at": "2025-03-15T16:26:54Z",
    "labels": [],
    "body": "Hi,\r\n\r\nI have some trouble saving results with `to_parquet` in the distributed backend (on the filesystem of the worker nodes. Note that the filesystem is shared across workers). \r\nHere's an example:\r\n\r\n```python\r\nimport numpy as np\r\nfrom dask.distributed import Client\r\nimport dask.dataframe as dd\r\nimport dask\r\nimport random\r\nimport pandas as pd\r\n\r\nclient = Client('xx.xx.xx.xx:8786')  # connect to remote scheduler\r\nremote_folder = '/users/myaccount/dask_testfolder/'\r\n\r\n@dask.delayed\r\ndef create_content(r,c):\r\n    return pd.DataFrame(np.random.rand(r,c))\r\n\r\nX = create_content(10000,100)\r\ndf = dd.from_delayed(X)\r\n\r\n# this works fine; creates a single file /users/myaccount/dask_testfolder/0.part\r\ndf.to_csv(remote_folder)\r\n\r\n#this fails\r\ndf.to_parquet(remote_folder)\r\n```\r\nHere's the error thrown by `to_parquet`:\r\n```\r\n---------------------------------------------------------------------------\r\nPermissionError                           Traceback (most recent call last)\r\n/anaconda3/envs/py36/lib/python3.6/site-packages/dask/bytes/local.py in mkdirs(self, path)\r\n     43         try:\r\n---> 44             os.makedirs(path)\r\n     45         except OSError:\r\n\r\n/anaconda3/envs/py36/lib/python3.6/os.py in makedirs(name, mode, exist_ok)\r\n    209         try:\r\n--> 210             makedirs(head, mode, exist_ok)\r\n    211         except FileExistsError:\r\n\r\n/anaconda3/envs/py36/lib/python3.6/os.py in makedirs(name, mode, exist_ok)\r\n    209         try:\r\n--> 210             makedirs(head, mode, exist_ok)\r\n    211         except FileExistsError:\r\n\r\n/anaconda3/envs/py36/lib/python3.6/os.py in makedirs(name, mode, exist_ok)\r\n    209         try:\r\n--> 210             makedirs(head, mode, exist_ok)\r\n    211         except FileExistsError:\r\n\r\n/anaconda3/envs/py36/lib/python3.6/os.py in makedirs(name, mode, exist_ok)\r\n    219     try:\r\n--> 220         mkdir(name, mode)\r\n    221     except OSError:\r\n\r\nPermissionError: [Errno 13] Permission denied: '/users'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-5-b0722763efbb> in <module>()\r\n     20 \r\n     21 #this fails\r\n---> 22 df.to_parquet(remote_folder+'/subdoldwe')\r\n\r\n/anaconda3/envs/py36/lib/python3.6/site-packages/dask/dataframe/core.py in to_parquet(self, path, *args, **kwargs)\r\n    953         \"\"\" See dd.to_parquet docstring for more information \"\"\"\r\n    954         from .io import to_parquet\r\n--> 955         return to_parquet(path, self, *args, **kwargs)\r\n    956 \r\n    957     def to_csv(self, filename, **kwargs):\r\n\r\n/anaconda3/envs/py36/lib/python3.6/site-packages/dask/dataframe/io/parquet.py in to_parquet(path, df, compression, write_index, has_nulls, fixed_text, object_encoding, storage_options, append, ignore_divisions, partition_on, compute)\r\n    361     fs, paths, myopen = get_fs_paths_myopen(path, None, 'wb',\r\n    362                                             **(storage_options or {}))\r\n--> 363     fs.mkdirs(path)\r\n    364     sep = fs.sep\r\n    365     metadata_fn = sep.join([path, '_metadata'])\r\n\r\n/anaconda3/envs/py36/lib/python3.6/site-packages/dask/bytes/local.py in mkdirs(self, path)\r\n     44             os.makedirs(path)\r\n     45         except OSError:\r\n---> 46             assert os.path.isdir(path)\r\n     47 \r\n     48     def open(self, path, mode='rb', **kwargs):\r\n```\r\n\r\nI'm not sure what's going on, but I have the feeling that dask somehow tries to write the parquet files on my local machine (where I execute the above code) instead of on the remote workers.\r\nOn my machine the `remote_folder` doesnt exist (e.g. /users/) and that's why it fails with permission errors.\r\n\r\nWhat puzzles me is that `to_csv` works flawless and creates the .csv on the workers!\r\n\r\nAs a sidenote: There's similar issues when I try loading data located on the worker file-system via read_csv:\r\n```\r\ndd.read_csv(remote_folder+'/*')\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-8-331a662ae29b> in <module>()\r\n----> 1 dd.read_csv(remote_folder+'/*')\r\n\r\nanaconda3/envs/py36/lib/python3.6/site-packages/dask/dataframe/io/csv.py in read(urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\r\n    338                            enforce=enforce, assume_missing=assume_missing,\r\n    339                            storage_options=storage_options,\r\n--> 340                            **kwargs)\r\n    341     read.__doc__ = READ_DOC_TEMPLATE.format(reader=reader_name,\r\n    342                                             file_type=file_type)\r\n\r\nanaconda3/envs/py36/lib/python3.6/site-packages/dask/dataframe/io/csv.py in read_pandas(reader, urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\r\n    218                                   sample=sample,\r\n    219                                   compression=compression,\r\n--> 220                                   **(storage_options or {}))\r\n    221 \r\n    222     if not isinstance(values[0], (tuple, list)):\r\n\r\nanaconda3/envs/py36/lib/python3.6/site-packages/dask/bytes/core.py in read_bytes(urlpath, delimiter, not_zero, blocksize, sample, compression, **kwargs)\r\n    137     client = None\r\n    138     if len(paths) == 0:\r\n--> 139         raise IOError(\"%s resolved to no files\" % urlpath)\r\n    140 \r\n    141     if blocksize is not None:\r\n\r\nOSError: /users/myaccount/dask_testfolder//* resolved to no files\r\n```\r\n\r\nI all seems to boil down to: How do I tell dask to look at the remote workers' filesystem instead of my local one??\r\n\r\n",
    "comments": [
      {
        "user": "martindurant",
        "body": "This is not unexpected: when you do `to_parquet`, each worker writes files from the chunks of data, but the metadata file is written from your local process. One workaround would be to use `to_parquet(..., compute=False)`, which will give you back a delayed object, and then call `compute()` on this object - then the final write should happen in a worker too.\r\n\r\nSimilarly for the read_csv case, the file-list is generated in the local process. You could maybe execute the file listing on a worker like\r\n\r\n    files = client.gather(client.submit(glob.blog, filepath))\r\n\r\nand pass the list of files instead of the glob string to read_csv.\r\n"
      },
      {
        "user": "redst4r",
        "body": "Thx for your help!\r\n\r\n```python\r\ndelayed_write= df.to_parquet(remote_folder, compute=False)\r\n```\r\ngives me the same error as before (even without calling `compute()` afterwards).\r\n\r\n```python\r\nfiles = client.gather(client.submit(glob.glob, remote_folder+'/*.csv'))\r\ndd.read_csv(files)\r\n```\r\nfails with \r\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-156-793138575d1c> in <module>()\r\n----> 1 df_read_csv= dd.read_csv(files)\r\n\r\nanaconda3/envs/py36/lib/python3.6/site-packages/dask/dataframe/io/csv.py in read(urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\r\n    338                            enforce=enforce, assume_missing=assume_missing,\r\n    339                            storage_options=storage_options,\r\n--> 340                            **kwargs)\r\n    341     read.__doc__ = READ_DOC_TEMPLATE.format(reader=reader_name,\r\n    342                                             file_type=file_type)\r\n\r\nanaconda3/envs/py36/lib/python3.6/site-packages/dask/dataframe/io/csv.py in read_pandas(reader, urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\r\n    218                                   sample=sample,\r\n    219                                   compression=compression,\r\n--> 220                                   **(storage_options or {}))\r\n    221 \r\n    222     if not isinstance(values[0], (tuple, list)):\r\n\r\nanaconda3/envs/py36/lib/python3.6/site-packages/dask/bytes/core.py in read_bytes(urlpath, delimiter, not_zero, blocksize, sample, compression, **kwargs)\r\n    156         for path in paths:\r\n    157             try:\r\n--> 158                 size = fs.logical_size(path, compression)\r\n    159             except KeyError:\r\n    160                 raise ValueError('Cannot read compressed files (%s) in byte chunks,'\r\n\r\nanaconda3/envs/py36/lib/python3.6/site-packages/dask/bytes/core.py in logical_size(self, path, compression)\r\n    506             compression = infer_compression(path)\r\n    507         if compression is None:\r\n--> 508             return self.size(path)\r\n    509         else:\r\n    510             with self.open(path, 'rb') as f:\r\n\r\nanaconda3/envs/py36/lib/python3.6/site-packages/dask/bytes/local.py in size(self, path)\r\n     69         \"\"\"Size in bytes of the file at path\"\"\"\r\n     70         path = self._trim_filename(path)\r\n---> 71         return os.stat(path).st_size\r\n     72 \r\n     73 \r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '/users/myaccount/dask_testfolder/3.csv'\r\n```\r\nthe `files`-list has the expected content however (first entry being `/users/myaccount/dask_testfolder/3.csv`)\r\n\r\nThe cheap way out of this would be to start the \"client\" session also on the worker/scheduler node, but I guess that's not really the point...\r\n\r\nI wonder how it works for \"truely remote\" content, e.g. data on S3. Here dask also seem to know that it shouldn't look for the data on the local machine.\r\n\r\n\r\n"
      },
      {
        "user": "martindurant",
        "body": "Oh I see - it's trying to make the directory to write into. You could monkey-patch dask.bytes.local.LocalFileSystem.mkdirs to a no-op, and make sure that the directory does exist for your workers... inelegant, but it may work.\r\nEven S3, GCS et al., only work if both the client and the workers can access the remote source, which often means moving credentials around. I was not aware that this was a pain for anyone - it would be possible to do all of the current client logic in a worker, but it would take effort to implement."
      }
    ]
  },
  {
    "issue_number": 7673,
    "title": "da.from_zarr ignores Zarr's dimension_separator",
    "author": "joshmoore",
    "state": "closed",
    "created_at": "2021-05-19T08:59:04Z",
    "updated_at": "2025-03-13T08:56:09Z",
    "labels": [
      "array",
      "io",
      "needs attention"
    ],
    "body": "**What happened**:\r\n\r\nZarr arrays that have been stored with \"/\" as the `dimension_separator` attribute (in .zarray) have no data in them when opened with `da.from_zarr`. See https://github.com/zarr-developers/zarr-python/pull/715\r\n\r\n**What you expected to happen**:\r\n\r\nAll Zarr stores in the zarr-python repository were updated in https://github.com/zarr-developers/zarr-python/pull/716 to read this new metadata. The two options I can think of are either using a full store like FSStore in `from_zarr` or adding support for `dimension_separator` directly.\r\n\r\ncc: @martindurant \r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport zarr as zr\r\nimport dask.array as da\r\n\r\nz = zr.storage.FSStore(\r\n    \"foo.zarr\",\r\n    dimension_separator=\"/\",\r\n    auto_mkdir=True,\r\n    mode=\"w\",\r\n)\r\na = zr.creation.array(store=z, data=[range(4) for x in range(4)])\r\n\r\n# Passes\r\nbase = da.from_zarr(z, \"\")\r\nm = base.max().compute()\r\nassert m == 3, m\r\n\r\n# Fails\r\nbase = da.from_zarr(\"foo.zarr\")\r\nm = base.max().compute()\r\nassert m == 3, m\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nOriginally reported in: https://github.com/zarr-developers/zarr-python/pull/709#issuecomment-834118324\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2021.5.0\r\n- Python version: 3.9.4\r\n- Operating System: OSX 11.2.3\r\n- Install method (conda, pip, source):\r\n```\r\nconda create -n dask dask zarr fsspec\r\n```\r\n",
    "comments": [
      {
        "user": "martindurant",
        "body": "In the end, I think we've decided that this can be solved elsewhere, so I suggest this issue be closed."
      },
      {
        "user": "joshmoore",
        "body": "@martindurant: I may have missed something. What's the solution?"
      },
      {
        "user": "martindurant",
        "body": "I though to either pass the options to, and implement the logic in fsstore, or make a new store that drives from it. I didn't mean to imply that it was solved, only that it could be. Or did I misunderstand and this really is Dask's fault?"
      }
    ]
  },
  {
    "issue_number": 11691,
    "title": "Errors with Zarr v3 and da.to_zarr()",
    "author": "will-moore",
    "state": "closed",
    "created_at": "2025-01-23T10:44:50Z",
    "updated_at": "2025-03-13T07:20:46Z",
    "labels": [
      "needs triage"
    ],
    "body": "I'm having various issues and errors with `da.to_zarr()` using:\n\n```\ndask==2025.1.0\nzarr==3.0.1\nfsspec==2024.12.0\n```\n\n```\nfrom skimage import data\nimport dask.array as da\nimport zarr\n\ndask_data = da.from_array(data.coins(), chunks=(64, 64))\n\nda.to_zarr(dask_data, \"test_dask_to_zarr.zarr\", compute=True, storage_options={\"chunks\": (64, 64)})\n\n# Traceback (most recent call last):\n#   File \"/Users/wmoore/Desktop/python-scripts/zarr_scripts/test_dask_to_zarr.py\", line 7, in <module>\n#     da.to_zarr(dask_data, \"test_dask_to_zarr.zarr\", compute=True, storage_options={\"chunks\": (64, 64)})\n#   File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/dask/array/core.py\", line 3891, in to_zarr\n#     store = zarr.storage.FsspecStore.from_url(\n#             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#   File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/zarr/storage/_fsspec.py\", line 182, in from_url\n#     return cls(fs=fs, path=path, read_only=read_only, allowed_exceptions=allowed_exceptions)\n#            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#   File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/zarr/storage/_fsspec.py\", line 96, in __init__\n#     raise TypeError(\"Filesystem needs to support async operations.\")\n# TypeError: Filesystem needs to support async operations.\n```\n\nTrying to use a local store has a different error:\n\n```\nstore = zarr.storage.LocalStore(\"test_dask_to_zarr.zarr\", read_only=False)\nda.to_zarr(dask_data, store, compute=True, storage_options={\"chunks\": (64, 64)})\n\n#   File \"/Users/wmoore/Desktop/python-scripts/zarr_scripts/test_dask_to_zarr.py\", line 46, in <module>\n#     da.to_zarr(dask_data, store, compute=True, storage_options={\"chunks\": (64, 64)})\n#   File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/dask/array/core.py\", line 3891, in to_zarr\n#     store = zarr.storage.FsspecStore.from_url(\n#             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#   File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/zarr/storage/_fsspec.py\", line 174, in from_url\n#     fs, path = url_to_fs(url, **opts)\n#                ^^^^^^^^^^^^^^^^^^^^^^\n#   File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/fsspec/core.py\", line 403, in url_to_fs\n#     chain = _un_chain(url, kwargs)\n#             ^^^^^^^^^^^^^^^^^^^^^^\n#   File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/fsspec/core.py\", line 335, in _un_chain\n#     if \"::\" in path:\n#        ^^^^^^^^^^^^\n# TypeError: argument of type 'LocalStore' is not iterable\n```\n\nAnd also tried with FsspecStore:\n```\nstore = zarr.storage.FsspecStore(\"test_dask_to_zarr.zarr\", read_only=False)\nda.to_zarr(dask_data, store, compute=True, storage_options={\"chunks\": (64, 64)})\n\n# Traceback (most recent call last):\n#   File \"/Users/wmoore/Desktop/python-scripts/zarr_scripts/test_dask_to_zarr.py\", line 32, in <module>\n#     store = zarr.storage.FsspecStore(\"test_dask_to_zarr_v3.zarr\", read_only=False)\n#             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#   File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/zarr/storage/_fsspec.py\", line 95, in __init__\n#     if not self.fs.async_impl:\n#            ^^^^^^^^^^^^^^^^^^\n# AttributeError: 'str' object has no attribute 'async_impl'\n```\n\nMany thanks for your help",
    "comments": [
      {
        "user": "joshmoore",
        "body": "> TypeError: Filesystem needs to support async operations.\n\nhttps://github.com/zarr-developers/zarr-python/issues/2554 would imply that you need a new release of zarr-python with https://github.com/zarr-developers/zarr-python/pull/2533\n"
      },
      {
        "user": "will-moore",
        "body": "Thanks @joshmoore \n\nI updated to use `main` branch of `zarr-python`:\n```\n$ pip freeze | grep zarr-python\n-e git+ssh://git@github.com/zarr-developers/zarr-python.git@fb37ff0942c7417a7767cc4d537de7173a9b3329#egg=zarr\n```\n```\ndask==2025.1.0\nfsspec==2024.12.0\n```\n\nThis:\n```\nfrom skimage import data\nimport dask.array as da\n\ndask_data = da.from_array(data.coins(), chunks=(64, 64))\nda.to_zarr(dask_data, \"test_dask_to_zarr.zarr\", compute=True, storage_options={\"chunks\": (64, 64)})\n```\n\nnow gives me:\n\n```\n$ python test_dask_to_zarr.py \nTraceback (most recent call last):\n  File \"/Users/wmoore/Desktop/python-scripts/zarr_scripts/test_dask_to_zarr.py\", line 17, in <module>\n    da.to_zarr(dask_data, \"test_dask_to_zarr.zarr\", compute=True, storage_options={\"chunks\": (64, 64)})\n  File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/dask/array/core.py\", line 3901, in to_zarr\n    z = zarr.create(\n        ^^^^^^^^^^^^\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/api/synchronous.py\", line 707, in create\n    sync(\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/core/sync.py\", line 142, in sync\n    raise return_result\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/core/sync.py\", line 98, in _runner\n    return await coro\n           ^^^^^^^^^^\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/api/asynchronous.py\", line 1044, in create\n    return await AsyncArray._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/core/array.py\", line 610, in _create\n    result = await cls._create_v3(\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/core/array.py\", line 726, in _create_v3\n    await array._save_metadata(metadata, ensure_parents=True)\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/core/array.py\", line 1315, in _save_metadata\n    await gather(*awaitables)\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/abc/store.py\", line 487, in set_or_delete\n    await byte_setter.set(value)\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/storage/_common.py\", line 144, in set\n    await self.store.set(self.path, value)\n  File \"/Users/wmoore/Desktop/ZARR_PYTHON/zarr-python/src/zarr/storage/_fsspec.py\", line 276, in set\n    await self.fs._pipe_file(path, value.to_bytes())\n  File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/fsspec/implementations/asyn_wrapper.py\", line 27, in wrapper\n    return await asyncio.to_thread(func, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/asyncio/threads.py\", line 25, in to_thread\n    return await loop.run_in_executor(None, func_call)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/fsspec/spec.py\", line 787, in pipe_file\n    with self.open(path, \"wb\", **kwargs) as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/fsspec/spec.py\", line 1310, in open\n    f = self._open(\n        ^^^^^^^^^^^\n  File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 200, in _open\n    return LocalFileOpener(path, mode, fs=self, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 364, in __init__\n    self._open()\n  File \"/Users/wmoore/opt/anaconda3/envs/zarrv3_py312/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 369, in _open\n    self.f = open(self.path, mode=self.mode)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/wmoore/Desktop/python-scripts/zarr_scripts/test_dask_to_zarr.zarr/zarr.json'\n```\n"
      },
      {
        "user": "will-moore",
        "body": "The other 2 tests above, using `LocalStore` or `FsspecStore` still have the same errors as before (no change wtih https://github.com/zarr-developers/zarr-python/pull/2533)"
      }
    ]
  },
  {
    "issue_number": 11706,
    "title": "arange crashes for parameters near 2**63",
    "author": "crusaderky",
    "state": "closed",
    "created_at": "2025-01-30T17:07:24Z",
    "updated_at": "2025-03-12T21:11:12Z",
    "labels": [
      "needs triage"
    ],
    "body": "python 3.11\nnumpy 2.2.2\ndask 2025.1.0\n\n```python\n>>> start, stop, step = 0., -9_131_138_316_486_228_481, -92_233_720_368_547_759\n>>> da.arange(start, stop, step)\n\nNotImplementedError: An error occurred while calling the arange method registered to the numpy backend.\nOriginal Message: Can not use auto rechunking with object dtype. We are unable to estimate the size in bytes of object data\n```\n\nThe problem is that da.arange internally multiplies step by 100, which sends it beyond the limit of np.int64, which in turn triggers edge case behaviour in numpy:\n\n```python\n>>> np.arange(start, stop, step * 100)\narray([0.0], dtype=object)\n```\n\nThe last person to ever touch this was... myself, a whopping 7 years ago: https://github.com/dask/dask/pull/3722\n",
    "comments": [
      {
        "user": "crusaderky",
        "body": "#11707 only partially fixed this. This produces wrong results in 2025.2.0:\n```python\n>>> import dask.array as da\n>>> start, stop = -72057594037927945, -72057594037927938\n>>> np.arange(start, stop, 1.0)\narray([-7.2057594e+16, -7.2057594e+16, -7.2057594e+16, -7.2057594e+16,\n       -7.2057594e+16, -7.2057594e+16, -7.2057594e+16])\n>>> da.arange(start, stop, 1).compute()\narray([-72057594037927945, -72057594037927944, -72057594037927943,\n       -72057594037927942, -72057594037927941, -72057594037927940,\n       -72057594037927939])\n>>> da.arange(start, stop, 1.0).compute()\narray([], dtype=float64)\n```"
      }
    ]
  },
  {
    "issue_number": 10069,
    "title": "Improving support for pandas extension dtypes",
    "author": "charlesbluca",
    "state": "open",
    "created_at": "2023-03-15T12:48:51Z",
    "updated_at": "2025-03-12T11:15:15Z",
    "labels": [
      "dataframe",
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please do a quick search of existing issues to make sure that this has not been asked before. -->\r\n\r\nCurrently, support for Pandas extension dtypes in Dask is somewhat patchy, with many operations giving unexpected results or even failing outright; thought it might be worthwhile to gather some of the issues I've come across in one place (will continue updating as I encounter more issues):\r\n\r\nIssues with nullable/pyarrow dtypes\r\n-------------\r\n- https://github.com/dask/dask/issues/6018\r\n- https://github.com/dask/dask/issues/8530\r\n- https://github.com/dask/dask/issues/9401\r\n- https://github.com/dask/dask/issues/9765\r\n- https://github.com/dask/dask/issues/9767\r\n\r\nIssues with categorical dtype\r\n-------------\r\n- https://github.com/dask/dask/issues/5756\r\n- https://github.com/dask/dask/issues/6587\r\n- https://github.com/dask/dask/issues/7015\r\n- https://github.com/dask/dask/issues/9467\r\n- https://github.com/dask/dask/issues/9515\r\n- https://github.com/dask/dask/issues/9966\r\n\r\nSome thoughts that jump out to me:\r\n\r\n- at the time of opening this, it seems like reading/writing categorical columns to parquet is a pretty common issue; I'm interested in if this has come up much in the work that Coiled is doing (from my perspective at RAPIDS, we haven't looked much into Dask's categorical support)\r\n- I would imagine that resolving some of the issues with nullable columns (such as #9765, which is concerned with adding handling for `pd.NA` in places expecting only `np.nan`) would help us move towards wider adoption of pyarrow numeric types, which I know @jrbourbeau had been playing around with\r\n\r\nInterested if anyone else has been thinking about an approach here, or if there are any issues I might've missed.",
    "comments": [
      {
        "user": "dbalabka",
        "body": "@charlesbluca , thanks for the excellent summary. The lack of robust types of support is one of the significant issues we are facing with Dask. It is why we are trying to avoid using the `categorical` type and waste memory using `string` instead.\n\nI found out that there might be an issue with categorical type when dask operates with multiple partitions and high-cardinality categorical values. This happens when various partitions of data already loaded from the parquet are converted from `string` into `categorical`. In this case, Pandas infers the type for each partition separately, which leads to the index type being different for different partitions.\n\nHere is an example:\nhttps://github.com/dask/dask/issues/6587#issuecomment-2717049419"
      }
    ]
  },
  {
    "issue_number": 6587,
    "title": "Unable to write parquet with \"empty\" categorical column with pyarrow engine",
    "author": "odovad",
    "state": "open",
    "created_at": "2020-09-02T14:24:04Z",
    "updated_at": "2025-03-12T11:12:40Z",
    "labels": [
      "dataframe",
      "io",
      "parquet",
      "needs attention"
    ],
    "body": "I encountered an issue when working on a concat involving multiples files.\r\nIt seems that the issue happens also when applied on only one file with a partition containing a categorical column with no values.\r\n\r\nMaybe this issue is linked : [Issue 6243](https://github.com/dask/dask/issues/6243)\r\nIf I am not mistaken, it worked with Pyarrow 0.17.1\r\n\r\n```python\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\n\r\nnames = [ 'name']\r\naddress = names\r\n\r\ndf1 = dd.from_pandas(pd.DataFrame({'name':names}), npartitions=1)\r\ndf2 = dd.from_pandas(pd.DataFrame({'address':address}), npartitions=1)\r\ndf1['name'] = df1['name'].astype('category')\r\n\r\ndf = dd.concat([df1, df2])\r\ndel df['address']\r\n\r\ndf.to_parquet('does_not_work')\r\n```\r\n\r\nThe error that is raised : \r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py in _append_row_groups(metadata, md)\r\n     33     try:\r\n---> 34         metadata.append_row_groups(md)\r\n     35     except RuntimeError as err:\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/_parquet.pyx in pyarrow._parquet.FileMetaData.append_row_groups()\r\n\r\nRuntimeError: AppendRowGroups requires equal schemas.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-46-4faae0e6b3fa> in <module>\r\n     12 del df['address']\r\n     13 \r\n---> 14 df.to_parquet('does_not_work')\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/core.py in to_parquet(self, path, *args, **kwargs)\r\n   3948         from .io import to_parquet\r\n   3949 \r\n-> 3950         return to_parquet(self, path, *args, **kwargs)\r\n   3951 \r\n   3952     @derived_from(pd.DataFrame)\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/core.py in to_parquet(df, path, engine, compression, write_index, append, ignore_divisions, partition_on, storage_options, write_metadata_file, compute, compute_kwargs, schema, **kwargs)\r\n    506         if compute_kwargs is None:\r\n    507             compute_kwargs = dict()\r\n--> 508         out = out.compute(**compute_kwargs)\r\n    509     return out\r\n    510 \r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/base.py in compute(self, **kwargs)\r\n    165         dask.base.compute\r\n    166         \"\"\"\r\n--> 167         (result,) = compute(self, traverse=False, **kwargs)\r\n    168         return result\r\n    169 \r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/base.py in compute(*args, **kwargs)\r\n    445         postcomputes.append(x.__dask_postcompute__())\r\n    446 \r\n--> 447     results = schedule(dsk, keys, **kwargs)\r\n    448     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    449 \r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/threaded.py in get(dsk, result, cache, num_workers, pool, **kwargs)\r\n     74                 pools[thread][num_workers] = pool\r\n     75 \r\n---> 76     results = get_async(\r\n     77         pool.apply_async,\r\n     78         len(pool._pool),\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/local.py in get_async(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\r\n    484                         _execute_task(task, data)  # Re-execute locally\r\n    485                     else:\r\n--> 486                         raise_exception(exc, tb)\r\n    487                 res, worker_id = loads(res_info)\r\n    488                 state[\"cache\"][key] = res\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/local.py in reraise(exc, tb)\r\n    314     if exc.__traceback__ is not tb:\r\n    315         raise exc.with_traceback(tb)\r\n--> 316     raise exc\r\n    317 \r\n    318 \r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/local.py in execute_task(key, task_info, dumps, loads, get_id, pack_exception)\r\n    220     try:\r\n    221         task, data = loads(task_info)\r\n--> 222         result = _execute_task(task, data)\r\n    223         id = get_id()\r\n    224         result = dumps((result, id))\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/core.py in _execute_task(arg, cache, dsk)\r\n    119         # temporaries by their reference count and can execute certain\r\n    120         # operations in-place.\r\n--> 121         return func(*(_execute_task(a, cache) for a in args))\r\n    122     elif not ishashable(arg):\r\n    123         return arg\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/utils.py in apply(func, args, kwargs)\r\n     27 def apply(func, args, kwargs=None):\r\n     28     if kwargs:\r\n---> 29         return func(*args, **kwargs)\r\n     30     else:\r\n     31         return func(*args)\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py in write_metadata(parts, fmd, fs, path, append, **kwargs)\r\n   1034                 i_start = 1\r\n   1035             for i in range(i_start, len(parts)):\r\n-> 1036                 _append_row_groups(_meta, parts[i][0][\"meta\"])\r\n   1037             with fs.open(metadata_path, \"wb\") as fil:\r\n   1038                 _meta.write_metadata_file(fil)\r\n\r\n/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py in _append_row_groups(metadata, md)\r\n     35     except RuntimeError as err:\r\n     36         if \"requires equal schemas\" in str(err):\r\n---> 37             raise RuntimeError(\r\n     38                 \"Schemas are inconsistent, try using \"\r\n     39                 '`to_parquet(..., schema=\"infer\")`, or pass an explicit '\r\n\r\nRuntimeError: Schemas are inconsistent, try using `to_parquet(..., schema=\"infer\")`, or pass an explicit pyarrow schema.\r\n``` \r\n\r\n**Environment**:\r\n\r\n- Dask version: 2.24.0\r\n- Pyarrow version: 1.0.1\r\n- Python version: 3.8\r\n- Operating System: Dask-Dev image\r\n- Install method (conda, pip, source) : Dask-Dev image",
    "comments": [
      {
        "user": "jsignell",
        "body": "Thanks for reporting this @odovad. I am not sure if this is a pyarrow bug or not (ping @jorisvandenbossche just in case). \r\n\r\nAs a workaround you can specify the schema explicitly. For instance the following seems to work as expected:\r\n\r\n```python\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\nnames = [ 'name']\r\naddress = names\r\n\r\ndf1 = dd.from_pandas(pd.DataFrame({'name':names}), npartitions=1)\r\ndf2 = dd.from_pandas(pd.DataFrame({'address':address}), npartitions=1)\r\ndf1['name'] = df1['name'].astype('category')\r\n\r\ndf = dd.concat([df1, df2])\r\ndel df['address']\r\n\r\ndf.to_parquet('does_not_work', engine=\"pyarrow\", schema={\"name\": pa.string()})\r\n```"
      },
      {
        "user": "jorisvandenbossche",
        "body": "Hmm, with both pyarrow master and dask master, the example of the OP works for me. \r\n\r\nWith released versions it indeed fails. So it might be a recent change in dask fixed it? In any case, in principle the issue with empty / all null partitions should only come up with *object dtype* columns, where pyarrow then cannot correctly infer the type. But in theory for categorical columns this shouldn't be a problem."
      },
      {
        "user": "jorisvandenbossche",
        "body": "> Hmm, with both pyarrow master and dask master, the example of the OP works for me.\r\n\r\nWhoops, sorry, in my dev environment I have also fastparquet installed, which was being used by default. With `engine=\"pyarrow\"` it still fails with master versions of both."
      }
    ]
  },
  {
    "issue_number": 11353,
    "title": "Futures not always resolved when using dataframe.reduction",
    "author": "DaniJG",
    "state": "open",
    "created_at": "2024-08-29T10:46:03Z",
    "updated_at": "2025-03-10T01:51:09Z",
    "labels": [
      "needs attention",
      "needs triage",
      "dask-expr"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\nWhen using `dataframe.reduction`, if a future is passed as part of the `chunk_kwargs`, they are not always resolved correctly by the time the chunk function is executed. Ie, instead of the resolved future's value, sometimes the actual future is received.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\nThe following program:\r\n\r\n```python\r\ndf = pd.DataFrame({'Store': range(50), 'Product': range(400, 450), 'Sales': range(1000, 1050)})\r\nddf = dd.from_pandas(df, npartitions=2)\r\nddf = ddf.groupby([\"Store\", \"Product\"], sort=False)[[\"Sales\"]].sum().reset_index()\r\n\r\nfuture_value = client.scatter(99, broadcast=True)\r\ndef dummy_reduction(df, argument1):\r\n    # do something that will blow if arg isnt a number\r\n    df[\"Sales\"] = df[\"Sales\"] + (argument1 + 100)\r\n    return df\r\nddf = ddf.reduction(\r\n    dummy_reduction,\r\n    aggregate=lambda df: df,\r\n    chunk_kwargs={\r\n        \"argument1\": future_value,\r\n    },\r\n    meta={\"Store\": \"i8\", \"Product\": \"i8\", \"Sales\": \"i8\"},\r\n)\r\n\r\nddf = ddf.persist()\r\nprint(ddf.head())\r\n```\r\n\r\nresults in the following error:\r\n```\r\ndf[\"Sales\"] = df[\"Sales\"] + (argument1 + 100)\r\n                             ~~~~~~~~~~^~~~~\r\nTypeError: unsupported operand type(s) for +: 'Future' and 'int'\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nIt seems to depend on transformations done to the dataframe before the reduction. In the MCVE above, if your remove the `group_by` on line 3, the futures are resolved as expected and the code works. \r\n\r\nI was also expecting chunk to be called once per partition, however it doesnt seem to be always the case:\r\n- when including the `group_by`, chunk gets called twice, even though the group_by results in a single partition. It is during that first invocation of chunk that the future is not resolved\r\n- If I remove the `group_by`, chunk gets called 2 times as expected (given dataframe has 2 partitions). In both of those invocations, the code works as the resolved future value is passed to the chunk function\r\n\r\nOut of curiosity, I have checked how `map_partitions` handles future args. When using map_partitions, futures are handled correctly and the resolved value is passed to the function. This means the following similar program works as expected:\r\n```\r\ndf = pd.DataFrame({'Store': range(50), 'Product': range(400, 450), 'Sales': range(1000, 1050)})\r\nddf = dd.from_pandas(df, npartitions=2)\r\nddf = ddf.groupby([\"Store\", \"Product\"], sort=False)[[\"Sales\"]].sum().reset_index()\r\n\r\nfuture_value = client.scatter(99, broadcast=True)\r\ndef dummy_map_partition(df, argument1):\r\n    # do something that will blow if arg isnt a number\r\n    df[\"Sales\"] = df[\"Sales\"] + (argument1 + 100)\r\n    return df\r\nddf = ddf.map_partitions(\r\n    dummy_map_partition,\r\n    argument1 = future_value,\r\n    meta={\"Store\": \"i8\", \"Product\": \"i8\", \"Sales\": \"i8\"},\r\n)\r\n\r\nddf = ddf.persist()\r\nprint(ddf.head())\r\n```\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.8.1 (personally first noticed issue in 2024.5.2)\r\n- Python version: 3.12.4\r\n- Operating System: mac 14.5\r\n- Install method (conda, pip, source): poetry\r\n",
    "comments": []
  },
  {
    "issue_number": 11363,
    "title": "order: not optimal scheduling for patterns where we slice subsets into 2 different datasets and then combine them again",
    "author": "phofl",
    "state": "open",
    "created_at": "2024-09-02T15:39:26Z",
    "updated_at": "2025-03-10T01:51:08Z",
    "labels": [
      "needs attention",
      "dask-order"
    ],
    "body": "[stackstac-order.pdf](https://github.com/user-attachments/files/16839383/stackstac-order.pdf)\r\n[stackstac-cpath.pdf](https://github.com/user-attachments/files/16839384/stackstac-cpath.pdf)\r\n[stackstac-pressure.pdf](https://github.com/user-attachments/files/16839385/stackstac-pressure.pdf)\r\n\r\n\r\nThis is based on a stackstac thing I've worker on, It looks like some branches are only run halfway, which keeps the original task in memory. Not sure if this is a bug exactly\r\n\r\nSee  https://gist.github.com/phofl/d33739ff37085054397bee16d2b49c72 for an example\r\n\r\nI would have expected that the branch right in the middle with 53 -> 54 would execute 192 next instead of jumping to a different branch that is loading a new file\r\n\r\nThis might be related to the weird topology that stackstac creates\r\n\r\ncc @fjetter ",
    "comments": [
      {
        "user": "fjetter",
        "body": "> I would have expected that the branch right in the middle with 53 -> 54 would execute 192 next instead of jumping to a different branch that is loading a new file\r\n\r\nThis is more or less intentional. The algorithm detects that it will hit a wall if it continues down that path (it will hit 227 which it cannot deal with).\r\nI remember pondering these situations in the past and I think I decided back than that this is a zero sum decision and did not allow the execution of such a code branch. There might've also been a good reason for it but I don't remember. I'll have a look and see what breaks...\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 11381,
    "title": "Appending to partitioned parquet with metadata throws appended dtypes differ even though they should be the same",
    "author": "paluchs",
    "state": "open",
    "created_at": "2024-09-10T08:42:17Z",
    "updated_at": "2025-03-10T01:51:06Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "**Problem Description**\r\n\r\nWhen trying to append a partition to a partitioned parquet file on s3 I get the following error message:\r\n```\r\nValueError: Appended dtypes differ.\r\n{('pseudo_vn', string[pyarrow]), ('municipality_residence_3_year_ago', 'Int64'), ('pseudo_vn', 'object'), ('municipality_residence_3_year_ago', Int64Dtype()), ('stat_year', Int64Dtype()), ('__null_dask_index__', 'int64'), ('__null_dask_index__', dtype('int64'))}\r\n```\r\n\r\nThe error occurs in initialize_write in the ArrowDatasetEngine\r\nThere is also a TODO on line 776 stating:\r\n_TODO Coerce values for compatible but different dtypes_\r\n\r\nThe issue seems to be related to the metadata written by pandas.\r\nIf I read a csv with dtype_backend = 'pyarrow' and then try to append; initialize_write compares the datatypes in the table which are pyarrow dtypes with the datatypes in the metadata which are pandas dtypes (even though they were also read from csv with dtype_backend = 'pyarrow').\r\n\r\n**MCVE**\r\n[test1.csv](https://github.com/user-attachments/files/16942054/test1.csv)\r\n[test2.csv](https://github.com/user-attachments/files/16942055/test2.csv)\r\n\r\n```python\r\nimport dask.dataframe as dd\r\n\r\ndf1 = dd.read_csv(\"~/Desktop/test1.csv\", sep=\",\", dtype={\"stat_year\": \"Int64\", \"some_string\": \"str\", \"year_of_birth\": \"Int64\", \"state_of_birth_hist\": \"Int64\", \"state_of_birth\": \"Int64\", \"t\": \"Int64\"}, dtype_backend='pyarrow', na_values=\"Null\")\r\ndf2 = dd.read_csv(\"~/Desktop/test2.csv\", sep=\",\", dtype={\"stat_year\": \"Int64\", \"some_string\": \"str\", \"year_of_birth\": \"Int64\", \"state_of_birth_hist\": \"Int64\", \"state_of_birth\": \"Int64\", \"t\": \"Int64\"}, dtype_backend='pyarrow', na_values=\"Null\")\r\n\r\ndf1.to_parquet(\"~/Desktop/test.parquet\", write_metadata_file=True, append=False, partition_on=[\"stat_year\"])\r\ndf2.to_parquet(\"~/Desktop/test.parquet\", write_metadata_file=True, append=True, partition_on=[\"stat_year\"])\r\n```\r\n\r\n**Discussion**\r\nCoercing values for compatible but different dtypes would probably solve the issue, however, if I read a csv with pyarrow dtypes I would expect to store the parquet with pyarrow dtypes as well. So possibly there could be an option in to_parquet called dtype_backend like in read_csv, to make sure the dtypes are compatible and consintent with expectation.\r\n\r\n**Environment**\r\nPython: 3.11\r\nDask 2024.8.2\r\nPyarrow: 17.0.0\r\nInstall method: poetry\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "This is a weird one.\r\n\r\nDaks is removing pandas metadata when inferring the arrow schema, but the metadata fail doesn't do that, so the metadata file thinks that it's object while the actual data is arrow strings.\r\n\r\ncc @rjzamora this probably breaks every string use case\r\n\r\nre the initial example @paluchs: specifying dtype directly overwrites the dtype_backend, so you either have to specify arrow dtypes manually with pd.ArrowDtype or remove the dtype kwarg"
      },
      {
        "user": "rjzamora",
        "body": ">Daks is removing pandas metadata when inferring the arrow schema, but the metadata fail doesn't do that, so the metadata file thinks that it's object while the actual data is arrow strings.\r\n\r\nThanks for the ping @phofl . If I understanding correctly, we are writing the wrong dtype to the global `_metadata` file on the first write. Very interesting - I'll look into this."
      },
      {
        "user": "rjzamora",
        "body": "Still trying to understand the underlying issue(s) here. As far as I can tell, the problem is **not** in `to_parquet`, but in `dd.read_csv`. After reading in the csv data, our `meta` tells us that we have a `\"string[pyarrow]\"` dtype, but the actual computed data has an `\"object\"` dtype.\r\n\r\nIf we modify the `dtype` argument, the problem is resolved:\r\n\r\n```python\r\ndf1 = dd.read_csv(\"~/Desktop/test1.csv\", sep=\",\", dtype={\"stat_year\": \"Int64\", \"some_string\": \"string[pyarrow]\", \"year_of_birth\": \"Int64\", \"state_of_birth_hist\": \"Int64\", \"state_of_birth\": \"Int64\", \"t\": \"Int64\"}, na_values=\"Null\")\r\ndf2 = dd.read_csv(\"~/Desktop/test2.csv\", sep=\",\", dtype={\"stat_year\": \"Int64\", \"some_string\": \"string[pyarrow]\", \"year_of_birth\": \"Int64\", \"state_of_birth_hist\": \"Int64\", \"state_of_birth\": \"Int64\", \"t\": \"Int64\"}, na_values=\"Null\")\r\n```"
      }
    ]
  },
  {
    "issue_number": 11389,
    "title": "mode on `axis=1`",
    "author": "marcdelabarrera",
    "state": "open",
    "created_at": "2024-09-16T14:55:33Z",
    "updated_at": "2025-03-10T01:51:04Z",
    "labels": [
      "dataframe",
      "needs attention",
      "enhancement"
    ],
    "body": "The `mode` method in a `dask` `DataFrame` does not allow for the argument `axis=1`. It would be great to have since it seems that in `pandas`, that operation is very slow and seems straightforward to parallelize. \r\nI would like to be able to do this in dask.\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport dask.dataframe as dd\r\nnp.random.seed(0)\r\nN_ROWS = 1_000\r\ndf = pd.DataFrame({'a':np.random.randint(0, 100, N_ROWS),\r\n              'b':np.random.randint(0, 100, N_ROWS),\r\n              'c':np.random.randint(0, 100, N_ROWS)})\r\ndf['d'] = df['a'] #ensure mode is column 'a', unless b=c, then there are two modes\r\n\r\ndf.mode(axis=1)\r\n```\r\n\r\nFor reference, in pandas with `N_ROWS = 100_000`, the mode operation takes 20 seconds, and the time seems to grow linearly with number of observations. ",
    "comments": [
      {
        "user": "phofl",
        "body": "Thanks for the report. This should be trivial to add over in https://github.com/dask/dask-expr, are you interested in opening a PR? Implementation should be the same as sum and friends with axis=1"
      },
      {
        "user": "thyripian",
        "body": "take"
      },
      {
        "user": "thyripian",
        "body": "**Issue Update:**\r\n\r\nUpon further investigation, I found that the problem is related to functionality implemented in the Dask Expressions package (dask-expr). Specifically, the row-wise mode functionality for DataFrames (i.e., with axis=1) needs to be addressed within the dask_expr/_collection.py file, rather than the core Dask package.\r\n\r\nThe necessary changes to dask-expr involve updating how row-wise modes are computed and ensuring consistent handling of metadata during the process. I have completed and thoroughly tested these changes in dask-expr, and the solution works as expected.\r\n\r\nTo fully resolve this issue, I plan to submit a PR to the dask-expr repository with the necessary changes. Once that is approved, I will follow up with a PR for this issue in the core Dask repository, specifically for updating test_core.py to include functionality that tests the new mode feature."
      }
    ]
  },
  {
    "issue_number": 11391,
    "title": "Circular imports in dask-histogram/dask-awkward",
    "author": "martindurant",
    "state": "open",
    "created_at": "2024-09-17T13:02:51Z",
    "updated_at": "2025-03-10T01:51:03Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "In dask/main since https://github.com/dask/dask/pull/11248/ (which was included in 2024.9.0 ).\r\n\r\nBecause dask-awkward and dask-histogram have custom sizeof implementations listed in entrypoints, they are imported when dask is whenever they happen to be in the environment. Naturally, both packages depend in turn on dask and import it. This was not a problem before. However, with the linked PR, specifically [this line](https://github.com/dask/dask/pull/11248/files#diff-ef7533f0d0b45275a9a148b9908353aa4b8209025777fe7106929c8ab32768a0R10), dask can no longer be imported at all if either package is in the environment (full traceback below). Removing the one line makes import possible.\r\n\r\nI note that dask._task_spec itself imports many things, but that `GraphNode` isn't actually needed at import time in dask.optimization, it could be imported later in affected functions. We are willing to move around our own imports, but I have not found a fix yet on our side.\r\n\r\nNote that this is blocking the update of dask in downstream packages ( https://github.com/dask-contrib/dask-awkward/issues/544 ).\r\n\r\ncc @fjetter \r\n\r\nOriginal issue: https://github.com/dask-contrib/dask-histogram/issues/149\r\n\r\n```\r\nImportError                               Traceback (most recent call last)\r\nCell In[1], line 1\r\n----> 1 import dask\r\n\r\nFile ~/code/dask/dask/__init__.py:5\r\n      3 from dask import config, datasets\r\n      4 from dask._version import get_versions\r\n----> 5 from dask.base import (\r\n      6     annotate,\r\n      7     compute,\r\n      8     get_annotations,\r\n      9     is_dask_collection,\r\n     10     optimize,\r\n     11     persist,\r\n     12     visualize,\r\n     13 )\r\n     14 from dask.core import istask\r\n     15 from dask.delayed import delayed\r\n\r\nFile ~/code/dask/dask/base.py:1039\r\n   1030     from dask import threaded\r\n   1032     named_schedulers.update(\r\n   1033         {\r\n   1034             \"threads\": threaded.get,\r\n   1035             \"threading\": threaded.get,\r\n   1036         }\r\n   1037     )\r\n-> 1039     from dask import multiprocessing as dask_multiprocessing\r\n   1041     named_schedulers.update(\r\n   1042         {\r\n   1043             \"processes\": dask_multiprocessing.get,\r\n   1044             \"multiprocessing\": dask_multiprocessing.get,\r\n   1045         }\r\n   1046     )\r\n   1049 get_err_msg = \"\"\"\r\n   1050 The get= keyword has been removed.\r\n   1051\r\n   (...)\r\n   1065     x.compute(scheduler=client)\r\n   1066 \"\"\".strip()\r\n\r\nFile ~/code/dask/dask/multiprocessing.py:19\r\n     17 from dask import config\r\n     18 from dask.local import MultiprocessingPoolExecutor, get_async, reraise\r\n---> 19 from dask.optimization import cull, fuse\r\n     20 from dask.system import CPU_COUNT\r\n     21 from dask.typing import Key\r\n\r\nFile ~/code/dask/dask/optimization.py:10\r\n      7 from typing import Any\r\n      9 from dask import config, core, utils\r\n---> 10 from dask._task_spec import GraphNode\r\n     11 # from dask.base import normalize_token, tokenize\r\n     12 from dask.core import (\r\n     13     flatten,\r\n     14     get_dependencies,\r\n   (...)\r\n     19     toposort,\r\n     20 )\r\n\r\nFile ~/code/dask/dask/_task_spec.py:90\r\n     88 from dask.base import tokenize\r\n     89 from dask.core import reverse_dict\r\n---> 90 from dask.sizeof import sizeof\r\n     91 from dask.typing import Key as KeyType\r\n     92 from dask.utils import is_namedtuple_instance\r\n\r\nFile ~/code/dask/dask/sizeof.py:318\r\n    312         except Exception:\r\n    313             logger.exception(\r\n    314                 f\"Failed to register sizeof entry point {entry_point.name}\"\r\n    315             )\r\n--> 318 _register_entry_point_plugins()\r\n\r\nFile ~/code/dask/dask/sizeof.py:309, in _register_entry_point_plugins()\r\n    307 \"\"\"Register sizeof implementations exposed by the entry_point mechanism.\"\"\"\r\n    308 for entry_point in importlib_metadata.entry_points(group=\"dask.sizeof\"):\r\n--> 309     registrar = entry_point.load()\r\n    310     try:\r\n    311         registrar(sizeof)\r\n\r\nFile ~/conda/envs/py310/lib/python3.10/site-packages/importlib_metadata/__init__.py:183, in EntryPoint.load(self)\r\n    178 \"\"\"Load the entry point from its definition. If only a module\r\n    179 is indicated by the value, return that module. Otherwise,\r\n    180 return the named object.\r\n    181 \"\"\"\r\n    182 match = cast(Match, self.pattern.match(self.value))\r\n--> 183 module = import_module(match.group('module'))\r\n    184 attrs = filter(None, (match.group('attr') or '').split('.'))\r\n    185 return functools.reduce(getattr, attrs, module)\r\n\r\nFile ~/conda/envs/py310/lib/python3.10/importlib/__init__.py:126, in import_module(name, package)\r\n    124             break\r\n    125         level += 1\r\n--> 126 return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\nFile ~/code/dask-awkward/src/dask_awkward/__init__.py:6\r\n      2 from dask_awkward import sizeof\r\n      4 from dask_awkward import config  # isort:skip; load awkward config\r\n----> 6 import dask_awkward.lib.core as core\r\n      7 import dask_awkward.lib.describe as describe\r\n      8 import dask_awkward.lib.inspect as inspect\r\n\r\nFile ~/code/dask-awkward/src/dask_awkward/lib/__init__.py:1\r\n----> 1 import dask_awkward.lib.str as str\r\n      2 import dask_awkward.lib.utils as utils\r\n      3 from dask_awkward.lib.core import Array, PartitionCompatibility, Record, Scalar\r\n\r\nFile ~/code/dask-awkward/src/dask_awkward/lib/str.py:10\r\n      7 import awkward.operations.str as akstr\r\n      8 from typing_extensions import ParamSpec\r\n---> 10 from dask_awkward.lib.core import Array, map_partitions\r\n     12 T = TypeVar(\"T\")\r\n     13 P = ParamSpec(\"P\")\r\n\r\nFile ~/code/dask-awkward/src/dask_awkward/lib/core.py:38\r\n     24 from awkward.typetracer import (\r\n     25     MaybeNone,\r\n     26     OneOf,\r\n   (...)\r\n     29     is_unknown_scalar,\r\n     30 )\r\n     31 from dask.base import (\r\n     32     DaskMethodsMixin,\r\n     33     dont_optimize,\r\n   (...)\r\n     36     unpack_collections,\r\n     37 )\r\n---> 38 from dask.blockwise import BlockwiseDep\r\n     39 from dask.blockwise import blockwise as dask_blockwise\r\n     40 from dask.context import globalmethod\r\n\r\nFile ~/code/dask/dask/blockwise.py:13\r\n     10 import tlz as toolz\r\n     12 import dask\r\n---> 13 from dask.base import clone_key, get_name_from_key, tokenize\r\n     14 from dask.core import flatten, ishashable, keys_in_tasks, reverse_dict\r\n     15 from dask.highlevelgraph import HighLevelGraph, Layer\r\n\r\nImportError: cannot import name 'clone_key' from partially initialized module 'dask.base' (most likely due to a circular import) (/Users/mdurant/code/dask/dask/base.py)\r\n```",
    "comments": [
      {
        "user": "martindurant",
        "body": "I am happy to make a PR deferring the import in optimization.  "
      },
      {
        "user": "martindurant",
        "body": "I'd like to add, that `intake` solves this by explicitly doing entrypoints-based imports only after every other module of the package has already been imported. Dask should also do the same, so as not to affect those that use entrypoints like this."
      },
      {
        "user": "oshadura",
        "body": "I see the same issue in latest `coffea` unit tests, while building recent `coffea` docker image: https://github.com/CoffeaTeam/af-images/actions/runs/10937570567/job/30363739367\r\nProbably it is already affecting many `coffea` users who updated dask"
      }
    ]
  },
  {
    "issue_number": 11394,
    "title": "Discrepancy in column property with actual structure after grouping",
    "author": "dbalabka",
    "state": "open",
    "created_at": "2024-09-17T16:04:55Z",
    "updated_at": "2025-03-10T01:51:02Z",
    "labels": [
      "dataframe",
      "needs attention"
    ],
    "body": "**Describe the issue**:\r\nAfter `groupby` and `reset_index`, DataFrame `columns`  property have one column missing and one with an incorrect name, while computed DataFrame have proper structure.\r\n\r\n\r\n**Minimal Complete Verifiable Example**:\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\ndata = {\r\n    'id': [1, 1, 1, 2, 2, 2],\r\n    'date': pd.to_datetime(['2023-01-01', '2023-01-04', '2023-01-05', '2023-01-01', '2023-01-04', '2023-01-05']),\r\n    'metric': [1,1,1,1,1,1]\r\n}\r\npd_df = pd.DataFrame(data).astype({'id': 'int64', 'metric': 'int64', 'date': 'datetime64[ns]'})\r\ndf = dd.from_pandas(pd_df)\r\n\r\ndf = (\r\n    df\r\n    .groupby(by=['id'])\r\n    .apply(lambda x: x, include_groups=False, meta={'date': 'datetime64[ns]', \"metric\": \"int64\", })\r\n    .reset_index(drop=False)\r\n    .persist()\r\n)\r\nprint('Actual:')\r\nprint(df.compute())\r\nprint(df.columns)\r\n\r\npd_df = (\r\n    pd_df\r\n    .groupby(by=['id'])\r\n    .apply(lambda x: x, include_groups=False)\r\n    .reset_index(drop=False)\r\n)\r\nprint(\"\\n\\nExpected:\")\r\nprint(pd_df)\r\nprint(pd_df.columns)\r\n```\r\n```\r\nActual:\r\n   id  level_1       date  metric\r\n0   1        0 2023-01-01       1\r\n1   1        1 2023-01-04       1\r\n2   1        2 2023-01-05       1\r\n3   2        3 2023-01-01       1\r\n4   2        4 2023-01-04       1\r\n5   2        5 2023-01-05       1\r\nIndex(['index', 'date', 'metric'], dtype='object')      <----------  extra 'index' column and missing 'id' and 'level_1'\r\n\r\n\r\nExpected:\r\n   id  level_1       date  metric\r\n0   1        0 2023-01-01       1\r\n1   1        1 2023-01-04       1\r\n2   1        2 2023-01-05       1\r\n3   2        3 2023-01-01       1\r\n4   2        4 2023-01-04       1\r\n5   2        5 2023-01-05       1\r\nIndex(['id', 'level_1', 'date', 'metric'], dtype='object')\r\n```\r\n\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.8.0\r\n- Python version: 3.10\r\n- Operating System: WSL\r\n- Install method (conda, pip, source): poetry\r\n",
    "comments": []
  },
  {
    "issue_number": 11395,
    "title": "When adding collumns from 2 dataframes will not compute in some instances, fix for one instance seems to break the other",
    "author": "jimhonetech",
    "state": "open",
    "created_at": "2024-09-18T10:23:45Z",
    "updated_at": "2025-03-10T01:51:01Z",
    "labels": [
      "needs attention",
      "dask-expr"
    ],
    "body": "**Describe the issue**:\r\nUpgrading to pandas 2, existing code. WE add some collumns together as part of our process some data seems to require a compute mid process but this then causes other data to fail, this seems buggy, there are 2 errors generated in the code below, it seems the fix for one bit of data breaks the process for the other data.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\npreds1=pd.DataFrame({\r\n    \"prediction_probability\":[1.0] * 2,\r\n    \"prediction\":  [1,1],\r\n    \"num_runs\": [1,1],\r\n    \"Idx\":[1,4],\r\n})\r\n\r\npreds1=preds1.set_index(\"Idx\")\r\n\r\n\r\nads1=pd.DataFrame({\r\n    \"prediction_probability\":[1.0] * 2,\r\n    \"prediction\":  [1,1],\r\n    \"num_runs\": [1,1,],\r\n    \"Idx\":[1,4],\r\n})\r\n\r\nads1=ads1.set_index(\"Idx\")\r\n\r\npreds2=pd.DataFrame({\r\n    \"prediction_probability\":[1.0] * 4,\r\n    \"prediction\":  [1,1,1,1],\r\n    \"num_runs\": [1,1,1,1],\r\n    \"Idx\":[1,2,3,4],\r\n})\r\n\r\npreds2=preds2.set_index(\"Idx\")\r\n\r\n\r\nads2=pd.DataFrame({\r\n    \"prediction_probability\":[1.0] * 2,\r\n    \"prediction\":  [1,1],\r\n    \"num_runs\": [1,1],\r\n    \"Idx\":[1,2],\r\n})\r\n\r\nads2=ads2.set_index(\"Idx\")\r\n\r\n# computing at end\r\n# this works\r\npreds_dd = dd.from_pandas(preds1)\r\nads_dd = dd.from_pandas(ads1)\r\npreds_dd[\"prediction\"] = preds_dd.prediction.add(\r\n                    ads_dd.prediction, fill_value=0\r\n)\r\nprint(preds_dd.compute())\r\n\r\n# this fails\r\npreds_dd = dd.from_pandas(preds2)\r\nads_dd = dd.from_pandas(ads2)\r\npreds_dd[\"prediction\"] = preds_dd.prediction.add(\r\n                    ads_dd.prediction, fill_value=0\r\n)\r\nprint(preds_dd.compute())\r\n\r\n\r\n# extra compute in the middle on the series \r\n\r\n# this fails\r\npreds_dd = dd.from_pandas(preds1)\r\nads_dd = dd.from_pandas(ads1)\r\npreds_dd[\"prediction\"] = preds_dd.prediction.add(\r\n                    ads_dd.prediction.compute(), fill_value=0\r\n)\r\n\r\nprint(preds_dd.compute())\r\n\r\n# this works\r\npreds_dd = dd.from_pandas(preds1)\r\nads_dd = dd.from_pandas(ads1)\r\npreds_dd[\"prediction\"] = preds_dd.prediction.add(\r\n                    ads_dd.prediction.compute(), fill_value=0\r\n)\r\n\r\nprint(preds_dd.compute())\r\n```\r\n\r\n**Anything else we need to know?**:\r\nstack trace\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\nCell In[6], line 6\r\n      2 ads_dd = dd.from_pandas(ads1)\r\n      3 preds_dd[\"prediction\"] = preds_dd.prediction.add(\r\n      4                     ads_dd.prediction.compute(), fill_value=0\r\n      5 )\r\n----> 6 print(preds_dd.compute())\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_collection.py:476, in FrameBase.compute(self, fuse, **kwargs)\r\n    474 if not isinstance(out, Scalar):\r\n    475     out = out.repartition(npartitions=1)\r\n--> 476 out = out.optimize(fuse=fuse)\r\n    477 return DaskMethodsMixin.compute(out, **kwargs)\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_collection.py:591, in FrameBase.optimize(self, fuse)\r\n    573 def optimize(self, fuse: bool = True):\r\n    574     \"\"\"Optimizes the DataFrame.\r\n    575 \r\n    576     Runs the optimizer with all steps over the DataFrame and wraps the result in a\r\n   (...)\r\n    589         The optimized Dask Dataframe\r\n    590     \"\"\"\r\n--> 591     return new_collection(self.expr.optimize(fuse=fuse))\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:94, in Expr.optimize(self, **kwargs)\r\n     93 def optimize(self, **kwargs):\r\n---> 94     return optimize(self, **kwargs)\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:3070, in optimize(expr, fuse)\r\n   3049 \"\"\"High level query optimization\r\n   3050 \r\n   3051 This leverages three optimization passes:\r\n   (...)\r\n   3066 optimize_blockwise_fusion\r\n   3067 \"\"\"\r\n   3068 stage: core.OptimizerStage = \"fused\" if fuse else \"simplified-physical\"\r\n-> 3070 return optimize_until(expr, stage)\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:3031, in optimize_until(expr, stage)\r\n   3028     return expr\r\n   3030 # Lower\r\n-> 3031 expr = expr.lower_completely()\r\n   3032 if stage == \"physical\":\r\n   3033     return expr\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_core.py:447, in Expr.lower_completely(self)\r\n    445 lowered = {}\r\n    446 while True:\r\n--> 447     new = expr.lower_once(lowered)\r\n    448     if new._name == expr._name:\r\n    449         break\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_core.py:402, in Expr.lower_once(self, lowered)\r\n    399 expr = self\r\n    401 # Lower this node\r\n--> 402 out = expr._lower()\r\n    403 if out is None:\r\n    404     out = expr\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:3435, in MaybeAlignPartitions._lower(self)\r\n   3432 def _lower(self):\r\n   3433     # This can be expensive when something that has expensive division\r\n   3434     # calculation is in the Expression\r\n-> 3435     dfs = self.args\r\n   3436     if (\r\n   3437         len(dfs) == 1\r\n   3438         or all(\r\n   (...)\r\n   3441         or len(self.divisions) == 2\r\n   3442     ):\r\n   3443         return self._expr_cls(*self.operands)\r\n\r\nFile [~/.pyenv/versions/3.10.14/lib/python3.10/functools.py:981](http://localhost:8888/home/honej/.pyenv/versions/3.10.14/lib/python3.10/functools.py#line=980), in cached_property.__get__(self, instance, owner)\r\n    979 val = cache.get(self.attrname, _NOT_FOUND)\r\n    980 if val is _NOT_FOUND:\r\n--> 981     val = self.func(instance)\r\n    982     try:\r\n    983         cache[self.attrname] = val\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:3430, in MaybeAlignPartitions.args(self)\r\n   3427 @functools.cached_property\r\n   3428 def args(self):\r\n   3429     dfs = [op for op in self.operands if isinstance(op, Expr)]\r\n-> 3430     return [op for op in dfs if not is_broadcastable(dfs, op)]\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:3430, in <listcomp>(.0)\r\n   3427 @functools.cached_property\r\n   3428 def args(self):\r\n   3429     dfs = [op for op in self.operands if isinstance(op, Expr)]\r\n-> 3430     return [op for op in dfs if not is_broadcastable(dfs, op)]\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:3086, in is_broadcastable(dfs, s)\r\n   3081     except (TypeError, ValueError):\r\n   3082         return False\r\n   3084 return (\r\n   3085     s.ndim == 1\r\n-> 3086     and s.npartitions == 1\r\n   3087     and s.known_divisions\r\n   3088     and any(compare(s, df) for df in dfs if df.ndim == 2)\r\n   3089     or s.ndim == 0\r\n   3090 )\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:398, in Expr.npartitions(self)\r\n    396     return self.operands[idx]\r\n    397 else:\r\n--> 398     return len(self.divisions) - 1\r\n\r\nFile [~/.pyenv/versions/3.10.14/lib/python3.10/functools.py:981](http://localhost:8888/home/honej/.pyenv/versions/3.10.14/lib/python3.10/functools.py#line=980), in cached_property.__get__(self, instance, owner)\r\n    979 val = cache.get(self.attrname, _NOT_FOUND)\r\n    980 if val is _NOT_FOUND:\r\n--> 981     val = self.func(instance)\r\n    982     try:\r\n    983         cache[self.attrname] = val\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:382, in Expr.divisions(self)\r\n    380 @functools.cached_property\r\n    381 def divisions(self):\r\n--> 382     return tuple(self._divisions())\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:2633, in Binop._divisions(self)\r\n   2631     return tuple(self.operation(left_divisions, right_divisions))\r\n   2632 else:\r\n-> 2633     return super()._divisions()\r\n\r\nFile ~/git/ukho-bathymetry-ml/ukho-bathymetry-ml/.venv/lib/python3.10/site-packages/dask_expr/_expr.py:530, in Blockwise._divisions(self)\r\n    528 for arg in dependencies:\r\n    529     if not self._broadcast_dep(arg):\r\n--> 530         assert arg.divisions == dependencies[0].divisions\r\n    531 return dependencies[0].divisions\r\n\r\nAssertionError:\r\n```\r\n\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.8.2\r\n- Python version: 3.10.14\r\n- Operating System:Ubuntu\r\n- Install method (conda, pip, source):Poetry\r\n",
    "comments": [
      {
        "user": "jimhonetech",
        "body": "I've got round this by checking to see if the divisions are equal, if they are not I have to compute the series before I add it, if they are the same that will break it so I have to not compute the series"
      }
    ]
  },
  {
    "issue_number": 11397,
    "title": "Improve error message for boolean index assignment with `nan` shape",
    "author": "lucascolley",
    "state": "open",
    "created_at": "2024-09-18T21:51:27Z",
    "updated_at": "2025-03-10T01:51:00Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "When attempting to perform boolean index assignment involving a Dask array with unknown shape, one receives the error message:\r\n\r\n```\r\nE   ValueError: Boolean index assignment in Dask expects equally shaped arrays.\r\nE   Example: da1[da2] = da3 where da1.shape == (4,), da2.shape == (4,) and da3.shape == (4,).\r\nE   Alternatively, you can use the extended API that supportsindexing with tuples.\r\nE   Example: da1[(da2,)] = da3.\r\n```\r\n\r\nSince the example in the error message only includes arrays where the shape is known, I find it potentially misleading. One can hit this error message using arrays that are in fact the same shape, but this is just unknown to Dask at the time of execution.\r\n\r\nAdding a suggestion to check whether any shapes are `nan` and perhaps pointing the user towards manually computing the chunk sizes if they need to would be useful. Better still if a different error message were shown when `nan` shapes are involved.",
    "comments": [
      {
        "user": "lucascolley",
        "body": "https://github.com/dask/dask/blob/e71d4361a57e7990e4a325021e6382877ac9ab49/dask/array/core.py#L1898-L1922\r\n\r\nLooks like a `nan` check should occur in the case that `key` is an `Array` also"
      }
    ]
  },
  {
    "issue_number": 11398,
    "title": "Boolean index assignment fails for values of `ndim>1`",
    "author": "lucascolley",
    "state": "open",
    "created_at": "2024-09-18T23:19:17Z",
    "updated_at": "2025-03-10T01:50:59Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "This works in NumPy, but fails in Dask.\r\n\r\n```python\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import dask.array as da\r\n\r\nIn [3]: x = np.asarray([[1, 2]])\r\n\r\nIn [4]: y = np.asarray([[3, 4]])\r\n\r\nIn [5]: i = np.asarray([True])\r\n\r\nIn [6]: xd = da.asarray(x)\r\n\r\nIn [7]: yd = da.asarray(y)\r\n\r\nIn [8]: id = da.asarray(i)\r\n\r\nIn [9]: x[i] = y[i]\r\n\r\nIn [10]: x\r\nOut[10]: array([[3, 4]])\r\n\r\nIn [11]: xd[id] = yd[id]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[11], line 1\r\n----> 1 xd[id] = yd[id]\r\n\r\nFile ~/programming/pixi-dev-scipystack/scipy/.pixi/envs/array-api/lib/python3.12/site-packages/dask/array/core.py:1903, in Array.__setitem__(self, key, value)\r\n   1900 from dask.array.routines import where\r\n   1902 if isinstance(value, Array) and value.ndim > 1:\r\n-> 1903     raise ValueError(\"boolean index array should have 1 dimension\")\r\n   1904 try:\r\n   1905     y = where(key, value, self)\r\n\r\nValueError: boolean index array should have 1 dimension\r\n\r\nIn [12]: yd[id].ndim\r\nOut[12]: 2\r\n\r\n```\r\n\r\n**Anything else we need to know?**:\r\nI think the error message is unclear too. I initially thought that the \"boolean index array\" referred to `id` here (the index array), as opposed to `yd[id]`, the value array being assigned.\r\n",
    "comments": [
      {
        "user": "davidhassell",
        "body": "This looks like it is due to the fact that it was decided that the pre-2021.4.1 `__setitem__` API should be preserved when `__setitem__` was extended to more match numpy's functionality (which was introduced in version `2021.4.1`).\n\nThe old API insisted that _index_ was an `da.Array` object, and put a restriction on the allowed rank of the _assignment value_. This was because it could only do an assignment using `da.where` under the hood.\n\nYou'll find that `xd[i] = yd[i]` works, because n this case `i` is a numpy array rather than a `da.Array` object, so the new numpy-like API kicks in.\n\nThis same issue has arisen before (#8640), which [noted](https://github.com/dask/dask/issues/8460#issuecomment-1022455209):\n\n> I'm sure that this could be \"fixed\" (by which I mean so that dask behaves like numpy in all cases - other meanings of \"fixed\" are probably available), but it'd be good to get some wider feedback on whether we _should_ do so. Doing so would possibly break some existing applications that utilise the original `Array.__setitem__` API, either by raising a new exception or, worse, running but giving incorrect results."
      },
      {
        "user": "lucascolley",
        "body": "Thanks for looking at this @davidhassell !\n\n> it'd be good to get some wider feedback on whether we should do so\n\n@crusaderky, you've been thinking about this sort of thing a lot more closely than I have recently - do you have any thoughts about whether any changes would be useful?"
      },
      {
        "user": "crusaderky",
        "body": "At this stage I think it is key for dask to be as array API compliant as possible. And selection by n-dimensional dask boolean index is a prescribed feature of the array API."
      }
    ]
  },
  {
    "issue_number": 11820,
    "title": "⚠️ Upstream CI failed ⚠️",
    "author": "github-actions[bot]",
    "state": "closed",
    "created_at": "2025-03-07T11:15:47Z",
    "updated_at": "2025-03-07T12:13:05Z",
    "labels": [
      "upstream"
    ],
    "body": "[Workflow Run URL](https://github.com/dask/dask/actions/runs/13719081432)\n<details><summary>Python 3.12 Test Summary</summary>\n\n```\ndask/array/tests/test_overlap.py::test_overlap_not_blowing_up_graph: ModuleNotFoundError: No module named 'numba'\n```\n\n</details>\n",
    "comments": []
  },
  {
    "issue_number": 2952,
    "title": "can't drop duplicated on dask dataframe index",
    "author": "thebeancounter",
    "state": "closed",
    "created_at": "2017-12-03T10:06:36Z",
    "updated_at": "2025-03-06T23:14:11Z",
    "labels": [
      "dataframe"
    ],
    "body": "please note my stackoverflow [question](https://stackoverflow.com/questions/47534099/dask-dataframe-drop-duplicate-index-values) \r\n\r\nI am using dask dataframe with python 2.7 and want to drop duplicated index values from my df. \r\n\r\nWhen using pandas i would use \r\n\r\n    df = df[~df.index.duplicated(keep = \"first\")]\r\n\r\nAnd it works \r\n\r\nWhen trying to do the same with dask dataframe i get \r\n\r\n> AttributeError: 'Index' object has no attribute 'duplicated'\r\n\r\nI could reset the index and than use the column that was the index to drop duplicated but I would like to avoid it if possible \r\n\r\nI could use df.compute() and than drop the duplicated index values but this df is too big for memory. \r\n\r\nI tried using the following code as suggested by jezrael in stackoverflow \r\n`rxTable[~rxTable.index.to_Series().duplicated()]`\r\nand got \r\n\r\n> AttributeError: 'Index' object has no attribute 'to_Series'\r\n\r\nIt worked a few days ago and just stopped, i can't find any difference in the code and data. \r\n\r\nHow can i drop the duplicated index values from my dataframe using dask dataframe?\r\n\r\nThanks ",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "Reminder, it's helpful to have reproducible examples :)\r\n\r\nThis may be your best shot right now:\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\na = pd.DataFrame({\"A\": [1, 2, 3, 4]},\r\n                 index=[0, 0, 1, 1])\r\nb = dd.from_pandas(a, 2)\r\n\r\na.groupby(a.index).first()\r\n\r\ndef chunk(x):\r\n    return x.first()\r\n\r\n\r\ndef agg(x):\r\n    return x.first()\r\n```\r\n\r\nA PR implementing `first` and `last` on groupby objects would be helpful if you have time."
      },
      {
        "user": "thebeancounter",
        "body": "@TomAugspurger  Thanks! "
      },
      {
        "user": "thebeancounter",
        "body": "@TomAugspurger  BTW - by pr your mean pull request? please clarify what is the need and I will try to do it "
      }
    ]
  },
  {
    "issue_number": 9471,
    "title": "`numeric_only` compatibility with `pandas=1.5`",
    "author": "jrbourbeau",
    "state": "open",
    "created_at": "2022-09-07T18:50:05Z",
    "updated_at": "2025-03-06T18:34:30Z",
    "labels": [
      "dataframe",
      "upstream"
    ],
    "body": "The upcoming `panads=1.5` release contains several deprecations around `pandas` use of `numeric_only` (xref https://github.com/pandas-dev/pandas/issues/46560). All of these changes are backwards compatible, so user code shouldn't break, but users will start getting lots of deprecation warnings from `dask`s internal use of `pandas` (which is scattered throughout `dask.dataframe`). Ideally, `dask` would emit the same deprecation `pandas` does and users wouldn't see any deprecations due to `dask`s internal use of `pandas`. \r\n\r\nThere have been a few attempts at adding `numeric_only` compatibility to `dask` with `pandas=1.5` (https://github.com/dask/dask/pull/9269, https://github.com/dask/dask/pull/9271, https://github.com/dask/dask/pull/9241) but, as @rhshadrach highlights in https://github.com/pandas-dev/pandas/issues/46560#issuecomment-1180871534, exactly what the current default value is for `numeric_only` and when a deprecation warning isn't super straightforward. \r\n\r\nOpening a dedicated issue so we don't loose track of this issue. \r\n\r\ncc @rhshadrach as he appears to be leading the `numeric_only` changes upstream in `pandas`\r\ncc @mroeschke @jorisvandenbossche in case you have thoughts ",
    "comments": [
      {
        "user": "rhshadrach",
        "body": "> Ideally, `dask` would emit the same deprecation `pandas` does and users wouldn't see any deprecations due to `dask`s internal use of `pandas`.\r\n\r\nI'm guessing that it is not easy / feasible to separate off internal vs non-internal use of pandas, as otherwise you'd be able to wrap internal uses with `warnings.catch_warnings`. Is that right?"
      },
      {
        "user": "rhshadrach",
        "body": "@jrbourbeau - not sure if this is important, but all the deprecations involving `numeric_only` have been enforced on the main branch of pandas."
      },
      {
        "user": "jrbourbeau",
        "body": "Thanks for the heads up @rhshadrach! I'm seeing some test failures in our upstream build (xref https://github.com/dask/dask/issues/9736) that I think are related. I know you lead the `numeric_only` efforts on the `pandas` side -- do you happen to have interest / bandwidth to help make the corresponding changes on the `dask` side? (no obligation though) "
      }
    ]
  },
  {
    "issue_number": 8816,
    "title": "`test_blockwise_dataframe_io[True-False-hdf]` is flaky on osx CI",
    "author": "jcrist",
    "state": "open",
    "created_at": "2022-03-15T20:21:29Z",
    "updated_at": "2025-03-06T18:34:24Z",
    "labels": [
      "dataframe",
      "tests",
      "needs attention"
    ],
    "body": "I noticed this test periodically failing due to a pytables locking issue today (may have started earlier). \r\n\r\n<details>\r\n<summary>Traceback </summary>\r\n\r\n```\r\n_________________ test_blockwise_dataframe_io[True-False-hdf] __________________\r\n[gw0] darwin -- Python 3.8.12 /Users/runner/miniconda3/envs/test-environment/bin/python\r\n\r\nc = <Client: 'tcp://127.0.0.1:50922' processes=2 threads=2, memory=28.00 GiB>\r\ntmpdir = local('/private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pytest-of-runner/pytest-0/popen-gw0/test_blockwise_dataframe_io_Tr3')\r\nio = 'hdf', fuse = False, from_futures = True\r\n\r\n    @pytest.mark.filterwarnings(\r\n        \"ignore:Running on a single-machine scheduler when a distributed client \"\r\n        \"is active might lead to unexpected results.\"\r\n    )\r\n    @pytest.mark.parametrize(\r\n        \"io\",\r\n        [\"parquet-pyarrow\", \"parquet-fastparquet\", \"csv\", \"hdf\"],\r\n    )\r\n    @pytest.mark.parametrize(\"fuse\", [True, False, None])\r\n    @pytest.mark.parametrize(\"from_futures\", [True, False])\r\n    def test_blockwise_dataframe_io(c, tmpdir, io, fuse, from_futures):\r\n        pd = pytest.importorskip(\"pandas\")\r\n        dd = pytest.importorskip(\"dask.dataframe\")\r\n    \r\n        df = pd.DataFrame({\"x\": [1, 2, 3] * 5, \"y\": range(15)})\r\n    \r\n        if from_futures:\r\n            parts = [df.iloc[:5], df.iloc[5:10], df.iloc[10:15]]\r\n            futs = c.scatter(parts)\r\n            ddf0 = dd.from_delayed(futs, meta=parts[0])\r\n        else:\r\n            ddf0 = dd.from_pandas(df, npartitions=3)\r\n    \r\n        if io.startswith(\"parquet\"):\r\n            if io == \"parquet-pyarrow\":\r\n                pytest.importorskip(\"pyarrow.parquet\")\r\n                engine = \"pyarrow\"\r\n            else:\r\n                pytest.importorskip(\"fastparquet\")\r\n                engine = \"fastparquet\"\r\n            ddf0.to_parquet(str(tmpdir), engine=engine)\r\n            ddf = dd.read_parquet(str(tmpdir), engine=engine)\r\n        elif io == \"csv\":\r\n            ddf0.to_csv(str(tmpdir), index=False)\r\n            ddf = dd.read_csv(os.path.join(str(tmpdir), \"*\"))\r\n        elif io == \"hdf\":\r\n            pytest.importorskip(\"tables\")\r\n            fn = str(tmpdir.join(\"h5\"))\r\n>           ddf0.to_hdf(fn, \"/data*\")\r\n\r\ndask/tests/test_distributed.py:370: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ndask/dataframe/core.py:1620: in to_hdf\r\n    return to_hdf(self, path_or_buf, key, mode, append, **kwargs)\r\ndask/dataframe/io/hdf.py:251: in to_hdf\r\n    compute_as_if_collection(\r\ndask/base.py:319: in compute_as_if_collection\r\n    return schedule(dsk2, keys, **kwargs)\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/distributed/client.py:3015: in get\r\n    results = self.gather(packed, asynchronous=asynchronous, direct=direct)\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/distributed/client.py:2167: in gather\r\n    return self.sync(\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/distributed/utils.py:311: in sync\r\n    return sync(\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/distributed/utils.py:378: in sync\r\n    raise exc.with_traceback(tb)\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/distributed/utils.py:351: in f\r\n    result = yield future\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/tornado/gen.py:762: in run\r\n    value = future.result()\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/distributed/client.py:2030: in _gather\r\n    raise exception.with_traceback(traceback)\r\ndask/dataframe/io/hdf.py:27: in _pd_to_hdf\r\n    pd_to_hdf(*args, **kwargs)\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/pandas/core/generic.py:2606: in to_hdf\r\n    pytables.to_hdf(\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/pandas/io/pytables.py:277: in to_hdf\r\n    with HDFStore(\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/pandas/io/pytables.py:561: in __init__\r\n    self.open(mode=mode, **kwargs)\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/pandas/io/pytables.py:710: in open\r\n    self._handle = tables.open_file(self._path, self._mode, **kwargs)\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/tables/file.py:300: in open_file\r\n    return File(filename, mode, title, root_uep, filters, **kwargs)\r\n../../../miniconda3/envs/test-environment/lib/python3.8/site-packages/tables/file.py:750: in __init__\r\n    self._g_new(filename, mode, **params)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n>   ???\r\nE   tables.exceptions.HDF5ExtError: HDF5 error back trace\r\nE   \r\nE     File \"H5F.c\", line 620, in H5Fopen\r\nE       unable to open file\r\nE     File \"H5VLcallback.c\", line 3502, in H5VL_file_open\r\nE       failed to iterate over available VOL connector plugins\r\nE     File \"H5PLpath.c\", line 579, in H5PL__path_table_iterate\r\nE       can't iterate over plugins in plugin path '(null)'\r\nE     File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\r\nE       can't open directory: /Users/runner/miniconda3/envs/test-environment/lib/hdf5/plugin\r\nE     File \"H5VLcallback.c\", line 3351, in H5VL__file_open\r\nE       open failed\r\nE     File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\r\nE       unable to open file\r\nE     File \"H5Fint.c\", line 1898, in H5F_open\r\nE       unable to lock the file\r\nE     File \"H5FD.c\", line 1625, in H5FD_lock\r\nE       driver lock request failed\r\nE     File \"H5FDsec2.c\", line 1002, in H5FD__sec2_lock\r\nE       unable to lock file, errno = 35, error message = 'Resource temporarily unavailable'\r\nE   \r\nE   End of HDF5 error back trace\r\nE   \r\nE   Unable to open/create file '/private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pytest-of-runner/pytest-0/popen-gw0/test_blockwise_dataframe_io_Tr3/h5'\r\n\r\ntables/hdf5extension.pyx:486: HDF5ExtError\r\n```\r\n\r\n</details>\r\n\r\nLog from failing test: https://github.com/dask/dask/runs/5558289646?check_suite_focus=true#step:6:21912",
    "comments": [
      {
        "user": "pavithraes",
        "body": "Just to note, a next step here would be to try to reproduce this on an intel-mac. :)"
      },
      {
        "user": "jrbourbeau",
        "body": "@mrocklin ran into a similar failure for `test test_blockwise_dataframe_io[True-None-hdf]` (note the pytest parametrize values are slightly different)"
      },
      {
        "user": "mrocklin",
        "body": "We might be getting close to the time when we can just drop support for pytables maybe?"
      }
    ]
  },
  {
    "issue_number": 11804,
    "title": "Unclear `UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected` message",
    "author": "MarcoGorelli",
    "state": "closed",
    "created_at": "2025-03-03T18:07:46Z",
    "updated_at": "2025-03-05T14:54:31Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\nimport pandas as pd\ndf = dd.from_pandas(\n    pd.DataFrame(\n        {\n            'a': ['a', 'a', 'b', 'b', 'b'],\n            'b': [1,2,3,5,3],\n            'c': [5,4,3,2,1],\n            'i': [0,1,2,3,4],\n        }\n    ), npartitions=2\n)\ndf.groupby('a')['b'].transform('mean')\n```\ngives\n```\n/home/marcogorelli/polars-api-compat-dev/t.py:35: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n  Before: .apply(func)\n  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n  or:     .apply(func, meta=('x', 'f8'))            for series result\n```\n\nIt's not clear what it's telling me to do, as I didn't use `apply`\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.02\n- Python version: 3.12\n- Operating System:\n- Install method (conda, pip, source):\n",
    "comments": []
  },
  {
    "issue_number": 11811,
    "title": "from_pandas documentation issue",
    "author": "faulaire",
    "state": "closed",
    "created_at": "2025-03-05T12:01:28Z",
    "updated_at": "2025-03-05T12:41:11Z",
    "labels": [
      "needs triage"
    ],
    "body": "**Describe the issue**:\nMinor issue in from_pandas documentation: name parameter doesn't exists anymore in daskexpr.\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: 3.12.7\n- Operating System: RHEL8\n- Install method (conda, pip, source): pip\n",
    "comments": []
  },
  {
    "issue_number": 11805,
    "title": "Make repartition a no-op when divisions match",
    "author": "faulaire",
    "state": "closed",
    "created_at": "2025-03-04T07:56:56Z",
    "updated_at": "2025-03-05T11:00:56Z",
    "labels": [
      "needs triage"
    ],
    "body": "This issue seems to be a regression of https://github.com/dask/dask/pull/9924 in daskexpr.\n\nRepartition must be a no-op when divisions match.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask\nimport pandas as pd\nfrom dask.dataframe import from_pandas\n\ndd = from_pandas(pd.Series([1., 2., 3.]), npartitions=2)\ndd2 = dd.repartition(dd.divisions)\nassert dd2 is dd\n```\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: 3.12.7\n- Operating System: RHEL 8\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "phofl",
        "body": "We are not guaranteeing that you get the same python object back anymore, but it's still a no-op.\n\nYou can see this with:\n\n```\ndd2.optimize(fuse=False).pprint()\n```\n\nThis returns the expression that is going to get executed and that is \n\n```\nFromPandas: frame='<pandas>' npartitions=2\n```\n\nWe are adding multiple optimisation techniques that can change the divisions, so we have to add a placeholder to provide what the user explicitly asked for"
      }
    ]
  },
  {
    "issue_number": 11447,
    "title": "Dask.Visualize ignoring engine setting",
    "author": "frbelotto",
    "state": "open",
    "created_at": "2024-10-18T17:27:03Z",
    "updated_at": "2025-03-05T02:28:51Z",
    "labels": [
      "good first issue",
      "dataframe",
      "bug"
    ],
    "body": "Hello guys!\r\nI am running some codes on a jupiter notebook remote server. As I can´t install graphviz, I am trying to use ipycytoscape to renderize the visualize results.\r\n\r\nIf I run a example code, it does create an html file containing the visualization\r\n\r\n```\r\nimport dask\r\nimport dask.array as da\r\n\r\nwith dask.config.set({\"visualization.engine\": \"cytoscape\"}):\r\n    x = da.ones((15, 15), chunks=(5, 5))\r\n    y = x + x.T\r\n    y.visualize()\r\n```\r\n![image](https://github.com/user-attachments/assets/f3d2f61c-87e3-4670-bdeb-f09fdfaed658)\r\n\r\nSo far, no issue.\r\n\r\nBut as soon as I tries do display my real dask.dataframe graphs, it ignores the parameters and tries to use graphviz\r\nimport dask\r\n\r\n```\r\ndf = dask.dataframe.read_parquet(path= 'Temp/', dtype_backend='pyarrow')\r\nwith dask.config.set({\"visualization.engine\": \"cytoscape\"}):\r\n    df.visualize()\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/ce7cb574-46a7-429e-b3c7-aced63a8f5a1)\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nFile /projeto/libs/lib/python3.11/site-packages/dask/utils.py:327, in import_required(mod_name, error_msg)\r\n    326 try:\r\n--> 327     return import_module(mod_name)\r\n    328 except ImportError as e:\r\n\r\nFile /usr/local/lib/python3.11/importlib/__init__.py:126, in import_module(name, package)\r\n    125         level += 1\r\n--> 126 return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\nFile <frozen importlib._bootstrap>:1206, in _gcd_import(name, package, level)\r\n\r\nFile <frozen importlib._bootstrap>:1178, in _find_and_load(name, import_)\r\n\r\nFile <frozen importlib._bootstrap>:1142, in _find_and_load_unlocked(name, import_)\r\n\r\nModuleNotFoundError: No module named 'graphviz'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[26], line 6\r\n      4 df = dd.read_parquet(path= 'Temp/', dtype_backend='pyarrow')\r\n      5 with dask.config.set({\"visualization.engine\": \"cytoscape\"}):\r\n----> 6     df.visualize()\r\n\r\nFile /projeto/libs/lib/python3.11/site-packages/dask_expr/_collection.py:643, in FrameBase.visualize(self, tasks, **kwargs)\r\n    641 if tasks:\r\n    642     return super().visualize(**kwargs)\r\n--> 643 return self.expr.visualize(**kwargs)\r\n\r\nFile /projeto/libs/lib/python3.11/site-packages/dask_expr/_core.py:715, in Expr.visualize(self, filename, format, **kwargs)\r\n    697 \"\"\"\r\n    698 Visualize the expression graph.\r\n    699 Requires ``graphviz`` to be installed.\r\n   (...)\r\n    711    Additional keyword arguments to forward to ``to_graphviz``.\r\n    712 \"\"\"\r\n    713 from dask.dot import graphviz_to_file\r\n--> 715 g = self._to_graphviz(**kwargs)\r\n    716 graphviz_to_file(g, filename, format)\r\n    717 return g\r\n\r\nFile /projeto/libs/lib/python3.11/site-packages/dask_expr/_core.py:631, in Expr._to_graphviz(self, rankdir, graph_attr, node_attr, edge_attr, **kwargs)\r\n    621 def _to_graphviz(\r\n    622     self,\r\n    623     rankdir=\"BT\",\r\n   (...)\r\n    627     **kwargs,\r\n    628 ):\r\n    629     from dask.dot import label, name\r\n--> 631     graphviz = import_required(\r\n    632         \"graphviz\",\r\n    633         \"Drawing dask graphs with the graphviz visualization engine requires the `graphviz` \"\r\n    634         \"python library and the `graphviz` system library.\\n\\n\"\r\n    635         \"Please either conda or pip install as follows:\\n\\n\"\r\n    636         \"  conda install python-graphviz     # either conda install\\n\"\r\n    637         \"  python -m pip install graphviz    # or pip install and follow installation instructions\",\r\n    638     )\r\n    640     graph_attr = graph_attr or {}\r\n    641     node_attr = node_attr or {}\r\n\r\nFile /projeto/libs/lib/python3.11/site-packages/dask/utils.py:329, in import_required(mod_name, error_msg)\r\n    327     return import_module(mod_name)\r\n    328 except ImportError as e:\r\n--> 329     raise RuntimeError(error_msg) from e\r\n\r\nRuntimeError: Drawing dask graphs with the graphviz visualization engine requires the `graphviz` python library and the `graphviz` system library.\r\n\r\nPlease either conda or pip install as follows:\r\n\r\n  conda install python-graphviz     # either conda install\r\n  python -m pip install graphviz    # or pip install and follow installation instructions\r\n```\r\n\r\nAm I doing anythong wrong?\r\n**Environment**:\r\n\r\n- Dask version: 2024.10.0\r\n- Python version: 3.11.9\r\n- Install method (conda, pip, source): PIP\r\n",
    "comments": [
      {
        "user": "rjzamora",
        "body": "Thanks for the issue @frbelotto - I can reproduce the problem with `dask.dataframe` as well."
      },
      {
        "user": "JP-sDEV",
        "body": "If I can also reproduce the problem, could I be assigned this bug @rjzamora ? Thanks! "
      },
      {
        "user": "rjzamora",
        "body": ">If I can also reproduce the problem, could I be assigned this bug\r\n\r\nSounds good @JP-sDEV - Thanks!"
      }
    ]
  },
  {
    "issue_number": 11802,
    "title": "assertionerror when trying to compute reversed cumulative sum",
    "author": "MarcoGorelli",
    "state": "closed",
    "created_at": "2025-03-03T15:08:53Z",
    "updated_at": "2025-03-04T12:01:02Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport pandas as pd\nimport dask.dataframe as dd\n\ndf = dd.from_pandas(pd.DataFrame({'a': [1,2,3], 'b': [4,5,6]}), npartitions=2)\ndf['a'][::-1].cumsum()\n```\nthrows\n```\nIn [7]: df['a'][::-1].cumsum().compute()\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[7], line 1\n----> 1 df['a'][::-1].cumsum().compute()\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4134, in Series.__getitem__(self, key)\n   4132 if isinstance(key, Series) or self.npartitions == 1:\n   4133     return super().__getitem__(key)\n-> 4134 return self.loc[key]\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_indexing.py:84, in LocIndexer.__getitem__(self, key)\n     81 if isinstance(cindexer, np.generic):\n     82     cindexer = cindexer.item()\n---> 84 return self._loc(iindexer, cindexer)\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_indexing.py:103, in LocIndexer._loc(self, iindexer, cindexer)\n    100 iindexer = self._maybe_partial_time_string(iindexer, unit=unit)\n    102 if isinstance(iindexer, slice):\n--> 103     return self._loc_slice(iindexer, cindexer)\n    104 elif is_series_like(iindexer) and not is_bool_dtype(iindexer.dtype):\n    105     return new_collection(LocList(self.obj, iindexer.values, cindexer))\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_indexing.py:157, in LocIndexer._loc_slice(self, iindexer, cindexer)\n    155 def _loc_slice(self, iindexer, cindexer):\n    156     assert isinstance(iindexer, slice)\n--> 157     assert iindexer.step in (None, 1)\n    158     return new_collection(LocSlice(self.obj, iindexer, cindexer))\n\nAssertionError: \n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n\n- Dask version: 2025.2.0\n- Python version: 3.12\n- Operating System: linux\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Thanks!\n\nI think we aren’t really supporting negative steps, that Said this shouldnt be too hard to make it work.\n\nLet me take a Look "
      }
    ]
  },
  {
    "issue_number": 11403,
    "title": "Delayed function with string argument value matching dask_key_name causes a circular reference detection",
    "author": "syagev",
    "state": "open",
    "created_at": "2024-09-21T22:28:02Z",
    "updated_at": "2025-03-03T02:03:00Z",
    "labels": [
      "needs attention"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\nDelayed function with a string argument value that happens to match the `dask_key_name` provided in that same call, causes a circular reference detection\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\n@delayed\r\ndef task(arg: str):\r\n    return arg\r\n\r\ntask('funky', dask_key_name='funky').compute()\r\n\r\n>>>  \r\nRuntimeError: Cycle detected between the following keys:\r\nE             -> funky\r\nE             -> funky\r\n```\r\n\r\n**Anything else we need to know?**:\r\nFunky edge case :)\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.9.0\r\n- Python version: 3.11\r\n- Operating System: Win11\r\n- Install method (conda, pip, source): pip \r\n",
    "comments": [
      {
        "user": "jrbourbeau",
        "body": "Thanks for the issue @syagev. I think this is expected given how we handle task names today. One needs to be careful when using `dask_key_name`. Though we recently added a new class for tasks which should let us improve situations like this in the future (xref https://github.com/dask/dask/pull/11248) \r\n\r\n> Funky edge case :)\r\n\r\nIndeed!"
      }
    ]
  },
  {
    "issue_number": 11404,
    "title": "Invalid `validate` argument in `dask.dataframe.merge`",
    "author": "noahblakesmith",
    "state": "open",
    "created_at": "2024-09-22T01:39:45Z",
    "updated_at": "2025-03-03T02:02:59Z",
    "labels": [
      "dataframe",
      "needs attention",
      "enhancement"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\nI understand your [documentation](https://docs.dask.org/en/stable/generated/dask.dataframe.merge.html) suggests `merge` allows a `validate` argument, like the `pandas` [equivalent](https://pandas.pydata.org/docs/reference/api/pandas.merge.html). However, I get an error when I try.\r\n\r\nThanks so much in advance!\r\n\r\n**Minimal complete verifiable example**:\r\n\r\n```python\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\na = pd.DataFrame({\"id\": [\"foo\", \"bar\", \"baz\", \"foo\"], \"value\": [1, 2, 3, 5]})\r\n\r\nb = pd.DataFrame({\"id\": [\"foo\", \"bar\", \"baz\", \"foo\"], \"value\": [5, 6, 7, 8]})\r\n\r\nc = dd.merge(\r\n    left=a,\r\n    right=b,\r\n    on=[\"id\"],\r\n    indicator=True,\r\n    validate=\"one_to_one\"\r\n)\r\n\r\nTraceback (most recent call last):\r\n  File \"~/example.py\", line 8, in <module>\r\n    c = dd.merge(\r\nTypeError: merge() got an unexpected keyword argument 'validate'\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.9.0\r\n- Python version: 3.10.14\r\n- Operating System: Linux\r\n- Install method (conda, pip, source): pip\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "That's indeed not supported. Might be easy to add, but haven't looked closely yet."
      }
    ]
  },
  {
    "issue_number": 11435,
    "title": "Efficient cloning of taskgraphs on top of new input partitions?",
    "author": "lgray",
    "state": "open",
    "created_at": "2024-10-16T17:23:26Z",
    "updated_at": "2025-03-03T02:02:58Z",
    "labels": [
      "discussion",
      "needs attention",
      "enhancement",
      "needs triage"
    ],
    "body": "Hi!\r\n\r\nI'm a high energy particle physicist using dask, [dask-awkward](https://github.com/dask-contrib/dask-awkward), and [dask-histogram](https://github.com/dask-contrib/dask-histogram) to compute complex analyses over billions of rows with many variations of systematic uncertainties to create rich statistical models of our collider data.\r\n\r\nThe task graphs that we generate are often rather complex and can easily reach thousands of nodes before optimization for a single dataset, and we then construct the task graph for multiple datasets. This can be time consuming (tens of minutes for a ~200 datasets!), resulting in sluggish user experience that is largely because of repeated work. The structure of the task graph likely only needs to be calculated 2-3 times at most, depending on if the dataset is signal simulation, background simulation, or experiment data. \r\n\r\nWould it be possible to instead calculate the structure, which is the expensive part, once and then re-key the graph for new input partitions (and rekeying all dependents of the inputs)? This should be much faster since it's essentially walking through the taskgraph, calculating some hashes, and creating the appropriate number of new partitions. This approach may even work for tree-reduction or repartitioning layers (in modes where you don't need to know the partition structure a priori), since these are often calculated only when the low level graph is finally materialized.\r\n\r\nHas this operation already been implemented somewhere (I couldn't find anything)?\r\nIs this a reasonable thing to do?\r\nAre there any missing necessary components in dask to implementing a strategy like this?\r\n\r\nThanks in advance!",
    "comments": [
      {
        "user": "mrocklin",
        "body": "My first suggestion would be to profile graph construction.  That might\r\ngive you a better understanding of what's slowing things down and give you\r\nideas on where to focus effort.  It could be that what you suggest is the\r\nbest course of action, but if you haven't yet gone through a round of\r\nprofiling and optimization I wouldn't be surprised if there was a 10x cost\r\nreduction in there somewhere.\r\n\r\nOn Wed, Oct 16, 2024, 12:23 PM Lindsey Gray ***@***.***>\r\nwrote:\r\n\r\n> Hi!\r\n>\r\n> I'm a high energy particle physicist using dask, dask-awkward\r\n> <https://github.com/dask-contrib/dask-awkward>, and dask-histogram\r\n> <https://github.com/dask-contrib/dask-histogram> to compute complex\r\n> analyses over billions of rows with many variations of systematic\r\n> uncertainties to create rich statistical models of our collider data.\r\n>\r\n> The task graphs that we generate are often rather complex and can easily\r\n> reach thousands of nodes before optimization for a single dataset, and we\r\n> then construct the task graph for multiple datasets. This can be time\r\n> consuming (tens of minutes for a ~200 datasets!), resulting in sluggish\r\n> user experience that is largely because of repeated work. The structure of\r\n> the task graph likely only needs to be calculated 2-3 times at most,\r\n> depending on if the dataset is signal simulation, background simulation, or\r\n> experiment data.\r\n>\r\n> Would it be possible to instead calculate the structure, which is the\r\n> expensive part, once and then re-key the graph for new input partitions\r\n> (and rekeying all dependents of the inputs)? This should be much faster\r\n> since it's essentially walking through the taskgraph, calculating some\r\n> hashes, and creating the appropriate number of new partitions. This\r\n> approach may even work for tree-reduction or repartitioning layers (in\r\n> modes where you don't need to know the partition structure a priori), since\r\n> these are often calculated only when the low level graph is finally\r\n> materialized.\r\n>\r\n> Has this operation already been implemented somewhere? Is this a\r\n> reasonable thing to do? Are there any missing necessary components in dask\r\n> to implementing a strategy like this?\r\n>\r\n> Thanks in advance!\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/dask/dask/issues/11435>, or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AACKZTCUKHDLNN2VTFFHL53Z32OKNAVCNFSM6AAAAABQB6VTB6VHI2DSMVQWIX3LMV43ASLTON2WKOZSGU4TENJWGM3DSNA>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>\r\n"
      },
      {
        "user": "lgray",
        "body": "@mrocklin Thanks for the reply - we have already done one pass of extensive profile guided optimization, which got this process from taking > 1 hour to the ~10 minutes quoted above."
      },
      {
        "user": "mrocklin",
        "body": "I think the next question would then be \"what is the slow part?\"\r\n\r\nAt the end of the day making layers of a Dask graph is just constructing Python dictionaries.  How to make a Python dictionary quickly isn't something that we really control.  It has much more to do with the code in the libraries you mention, rather than in anything in the core dask library itself.  If you think you can make a dictionary more quickly by creating a template first then that makes sense to pursue.  This isn't something that we have much say in.  \r\n\r\nIf there is some function in the Dask library that is particularly slow then we can look at that function.  My guess though is that you'll find that most of the issue here is downstream of the core dask library."
      }
    ]
  },
  {
    "issue_number": 11439,
    "title": "Generic type support for DaskCollection interface",
    "author": "dbalabka",
    "state": "open",
    "created_at": "2024-10-17T14:18:03Z",
    "updated_at": "2025-03-03T02:02:57Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "**Context**\r\nI would like to provide proper type hints for the `get_mean` function to improve the static analysis using the mypy and make my code prone to silly type issues.\r\n```python\r\nimport numpy as np\r\nimport dask .dataframe as dd\r\nimport pandas as pd\r\nfrom dask.typing import DaskCollection\r\nfrom dask_expr import Scalar as DaskExprScalar\r\nfrom dask.dataframe.core import Scalar as DaskScalar\r\n\r\nddf = dd.from_pandas(pd.DataFrame({'x': [1, 2, 3, 4, 5]}))\r\n\r\ndef get_mean(ddf: dd.DataFrame) -> DaskExprScalar:\r\n    return ddf['x'].mean()\r\n\r\nassert(True == isinstance(get_mean(ddf), DaskExprScalar))\r\nassert(True == isinstance(get_mean(ddf), DaskCollection))\r\n\r\nprint(type(get_mean(ddf)))\r\n```\r\n```\r\n<class 'dask_expr._collection.Scalar'>\r\n```\r\n\r\n**Request**\r\nMypy static analysis will benefit from extra information about the generic type that the `compute()` function returns:\r\n```\r\ndef get_mean(ddf: dd.DataFrame) -> DaskExprScalar[np.float64]:\r\n    return ddf['x'].mean()\r\n\r\nassert(np.float64 == type(get_mean(ddf).compute()))\r\n```\r\nAlso, Dask documentation can provide some guidelines on how to apply proper typing. ",
    "comments": []
  },
  {
    "issue_number": 11442,
    "title": "Zetas- Ram crashes dask (GPU/CPU)",
    "author": "kh-abd-kh",
    "state": "open",
    "created_at": "2024-10-18T12:15:33Z",
    "updated_at": "2025-03-03T02:02:56Z",
    "labels": [
      "documentation",
      "needs attention",
      "gpu"
    ],
    "body": "Hi, Thank you for your  hard work.\r\n\r\nI am evaluating 2d  zetas(twisted with harmonic polynomials): the lemniscate.\r\njust sum over x,y for (x^4 - 6. * x^2 * y^2 + y^4)/(x^2+y^2)^4\r\nI can do this in numba/cython/c/cuda up to 10**12 terms (1 000 000,1 000 000)\r\n\r\nNow, with dask,  it crahes my WSL completly even for small values (100 000,100 000)\r\n\r\nHere the GPU code (the CPU is just Client)\r\n\r\n############################################\r\n\r\n```python\r\nimport numpy as np\r\nimport cupy as cp\r\n\r\nimport dask.array as da\r\nimport dask\r\n\r\nfrom dask_cuda import LocalCUDACluster\r\nfrom dask.distributed import Client\r\n\r\ncluster = LocalCUDACluster()\r\nclient = Client(cluster)\r\nclient\r\n\r\ndef zet(x,y):\r\n    return ((x+1)**4 -6.*((x+1)**2)*((y+1)**2) + (y+1)**4)/((x+1)**2+(y+1)**2)**4\r\n\r\nwith dask.config.set({\"array.backend\": \"cupy\"}):\r\n    xg = da.fromfunction(zet,shape=(50000,50000),dtype=\"float64\")\r\n\r\n%time xg.sum().compute()\r\n```\r\n\r\n\r\n############################################\r\n\r\n\r\nIt crahes for higher values of shape (100000,100000). Also when I put some higher values\r\nof chunks (i have 6G gpu), it crashes.\r\n\r\nThe RAM consumption increases then it craches and the WSL terminal disapper\r\n\r\n```\r\nsystem:\r\n\r\n└─$ pip list|grep dask\r\ndask                         2024.9.0\r\ndask-cuda                    24.12.0a7\r\ndask-cudf-cu12               24.12.0a188\r\ndask-expr                    1.1.14\r\nrapids-dask-dependency       24.12.0a6\r\n#################################\r\ncuda-python                  12.6.0\r\ncudf-cu12                    24.12.0a188\r\ncupy-cuda12x                 13.3.0\r\ndask-cuda                    24.12.0a7\r\ndask-cudf-cu12               24.12.0a188\r\n\r\nlibcudf-cu12                 24.12.0a188\r\nnumba-cuda                   0.0.17\r\nnvidia-cublas-cu12           12.6.3.3\r\nnvidia-cuda-cupti-cu12       12.6.80\r\nnvidia-cuda-nvcc-cu12        12.6.77\r\nnvidia-cuda-runtime-cu12     12.6.77\r\nnvidia-cudnn-cu12            9.5.0.50\r\nnvidia-cufft-cu12            11.3.0.4\r\nnvidia-cusolver-cu12         11.7.1.2\r\nnvidia-cusparse-cu12         12.5.4.2\r\nnvidia-nccl-cu12             2.23.4\r\nnvidia-nvcomp-cu12           4.0.1.0\r\nnvidia-nvjitlink-cu12        12.6.77\r\npycuda                       2024.1.2\r\npylibcudf-cu12               24.12.0a188\r\npynvjitlink-cu12             0.3.0\r\nrmm-cu12                     24.12.0a16\r\n\r\nOver kali/Ubuntu22  WSL \r\nDevice 0: \"NVIDIA GeForce GTX 1660 Ti with Max-Q Design\"\r\n  CUDA Driver Version / Runtime Version          12.7 / 12.6\r\n  CUDA Capability Major/Minor version number:    7.5\r\n  Total amount of global memory:                 6144 MBytes (6442123264 bytes)\r\n\r\nMy system: \r\nRyzen 7/4800  16GB RAM\r\nWin 11\r\n```\r\n\r\nAny help will very much appreciated.\r\n\r\nThank you.\r\n",
    "comments": [
      {
        "user": "kh-abd-kh",
        "body": "sorry: the editor refactor my code and removed spaces."
      },
      {
        "user": "phofl",
        "body": "@dask/gpu "
      },
      {
        "user": "pentschev",
        "body": "If I understand correctly from your code, it seems the data you're generating is `shape=(50000,50000),dtype=\"float64\"`, is that correct? This is 20GB where you have a 6GB GPU, that will not fit."
      }
    ]
  },
  {
    "issue_number": 11457,
    "title": "``test_tokenize_dataclass`` is failing for Python 3.13",
    "author": "phofl",
    "state": "open",
    "created_at": "2024-10-24T13:34:57Z",
    "updated_at": "2025-03-03T02:02:55Z",
    "labels": [
      "upstream",
      "needs attention"
    ],
    "body": "Reproducer:\r\n\r\n```\r\n    a1 = ADataClass(1)\r\n    # Same name, same values, new definition: tokenize differently\r\n    ADataClassRedefinedDifferently = dataclasses.make_dataclass(\r\n        \"ADataClass\", [(\"a\", Union[int, str])]\r\n    )\r\n    assert check_tokenize(a1) != check_tokenize(ADataClassRedefinedDifferently(1))\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "jrbourbeau",
        "body": "Okay, so this is a weird one. After poking around I found that adding this line makes this test pass again\r\n\r\n```diff\r\ndiff --git a/dask/tokenize.py b/dask/tokenize.py\r\nindex 72ba96b0b..9655faa80 100644\r\n--- a/dask/tokenize.py\r\n+++ b/dask/tokenize.py\r\n@@ -263,6 +263,7 @@ def _normalize_dataclass(obj):\r\n     params = obj.__dataclass_params__\r\n     params = [(attr, getattr(params, attr)) for attr in params.__slots__]\r\n\r\n+    normalize_object(type(obj))\r\n     return normalize_object(type(obj)), params, fields\r\n```\r\n\r\nSomehow `normalize_object(type(obj))` returns a different result the first time it's called, but subsequent calls all return the same result 🤷 Feels like some sort of caching issue, but I haven't been able to narrow down further yet "
      },
      {
        "user": "fjetter",
        "body": "> Somehow normalize_object(type(obj)) returns a different result the first time it's called, but subsequent calls all return the same result 🤷 Feels like some sort of caching issue, but I haven't been able to narrow down further ye\r\n\r\nI've encountered something like this in the past with pickle. Pickle is mutating the objects by attaching new attributes to cache stuff. Hard to tell or reproduce. In those worst case situations we're ending up calling pickle multiple times when tokenizing an object. That's very slow but we don't have a better way of doing this right now. No idea if the situation is similar for dataclasses"
      }
    ]
  },
  {
    "issue_number": 11458,
    "title": "Deprecate legacy low level optimizers",
    "author": "fjetter",
    "state": "open",
    "created_at": "2024-10-25T10:40:59Z",
    "updated_at": "2025-03-03T02:02:54Z",
    "labels": [
      "discussion",
      "needs attention",
      "deprecation"
    ],
    "body": "We're offering some functionality for low level task graph rewrites/optimizations, see https://docs.dask.org/en/stable/optimize.html\r\n\r\nI propose to deprecate and remove most, if not all, of this functionality. With https://github.com/dask/dask/issues/9969 most of this will break. Some/all of this _could_ be migrated but I am not entirely convinced this is worth it. \r\n\r\n\r\nExplicitly, I propose to remove\r\n- cull (keep internally)\r\n- fuse (keep internally, possibly replace with simpler task spec version)\r\n- fuse_linear (keep internally, possibly replace with simpler task spec version)\r\n- inline_functions (There is rudimentary support for this in task spec already)\r\n- functions_of\r\n- Rewrite Rules\r\n\r\n\r\n(Re)Moving these things also means that magical kwargs to compute like `fuse_keys`, `fast_functions`, `inline_functions_fast_functions` would vanish.\r\n\r\nFor now, we'd likely keep the fuse functions but keep it pretty much internal. It is currently only executed for the array code so keeping it there in place still makes sense (for now) but I don't believe this is a valid function to expose.\r\n\r\nI'm a bit on the fence regarding custom optimizers, e.g. the possibility to register an optimizer function with something like `dask.config.set(array_optimize=my_optimizer)`, see also https://docs.dask.org/en/stable/optimize.html#customizing-optimization I suspect there are very few power users even capable of writing this kind of code. It will certainly break and I'm not entirely convinced this is worth restoring.\r\n\r\ncc @mrocklin in case you have insights or an opinion on any of this",
    "comments": [
      {
        "user": "mrocklin",
        "body": "I'm generally ok with removing optimizers if we think they aren't useful for performance.  I'd want to verify that things like `cull` really are handled broadly.  You'd probably have a better sense for that then me though.  \r\n\r\nIn general, I don't have conviction that this is a bad idea, so no blockers from me (but I can't say if it's a good idea with any confidence)"
      },
      {
        "user": "mrocklin",
        "body": "Also, it's probably worth checking with a few of the downstream collections like xarray, rapids, geopandas"
      },
      {
        "user": "fjetter",
        "body": "With https://github.com/dask/dask/pull/11525 the first couple low level optimizers are removed from the default optimization chain for arrays. Once we remove the legacy dataframe I believe most of the low level optimizers are dead"
      }
    ]
  },
  {
    "issue_number": 11470,
    "title": "Newer `dask[distributed]` version `scatter` method hangs",
    "author": "ebaker-gh",
    "state": "open",
    "created_at": "2024-10-30T03:08:40Z",
    "updated_at": "2025-03-03T02:02:53Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n- Using `dask[distributed]==2022.2.0`, this same code runs in 23 seconds for a `0.8 GB` DataFrame.\r\n- Using `dask[distributed]==2024.8.0`, this same code **times out** for a `0.8 GB` DataFrame (many hours, potentially hangs indefinitely).\r\n- Using `dask[distributed]==2024.8.0`, this same code runs in 1 second for a `0.0073 GB` DataFrame (sampled down original data but didn't change functionality).\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\n  logger.info(\"Broadcasting Pandas DataFrame in full to all nodes...\")\r\n  shared_df = dask_client.scatter(pdf, broadcast=True)\r\n```\r\n\r\n**Anything else we need to know?**:\r\n- I tried upgrading to the newest `Pandas`, `PyArrow`, `Dask` versions all available in `Python3.9` (which I'm currently restricted to at work), but nothing seems to have improved it.\r\n\r\n**Environment**:\r\n\r\n- Dask version: `dask[distributed]==2024.8.0`\r\n- Python version: `Python3.9`\r\n- Operating System: `Linux`\r\n- Install method (conda, pip, source): `pip`\r\n",
    "comments": [
      {
        "user": "fjetter",
        "body": "Is the same slowness observed with `scatter(..., hash=False)`?\r\n\r\nIf so, can you please describe the data you are scattering? What is the data type, if it pandas, what are the column dtypes. It would be ideal if you could provide a minimal example.\r\n\r\nI strongly suspect that this is related to changes in `tokenize` but I'm surprised that this won't terminate"
      },
      {
        "user": "ebaker-gh",
        "body": "@fjetter I appreciate the quick feedback - I tried your suggestion of using `hash=False`, but no luck.\r\n\r\n---\r\n\r\n**Background**: \r\nWe are scattering a Pandas DataFrame in full to all workers once so that we can perform cluster-robust linear regression while keeping the whole dataframe intact, without having to pass the large df around to all ~250 metric analysis tasks that are being executed on the workers.\r\n\r\n---\r\n\r\n**DataFrame Info:**\r\nShape (_row, col_): `(1_453_548, 85)`\r\n\r\nMost of the columns are metrics of some form (`date`, `integer`, `float`, `bool`, or `string`). \r\nThere are some packed JSON strings in there as well, but they are fairly small.\r\nNote that Pandas is treating many of the columns that should be `strings` as `objects`.\r\n\r\nThe `dtypes` for the columns is as follows:\r\n```\r\n[\r\n  object, object, object, datetime64[us], object, object, object, datetime64[us], datetime64[us], datetime64[us], \r\n  datetime64[us], datetime64[us], datetime64[us], datetime64[us], bool, float64, float64, bool, bool, int32, int32, int32, \r\n  int32, int32, float64, float64, float64, float64, float64, float64, float64, float64, int32, int32, int32, float64, float64, float64, \r\n  float64, float64, float64, float64, float64, float64, float64, float64, object, object, int32, datetime64[us], float64, float64, \r\n  float64, float64, float64, float64, float64, float64, float64, float64, float64, float64, int32, int32, float64, int32, float64, \r\n  float64, float64, float64, float64, object, datetime64[us], object, object, object, datetime64[us], datetime64[us], \r\n  datetime64[us], object, object, object, object\r\n]\r\n```"
      },
      {
        "user": "ebaker-gh",
        "body": "\r\n**Alternative Attempts:**\r\n\r\nI just tested the following other versions and am seeing the same behavior:\r\n- `dask[distributed]==2023.12.1`\r\n- `dask[distributed]==2023.6.1`\r\n\r\nI also tried to use the following, but the `timeout=300` (5 minutes) functionality didn't do anything - The call still hung after the log for hours which was strange:\r\n\r\n```python\r\nlogger.info(\"Broadcasting dataframe to all nodes (with timeout)...\")\r\nshared_df = dask_client.scatter(\r\n    pdf,\r\n    broadcast=True,\r\n    hash=False,\r\n    timeout=300,\r\n)\r\n```\r\n\r\nFor each execution of the job, when the scatter command is run, the Dask dashboard just shows the following with some of the worker activity lightly fluctuating:\r\n\r\n![image](https://github.com/user-attachments/assets/9879f87b-a4e0-4aa4-97fe-4e902702a8b2)\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 11500,
    "title": "Dask fails with IPv6 network",
    "author": "nambar12",
    "state": "open",
    "created_at": "2024-11-06T12:49:11Z",
    "updated_at": "2025-03-03T02:02:52Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "We've implemented a custom scheduler for dask and configured it with:\r\n\r\n```\r\njobqueue:\r\n    <ourscheduler>:\r\n      interface: eth0\r\n      protocol: \"tcp://\"\r\n\r\n```\r\nthis works OK on machine which have eth0 with IPv4 or both IPv4 and IPv6 addresses. It doesn't work on machines which have only IPv6 address. The error message is:\r\n\r\nit fails in distributed/utils.get_ip_interface() with \"interface eth0 doesn't have an IPv4 address\" message\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/infrastructure/nambar/pyapitest/test_dask.py\", line 7, in <module>\r\n    cluster = NetbatchCluster(queue='iil_critical', qslot=\"/admin/nambar\", log_directory=\"/tmp\",\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/infrastructure/nambar/pyapitest/.venv/lib/python3.12/site-packages/nbdask/nbdask.py\", line 144, in __init__\r\n    super().__init__(name=name, config_name=\"netbatch\", log_directory=log_directory,\r\n  File \"/infrastructure/nambar/pyapitest/.venv/lib/python3.12/site-packages/dask_jobqueue/core.py\", line 663, in __init__\r\n    super().__init__(\r\n  File \"/infrastructure/nambar/pyapitest/.venv/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 284, in __init__\r\n    self.sync(self._start)\r\n  File \"/infrastructure/nambar/pyapitest/.venv/lib/python3.12/site-packages/distributed/utils.py\", line 364, in sync\r\n    return sync(\r\n           ^^^^^\r\n  File \"/infrastructure/nambar/pyapitest/.venv/lib/python3.12/site-packages/distributed/utils.py\", line 440, in sync\r\n    raise error\r\n  File \"/infrastructure/nambar/pyapitest/.venv/lib/python3.12/site-packages/distributed/utils.py\", line 414, in f\r\n    result = yield future\r\n             ^^^^^^^^^^^^\r\n  File \"/infrastructure/nambar/pyapitest/.venv/lib/python3.12/site-packages/tornado/gen.py\", line 766, in run\r\n    value = future.result()\r\n            ^^^^^^^^^^^^^^^\r\n  File \"/infrastructure/nambar/pyapitest/.venv/lib/python3.12/site-packages/distributed/deploy/spec.py\", line 335, in _start\r\n    raise RuntimeError(f\"Cluster failed to start: {e}\") from e\r\nRuntimeError: Cluster failed to start: interface 'eth0' doesn't have an IPv4 address\r\n\r\n```\r\nExample of machine with both IPv4 and IPv6\r\n\r\nExample of machine with IPv6 only:\r\n2: eth0: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP group default qlen 1000\r\n    link/ether 30:3e:a7:00:67:62 brd ff:ff:ff:ff:ff:ff\r\n    altname enp8s0f0\r\n    altname ens2f0\r\n\r\nThe distributed package code seems to indeed check only for IPv6 but Dask documentation states that both IPv4 and IPv6 are supported. Is IPv6 expected to be supported?\r\n\r\n- Dask version: 2024.10.0\r\n- dask-jobqueue    0.9.0\r\n- Python version: 3.12.3\r\n- Operating System: SUSE Linux Enterprise Server 15 SP4\r\n- Install method (conda, pip, source): pip\r\n",
    "comments": []
  },
  {
    "issue_number": 11613,
    "title": "``apply_gufunc`` increases chunk sizes with ``allow_rechunk=True`` when core dimension is not chunked to -1",
    "author": "phofl",
    "state": "closed",
    "created_at": "2024-12-18T16:37:49Z",
    "updated_at": "2025-02-25T13:02:31Z",
    "labels": [
      "array",
      "needs attention",
      "bug"
    ],
    "body": "see https://github.com/pydata/xarray/issues/9907 for more discussion about this and why it matters",
    "comments": []
  },
  {
    "issue_number": 8853,
    "title": "Type hints doesn't work as expected, because they aren't present in dask source code.",
    "author": "karolzlot",
    "state": "closed",
    "created_at": "2022-03-26T13:07:04Z",
    "updated_at": "2025-02-25T11:49:59Z",
    "labels": [
      "feature"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**What happened**:\r\n\r\nI'm trying to make a function which accepts dask.dataframe. Type hints doesn't work as expected however. If I switch back to pandas, then type hints works ok.\r\n\r\n\r\n**What you expected to happen**:\r\n\r\nNo red underlines in vscode indicating errors.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport dask.dataframe as dd\r\n\r\ndef func1(\r\n    df: dd.DataFrame\r\n) -> dd.DataFrame:\r\n\r\n    df = df.set_index(\"aaa\")\r\n\r\n    df = df.rename(columns=lambda x: str(x).strip())\r\n\r\n    return df\r\n```\r\n![image](https://user-images.githubusercontent.com/11590960/160240465-79fb13a0-3326-4da3-8b42-d70c8696cfae.png)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/11590960/160240462-bbe5d7c1-f7d4-414c-bedb-edccb9c6e0d1.png)\r\n\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\nI use vscode and:\r\n\r\n![image](https://user-images.githubusercontent.com/11590960/160240698-b92e733f-081e-43ba-812a-2facfe8f6883.png)\r\n\r\nIt seems that dask (unlike pandas) doesn't have type hints defined, and they are generated by pylance...\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2022.3.0\r\n- Python version: 3.10.4\r\n- Operating System: W10\r\n- Install method (conda, pip, source): conda\r\n\r\n<!-- If you are reporting an issue such as scale stability, cluster deadlock.\r\nPlease provide a cluster dump state with this issue, by running client.dump_cluster_state()\r\n\r\nhttps://distributed.dask.org/en/stable/api.html?highlight=dump_cluster_state#distributed.Client.dump_cluster_state\r\n\r\n\r\n\r\n<details>\r\n<summary>Cluster Dump State:</summary>\r\n\r\n</details>\r\n-->",
    "comments": [
      {
        "user": "pavithraes",
        "body": "Hi @karolzlot, welcome to Dask, thanks for opening this issue! I'm not able to reproduce this (I'm on a macOS system, but I matched your Python and Dask versions). Could you please share more information about the extensions you're using and the `Python Language Server` (I'm assuming it's Pylance, but I can't reproduce using Jedi either). It may be a Windows thing?\r\n\r\nIt also seems like this is more of a usage question than a bug report or feature request. We encourage people with such questions to ask them at the [Dask Discourse](https://dask.discourse.group/). Would you mind opening a discussion topic about this over there? If you do so, feel free to close this issue and we will continue the conversation on discourse!"
      },
      {
        "user": "karolzlot",
        "body": "@pavithraes \r\nYes, I'm using pylance.\r\n\r\nI'm not sure why you can't reproduce it.\r\n\r\nYou need to enable \"Use Library Code For Types\" setting and then you should be able to see those red underlines.\r\n\r\nIt's not about usage, it's a feature request. I don't see type hints in source code and this issue is about adding them."
      },
      {
        "user": "bryanwweber",
        "body": "This is a known issue that is being worked on, see https://github.com/dask/dask/pull/8706"
      }
    ]
  },
  {
    "issue_number": 8707,
    "title": "dataframe.read_hdf raises exception for empty datasets",
    "author": "struktured",
    "state": "closed",
    "created_at": "2022-02-10T21:48:59Z",
    "updated_at": "2025-02-25T04:21:09Z",
    "labels": [
      "dataframe",
      "io"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**What happened**:\r\n\r\n```ValueError: Start row number (0) is above or equal to stop row number (0)``` when calling `dataframe.read_hdf`\r\n\r\n**What you expected to happen**:\r\n\r\nAn empty dataframe to be loaded without error.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport dask.dataframe as ddf\r\n# Exception is thrown below\r\nddf.read_hdf('some_data.h5', key='an_empty_dataset')\r\n```\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2021.10.0\r\n- Python version: 3.8\r\n- Operating System: Linux\r\n- Install method (conda, pip, source): conda\r\n\r\n",
    "comments": [
      {
        "user": "pavithraes",
        "body": "@struktured Thanks for opening this issue, and for including a minimal example! I'll take a look at this :)"
      },
      {
        "user": "pavithraes",
        "body": "@struktured Thanks for your patience!\r\n\r\nI couldn't reproduce this error, and I'm wondering if it's specific to your data? I'm also curious if you can read this (or smaller+similar data) successfully with [pandas' read_hdf](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_hdf.html#pandas-read-hdf)?\r\n\r\n[A minimal example that shows this erorr](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) will allow us to help you better. :) You can do something like:\r\n\r\n```python\r\nimport dask\r\nimport dask.dataframe as dd\r\n\r\nddf = dask.datasets.timeseries()\r\nddf.to_hdf(\"some_data.h5\", \"data\") # create toy dataset (similar to your actual data)\r\nddf_hdf = dd.read_hdf(\"some_data.h5\", \"data'\")\r\n```\r\n\r\nAlso, since this is a usage question, I'd encourage you to move it to [Dask's Discourse forum](https://dask.discourse.group/) and close this issue. We can open a dedicated issue if this turns out to be a bug in Dask!"
      },
      {
        "user": "struktured",
        "body": "It's definitely a bug but I can only manage to produce the empty dataset shape using proprietary tools.  Here's an example h5 schema that would generate the error. It has one dataset inside called `maybe_empty` with two columns `x` and `y`:\r\n\r\n```\r\nh5ls -v empty.h5\r\nOpened \"empty.h5\" with sec2 driver.\r\nmaybe_empty              Dataset {0/0}\r\n    Location:  1:800\r\n    Links:     1\r\n    Chunks:    {512} 8192 bytes\r\n    Storage:   0 logical bytes, 0 allocated bytes\r\n    Filter-0:  blosc-32001 OPT {2, 2, 16, 8192, 6, 1, 1}\r\n    Type:      struct {\r\n                   \"x\"                +0    native double\r\n                   \"y\"                +8    native double\r\n               } 16 bytes\r\n```\r\n\r\n```python\r\nimport dask.dataframe as ddf\r\nr = ddf.read_hdf(\"empty.h5\", key='maybe_empty')\r\n```\r\n\r\nwill then produce:\r\n\r\n```python\r\nValueError: Start row number (0) is above or equal to stop row number (0)\r\n```\r\n\r\nI can upload the empty h5 file to github if that helps debug it. I might also take a stab at fixing the bug myself next week if time permits.\r\n"
      }
    ]
  },
  {
    "issue_number": 11776,
    "title": "`set_index` with Series doesn't respect `drop=True` parameter",
    "author": "marvinsxtr",
    "state": "closed",
    "created_at": "2025-02-24T14:34:38Z",
    "updated_at": "2025-02-24T14:38:23Z",
    "labels": [
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\nWhen setting an index by passing a column as a `Series` to `set_index()`, the `drop=True` parameter is ignored and the column is retained in the DataFrame. This behavior is inconsistent with passing a column name (string) to `set_index()`.\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.dataframe as dd\n\n# Create a sample dataframe\ndf = dd.from_dict({'id': range(1000), 'value': [i*0.1 for i in range(1000)]}, npartitions=2)\n\n# Case 1: Using column name (works as expected - column is dropped)\ndf1 = df.set_index('id', drop=True)\nprint(\"Using column name:\")\nprint(df1.compute())\n\n# Case 2: Using Series object (bug - column is not dropped)\ndf2 = df.set_index(df[\"id\"], drop=True)\nprint(\"\\nUsing Series object:\")\nprint(df2.compute())\n```\n\nExpected Behavior:\nBoth approaches should produce the same result, with the 'id' column dropped from the resulting DataFrame when `drop=True`:\n\n```\n     value\nid        \n0      0.0\n1      0.1\n...    ...\n```\n\nActual Behavior:\nWhen using a Series object, the column is not dropped:\n\n```\n      id  value\nid             \n0      0    0.0\n1      1    0.1\n...   ...    ...\n```\n\n**Environment**:\n\n- Dask version: 2025.1.0\n- Python version: 3.12\n- Operating System: Ubuntu\n- Install method (conda, pip, source): pip\n",
    "comments": []
  },
  {
    "issue_number": 11520,
    "title": "libfabric transport support",
    "author": "BenWibking",
    "state": "open",
    "created_at": "2024-11-14T01:56:20Z",
    "updated_at": "2025-02-24T02:01:30Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "Current exascale computers use non-Infiniband networks, such as HPE Slingshot and (soon) Ultra Ethernet. Both provide application code interfaces using libfabric, and cannot use UCX. On these machines, the only way to use Dask is to transport data over TCP, which is slow.\r\n\r\nIt would be great if Dask would support libfabric as a transport layer!",
    "comments": []
  },
  {
    "issue_number": 11554,
    "title": "Use functools.cache more",
    "author": "dcherian",
    "state": "open",
    "created_at": "2024-11-21T18:31:43Z",
    "updated_at": "2025-02-24T02:01:29Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please do a quick search of existing issues to make sure that this has not been asked before. -->\r\n\r\nIt'd be great to stick a cache on `dask.array.svg.svg` and `normalize_chunks`. \r\nThese seem like highly cacheable functions, and would help with https://github.com/pydata/xarray/issues/8902. The svg would help with the `ds._repr_html_()` of that dataset.",
    "comments": [
      {
        "user": "fjetter",
        "body": "normalize_chunks is a bit weird since it accepts all kinds of input, some of them are mutable and therefore not hashable"
      },
      {
        "user": "fjetter",
        "body": "If it's always a tuple in xarray we can likely offer a \"cachable\" version (or we can cache on a best effort basis)"
      },
      {
        "user": "dcherian",
        "body": "We are indeed sending dask `tuples` always!\r\n\r\nhttps://github.com/pydata/xarray/blob/1bb867d573390509dbc0379f0fd318a6985dab45/xarray/core/dataset.py#L246-L258\r\n\r\n`preferred_chunks` comes from the storage library is always a tuple of ints.\r\n\r\n`chunk_shape` can be either a tuple of ints or with some \"auto\" sprinkled in : `('auto', 721, 1440)` "
      }
    ]
  },
  {
    "issue_number": 11547,
    "title": "fail fast with large graphs",
    "author": "dcherian",
    "state": "open",
    "created_at": "2024-11-20T17:21:22Z",
    "updated_at": "2025-02-24T02:01:29Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please do a quick search of existing issues to make sure that this has not been asked before. -->\r\n\r\nupstream issue: https://github.com/pydata/xarray/issues/9802\r\n\r\n> More data, bigger the graph is, to the point where the graph is so huge (31GB at my maximum) that the .compute() fails, with a \"error to serialize\" error in msgpack.\r\n\r\nCan dask just fail faster and tell the user that something is absolutely wrong if they've constructed this graph? Some way to find out what these large objects are would also be useful (layer names?).",
    "comments": [
      {
        "user": "phofl",
        "body": ">Some way to find out what these large objects are would also be useful (layer names?).\r\n\r\nWe chatted about this a few times internally but were worried about slowdown mostly. A debug utility might be a good solution here maybe?\r\n\r\nThat graph is indeed huge (I thought we fixed the msg pack issue though, cc @fjetter ?\r\n\r\nBut anyway, that's probably too large even if we could ship this?"
      },
      {
        "user": "dcherian",
        "body": "> But anyway, that's probably too large even if we could ship this?\r\n\r\nIt'd be nice to have the user opt-in to trying to execute such a graph. Most workloads don't need anywhere close to this."
      }
    ]
  },
  {
    "issue_number": 11566,
    "title": "Should `dask.persist` raise on non-persistable objects?",
    "author": "hendrikmakait",
    "state": "open",
    "created_at": "2024-11-25T18:20:34Z",
    "updated_at": "2025-02-24T02:01:27Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "# Problem\r\n\r\nUntil [recently](https://github.com/dask/distributed/issues/8948), `dask.persist()` supported both persistable Dask collections and ordinary Python objects as inputs. The Dask collections would be persisted (as expected) while the Python objects would be handled transparently and returned as-is in the output. \r\n\r\nTo the best of my knowledge, this behavior is not documented anywhere, and there is only a single test for this (`test_distributed.py::test_persist_nested`). \r\n\r\nTo me, this behavior seems odd: I would argue that it's reasonable for a user to expect that `dask.persist(some_large_pandas_dataframe)` actually persists that large object on a `distributed` cluster to make it available. It would also hide user errors where the user intends to persist a collection but instead persists `Future`s, e.g., by calling `persist(df.compute())` instead of `persist(df)`. \r\n\r\n# Possible solution\r\n\r\nInstead of fixing this undocumented behavior, I suggest that `persist` should raise on inputs that are no persistable Dask collection. This clarifies the intended and supported behavior, limits the amount of hidden magic, and allows us to raise meaningful errors on anti-patterns like persisting `Future`s.\r\n\r\n# Caveat\r\n\r\nThis would break current undocumented Dask behavior, and it's unclear how much users or downstream libraries rely on this.",
    "comments": [
      {
        "user": "mrocklin",
        "body": "Historically this has been useful to make algorithms that accept both Dask and non-Dask inputs.  The same is true (or at least has been true) of `dask.compute` and other top level functions.  "
      },
      {
        "user": "TomAugspurger",
        "body": "https://github.com/dask/dask-ml/pull/1004 has an example of what a workaround looks like. It's not great, but not awful. That's calling `.persist` on a `dict[int, Delayed | Future]`. I'm not sure yet why the code works like that."
      },
      {
        "user": "dcherian",
        "body": "IMO this would have a big blast radius and it would be easier to just add the workaround to `dask.persist` and/or `dask.compute`"
      }
    ]
  },
  {
    "issue_number": 11572,
    "title": "``CategoricalAccessor.as_know()`` produces incorrect dtype with \"p2p\" shuffle enabled",
    "author": "rjzamora",
    "state": "open",
    "created_at": "2024-12-02T23:37:28Z",
    "updated_at": "2025-02-24T02:01:26Z",
    "labels": [
      "needs attention",
      "bug"
    ],
    "body": "**Context**: https://github.com/dask/dask-expr/pull/659 recently adjusted the default shuffle method used by `series.unique()` (from \"tasks\" to \"p2p\"). I don't think there was anything wrong with that particular PR. However, that change exposed the fact that `.cat.as_known()` does not properly preserve the original dtype of the categories when \"p2p\" is enabled:\r\n\r\n```python\r\nimport dask\r\nimport dask.dataframe as dd\r\nfrom distributed import Client, LocalCluster\r\n\r\nwith LocalCluster() as cluster:\r\n    with Client(cluster) as client:\r\n        df = dd.from_dict({\"qid\": [1, 2, 1, 0, 2]}, npartitions=3)\r\n    \r\n        with dask.config.set({\"dataframe.shuffle.method\": \"tasks\"}):\r\n            known_tasks = df.qid.astype(\"category\").cat.as_known()\r\n    \r\n        with dask.config.set({\"dataframe.shuffle.method\": \"p2p\"}):\r\n            known_p2p = df.qid.astype(\"category\").cat.as_known()\r\n    \r\n        dd.assert_eq(known_tasks, known_p2p)\r\n```\r\n\r\n```\r\n...\r\nAssertionError: Attributes of Series are different\r\n\r\nAttribute \"dtype\" are different\r\n[left]:  CategoricalDtype(categories=[2, 0, 1], ordered=False, categories_dtype=int64)\r\n[right]: CategoricalDtype(categories=['__UNKNOWN_CATEGORIES__', 2, 0, 1], ordered=False, categories_dtype=object)\r\n```\r\n\r\nI don't think the problem is in \"p2p\" itself. Rather, the proper metadata seems to be lost before the data is shuffled. For \"tasks\", the proper metadata is recovered after the intermediate compute step. However, we aren't as lucky for \"p2p\", because we round-trip the data to PyArrow.",
    "comments": []
  },
  {
    "issue_number": 11567,
    "title": "Calling dask.array.compute_chunk_sizes() with Asynchronous Client",
    "author": "Karl5766",
    "state": "open",
    "created_at": "2024-11-26T17:23:31Z",
    "updated_at": "2025-02-24T02:01:26Z",
    "labels": [
      "needs attention",
      "needs triage"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\nThe issue I'm encountering happens when I try to run compute_chunk_sizes() function on a dask array on an asynchronous client, which seems to happen because a coroutine is not awaited after async function is called?\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\nIn the following code:\r\n\r\n```python\r\nasync def main():\r\n    import dask.array as da\r\n    import numpy as np\r\n    from distributed import Client\r\n    client = Client(threads_per_worker=12, n_workers=1, asynchronous=True)\r\n\r\n    depth = (1, 0)\r\n    arr = da.from_array(np.zeros((4, 4), dtype=np.int64), chunks=(2, 2))\r\n    print(arr.chunksize)\r\n    padded = arr.map_overlap(func=lambda x: x, depth=depth)\r\n    print(padded.chunksize)  # wrong chunk sizes\r\n    padded.compute_chunk_sizes()  # crush!!\r\n\r\n    await client.close()\r\n\r\n\r\nif __name__ == '__main__':\r\n    import asyncio\r\n    asyncio.run(main())\r\n```\r\nI got a program crush with message “‘coroutine’ object is not iterable”. This would run fine in a synchronous environment, but in my case I need an asynchronous client and was unable to run this function. How should this be handled?\r\n\r\n**Anything else we need to know?**:\r\n\r\nThis is a repost from https://dask.discourse.group/t/calling-dask-array-compute-chunk-sizes-with-asynchronous-client/3684\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.11.2\r\n- Python version: 3.10.4\r\n- Operating System: Windows 11\r\n- Install method (conda, pip, source): pip 24.1.2\r\n",
    "comments": []
  },
  {
    "issue_number": 11585,
    "title": "dask.array buggy with pandas multiindex.values / object dtype arrays",
    "author": "dcherian",
    "state": "open",
    "created_at": "2024-12-05T16:55:49Z",
    "updated_at": "2025-02-24T02:01:25Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport dask.array\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nidx = pd.MultiIndex.from_product([list(\"abc\"), [0, 1]])\r\ndask.array.from_array(idx.values, chunks=-1)[0].compute()\r\n```\r\n\r\nInterestingly\r\n```\r\narray = np.array([('a', 0), ('a', 1), ('b', 0), ('b', 1), ('c', 0), ('c', 1)], dtype=object)\r\ndask.array.from_array(array, chunks=-1)[0].compute()\r\n```\r\n\r\nsucceeds :/ even though that should be identical to `idx.values`\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.12.0\r\n- Python version:\r\n- Operating System:\r\n- Install method (conda, pip, source):\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "That's not exactly the same unfortunately :(\r\n\r\nThe first creates an array of tuples with one dimension, the second one an array with 2 dimensions but scalars instead of tuples\r\n\r\nEdit: talking about the underlying numpy array, not the dask array"
      },
      {
        "user": "phofl",
        "body": "This is tricky, not sure how to tackle this in an elegant way yet"
      },
      {
        "user": "dcherian",
        "body": "Right, my bad on the two examples not being the same. What changed here suddenly?"
      }
    ]
  },
  {
    "issue_number": 11595,
    "title": "Supporting inconsistent schemas in read_json",
    "author": "praateekmahajan",
    "state": "open",
    "created_at": "2024-12-10T18:24:48Z",
    "updated_at": "2025-02-24T02:01:24Z",
    "labels": [
      "dataframe",
      "needs attention",
      "feature"
    ],
    "body": "If you have two (jsonl) files where one contains columns `{\"id\", \"text\"}` and the other contains `{\"text\", \"id\", \"meta\"}` and you wish to read the two files using `dd.read_json([file1.jsonl, file2.jsonl], lines=True)` we run into an error\r\n\r\n```\r\nMetadata mismatch found in `from_delayed`.\r\n\r\nPartition type: `pandas.core.frame.DataFrame` \r\n(or it is Partition type: `cudf.core.dataframe.DataFrame` when backend=='cudf')\r\n+---------+-------+----------+\r\n| Column  | Found | Expected |\r\n+---------+-------+----------+\r\n| 'meta1' | -     | object   |\r\n+---------+-------+----------+ \r\n```\r\nFor what it's worth this isn't an issue in read_parquet (cpu) and for gpu the fix is in the works https://github.com/rapidsai/cudf/pull/17554/files\r\n\r\n## Guessing the rootcause\r\nIIUC in both pandas and cudf, we call `read_json_file` ([here](https://github.com/dask/dask/blob/a9396a913c33de1d5966df9cc1901fd70107c99b/dask/dataframe/io/json.py#L315)). \r\n\r\nIn the pandas case, even if `dtype` is specified, pandas doesn't prune out the non-specified columns, while cudf does (assuming prune_columns=True). Therefore the pandas case continues to fail, while `cudf` case fails on a column order vs metadata column order mismatch error (since one file has `id, text`, while the other has `text, id`.\r\n\r\nOne possible hack could be supporting `columns` arg and then performing `engine(.....)[columns]`. Another could be\r\n\r\n## MRE\r\n```python\r\nimport dask.dataframe as dd\r\nimport dask\r\nimport tempfile\r\nimport pandas as pd\r\nimport os\r\n\r\nrecords = [\r\n    {\"id\": 123, \"text\": \"foo\"},\r\n    {\r\n        \"text\": \"bar\",\r\n        \"meta1\": [{\"field1\": \"cat\"}],\r\n        \"id\": 456,\r\n    },\r\n]\r\ncolumns = [\"text\", \"id\"]\r\nwith tempfile.TemporaryDirectory() as tmpdir:\r\n    file1 = os.path.join(tmpdir, \"part.0.jsonl\")\r\n    file2 = os.path.join(tmpdir, \"part.1.jsonl\")\r\n\r\n    pd.DataFrame(records[:1]).to_json(file1, orient=\"records\", lines=True)\r\n    pd.DataFrame(records[1:]).to_json(file2, orient=\"records\", lines=True)\r\n\r\n    for backend in [\"pandas\", \"cudf\"]:\r\n        read_kwargs = dict()\r\n        if backend == \"cudf\":\r\n            read_kwargs[\"dtype\"] = {\"id\": \"str\", \"text\": \"str\"}\r\n            read_kwargs[\"prune_columns\"] = True\r\n        print(\"=\"*30)\r\n        print(f\"==== {backend=} ====\")\r\n        print(\"=\"*30)\r\n        try:\r\n            with dask.config.set({\"dataframe.backend\": backend}):\r\n                df = dd.read_json(\r\n                    [file1, file2],\r\n                    lines=True,\r\n                    **read_kwargs,\r\n                )\r\n                print(f\"{df.columns=}\")\r\n                print(f\"{df.compute().columns=}\")\r\n                print(f\"{type(df.compute())=}\")\r\n                display((df.compute()))\r\n\r\n        except Exception as e:\r\n            print(f\"{backend=} failed due to {e} \\n\")\r\n\r\n```\r\n\r\ncc @rjzamora \r\n\r\n",
    "comments": [
      {
        "user": "praateekmahajan",
        "body": "The following notebook compares the issues across json / parquet and also across two more variables of schema consistency and metadata consistency\r\nhttps://gist.github.com/praateekmahajan/f6c41fec9b9bba0b81fa2596541ec74e"
      }
    ]
  },
  {
    "issue_number": 11607,
    "title": "Inconsistent in-place assignment behavior between Dask and NumPy",
    "author": "DominikStiller",
    "state": "open",
    "created_at": "2024-12-17T13:22:33Z",
    "updated_at": "2025-02-24T02:01:23Z",
    "labels": [
      "array",
      "needs attention",
      "bug"
    ],
    "body": "When using in-place/augmented assignment operators (`+=, -+, ...`) on a NumPy array that is part of a Dask array's graph, the computed Dask array will be affected by changes to the NumPy array. I am unsure if this is the intended behavior of Dask's lazy evaluations, but it is contrary to what I expected and to what NumPy does.\r\n\r\nFor example, in the code below, I would expect modifications to `b` (even in-place), after it has been used in a calculation with `a`, to not change the result of `a`. In that case, `a` would be zero (`10-10 = 0`). With Dask, it is not (`10-(10/2) = 5`).\r\n\r\n**Minimal Complete Verifiable Example**:\r\nDask:\r\n```python\r\n>>> b = np.array(10.)\r\n>>> a = dask.array.from_array([10.]) - b\r\n>>> b /= 2\r\n>>> a.compute()\r\narray([5.])\r\n```\r\n\r\nNumPy:\r\n```python\r\n>>> b = np.array([10.])\r\n>>> a = np.array([10.]) - b\r\n>>> b /= 2\r\n>>> a\r\narray([0.])\r\n```\r\n\r\nThis issue is related to #5199 .\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.12.0\r\n- Python version: 3.12.8\r\n- Operating System: MacOS Sequoia 15.1.1\r\n- Install method (conda, pip, source): conda\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Hi, thanks for your report.\r\n\r\nThis is a bit annoying, I agree. \r\n\r\nThe main difference is that NumPy is eager, i.e. the result is calculated immediately, while dask is lazy. We *should* protect against this kind of behavior though.\r\n\r\ncc @fjetter "
      }
    ]
  },
  {
    "issue_number": 11752,
    "title": "`__setitem__` for scalars on sparse arrays",
    "author": "ilan-gold",
    "state": "closed",
    "created_at": "2025-02-17T14:49:45Z",
    "updated_at": "2025-02-21T15:14:39Z",
    "labels": [
      "array",
      "bug"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\nSetting scalars on dask arrays seems to have broken\n\n**Minimal Complete Verifiable Example**:\n\n```python\nimport dask.array as da\nfrom scipy import sparse\nimport numpy as np\n\narr = da.from_array(sparse.csr_matrix(np.array([[1, 2], [3, 4]])))\narr[0, 0] = 1\n```\n\n\n```pytb\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 6\n      3 import numpy as np\n      5 arr = da.from_array(sparse.csr_matrix(np.array([[1, 2], [3, 4]])))\n----> 6 arr[0, 0] = 1\n\nFile ~/Projects/Theis/anndata/venv/lib/python3.12/site-packages/dask/array/core.py:1949, in Array.__setitem__(self, key, value)\n   1945         raise ValueError(\"cannot convert float infinity to integer\")\n   1947 # Suppress dtype broadcasting; __setitem__ can't change the dtype.\n   1948 # Use asanyarray to retain e.g. np.ma objects.\n-> 1949 value = asanyarray(value, dtype=self.dtype, like=self)\n   1951 if isinstance(key, Array) and (\n   1952     key.dtype.kind in \"iu\"\n   1953     or (key.dtype == bool and key.ndim == 1 and self.ndim > 1)\n   1954 ):\n   1955     key = (key,)\n\nFile ~/Projects/Theis/anndata/venv/lib/python3.12/site-packages/dask/array/core.py:4900, in asanyarray(a, dtype, order, like, inline_array)\n   4895         return a.map_blocks(\n   4896             # Pass the dtype parameter to np.asanyarray, not to map_blocks\n   4897             partial(np.asanyarray, like=like_meta, dtype=dtype, order=order)\n   4898         )\n   4899     else:\n-> 4900         a = np.asanyarray(a, like=like_meta, dtype=dtype, order=order)\n   4902 a = from_array(\n   4903     a,\n   4904     chunks=a.shape,\n   (...)\n   4907     inline_array=inline_array,\n   4908 )\n   4909 return _as_dtype(a, dtype)\n\nTypeError: The `like` argument must be an array-like that implements the `__array_function__` protocol.\n```\n\n**Anything else we need to know?**:\nThis worked on `2025.1.0`\n\n**Environment**:\n\n- Dask version: `2025.2.0`\n- Python version: `3.12.2`\n- Operating System: `mac`\n- Install method (conda, pip, source): `uv`\n",
    "comments": [
      {
        "user": "phofl",
        "body": "@crusaderky I think this is caused by one of your setitem PRs?"
      },
      {
        "user": "crusaderky",
        "body": "#11755 is ready for review"
      }
    ]
  },
  {
    "issue_number": 11749,
    "title": "Support `scipy.sparse.sparray`s",
    "author": "flying-sheep",
    "state": "closed",
    "created_at": "2025-02-14T14:56:43Z",
    "updated_at": "2025-02-21T13:28:08Z",
    "labels": [
      "needs triage"
    ],
    "body": "Since older versions of scipy don’t have it, there could be a conditional branch here: `if hasattr(scipy.sparse, \"sparray\"): ...`\n\nhttps://github.com/dask/dask/blob/4d71629d1f22ced0dd780919f22e70a642ec6753/dask/array/backends.py#L212-L232",
    "comments": []
  },
  {
    "issue_number": 11765,
    "title": "What scalar type is expected for DataFrame.divisions?",
    "author": "TomAugspurger",
    "state": "closed",
    "created_at": "2025-02-19T19:53:36Z",
    "updated_at": "2025-02-21T10:16:53Z",
    "labels": [
      "dataframe"
    ],
    "body": "**Describe the issue**:\n\nOn dask main, `DataFrame.divisions` is a `tuple[np.ndarrray]` where each element is a scalar ndarray:\n\n**Minimal Complete Verifiable Example**:\n\n```python\nIn [1]: import dask.dataframe as dd, pandas as pd\nIn [2]: index = [1, 5, 10, 11, 12, 100, 200, 300]\n   ...: df = pd.DataFrame({\"a\": range(8), \"index\": index}).set_index(\"index\")\n   ...: ddf = dd.from_pandas(df, npartitions=3)\n   ...: ddf.divisions\n   ...: \nOut[2]: (np.int64(1), np.int64(11), np.int64(200), np.int64(300))\n```\n\nIn this case, that's coming from `sorted_division_locations`, which eventually does a bunch of `Index.__getitem__` calls, which returns scalar ndarrays:\n\n```\n(Pdb) pp seq\nIndex([1, 5, 10, 11, 12, 100, 200, 300], dtype='int64', name='index')\n(Pdb) pp seq[0]\nnp.int64(1)\n```\n\n**Anything else we need to know?**:\n\nWould we prefer that divisions be a tuple of plain Python scalars? IIRC, that's what it was previously, and would I think fit better with how it's used (comparisons).\n\nxref https://github.com/rapidsai/dask-upstream-testing/issues/9, where `pd.isna` check downstream of `.divisions` is causing issues for dask-cudf. But I suspect that solving this when we build divisions might be preferable. I'm able to solve this pretty easily in `sorted_division_locations`, but I think there are other spots where divisions is created (like indexing) and wanted to check whether anyone had a preference before going any further.\n\n**Environment**:\n\n- Dask version: `2025.2.0+11.gf67825a60`\n- Python version:\n- Operating System:\n- Install method (conda, pip, source):\n",
    "comments": [
      {
        "user": "phofl",
        "body": "Python scalars would definitely be preferable, so happy with any changes you want to make"
      }
    ]
  },
  {
    "issue_number": 11759,
    "title": "`optimize` not lowering fully",
    "author": "fjetter",
    "state": "closed",
    "created_at": "2025-02-18T15:15:44Z",
    "updated_at": "2025-02-19T11:32:23Z",
    "labels": [
      "dataframe",
      "dask-expr"
    ],
    "body": "```python\nimport pandas as pd\nfrom dask.dataframe import from_pandas\ndf = pd.DataFrame(\n    {\n        \"A\": [1, 2, 3, 4, 5, 6, 7, 8, 9],\n        \"B\": [9, 8, 7, 6, 5, 4, 3, 2, 1],\n        \"C\": [True, False, True] * 3,\n    },\n    columns=list(\"ABC\"),\n)\nddf = from_pandas(df, 2)\nexpr = ddf[ddf.C.repartition([0, 2, 5, 8])].expr\nexpr = expr.optimize()\nexpr.__dask_graph__()\n```\n\nThe graph cannot be generated and is running into a `NotImplementedError: Expressions should define either _layer (full dictionary) or _task (single task).  This expression <class 'dask.dataframe.dask_expr._expr.FilterAlign'> defines neither` exception.\n\nThe problem arises after the second `simplify` in `optimize_until` which generates another expression that is not fully lowered",
    "comments": []
  },
  {
    "issue_number": 11746,
    "title": "KeyError when indexing into Series after calling `to_series` on Scalar",
    "author": "MarcoGorelli",
    "state": "closed",
    "created_at": "2025-02-14T09:26:09Z",
    "updated_at": "2025-02-18T15:24:17Z",
    "labels": [
      "dataframe",
      "bug"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\n\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\n\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\n\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\n-->\n\n**Describe the issue**:\n\n**Minimal Complete Verifiable Example**:\n\n```python\nIn [1]: import dask.dataframe as dd\n\nIn [2]: import pandas as pd\n\nIn [3]: data = {\"a\": [1, 3, 2]}\n\nIn [4]: df = dd.from_pandas(pd.DataFrame(data), npartitions=2)\n\nIn [5]: df['a'].sum().to_series().fillna(0)[0].compute()\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-> 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:2606, in pandas._libs.hashtable.Int64HashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:2630, in pandas._libs.hashtable.Int64HashTable.get_item()\n\nKeyError: 0\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[5], line 1\n----> 1 df['a'].sum().to_series().fillna(0)[0].compute()\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:489, in FrameBase.compute(self, fuse, concatenate, **kwargs)\n    487     out = out.repartition(npartitions=1)\n    488 out = out.optimize(fuse=fuse)\n--> 489 return DaskMethodsMixin.compute(out, **kwargs)\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/base.py:374, in DaskMethodsMixin.compute(self, **kwargs)\n    350 def compute(self, **kwargs):\n    351     \"\"\"Compute this dask collection\n    352 \n    353     This turns a lazy Dask collection into its in-memory equivalent.\n   (...)\n    372     dask.compute\n    373     \"\"\"\n--> 374     (result,) = compute(self, traverse=False, **kwargs)\n    375     return result\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/base.py:662, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    659     postcomputes.append(x.__dask_postcompute__())\n    661 with shorten_traceback():\n--> 662     results = schedule(dsk, keys, **kwargs)\n    664 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-> 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 0\n```\nIf I `compute` before the `[0]`, then I get:\n```\nIn [6]: df['a'].sum().to_series().fillna(0).compute()\nOut[6]: \n0    6\ndtype: int64\n```\nso I'd have expected the `[0]` to work\n\n**Anything else we need to know?**:\nspotted in Narwhals\n\n**Environment**:\n\n- Dask version: 2025.1.0\n- Python version: 3.12\n- Operating System: linux\n- Install method (conda, pip, source): pip\n",
    "comments": [
      {
        "user": "MarcoGorelli",
        "body": "Alternatively, `Scalar.fillna` would also work for us, but that raises too:\n```python\nIn [1]: import dask.dataframe as dd\n\nIn [2]: import pandas as pd\n\nIn [3]: data = {\"a\": [1, 3, 2]}\n\nIn [4]: df = dd.from_pandas(pd.DataFrame(data), npartitions=2)\n\nIn [5]: df['a'].sum().fillna(0)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py:152, in Expr.__getattr__(self, key)\n    151 try:\n--> 152     return object.__getattribute__(self, key)\n    153 except AttributeError as err:\n\nAttributeError: 'Sum' object has no attribute '_validate_axis'\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:628, in FrameBase.__getattr__(self, key)\n    625 try:\n    626     # Fall back to `expr` API\n    627     # (Making sure to convert to/from Expr)\n--> 628     val = getattr(self.expr, key)\n    629     if callable(val):\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py:173, in Expr.__getattr__(self, key)\n    172 link = \"https://github.com/dask-contrib/dask-expr/blob/main/README.md#api-coverage\"\n--> 173 raise AttributeError(\n    174     f\"{err}\\n\\n\"\n    175     \"This often means that you are attempting to use an unsupported \"\n    176     f\"API function. Current API coverage is documented here: {link}.\"\n    177 )\n\nAttributeError: 'Sum' object has no attribute '_validate_axis'\n\nThis often means that you are attempting to use an unsupported API function. Current API coverage is documented here: https://github.com/dask-contrib/dask-expr/blob/main/README.md#api-coverage.\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\nCell In[5], line 1\n----> 1 df['a'].sum().fillna(0)\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:2003, in FrameBase.fillna(self, value, axis)\n   2001 @derived_from(pd.DataFrame)\n   2002 def fillna(self, value=None, axis=None):\n-> 2003     axis = self._validate_axis(axis)\n   2004     if axis == 1:\n   2005         return self.map_partitions(M.fillna, value, axis=axis)\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:634, in FrameBase.__getattr__(self, key)\n    631     return val\n    632 except AttributeError:\n    633     # Raise original error\n--> 634     raise err\n\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:623, in FrameBase.__getattr__(self, key)\n    620 def __getattr__(self, key):\n    621     try:\n    622         # Prioritize `FrameBase` attributes\n--> 623         return object.__getattribute__(self, key)\n    624     except AttributeError as err:\n    625         try:\n    626             # Fall back to `expr` API\n    627             # (Making sure to convert to/from Expr)\n\nAttributeError: 'Scalar' object has no attribute '_validate_axis'\n```"
      },
      {
        "user": "phofl",
        "body": "Thanks for the report. The first one is definitely a bug. Put up a PR to fix this.\n\nThe second one is a bit more ambiguous. Generally, we aren't aiming to implement these methods on a Scalar, the fact that they are available is more a bug at the moment and we are thinking on how we can get rid of this."
      },
      {
        "user": "MarcoGorelli",
        "body": "thanks! i was just experimenting with the second one as a potential workaround, but if the first is a bug and can be fixed then that'd be perfect 🙌 "
      }
    ]
  },
  {
    "issue_number": 11751,
    "title": "Array slicing very slow with small chunksizes",
    "author": "Illviljan",
    "state": "closed",
    "created_at": "2025-02-17T06:28:35Z",
    "updated_at": "2025-02-17T15:29:34Z",
    "labels": [
      "needs triage"
    ],
    "body": "### What happened?\n\nThe lazy computation time seems to be dependent on the indexers size in `Dataset.reindex`, which at some point calls the dask array function slice_array. \n\n### What did you expect to happen?\n\nClose to constant time with lazy reindexing. \n\n### Minimal Complete Verifiable Example\nWith xarray:\n```Python\nimport numpy as np\nimport dask.array as da\n\nimport xarray as xr\n\nds = xr.Dataset(\n    data_vars={\n        \"variable_name\": (\"time\", da.from_array(\n            np.array([\"test\"], dtype=str), chunks=(1,)\n        ))\n    },\n    coords={\"time\": (\"time\", np.array([0]))}\n)\n\n%timeit ds.reindex(time=np.linspace(0, 10, 50), method=\"nearest\")\n# 8.72 ms ± 148 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n%timeit ds.reindex(time=np.linspace(0, 10, 100), method=\"nearest\")\n# 16.3 ms ± 424 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n%timeit ds.reindex(time=np.linspace(0, 10, 1000), method=\"nearest\")\n# 152 ms ± 1.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n```\n\n### MVCE confirmation\n\n- [x] Minimal example — the example is as focused as reasonably possible to demonstrate the underlying issue in xarray.\n- [x] Complete example — the example is self-contained, including all data and the text of any traceback.\n- [x] Verifiable example — the example copy & pastes into an IPython prompt or [Binder notebook](https://mybinder.org/v2/gh/pydata/xarray/main?urlpath=lab/tree/doc/examples/blank_template.ipynb), returning the result.\n- [x] New issue — a search of GitHub Issues suggests this is not a duplicate.\n- [x] Recent environment — the issue occurs with the latest version of xarray and its dependencies.\n\n### Relevant log output\n\n```Python\n\n```\n\n### Anything else we need to know?\n\nThis case shows up for example when using `ds.interp` with string variables.\n\nhttps://github.com/pydata/xarray/issues/10054\n\n### Environment\n\n<details>\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:04:44) [MSC v.1940 64 bit (AMD64)]\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: en\nLOCALE: ('Swedish_Sweden', '1252')\nlibhdf5: 1.14.3\nlibnetcdf: 4.9.2\n\nxarray: 2024.7.1.dev363+g99426cbb.d20240904\npandas: 2.2.2\nnumpy: 2.2.1\nscipy: 1.14.1\nnetCDF4: 1.7.1\npydap: 3.5\nh5netcdf: 1.3.0\nh5py: 3.11.0\nzarr: 2.18.2\ncftime: 1.6.4\nnc_time_axis: 1.4.1\niris: 3.9.0\nbottleneck: 1.4.0\ndask: 2024.11.2\ndistributed: 2024.11.2\nmatplotlib: 3.9.2\ncartopy: 0.23.0\nseaborn: 0.13.2\nnumbagg: None\nfsspec: 2024.6.1\ncupy: None\npint: None\nsparse: None\nflox: 0.9.10\nnumpy_groupies: 0.11.2\nsetuptools: 73.0.1\npip: 24.2\nconda: None\npytest: 8.3.2\nmypy: 1.14.1\nIPython: 8.27.0\nsphinx: 8.0.2\n\n</details>\n",
    "comments": [
      {
        "user": "phofl",
        "body": "This is the same as #11735 kind off. We are constructing the low level graph directly, which means that this depends on the new size of your array. We are working on a way of deferring the construction, but this will take a bit.\n\nClosing for now, there isn't much we can do directly"
      },
      {
        "user": "dcherian",
        "body": "Unlike that one, my profiling of this workload showed 50% time on this tokenize line:\nhttps://github.com/dask/dask/blob/388bb0d7fbb0577bfd689dcdda8e78faa21d583f/dask/array/_shuffle.py#L238\n\nDo we expect de-duplication of that sorter to be that useful? I'm having a hard time imagining that.\n\nI agree that the O(len(indexer)) scaling seems to be unavoidable if you support non-uniform chunk sizes."
      },
      {
        "user": "phofl",
        "body": "You are indexing a single dimension here, so each chunk along the other axes for any given chunk on this dimension has the same sorter. This reduced graph size by 70-90% depending on the chunking structure "
      }
    ]
  },
  {
    "issue_number": 6354,
    "title": "Cannot compute min or max of dates in dask array when converted from dask dataframe using to_dask_array",
    "author": "kylejn27",
    "state": "open",
    "created_at": "2020-06-29T18:13:04Z",
    "updated_at": "2025-02-17T14:33:58Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "**What happened**:\r\n\r\nComputing the minimum date from a dask array causes an exception to be thrown. That dask array was converted from a dask dataframe using `to_dask_array`.\r\n\r\n**What you expected to happen**:\r\n\r\nI expected to be able to compute the minimum date from the dask array. \r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\nusing `to_dask_array` raises an exception\r\n```python\r\nimport pandas as pd\r\nfrom datetime import date\r\nimport dask.dataframe as dd\r\n\r\ndates_df = pd.Series(pd.date_range(date(2014,1,1),date(2015,1,1), freq=\"M\"))\r\ndates_dd = dd.from_pandas(dates_df, npartitions=1)\r\n\r\ndates_da = dates_dd.to_dask_array()\r\nprint(dates_da.min().compute())\r\n```\r\n`UFuncTypeError: ufunc 'add' cannot use operands with types dtype('<M8[ns]') and dtype('<M8[ns]')`\r\n\r\nUsing `from_array` works as expected\r\n```python\r\nimport pandas as pd\r\nfrom datetime import date\r\nimport dask.dataframe as dd\r\n\r\ndates_df = pd.Series(pd.date_range(date(2014,1,1),date(2015,1,1), freq=\"M\"))\r\n\r\ndates_da = dd.from_array(dates_df.values)\r\nprint(dates_da.min().compute())\r\n```\r\n`2014-01-31 00:00:00`\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2.16.0\r\n- Python version: 3.7.6\r\n- Operating System: macOS Catalina 10.15.5\r\n- Install method (conda, pip, source): conda\r\n",
    "comments": [
      {
        "user": "TomAugspurger",
        "body": "Going through dataframe isn't necessary. This issue is the unknown chunks. This operation fails when discovering the output dtype.\r\n\r\n```pytb\r\nIn [71]: a = da.ones((4,), dtype='datetime64[ns]')\r\n\r\nIn [72]: a._chunks\r\nOut[72]: ((4,),)\r\n\r\nIn [73]: a._chunks = ((np.nan,),)\r\n\r\nIn [74]: a.min()\r\n---------------------------------------------------------------------------\r\nUFuncTypeError                            Traceback (most recent call last)\r\n<ipython-input-74-8f6244b3691f> in <module>\r\n----> 1 a.min()\r\n\r\n~/sandbox/dask/dask/array/core.py in min(self, axis, keepdims, split_every, out)\r\n   1960         from .reductions import min\r\n   1961\r\n-> 1962         return min(self, axis=axis, keepdims=keepdims, split_every=split_every, out=out)\r\n   1963\r\n   1964     @derived_from(np.ndarray)\r\n\r\n~/sandbox/dask/dask/array/reductions.py in min(a, axis, keepdims, split_every, out)\r\n    371         dtype=a.dtype,\r\n    372         split_every=split_every,\r\n--> 373         out=out,\r\n    374     )\r\n    375\r\n\r\n~/sandbox/dask/dask/array/reductions.py in reduction(x, chunk, aggregate, axis, keepdims, dtype, split_every, combine, name, out, concatenate, output_size, meta)\r\n    184         name=name,\r\n    185         concatenate=concatenate,\r\n--> 186         reduced_meta=reduced_meta,\r\n    187     )\r\n    188     if keepdims and output_size != 1:\r\n\r\n~/sandbox/dask/dask/array/reductions.py in _tree_reduce(x, aggregate, axis, keepdims, dtype, split_every, combine, name, concatenate, reduced_meta)\r\n    249         dtype=dtype,\r\n    250         name=(name or funcname(aggregate)) + \"-aggregate\",\r\n--> 251         reduced_meta=reduced_meta,\r\n    252     )\r\n    253\r\n\r\n~/sandbox/dask/dask/array/reductions.py in partial_reduce(func, x, split_every, keepdims, dtype, name, reduced_meta)\r\n    313     if is_arraylike(meta) and meta.ndim != len(out_chunks):\r\n    314         if len(out_chunks) == 0:\r\n--> 315             meta = meta.sum()\r\n    316         else:\r\n    317             meta = meta.reshape((0,) * len(out_chunks))\r\n\r\n~/Envs/dask-dev/lib/python3.7/site-packages/numpy/core/_methods.py in _sum(a, axis, dtype, out, keepdims, initial, where)\r\n     36 def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\r\n     37          initial=_NoValue, where=True):\r\n---> 38     return umr_sum(a, axis, dtype, out, keepdims, initial, where)\r\n     39\r\n     40 def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\r\n\r\nUFuncTypeError: ufunc 'add' cannot use operands with types dtype('<M8[ns]') and dtype('<M8[ns]')\r\n```\r\n\r\n\r\nMy only suggestion is to expose a `meta` to these methods and pass it through as `reduced_meta` in `dask.array.reductions.reduction`. Does that sound reasonable?"
      },
      {
        "user": "kylejn27",
        "body": "> My only suggestion is to expose a meta to these methods and pass it through as reduced_meta in dask.array.reductions.reduction. Does that sound reasonable?\r\n\r\nyeah that sounds great!\r\n\r\n> This issue is the unknown chunks\r\n\r\nThat's strange, I tested it the same \"to_dask_array\" example with `lengths=True` to compute the chunk sizes and it still raised the exception. Do you know why that would happen? \r\n"
      },
      {
        "user": "TomAugspurger",
        "body": "Oh sorry, I misread the original example. The second example used `dd.Series.min()`, I thought it was using `da.Array`. So the missing chunk size is a red herring\r\n\r\n```python\r\nIn [25]: a = da.ones((4,), dtype='datetime64[ns]')\r\n\r\nIn [26]: a.min()\r\n---------------------------------------------------------------------------\r\nUFuncTypeError                            Traceback (most recent call last)\r\n<ipython-input-26-8f6244b3691f> in <module>\r\n----> 1 a.min()\r\n\r\n~/sandbox/dask/dask/array/core.py in min(self, axis, keepdims, split_every, out)\r\n   1960         from .reductions import min\r\n   1961\r\n-> 1962         return min(self, axis=axis, keepdims=keepdims, split_every=split_every, out=out)\r\n   1963\r\n   1964     @derived_from(np.ndarray)\r\n\r\n~/sandbox/dask/dask/array/reductions.py in min(a, axis, keepdims, split_every, out)\r\n    371         dtype=a.dtype,\r\n    372         split_every=split_every,\r\n--> 373         out=out,\r\n    374     )\r\n    375\r\n\r\n~/sandbox/dask/dask/array/reductions.py in reduction(x, chunk, aggregate, axis, keepdims, dtype, split_every, combine, name, out, concatenate, output_size, meta)\r\n    184         name=name,\r\n    185         concatenate=concatenate,\r\n--> 186         reduced_meta=reduced_meta,\r\n    187     )\r\n    188     if keepdims and output_size != 1:\r\n\r\n~/sandbox/dask/dask/array/reductions.py in _tree_reduce(x, aggregate, axis, keepdims, dtype, split_every, combine, name, concatenate, reduced_meta)\r\n    249         dtype=dtype,\r\n    250         name=(name or funcname(aggregate)) + \"-aggregate\",\r\n--> 251         reduced_meta=reduced_meta,\r\n    252     )\r\n    253\r\n\r\n~/sandbox/dask/dask/array/reductions.py in partial_reduce(func, x, split_every, keepdims, dtype, name, reduced_meta)\r\n    313     if is_arraylike(meta) and meta.ndim != len(out_chunks):\r\n    314         if len(out_chunks) == 0:\r\n--> 315             meta = meta.sum()\r\n    316         else:\r\n    317             meta = meta.reshape((0,) * len(out_chunks))\r\n\r\n~/Envs/dask-dev/lib/python3.7/site-packages/numpy/core/_methods.py in _sum(a, axis, dtype, out, keepdims, initial, where)\r\n     36 def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\r\n     37          initial=_NoValue, where=True):\r\n---> 38     return umr_sum(a, axis, dtype, out, keepdims, initial, where)\r\n     39\r\n     40 def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\r\n\r\nUFuncTypeError: ufunc 'add' cannot use operands with types dtype('<M8[ns]') and dtype('<M8[ns]')\r\n```\r\n"
      }
    ]
  },
  {
    "issue_number": 11610,
    "title": "`dataframe.read_parquet` crashed with DefaultAzureCredential cannot be deterministically hashed",
    "author": "seanslma",
    "state": "open",
    "created_at": "2024-12-18T01:53:03Z",
    "updated_at": "2025-02-17T02:01:02Z",
    "labels": [
      "needs attention",
      "dask-expr"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\nDask 2024.2.1 version in python 3.9 works as expected.\r\n\r\nDask 2024.12.0 version in python 3.12 crashed with \r\n```\r\n  File \"/home/user/conda-envs/dev-env/lib/python3.12/site-packages/dask/utils.py\", line 772, in __call__\r\n    return meth(arg, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/conda-envs/dev-env/lib/python3.12/site-packages/dask/tokenize.py\", line 159, in normalize_seq\r\n    return type(seq).__name__, _normalize_seq_func(seq)\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/conda-envs/dev-env/lib/python3.12/site-packages/dask/tokenize.py\", line 152, in _normalize_seq_func\r\n    return tuple(map(_inner_normalize_token, seq))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/conda-envs/dev-env/lib/python3.12/site-packages/dask/tokenize.py\", line 146, in _inner_normalize_token\r\n    return normalize_token(item)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/conda-envs/dev-env/lib/python3.12/site-packages/dask/utils.py\", line 772, in __call__\r\n    return meth(arg, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/conda-envs/dev-env/lib/python3.12/site-packages/dask/tokenize.py\", line 210, in normalize_object\r\n    _maybe_raise_nondeterministic(\r\n  File \"/home/user/conda-envs/dev-env/lib/python3.12/site-packages/dask/tokenize.py\", line 89, in _maybe_raise_nondeterministic\r\n    raise TokenizationError(msg)\r\ndask.tokenize.TokenizationError: Object <azure.identity.aio._credentials.default.DefaultAzureCredential object at 0x7fb2dad44d40> cannot be deterministically hashed. See https://docs.dask.org/en/latest/custom-collections.html#implementing-deterministic-hashing for more information.\r\n```\r\n\r\nNote that in the following example if i replace `storage_options` by `filesystem` it works. \r\n```python\r\nfrom adlfs.spec import AzureBlobFileSystem\r\nfilesystem = AzureBlobFileSystem(\r\n    **storage_options,\r\n)\r\n```\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport pyarrow as pa\r\nimport dask.dataframe as dd\r\nfrom azure.identity.aio import DefaultAzureCredential\r\n\r\nDEV_PA_SCHEMAS = pa.schema([\r\n    ('dev_code', pa.string()),\r\n    ('dev_value', pa.float64()),\r\n])\r\n\r\nstorage_options = dict(\r\n    account_name='my_azure_blob_storage_name',\r\n    credential=DefaultAzureCredential(),\r\n)\r\n\r\nd = dd.read_parquet(\r\n    [\r\n        'az://my-container/2024-12-17/file1.parquet',\r\n        'az://my-container/2024-12-17/file2.parquet',\r\n    ],\r\n    filters=None,\r\n    index=False,\r\n    columns=['dev_code'],\r\n    engine='pyarrow',\r\n    storage_options=storage_options,\r\n    open_file_options=dict(precache_options=dict(method='parquet')),\r\n    schema=DEV_PA_SCHEMAS,\r\n)['dev_code'].unique().compute()\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**: Azure Kubernetes pod\r\n\r\n- Dask version:  2024.12.0\r\n- Python version: 3.12.8\r\n- Operating System: Ubuntu 22.04\r\n- Install method (conda, pip, source): conda\r\n- Pandas version: 2.2.3\r\n- Pyarrow version: 18.1.0\r\n",
    "comments": []
  },
  {
    "issue_number": 11612,
    "title": "bug: group-by with the same root name and different output names raises",
    "author": "MarcoGorelli",
    "state": "open",
    "created_at": "2024-12-18T13:25:01Z",
    "updated_at": "2025-02-17T02:01:01Z",
    "labels": [
      "needs attention",
      "enhancement",
      "dask-expr"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\ncheck this out\r\n```python\r\nIn [1]: import dask.dataframe as dd\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]: df = pd.DataFrame({'a': [1,1,2], 'b': [4,5,6]})\r\n\r\nIn [4]: df.groupby('a').agg(c=('b', 'mean'), d=('b', 'mean'))\r\nOut[4]: \r\n     c    d\r\na          \r\n1  4.5  4.5\r\n2  6.0  6.0\r\n\r\nIn [5]: dd.from_pandas(df).groupby('a').agg(c=('b', 'mean'), d=('b', 'mean'))\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[5], line 1\r\n----> 1 dd.from_pandas(df).groupby('a').agg(c=('b', 'mean'), d=('b', 'mean'))\r\n\r\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask_expr/_groupby.py:1955, in GroupBy.agg(self, *args, **kwargs)\r\n   1954 def agg(self, *args, **kwargs):\r\n-> 1955     return self.aggregate(*args, **kwargs)\r\n\r\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask_expr/_groupby.py:1934, in GroupBy.aggregate(self, arg, split_every, split_out, shuffle_method, **kwargs)\r\n   1931 if arg == \"size\":\r\n   1932     return self.size()\r\n-> 1934 result = new_collection(\r\n   1935     GroupbyAggregation(\r\n   1936         self.obj.expr,\r\n   1937         arg,\r\n   1938         self.observed,\r\n   1939         self.dropna,\r\n   1940         split_every,\r\n   1941         split_out,\r\n   1942         self.sort,\r\n   1943         shuffle_method,\r\n   1944         self._slice,\r\n   1945         *self.by,\r\n   1946     )\r\n   1947 )\r\n   1948 if relabeling and result is not None:\r\n   1949     if order is not None:\r\n\r\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask_expr/_collection.py:4835, in new_collection(expr)\r\n   4833 def new_collection(expr):\r\n   4834     \"\"\"Create new collection from an expr\"\"\"\r\n-> 4835     meta = expr._meta\r\n   4836     expr._name  # Ensure backend is imported\r\n   4837     return get_collection_type(meta)(expr)\r\n\r\nFile ~/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/functools.py:993, in cached_property.__get__(self, instance, owner)\r\n    991 val = cache.get(self.attrname, _NOT_FOUND)\r\n    992 if val is _NOT_FOUND:\r\n--> 993     val = self.func(instance)\r\n    994     try:\r\n    995         cache[self.attrname] = val\r\n\r\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask_expr/_groupby.py:439, in GroupbyAggregation._meta(self)\r\n    437 @functools.cached_property\r\n    438 def _meta(self):\r\n--> 439     return self._lower()._meta\r\n\r\nFile ~/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/functools.py:993, in cached_property.__get__(self, instance, owner)\r\n    991 val = cache.get(self.attrname, _NOT_FOUND)\r\n    992 if val is _NOT_FOUND:\r\n--> 993     val = self.func(instance)\r\n    994     try:\r\n    995         cache[self.attrname] = val\r\n\r\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask_expr/_reductions.py:440, in ApplyConcatApply._meta(self)\r\n    438 @functools.cached_property\r\n    439 def _meta(self):\r\n--> 440     meta = self._meta_chunk\r\n    441     aggregate = self.aggregate or (lambda x: x)\r\n    442     if self.combine:\r\n\r\nFile ~/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/functools.py:993, in cached_property.__get__(self, instance, owner)\r\n    991 val = cache.get(self.attrname, _NOT_FOUND)\r\n    992 if val is _NOT_FOUND:\r\n--> 993     val = self.func(instance)\r\n    994     try:\r\n    995         cache[self.attrname] = val\r\n\r\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask_expr/_groupby.py:213, in GroupByApplyConcatApply._meta_chunk(self)\r\n    210 @functools.cached_property\r\n    211 def _meta_chunk(self):\r\n    212     meta = meta_nonempty(self.frame._meta)\r\n--> 213     return self.chunk(meta, *self._by_meta, **self.chunk_kwargs)\r\n\r\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask_expr/_groupby.py:530, in DecomposableGroupbyAggregation.chunk_kwargs(self)\r\n    527 @property\r\n    528 def chunk_kwargs(self) -> dict:\r\n    529     return {\r\n--> 530         \"funcs\": self.agg_args[\"chunk_funcs\"],\r\n    531         \"sort\": self.sort,\r\n    532         **_as_dict(\"observed\", self.observed),\r\n    533         **_as_dict(\"dropna\", self.dropna),\r\n    534     }\r\n\r\nFile ~/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/functools.py:993, in cached_property.__get__(self, instance, owner)\r\n    991 val = cache.get(self.attrname, _NOT_FOUND)\r\n    992 if val is _NOT_FOUND:\r\n--> 993     val = self.func(instance)\r\n    994     try:\r\n    995         cache[self.attrname] = val\r\n\r\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask_expr/_groupby.py:411, in GroupbyAggregationBase.agg_args(self)\r\n    408 @functools.cached_property\r\n    409 def agg_args(self):\r\n    410     keys = [\"chunk_funcs\", \"aggregate_funcs\", \"finalizers\"]\r\n--> 411     return dict(zip(keys, _build_agg_args(self.spec)))\r\n\r\nFile ~/polars-api-compat-dev/.venv/lib/python3.12/site-packages/dask/dataframe/groupby.py:875, in _build_agg_args(spec)\r\n    873 for funcs in by_name.values():\r\n    874     if len(funcs) != 1:\r\n--> 875         raise ValueError(f\"conflicting aggregation functions: {funcs}\")\r\n    877 chunks = {}\r\n    878 aggs = {}\r\n\r\nValueError: conflicting aggregation functions: [('mean', 'b'), ('mean', 'b')]\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nSpotted in Narwhals (because we are so awesome 😎 )\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.12.1\r\n- Python version: 3.12\r\n- Operating System: linux\r\n- Install method (conda, pip, source): pip\r\n",
    "comments": []
  },
  {
    "issue_number": 11619,
    "title": "Using nested keys in array graphs creates large number of unnecessary tasks for higher-dimensional arrays",
    "author": "hendrikmakait",
    "state": "open",
    "created_at": "2024-12-20T17:26:03Z",
    "updated_at": "2025-02-17T02:01:00Z",
    "labels": [
      "array",
      "needs attention"
    ],
    "body": "While investigating https://github.com/dask/distributed/issues/8958, I noticed this:\r\n\r\n```\r\n<Task None concrete(<Task None _identity_cast(<Task None _identity_cast(<Task None _identity_cast(<Task None _identity_cast(<Task None _identity_cast(Alias(('getitem-f7fd4f245dfedafeb33a2841a9c414ca', 2, 3, 19, 5, 0)), typ=<class 'list'>)>, <Task None _identity_cast(Alias(('getitem-f7fd4f245dfedafeb33a2841a9c414ca', 2, 3, 19, 5.9, 0)), typ=<class 'list'>)>, typ=<class 'list'>)>, typ=<class 'list'>)>, typ=<class 'list'>)>, typ=<class 'list'>)>)>=\r\n```\r\n\r\nBasically, the embedding of keys into nested data structures creates a large overhead of task objects. For the workload I investigated, this appears to have contributed up to 50% of all tasks. (Take that number with a grain of salt.)\r\n\r\nWe should avoid using these nested data structures for keys entirely. In array-code, I've identified the usage of `concrete` (the example above) as a culprit that can be trivially removed. However, `concatenate3` and related functions are other culprits that require a bit more rewriting.",
    "comments": []
  },
  {
    "issue_number": 11637,
    "title": "dask scalars don't preserve dtype",
    "author": "MarcoGorelli",
    "state": "open",
    "created_at": "2025-01-05T18:54:22Z",
    "updated_at": "2025-02-17T02:00:58Z",
    "labels": [
      "needs attention",
      "dask-expr"
    ],
    "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**Describe the issue**:\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]: import dask.dataframe as dd\r\n\r\nIn [4]: from datetime import date, datetime, timedelta\r\n\r\nIn [5]: dfpd = pd.DataFrame({'a': [date(2020,1,1)]}, dtype='date32[pyarrow]')\r\n\r\nIn [6]: df = dd.from_pandas(dfpd)\r\n\r\nIn [7]: df\r\nOut[7]: \r\nDask DataFrame Structure:\r\n                                  a\r\nnpartitions=1                      \r\n0              date32[day][pyarrow]\r\n0                               ...\r\nDask Name: frompandas, 1 expression\r\nExpr=df\r\n\r\nIn [8]: df.assign(b=df['a'][0])\r\nOut[8]: \r\nDask DataFrame Structure:\r\n                                  a       b\r\nnpartitions=1                              \r\n0              date32[day][pyarrow]  object\r\n0                               ...     ...\r\nDask Name: assign, 4 expressions\r\nExpr=Assign(frame=df)\r\n```\r\n\r\nColumn `'b'` now has dtype '`object'`, but I've have expected it to preserve the `date32[day][pyarrow]` dtype\r\n\r\n**Anything else we need to know?**: spotted in [Narwhals](https://github.com/narwhals-dev/narwhals)\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2024.10.0\r\n- Python version:\r\n- Operating System:\r\n- Install method (conda, pip, source):\r\n",
    "comments": [
      {
        "user": "phofl",
        "body": "I think this is something we are inheriting from pandas. We are extracting one element from a representative pandas dtype but pandas is casting this to a python scalar instead of preserving the dtype. "
      }
    ]
  }
]