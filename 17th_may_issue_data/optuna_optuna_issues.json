[
  {
    "issue_number": 6164,
    "title": "Add warning when sophisticated sampler falls back to RandomSampler because of search space issues",
    "author": "1kastner",
    "state": "open",
    "created_at": "2025-06-17T10:28:17Z",
    "updated_at": "2025-06-17T11:28:43Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nIn certain situations, more sophisticated samplers cannot be applied. Then they fall back to randomly sampling the results. It is highly appreciated that the samplers are programmed in such a robust way. However, in some situations the surprise might appear at the end of a time- and energy-consuming optimization study, when the sampler was actually on par with a random sampler (see, e.g., https://github.com/optuna/optuna/discussions/6088). In some cases, remodelling the search space can alleviate this issue and users should get the feedback of having a look at it early (and not at the end of the optimization study).\n\n### Description\n\nSamplers should consistently issue warnings when they fall back to the RandomSampler. In this context, the CMA-ES sampler is a great example, see e.g. https://github.com/optuna/optuna/blob/master/optuna/samplers/_cmaes.py#L374. If I had received a similar feedback, e.g., when using BoTorch, that would have saved me quite some time (again, see https://github.com/optuna/optuna/discussions/6088).\n\n### Alternatives (optional)\n\nLet people read the code and deduce such insights on their own.\n\n### Additional context (optional)\n\nAs part of my research, I want to understand the behavior of these algorithms better. So it could be great if the algorithms could inform the user (i.e., me) about its internal steps better, e.g., by keeping records in the database or logging *interesting information* to debug.",
    "comments": []
  },
  {
    "issue_number": 6163,
    "title": "optuna.study.copy_study() fails if a value isn't contained in the distribution",
    "author": "Zaktara",
    "state": "open",
    "created_at": "2025-06-17T09:02:31Z",
    "updated_at": "2025-06-17T09:08:28Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nWhen using enqueue_trial() with a parameter value outside the defined distribution, it works as expected with SQLite storage and the trial appears in the Dashboard.\n\nHowever, copying the study to another storage using copy_study() results in the following error:\n\nValueError: The value 2 of parameter 'x' isn't contained in the distribution.\n\nIs this a bug or intended behavior?\n\n### Environment\n\n- Optuna version:4.4.0\n- Python version:3.11.10\n- OS:Windows-10-10.0.19045-SP0\n\n\n### Error messages, stack traces, or logs\n\n```shell\n[I 2025-06-17 10:13:13,753] A new study created in RDB with name: test_study\nG:\\ProgramFiles\\anaconda3\\envs\\Optuna\\Lib\\site-packages\\optuna\\trial\\_trial.py:656: UserWarning: Fixed parameter 'x' with value 2.0 is out of range for distribution FloatDistribution(high=10.0, log=False, low=5.0, step=None).\n  warnings.warn(\n[I 2025-06-17 10:13:13,813] Trial 0 finished with value: 1.0 and parameters: {'x': 2.0}. Best is trial 0 with value: 1.0.\n[I 2025-06-17 10:13:13,853] Trial 1 finished with value: 2.9272726216519858 and parameters: {'x': 5.8545452433039715}. Best is trial 0 with value: 1.0.\n[I 2025-06-17 10:13:13,887] Trial 2 finished with value: 3.6878331122912655 and parameters: {'x': 7.375666224582531}. Best is trial 0 with value: 1.0.\n#trials: 3\n[I 2025-06-17 10:13:14,055] A new study created in RDB with name: test_study\nTraceback (most recent call last):\n  File \"G:\\Personal\\Development\\playground-series-s5e6\\example.py\", line 25, in <module>\n    main()\n  File \"G:\\Personal\\Development\\playground-series-s5e6\\example.py\", line 18, in main\n    optuna.study.copy_study(from_study_name=\"test_study\",\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ProgramFiles\\anaconda3\\envs\\Optuna\\Lib\\site-packages\\optuna\\_convert_positional_args.py\", line 135, in converter_wrapper\n    return func(**kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^\n  File \"G:\\ProgramFiles\\anaconda3\\envs\\Optuna\\Lib\\site-packages\\optuna\\study\\study.py\", line 1572, in copy_study\n    to_study.add_trials(from_study.get_trials(deepcopy=False))\n  File \"G:\\ProgramFiles\\anaconda3\\envs\\Optuna\\Lib\\site-packages\\optuna\\study\\study.py\", line 1019, in add_trials\n    self.add_trial(trial)\n  File \"G:\\ProgramFiles\\anaconda3\\envs\\Optuna\\Lib\\site-packages\\optuna\\study\\study.py\", line 970, in add_trial\n    trial._validate()\n  File \"G:\\ProgramFiles\\anaconda3\\envs\\Optuna\\Lib\\site-packages\\optuna\\trial\\_frozen.py\", line 343, in _validate\n    raise ValueError(\nValueError: The value 2.0 of parameter 'x' isn't contained in the distribution FloatDistribution(high=10.0, log=False, low=5.0, step=None).\n```\n\n### Steps to reproduce\n\nimport optuna\n\n\ndef objective(trial: optuna.trial.Trial) -> float:\n    x = trial.suggest_float(\"x\", low=5.0, high=10.0)\n\n    return abs(x / 2)\n\n\ndef main():\n    study = optuna.create_study(direction=\"minimize\", study_name=\"test_study\", storage=\"sqlite:///example.db\")\n\n    study.enqueue_trial({\"x\": 2.0}) # Outside the range [5.0, 10.0].\n\n    study.optimize(objective, n_trials=3)\n    print(f\"#trials: {len(study.trials)}\")\n\n    optuna.study.copy_study(from_study_name=\"test_study\", \n                            from_storage=\"sqlite:///example.db\", \n                            to_storage=\"sqlite:///example_new.db\", \n                            to_study_name=\"test_study\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n### Additional context (optional)\n\nThe copy_study() creates the study in the target db, but it is empty.",
    "comments": [
      {
        "user": "Zaktara",
        "body": "Sorry, the code correctly formatted:\n\n```\nimport optuna\n\ndef objective(trial: optuna.trial.Trial) -> float:\n    x = trial.suggest_float(\"x\", low=5.0, high=10.0)\n\n    return abs(x / 2)\n\ndef main():\n    study = optuna.create_study(direction=\"minimize\", study_name=\"test_study\", storage=\"sqlite:///example.db\")\n\n    study.enqueue_trial({\"x\": 2.0}) # Outside the range [5.0, 10.0].\n\n    study.optimize(objective, n_trials=3)\n    print(f\"#trials: {len(study.trials)}\")\n\n    optuna.study.copy_study(from_study_name=\"test_study\", \n                            from_storage=\"sqlite:///example.db\", \n                            to_storage=\"sqlite:///example_new.db\", \n                            to_study_name=\"test_study\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n"
      }
    ]
  },
  {
    "issue_number": 3815,
    "title": "Reduce warnings in unit tests",
    "author": "not522",
    "state": "closed",
    "created_at": "2022-07-27T01:21:17Z",
    "updated_at": "2025-06-16T23:06:30Z",
    "labels": [
      "test",
      "contribution-welcome",
      "stale"
    ],
    "body": "### Motivation\n\nThe Optuna's unit tests raise many warnings, which make it hard to recognize unexpected warning messages.\n\n### Description\n\nWe can filter expected warnings using `@pytest.mark.filterwarnings` or `warnings.simplefilter(\"ignore\")`. Some warnings may be unexpected or unsuitable. We should fix tests or Optuna's codes in such cases.\r\n\r\nThis issue is contribution-welcome. We welcome any fixes, even how trivial.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "jmsykes83",
        "body": "I will work on this.  "
      },
      {
        "user": "nzw0301",
        "body": "Hi @jmsykes83 thank you for letting us know your interest! I'm also working on this prioritised issue too since this issue affects many developers' experiences. Do you have specific modules you would like to resolve this issue? I'd like to avoid conflicts with your forthcoming pull request. "
      }
    ]
  },
  {
    "issue_number": 6160,
    "title": "Feature Request: Add Latin Hypercube Sampler",
    "author": "RomanN27",
    "state": "closed",
    "created_at": "2025-06-16T14:53:51Z",
    "updated_at": "2025-06-16T15:06:10Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nIn general, having more samplers is beneficial. I believe Latin Hypercube Sampling (LHS) is particularly well-suited for Optuna's define-by-run design because it doesn't require the user to explicitly define a grid. This makes it a strong candidate as the default choice when someone wants to perform grid-search-like hyperparameter optimization (e.g., for warm-starting more advanced optimization algorithms).\n\n### Description\n\nI’ve implemented an initial version of a Latin Hypercube Sampler (LHS) for Optuna. Much of the code is adapted from `GridSampler`, so you may notice some comments still referencing grid sampling—please don’t be surprised by that.\n\n```python\nimport random\n\nimport numpy as np\nfrom typing import Any, Dict, Optional, Sequence\nimport optuna\nfrom optuna import Trial\nfrom optuna._transform import _SearchSpaceTransform\nfrom optuna.distributions import BaseDistribution\nfrom optuna.samplers import BaseSampler, GridSampler, GPSampler\nfrom optuna.samplers._lazy_random_state import LazyRandomState\nfrom optuna.study import Study\nfrom optuna.trial import FrozenTrial, TrialState\nfrom optuna.logging import get_logger\n_logger = get_logger(__name__)\n\nclass LatinHypercubeSampler(BaseSampler):\n    \"\"\"Optuna sampler using Latin Hypercube Sampling (LHS).\n\n    Latin Hypercube Sampling ensures better space coverage than random sampling\n    by dividing each dimension into equal segments and sampling once from each segment.\n\n    Args:\n        seed: Random seed for the sampler.\n    \"\"\"\n\n    def __init__(self, seed: Optional[int] = None, n_trials=100):\n        self._rng = np.random.RandomState(seed)\n        self._search_space = {}\n        self._lhs_points = {}\n\n        self._n_trials = n_trials  # Initial expected number of trials\n        self._resample_trigger = 50  # Minimum number of new dimensions before resampling\n        self._rng = LazyRandomState(seed or 0)\n\n    def infer_relative_search_space(\n            self, study: Study, trial: FrozenTrial\n    ) -> Dict[str, BaseDistribution]:\n        \"\"\"Define the search space based on previous trials.\"\"\"\n        return {}\n\n    def before_trial(self, study: Study, trial: FrozenTrial) -> None:\n        # Instead of returning param values, GridSampler puts the target grid id as a system attr,\n        # and the values are returned from `sample_independent`. This is because the distribution\n        # object is hard to get at the beginning of trial, while we need the access to the object\n        # to validate the sampled value.\n\n        # When the trial is created by RetryFailedTrialCallback or enqueue_trial, we should not\n        # assign a new grid_id.\n        if \"grid_id\" in trial.system_attrs or \"fixed_params\" in trial.system_attrs:\n            return\n\n\n\n        target_grids = self._get_unvisited_grid_ids(study)\n\n        if len(target_grids) == 0:\n            # This case may occur with distributed optimization or trial queue. If there is no\n            # target grid, `GridSampler` evaluates a visited, duplicated point with the current\n            # trial. After that, the optimization stops.\n\n            _logger.warning(\n                \"`LatinHypercubeSampler` is re-evaluating a configuration because the grid has been \"\n                \"exhausted. This may happen due to a timing issue during distributed optimization \"\n                \"or when re-running optimizations on already finished studies.\"\n            )\n\n            # One of all grids is randomly picked up in this case.\n            target_grids = list(range(self._n_trials))\n\n        # In distributed optimization, multiple workers may simultaneously pick up the same grid.\n        # To make the conflict less frequent, the grid is chosen randomly.\n        random.seed() # a seed will be set for all libraries. but in this case we dont want a fixed seed\n        self._grid_id = int(random.choice(target_grids))\n\n        study._storage.set_trial_system_attr(trial._trial_id,\"n_trials\",self._n_trials)\n        study._storage.set_trial_system_attr(trial._trial_id, \"grid_id\", self._grid_id)\n\n    def _get_unvisited_grid_ids(self, study: Study) -> list[int]:\n        # List up unvisited grids based on already finished ones.\n        visited_grids = []\n        running_grids = []\n\n        # We directly query the storage to get trials here instead of `study.get_trials`,\n        # since some pruners such as `HyperbandPruner` use the study transformed\n        # to filter trials. See https://github.com/optuna/optuna/issues/2327 for details.\n        trials = study._storage.get_all_trials(study._study_id, deepcopy=False)\n\n        for t in trials:\n            if \"grid_id\" in t.system_attrs and t.system_attrs[\"n_trials\"]==self._n_trials:\n                if t.state.is_finished():\n                    visited_grids.append(t.system_attrs[\"grid_id\"])\n                elif t.state == TrialState.RUNNING:\n                    running_grids.append(t.system_attrs[\"grid_id\"])\n\n        unvisited_grids = set(range(self._n_trials)) - set(visited_grids) - set(running_grids)\n\n        # If evaluations for all grids have been started, return grids that have not yet finished\n        # because all grids should be evaluated before stopping the optimization.\n        if len(unvisited_grids) == 0:\n            # unvisited_grids = set(range(self._n_trials)) - set(visited_grids)\n            raise RuntimeError(\n                \"All grids have been visited. Please increase the number of trials or \"\n                \"reconfigure the sampler.\"\n            )\n\n        return list(unvisited_grids)\n\n    def sample_relative(\n            self, study: Study, trial: FrozenTrial, search_space: Dict[str, BaseDistribution]\n    ) -> Dict[str, Any]:\n        \"\"\"Sample from the given search space using LHS.\"\"\"\n        return {}\n\n    def sample_independent(\n            self, study: Study, trial: FrozenTrial, param_name: str, param_distribution: BaseDistribution\n    ) -> Any:\n        \"\"\"Sample a parameter value from the given distribution using LHS.\"\"\"\n        # Check if we need to regenerate the LHS design\n        n_dims = len(self._search_space)\n        if param_name not in self._search_space:\n            self._search_space[param_name] = param_distribution\n            if len(self._search_space) >= n_dims + self._resample_trigger:\n                self._generate_lhs_points()\n\n        if len(self._lhs_points) == 0 or param_name not in self._lhs_points:\n            self._generate_lhs_points()\n\n        # If we've used all points, regenerate with double the number of trials\n        if self._grid_id >= len(self._lhs_points[param_name]):\n            self._n_trials *= 2\n            self._generate_lhs_points()\n\n        # Get the next LHS point for this parameter\n        point = self._lhs_points[param_name][self._grid_id]\n\n\n        return self._transform_to_distribution(point,param_distribution)\n\n\n\n    def _generate_lhs_points(self):\n        \"\"\"Generate Latin Hypercube Sampling points for all parameters.\"\"\"\n        n_dims = len(self._search_space)\n        if n_dims == 0:\n            return\n\n        # Generate LHS samples in unit hypercube [0,1]^n\n        samples = self._lhs_samples(self._n_trials, n_dims)\n\n        # Assign each dimension to a parameter\n        for i, (param_name, _) in enumerate(self._search_space.items()):\n            self._lhs_points[param_name] = samples[:, i]\n\n    def _lhs_samples(self, n_samples: int, n_dims: int) -> np.ndarray:\n        \"\"\"Generate Latin Hypercube Samples.\n\n        Args:\n            n_samples: Number of samples to generate\n            n_dims: Number of dimensions\n\n        Returns:\n            Array of shape (n_samples, n_dims) with values in [0, 1]\n        \"\"\"\n        # Generate a Latin Hypercube design with random sampling within strata\n        result = np.zeros((n_samples, n_dims))\n\n        # Generate random permutations for each dimension\n        for i in range(n_dims):\n            perm = self._rng.rng.permutation(n_samples)\n            result[:, i] = (perm + self._rng.rng.uniform(0, 1, n_samples)) / n_samples\n\n        return result\n\n    def after_trial(\n        self,\n        study: Study,\n        trial: FrozenTrial,\n        state: TrialState,\n        values: Sequence[float] | None,\n    ) -> None:\n        target_grids = self._get_unvisited_grid_ids(study)\n\n        if len(target_grids) == 0:\n            study.stop()\n        elif len(target_grids) == 1:\n            grid_id = study._storage.get_trial_system_attrs(trial._trial_id)[\"grid_id\"]\n            if grid_id == target_grids[0]:\n                study.stop()\n\n    def _transform_to_distribution(self, unit_value: float, distribution: BaseDistribution) -> Any:\n        \"\"\"Transform a value from unit interval [0,1] to the target distribution.\"\"\"\n        if isinstance(distribution, optuna.distributions.FloatDistribution):\n            if not distribution.log:\n                return distribution.low + unit_value * (distribution.high - distribution.low)\n            else:\n                log_low = np.log(distribution.low)\n                log_high = np.log(distribution.high)\n                return np.exp(log_low + unit_value * (log_high - log_low))\n\n        elif isinstance(distribution, optuna.distributions.IntDistribution):\n            if not distribution.log:\n                r = distribution.high - distribution.low\n                value = int(np.floor(unit_value * (r + 1)))\n                return distribution.low + value\n            else:\n                log_low = np.log(distribution.low)\n                log_high = np.log(distribution.high)\n                float_log= np.exp(log_low + unit_value * (log_high - log_low))\n                int_log = int(np.floor(float_log))\n                return int_log\n\n        elif isinstance(distribution, optuna.distributions.CategoricalDistribution):\n            choices = distribution.choices\n            return choices[int(np.floor(unit_value * len(choices)))]\n        else:\n            raise Exception(f\"Distribution type {distribution.__class__.__name__} not supported\")\n\n\n```\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1080,
    "title": "Is it possible to specify a starting point for the values suggested?",
    "author": "vmgustavo",
    "state": "closed",
    "created_at": "2020-04-02T13:27:29Z",
    "updated_at": "2025-06-16T07:30:39Z",
    "labels": [
      "question"
    ],
    "body": "Instead of starting from a random point I would like to define a specific set of parameters to start the search. Lets say I have prior knowledge about the problem and I know a set of parameters that would perform well and it should try to improve from there, is it possible to specify this initial set? Or is it always random?",
    "comments": [
      {
        "user": "hvy",
        "body": "Hi @vmgustavo. You can \"queue\" parameter configurations before starting the optimizations using [`Study.enqueue_trial`](https://optuna.readthedocs.io/en/latest/reference/study.html?#optuna.study.Study.enqueue_trial). Would that work for you? Let me leave a reference to #417 as it sounds like a duplicate."
      },
      {
        "user": "vmgustavo",
        "body": "It seems to be enough for me. Thanks, I will try to use it."
      },
      {
        "user": "hvy",
        "body": "Let me close this ticket for now as resolved. Feel free to reopen as needed."
      }
    ]
  },
  {
    "issue_number": 6136,
    "title": "Use `return a, b` instead of `return (a, b)`",
    "author": "nabenabe0928",
    "state": "closed",
    "created_at": "2025-06-07T23:01:42Z",
    "updated_at": "2025-06-16T03:08:53Z",
    "labels": [
      "code-fix",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Motivation\n\nIn Optuna, we use `return a, b` rather than `return (a, b)`, so this issue is for more consistency among the codebase.\n\n> [!NOTE]\n> Based on [PEP8](https://peps.python.org/pep-0008/#when-to-use-trailing-commas), we do not replace `return (a, )` with `return a,`.\n\n### Suggestion\n\nPlease refactor each file below:\n\n- [x] optuna/_gp/optim_mixed.py\n- [x] optuna/storages/_rdb/models.py\n- [x] optuna/storages/_rdb/alembic/versions/v3.0.0.c.py\n- [x] optuna/storages/_rdb/alembic/versions/v3.0.0.d.py\n- [x] optuna/visualization/_terminator_improvement.py\n- [x] tests/trial_tests/test_trial.py\n\n> [!IMPORTANT]\n> Please modify only one file per PR to encourage contributions from more contributors.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "Ajay-Satish-01",
        "body": "@nabenabe0928 I took up `optuna/_gp/optim_mixed.py` and also fixed another type bug on the fly. Please let me know if anything has to be changed.\n\nThanks"
      },
      {
        "user": "nabenabe0928",
        "body": "Thank you, everyone, for the contributions:)\nThis issue has been resolved!"
      }
    ]
  },
  {
    "issue_number": 5149,
    "title": "add domain aware BO algorithm",
    "author": "okaikov",
    "state": "closed",
    "created_at": "2023-12-17T12:10:41Z",
    "updated_at": "2025-06-15T23:06:04Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nimplement a new BO algorithm to leverage user expert knowledge to reduce sample size needed for convergence\n\n### Description\n\nLeveraging Domain Information for the Efficient Automated Design of Deep Learning Accelerators - https://www.cs.utexas.edu/~lin/papers/hpca23.pdf\r\n\r\nin the above paper is mentioned a new algorithm for leveraging user expert knowledge to define new features to help the BO model to learn more quickly. \n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Hi, optuna repo has released optuna-hub hosts multiple third-party modules on https://hub.optuna.org/. So, adding comments on https://github.com/optuna/optuna/discussions/5606 becomes one of the most appropriate places to make a feature request."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 6146,
    "title": "Remove the version constraint on blackdoc",
    "author": "c-bata",
    "state": "closed",
    "created_at": "2025-06-11T09:04:18Z",
    "updated_at": "2025-06-13T04:54:58Z",
    "labels": [
      "code-fix",
      "contribution-welcome"
    ],
    "body": "### Motivation\n\nblackdoc 0.3.10 was released yesterday, causing the `checks` workflow to fail.\n* \n* https://pypi.org/project/blackdoc/#history\n* https://github.com/optuna/optuna/actions/runs/15572122231/job/43850030515\n\nI opened a PR to add the version constraint, `blackdoc<0.3.10`, as a hotfix. This issue suggests fixing the above error and remove the version constraint.\n\n\n### Suggestion\n\nRemove the version constraint added at https://github.com/optuna/optuna/pull/6145.\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5138,
    "title": "`JSONDecodeError` in `JournalStorage`",
    "author": "not522",
    "state": "closed",
    "created_at": "2023-12-05T02:11:50Z",
    "updated_at": "2025-06-10T23:06:28Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\r\n\r\nThe all CIs should be passed but `test_pop_waiting_trial_multiprocess_safe` in `tests/storages_tests/test_journal.py` sometimes fails. https://github.com/optuna/optuna/actions/runs/7079914046/job/19267103786\r\n\r\n### Environment\r\n\r\n- Optuna version: master\r\n- Python version: (at least) 3.7 & 3.8\r\n- OS: Ubuntu\r\n- (Optional) Other libraries and their versions:\r\n\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\n___________________ test_pop_waiting_trial_multiprocess_safe ___________________\r\nconcurrent.futures.process._RemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/concurrent/futures/process.py\", line 239, in _process_worker\r\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\r\n  File \"/home/runner/work/optuna/optuna/tests/storages_tests/test_journal.py\", line 103, in pop_waiting_trial\r\n    study = optuna.load_study(storage=storage, study_name=study_name)\r\n  File \"/home/runner/work/optuna/optuna/optuna/_convert_positional_args.py\", line 83, in converter_wrapper\r\n    return func(**kwargs)\r\n  File \"/home/runner/work/optuna/optuna/optuna/study/study.py\", line 1350, in load_study\r\n    return Study(study_name=study_name, storage=storage, sampler=sampler, pruner=pruner)\r\n  File \"/home/runner/work/optuna/optuna/optuna/study/study.py\", line 85, in __init__\r\n    self._directions = storage.get_study_directions(study_id)\r\n  File \"/home/runner/work/optuna/optuna/optuna/storages/_journal/storage.py\", line 213, in get_study_directions\r\n    self._sync_with_backend()\r\n  File \"/home/runner/work/optuna/optuna/optuna/storages/_journal/storage.py\", line 149, in _sync_with_backend\r\n    logs = self._backend.read_logs(self._replay_result.log_number_read)\r\n  File \"/home/runner/work/optuna/optuna/optuna/storages/_journal/file.py\", line 175, in read_logs\r\n    raise last_decode_error\r\n  File \"/home/runner/work/optuna/optuna/optuna/storages/_journal/file.py\", line 184, in read_logs\r\n    logs.append(json.loads(line))\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/json/__init__.py\", line 357, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/json/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/json/decoder.py\", line 353, in raw_decode\r\n    obj, end = self.scan_once(s, idx)\r\njson.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 146 (char 145)\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\n    def test_pop_waiting_trial_multiprocess_safe() -> None:\r\n        with NamedTemporaryFilePool() as file:\r\n            file_storage = optuna.storages.JournalFileStorage(file.name)\r\n            storage = optuna.storages.JournalStorage(file_storage)\r\n            study = optuna.create_study(storage=storage)\r\n            num_enqueued = 10\r\n            for i in range(num_enqueued):\r\n                study.enqueue_trial({\"i\": i})\r\n    \r\n            trial_id_set = set()\r\n            with ProcessPoolExecutor(10) as pool:\r\n                futures = []\r\n                for i in range(num_enqueued):\r\n                    future = pool.submit(pop_waiting_trial, file.name, study.study_name)\r\n                    futures.append(future)\r\n    \r\n                for future in as_completed(futures):\r\n>                   trial_id = future.result()\r\n\r\ntests/storages_tests/test_journal.py:124: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/concurrent/futures/_base.py:437: in result\r\n    return self.__get_result()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = None\r\n\r\n    def __get_result(self):\r\n        if self._exception:\r\n            try:\r\n>               raise self._exception\r\nE               json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 146 (char 145)\r\n\r\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/concurrent/futures/_base.py:389: JSONDecodeError\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nRun `test_pop_waiting_trial_multiprocess_safe` in `tests/storages_tests/test_journal.py`.\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "araffin",
        "body": "Hello,\r\nI can report similar issues when using many processes at once with the journal storage."
      },
      {
        "user": "ernestum",
        "body": "Same (100 workers in Parallel, 15 failed with this error)"
      },
      {
        "user": "not522",
        "body": "Thank you for your reports. I investigated this bug and found that it is caused by the wrong reading log logic. We plan to fix it until the next minor release."
      }
    ]
  },
  {
    "issue_number": 6142,
    "title": "Incompatible types when running mypy hook",
    "author": "Ajay-Satish-01",
    "state": "closed",
    "created_at": "2025-06-08T21:22:21Z",
    "updated_at": "2025-06-09T03:49:57Z",
    "labels": [
      "code-fix"
    ],
    "body": "### Motivation\n\nmypy pre-commit hook fails due to int type errors.\n \n- `optuna/study/_multi_objective.py`\n- `optuna/_gp/optim_mixed.py`\n\n![Image](https://github.com/user-attachments/assets/a4964edf-c1d2-4a11-9b96-8cba424f1ecd)\n\n### Suggestion\n\nFix these by adding explicit types\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "Thank you for the report, this issue is addressed in this PR:\n- https://github.com/optuna/optuna/pull/6137"
      }
    ]
  },
  {
    "issue_number": 5097,
    "title": "BruteForceSampler fails to resume study when run fails",
    "author": "liukidar",
    "state": "closed",
    "created_at": "2023-11-06T19:19:37Z",
    "updated_at": "2025-06-08T23:06:05Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\r\n\r\nWhen running a study using BruteForceSampler, if any run fails (for example for out of memory errors) and the study is interrupted before completion of all the possible trials, we should be able to start the study again and resume from where we stopped (by simpling starting the program again).\r\n\r\n### Environment\r\n\r\n- Optuna version:3.4.0\r\n- Python version:3.11.4\r\n- OS:Linux-3.10.0-1160.80.1.el7.x86_64-x86_64-with-glibc2.17\r\n- (Optional) Other libraries and their versions:\r\n\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\n[I 2023-11-07 09:09:52,541] A new study created in memory with name: no-name-67f1e9f8-0564-4b63-8b9d-515b5887aaff\r\nUsing parameters: {'x': 'a', 'y': 1}\r\n[I 2023-11-07 09:09:52,543] Trial 0 finished with value: 98.0 and parameters: {'x': 'a', 'y': 1}. Best is trial 0 with value: 98.0.\r\nUsing parameters: {'x': 'a', 'y': 2}\r\n[I 2023-11-07 09:09:52,566] Trial 1 finished with value: 99.0 and parameters: {'x': 'a', 'y': 2}. Best is trial 0 with value: 98.0.\r\nUsing parameters: {'x': 'a', 'y': 7}\r\n[I 2023-11-07 09:09:52,579] Trial 2 finished with value: 104.0 and parameters: {'x': 'a', 'y': 7}. Best is trial 0 with value: 98.0.\r\nUsing parameters: {'x': 'a', 'y': 9}\r\n[I 2023-11-07 09:09:52,592] Trial 3 finished with value: 106.0 and parameters: {'x': 'a', 'y': 9}. Best is trial 0 with value: 98.0.\r\nUsing parameters: {'x': 'c'}\r\n[I 2023-11-07 09:09:52,604] Trial 4 finished with value: 109.0 and parameters: {'x': 'c'}. Best is trial 0 with value: 98.0.\r\nUsing parameters: {'x': 'a', 'y': 6}\r\n[I 2023-11-07 09:09:52,617] Trial 5 finished with value: 103.0 and parameters: {'x': 'a', 'y': 6}. Best is trial 0 with value: 98.0.\r\n[W 2023-11-07 09:09:52,628] Trial 6 failed with parameters: {} because of the following error: FileNotFoundError().\r\nTraceback (most recent call last):\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\r\n    value_or_values = func(trial)\r\n                      ^^^^^^^^^^^\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/home/projects/pc-optimization/optuna_test.py\", line 6, in objective_that_fails\r\n    raise FileNotFoundError()\r\nFileNotFoundError\r\n[W 2023-11-07 09:09:52,630] Trial 6 failed with value None.\r\nparam_name mismatch: x != None. Children: {0: _TreeNode(param_name='y', children={0: _TreeNode(param_name=None, children=None), 1: _TreeNode(param_name=None, children={}), 2: _TreeNode(param_name=None, children={}), 3: _TreeNode(param_name=None, children=None), 4: _TreeNode(param_name=None, children=None), 5: _TreeNode(param_name=None, children=None), 6: _TreeNode(param_name=None, children={}), 7: _TreeNode(param_name=None, children={}), 8: _TreeNode(param_name=None, children=None), 9: _TreeNode(param_name=None, children={})}), 1: _TreeNode(param_name=None, children=None), 2: _TreeNode(param_name=None, children={})}\r\n\r\n\r\nTrying second optimization:\r\n\r\n\r\n[W 2023-11-07 09:09:52,642] Trial 7 failed with parameters: {} because of the following error: ValueError(\"param_name mismatch: x != None. Children: {0: _TreeNode(param_name='y', children={0: _TreeNode(param_name=None, children=None), 1: _TreeNode(param_name=None, children={}), 2: _TreeNode(param_name=None, children={}), 3: _TreeNode(param_name=None, children=None), 4: _TreeNode(param_name=None, children=None), 5: _TreeNode(param_name=None, children=None), 6: _TreeNode(param_name=None, children={}), 7: _TreeNode(param_name=None, children={}), 8: _TreeNode(param_name=None, children=None), 9: _TreeNode(param_name=None, children={})}), 1: _TreeNode(param_name=None, children=None), 2: _TreeNode(param_name=None, children={})}\").\r\nTraceback (most recent call last):\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\r\n    value_or_values = func(trial)\r\n                      ^^^^^^^^^^^\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/home/projects/pc-optimization/optuna_test.py\", line 19, in objective_that_cant_fail\r\n    x = trial.suggest_categorical('x', ['a', 'b', 'c'])\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 405, in suggest_categorical\r\n    return self._suggest(name, CategoricalDistribution(choices=choices))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 630, in _suggest\r\n    param_value = self.study.sampler.sample_independent(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/samplers/_brute_force.py\", line 203, in sample_independent\r\n    self._populate_tree(tree, (t for t in trials if t.number != trial.number), trial.params)\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/samplers/_brute_force.py\", line 172, in _populate_tree\r\n    leaf.set_leaf()\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/samplers/_brute_force.py\", line 54, in set_leaf\r\n    self.expand(None, [])\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/samplers/_brute_force.py\", line 47, in expand\r\n    raise ValueError(f\"param_name mismatch: {self.param_name} != {param_name}. Children: {self.children}\")\r\nValueError: param_name mismatch: x != None. Children: {0: _TreeNode(param_name='y', children={0: _TreeNode(param_name=None, children=None), 1: _TreeNode(param_name=None, children={}), 2: _TreeNode(param_name=None, children={}), 3: _TreeNode(param_name=None, children=None), 4: _TreeNode(param_name=None, children=None), 5: _TreeNode(param_name=None, children=None), 6: _TreeNode(param_name=None, children={}), 7: _TreeNode(param_name=None, children={}), 8: _TreeNode(param_name=None, children=None), 9: _TreeNode(param_name=None, children={})}), 1: _TreeNode(param_name=None, children=None), 2: _TreeNode(param_name=None, children={})}\r\n[W 2023-11-07 09:09:52,644] Trial 7 failed with value None.\r\nTraceback (most recent call last):\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/home/projects/pc-optimization/optuna_test.py\", line 37, in <module>\r\n    study.optimize(objective_that_cant_fail, n_trials=100, n_jobs=1, gc_after_trial=True)\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/study/study.py\", line 451, in optimize\r\n    _optimize(\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 66, in _optimize\r\n    _optimize_sequential(\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 163, in _optimize_sequential\r\n    frozen_trial = _run_trial(study, func, catch)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 212, in _run_trial\r\n    frozen_trial = _tell_with_warning(\r\n                   ^^^^^^^^^^^^^^^^^^^\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/study/_tell.py\", line 174, in _tell_with_warning\r\n    study.sampler.after_trial(study, frozen_trial, state, values)\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/samplers/_brute_force.py\", line 226, in after_trial\r\n    self._populate_tree(\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/samplers/_brute_force.py\", line 172, in _populate_tree\r\n    leaf.set_leaf()\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/samplers/_brute_force.py\", line 54, in set_leaf\r\n    self.expand(None, [])\r\n  File \"/jmain02/home/J2AD009/ttl04/lxp14-ttl04/.conda/envs/pcax/lib/python3.11/site-packages/optuna/samplers/_brute_force.py\", line 47, in expand\r\n    raise ValueError(f\"param_name mismatch: {self.param_name} != {param_name}. Children: {self.children}\")\r\nValueError: param_name mismatch: x != None. Children: {0: _TreeNode(param_name='y', children={0: _TreeNode(param_name=None, children=None), 1: _TreeNode(param_name=None, children={}), 2: _TreeNode(param_name=None, children={}), 3: _TreeNode(param_name=None, children=None), 4: _TreeNode(param_name=None, children=None), 5: _TreeNode(param_name=None, children=None), 6: _TreeNode(param_name=None, children={}), 7: _TreeNode(param_name=None, children={}), 8: _TreeNode(param_name=None, children=None), 9: _TreeNode(param_name=None, children={})}), 1: _TreeNode(param_name=None, children=None), 2: _TreeNode(param_name=None, children={})}\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\n1. Run a study using BruteForceSampler\r\n2. Interrupt the study after a run has failed\r\n3. Try to resume the study. The study will crash when the sampler tries to create the trials tree\r\n\r\n```python\r\nimport optuna\r\nimport random\r\n\r\ndef objective_that_fails(trial: optuna.Trial):\r\n    if random.randint(0, 5) == 0:\r\n        raise FileNotFoundError()\r\n    \r\n    x = trial.suggest_categorical('x', ['a', 'b', 'c'])\r\n    if x == 'a':\r\n        y = trial.suggest_int('y', 0, 9)\r\n    else:\r\n        y = 10\r\n    \r\n    print(\"Using parameters:\", trial.params)\r\n    \r\n    return ord(x) + y / 100.0\r\n\r\ndef objective_that_cant_fail(trial: optuna.Trial):\r\n    x = trial.suggest_categorical('x', ['a', 'b', 'c'])\r\n    if x == 'a':\r\n        y = trial.suggest_int('y', 0, 9)\r\n    else:\r\n        y = 10\r\n    \r\n    print(\"Using parameters:\", trial.params)\r\n    \r\n    return ord(x) + y / 100.0\r\n\r\nif __name__ == '__main__':\r\n    study = optuna.create_study(sampler=optuna.samplers.BruteForceSampler())\r\n    try:\r\n        study.optimize(objective_that_fails, n_trials=100, n_jobs=1, gc_after_trial=True)\r\n    except Exception as e:\r\n        print(e)\r\n\r\n    print(\"\\n\\nTrying second optimization:\\n\\n\")\r\n    study.optimize(objective_that_cant_fail, n_trials=100, n_jobs=1, gc_after_trial=True)\r\n    \r\n    print(study.best_params)\r\n    print(study.best_value)\r\n    print(study.best_trial)\r\n```\r\nThe above code results in the before mentioned error: as soon as you sample a 0 the first optimization crashes and you cannot resume it.\r\n\r\n\r\n### Additional context (optional)\r\n\r\n- In the log I also print the children of the node that caused the error, not sure if this is helpful;\r\n- the trial crashed because of oom error and wasn't able to sample any parameter I think (if I try to print the trial parameters via study.get_trials i get {})",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Thank you for reporting the bug. If possible, could you share the minimal reproducible code block with us for smooth investigation?"
      },
      {
        "user": "liukidar",
        "body": "> Thank you for reporting the bug. If possible, could you share the minimal reproducible code block with us for smooth investigation?\r\n\r\nYes, I will edit the post with the code (as soon as I manage to reproduce it on a small scale). I actually have an update, so I'm not sure if i should edit the main post or not. It seems that the bruteforce sampler crashes immediately after a run fails, not when you resume the study. It's is quite difficult to tell as I'm running ~100 study optimizations in parallel, but it seems that one failed for OOM and caused all the other processes to start failing because of the bruteforce sampler error.\r\n\r\nEDIT (added code and output)\r\n\r\nEDIT 2:  I temporarely fixed the problem by having the BruteForceSampler not load previously FAILED runs (commenting lines 222 and 194 in _brute_force.py), so basically we need a way to detect if the parameters of a run were not sampled and ignore that run. Given that optuna support dynamic sampling I'm not too sure how we can proceed with failed runs."
      },
      {
        "user": "liamd101",
        "body": "Hi! I'm also running in to the exact same issue with the BruteForceSampler. Commenting out the lines you mentioned (which are now [193](https://github.com/optuna/optuna/blob/1925ca1a051b3bf11819478b65d830de614774b2/optuna/samplers/_brute_force.py#L193) and [221](https://github.com/optuna/optuna/blob/1925ca1a051b3bf11819478b65d830de614774b2/optuna/samplers/_brute_force.py#L221) in `samplers/_brute_force.py`) did fix the issue; however, this feels like a hacky workaround"
      }
    ]
  },
  {
    "issue_number": 6132,
    "title": "Continue study after a break",
    "author": "michael135",
    "state": "closed",
    "created_at": "2025-06-05T13:48:02Z",
    "updated_at": "2025-06-06T07:39:24Z",
    "labels": [],
    "body": "I the study has stopped for any external reason (electricity outage etc.).\nHow a study can be continued from the same or previous trial ?",
    "comments": []
  },
  {
    "issue_number": 5610,
    "title": "Enhancing Hyperparameter Study Parallelization with Batch Update to Expected Improvement Function",
    "author": "adendek",
    "state": "closed",
    "created_at": "2024-08-04T16:09:44Z",
    "updated_at": "2025-06-01T23:06:08Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nIn exploring the Tree-structured Parzen Estimator (TPE) implementation for insights on improving study parallelization, I've identified a potential enhancement beneficial for my project, which utilizes this tool for optimizing gradient boosting tree (GBT) hyperparameters. Despite our dataset being relatively small (on the order of tens of MB), we train a significant number of models (on the order of hundreds) for each we would like to preform hyperpareter optimization. Leveraging a serverless-like platform like [Azure Container Apps](https://azure.microsoft.com/en-us/products/container-apps), we have the capability to concurrently train up to 1,000 trials. However, it appears there's no existing method to run multiple trials in parallel while still benefiting from TPE or Bayesian optimization techniques.\n\n### Description\n\nI propose a strategy to facilitate study parallelization as follows:\r\n\r\n\r\n* Initial Parallel Trials: Execute the initial `n_trials` concurrently, leveraging their random and independent nature.\r\n* Batch Sampling Modification: Adapt the sampling method to select the `top_k` values (where k equals `batch_size`) instead of a single maximum [value](https://github.com/optuna/optuna/blob/master/optuna/samplers/_tpe/sampler.py#L537), enhancing the efficiency of trial selection: \r\n```python\r\n# current approach\r\nbest_idx = np.argmax(acquisition_func_vals)\r\n# proposed approach \r\nbest_idxs = top_k(acquisition_func_vals, batch_size)\r\n```\r\n\r\nThe top_k implementation can reference [this Stack Overflow answer](https://stackoverflow.com/a/23734295/1877600)..\r\n* Parallel Experiment Execution: Conduct `batch_size` experiments in parallel, optimizing resource utilization.\r\n* Batch Updates to Sampling Function: Implement batch updates, allowing for more efficient sampling and selection processes.\r\n* Execution Backend Specification (Optional): Provide users with the option to specify the execution backend, enhancing flexibility.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\nI am prepared to spearhead the implementation of this proposal. However, before commencing, I seek to confirm whether this concept is in alignment with Optuna's development roadmap and if it would be a welcome contribution.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 3945,
    "title": "Introducing sampler-dependent visualization",
    "author": "contramundum53",
    "state": "closed",
    "created_at": "2022-08-30T06:36:29Z",
    "updated_at": "2025-05-31T07:01:48Z",
    "labels": [
      "feature",
      "needs-discussion",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nBlack-box optimization is inherently heuristic, and in many problems the algorithms don't work well out of the box but needs some hyper-hyper-parameter tuning.\r\n\r\nHowever, Optuna currently don't provide any kind of visualization on the internal states of the samplers, e.g., the posterior distribution of Gaussian processes or the estimated low/high distribution in TPE. \r\n\r\nIf we could support such fine-grained visualization of the algorithms, I think it will be of great help for understanding the cause behind (possibly undesirable) behaviors of the heuristics and tuning hyper-hyper-parameters.\r\n\r\n### Description\r\n\r\nThis is one simple example of such visualizations.\r\n![](https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F118139%2F79792f9b-76cc-8a56-f94e-0b8c20d3f9f3.gif?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=e051b727d3d0b2dae5eb587b950ae574)\r\n(Image taken from [here](https://qiita.com/masasora/items/cc2f10cb79f8c0a6bbaa).)\r\n\r\nThis GIF shows the posterior distribution of the surrogate model (namely Gaussian process) and the computed acquisition function. Such visualization is helpful (and perhaps crucial) for tuning of e.g. the kernel function of Gaussian process.\r\n\r\nWe could introduce somewhat similar visualizations for TPE and CMA-ES. \r\n\r\nSome potential obstacles are:\r\n* The increased code complexity. Since we need access to the internal state (or possibly local variables inside a function) of the samplers, and since such visualization differs from one sampler to another, we would need extensive discussion for code design.\r\n* Support for higher-dimensional problems. It would be hard or impossible to visualize problems of dimension greater than two or three. Still, I think the behavior in lower dimensional problems do provide important insights.\r\n\r\n\r\n### Alternatives (optional)\r\n\r\nAlternatively, we could simply provide hooks for accessing internal states during optimization (e.g. the `botorch` model used to fit the parameters and function values) and let the user visualize them. However, we would still need to decide which internal states to make public, and the backward compatibility is easily broken.\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "blazespinnaker",
        "body": "I think this would be exceedingly useful, not only to better understand the behavior but also to potentially alter it.  For the latter, hooks would be the best place to start, perhaps with Chain of Responsibility pattern in order to add/inspect behavior on default implementations.  A use case might be inspecting and altering some of the scoring logic, for example around pruned trials.\r\n\r\nFor hooks, I'm not sure what the risks are for \"backward compatibility\" .. can you elaborate?  Some care will likely need to be made around api grammar and design, of course, and also regression potential, but in general hooks I've found are fairly straightforward.\r\n"
      },
      {
        "user": "contramundum53",
        "body": "Since what internal states a `Sampler` computes belongs to the realm of implementation details, an additional feature / a simple refactoring of a `Sampler` could change the necessary internal states. For example, currently Optuna's `BoTorchSampler` uses `botorch.SingleTaskGP` as the surrogate model, but it can change in the future."
      },
      {
        "user": "contramundum53",
        "body": "I made an early prototype of TPE visualization. The following GIFs compare the effect of `consider_magic_clip` option in `TPESampler`. (Other parameters are left to be default.)\r\n\r\n|`consider_magic_clip=True` | `consider_magic_clip=False` |\r\n|-|-|\r\n|![anim](https://user-images.githubusercontent.com/24619922/187394627-cb267f92-fce9-4ab1-b65e-73bb0e37534c.gif)| ![anim](https://user-images.githubusercontent.com/24619922/187395830-db31c9e8-aca8-4a88-8772-60cbb3214f3a.gif)|\r\n\r\nNote that in the particular case of `TPESampler` with `multivariate=False` (current default), all variables are sampled independently, so the same visualization can be done no matter how many variables you have.\r\n"
      }
    ]
  },
  {
    "issue_number": 6113,
    "title": "GPSampler crashes when torch default device is cuda",
    "author": "argusdusty",
    "state": "open",
    "created_at": "2025-05-31T05:56:26Z",
    "updated_at": "2025-05-31T05:56:26Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nSuccessful run to completion.\n\n### Environment\n\n- Optuna version:4.3.0\n- Python version:3.11.4\n- OS:Windows-10-10.0.26100-SP0\n- Torch version:2.6.0+cu124\n\n\n### Error messages, stack traces, or logs\n\n```shell\n...\n  File \"C:\\Users\\Argusdusty\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\samplers\\_gp\\sampler.py\", line 230, in sample_relative\n    kernel_params = gp.fit_kernel_params(\n                    ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Argusdusty\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\_gp\\gp.py\", line 264, in fit_kernel_params\n    return _fit_kernel_params(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Argusdusty\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\_gp\\gp.py\", line 183, in _fit_kernel_params\n    np.log(initial_kernel_params.inverse_squared_lengthscales.detach().numpy()),\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Argusdusty\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_device.py\", line 104, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n```\n\n### Steps to reproduce\n\n1. `import torch; torch.set_default_device(\"cuda\")` with a CUDA-enabled PyTorch.\n2. Create study with `sampler=optuna.samplers.GPSampler()`\n3. `study.optimize(...)`\n```python\nimport optuna\nimport torch\n\ntorch.set_default_device(\"cuda\")\n\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -100, 100)\n    return x ** 2\n\nstudy = optuna.create_study(sampler=optuna.samplers.GPSampler())\nstudy.optimize(objective, n_trials=100)\n```\n\n\n### Additional context (optional)\n\nA successful patch in my local workspace has been to modify all calls to torch.tensor, torch.ones, torch.empty, torch.eye, etc. (calls that create new tensors), under optuna/_gp/, to add `device='cpu'`",
    "comments": []
  },
  {
    "issue_number": 6085,
    "title": "RetryFailedTrialCallback not working?",
    "author": "Sebastian-0",
    "state": "open",
    "created_at": "2025-05-22T08:17:03Z",
    "updated_at": "2025-05-28T20:49:31Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nWhen using the `RetryFailedTrialCallback` I expect failed trials to retry and not terminate the optimizer immediately.\n\nAccording to [the docs](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.storages.RetryFailedTrialCallback.html):\n> When a trial fails, this callback can be used with a class in [optuna.storages](https://optuna.readthedocs.io/en/stable/reference/storages.html#module-optuna.storages) to recreate the trial in TrialState.WAITING to queue up the trial to be run again.\n\nSo, for any errors like exceptions, wrong return type, etc... it should retry up to specified amount of times.\n\n### Environment\n\n- Optuna version: 4.3.0\n- Python version: 3.12.3\n- OS: Linux-6.11.0-112021-tuxedo-x86_64-with-glibc2.39\n\n\n### Error messages, stack traces, or logs\n\n```shell\n$ python3 test.py \n/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/test.py:8: ExperimentalWarning: RetryFailedTrialCallback is experimental (supported from v2.8.0). The interface can change in the future.\n  failed_trial_callback=RetryFailedTrialCallback(max_retry=5),\n[I 2025-05-22 10:10:00,274] A new study created in RDB with name: test-2025-05-22_10:10:00\n/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n  optuna.storages.fail_stale_trials(study)\n[W 2025-05-22 10:10:05,323] Trial 0 failed with parameters: {'val': 0.4} because of the following error: RuntimeError('Throw some error').\nTraceback (most recent call last):\n  File \"/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/test.py\", line 17, in obj\n    raise RuntimeError(\"Throw some error\")\nRuntimeError: Throw some error\n[W 2025-05-22 10:10:05,323] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/test.py\", line 27, in <module>\n    study.optimize(obj, n_trials=5)\n  File \"/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/venv/lib/python3.12/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/home/sebastian/IntuiCell/intui/experiments/suitability/tmp/test.py\", line 17, in obj\n    raise RuntimeError(\"Throw some error\")\nRuntimeError: Throw some error\n```\n\n### Steps to reproduce\n\n1. Set up a venv\n```sh\npython3 -m venv venv\n. venv/bin/activate\npip3 install optuna==4.3.0\n```\n2. Run the code below\n3. Observe that there are no retries, it just terminates on the first error.\n  - You would expect it to retry and fail on the test exception 5 times before exiting.\n  - Alternatively, remove the `RuntimeError` and return `None` from `obj` and you get the same behaviour\n```python\nimport optuna\nfrom optuna.storages import RetryFailedTrialCallback\nfrom datetime import datetime\n\nstorage = optuna.storages.RDBStorage(\n    url=\"sqlite:///db.sqlite3\",\n    failed_trial_callback=RetryFailedTrialCallback(max_retry=5),\n)\n\n\ndef obj(trial: optuna.trial.Trial):\n    val = trial.suggest_float(\"val\", 0, 1, step=0.1)\n    raise RuntimeError(\"Throw some error\")\n    return val\n\n\nstudy = optuna.create_study(\n    study_name=f\"test-{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}\",\n    storage=storage,\n)\n\nstudy.optimize(obj, n_trials=5)\n\n```\n\n\n### Additional context (optional)\n\nMaybe I misunderstand how Optuna operates, but after reading the documentation extensively I think the above approach should work. If not, then I don't see what the actual use for `RetryFailedTrialCallback` is?",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "Please use the `catch` argument in [study.optimize](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize).\nThe error you are seeing is indeed intended.\n\nif you already know types of exceptions you would see, you could also catch them inside your objective and simply throw a customized exception so that optuna only catches known exceptions."
      },
      {
        "user": "Sebastian-0",
        "body": "Thanks for the response @nabenabe0928 \n\nI tried adding the argument as you suggest:\n```python\nstudy.optimize(obj, n_trials=5, catch=(RuntimeError))\n```\nHowever there are still no retries happening. The study runs each of the five trials until they fail once and then exits, but it should run each of them five times thanks to the `RetryFailedTrialCallback`.\n\nMy original question still stands, the `RetryFailedTrialCallback` seems to do nothing?\n"
      },
      {
        "user": "nabenabe0928",
        "body": "@Sebastian-0 \n\n`catch=(RuntimeError)` should be `catch=(RuntimeError, )`.\nYours missed the comma inside the tuple, making the argument an exception rather than an iterator of exceptions."
      }
    ]
  },
  {
    "issue_number": 822,
    "title": "What happens when the range of parameters changes?",
    "author": "louisabraham",
    "state": "closed",
    "created_at": "2019-12-29T05:26:27Z",
    "updated_at": "2025-05-27T09:01:03Z",
    "labels": [
      "question"
    ],
    "body": "Hi,\r\n\r\nSuppose I run a study with some parameters, e.g. `x` has distribution `{\"name\": \"UniformDistribution\", \"attributes\": {\"low\": 0, \"high\": 1}}`, and I find that `x=0.99` is frequently proposed.\r\n\r\nIf I restart my study with `{\"name\": \"UniformDistribution\", \"attributes\": {\"low\": 0, \"high\": 5}}`, how will the samplers react?\r\n\r\nBut if the type of the observations change (numerical / categorical), I don't think it will work.\r\n\r\nIf the range is simply expanded, I think (from my understanding of the code) that TPESampler will just be able to use the observations. Am I correct?\r\n\r\nWhat if the range is shrunk (or shifted)? The problem is that the previous observations are not restricted to the current distribution range, so at least there will problems with `n_startup_trials` that will not be respected.\r\n\r\nIMO:\r\n\r\n- The behavior should be documented because I don't think I'm the first to ask the question\r\n\r\n- In case of a range modification, I think it would be useful and not very difficult to ignore the observations outside of the current distribution.\r\n\r\n- In case of a type modification, I think categorical -> numerical could be supported. Numerical to categorical would just ignore the past.",
    "comments": [
      {
        "user": "hvy",
        "body": "Sorry for the delayed response and thank you for bringing this up. You are right that this can be confusing. There seems to be two issues and I suggest we start with the former.\r\n- The behavior have not been discussed throughly enough.\r\n- The behavior is not documented."
      },
      {
        "user": "hvy",
        "body": "It's up to each sampler how to handle these dynamic changes and we currently have no means to control it universally. As for the TPE sampler.\r\n1. All parameter-value pairs previously collected will be used for the upper/lower kernel density estimations. The range is ignored here.\r\n2. When sampling from the lower density estimation, the parameters are actually guaranteed to be within the range according to this logic https://github.com/optuna/optuna/blob/master/optuna/samplers/tpe/sampler.py#L315.\r\n\r\nChanging 1. to only look at parameter-value pairs within the current range would probably make more sense as you say. I'm not so sure about handling new types. In my understanding, users might casually want to resume studies with slightly adjusted ranges, but are the cases besides those?"
      },
      {
        "user": "louisabraham",
        "body": "Hi, thank you very much for your answer.\r\n\r\nI think 1. is good as it exploits all available information.\r\nI didn't notice the behavior of 2. but it basically solves my issue.\r\n\r\nTo summarize: \r\n- all past data is used\r\n- the suggested parameters are always in the current required range\r\n\r\nThis behavior seems very logical. It could just be documented."
      }
    ]
  },
  {
    "issue_number": 6096,
    "title": "`operator does not exist: trialvaluetype = character varying` in CASE statement on PostgreSQL 17.3 with Optuna 4.3.0",
    "author": "vcovo",
    "state": "open",
    "created_at": "2025-05-27T08:26:13Z",
    "updated_at": "2025-05-27T08:26:13Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\n**Expected:**\nAble to run a study with numerous trials and be able to access the best trial(s) so far without any database errors. PostgreSQL should be able to compare the `trialvaluetype` ENUM with string literals in the `CASE` statement generated by Optuna/SQLAlchemy for finding the best trial.\n\n**Actual:**\nThe script fails when `study.best_trial` (or any operation that calls `_storage.get_best_trial()`) is accessed. Able to run some trials until this occurs. The root cause is a `psycopg.errors.UndefinedFunction` error from PostgreSQL, indicating it cannot compare the ENUM type `trialvaluetype` (used for `trial_values.value_type`) with a `character varying` type within a `CASE` statement.\n\n### Environment\n\n    *   Optuna version: 4.3.0\n    *   Python version: 3.13.x\n    *   SQLAlchemy version: ~2.0.35 (e.g., 2.0.35)\n    *   Psycopg version: 3.2.4 (psycopg3)\n    *   PostgreSQL version: 17.3 (Homebrew)\n    *   OS: macOS\n\n\n### Error messages, stack traces, or logs\n\n```shell\n[I 2025-05-27 10:23:14,481] A new study created in RDB with name: study\nStudy created. ID: 4. Exiting to check schema.\nTraceback (most recent call last):\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\n    self.dialect.do_execute(\n    ~~~~~~~~~~~~~~~~~~~~~~~^\n        cursor, str_statement, effective_parameters, context\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/engine/default.py\", line 941, in do_execute\n    cursor.execute(statement, parameters)\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/psycopg/cursor.py\", line 97, in execute\n    raise ex.with_traceback(None)\npsycopg.errors.UndefinedFunction: operator does not exist: trialvaluetype = character varying\nLINE 3: ...$3::INTEGER ORDER BY CASE trial_values.value_type WHEN $4::V...\n                                                             ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/storages/_rdb/storage.py\", line 76, in _create_scoped_session\n    yield session\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/storages/_rdb/storage.py\", line 907, in get_best_trial\n    trial_id = models.TrialModel.find_max_value_trial_id(study_id, 0, session)\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/storages/_rdb/models.py\", line 208, in find_max_value_trial_id\n    .one_or_none()\n     ~~~~~~~~~~~^^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/orm/query.py\", line 2754, in one_or_none\n    return self._iter().one_or_none()  # type: ignore\n           ~~~~~~~~~~^^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/orm/query.py\", line 2827, in _iter\n    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(\n                                                  ~~~~~~~~~~~~~~~~~~~~^\n        statement,\n        ^^^^^^^^^^\n        params,\n        ^^^^^^^\n        execution_options={\"_sa_orm_load_options\": self.load_options},\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/orm/session.py\", line 2362, in execute\n    return self._execute_internal(\n           ~~~~~~~~~~~~~~~~~~~~~~^\n        statement,\n        ^^^^^^^^^^\n    ...<4 lines>...\n        _add_event=_add_event,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/orm/session.py\", line 2247, in _execute_internal\n    result: Result[Any] = compile_state_cls.orm_execute_statement(\n                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        self,\n        ^^^^^\n    ...<4 lines>...\n        conn,\n        ^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/orm/context.py\", line 305, in orm_execute_statement\n    result = conn.execute(\n        statement, params or {}, execution_options=execution_options\n    )\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1418, in execute\n    return meth(\n        self,\n        distilled_parameters,\n        execution_options or NO_OPTIONS,\n    )\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/sql/elements.py\", line 515, in _execute_on_connection\n    return connection._execute_clauseelement(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        self, distilled_params, execution_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1640, in _execute_clauseelement\n    ret = self._execute_context(\n        dialect,\n    ...<8 lines>...\n        cache_hit=cache_hit,\n    )\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1846, in _execute_context\n    return self._exec_single_context(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dialect, context, statement, parameters\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1986, in _exec_single_context\n    self._handle_dbapi_exception(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        e, str_statement, effective_parameters, cursor, context\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 2355, in _handle_dbapi_exception\n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\n    self.dialect.do_execute(\n    ~~~~~~~~~~~~~~~~~~~~~~~^\n        cursor, str_statement, effective_parameters, context\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/sqlalchemy/engine/default.py\", line 941, in do_execute\n    cursor.execute(statement, parameters)\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/psycopg/cursor.py\", line 97, in execute\n    raise ex.with_traceback(None)\nsqlalchemy.exc.ProgrammingError: (psycopg.errors.UndefinedFunction) operator does not exist: trialvaluetype = character varying\nLINE 3: ...$3::INTEGER ORDER BY CASE trial_values.value_type WHEN $4::V...\n                                                             ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\n[SQL: SELECT trials.trial_id AS trials_trial_id \nFROM trials JOIN trial_values ON trials.trial_id = trial_values.trial_id \nWHERE trials.study_id = %(study_id_1)s::INTEGER AND trials.state = %(state_1)s AND trial_values.objective = %(objective_1)s::INTEGER ORDER BY CASE trial_values.value_type WHEN %(param_1)s::VARCHAR THEN %(param_2)s::INTEGER WHEN %(param_3)s::VARCHAR THEN %(param_4)s::INTEGER WHEN %(param_5)s::VARCHAR THEN %(param_6)s::INTEGER END DESC, trial_values.value DESC \n LIMIT %(param_7)s::INTEGER]\n[parameters: {'study_id_1': 4, 'state_1': 'COMPLETE', 'objective_1': 0, 'param_1': 'INF_NEG', 'param_2': -1, 'param_3': 'FINITE', 'param_4': 0, 'param_5': 'INF_POS', 'param_6': 1, 'param_7': 1}]\n(Background on this error at: https://sqlalche.me/e/20/f405)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/vcovo/code/coins/src/backtesting/optuna_tester.py\", line 31, in <module>\n    main()\n    ~~~~^^\n  File \"/Users/vcovo/code/coins/src/backtesting/optuna_tester.py\", line 26, in main\n    study.optimize(objective, n_trials=1)\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n    ~~~~~~~~~^\n        study=self,\n        ^^^^^^^^^^^\n    ...<7 lines>...\n        show_progress_bar=show_progress_bar,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n    ~~~~~~~~~~~~~~~~~~~~^\n        study,\n        ^^^^^^\n    ...<8 lines>...\n        progress_bar=progress_bar,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 221, in _run_trial\n    study._log_completed_trial(frozen_trial)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/study/study.py\", line 1141, in _log_completed_trial\n    best_trial = self.best_trial\n                 ^^^^^^^^^^^^^^^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/study/study.py\", line 162, in best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/storages/_cached_storage.py\", line 180, in get_best_trial\n    return self._backend.get_best_trial(study_id)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/storages/_rdb/storage.py\", line 898, in get_best_trial\n    with _create_scoped_session(self.scoped_session) as session:\n         ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/Users/vcovo/code/venvs/coins/lib/python3.13/site-packages/optuna/storages/_rdb/storage.py\", line 94, in _create_scoped_session\n    raise optuna.exceptions.StorageInternalError(message) from e\noptuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length.\n```\n\n### Steps to reproduce\n\n1. Set up environment with requirements listed above\n2. `dropdb optuna_db` `createdb optuna_db\n3. Run the following python code:\n```python\nimport optuna\nfrom optuna.storages import RDBStorage\nfrom sqlalchemy import NullPool\n\n\ndef main():\n    study_name = \"study\"\n    optuna_database_url = \"postgresql+psycopg://user:password@localhost:5432/optuna_db\"\n\n    print(f\"Attempting to create study: {study_name} in {optuna_database_url}\")\n\n    storage = RDBStorage(\n        url=optuna_database_url,\n\n        engine_kwargs={\"poolclass\": NullPool},\n    )\n\n    study = optuna.create_study(study_name=study_name, storage=storage, load_if_exists=True, direction=\"maximize\")\n\n    print(f\"Study created. ID: {study._study_id}. Exiting to check schema.\")\n\n    # Add a dummy trial to trigger the error later\n    def objective(trial):\n        return trial.suggest_float(\"x\", 0, 1)\n\n    study.optimize(objective, n_trials=1)\n    print(f\"Best trial: {study.best_trial.value}\")  # This will trigger the error\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n\n### Additional context (optional)\n\nIt appears SQLAlchemy (when used by Optuna 4.3.0) generates a `CASE` statement where string literals are explicitly cast to `VARCHAR` before comparison with the `trialvaluetype` ENUM. PostgreSQL 17.3 seems to not support this comparison directly.\nConsider revising the SQL generation for `CASE` statements involving ENUMs in the PostgreSQL dialect, perhaps by:\n    a. Omitting the `::VARCHAR` cast on the string literals.\n    b. Casting the string literals directly to the target ENUM type (e.g., `'FINITE'::trialvaluetype`).",
    "comments": []
  },
  {
    "issue_number": 6095,
    "title": "The test `test_parallel_optimize_with_sleep` eventually fails",
    "author": "HideakiImamura",
    "state": "open",
    "created_at": "2025-05-27T04:33:25Z",
    "updated_at": "2025-05-27T04:33:25Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nThe test `test_parallel_optimize_with_sleep` must succeeds.\n\n### Environment\n\n- Optuna version: Current master\n- Python version: 3.9\n- OS: Linux\n- (Optional) Other libraries and their versions:\n\n\n### Error messages, stack traces, or logs\n\n```shell\n______________________ test_parallel_optimize_with_sleep _______________________\n\n    def test_parallel_optimize_with_sleep() -> None:\n        def objective(trial: Trial) -> float:\n            x = trial.suggest_int(\"x\", 0, 1)\n            time.sleep(x)\n            y = trial.suggest_int(\"y\", 0, 1)\n            return x + y\n    \n        # Seed is fixed to reproduce the same result.\n        # See: https://github.com/optuna/optuna/issues/5780\n        study = optuna.create_study(sampler=samplers.BruteForceSampler(seed=42))\n        study.optimize(objective, n_jobs=2)\n        expected_suggested_values = [\n            {\"x\": 0, \"y\": 0},\n            {\"x\": 0, \"y\": 1},\n            {\"x\": 1, \"y\": 0},\n        ]\n        all_suggested_values = [t.params for t in study.trials]\n>       assert len(all_suggested_values) == len(expected_suggested_values)\nE       AssertionError: assert 4 == 3\nE        +  where 4 = len([{'x': 0, 'y': 1}, {'x': 1, 'y': 1}, {'x': 1, 'y': 0}, {'x': 0, 'y': 0}])\nE        +  and   3 = len([{'x': 0, 'y': 0}, {'x': 0, 'y': 1}, {'x': 1, 'y': 0}])\n\ntests/samplers_tests/test_brute_force.py:324: AssertionError\n```\n\n### Steps to reproduce\n\nSee https://github.com/optuna/optuna/actions/runs/15263202365/job/42924451020\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 4987,
    "title": "Adding SHAP-like beeswarm plot",
    "author": "oasidorshin",
    "state": "closed",
    "created_at": "2023-10-01T12:46:17Z",
    "updated_at": "2025-05-26T23:06:26Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nSHAP beeswarm plot looks like this:\r\n![Beeswarm-plot-of-SHAP-calculation-for-the-ten-highest-ranking-variables-Variables-are](https://github.com/optuna/optuna/assets/79442793/c0448959-15bf-4a90-87d3-f832dc1e85ae)\r\n\r\nIt is very useful for determining monotonic relationships between feature values and model predictions, example in the picture:\r\nhigher age -> more willing to get vaccine\n\n### Description\n\nFor optuna, similar plot with parameter values on y axis and function value on x axis would also be very useful for exactly the same purpose\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\nI can work on this issue if the idea is approved",
    "comments": [
      {
        "user": "contramundum53",
        "body": "Thank you for your suggestion!\r\nHow about [Parallel Coordinate Plot](https://optuna--5053.org.readthedocs.build/en/5053/reference/visualization/generated/optuna.visualization.plot_parallel_coordinate.html#optuna.visualization.plot_parallel_coordinate)? Do you think beeswarm plot looks better?"
      },
      {
        "user": "contramundum53",
        "body": "By the way, I like how it avoids putting many points in the same location. Do you think it is easy enough to do with plotly?"
      },
      {
        "user": "pangjac",
        "body": "> By the way, I like how it avoids putting many points in the same location. Do you think it is easy enough to do with plotly?\r\n\r\nWhen applying parallel coordinate plot and beeswarm plot specifically to plot feature importance, *parallel coordinate plot*, where each line represents a dimension and the lines to show their relationships across dimensions, is a way to show how model features clustered emerge among different features; while in a *beeswarm plot*, the dots are arranged in a way that showing the density and variation of the data specifically for one feature. \r\n\r\nThat's my two cents of applying them for different purpose of feature importance plot.  \r\n\r\nAnd yes, I like the idea of doing them with plotly, it is more easier to embed plotly object! \r\nFor my own work purpose, I have done similar developments. Please let me know if you need any input @oasidorshin 😄 "
      }
    ]
  },
  {
    "issue_number": 5287,
    "title": "Support curved line parallel coordinate plot",
    "author": "Alnusjaponica",
    "state": "closed",
    "created_at": "2024-02-29T04:33:54Z",
    "updated_at": "2025-05-26T23:06:24Z",
    "labels": [
      "feature",
      "needs-discussion",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nIf lines of similar colors intersect in a parallel coordinate plot, it is challenging to discern how the lines connect when only straight lines are used. Adding an option to curve the plots makes it easier to distinguish each trial. Examples of parallel coordinate plots with curved lines can be found at https://docs.wandb.ai/guides/app/features/panels/parallel-coordinates. or https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/46180.pdf.\r\n\r\n### Description\r\n\r\nSome libraries with this feature, such as Vizier or W&B, do not reveal the implementation and currently plotly does not have such option (See https://github.com/plotly/plotly.py/issues/2991) but there is an interesting [answer on Stack Overflow](https://stackoverflow.com/a/60401570). It might be a good start to examine how this strategy works.\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "not522",
        "body": "Thank you for your feature request. This feature would be nice to have, but it is need-discussion to put it in the optuna repository. It is easier to put it in OptunaHub, which we recently released, so I submitted the issue there. https://github.com/optuna/optunahub-registry/issues/115"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5596,
    "title": "Call for contributions to OptunaHub",
    "author": "y0z",
    "state": "open",
    "created_at": "2024-07-26T09:06:21Z",
    "updated_at": "2025-05-26T23:06:22Z",
    "labels": [
      "contribution-welcome"
    ],
    "body": "### Motivation\r\n\r\nOn July 16, OptunaHub β was released!\r\n- [Announcing OptunaHub 0.1.0-β](https://medium.com/optuna/announcing-optunahub-0-1-0-%CE%B2-69b35bb3e95e)\r\n\r\n[OptunaHub](https://hub.optuna.org/) is a feature-sharing platform for Optuna. Contributors can register their developed features, and users can easily use them with the same interface as the features in Optuna. As of July 2024, OptunaHub supports samplers, visualization functions, and pruners.\r\n\r\nSo, we are actively looking for contributions and feature registrations for OptunaHub!\r\n\r\n### Description\r\n\r\nIf you are new to OptunaHub, you can learn what OptunaHub is and how to use it in  [Announcing OptunaHub 0.1.0-β](https://medium.com/optuna/announcing-optunahub-0-1-0-%CE%B2-69b35bb3e95e).\r\n\r\nTo contribute to OptunaHub, please make pull requests to the [optunahub-registry](https://github.com/optuna/optunahub-registry) repository.\r\n- [Contribution-welcome issues are available here!](https://github.com/optuna/optunahub-registry/issues?q=is%3Aissue+is%3Aopen+label%3Acontribution-welcome)\r\n- We also welcome registering features other than those already listed on the contribution-welcome list. For example, implementations of well-known and/or state-of-the-art algorithms, domain/application-specific features are welcome.\r\n-  If there is a feature you want but cannot implement, you can request it [here](https://github.com/optuna/optunahub-registry/issues/new/choose). Please note that there is no guarantee that it will be implemented.\r\n\r\nFor contributors, the following resources are available.\r\n- Tutorials on developing and registering features for OptunaHub\r\n  - [How to Implement Your Sampler with OptunaHub](https://optuna.github.io/optunahub-registry/recipes/001_first.html)\r\n  - [How to Register Your Algorithm with OptunaHub](https://optuna.github.io/optunahub-registry/recipes/002_registration.html)\r\n  - [How to Implement Your Pruners with OptunaHub](https://optuna.github.io/optunahub-registry/recipes/003_pruner.html)\r\n  - [How to Implement Your Own Visualization Function](https://optuna.github.io/optunahub-registry/recipes/004_visualization.html)\r\n  - [How to Debug Your Algorithm Before Registering in OptunaHub](https://optuna.github.io/optunahub-registry/recipes/005_debugging.html)\r\n- [Contribution guidelines for optunahub-registry](https://github.com/optuna/optunahub-registry/blob/main/CONTRIBUTING.md)\r\n\r\n![image](https://github.com/user-attachments/assets/b55befef-1839-4484-b904-eef4b6fce5ef)\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 6093,
    "title": "Feature Request: Add Dark Mode Toggle to Documentation",
    "author": "Ashish-Pandey62",
    "state": "open",
    "created_at": "2025-05-26T02:48:43Z",
    "updated_at": "2025-05-26T02:58:43Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\n\nOptuna’s documentation currently lacks a dark mode, which is a highly requested feature for better readability (especially in low-light environments) and reduced eye strain.  \n\n\n### Description\n\n\n### **Proposed Solution**  \nEnable the **built-in dark mode toggle** in [`sphinx_rtd_theme`](https://sphinx-rtd-theme.readthedocs.io/en/stable/configuring.html#dark-mode) (requires version `>=1.0`). This is a minimal, non-disruptive change:  \n1. Update `docs/conf.py`:  \n   ```python\n   html_theme_options = {\n    \"navbar_end\": [\"theme-switcher\"],  #  the theme toggle is included\n   }\n   ```  \n2. Ensure proper sphinx version in `docs/requirements.txt`.  \n\n### **Why This Matters**  \n- **User Experience**: Many developers prefer dark themes (e.g., GitHub, VS Code, ReadTheDocs).  \n- **Accessibility**: Reduces eye strain and improves readability.  \n- **Minimal Effort**: No visual/structural changes to existing docs.  \n\n### **Implementation Notes**  \n- Backward-compatible (falls back to light mode if JS is disabled).  \n- Toggle state is saved via `localStorage`.  \n\n### **Next Steps**  \nIf maintainers approve, I’d be happy to submit a PR!  ",
    "comments": []
  },
  {
    "issue_number": 6091,
    "title": "Support Multi-Node DDP with Lightning",
    "author": "LarsKue",
    "state": "open",
    "created_at": "2025-05-25T05:25:53Z",
    "updated_at": "2025-05-25T05:25:53Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nAs far as I can see, optuna's high-level interface does not support integrating with multi-node DDP in pytorch lightning. The [current ddp example](https://github.com/optuna/optuna-examples/blob/4e2d478949fe13063f2a9940051d5b1dc67aa50a/pytorch/pytorch_lightning_ddp.py) does not use multiple nodes, and if you naively do so by passing\n\n```py3\ntrainer = lightning.Trainer(..., num_nodes=2)\n```\n\nthen lightning will hang as optuna does not sync trials correctly across nodes.\n\nNote that I would also deem it critical to support using this interface with sqlite, as (in my experience), most clusters do not have built-in support for mysql.\n\n### Description\n\nThe new interface should simply allow using multiple nodes in the called function, either automatically, or with some argument, e.g. `num_nodes` or some variation of the current `n_jobs`.\n\n### Alternatives (optional)\n\nIf I am mistaken and this is actually already supported, please do not hesitate to show me an example! That said, I have tried finding a solution for about a week now, without success, so I am assuming it's simply not implemented.\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 6029,
    "title": "Use `TYPE_CHECKING` if necessary",
    "author": "nabenabe0928",
    "state": "open",
    "created_at": "2025-04-01T07:32:43Z",
    "updated_at": "2025-05-23T05:25:37Z",
    "labels": [
      "code-fix",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "> [!NOTE]\n> An example is available here:\n> - https://github.com/optuna/optuna/pull/6030\n\n\n### Motivation\n\nSome modules are imported only for type checking, but this may cause circular import, leading to an error.\nAlthough such an error is not a big deal for most cases, it may require annoying changes sometimes as in:\n- https://github.com/optuna/optuna/pull/5391\n\nThe issue above motivates using `TYPE_CHECKING`.\n\n### Suggestion\n\nFirst, we can check which files to modify by the following:\n\n```shell\n$ pip install flake8-type-checking\n$ flake8 optuna\n```\n\nIn order to reduce review costs and to encourage more contributors to work on it, please, as a rule, fix one file per PR.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "- [x] `optuna/_callbacks.py`\n- [x] `optuna/_convert_positional_args.py`\n- [x] `optuna/_deprecated.py`\n- [x] `optuna/_experimental.py`\n- [ ] `optuna/_gp/gp.py`\n- [ ] `optuna/_gp/optim_mixed.py`\n- [ ] `optuna/_gp/optim_sample.py`\n- [ ] `optuna/_gp/prior.py`\n- [ ] `optuna/_gp/search_space.py`\n- [ ] `optuna/_imports.py`\n- [ ] `optuna/artifacts/_download.py`\n- [ ] `optuna/artifacts/_list_artifact_meta.py`\n- [ ] `optuna/artifacts/_upload.py`\n- [x] `optuna/distributions.py`\n- [ ] `optuna/importance/__init__.py`\n- [x] `optuna/importance/_base.py`\n- [ ] `optuna/importance/_fanova/_evaluator.py`\n- [ ] `optuna/importance/_mean_decrease_impurity.py`\n- [ ] `optuna/importance/_ped_anova/evaluator.py`\n- [ ] `optuna/importance/_ped_anova/scott_parzen_estimator.py`\n- [ ] `optuna/integration/__init__.py`\n- [ ] `optuna/integration/lightgbm.py`\n- [ ] `optuna/pruners/_hyperband.py`\n- [ ] `optuna/pruners/_nop.py`\n- [ ] `optuna/pruners/_percentile.py`\n- [ ] `optuna/pruners/_wilcoxon.py`\n- [ ] `optuna/samplers/_base.py`\n- [ ] `optuna/samplers/_brute_force.py`\n- [ ] `optuna/samplers/_cmaes.py`\n- [ ] `optuna/samplers/_ga/_base.py`\n- [x] `optuna/samplers/_gp/sampler.py`\n- [ ] `optuna/samplers/_grid.py`\n- [ ] `optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py`\n- [ ] `optuna/samplers/_nsgaiii/_sampler.py`\n- [ ] `optuna/samplers/_partial_fixed.py`\n- [ ] `optuna/samplers/_qmc.py`\n- [ ] `optuna/samplers/_random.py`\n- [ ] `optuna/samplers/_tpe/_truncnorm.py`\n- [ ] `optuna/samplers/_tpe/parzen_estimator.py`\n- [ ] `optuna/samplers/_tpe/sampler.py`\n- [ ] `optuna/samplers/nsgaii/_after_trial_strategy.py`\n- [ ] `optuna/samplers/nsgaii/_child_generation_strategy.py`\n- [ ] `optuna/samplers/nsgaii/_constraints_evaluation.py`\n- [ ] `optuna/samplers/nsgaii/_crossover.py`\n- [ ] `optuna/samplers/nsgaii/_crossovers/_blxalpha.py`\n- [ ] `optuna/samplers/nsgaii/_crossovers/_uniform.py`\n- [ ] `optuna/samplers/nsgaii/_elite_population_selection_strategy.py`\n- [ ] `optuna/samplers/nsgaii/_sampler.py`\n- [ ] `optuna/search_space/group_decomposed.py`\n- [ ] `optuna/search_space/intersection.py`\n- [ ] `optuna/storages/_base.py`\n- [ ] `optuna/storages/_cached_storage.py`\n- [ ] `optuna/storages/_callbacks.py`\n- [ ] `optuna/storages/_grpc/client.py`\n- [ ] `optuna/storages/_grpc/server.py`\n- [ ] `optuna/storages/_grpc/servicer.py`\n- [ ] `optuna/storages/_heartbeat.py`\n- [ ] `optuna/storages/_in_memory.py`\n- [ ] `optuna/storages/_rdb/models.py`\n- [ ] `optuna/storages/_rdb/storage.py`\n- [ ] `optuna/storages/journal/_file.py`\n- [ ] `optuna/storages/journal/_storage.py`\n- [ ] `optuna/study/_constrained_optimization.py`\n- [ ] `optuna/study/_frozen.py`\n- [ ] `optuna/study/_multi_objective.py`\n- [ ] `optuna/study/_optimize.py`\n- [ ] `optuna/study/_study_summary.py`\n- [ ] `optuna/study/study.py`\n- [ ] `optuna/terminator/callback.py`\n- [ ] `optuna/terminator/erroreval.py`\n- [ ] `optuna/terminator/improvement/emmr.py`\n- [ ] `optuna/terminator/improvement/evaluator.py`\n- [ ] `optuna/terminator/median_erroreval.py`\n- [ ] `optuna/terminator/terminator.py`\n- [ ] `optuna/testing/objectives.py`\n- [ ] `optuna/testing/samplers.py`\n- [ ] `optuna/testing/storages.py`\n- [ ] `optuna/testing/tempfile_pool.py`\n- [ ] `optuna/testing/threading.py`\n- [ ] `optuna/testing/trials.py`\n- [ ] `optuna/testing/visualization.py`\n- [ ] `optuna/trial/_base.py`\n- [ ] `optuna/trial/_fixed.py`\n- [ ] `optuna/trial/_frozen.py`\n- [ ] `optuna/trial/_trial.py`\n- [ ] `optuna/visualization/_contour.py`\n- [ ] `optuna/visualization/_edf.py`\n- [ ] `optuna/visualization/_hypervolume_history.py`\n- [ ] `optuna/visualization/_intermediate_values.py`\n- [ ] `optuna/visualization/_optimization_history.py`\n- [ ] `optuna/visualization/_parallel_coordinate.py`\n- [ ] `optuna/visualization/_param_importances.py`\n- [ ] `optuna/visualization/_pareto_front.py`\n- [ ] `optuna/visualization/_rank.py`\n- [ ] `optuna/visualization/_slice.py`\n- [ ] `optuna/visualization/_terminator_improvement.py`\n- [ ] `optuna/visualization/_timeline.py`\n- [ ] `optuna/visualization/_utils.py`\n- [ ] `optuna/visualization/matplotlib/_contour.py`\n- [ ] `optuna/visualization/matplotlib/_edf.py`\n- [ ] `optuna/visualization/matplotlib/_hypervolume_history.py`\n- [ ] `optuna/visualization/matplotlib/_intermediate_values.py`\n- [ ] `optuna/visualization/matplotlib/_optimization_history.py`\n- [ ] `optuna/visualization/matplotlib/_parallel_coordinate.py`\n- [ ] `optuna/visualization/matplotlib/_param_importances.py`\n- [ ] `optuna/visualization/matplotlib/_pareto_front.py`\n- [ ] `optuna/visualization/matplotlib/_rank.py`\n- [ ] `optuna/visualization/matplotlib/_slice.py`\n- [ ] `optuna/visualization/matplotlib/_terminator_improvement.py`\n- [ ] `optuna/visualization/matplotlib/_timeline.py`\n- [ ] `optuna/visualization/matplotlib/_utils.py`\n"
      },
      {
        "user": "ParagEkbote",
        "body": "@nabenabe0928 Could you keep this issue open as it not been completed yet?"
      }
    ]
  },
  {
    "issue_number": 6032,
    "title": "Not handling capitalization correctly when using MySQL",
    "author": "AlexanderTreml",
    "state": "open",
    "created_at": "2025-04-04T14:04:32Z",
    "updated_at": "2025-05-22T06:23:48Z",
    "labels": [
      "bug",
      "v5"
    ],
    "body": "### Expected behavior\n\nWhen two parameters share the same name up to capitalization, e.g. \"a\" and \"A\", only one is stored in the trials when using a MySQL database. Instead, both should be stored, just like when using in memory storage.\n\n### Environment\n\n- Optuna version: 4.2.1\n- Python version: 3.10.12\n- OS: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\n\n\n### Error messages, stack traces, or logs\n\n```shell\n[I 2025-04-04 15:59:35,803] A new study created in memory with name: works\n[I 2025-04-04 15:59:35,931] A new study created in RDB with name: worknt\n[I 2025-04-04 15:59:35,978] Trial 0 finished with value: 11.0 and parameters: {'a': 6, 'A': 5}. Best is trial 0 with value: 11.0.\n[I 2025-04-04 15:59:35,979] Trial 1 finished with value: 4.0 and parameters: {'a': 1, 'A': 3}. Best is trial 1 with value: 4.0.\n[I 2025-04-04 15:59:35,979] Trial 2 finished with value: 11.0 and parameters: {'a': 7, 'A': 4}. Best is trial 1 with value: 4.0.\n[I 2025-04-04 15:59:36,129] Trial 0 finished with value: 12.0 and parameters: {'a': 6}. Best is trial 0 with value: 12.0.\n[I 2025-04-04 15:59:36,271] Trial 1 finished with value: 9.0 and parameters: {'a': 9}. Best is trial 1 with value: 9.0.\n[I 2025-04-04 15:59:36,433] Trial 2 finished with value: 2.0 and parameters: {'a': 2}. Best is trial 2 with value: 2.0.\nIn memory:  {'a': 1, 'A': 3}\nMySQL:  {'a': 2}\n```\n\n### Steps to reproduce\n\n1. Configure the database in the example code\n2. Execute\n```python\nimport optuna\n\n# Insert your DB config here\nhyper_opt_db = {\n        \"host\": \"localhost\",\n        \"port\": 3306,\n        \"user\": \"optuna\",\n        \"password\": \"password\",\n        \"database\": \"hyper_opt\"\n    }\n\ndb_url = f\"mysql://{hyper_opt_db['user']}:{hyper_opt_db['password']}@{hyper_opt_db['host']}:{hyper_opt_db['port']}/{hyper_opt_db['database']}\"\n\ndef objective(trial):\n    a = trial.suggest_int(\"a\", 0, 10)\n    A = trial.suggest_int(\"A\", 0, 10)\n\n    return a + A\n\n\nstudy_in_memory = optuna.create_study(study_name=\"works\")\nstudy_in_db = optuna.create_study(study_name=\"worknt\", storage=db_url)\n\n# this works\nstudy_in_memory.optimize(objective, n_trials=3)\n\n# this does not\nstudy_in_db.optimize(objective, n_trials=3)\n\nprint(\"In memory: \", study_in_memory.best_params)\nprint(\"MySQL: \", study_in_db.best_params)\n```\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "not522",
        "body": "Thank you for your bug report! I confirmed it is reproduced in my environment. We are investigating the cause of the bug and how to fix it."
      },
      {
        "user": "fusawa-yugo",
        "body": "I'm working on this issue."
      }
    ]
  },
  {
    "issue_number": 6071,
    "title": "`BruteForceSampler` is significantly slower than `GridSampler`",
    "author": "nabenabe0928",
    "state": "open",
    "created_at": "2025-05-03T15:45:55Z",
    "updated_at": "2025-05-22T01:47:17Z",
    "labels": [
      "enhancement",
      "needs-discussion",
      "v5"
    ],
    "body": "### Motivation\n\nAlthough the functionality of `GridSampler` and `BruteForceSampler` is quite similar, their speeds are significantly different.\nIt would be great if the runtime of `BruteForceSampler` is improved.\n\n### Description\n\nIn the example below, `BruteForceSampler` is about 4x slower.\n\n![Image](https://github.com/user-attachments/assets/d514de54-cc21-485e-ac98-3764633da4b7)\n\n\n<details>\n<summary>Benchmarking Code</summary>\n\n```python\nimport matplotlib.pyplot as plt\nimport optuna\n\n\nplt.rcParams[\"font.family\"] = \"Times New Roman\"\nplt.rcParams[\"font.size\"] = 18\nplt.rcParams[\"mathtext.fontset\"] = \"stix\"  # The setting of math font\nplt.rcParams[\"text.usetex\"] = True\nsearch_space = {f\"c{i}\": [str(i) for i in range(5)] for i in range(6)}\n\n\ndef objective(trial: optuna.Trial) -> float:\n    for k, v in search_space.items():\n        trial.suggest_categorical(k, v)\n    return 0.0\n\n\n\nstudy = optuna.create_study(sampler=optuna.samplers.BruteForceSampler(seed=0))\nstudy.optimize(objective)\nstart = study.trials[0].datetime_start\nruntime_brute_force = [(t.datetime_complete - start).total_seconds() for t in study.trials]\nstudy = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space=search_space, seed=0))\nstudy.optimize(objective)\nstart = study.trials[0].datetime_start\nruntime_grid = [(t.datetime_complete - start).total_seconds() for t in study.trials]\n\ndx = [i + 1 for i in range(len(runtime_grid))]\n_, ax = plt.subplots()\nax.plot(dx, runtime_brute_force, color=\"blue\", label=\"BruteForceSampler\")\nax.plot(dx, runtime_grid, color=\"red\", label=\"GridSampler\")\nax.legend()\nax.set_xlabel(\"Number of Trials\")\nax.set_ylabel(\"Runtime [s]\")\nax.grid(which=\"minor\", color=\"gray\", linestyle=\":\")\nax.grid(which=\"major\", color=\"black\")\nax.set_yscale(\"log\")\nplt.savefig(\"runtime.png\", bbox_inches=\"tight\")\n\n```\n\n</details>\n\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5953,
    "title": "`TPESampler` warns numerical error",
    "author": "not522",
    "state": "open",
    "created_at": "2025-01-30T09:40:23Z",
    "updated_at": "2025-05-22T01:42:27Z",
    "labels": [
      "bug",
      "needs-discussion",
      "v5"
    ],
    "body": "### Expected behavior\n\n`TPESampler` should not warn numerical error. The current implementation raises it when suggested values on the tail of probability distribution.\n\n### Environment\n\n- Optuna version:4.3.0.dev\n- Python version:3.11.0\n- OS:macOS-15.1-x86_64-i386-64bit\n- (Optional) Other libraries and their versions:\n\n\n### Error messages, stack traces, or logs\n\n```shell\n$ python tmp.py \n[I 2025-01-30 18:28:11,847] A new study created in memory with name: no-name-3f714210-b391-4c23-94bb-56345197b9a7\n[I 2025-01-30 18:28:11,848] Trial 0 finished with value: 10012.55030990297 and parameters: {'x': 100.06273187307535}. Best is trial 0 with value: 10012.55030990297.\n...\n[I 2025-01-30 18:28:11,880] Trial 19 finished with value: 10067.928082503919 and parameters: {'x': 100.33906558516438}. Best is trial 10 with value: 10001.100652804867.\n/Users/naotomizuno/optuna/optuna/samplers/_tpe/sampler.py:513: RuntimeWarning: invalid value encountered in subtract\n  acq_func_vals = log_likelihoods_below - log_likelihoods_above\n[I 2025-01-30 18:28:11,884] Trial 20 finished with value: 15629.47890252108 and parameters: {'x': 125.01791432639196}. Best is trial 10 with value: 10001.100652804867.\n```\n\n### Steps to reproduce\n\nThis code outputs the above warning.\n\n```python\nimport optuna\n\ndef objective1(trial):\n    return trial.suggest_float(\"x\", 100, 101) ** 2\n\ndef objective2(trial):\n    return trial.suggest_float(\"x\", 0, 1) ** 2\n\nstudy = optuna.create_study()\nstudy.optimize(objective1, n_trials=20)\nstudy.optimize(objective2, n_trials=1)\n```\n\n\n### Additional context (optional)\n\nI think that the range of bisect is not enough. https://github.com/optuna/optuna/blob/733a717ad091d0e7cef16fe89cc3f7bf2bda56ed/optuna/samplers/_tpe/_truncnorm.py#L168\n\nWe have several choices for fixing this issue.\n- Use wider range\n- Use exponential search \n- Use the suggested method on https://github.com/optuna/optuna/pull/4105#discussion_r1065455150\n- Use the suggested method on https://github.com/optuna/optuna/pull/5598#issuecomment-2254507950",
    "comments": []
  },
  {
    "issue_number": 6084,
    "title": "`JournalStorage` fails frequently in distributed optimization setups in combination with `GrpcProxyStorage`",
    "author": "nabenabe0928",
    "state": "open",
    "created_at": "2025-05-20T05:19:28Z",
    "updated_at": "2025-05-20T08:21:26Z",
    "labels": [
      "bug",
      "needs-discussion"
    ],
    "body": "### Expected behavior\n\nSome processes are killed stochastically when using `JournalStorage` with `GrpcProxyStorage`.\nIt seems this problem happens due to the design assumption of `JournalStorage`, i.e., only one thread in a process takes care of a trial from the beginning to the end.\nHowever, `GrpcProxyStorage`, in principle, breaks this rule because the process for sampling and that for evaluating a trial are separated.\n\n![Image](https://github.com/user-attachments/assets/f772b9c5-cdd5-4115-8338-b07d7ce5fbaf)\n\n> [!NOTE]\n> This issue typically happens when using `enqueue_trial`, but I cannot deny the possibility that we face the same issue with other operations as well.\n\n### Environment\n\n- Optuna version: 4.3\n- Python version: 3.11\n- OS: Ubuntu 20.04\n\n\n### Error messages, stack traces, or logs\n\n```shell\n[I 2025-05-20 07:15:00,216] Using an existing study with name 'b99b661c-4229-4a54-92bf-f29b0ed7db25' instead of creating a new one.\n[I 2025-05-20 07:15:00,222] Using an existing study with name 'b99b661c-4229-4a54-92bf-f29b0ed7db25' instead of creating a new one.\n[I 2025-05-20 07:15:00,227] Using an existing study with name 'b99b661c-4229-4a54-92bf-f29b0ed7db25' instead of creating a new one.\n[I 2025-05-20 07:15:00,231] Using an existing study with name 'b99b661c-4229-4a54-92bf-f29b0ed7db25' instead of creating a new one.\n[I 2025-05-20 07:15:00,242] Using an existing study with name 'b99b661c-4229-4a54-92bf-f29b0ed7db25' instead of creating a new one.\n[I 2025-05-20 07:15:00,245] Using an existing study with name 'b99b661c-4229-4a54-92bf-f29b0ed7db25' instead of creating a new one.\n[I 2025-05-20 07:15:00,246] Using an existing study with name 'b99b661c-4229-4a54-92bf-f29b0ed7db25' instead of creating a new one.\n[I 2025-05-20 07:15:00,254] Using an existing study with name 'b99b661c-4229-4a54-92bf-f29b0ed7db25' instead of creating a new one.\n[I 2025-05-20 07:15:00,294] Trial 0 finished with value: 114.0 and parameters: {'y': 3.8116090154495232, 'x': 0.08687552782907204}. Best is trial 0 with value: 114.0.\n[I 2025-05-20 07:15:00,298] Trial 1 finished with value: 205.0 and parameters: {'y': -1.2209212453908358, 'x': -2.021884130213263}. Best is trial 0 with value: 114.0.\n[I 2025-05-20 07:15:00,308] Trial 2 finished with value: 311.0 and parameters: {'y': 0.7764554661270058, 'x': 3.346412592844274}. Best is trial 0 with value: 114.0.\n[I 2025-05-20 07:15:00,314] Trial 2 finished with value: 311.0 and parameters: {'y': 0.7764554661270058, 'x': 3.346412592844274}. Best is trial 0 with value: 114.0.\nProcess Process-7:\n[I 2025-05-20 07:15:00,315] Trial 3 finished with value: 534.0 and parameters: {'y': -3.0491387551257243, 'x': 4.990802171581327}. Best is trial 0 with value: 114.0.\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/shuhei/pfn-work/optuna-dev/optuna/simple_grpc.py\", line 27, in main\n    study.optimize(objective, n_trials=1)\n  File \"/home/shuhei/pfn-work/optuna-dev/optuna/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/home/shuhei/pfn-work/optuna-dev/optuna/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/home/shuhei/pfn-work/optuna-dev/optuna/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/shuhei/pfn-work/optuna-dev/optuna/optuna/study/_optimize.py\", line 209, in _run_trial\n    frozen_trial = _tell_with_warning(\n                   ^^^^^^^^^^^^^^^^^^^\n  File \"/home/shuhei/pfn-work/optuna-dev/optuna/optuna/study/_tell.py\", line 120, in _tell_with_warning\n    raise ValueError(f\"Cannot tell a {frozen_trial.state.name} trial.\")\nValueError: Cannot tell a COMPLETE trial.\n[I 2025-05-20 07:15:00,320] Trial 4 finished with value: 426.0 and parameters: {'y': -4.9408864997561945, 'x': 1.5103378665562222}. Best is trial 0 with value: 114.0.\n[I 2025-05-20 07:15:00,322] Trial 5 finished with value: 629.0 and parameters: {'y': -4.889657514236255, 'x': -2.4055138468929904}. Best is trial 0 with value: 114.0.\n[I 2025-05-20 07:15:00,324] Trial 6 finished with value: 826.0 and parameters: {'y': 4.624483520664185, 'x': 2.162965026496132}. Best is trial 0 with value: 114.0.\n```\n\n### Steps to reproduce\n\nInstall the dependencies:\n\n```shell\n$ pip install optuna grpcio protobuf\n```\n\nThen build a proxy server with the following code:\n\n```python\nimport os\n\nfrom optuna.storages import run_grpc_proxy_server\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalStorage\n\n\ntry:\n    os.remove(\"test-grpc.log\")\nexcept FileNotFoundError:\n    pass\nstorage = JournalStorage(JournalFileBackend(\"test-grpc.log\"))\nrun_grpc_proxy_server(storage, host=\"localhost\", port=13000)\n```\n\nLaunch another process and run the following code:\n\n```python\nfrom collections.abc import Callable\nimport multiprocessing\nimport os\nimport time\nimport uuid\n\nimport numpy as np\nimport optuna\n\n\ndef load_study(study_name: str, storage_builder: Callable[[], optuna.storages.BaseStorage]) -> optuna.Study:\n    sampler = optuna.samplers.RandomSampler()\n    return optuna.create_study(\n        study_name=study_name, sampler=sampler, storage=storage_builder(), load_if_exists=True\n    )\n\n\ndef main(study_name: str, worker_id: int, storage_builder: Callable[[], optuna.storages.BaseStorage]) -> None:\n    def objective(trial: optuna.Trial) -> float:\n        time.sleep(0.01)\n        x = trial.suggest_float(\"x\", -5, 5)\n        y = trial.suggest_float(\"y\", -5, 5)\n        return float(int((worker_id + 1) * 100 + x**2 + y**2))\n\n\n    study = load_study(study_name, storage_builder)\n    study.optimize(objective, n_trials=1)\n\n\ndef enqueue(study_name: str, storage_builder: Callable[[], optuna.storages.BaseStorage]) -> None:\n    study = load_study(study_name, storage_builder)\n    XY = np.random.random((50, 2)) * 10 - 5\n    for xy in XY:\n        study.enqueue_trial({\"x\": float(xy[0]), \"y\": float(xy[1])})\n\n\ndef execute(storage_builder: Callable[[], optuna.storages.BaseStorage]) -> None:\n    study_name = str(uuid.uuid4())\n    enqueue(study_name, storage_builder)\n    procs = []\n    for i in range(8):\n        proc = multiprocessing.Process(target=main, args=(study_name, i, storage_builder))\n        procs.append(proc)\n        proc.start()\n\n    for proc in procs:\n        proc.join()\n\n\nif __name__ == \"__main__\":\n    execute(lambda: optuna.storages.GrpcStorageProxy(host=\"localhost\", port=13000))\n\n```\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "Although I have not observed any issues without `enqueue_trial` yet, the racing condition may potentially happen in `create_new_trial`."
      },
      {
        "user": "nabenabe0928",
        "body": "The cause for `set_trial_state_values` should be from here:\n\nhttps://github.com/optuna/optuna/blob/f694a9b06009000593bc4e364bfa33ee0c710b25/optuna/storages/journal/_storage.py#L322-L329\n\nRewriting like this should fix this issue, but it's necessary to have scrutiny.\n\n```python\n        with self._thread_lock:\n            if state == TrialState.RUNNING:\n                self._sync_with_backend()\n                existing_trial = self._replay_result._trials.get(trial_id)\n                if existing_trial is not None and existing_trial.state != TrialState.WAITING:\n                    return False\n\n            self._write_log(JournalOperation.SET_TRIAL_STATE_VALUES, log)\n            self._sync_with_backend()\n            return True\n```"
      },
      {
        "user": "nabenabe0928",
        "body": "Another suspicion is in `create_new_trial`:\n\nhttps://github.com/optuna/optuna/blob/f694a9b06009000593bc4e364bfa33ee0c710b25/optuna/storages/journal/_storage.py#L229-L277\n\nHowever, thanks to the appropriate file lock of the log file, the log file is appropriately organized, leading to the correct length of `self._trials` in:\n\nhttps://github.com/optuna/optuna/blob/f694a9b06009000593bc4e364bfa33ee0c710b25/optuna/storages/journal/_storage.py#L520\n\nOne potential problem is that some trials say they started earlier than older trials, meaning that the `datetime_start` of the $i$-th trial may later than the $j$-th trial where $i < j$.\nIf this is not the problem, this part should be fine.\n\nhttps://github.com/optuna/optuna/blob/f694a9b06009000593bc4e364bfa33ee0c710b25/optuna/storages/journal/_storage.py#L229-L233"
      }
    ]
  },
  {
    "issue_number": 6083,
    "title": "Optuna studies spontaneously stop accepting new trials when multiprocessing",
    "author": "ConPalos",
    "state": "open",
    "created_at": "2025-05-16T19:39:41Z",
    "updated_at": "2025-05-19T14:22:36Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nI am building a study to tune a regression that uses 8 hyperparameters and 7 objectives. I expect that when I ask for 1,000 trials (or set a timeout of 900 seconds), that the code will continue generating trials until I tell it to stop. The first 129 trials (one more than the number of CPUs on my machine) succeed normally, but all subsequent trials fail.\n\n### Environment\n\n- Optuna version: 4.3.0\n- Python version: 3.10.9\n- OS: AWS Linux 2\n- (Optional) Other libraries and their versions:\n\n\n### Error messages, stack traces, or logs\n\n```shell\nFile \"/home/ec2-user/scripts/optuna.py\", line 183, in objective\n    'n_estimators': trial.suggest_int('estimators', 500, 10000),\n  File \"/home/ec2-user/scripts/venv/lib/python3.10/site-packages/optuna/_convert_positional_args.py\", line 134, in converter_wrapper\n    return func(**kwargs)  # type: ignore[call-arg]\n  File \"/home/ec2-user/scripts/venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 324, in suggest_int\n    suggested_value = int(self._suggest(name, distribution))\n  File \"/home/ec2-user/scripts/venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 627, in _suggest\n    elif self._is_relative_param(name, distribution):\n  File \"/home/ec2-user/scripts/venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 659, in _is_relative_param\n    if name not in self.relative_params:\n  File \"/home/ec2-user/scripts/venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 71, in relative_params\n    self._relative_params = self.study.sampler.sample_relative(\n  File \"/home/ec2-user/scripts/venv/lib/python3.10/site-packages/optuna/samplers/nsgaii/_sampler.py\", line 269, in sample_relative\n    parent_population = self.get_parent_population(study, generation)\n  File \"/home/ec2-user/scripts/venv/lib/python3.10/site-packages/optuna/samplers/_ga/_base.py\", line 174, in get_parent_population\n    return [trials[trial_id] for trial_id in cached_parent_population_ids]\n  File \"/home/ec2-user/scripts/venv/lib/python3.10/site-packages/optuna/samplers/_ga/_base.py\", line 174, in <listcomp>\n    return [trials[trial_id] for trial_id in cached_parent_population_ids]\nIndexError: list index out of range\n```\n\n### Steps to reproduce\n\n1. Set up a study (all studies seem to have this problem) - Edit: This is restricted to multi-objective studies on remote DBs\n2. Ask it to perform 500 trials (at least 2 more than the number of available CPUs is fine)\n3. Set `n_jobs` to -1\n\n### Minimal Reproducible Code\n```py\nimport optuna\n\ndef objective(trial):\n    x = trial.suggest_float('x', 0.0, 10.0)\n\n    return x**2, 2*x\n\nurl = 'mysql+pymysql://{username}:{password}@{not_leaking_my_db}:{port}/{db_name}' # I am using an AWS RDS MySQL server here\nstudy = optuna.create_study(\n    study_name='test',\n    storage=url,\n    directions=['maximize', 'maximize']\n)\nstudy.optimize(objective, timeout=10, n_jobs=-1) # this also works with a sufficiently large n_trials, but i suspect \"sufficient\" varies by machine\n```",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Hi, could you share [minimal reproducible code](https://stackoverflow.com/help/minimal-reproducible-example) with us?"
      },
      {
        "user": "nabenabe0928",
        "body": "This issue relates to:\n- https://github.com/optuna/optuna/pull/6066"
      },
      {
        "user": "ConPalos",
        "body": "I've added minimal reproducible code. This does seem to be very similar to #6066. I expect this behavior to be an edge case because it only happens when connecting to an external database (I cannot reproduce this in memory) and with multi-objective functions."
      }
    ]
  },
  {
    "issue_number": 5478,
    "title": "Speed up `TPESampler` using approximation in standard normal related computation",
    "author": "nabenabe0928",
    "state": "open",
    "created_at": "2024-06-04T10:58:53Z",
    "updated_at": "2025-05-18T23:06:21Z",
    "labels": [
      "feature",
      "needs-discussion"
    ],
    "body": "### Motivation\r\n\r\n`TPESampler` significantly slows down for high-dimensional objectives and I checked where the slowdown comes from.\r\n\r\nAccording to my check, `_get_internal_repr` in `TPESampler` and `ndtr`-related functions in `_truncnorm.py` were the causes.\r\n\r\nAs the Optuna design prefers the stateless nature of each sampler, I think it is hard to enhance `_get_internal_repr` while it is possible to enhance `ndtr`.\r\n\r\nIn fact, I could solve the following problem three times quicker with an approximation algorithm for standard normal distribution:\r\n\r\n```python\r\nimport time\r\n\r\nimport optuna\r\n\r\n\r\ndef objective(trial: optuna.Trial) -> float:\r\n    if (trial.number + 1) % 50 == 0:\r\n        print(trial.number + 1)\r\n\r\n    return sum(trial.suggest_float(f\"x{i}\", -5, 5) ** 2 for i in range(10))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    optuna.logging.set_verbosity(optuna.logging.CRITICAL)\r\n    sampler = optuna.samplers.TPESampler(seed=42)\r\n    study = optuna.create_study(sampler=sampler)\r\n    start = time.time()\r\n    study.optimize(objective, n_trials=1000)\r\n    print(time.time() - start, study.best_trial)\r\n``` \r\n\r\n\r\n### Description\r\n\r\nBy adding an option to use an approximation algorithm, we can speed up `TPESampler` routine.\r\n\r\nThere is a paper that discusses the approximation quality of standard normal distribution, c.f. APPROXIMATING THE CUMULATIVE DISTRIBUTION FUNCTION OF THE NORMAL DISTRIBUTION.\r\n\r\nI used the standard logistic function, i.e. Eq. (13) in the paper, due to its invertibility necessary for `ndtri_exp` (however, we can also use different approximation methods for different functions).\r\nNote that invertibility is supported by Eqs. (1), (4), (5), (7), (8), and (13).\r\nFor example, `_ndtr` is already vectorized, so we do not have to add an approximation algorithm for it and we can use better approximation such as Byrc (2001B) in Eq. (12) or Zelen and Severo (1964) in Eq. (2).\r\n\r\n```python\r\n_logistic_C = -math.pi / math.sqrt(3)\r\n\r\n\r\ndef _ndtri_exp(y: np.ndarray) -> np.ndarray:\r\n    # y = -log(1 + exp(cx)) --> x = 1/c * log(exp(-y) - 1)\r\n    return 1 / _logistic_C * _log_diff(-y, np.array(0.0))\r\n\r\n\r\ndef _ndtr(a: np.ndarray) -> np.ndarray:\r\n    return 1.0 / (1.0 + np.exp(_logistic_C * a))\r\n\r\n\r\ndef _log_ndtr(a: np.ndarray) -> np.ndarray:\r\n    return -_log_sum(np.zeros_like(a), _logistic_C * a)\r\n\r\n\r\ndef _log_ndtr_by_byrc(a: np.ndarray) -> np.ndarray:\r\n    # return np.frompyfunc(_log_ndtr_single, 1, 1)(a).astype(float)\r\n    n2, n1, n0 = (1.0, 5.575192695, 12.77436324)\r\n    d3, d2, d1, d0 = (_norm_pdf_C, 14.38718147, 31.53531977,  25.548726)\r\n    a1 = np.abs(a)\r\n    a2 = a1 * a1\r\n    a3 = a1 * a2\r\n    res = np.log(n2 * a2 + n1 * a1 + n0) - np.log(d3 * a3 + d2 * a2 + d1 * a1 + d0) - a2 * 0.5\r\n    a_is_positive = a > 0\r\n    res[a_is_positive] = _log_diff(np.array(0.0), res[a_is_positive])\r\n    return res\r\n```\r\n\r\n\r\n### Alternatives (optional)\r\n\r\nAnother option would be to add an option to use only the recent `K` trials for the surrogate training.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 6073,
    "title": "`GridSampler` duplicates enqueued parameters",
    "author": "nabenabe0928",
    "state": "closed",
    "created_at": "2025-05-06T07:51:09Z",
    "updated_at": "2025-05-16T05:10:53Z",
    "labels": [
      "bug",
      "needs-discussion"
    ],
    "body": "### Expected behavior\n\n`GridSampler` should not duplicate parameters, but it does when using `enqueue_trial` or `add_trial`.\n\nMost likely, the following part must be adapted as enqueued trials do not have `grid_id` in the system attributes.\n\nhttps://github.com/optuna/optuna/blob/1e14484a9916cadbb738b1c8e9f987075cec16c3/optuna/samplers/_grid.py#L245\n\n### Environment\n\n- Optuna version: 4.3.1\n- Python version: 3.12\n- OS: Ubuntu\n\n### Error messages, stack traces, or logs\n\n```shell\nSee below.\n```\n\n### Steps to reproduce\n\n```python\nimport optuna\n\n\nsearch_space = {f\"c{i}\": [str(i) for i in range(2)] for i in range(2)}\n\ndef objective(trial: optuna.Trial) -> float:\n    for k, v in search_space.items():\n        trial.suggest_categorical(k, v)\n    return 0.0\n\n\nstudy = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space=search_space))\nstudy.enqueue_trial({k: v[0] for k, v in search_space.items()})\nstudy.optimize(objective)\n\n```\n\nThe study duplicates `{\"c0\": 0, \"c1\": 0}`.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "We talked internally about this, and this behavior is intended, so I close this issue."
      }
    ]
  },
  {
    "issue_number": 6070,
    "title": "`BruteForceSampler` is supposed to sample each category uniformly, but it actually doesn't",
    "author": "nabenabe0928",
    "state": "open",
    "created_at": "2025-05-03T11:53:17Z",
    "updated_at": "2025-05-16T05:00:55Z",
    "labels": [
      "enhancement",
      "needs-discussion"
    ],
    "body": "### Expected behavior\n\n`BruteForceSampler` should sample each category uniformly or should prioritize categories that have not appeared in any trials yet, but it does not.\n\nFor example, the code provided in `Steps to Reproduce` should yield all the categories in `c0` sooner or later, but it actually does not until at least 80 trials.\nStatistically speaking, the probability that we do not get all the categories in 80 trials is $(\\frac{4}{5})^{80} \\simeq 1.77 \\times 10^{-8}$, which would not happen so easily.\n\n### Environment\n\n- Optuna version: 4.3.1\n- Python version: 3.11\n- OS: Ubuntu 20.04\n\n### Error messages, stack traces, or logs\n\n```shell\nUntil 10-th Trials...\nc0: unique_values=array(['0', '2', '4'], dtype=object), counts=array([2, 5, 3])\nc1: unique_values=array(['1', '3', '4'], dtype=object), counts=array([1, 5, 4])\nc2: unique_values=array(['0', '1', '3', '4'], dtype=object), counts=array([3, 2, 4, 1])\nc3: unique_values=array(['0', '2', '3', '4'], dtype=object), counts=array([1, 2, 3, 4])\n\nUntil 20-th Trials...\nc0: unique_values=array(['0', '2', '3', '4'], dtype=object), counts=array([4, 8, 1, 7])\nc1: unique_values=array(['1', '2', '3', '4'], dtype=object), counts=array([7, 1, 7, 5])\nc2: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([4, 4, 2, 7, 3])\nc3: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([6, 3, 4, 3, 4])\n\nUntil 40-th Trials...\nc0: unique_values=array(['0', '2', '3', '4'], dtype=object), counts=array([ 8, 13,  7, 12])\nc1: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([ 4, 12,  4, 11,  9])\nc2: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([9, 8, 5, 9, 9])\nc3: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([10,  5,  8,  8,  9])\n\nUntil 80-th Trials...\nc0: unique_values=array(['0', '2', '3', '4'], dtype=object), counts=array([19, 23, 16, 22])\nc1: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([12, 21, 16, 15, 16])\nc2: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([15, 15, 15, 18, 17])\nc3: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([23, 12, 17, 11, 17])\n\nUntil 150-th Trials...\nc0: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([35, 13, 35, 31, 36])\nc1: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([18, 45, 32, 26, 29])\nc2: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([28, 23, 32, 33, 34])\nc3: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([39, 22, 29, 26, 34])\n\nUntil 300-th Trials...\nc0: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([68, 39, 69, 50, 74])\nc1: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([52, 75, 66, 44, 63])\nc2: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([66, 55, 52, 63, 64])\nc3: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([59, 63, 59, 61, 58])\n\nUntil 625-th Trials...\nc0: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([125, 125, 125, 125, 125])\nc1: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([125, 125, 125, 125, 125])\nc2: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([125, 125, 125, 125, 125])\nc3: unique_values=array(['0', '1', '2', '3', '4'], dtype=object), counts=array([125, 125, 125, 125, 125])\n```\n\n### Steps to reproduce\n\nRun the following code:\n\n```python\nimport json\n\nimport numpy as np\nimport optuna\nimport pandas as pd\n\n\ndef objective(trial: optuna.Trial) -> float:\n    choices = [str(i) for i in range(5)]\n    [trial.suggest_categorical(f\"c{i}\", choices) for i in range(4)]\n    return 0.0\n\n\ndef get_dataframe() -> pd.DataFrame:\n    study = optuna.create_study(sampler=optuna.samplers.BruteForceSampler(avoid_premature_stop=True, seed=0))\n    study.optimize(objective)\n    return pd.DataFrame([t.params for t in study.trials])\n\n\ndf = get_dataframe()\nfor i in [10, 20, 40, 80, 150, 300, 625]:\n    print(f\"Until {i}-th Trials...\")\n    for j in range(4):\n        unique_values, counts = np.unique(df[f\"c{j}\"][:i], return_counts=True)\n        print(f\"c{j}: {unique_values=}, {counts=}\")\n\n    print()\n\n```\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3205,
    "title": "`nan` handling in Optuna",
    "author": "HideakiImamura",
    "state": "closed",
    "created_at": "2021-12-24T04:47:53Z",
    "updated_at": "2025-05-15T08:41:12Z",
    "labels": [
      "bug",
      "contribution-welcome",
      "no-stale",
      "v3"
    ],
    "body": "<!-- Please write a clear and concise description of what the bug is. -->\r\n\r\nA new version of https://github.com/optuna/optuna/issues/2169 adapted to the latest Optuna.\r\nAnd related to https://github.com/optuna/optuna/issues/3132.\r\n\r\n[note: fixed 2022/01/30]\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n~Regardless of the type of storage used, mark the corresponding trial as `FAILED` when an objective function returns `nan`, when `nan` is passed in `trial.report`, or when `nan` is passed in `study.tell`.~\r\nRegardless of the type of storage used, mark the corresponding trial as `FAILED` when an objective function returns `nan` or when `nan` is passed in `study.tell`.\r\nOn the other hand, regardless of the type of storage used, accept the reported `nan` when `nan` is reported with `trial.report`.\r\n\r\nWe need to fix the following.\r\n- [x] ~If `nan` is passed in `trial.report`, mark the trial as `FAILED` regardless of the type of storage used.~ If `nan` is passed in `trial.report`, accept the reported `nan` regardless of the type of storage used. #3348\r\n- [x] ~Add document for workaround when passing `nan` in `trial.report`. In a trial, if the intermediate value is `nan` at the beginning of the calculation, but it reverts to the appropriate value in the middle, the user can check if it is not `nan` before `trial.report`.~\r\n- [x] If `nan` is passed in `study.tell`, mark the trial as `FAILED` regardless of the type of storage used. https://github.com/optuna/optuna/pull/3144 \r\n\r\n## Environment\r\n\r\n<!--\r\nYou can get this information by typing the following:\r\n```\r\npython -c 'import optuna; print(optuna.__version__)'\r\npython -c 'import platform; print(platform.python_version())'\r\npython -c 'import platform; print(platform.platform())'\r\n```\r\n-->\r\n\r\n- Optuna version: a70cbff8dbc454f137b61622633404bd4e4657fc\r\n- Python version: 3.9.9\r\n- OS: macOS Catalina 10.15.7\r\n- (Optional) Other libraries and their versions:\r\npsycopg2:    2.9.2\r\nPyMySQL:     1.0.2\r\n\r\n## Error messages, stack traces, or logs\r\n\r\nIf we run the script `nan-objective.py`, we will get the following output. The script is provided in the section of `Reproducible examples`. I think this output is ideal and does not need to be modified. We should mark the trial as `FAILED` for all storages.\r\n<details>\r\n<summary>output</summary>\r\n\r\n```\r\n(venv) mamu@HideakinoMacBook-puro test % python nan-objective.py \r\n[I 2021-12-24 13:36:27,998] A new study created in memory with name: no-name-769e5aec-62ec-4719-ac85-6a6fbf375bee\r\n[W 2021-12-24 13:36:28,002] Trial 0 failed, because the objective function returned nan.\r\nIn-memory : Trial state is TrialState.FAIL\r\n[I 2021-12-24 13:36:28,086] A new study created in RDB with name: no-name-8d701653-5818-4c28-9a8f-0e38d8f9019c\r\n[W 2021-12-24 13:36:28,135] Trial 0 failed, because the objective function returned nan.\r\nSQLite    : Trial state is TrialState.FAIL\r\n[I 2021-12-24 13:36:28,295] A new study created in RDB with name: no-name-4367a62b-fe90-4cd0-89e8-74c18e272f21\r\n[W 2021-12-24 13:36:28,403] Trial 0 failed, because the objective function returned nan.\r\nMySQL     : Trial state is TrialState.FAIL\r\n[I 2021-12-24 13:36:28,722] A new study created in RDB with name: no-name-8cc33f98-2f7b-41ef-802e-8d46abb6e266\r\n[W 2021-12-24 13:36:28,850] Trial 0 failed, because the objective function returned nan.\r\nPostgreSQL: Trial state is TrialState.FAIL\r\n```\r\n</details>\r\n\r\nOn the other hand, if we run the script `nan-report.py`, we will get the following output. The script is provided in the section of `Reproducible examples`. I think we need to modify this behavior as explained in the section of `Expected behavior`.  \r\n<details>\r\n<summary>output</summary>\r\n\r\n```\r\n(venv) mamu@HideakinoMacBook-puro test % python nan-report.py \r\n[I 2021-12-24 13:39:15,478] A new study created in memory with name: no-name-f34999a1-b56e-45d5-894a-a18db92956f6\r\n[I 2021-12-24 13:39:15,479] Trial 0 finished with value: 1.0 and parameters: {}. Best is trial 0 with value: 1.0.\r\nIn-memory : Trial state is TrialState.COMPLETE\r\n[I 2021-12-24 13:39:15,603] A new study created in RDB with name: no-name-aa0bdcc0-dd7c-4c19-8bf3-df7451bab2eb\r\n[I 2021-12-24 13:39:15,704] Trial 0 finished with value: 1.0 and parameters: {}. Best is trial 0 with value: 1.0.\r\nSQLite    : Trial state is TrialState.COMPLETE\r\n[I 2021-12-24 13:39:15,878] A new study created in RDB with name: no-name-96a37aca-68a4-48d2-83b5-051be816f138\r\n[W 2021-12-24 13:39:16,019] Trial 0 failed because of the following error: StorageInternalError('An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. ')\r\nTraceback (most recent call last):\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1802, in _execute_context\r\n    self.dialect.do_execute(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py\", line 732, in do_execute\r\n    cursor.execute(statement, parameters)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/cursors.py\", line 146, in execute\r\n    query = self.mogrify(query, args)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/cursors.py\", line 125, in mogrify\r\n    query = query % self._escape_args(args, conn)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/cursors.py\", line 109, in _escape_args\r\n    return {key: conn.literal(val) for (key, val) in args.items()}\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/cursors.py\", line 109, in <dictcomp>\r\n    return {key: conn.literal(val) for (key, val) in args.items()}\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/connections.py\", line 517, in literal\r\n    return self.escape(obj, self.encoders)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/connections.py\", line 510, in escape\r\n    return converters.escape_item(obj, self.charset, mapping=mapping)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/converters.py\", line 25, in escape_item\r\n    val = encoder(val, mapping)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/converters.py\", line 60, in escape_float\r\n    raise ProgrammingError(\"%s can not be used with MySQL\" % s)\r\npymysql.err.ProgrammingError: nan can not be used with MySQL\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/optuna/storages/_rdb/storage.py\", line 54, in _create_scoped_session\r\n    session.commit()\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py\", line 1431, in commit\r\n    self._transaction.commit(_to_root=self.future)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py\", line 829, in commit\r\n    self._prepare_impl()\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py\", line 808, in _prepare_impl\r\n    self.session.flush()\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py\", line 3363, in flush\r\n    self._flush(objects)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py\", line 3503, in _flush\r\n    transaction.rollback(_capture_exception=True)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py\", line 70, in __exit__\r\n    compat.raise_(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py\", line 207, in raise_\r\n    raise exception\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py\", line 3463, in _flush\r\n    flush_context.execute()\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py\", line 456, in execute\r\n    rec.execute(self)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py\", line 630, in execute\r\n    util.preloaded.orm_persistence.save_obj(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py\", line 244, in save_obj\r\n    _emit_insert_statements(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py\", line 1221, in _emit_insert_statements\r\n    result = connection._execute_20(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1614, in _execute_20\r\n    return meth(self, args_10style, kwargs_10style, execution_options)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/sql/elements.py\", line 325, in _execute_on_connection\r\n    return connection._execute_clauseelement(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1481, in _execute_clauseelement\r\n    ret = self._execute_context(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1845, in _execute_context\r\n    self._handle_dbapi_exception(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 2026, in _handle_dbapi_exception\r\n    util.raise_(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py\", line 207, in raise_\r\n    raise exception\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py\", line 1802, in _execute_context\r\n    self.dialect.do_execute(\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py\", line 732, in do_execute\r\n    cursor.execute(statement, parameters)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/cursors.py\", line 146, in execute\r\n    query = self.mogrify(query, args)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/cursors.py\", line 125, in mogrify\r\n    query = query % self._escape_args(args, conn)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/cursors.py\", line 109, in _escape_args\r\n    return {key: conn.literal(val) for (key, val) in args.items()}\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/cursors.py\", line 109, in <dictcomp>\r\n    return {key: conn.literal(val) for (key, val) in args.items()}\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/connections.py\", line 517, in literal\r\n    return self.escape(obj, self.encoders)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/connections.py\", line 510, in escape\r\n    return converters.escape_item(obj, self.charset, mapping=mapping)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/converters.py\", line 25, in escape_item\r\n    val = encoder(val, mapping)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/pymysql/converters.py\", line 60, in escape_float\r\n    raise ProgrammingError(\"%s can not be used with MySQL\" % s)\r\nsqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) nan can not be used with MySQL\r\n[SQL: INSERT INTO trial_intermediate_values (trial_id, step, intermediate_value) VALUES (%(trial_id)s, %(step)s, %(intermediate_value)s)]\r\n[parameters: {'trial_id': 7, 'step': 1, 'intermediate_value': nan}]\r\n(Background on this error at: https://sqlalche.me/e/14/f405)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 213, in _run_trial\r\n    value_or_values = func(trial)\r\n  File \"/Users/mamu/Dev/pfn/code_snippets_for_optuna/test/nan-report.py\", line 5, in objective\r\n    trial.report(float(np.nan), 1)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/optuna/trial/_trial.py\", line 506, in report\r\n    self.storage.set_trial_intermediate_value(self._trial_id, step, value)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/optuna/storages/_cached_storage.py\", line 293, in set_trial_intermediate_value\r\n    self._backend.set_trial_intermediate_value(trial_id, step, intermediate_value)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/optuna/storages/_rdb/storage.py\", line 754, in set_trial_intermediate_value\r\n    self._set_trial_intermediate_value_without_commit(\r\n  File \"/Users/mamu/.pyenv/versions/3.9.9/lib/python3.9/contextlib.py\", line 126, in __exit__\r\n    next(self.gen)\r\n  File \"/Users/mamu/Dev/pfn/venv/lib/python3.9/site-packages/optuna/storages/_rdb/storage.py\", line 71, in _create_scoped_session\r\n    raise optuna.exceptions.StorageInternalError(message) from e\r\noptuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. \r\nAn exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. \r\nMySQL     : Trial state is TrialState.FAIL\r\n[I 2021-12-24 13:39:16,341] A new study created in RDB with name: no-name-1d3ad4c8-ae5b-4da8-aafb-5139770192a3\r\n[I 2021-12-24 13:39:16,519] Trial 0 finished with value: 1.0 and parameters: {}. Best is trial 0 with value: 1.0.\r\nPostgreSQL: Trial state is TrialState.COMPLETE\r\n```\r\n</details>\r\n\r\nIn addition, if we run the script `nan-tell.py`, we will get the following output. The script is provided in the section of `Reproducible examples`. I think we need to modify this behavior as explained in the section of `Expected behavior`. \r\n<details>\r\n<summary>output</summary>\r\n\r\n```\r\n(venv) mamu@HideakinoMacBook-puro 3205-3206 % python nan-tell.py\r\n[I 2021-12-24 14:57:53,054] A new study created in memory with name: no-name-0dab10fe-1de5-4ec6-8a17-145f92fdf286\r\nTrial 0 failed, because the objective function returned nan.\r\nIn-memory : Trial state is TrialState.RUNNING\r\n[I 2021-12-24 14:57:53,138] A new study created in RDB with name: no-name-7757614d-b4bc-43ba-9489-1e691ae1ad3d\r\nTrial 0 failed, because the objective function returned nan.\r\nSQLite    : Trial state is TrialState.RUNNING\r\n[I 2021-12-24 14:57:53,308] A new study created in RDB with name: no-name-a7651b7d-0b60-47e6-96c9-2c5563819437\r\nTrial 0 failed, because the objective function returned nan.\r\nMySQL     : Trial state is TrialState.RUNNING\r\n[I 2021-12-24 14:57:53,659] A new study created in RDB with name: no-name-831368be-d69a-4aeb-a00b-76818043fac1\r\nTrial 0 failed, because the objective function returned nan.\r\nPostgreSQL: Trial state is TrialState.RUNNING\r\n```\r\n</details>\r\n\r\n## Steps to reproduce\r\n\r\n1. Install docker\r\n2. Setup MySQL DB\r\n```\r\ndocker run --name mysql -e MYSQL_ROOT_PASSWORD=test -p 3306:3306 -p 33060:33060 -d mysql:8.0.22\r\ndocker run --network host -it --rm mysql:8.0.22  mysql -h 127.0.0.1 -uroot -ptest -e \"create database mysql;\"\r\n```\r\n3. Setup PostgreSQL DB\r\n```\r\ndocker run -it --rm --name postgres-test -e POSTGRES_PASSWORD=test -p 15432:5432 -d postgres\r\n```\r\n4. Run `nan-objectibe.py`, `nan-report.py` and `nan-tell.py`.\r\n\r\n## Reproducible examples (optional)\r\n- `nan-objective.py`\r\n```python\r\nimport optuna\r\nimport numpy as np\r\n\r\ndef objective(trial):\r\n    return float(np.nan)\r\n\r\n# In-memory storage\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=1)\r\nprint(f\"In-memory : Trial state is {study.trials[-1].state}\")\r\n\r\n# RDB storage (SQLite)\r\nstudy = optuna.create_study(storage='sqlite:///../sqlite.db')\r\nstudy.optimize(objective, n_trials=1)\r\nprint(f\"SQLite    : Trial state is {study.trials[-1].state}\")\r\n\r\n# RDB storage (MySQL)\r\nstudy = optuna.create_study(storage=\"mysql+pymysql://root:test@localhost/mysql\")\r\nstudy.optimize(objective, n_trials=1)\r\nprint(f\"MySQL     : Trial state is {study.trials[-1].state}\")\r\n\r\n# RDB storage (PostgreSQL)\r\nstudy = optuna.create_study(storage=\"postgresql+psycopg2://postgres:test@localhost:15432/postgres\")\r\nstudy.optimize(objective, n_trials=1)\r\nprint(f\"PostgreSQL: Trial state is {study.trials[-1].state}\")\r\n```\r\n\r\n- `nan-report.py`\r\n```python\r\nimport optuna\r\nimport numpy as np\r\n\r\ndef objective(trial):\r\n    trial.report(float(np.nan), 1)\r\n    return 1\r\n\r\n# In-memory storage\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=1)\r\nprint(f\"In-memory : Trial state is {study.trials[-1].state}\")\r\n\r\n# RDB storage (SQLite)\r\nstudy = optuna.create_study(storage='sqlite:///../sqlite.db')\r\nstudy.optimize(objective, n_trials=1)\r\nprint(f\"SQLite    : Trial state is {study.trials[-1].state}\")\r\n\r\n# RDB storage (MySQL)\r\nstudy = optuna.create_study(storage=\"mysql+pymysql://root:test@localhost/mysql\")\r\ntry:\r\n    study.optimize(objective, n_trials=1)\r\nexcept Exception as e:\r\n    print(e)\r\nfinally:\r\n    print(f\"MySQL     : Trial state is {study.trials[-1].state}\")\r\n\r\n# RDB storage (PostgreSQL)\r\nstudy = optuna.create_study(storage=\"postgresql+psycopg2://postgres:test@localhost:15432/postgres\")\r\nstudy.optimize(objective, n_trials=1)\r\nprint(f\"PostgreSQL: Trial state is {study.trials[-1].state}\")\r\n``` \r\n\r\n- `nan-tell.py`\r\n```python\r\nimport optuna\r\nimport numpy as np\r\n\r\n\r\n# In-memory storage\r\nstudy = optuna.create_study()\r\ntrial = study.ask()\r\ntry:\r\n    study.tell(trial, float(np.nan))\r\nexcept Exception as e:\r\n    print(e)\r\nfinally:\r\n    print(f\"In-memory : Trial state is {study.trials[-1].state}\")\r\n\r\n# RDB storage (SQLite)\r\nstudy = optuna.create_study(storage='sqlite:///../sqlite.db')\r\ntrial = study.ask()\r\ntry:\r\n    study.tell(trial, float(np.nan))\r\nexcept Exception as e:\r\n    print(e)\r\nfinally:\r\n    print(f\"SQLite    : Trial state is {study.trials[-1].state}\")\r\n\r\n# RDB storage (MySQL)\r\nstudy = optuna.create_study(storage=\"mysql+pymysql://root:test@localhost/mysql\")\r\ntrial = study.ask()\r\ntry:\r\n    study.tell(trial, float(np.nan))\r\nexcept Exception as e:\r\n    print(e)\r\nfinally:\r\n    print(f\"MySQL     : Trial state is {study.trials[-1].state}\")\r\n\r\n# RDB storage (PostgreSQL)\r\nstudy = optuna.create_study(storage=\"postgresql+psycopg2://postgres:test@localhost:15432/postgres\")\r\ntrial = study.ask()\r\ntry:\r\n    study.tell(trial, float(np.nan))\r\nexcept Exception as e:\r\n    print(e)\r\nfinally:\r\n    print(f\"PostgreSQL: Trial state is {study.trials[-1].state}\")\r\n\r\n```\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->\r\n",
    "comments": [
      {
        "user": "keisuke-umezawa",
        "body": "@xuzijian629 will work on it.\r\n"
      },
      {
        "user": "xuzijian629",
        "body": "memo: To run the scripts, you need to \r\n\r\n```\r\npip install pymysql\r\npip install psycopg2-binary\r\n```\r\n\r\nin addition to `pip install -e .`"
      },
      {
        "user": "HideakiImamura",
        "body": "note: We have privately discussed how to deal with `nan` with `trial.report`. As a result, we have changed the direction. We do not raise any error if a `nan` is reported but ignore the reported `nan` in `trial.report`. The issue description is fixed."
      }
    ]
  },
  {
    "issue_number": 6072,
    "title": "`Optuna` fails when storing DB",
    "author": "ek-ex",
    "state": "open",
    "created_at": "2025-05-06T03:38:16Z",
    "updated_at": "2025-05-09T19:23:14Z",
    "labels": [
      "bug"
    ],
    "body": "I'm new to Optuna and im building a project where I need to manage the loop independently as a requirement.\nThe following is the code: \n```\ndef evaluate_multi(x):\n    obj1 = (x - 1) ** 2\n    obj2 = (x + 1) ** 2\n    return obj1, obj2\n\nstorage_url = \"sqlite:////media/2nd/optimus/optuna.db\"\n\nstudy = optuna.create_study(directions=[\"minimize\", \"minimize\"], storage=storage_url)\n\nn_trials = 10000\n\nfor i in range(n_trials):\n    trial = study.ask()\n    x = trial.suggest_float(\"x\", -10, 10)\n    obj1, obj2 = evaluate_multi(x)\n    study.tell(trial, (obj1, obj2))\n    print(\n        f\"Trial {trial.number:02d}: x={x:.3f} → \"\n        f\"obj1={obj1:.3f}, obj2={obj2:.3f}\"\n    )\n\nprint(\"\\nPareto front trials:\")\nfor t in study.best_trials:\n    print(\n        f\"  Trial {t.number}: \"\n        f\"values={t.values}, params={t.params}\"\n    )\n```\n\nHowever the code fails eactly at the 51th trial with the following error. \n\n```\nTraceback (most recent call last):\n  File \"/home/repo/optimus/opt_multi.py\", line 25, in <module>\n    x = trial.suggest_float(\"x\", -10, 10)\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 161, in suggest_float\n    suggested_value = self._suggest(name, distribution)\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 627, in _suggest\n    elif self._is_relative_param(name, distribution):\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 659, in _is_relative_param\n    if name not in self.relative_params:\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 71, in relative_params\n    self._relative_params = self.study.sampler.sample_relative(\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/samplers/nsgaii/_sampler.py\", line 269, in sample_relative\n    parent_population = self.get_parent_population(study, generation)\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/samplers/_ga/_base.py\", line 174, in get_parent_population\n    return [trials[trial_id] for trial_id in cached_parent_population_ids]\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/samplers/_ga/_base.py\", line 174, in <listcomp>\n    return [trials[trial_id] for trial_id in cached_parent_population_ids]\nIndexError: list index out of range\n```\n\nHowever, the code will run perfectly file when im not using a storage\n```\nstudy = optuna.create_study(directions=[\"minimize\", \"minimize\"])\n...\n...\nTrial 9996: x=4.238 → obj1=10.486, obj2=27.439\nTrial 9997: x=1.167 → obj1=0.028, obj2=4.695\nTrial 9998: x=-8.833 → obj1=96.697, obj2=61.363\nTrial 9999: x=-7.126 → obj1=66.030, obj2=37.526\n\n```\n\nIs there any reason why this fails when running this code and saving to a db? \n\n### Environment\n\n- Optuna version:4.3.0\n- Python version:3.10.12\n- OS:Linux-6.8.0-59-generic-x86_64-with-glibc2.35\n\n### Error messages, stack traces, or logs\n\n```shell\nTraceback (most recent call last):\n  File \"/home/repo/optimus/opt_multi.py\", line 25, in <module>\n    x = trial.suggest_float(\"x\", -10, 10)\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 161, in suggest_float\n    suggested_value = self._suggest(name, distribution)\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 627, in _suggest\n    elif self._is_relative_param(name, distribution):\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 659, in _is_relative_param\n    if name not in self.relative_params:\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 71, in relative_params\n    self._relative_params = self.study.sampler.sample_relative(\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/samplers/nsgaii/_sampler.py\", line 269, in sample_relative\n    parent_population = self.get_parent_population(study, generation)\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/samplers/_ga/_base.py\", line 174, in get_parent_population\n    return [trials[trial_id] for trial_id in cached_parent_population_ids]\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/samplers/_ga/_base.py\", line 174, in <listcomp>\n    return [trials[trial_id] for trial_id in cached_parent_population_ids]\nIndexError: list index out of range\n```\n",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "I think this issue relates to this PR:\n- https://github.com/optuna/optuna/pull/6066"
      },
      {
        "user": "leevers",
        "body": "There is a bug in the NSGAII base class that causes this exception. If you start a new .db for every study you *shouldn't* get the error or you can try to use the following sampler that overrides the failing function.\n\n```python\n\nclass CustomNSGAIISampler(NSGAIISampler):\n    def get_parent_population(self, study, generation):\n        \"\"\"Get the parent population of the given generation.\n\n        This method caches the parent population in the study's system attributes.\n\n        Args:\n            study:\n                Target study object.\n            generation:\n                Target generation number.\n\n        Returns:\n            List of parent frozen trials. If `generation == 0`, returns an empty list.\n        \"\"\"\n        if generation == 0:\n            return []\n\n        study_system_attrs = study._storage.get_study_system_attrs(study._study_id)\n        cached_parent_population_ids = study_system_attrs.get(\n            'NSGAIISampler:generation' + str(generation), None\n        )\n\n        if cached_parent_population_ids is not None:\n            trials = study._get_trials(deepcopy=False)\n            trials_by_id = {}\n            for t in trials:\n                trials_by_id[t._trial_id] = t\n            return [trials_by_id[trial_id] for trial_id in cached_parent_population_ids]\n        else:\n            parent_population = self.select_parent(study, generation)\n            study._storage.set_study_system_attr(\n                study._study_id,\n                'NSGAIISampler:generation' + str(generation),\n                [trial._trial_id for trial in parent_population],\n            )\n            return parent_population\n```"
      },
      {
        "user": "ek-ex",
        "body": "@leevers thanks for the code. However, It only works when not having to read the study from the DB on each iteration. In my case, I have to do something like this:\n\n```\nstudy = optuna.create_study(directions=[\"minimize\", \"minimize\"],\n                            storage=storage_url,\n                            sampler=CustomNSGAIISampler())\n\nname = study.study_name\n\nfor i in range(10000):\n    # Simulates exposing Optuna in a Rest API using FastAPI. Keeping the Rest service stateless, each request reads the study from the DB.\n    study = optuna.load_study(study_name=name, storage=storage_url)\n\n    trial = study.ask()\n    x = trial.suggest_float(\"x\", -10, 10)\n...\n```\n\nWhich still returns this error:\n\n```\nTrial 48: x=-0.926 → obj1=3.710, obj2=0.005\nTrial 49: x=1.044 → obj1=0.002, obj2=4.177\nTrial 50: x=-4.929 → obj1=35.152, obj2=15.436\nTraceback (most recent call last):\n  File \"/home/repo/optimus/opt_multi.py\", line 30, in <module>\n    x = trial.suggest_float(\"x\", -10, 10)\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 161, in suggest_float\n    suggested_value = self._suggest(name, distribution)\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 627, in _suggest\n    elif self._is_relative_param(name, distribution):\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 659, in _is_relative_param\n    if name not in self.relative_params:\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 71, in relative_params\n    self._relative_params = self.study.sampler.sample_relative(\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/samplers/nsgaii/_sampler.py\", line 269, in sample_relative\n    parent_population = self.get_parent_population(study, generation)\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/samplers/_ga/_base.py\", line 174, in get_parent_population\n    return [trials[trial_id] for trial_id in cached_parent_population_ids]\n  File \"/home/repo/optimus/.venv/lib/python3.10/site-packages/optuna/samplers/_ga/_base.py\", line 174, in <listcomp>\n    return [trials[trial_id] for trial_id in cached_parent_population_ids]\nIndexError: list index out of range\n\n```\nSo I think this issues can be related but not the same as https://github.com/optuna/optuna/pull/6066 "
      }
    ]
  },
  {
    "issue_number": 5470,
    "title": "With very short heartbeat, not only trials may fail but also the whole process crushes",
    "author": "eukaryo",
    "state": "closed",
    "created_at": "2024-05-31T08:15:43Z",
    "updated_at": "2025-05-07T23:06:41Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\r\n\r\nIf the `heartbeat_interval` is too short, updating heartbeat may miss the deadline even if the process is alive. In that case, it is acceptable for the corresponding `trial` that was being calculated to `FAILED`, but the process should not crash.\r\n\r\n### Environment\r\n\r\n- Optuna version: v3.6.1\r\n- Python version: 3.11\r\n- OS: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n  - Ubuntu 20.04 on WSL2\r\n- (Optional) Other libraries and their versions: mysql  Ver 8.0.36-0ubuntu0.20.04.1 for Linux on x86_64 ((Ubuntu))\r\n- CPU: AMD Ryzen 5 5625U\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/mnt/c/heartbeat-rdb/tunarepo/optuna/optuna/study/_optimize.py\", line 208, in _run_trial\r\n    frozen_trial = _tell_with_warning(\r\n                   ^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/c/heartbeat-rdb/tunarepo/optuna/optuna/study/_tell.py\", line 122, in _tell_with_warning\r\n    raise ValueError(f\"Cannot tell a {frozen_trial.state.name} trial.\")\r\nValueError: Cannot tell a FAIL trial.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/c/heartbeat-rdb/tunarepo/optuna/../../example.py\", line 18, in <module>\r\n    study.optimize(objective, n_trials=100000, n_jobs=100)\r\n  File \"/mnt/c/heartbeat-rdb/tunarepo/optuna/optuna/study/study.py\", line 475, in optimize\r\n    _optimize(\r\n  File \"/mnt/c/heartbeat-rdb/tunarepo/optuna/optuna/study/_optimize.py\", line 99, in _optimize\r\n    f.result()\r\n  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\r\n    return self.__get_result()\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\r\n    raise self._exception\r\n  File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/c/heartbeat-rdb/tunarepo/optuna/optuna/study/_optimize.py\", line 159, in _optimize_sequential\r\n    frozen_trial = _run_trial(study, func, catch)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/c/heartbeat-rdb/tunarepo/optuna/optuna/study/_optimize.py\", line 238, in _run_trial\r\n    assert False, \"Should not reach.\"\r\n           ^^^^^\r\nAssertionError: Should not reach.\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\n```python\r\nimport optuna\r\n\r\n\r\ndef objective(trial):\r\n    x0 = trial.suggest_float(\"x0\", -500.0, 500.0)\r\n    x1 = trial.suggest_float(\"x1\", -500.0, 500.0)\r\n    return x0**2 + x1**2\r\n\r\n\r\nsampler = optuna.samplers.RandomSampler()\r\nstorage = optuna.storages.RDBStorage(\r\n    url=\"mysql+pymysql://optuna:your_password@localhost/optuna\", heartbeat_interval=2.0\r\n)\r\nstudy = optuna.create_study(\r\n    study_name=\"example_study\", storage=storage, sampler=sampler\r\n)\r\nstudy.optimize(objective, n_trials=1000000, n_jobs=100)\r\n```\r\n\r\n\r\n### Additional context (optional)\r\n\r\nThis issue may be related to https://github.com/optuna/optuna/issues/5280 .",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 6077,
    "title": "Describe how to use Optuna with Spark using ask-and-tell functionality",
    "author": "fritshermans",
    "state": "open",
    "created_at": "2025-05-07T17:50:50Z",
    "updated_at": "2025-05-07T18:51:13Z",
    "labels": [
      "document"
    ],
    "body": "### What is an issue?\n\nMultiple issues were raised over the last years of people who asked for Spark support. However, it's pretty simple to use Optuna in combination with Spark using the ask-and-tell functionality. I described it in a [blog post](https://fritshermans.github.io/posts/optuna_spark.html). I think it would be useful to add a small example to the documentation of [ask-and-tell](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html).\n\nPlease let me know what you think of it. I would be happy to create a PR.",
    "comments": []
  },
  {
    "issue_number": 2843,
    "title": "Add Spark (PySpark) support for distributed optimization",
    "author": "candalfigomoro",
    "state": "closed",
    "created_at": "2021-08-04T10:02:05Z",
    "updated_at": "2025-05-07T17:44:32Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "I propose to add support for Spark (PySpark) for massively parallel hyperparameter optimization.\r\n\r\n## Motivation\r\n\r\nWhen a Spark cluster is available, it can be used for massive parallelization of the hyperparameters search in order to speedup hyperparameter optimization.\r\n\r\nThere had been some proposals to use joblibspark, but Optuna dropped joblib (see https://github.com/optuna/optuna/issues/1381#issuecomment-772365885).\r\n\r\n## Description\r\n\r\nIt should be possible to distribute trials to different Spark worker nodes.\r\n\r\n## Alternatives (optional)\r\n\r\n- the hyperopt package provides a SparkTrials class for Spark support (see http://hyperopt.github.io/hyperopt/scaleout/spark/).\r\n- a simple random search without Optuna\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "candalfigomoro",
        "body": "@HideakiImamura \r\nAny idea how it might be designed?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5558,
    "title": "Speed up MOTPE with cache",
    "author": "nabenabe0928",
    "state": "open",
    "created_at": "2024-07-10T01:09:52Z",
    "updated_at": "2025-05-06T23:51:00Z",
    "labels": [
      "feature",
      "needs-discussion"
    ],
    "body": "### Motivation\r\n\r\nAs MOTPE can be speeded up using cache and MOTPE is still not super fast, I would like to speed up MOTPE.\r\n\r\n### Description\r\n\r\nThere are already two PRs in the past, but they were closed because of design problems:\r\n- https://github.com/optuna/optuna/pull/5454\r\n- https://github.com/optuna/optuna/pull/5464\r\n\r\nI would like to address these problems in the future if our team comes to an agreement.\r\n\r\nFor [cache across dimensions](https://github.com/optuna/optuna/pull/5464), we can also speed up by changing the default value for `multivariate` as `multivariate=True` calls HSSP only once.\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 4682,
    "title": "Standardize citation format and linking rules for papers in the document",
    "author": "Alnusjaponica",
    "state": "closed",
    "created_at": "2023-05-17T08:03:31Z",
    "updated_at": "2025-05-06T23:06:16Z",
    "labels": [
      "document",
      "contribution-welcome",
      "stale"
    ],
    "body": "### Motivation\r\nIt would be better to  establish a set of guidelines to ensure consistent citation format and linking for the papers referenced within our document. This will improve readability, provide proper credit to the original authors, and facilitate easy access to the cited works. The purpose of this issue is to discuss and define the rules that will be followed throughout the document.\r\n\r\n### Suggestion\r\n- Citation Format for Reference List and In-text Citations:\r\n  - There are several citation format traditionally used in each field, such as [AMS Style](https://www.ams.org/publications/authors/AMS-StyleGuide-online.pdf), [CSE Style](https://dal.ca.libguides.com/CitationStyleGuide/CSE), [IEEE Style](https://ieeeauthorcenter.ieee.org/wp-content/uploads/IEEE-Reference-Guide.pdf), and so on. Any of them will be fine as long as it is consistent.\r\n- Docstrings Style:\r\n  - https://pypi.org/project/sphinxcontrib-bibtex/\r\n  - Use reference section as in https://www.sphinx-doc.org/ja/master/usage/extensions/napoleon.html\r\n- Linking to Papers:\r\n  - Use [DOI](https://www.doi.org/)s.\r\n\r\n### Tasks\r\n- [ ] Discuss and finalize the rules for citation and linking format.\r\n- [ ] List up all references to papers in the document\r\n- [ ] Update the document with the standardized citation format and links for all referenced papers.\r\n\r\nFeel free to add any suggestions, concerns, or examples related to citation format and linking that should be considered during the discussion.",
    "comments": [
      {
        "user": "Alnusjaponica",
        "body": "Following links are a list of (implicit or explicit) citations in the documentation.\r\n- Optuna\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/README.md?plain=1#L165-L166\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/docs/source/index.rst?plain=1#L105-L107\r\n- benchmarks\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/benchmarks/bayesmark/report_bayesmark.py#L151\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/benchmarks/bayesmark/report_bayesmark.py#L246\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/benchmarks/bayesmark/report_template.md?plain=1#L11\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/_hypervolume/hssp.py#L21-L22\r\n- Fanova\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/importance/_fanova/_evaluator.py#L25-L27\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/importance/_fanova/_fanova.py#L3-L4\r\n- Hyperband\r\n    - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_hyperband.py#L25\r\n    - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_hyperband.py#L62\r\n    - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_hyperband.py#L191\r\n    - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_hyperband.py#L223\r\n    - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_hyperband.py#L239\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_hyperband.py#L196\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_successive_halving.py#L15\r\n- Successive Halving\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_successive_halving.py#L18\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_successive_halving.py#L73\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_successive_halving.py#L99\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/pruners/_successive_halving.py#L105\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_nsgaiii.py#L49-L51\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_nsgaiii.py#L52-L55\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_nsgaiii.py#L424\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_qmc.py#L37-L39\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_cmaes.py#L90-L91\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_cmaes.py#L92-L95\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_cmaes.py#L96-L99\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_cmaes.py#L100-L102\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_cmaes.py#L103-L105\r\n- TPE\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/sampler.py#L71-L72\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/sampler.py#L131\r\n- TPE\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/sampler.py#L73-L74\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/sampler.py#L124-L125\r\n- Multiobjective TPE\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/sampler.py#L75-L76\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/sampler.py#L134\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/multi_objective/samplers/_motpe.py#L33-L34\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/multi_objective_sampler.py#L29-L30\r\n- Multiobjective TPE\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/sampler.py#L77\r\n  -  https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/multi_objective/samplers/_motpe.py#L35\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/multi_objective_sampler.py#L31\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/_tpe/sampler.py#L141\r\n- NSGAII\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/multi_objective/samplers/_nsga2.py#L37-L38\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_sampler.py#L48-L49\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_blxalpha.py#L16-L18\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_blxalpha.py#L37-L38\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_sbx.py#L17-L20\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_sbx.py#L41-L42\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_sbx.py#L17-L20\r\n- SPXCrossover\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_spx.py#L18-L22\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_spx.py#L41\r\n- UNDXCrossover\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_undx.py#L17-L22\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_undx.py#L72\r\n- UniformCrossover\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_uniform.py#L13-L16\r\n  - https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_uniform.py#L35-L36\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_vsbx.py#L18-L21\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/samplers/nsgaii/_crossovers/_vsbx.py#L42-L43\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/terminator/terminator.py#L31\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/optuna/visualization/_edf.py#L52\r\n- https://github.com/optuna/optuna/blob/806448420863606c113aeb2e33457acf022be066/tutorial/10_key_features/003_efficient_optimization_algorithms.py#L154"
      },
      {
        "user": "Alnusjaponica",
        "body": "It seems [sphinxcontrib-bibtex](https://pypi.org/project/sphinxcontrib-bibtex/) would be good solution."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 6074,
    "title": "BrokenPipeError when combining Optuna with SubprocVecEnv (multiprocessing)",
    "author": "seb0908",
    "state": "closed",
    "created_at": "2025-05-06T17:56:03Z",
    "updated_at": "2025-05-06T18:05:44Z",
    "labels": [],
    "body": "### Don't use GitHub Issues to ask support questions.\n\nHi all,\n\nI am trying to train a reinforcement learning agent using the BOPTEST library, which provides a framework for testing Building Energy Management Systems (BEMS). I want to combine multiprocessing (via SubprocVecEnv) with hyperparameter optimization using Optuna. The setup runs in a Docker container and communicates with BOPTEST through a RESTful API.\n\nThe problem: when I try to use SubprocVecEnv together with Optuna, my program crashes with a BrokenPipeError (see below).\n\n- SubprocVecEnv without Optuna works fine.\n- Optuna with DummyVecEnv also works.\n- But combining both (SubprocVecEnv + Optuna) fails.\n\nTraining a BEMS agent is costly making acceleration necessary. Therefore, I want to use multiprocessing. Could someone help me how to change the code to get this working?\n\nHere my code: \n```import sys\nimport os\nimport numpy as np\nimport multiprocessing as mp\nimport optuna as op\nfrom optuna.pruners import MedianPrunerOptunaEvalCallback\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor\nfrom boptestGymEnv import BoptestGymEnv, NormalizedObservationWrapper, DiscretizedActionWrapper, \nfrom examples import run_baseline, run_sample, run_save_callback,\\\n    run_variable_episode, run_vectorized, run_multiaction, train_RL\n\nclass OptunaOptimization():\n\n  def __init__(self, url, n_envs, n_trials, max_timesteps):\n    self.url = url\n    self.n_envs = n_envs\n    self.n_trials = n_trials\n    self.max_timesteps = max_timesteps\n\n  def run_training(self):\n    study = op.create_study(direction=\"maximize\", pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2))\n    study.optimize(self.objective, n_trials = self.n_trials)\n\n  def objective(self,trial):\n    log_dir = os.path.join(\"./logs\", f\"trial_{trial.number}\")\n    os.makedirs(log_dir, exist_ok=True) \n    learning_rate = trial.suggest_float('learning_rate',1e-5,1e-2, log=True)\n    gamma = trial.suggest_float('gamma',0.9,0.999)\n    ent_coef = trial.suggest_float('ent_coef', 0.1, 1.0)\n    tau = trial.suggest_float('tau', 0.001, 0.1)\n    buffer_size = trial.suggest_int('buffer_size', 1e5, 1e7, log=True)\n    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512, 1024]) \n    return self.train_agent(trial, n_envs=self.n_envs, max_timesteps=self.max_timesteps, learning_rate=learning_rate, \n                            gamma=gamma, ent_coef=ent_coef, tau=tau, buffer_size=buffer_size, batch_size=batch_size)\n  \n  def train_agent(self, trial, n_envs=2, max_timesteps=int(1e+5), n_splits=20, verbose=1, \n                  learning_rate=0.1, gamma=0.99, ent_coef = 0.1, tau = 0.1, buffer_size=int(1e+6), \n                  batch_size=128):\n    \n    if self.n_envs == 1:\n      env_train = VecMonitor(venv=DummyVecEnv([run_vectorized.make_env(seed=1)]))\n    else:\n      env_train = VecMonitor(venv=SubprocVecEnv([run_vectorized.make_env(seed=1+i) for i in range(n_envs)]))\n\n    env_evaluate = DummyVecEnv([lambda: run_vectorized.make_env(seed=1)()])  \n        \n    eval_callback = OptunaEvalCallback(env_evaluate, trial=trial, n_eval_episodes=2, eval_freq=self.max_timesteps/n_splits, \n                                       log_path=None, best_model_save_path=None, deterministic=True, render=False)  \n\n    model = SAC('MlpPolicy', env_train, verbose=verbose, learning_rate=learning_rate, gamma=gamma, \n                ent_coef = ent_coef, tau = tau, buffer_size=buffer_size, batch_size=batch_size, \n                learning_starts=24, train_freq=1, tensorboard_log=\"./logs/\", device='cpu')\n\n    model.learn(total_timesteps=max_timesteps, callback=eval_callback, tb_log_name=\"SAC_Trial{}\".format(trial.number), progress_bar=True)\n    env_train.close()\n    env_evaluate.close()\n    return eval_callback.last_eval_mean_reward\n\nif __name__ == \"__main__\":\n  mp.set_start_method(\"spawn\", force=True)\n  opt = OptunaOptimization(url='http://127.0.0.1', n_envs=2, n_trials=20, max_timesteps=int(1e+4))\n  opt.run_training()\n```\n\nHere the error traceback:\n```Process SpawnProcess-1:\nTraceback (most recent call last):\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py\", line 314, in _bootstrap\n    self.run()\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\subproc_vec_env.py\", line 46, in _worker\n    observation, reset_info = env.reset(seed=data[0], **maybe_options)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py\", line 553, in reset\n    obs, info = self.env.reset(seed=seed, options=options)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\Documents\\Studium\\Masterarbeit\\Code\\project1-boptest-gym\\boptestGymEnv.py\", line 490, in reset\n    res = requests.put('{0}/initialize/{1}'.format(self.url,self.testid),\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyError: 'payload'\n[W 2025-05-06 19:55:02,720] Trial 0 failed with parameters: {'learning_rate': 0.0004144161427253373, 'gamma': 0.9823286814016307, 'ent_coef': 0.8818468464865998, 'tau': 0.01163540012881728, 'buffer_size': 254671, 'batch_size': 32} because of the following error: EOFError().\nTraceback (most recent call last):\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py\", line 328, in _recv_bytes\n    nread, err = ov.GetOverlappedResult(True)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nBrokenPipeError: [WinError 109] Die Pipe wurde beendet\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"c:\\Users\\sebas\\Documents\\Studium\\Masterarbeit\\Code\\project1-boptest-gym\\testcases\\testcase1_SAC.py\", line 44, in objective\n    return self.train_agent(trial, n_envs=self.n_envs, max_timesteps=self.max_timesteps, learning_rate=learning_rate,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\sebas\\Documents\\Studium\\Masterarbeit\\Code\\project1-boptest-gym\\testcases\\testcase1_SAC.py\", line 65, in train_agent\n    model.learn(total_timesteps=max_timesteps, callback=eval_callback, tb_log_name=\"SAC_Trial{}\".format(trial.number), progress_bar=True)\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py\", line 308, in learn\n    return super().learn(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 314, in learn\n    total_timesteps, callback = self._setup_learn(\n                                ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 297, in _setup_learn\n    return super()._setup_learn(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py\", line 424, in _setup_learn\n    self._last_obs = self.env.reset()  # type: ignore[assignment]\n                     ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py\", line 70, in reset\n    obs = self.venv.reset()\n          ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\subproc_vec_env.py\", line 145, in reset\n    results = [remote.recv() for remote in self.remotes]\n               ^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n          ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py\", line 337, in _recv_bytes\n    raise EOFError\nEOFError\n[W 2025-05-06 19:55:02,726] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py\", line 328, in _recv_bytes\n    nread, err = ov.GetOverlappedResult(True)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nBrokenPipeError: [WinError 109] Die Pipe wurde beendet\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\sebas\\Documents\\Studium\\Masterarbeit\\Code\\project1-boptest-gym\\testcases\\testcase1_SAC.py\", line 73, in <module>\n    opt.run_training()\n  File \"c:\\Users\\sebas\\Documents\\Studium\\Masterarbeit\\Code\\project1-boptest-gym\\testcases\\testcase1_SAC.py\", line 33, in run_training\n    study.optimize(self.objective, n_trials = self.n_trials)\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py\", line 475, in optimize\n    _optimize(\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"c:\\Users\\sebas\\Documents\\Studium\\Masterarbeit\\Code\\project1-boptest-gym\\testcases\\testcase1_SAC.py\", line 44, in objective\n    return self.train_agent(trial, n_envs=self.n_envs, max_timesteps=self.max_timesteps, learning_rate=learning_rate,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\sebas\\Documents\\Studium\\Masterarbeit\\Code\\project1-boptest-gym\\testcases\\testcase1_SAC.py\", line 65, in train_agent\n    model.learn(total_timesteps=max_timesteps, callback=eval_callback, tb_log_name=\"SAC_Trial{}\".format(trial.number), progress_bar=True)\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py\", line 308, in learn\n    return super().learn(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 314, in learn\n    total_timesteps, callback = self._setup_learn(\n                                ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 297, in _setup_learn\n    return super()._setup_learn(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py\", line 424, in _setup_learn\n    self._last_obs = self.env.reset()  # type: ignore[assignment]\n                     ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py\", line 70, in reset\n    obs = self.venv.reset()\n          ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\subproc_vec_env.py\", line 145, in reset\n    results = [remote.recv() for remote in self.remotes]\n               ^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n          ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py\", line 337, in _recv_bytes\n    raise EOFError\nEOFError\n```\n\n",
    "comments": []
  },
  {
    "issue_number": 6065,
    "title": "Record has changed since last read in table `study_system_attributes` [mariadb]",
    "author": "jvrodrisan",
    "state": "open",
    "created_at": "2025-04-27T08:18:39Z",
    "updated_at": "2025-04-28T10:10:48Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nWhen running an optimization using NSGAII and VSBXCrossover, I expect it to run without database errors. The NSGAII initialization is as follows:\n\n```python\nsampler = optuna.samplers.NSGAIISampler(\n                population_size=150,\n                mutation_prob=0.08, \n                crossover_prob=0.85,\n                swapping_prob=0.5,\n                seed=_seed,\n                crossover=optuna.samplers.nsgaii.VSBXCrossover(eta=15),\n                constraints_func=self.constraints_opti if self.constraints else None\n)\n```\nand database configuration below:\n```python\nstorage = optuna.storages.RDBStorage(url=self.db_url, heartbeat_interval=80, grace_period=160,\n                   failed_trial_callback=RetryFailedTrialCallback(max_retry=5))\n```\n\n\n### Environment\n\n- Optuna version: 4.2.1\n- Python version: 3.13.3\n- OS: Linux-6.12.21-amd64-x86_64-with-glibc2.41 (Debian 12)\n\n### Error messages, stack traces, or logs\n\n```shell\nile \"/home/jyck/.pyenv/versions/miniforge3-latest/envs/py313/lib/python3.13/site-packages/pymysql/connections.py\", line 1199, in read\n    first_packet = self.connection._read_packet()\n  File \"/home/jyck/.pyenv/versions/miniforge3-latest/envs/py313/lib/python3.13/site-packages/pymysql/connections.py\", line 775, in _read_packet\n    packet.raise_for_error()\n    ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/jyck/.pyenv/versions/miniforge3-latest/envs/py313/lib/python3.13/site-packages/pymysql/protocol.py\", line 219, in raise_for_error\n    err.raise_mysql_exception(self._data)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n  File \"/home/jyck/.pyenv/versions/miniforge3-latest/envs/py313/lib/python3.13/site-packages/pymysql/err.py\", line 150, in raise_mysql_exception\n    raise errorclass(errno, errval)\nsqlalchemy.exc.OperationalError: (pymysql.err.OperationalError) (1020, \"Record has changed since last read in table 'study_system_attributes'\")\n[SQL: UPDATE study_system_attributes SET value_json=%(value_json)s WHERE study_system_attributes.study_system_attribute_id = %(study_system_attributes_study_system_attribute_id)s]\n[parameters: {'value_json': '[5, [837, 877, 888, 943, 710, 871, 880, 908, 936, 937, 953, 357, 346, 853, 856, 869, 890, 934, 720, 802, 538, 652, 815, 513, 673, 688, 826, 20, 833,  ... (439 characters truncated) ... 64, 462, 460, 454, 453, 450, 448, 444, 443, 439, 435, 433, 431, 430, 429, 428, 424, 423, 421, 418, 416, 413, 408, 407, 404, 403, 402, 398, 395, 392]]', 'study_system_attributes_study_system_attribute_id': 47167}]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)\n```\n\n### Steps to reproduce\n\n1. Just run my optimization rutine on 20 different terminals locally, and after some trials (not spsecifics steps to reproduce), some of the executions stopped with the error message provided.\n\n\n\n### Additional context\n\nI have tried to use the GRPC proxy, but the problem happens similarly, it is even more common under it. It seems that there is an update to the DB at the same record on multiple threads and one of them fails. Tried different configurations for retry errors and heartbeat/grace periods but the issue remains. \n\nI am using mariadb locally, version: `mariadb from 11.8.1-MariaDB, client 15.2 for debian-linux-gnu (x86_64) using  EditLine wrapper`\n\nI have also tried Optuna 4.3.0 but it raises another error with NSGAII which will raise once this is solved to not have crossed issues.",
    "comments": []
  },
  {
    "issue_number": 6062,
    "title": "Fix optuna.create_study function.",
    "author": "jeong2624",
    "state": "closed",
    "created_at": "2025-04-23T06:24:12Z",
    "updated_at": "2025-04-27T03:14:22Z",
    "labels": [],
    "body": "### Expected behavior\n\nDear Development Team,\n\nI used the Optuna Python package to obtain hyperparameter values for machine learning algorithms.\n\nAs I understand, there is an `n_jobs` option in the `optuna.optimize()` function, which allows parallel processing. I set `n_jobs` to a value greater than 1, and I expected the results to be the same due to setting the same seed. However, they were not.\n\nTherefore, I would appreciate it if you could revise the code accordingly to ensure consistent results when using `n_jobs > 1` with the same seed.\nIf this is not possible due to internal constraints, please provide an example of how to write the code properly to achieve reproducible results when performing parallel optimization with Optuna.\n\n### Environment\n\n- Optuna version: 4.3.0\n- Python version: 3.11.12\n- OS: Linux-6.1.123+-x86_64-with-glibc2.35 (Google colab)\n\n\n### Error messages, stack traces, or logs\n\n```shell\n1st result : \nBest Score (RMSE): 5.3783\nBest Parameters: {'n_estimators': 132, 'max_depth': 16, 'min_samples_split': 2, 'min_samples_leaf': 7}\n\nRMSE_train: 3.212106\nRMSE_test : 5.993280\ncoef_train: 0.932680\ncoef_test : 0.239764\n\n2nd result :\nBest Score (RMSE): 5.3783\nBest Parameters: {'n_estimators': 112, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 7}\n\nRMSE_train: 3.212106\nRMSE_test : 6.213546\ncoef_train: 0.902134\ncoef_test : 0.339764\n```\n\n### Steps to reproduce\n\n# Optimization function using Optuna package\ndef objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 32),\n        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n        \"random_state\": SEED\n    }\n\n    regressor = RandomForestRegressor(**params)\n\n    kf = KFold(n_splits=5, shuffle = True, random_state = SEED)\n    scores = []\n\n    for train_idx, val_idx in kf.split(X_train):\n        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n\n        regressor.fit(X_train_fold, y_train_fold)\n        y_pred = regressor.predict(X_val_fold)\n        rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n        scores.append(rmse)\n\n    return np.mean(scores)\n\nstudy = optuna.create_study(direction = 'minimize', sampler = TPESampler(seed = SEED), study_name = \"Random Forest\")\nstudy.optimize(objective, n_trials = 20, show_progress_bar = True, n_jobs = 10)\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2572,
    "title": "Large number of binary hyperparameters",
    "author": "FelixNeutatz",
    "state": "closed",
    "created_at": "2021-04-07T13:48:48Z",
    "updated_at": "2025-04-26T09:02:23Z",
    "labels": [
      "question",
      "stale"
    ],
    "body": "Dear all,\r\n\r\nI am currently experimenting with a large number of binary hyperparameters (>1k/10k). Unfortunately, Optuna is getting slow when I use the TPESampler and I do not want to use random sampling. Do you have any tip for me how to apply Bayesian optimization here?\r\n\r\nThank you for your help.\r\n\r\nBest regards,\r\nFelix\r\n",
    "comments": [
      {
        "user": "fsmosca",
        "body": "What is binary hyperparameters?\r\n\r\nWhat is the slow down from using 10 param to 1k param?\r\n\r\nIn TPE you can use multivariate=True if you want the sampling to consider dependencies among parameters. By using TPE sampler you are already using bayesian optimization. Do you mean other optuna samplers?\r\n"
      },
      {
        "user": "FelixNeutatz",
        "body": "Hi @fsmosca,\r\n\r\nthank you for your quick response. The multivariate idea was a good tip. \r\n\r\nHere, is a toy example of what I mean:\r\n\r\n```\r\nimport numpy as np\r\nimport optuna\r\nfrom optuna.samplers import TPESampler\r\nimport random\r\nimport time\r\n\r\nnumber_parameters = []\r\nall_time = []\r\n\r\nfor power_i in range(5):\r\n    number_of_bin_parameters = np.power(10, power_i)\r\n\r\n    def my_loss(trial):\r\n        x = []\r\n        for i in range(number_of_bin_parameters):\r\n            x.append(trial.suggest_categorical('x' + str(i), [True, False]))\r\n        return random.random()\r\n\r\n    start = time.time()\r\n\r\n    study = optuna.create_study(direction='minimize', sampler=TPESampler())\r\n    study.optimize(my_loss, n_trials=100)\r\n\r\n    all_time.append(time.time() - start)\r\n    number_parameters.append(number_of_bin_parameters)\r\n\r\nprint(number_parameters)\r\nprint(all_time)\r\n```\r\n\r\nThis yields the following numbers:\r\n![image](https://user-images.githubusercontent.com/5217389/114007503-b1f7f880-9861-11eb-8444-1cc134c1e934.png)\r\n\r\nThere is a significant increase in the runtime with an increasing number of parameters. Even if I run it with the RandomSampler instead, I see the same behavior:\r\n![image](https://user-images.githubusercontent.com/5217389/114010881-e15c3480-9864-11eb-9f38-8972b4d865d2.png)\r\n\r\nIs there any way to make this faster and more scalable with respect to the number of parameters?\r\n\r\nBest regards,\r\nFelix\r\n\r\n"
      },
      {
        "user": "fsmosca",
        "body": "Here is a comparison with different sampler/optimizers.\r\n\r\nNotes:\r\n1. cmaes when optimizing at 2k params almost used all my 4-core/8-threads processing power. This needs to be reported as it should not use my cpu at 90 to 95% without prior knowledge.\r\n2. bayes_opt is from https://github.com/fmfn/BayesianOptimization, this is not in optuna. I used it via nevergrad.\r\n3. I will not complete the test on 10k params, as my 12GB RAM is almost consumed.\r\n\r\n![](https://i.imgur.com/Fw6U0sr.png)\r\n\r\n```\r\n num_param  random     tpe  tpe_multivar  hyper_opt     cmaes  scikit_opt  bayes_opt\r\n       100    0.23    7.65          6.78       4.07      0.66       25.35     268.00\r\n       500    1.16   38.03         33.71      20.19      6.20      115.36     438.64\r\n      1000    2.44   76.77         67.93      40.36    797.52      235.75     768.38\r\n      2000    5.68  159.24        154.20      84.58  18838.26      493.46     827.38\r\n```\r\n\r\nYou can [browse the plot here](https://fsmosca.github.io/Mabigat/optimizer_comparison.html).\r\n\r\nbayes_opt is interesting it seems like it does not spend too much time from 1k to 2k num_params.\r\n\r\nI am not sure though if optuna developers have attempted to make some serious optimization on the samplers. Profiling to find some hotspots and bottleneck would be interesting."
      }
    ]
  },
  {
    "issue_number": 6054,
    "title": "Support `ax=` argument in `optuna.visualization.matplotlib.plot_*` functions",
    "author": "vsbuffalo",
    "state": "open",
    "created_at": "2025-04-18T03:27:34Z",
    "updated_at": "2025-04-26T02:48:15Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nCurrently, Optuna's Matplotlib-based visualizations (e.g., `plot_optimization_history`, `plot_contour`, `plot_param_importances`) return a `matplotlib.axes.Axes` object but do not accept an `ax=` argument. This prevents users from embedding these plots into a larger figure layout (e.g., a multi-panel diagnostic report or custom dashboard).\n\nWithout `ax=`, the only options are:\n- Attempting to copy artists between Axes (fragile, unreliable)\n- Saving figures and reloading them as images (inefficient)\n- Rewriting the entire plotting logic downstream\n\nThis makes integration with other plotting pipelines unnecessarily difficult.\n\n\n### Description\n\nThese issues would be solved with an optional `ax: matplotlib.axes.Axes | None = None` support to all functions in `optuna.visualization.matplotlib`. If provided, the function should render into the given Axes object instead of creating its own figure.\n\nThis would align with standard Matplotlib conventions and allow for flexible figure composition in research, diagnostics, and production settings.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "@vsbuffalo \nThank you for bringing this up!\nWe recently launched another platform named [OptunaHub](https://hub.optuna.org/?q=Visualization) to promptly support such unofficial, yet very useful, functionality!\n\nWould you mind contributing to OptunaHub?\n\nAbout our design principle, we have also realized your issue for a long time, but we are not thinking of supporting `ax` in the argument, as our visualization is intended for quick analysis rather than visualization for presentation.\nYet, any contributions to OptunaHub are very welcome:)"
      },
      {
        "user": "vsbuffalo",
        "body": "Hi @nabenabe0928, thank you for your reply. \n\n> About our design principle, we have also realized your issue for a long time, but we are not thinking of supporting ax in the argument, as our visualization is intended for quick analysis rather than visualization for presentation.\n\nI’d be happy to submit a minimal PR to Optuna core that addresses the core issue: adding an optional `ax=None` argument, following standard Matplotlib practice. This is the correct design for any plotting function, and it would be fully backward-compatible and low-maintenance.\n\nThe goal is not to expand features, but to fix a basic integration limitation. Let me know if you’d reconsider — I’d be ready to draft it."
      },
      {
        "user": "nabenabe0928",
        "body": "@vsbuffalo \nYes, I know what you mean and I have been also leaning to your side, but as we have OptunaHub to meet this kinda need, I would kindly ask you to contribute to OptunaHub rather than to Optima!"
      }
    ]
  },
  {
    "issue_number": 6060,
    "title": "There is no noticeable optimization.",
    "author": "lwq-star",
    "state": "closed",
    "created_at": "2025-04-23T05:08:35Z",
    "updated_at": "2025-04-23T05:25:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nWhy is the optimization result not significant when using the optuna library for hyperparameter optimization of the XGBoost regression model?\nThis is my hyperparameter optimization code.\n```\ndef objective(trial):\n    params = {\n    'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n    'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.1, log=True),\n    'max_depth': trial.suggest_int('max_depth', 10, 28),\n    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n    'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n    'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n    'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n    'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n    'random_state': 42\n    }\n    \n    model = XGBRegressor( **params)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    score = mean_squared_error(y_test, y_pred)\n    return score\n\nsampler = optuna.samplers.NSGAIISampler()\npruner = optuna.pruners.HyperbandPruner()\nstudy = optuna.create_study(direction='minimize', sampler=sampler, pruner=pruner)\nstudy.optimize(objective, n_trials=300, show_progress_bar=True, n_jobs = -1)\n```\nUsing `optuna.visualization.matplotlib.plot_optimization_history(study)` can display the optimization history. Despite 300 iterations, the results did not show significant improvement.\n\n![Image](https://github.com/user-attachments/assets/bb5be5ec-9352-4f43-af0e-fcaa263f5913)\n\n![Image](https://github.com/user-attachments/assets/387d117b-6151-4037-9bd1-20b150843daf)\n\n### Environment\n\n- Optuna version:4.2.1\n- Python version:3.12.8\n- Xgboost:3.0.0\n- OS:\n- (Optional) Other libraries and their versions:\n\n\n### Error messages, stack traces, or logs\n\n```shell\nno\n```\n\n### Steps to reproduce\n\n1.\n2.\n3.\n```python\n# python code\n```\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5453,
    "title": "Custom Executor Support",
    "author": "8W9aG",
    "state": "closed",
    "created_at": "2024-05-24T03:15:14Z",
    "updated_at": "2025-04-20T23:06:00Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nI am trying to get optuna to support [modal](https://modal.com/) for distributed processing since my system uses that for other multiprocessing issues. it seems like it should be a nice way to parallelise trials quickly.\n\n### Description\n\nAllow consumers of the study object to define a custom `concurrent.futures.Executor`, by default this would be the `ThreadPoolExecutor`, but consumers could define their own Executor to be used. In my case I would define an Executor to interface to modal. This would also allow extensions for other parallelisation methods, not just my modal case.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\nI would be willing to provide a PR for this, just want to get buy-in as to whether this is the correct approach.",
    "comments": [
      {
        "user": "charlesjhill",
        "body": "+1 - For my use case, I wrote a custom ask-and-tell loop just to leverage [submitit](https://github.com/facebookincubator/submitit), which implements the same interface as `ThreadPoolExecutor`. Being able to pass a custom executor would be a time-saver, and allow for re-use of Callbacks and other such goodies."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5502,
    "title": "Any bugs when using NumPy v2.0.0",
    "author": "nabenabe0928",
    "state": "closed",
    "created_at": "2024-06-19T05:12:01Z",
    "updated_at": "2025-04-15T23:06:17Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\n\nIf you encountered any bugs when using NumPy v2.0.0, the post including discussion, issue, and PR will be linked to here.\n\n### Environment\n\n- Optuna version: latest\r\n- Python version: 3.9 or later\r\n- (Optional) Other libraries and their versions: NumPy v2 or later\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\nThis post is not specific to one bug.\n```\n\n\n### Steps to reproduce\n\nPlease Make sure your NumPy version is v2.x and Optuna is the latest.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "In this PR, I found that `GPSampler` does not work on Windows with NumPy v2.0.0.\r\n- https://github.com/optuna/optuna/pull/5493"
      },
      {
        "user": "nabenabe0928",
        "body": "This post verifies the behaviors of `TPESampler` between NumPy v1.26.4 and v2.0.0.\r\n- https://github.com/optuna/optuna/pull/5493#issuecomment-2177543043"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 6024,
    "title": "Support some mutation algorithm like polynomial mutation",
    "author": "hrntsm",
    "state": "closed",
    "created_at": "2025-03-28T08:19:19Z",
    "updated_at": "2025-04-15T10:59:23Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nIn the current NSGA-II sampling, if it target to mutation, it becomes `independent_sampling`. Therefore, the mutations are randomly sampled and classified as uniform mutation.\n\nTo expand the functionality, I would like to introduce other mutation algorithm.\n\n### Description\n\nFor example, I would like to implement the polynomial mutation mentioned in the attached paper.\n[Analysing mutation schemes for real-parameter genetic algorithms](https://www.researchgate.net/publication/262320509_Analysing_mutation_schemes_for_real-parameter_genetic_algorithms)\n\n\nI'm thinking of adding an `else` to the following part to add mutation process like crossover\n\nhttps://github.com/optuna/optuna/blob/4e14cc83ce460a51e7408ce017d455bc9efe83d6/optuna/samplers/nsgaii/_child_generation_strategy.py#L98-L100\n\n### Alternatives (optional)\n\nSince the current NSGA has an argument `child_generation_strategy` and mutates within that argument, I do not think it is necessary to add it directly to optuna since you can create that argument yourself.\n\nCan I add these sampler related method in the `sampler` section of optunahub?\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "hrntsm",
        "body": "I close this issue as I have implemented it on optunahab's side."
      }
    ]
  },
  {
    "issue_number": 6048,
    "title": "Correct return type annotations for `Axes` in visualization functions",
    "author": "sawa3030",
    "state": "open",
    "created_at": "2025-04-15T01:27:08Z",
    "updated_at": "2025-04-15T01:27:08Z",
    "labels": [
      "code-fix"
    ],
    "body": "### Motivation\n\nIn some visualization functions, the return type is currently annotated as `plt.Axes`. However, in reality, the returned value is often a NumPy array of `plt.Axes` (`np.ndarray[plt.Axes]`). For more context, please refer to [this comment](https://github.com/optuna/optuna/pull/6028#discussion_r2023882810). \nref: [Optional mypy check #6028](https://github.com/optuna/optuna/pull/6028) \n\n### Suggestion\n\nCheck the visualization functions that return `Axes`, and update the type annotations as needed. For example, where `plt.subplots()` is used, consider returning `Union[plt.Axes, np.ndarray[plt.Axes]]`.\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 4539,
    "title": "Reintroduce Joblib-based multiprocessing backend option",
    "author": "j-adamczyk",
    "state": "closed",
    "created_at": "2023-03-23T22:29:01Z",
    "updated_at": "2025-04-14T06:09:22Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nIn the current version of Optuna, there is no way to easily perform multiprocessing inside a single Python script. Running multiple terminals is impossible in automated cloud environments, and even if this is possible, it's plainly a bad design. Scikit-learn and related APIs support `n_jobs` exactly for this purpose - if I set `n_jobs=-1`, I utilize all my CPU cores. Of course, using threads first is a reasonable design for Optuna, but there should be an option to change this behavior to use processes instead.\r\n\r\nThis would be useful for CPU-bound jobs there a single job cannot be parallelized easily. The most common use case is SVM in Scikit-learn, which is single-threaded, but it requires extensive hyperparameter tuning, which can be done in parallel. Another use case is training multiple neural networks on the same GPU, within multiple CPU processes, e.g. Graph Neural Networks (GNNs).\r\n\r\nAn additional advantage would be that `OptunaSearchCV` would have the same meaning of `n_jobs` as the Scikit-learn, which it integrates with.\n\n### Description\n\nThe problem lies in the `_optimize` function, [here](https://github.com/optuna/optuna/blob/09adcbe7ca16e5f10137bb9923e5d165baf45a7b/optuna/study/_optimize.py#L85):\r\n```\r\nwith ThreadPoolExecutor(max_workers=n_jobs) as executor:\r\n    for n_submitted_trials in itertools.count():\r\n        if study._stop_flag:\r\n...\r\n```\r\nSince thread-based executor is hardcoded here, there is no way to specify anything else. Even setting the Joblib backend, [which used to work](https://github.com/optuna/optuna/issues/2202), cannot work here.\r\n\r\nHowever, if we could specify the executor, the user could use any backend supported by Joblib: regular Python multithreading or multiprocessing, Loky (efficient multiprocessing, default in Scikit-learn) or even anything else. I suggest using Joblib, as this is the easiest and the most flexible option, also arguably the most popular.\r\n\r\nThere would be 2 changes required:\r\n1. Add `parallel_backend` option to `_optimize()` and function that are calling it, specifying the Joblib backend to use.\r\n2. Use `joblib.Parallel()` instead of `ThreadPoolExecutor` in `_optimize()` for multiple jobs case.\r\n\r\nNote that this does not require any changes to the RDB backend, as this is exactly equivalent to running Optuna in separate terminals.\n\n### Alternatives (optional)\n\nCurrently, the only alternative is to manually launch multiple Optuna trials via Joblib (taken from [here](https://github.com/optuna/optuna/issues/2862)):\r\n```\r\njoblib.Parallel(n_jobs)(\r\n    joblib.delayed(optimize_study)(study.study_name,\r\n                                  storage_string,\r\n                                  objective, n_trials=25)\r\n    for i in range(n_jobs)\r\n)\r\n\r\n```\r\nHowever, this requires a manual wrapper around a core, important functionality. I have used this approach in multiple projects, but copy-pasting this so many times makes me feel like this should just be built in.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "c-bata",
        "body": "> In the current version of Optuna, there is no way to easily perform multiprocessing inside a single Python script.\r\n\r\nSince the `concurrent.futures` module provides a high-level API, I think it would basically work if you changed it to `ProcessPoolExecutor` like the following.\r\n\r\n```python\r\nimport optuna\r\n# from concurrent.futures import ThreadPoolExecutor\r\nfrom concurrent.futures import ProcessPoolExecutor as ThreadPoolExecutor\r\n\r\n\r\ndef objective(trial):\r\n    x = trial.suggest_float(\"x\", -100, 100)\r\n    y = trial.suggest_float(\"y\", -100, 100)\r\n    return x**2 + y\r\n\r\n\r\ndef main():\r\n    study = optuna.create_study(storage=\"sqlite:///db.sqlite3\")   # Please use RDBStorage, JournalStorage or DaskStorage.\r\n    with ThreadPoolExecutor(max_workers=5) as pool:\r\n        for i in range(5):\r\n            pool.submit(study.optimize, objective, n_trials=10)\r\n    print(f\"Best params: {study.best_params}\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nUsing `concurrent.futures` makes it unnecessary for users to install joblib as an additional dependency and simplifies Optuna's source code. What do you think?"
      },
      {
        "user": "j-adamczyk",
        "body": "@c-bata that's a nice solution, I think even `from concurrent.futures import ProcessPoolExecutor as ThreadPoolExecutor` would suffice, as long as I import this before Optuna. However, this has 2 downsides:\r\n1. This is a totally nonobvious hack, relying on import ordering and Optuna internals.\r\n2. This still means that `n_jobs` has different meaning for Scikit-learn and its integration with Optuna.\r\n3. I would still need to modify the code outside Optuna, when it can be parallelized internally.\r\n\r\nBut this would make an easy change for Optuna behavior. Simply using either `ProcessPoolExecutor` or `ThreadPoolExecutor` in `_optimize()` would suffice for many use cases. However, arbitraty executors should also be supported, since they may offer major advantages. Most notably, [loky backend and executor](https://github.com/joblib/loky) is a more robust solution than plain multiprocessing, and faster e.g. [when using Numpy arrays](https://stackoverflow.com/questions/66605590/how-does-the-loky-backend-of-joblib-handle-access-to-global-variables). Using Joblib would make all those 3 options use the same API, but is not strictly necessary.\r\n\r\nHowever, Scikit-learn already depends on Joblib, so a large chunk of Optuna users already depend on it anyway. Also, Optuna used to depend on Joblib, since older issues reference it. This is a relatively self-contained and lightweight dependency.\r\n\r\nI see 3 options:\r\n1. Still require user to do manual loops or import hacking - this issue is about changing this.\r\n2. Add option to choose Python-based processes or threads, and add argument to `_optimize()` and other functions to support switching between the two. Does not add any dependencies and is simpel, but is less robust and slower (when using Numpy at least) than option 3.\r\n3. Add Joblib dependency and use it in `_optimize()`, with threads as the default backend, but with processes and Loky options. This is the most robust solution and still easy to implement, but adds a dependency (but a small and common one)."
      },
      {
        "user": "okaikov",
        "body": "Hi,\r\nAny updates regarding this feature request?"
      }
    ]
  },
  {
    "issue_number": 6036,
    "title": "Storage Internal Error on SLURM",
    "author": "aquaresima",
    "state": "closed",
    "created_at": "2025-04-09T21:32:09Z",
    "updated_at": "2025-04-12T12:30:48Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nHello,\n\nI am running Optuna on multiple nodes of a Slurm cluster. I am calling Optuna via Julia\nThe optuna study is shared across multiple nodes, following th Easy Parallelism guidelines. \n\nThe storage is a sqlite db in the local storage of my cluster submit node\n\nIn _some instances_ of my cluster runs (some other ones go just smooth), one or two nodes will yield the error below:\n`optuna.exceptions.StorageInternalError`. When it occurs, none of the distributed processes can write to the db anymore.\n\nI would like to understand the origin of this problem, so I can fix it. Importantly, it is not an error on the Julia side, the task I am running continues, it s just the connection with the db that breaks.\n\nFor example, is there a way to handle the exception and show which data are in the commit? Could the error be due to collisions in opening the db file? I run multiple slurm batch runs with the same target database.\n\nEDIT:\nIt is almost certainly a collision problem. If I add a sleep(300) before running `opt_study.optimize(objective, n_trials=10)` I have no error in 8 batch runs that use the same sqlite db.\n\nHow am I supposed to manage this collision problem? Has Optuna some form of db lock? The `sleep` solution is just not reliable...\n\nThanks,\nAlessio\n\n\n### Environment\n\n- Optuna version: 4.2.1\n- Python version: python3.12\n- OS: SLURM cluster on Linux 4.18.0-553.47.1.el8_10.x86_64\n- (Optional) Other libraries and their versions: Julia 1.11.4\n\n\n### Error messages, stack traces, or logs\n\n```shell\nError: Returned: PyError ($(Expr(:escape, :(ccall(#= /pasteur/appa/homes/aquaresi/.julia_maestro-submit/packages/PyCall/1gn3u/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'optuna.exceptions.StorageInternalError'>\n│ StorageInternalError('An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. ')\n│   File \"/pasteur/appa/homes/aquaresi/.julia_maestro-submit/conda/3/x86_64/lib/python3.12/site-packages/optuna/trial/_trial.py\", line 161, in suggest_float\n│     suggested_value = self._suggest(name, distribution)\n│                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n│   File \"/pasteur/appa/homes/aquaresi/.julia_maestro-submit/conda/3/x86_64/lib/python3.12/site-packages/optuna/trial/_trial.py\", line 627, in _suggest\n│     elif self._is_relative_param(name, distribution):\n│          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n│   File \"/pasteur/appa/homes/aquaresi/.julia_maestro-submit/conda/3/x86_64/lib/python3.12/site-packages/optuna/trial/_trial.py\", line 659, in _is_relative_param\n│     if name not in self.relative_params:\n│                    ^^^^^^^^^^^^^^^^^^^^\n│   File \"/pasteur/appa/homes/aquaresi/.julia_maestro-submit/conda/3/x86_64/lib/python3.12/site-packages/optuna/trial/_trial.py\", line 71, in relative_params\n│     self._relative_params = self.study.sampler.sample_relative(\n│                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n│   File \"/pasteur/appa/homes/aquaresi/.julia_maestro-submit/conda/3/x86_64/lib/python3.12/site-packages/optuna/samplers/nsgaii/_sampler.py\", line 272, in sample_relative\n│     study._storage.set_trial_system_attr(trial._trial_id, _GENERATION_KEY, generation)\n│   File \"/pasteur/appa/homes/aquaresi/.julia_maestro-submit/conda/3/x86_64/lib/python3.12/site-packages/optuna/storages/_cached_storage.py\", line 198, in set_trial_system_attr\n│     self._backend.set_trial_system_attr(trial_id, key=key, value=value)\n│   File \"/pasteur/appa/homes/aquaresi/.julia_maestro-submit/conda/3/x86_64/lib/python3.12/site-packages/optuna/storages/_rdb/storage.py\", line 727, in set_trial_system_attr\n│     with _create_scoped_session(self.scoped_session, True) as session:\n│          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n│   File \"/pasteur/appa/homes/aquaresi/.julia_maestro-submit/conda/3/x86_64/lib/python3.12/contextlib.py\", line 158, in __exit__\n│     self.gen.throw(value)\n│   File \"/pasteur/appa/homes/aquaresi/.julia_maestro-submit/conda/3/x86_64/lib/python3.12/site-packages/optuna/storages/_rdb/storage.py\", line 94, in _create_scoped_session\n│     raise optuna.exceptions.StorageInternalError(message) from e\n└ @ Main ~/spiking/network_models/papers/Quaresima25_TimeToSpace/simulation_optimization.jl:169\n```\n\n### Steps to reproduce\n\nin Julia.\n\n```\nusing Distributed\n\n@everywhere opt_study = optuna.create_study(directions=[\"maximize\", \"minimize\"], study_name=\"study_1\", storage=\"all_studies.db\",  load_if_exists=true)\n\nfunction objective(trial)\n            input_density = trial.suggest_float(\"p_input\", 0.1, 0.3)\n            input_strength = trial.suggest_float(\"w_input\", 10, 50)\n            noiseEd = trial.suggest_float(\"noiseEd\", 2.0, 4.5)\n            v0 = trial.suggest_float(\"v0\", -65, -55)\n            \n            return accuracy, stability = task(v0, noiseEd, input_density, input_strength)\n    end\n\ntasks = @sync @distributed for w in workers()\n    @info \"Worker: $(getpid()) active\"\n    opt_study.optimize(objective, n_trials=10)\nend\n\nwait(tasks)\n```\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "y0z",
        "body": "Thank you for your report.\nThis is a known issue when using SQLite, which is unsuitable for distributed optimization.\nPlease consider using another database system.\n- https://optuna.readthedocs.io/en/latest/tutorial/10_key_features/004_distributed.html\n- [Can multiple applications or multiple instances of the same application access a single database file at the same time?](https://sqlite.org/faq.html#q5)\n\n<img width=\"1132\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9028d119-11f0-49d7-91cf-4b2d29006dd2\" />"
      },
      {
        "user": "aquaresima",
        "body": "Hi, ok, thanks. \n\nI looked into the JournalFile database [1], and I think that will do. \n\nhttps://medium.com/optuna/distributed-optimization-via-nfs-using-optunas-new-operation-based-logging-storage-9815f9c3f932\n"
      }
    ]
  },
  {
    "issue_number": 5600,
    "title": "Postgres connection drops after ~one hour of inactivity",
    "author": "ziadloo",
    "state": "open",
    "created_at": "2024-07-29T03:37:18Z",
    "updated_at": "2025-04-09T22:57:34Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nThe connection should be kept alive no matter how long it's been inactive. Or, there must be a retry mechanism implemented.\r\n\r\n### Environment\r\n\r\n- Optuna version: 3.6.1\r\n- Python version: 3.10.13\r\n- OS: Linux-6.6.40-1-MANJARO-x86_64-with-glibc2.39\r\n- psycopg2: Version: 2.9.9\r\n- PostgreSQL 16.3\r\n\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\nINFO:root:Timing Info: ds_fetch : 14.60ms, model_fwd : 36.66ms, crit_fwd : 0.45ms, bwd : 1.29ms, optim : 0.29ms, metrics : 1.24ms, train_total : 21081.21ms, test_total : 1672.35ms\r\nTraceback (most recent call last):\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\r\n    self.dialect.do_execute(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 924, in do_execute\r\n    cursor.execute(statement, parameters)\r\npsycopg2.OperationalError: server closed the connection unexpectedly\r\n        This probably means the server terminated abnormally\r\n        before or while processing the request.\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py\", line 72, in _create_scoped_session\r\n    yield session\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py\", line 776, in get_trial\r\n    trial_model = models.TrialModel.find_or_raise_by_id(trial_id, session)\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/storages/_rdb/models.py\", line 254, in find_or_raise_by_id\r\n    trial = query.one_or_none()\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/orm/query.py\", line 2754, in one_or_none\r\n    return self._iter().one_or_none()  # type: ignore\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/orm/query.py\", line 2827, in _iter\r\n    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 2351, in execute\r\n    return self._execute_internal(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 2236, in _execute_internal\r\n    result: Result[Any] = compile_state_cls.orm_execute_statement(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/orm/context.py\", line 293, in orm_execute_statement\r\n    result = conn.execute(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1418, in execute\r\n    return meth(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/sql/elements.py\", line 515, in _execute_on_connection\r\n    return connection._execute_clauseelement(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1640, in _execute_clauseelement\r\n    ret = self._execute_context(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1846, in _execute_context\r\n    return self._exec_single_context(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1986, in _exec_single_context\r\n    self._handle_dbapi_exception(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 2353, in _handle_dbapi_exception\r\n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\r\n    self.dialect.do_execute(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 924, in do_execute\r\n    cursor.execute(statement, parameters)\r\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly\r\n        This probably means the server terminated abnormally\r\n        before or while processing the request.\r\n\r\n[SQL: SELECT trials.trial_id AS trials_trial_id, trials.number AS trials_number, trials.study_id AS trials_study_id, trials.state AS trials_state, trials.datetime_start AS trials_datetime_start, trials.datetime_complete AS trials_datetime_complete \r\nFROM trials \r\nWHERE trials.trial_id = %(trial_id_1)s]\r\n[parameters: {'trial_id_1': 843}]\r\n(Background on this error at: https://sqlalche.me/e/20/e3q8)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 208, in _run_trial\r\n    frozen_trial = _tell_with_warning(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/study/_tell.py\", line 106, in _tell_with_warning\r\n    frozen_trial = _get_frozen_trial(study, trial)\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/study/_tell.py\", line 40, in _get_frozen_trial\r\n    return study._storage.get_trial(trial_id)\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/storages/_cached_storage.py\", line 213, in get_trial\r\n    return self._backend.get_trial(trial_id)\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py\", line 775, in get_trial\r\n    with _create_scoped_session(self.scoped_session) as session:\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py\", line 90, in _create_scoped_session\r\n    raise optuna.exceptions.StorageInternalError(message) from e\r\noptuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mehran/Documents/GitHub/gtn_applications/./search_upstream.py\", line 276, in <module>\r\n    search()\r\n  File \"/home/mehran/Documents/GitHub/gtn_applications/./search_upstream.py\", line 259, in search\r\n    study.optimize(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/study/study.py\", line 451, in optimize\r\n    _optimize(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 62, in _optimize\r\n    _optimize_sequential(\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 159, in _optimize_sequential\r\n    frozen_trial = _run_trial(study, func, catch)\r\n  File \"/home/mehran/.conda/envs/pytorch/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 240, in _run_trial\r\n    assert False, \"Should not reach.\"\r\nAssertionError: Should not reach.\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\n1. Create a python file with the following code and a `.env` file with Postgres credentials in it.\r\n2. Run the code\r\n3. Be patient (it's set to wait for one hour and 15 minutes)\r\n```python\r\nimport optuna\r\nimport time\r\nimport random\r\nfrom dotenv import load_dotenv\r\n\r\n\r\ndef objective(trial):\r\n    trial.suggest_int(\"dummy\", 0, 10)\r\n    time.sleep(int(3600 * 1.25))\r\n    for epoch in range(40):\r\n        loss = random.random()\r\n        trial.report(loss, epoch)\r\n        if trial.should_prune():\r\n            raise optuna.exceptions.TrialPruned()\r\n\r\n\r\nstudy = optuna.create_study(\r\n    storage=f\"postgresql+psycopg2://{os.getenv('POSTGRES_USERNAME')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('POSTGRES_HOST')}:{os.getenv('POSTGRES_PORT')}/{os.getenv('POSTGRES_DB')}\",\r\n    # storage=f\"postgresql://{os.getenv('POSTGRES_USERNAME')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('POSTGRES_HOST')}:{os.getenv('POSTGRES_PORT')}/{os.getenv('POSTGRES_DB')}\",\r\n    study_name=\"Connection failure\",\r\n)\r\nstudy.optimize(objective, n_trials=10)\r\n```\r\n\r\n\r\n### Additional context (optional)\r\n\r\nMy use case is composed of an upstream and a downstream training. While the optimization is targeting the upstream model, I cannot evaluate it unless it's trained for a few epochs and then used for training a downstream model. Only then I can call the `trial.report()` to report back to Optuna. And this takes a long time (more than an hour) and by then the Postgres connection is lost and as a result, the whole process fails.\r\n\r\nBTW, I tested with MySQL and SQLite and so far, they are not facing the same problem.",
    "comments": [
      {
        "user": "AMJasser",
        "body": "@ziadloo I am facing the same problem. Any workarounds?"
      },
      {
        "user": "ziadloo",
        "body": "I switched to MySQL"
      },
      {
        "user": "mukhamux",
        "body": "You can try this:\n\n```python\nimport optuna\nfrom optuna.storages import RDBStorage\nimport os\n\nurl = f\"postgresql+psycopg2://{os.getenv('POSTGRES_USERNAME')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('POSTGRES_HOST')}:{os.getenv('POSTGRES_PORT')}/{os.getenv('POSTGRES_DB')}\"\n\nstudy = optuna.create_study(\n    storage=RDBStorage(url=url, engine_kwargs={'pool_recycle': 60}),\n    study_name=\"Connection failure\",\n)\n```\n\n\nThere could be many sources of your problem, such as whether you have a proxy in front of the database or whether you launched the database on a localhost, and also with what parameters your database is launched. The proposed solution allows alchemy to update the connection every 60 minutes, which should eliminate (or at least reduce) network problems. More details: https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_recycle"
      }
    ]
  },
  {
    "issue_number": 6031,
    "title": "HEBO does not show similar performance (underperforms) when using search_space for the same number of trials",
    "author": "rafapm",
    "state": "closed",
    "created_at": "2025-04-01T23:51:05Z",
    "updated_at": "2025-04-04T17:38:55Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nWhen adding search_space (as suggested to speed up HEBO), the algorithm performs worse for the same number of trials. \n\nI tried both ways (with and without search_space):\n\n1) Without search_space:\n - The trials are slower, but they converge to a good solution faster\n\n2) With search_space:\n- The trials are much faster, but it does not converge to a good solution\n\nI also tried running the same example using the original HEBO repository, and the trials are much slower, but they do converge to a good solution faster. This is similar to not using search_space.\n\nI am assuming the speed should be faster when using search_space without sacrificing performance.\n\n### Environment\n\n- Optuna version: 4.2.0\n- Python version: 3.10.8\n- OS: Windows-10\n- Hebo version: 0.3.6\n- Pymoo version: 0.6.1.3\n\n### Error messages, stack traces, or logs\n\n```shell\nOutput log with search_space:\n\n[I 2025-04-01 16:35:37,781] A new study created in memory with name: no-name-38e37f9c-2640-4a0d-8a61-ca5aedeee479\n[I 2025-04-01 16:35:37,785] Trial 0 finished with value: 87.29607271153357 and parameters: {'x': -2.50919762305275, 'y': 9}. Best is trial 0 with value: 87.29607271153357.\n[I 2025-04-01 16:35:37,787] Trial 1 finished with value: 96.48669753655327 and parameters: {'x': -7.777319431304932, 'y': 6}. Best is trial 0 with value: 87.29607271153357.\n[I 2025-04-01 16:35:37,788] Trial 2 finished with value: 16.269725286678522 and parameters: {'x': -2.6962428092956543, 'y': -3}. Best is trial 2 with value: 16.269725286678522.\n[I 2025-04-01 16:35:37,791] Trial 3 finished with value: 24.70715594793728 and parameters: {'x': 4.868999481201172, 'y': 1}. Best is trial 2 with value: 16.269725286678522.\n[I 2025-04-01 16:35:37,793] Trial 4 finished with value: 2.706312034207258 and parameters: {'x': 1.6450872421264648, 'y': 0}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,796] Trial 5 finished with value: 16.59480561503733 and parameters: {'x': -0.7712364196777344, 'y': 4}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,798] Trial 6 finished with value: 57.360881368754235 and parameters: {'x': -5.688662528991699, 'y': -5}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,801] Trial 7 finished with value: 124.07108772601714 and parameters: {'x': 6.562856674194336, 'y': 9}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,802] Trial 8 finished with value: 54.66332151023471 and parameters: {'x': 5.446404457092285, 'y': -5}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,804] Trial 9 finished with value: 49.78602360271907 and parameters: {'x': -6.984699249267578, 'y': 1}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,806] Trial 10 finished with value: 103.63653103559409 and parameters: {'x': -1.9069690704345703, 'y': -10}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,808] Trial 11 finished with value: 36.13623100728728 and parameters: {'x': 0.3690948486328125, 'y': 6}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,812] Trial 12 finished with value: 63.001490627962085 and parameters: {'x': 3.741856575012207, 'y': -7}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,815] Trial 13 finished with value: 97.0041048775056 and parameters: {'x': -4.000513076782227, 'y': 9}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,817] Trial 14 finished with value: 83.64415525720051 and parameters: {'x': -8.924357414245605, 'y': -2}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,819] Trial 15 finished with value: 91.08974439220401 and parameters: {'x': 8.665433883666992, 'y': 4}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,821] Trial 16 finished with value: 64.67591790418737 and parameters: {'x': 7.979719161987305, 'y': -1}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,823] Trial 17 finished with value: 117.25946041406132 and parameters: {'x': -9.605178833007812, 'y': 5}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,825] Trial 18 finished with value: 57.96680495359965 and parameters: {'x': -4.686875820159912, 'y': -6}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,828] Trial 19 finished with value: 109.36595482027406 and parameters: {'x': 3.060384750366211, 'y': 10}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,831] Trial 20 finished with value: 81.94440051005313 and parameters: {'x': 0.9718027114868164, 'y': -9}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,833] Trial 21 finished with value: 50.68841643253472 and parameters: {'x': -1.2993907928466797, 'y': 7}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,835] Trial 22 finished with value: 56.72156139557137 and parameters: {'x': -6.381344795227051, 'y': -4}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,838] Trial 23 finished with value: 40.65859401511261 and parameters: {'x': 6.054634094238281, 'y': 2}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,841] Trial 24 finished with value: 88.54234030464795 and parameters: {'x': 7.24860954284668, 'y': -6}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,844] Trial 25 finished with value: 89.07805231231396 and parameters: {'x': -5.00779914855957, 'y': 8}. Best is trial 4 with value: 2.706312034207258.\n[I 2025-04-01 16:35:37,848] Trial 26 finished with value: 1.0072092889458872 and parameters: {'x': -0.08490753173828125, 'y': -1}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,850] Trial 27 finished with value: 14.41269959026613 and parameters: {'x': 2.3265209197998047, 'y': 3}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,853] Trial 28 finished with value: 34.20091868201507 and parameters: {'x': 4.266253471374512, 'y': -4}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,855] Trial 29 finished with value: 10.915514194406569 and parameters: {'x': -3.303863525390625, 'y': 0}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,857] Trial 30 finished with value: 151.235111326082 and parameters: {'x': -8.38063907623291, 'y': -9}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,859] Trial 31 finished with value: 112.2743718058482 and parameters: {'x': 9.34207534790039, 'y': 5}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,862] Trial 32 finished with value: 86.62778643382262 and parameters: {'x': 8.810663223266602, 'y': -3}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,865] Trial 33 finished with value: 72.3123339907188 and parameters: {'x': -8.444663047790527, 'y': 1}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,867] Trial 34 finished with value: 76.4128880875653 and parameters: {'x': -3.523192882537842, 'y': -8}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,869] Trial 35 finished with value: 51.129090468713 and parameters: {'x': 3.889613151550293, 'y': 6}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,871] Trial 36 finished with value: 29.313246243691538 and parameters: {'x': 2.0768356323242188, 'y': -5}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,873] Trial 37 finished with value: 64.23989140503636 and parameters: {'x': -0.48978710174560547, 'y': 8}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,875] Trial 38 finished with value: 31.023473009878217 and parameters: {'x': -5.56987190246582, 'y': 0}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,877] Trial 39 finished with value: 60.21770159859443 and parameters: {'x': 7.156654357910156, 'y': 3}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,879] Trial 40 finished with value: 133.58191535348305 and parameters: {'x': 5.794990539550781, 'y': -10}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,882] Trial 41 finished with value: 95.18388559549294 and parameters: {'x': -6.795872688293457, 'y': 7}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,885] Trial 42 finished with value: 28.500777694876888 and parameters: {'x': -1.8710365295410156, 'y': -5}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,887] Trial 43 finished with value: 4.756835085925559 and parameters: {'x': 0.8699626922607422, 'y': 2}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,889] Trial 44 finished with value: 10.444620531841792 and parameters: {'x': 2.538625717163086, 'y': -2}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,891] Trial 45 finished with value: 38.47656723062755 and parameters: {'x': -4.740945816040039, 'y': 4}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,893] Trial 46 finished with value: 145.3269907367876 and parameters: {'x': -9.814631462097168, 'y': -7}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,895] Trial 47 finished with value: 138.95252588787116 and parameters: {'x': 7.6126556396484375, 'y': 9}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,897] Trial 48 finished with value: 117.21660744353721 and parameters: {'x': 8.259334564208984, 'y': -7}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,899] Trial 49 finished with value: 184.14066341793023 and parameters: {'x': -9.172821998596191, 'y': 10}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,901] Trial 50 finished with value: 20.758301053023615 and parameters: {'x': -4.093690395355225, 'y': -2}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,903] Trial 51 finished with value: 35.11878978317782 and parameters: {'x': 3.181004524230957, 'y': 5}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,905] Trial 52 finished with value: 16.093814532101533 and parameters: {'x': 0.3062915802001953, 'y': -4}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,907] Trial 53 finished with value: 9.951662074840897 and parameters: {'x': -2.439602851867676, 'y': 2}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,909] Trial 54 finished with value: 135.17134972116037 and parameters: {'x': -7.360118865966797, 'y': -9}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,912] Trial 55 finished with value: 76.30954919300075 and parameters: {'x': 5.22585391998291, 'y': 7}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,914] Trial 56 finished with value: 43.38020231482005 and parameters: {'x': 6.510007858276367, 'y': -1}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,916] Trial 57 finished with value: 47.5844996323076 and parameters: {'x': -6.211642265319824, 'y': 3}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,918] Trial 58 finished with value: 37.292949195518304 and parameters: {'x': -1.1370792388916016, 'y': -6}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,919] Trial 59 finished with value: 66.05754326494935 and parameters: {'x': 1.4344139099121094, 'y': 8}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,922] Trial 60 finished with value: 83.83144344484026 and parameters: {'x': 4.453250885009766, 'y': -8}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,924] Trial 61 finished with value: 33.730051985243335 and parameters: {'x': -2.9546661376953125, 'y': 5}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,925] Trial 62 finished with value: 71.10038385081953 and parameters: {'x': -7.880379676818848, 'y': -3}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,928] Trial 63 finished with value: 87.98145016815761 and parameters: {'x': 9.379842758178711, 'y': 0}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,930] Trial 64 finished with value: 92.93485203347882 and parameters: {'x': 9.588266372680664, 'y': -1}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,942] Trial 65 finished with value: 73.02437060571629 and parameters: {'x': -8.0015230178833, 'y': 3}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,947] Trial 66 finished with value: 45.47187418818453 and parameters: {'x': -3.077641010284424, 'y': -6}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,955] Trial 67 finished with value: 85.75967105577001 and parameters: {'x': 4.664726257324219, 'y': 8}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,959] Trial 68 finished with value: 65.74553262013069 and parameters: {'x': 1.3211860656738281, 'y': -8}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,961] Trial 69 finished with value: 36.91179717876457 and parameters: {'x': -0.9548807144165039, 'y': 6}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,963] Trial 70 finished with value: 45.3910019413197 and parameters: {'x': -6.032495498657227, 'y': -3}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,965] Trial 71 finished with value: 41.942199200057075 and parameters: {'x': 6.398609161376953, 'y': 1}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,968] Trial 72 finished with value: 74.24812927691164 and parameters: {'x': 5.024751663208008, 'y': -7}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,970] Trial 73 finished with value: 133.2261346169728 and parameters: {'x': -7.226765155792236, 'y': 9}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,971] Trial 74 finished with value: 9.332870644457444 and parameters: {'x': -2.3093008995056152, 'y': -2}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,973] Trial 75 finished with value: 16.011453361331405 and parameters: {'x': 0.10702037811279297, 'y': 4}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,975] Trial 76 finished with value: 35.77170909405686 and parameters: {'x': 3.2820281982421875, 'y': -5}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,977] Trial 77 finished with value: 22.345939299257452 and parameters: {'x': -4.2832159996032715, 'y': 2}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,981] Trial 78 finished with value: 168.68782406942046 and parameters: {'x': -9.364177703857422, 'y': -9}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,983] Trial 79 finished with value: 118.94664271367947 and parameters: {'x': 8.363410949707031, 'y': 7}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,985] Trial 80 finished with value: 141.7502217433066 and parameters: {'x': 7.794242858886719, 'y': -9}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,989] Trial 81 finished with value: 147.57450974780022 and parameters: {'x': -9.92846965789795, 'y': 7}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,991] Trial 82 finished with value: 39.539310491751394 and parameters: {'x': -4.8517327308654785, 'y': -4}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,993] Trial 83 finished with value: 11.3896052397331 and parameters: {'x': 2.7183828353881836, 'y': 2}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,995] Trial 84 finished with value: 4.561644879377127 and parameters: {'x': 0.7494297027587891, 'y': -2}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:37,998] Trial 85 finished with value: 27.762249350985257 and parameters: {'x': -1.662001609802246, 'y': 5}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,000] Trial 86 finished with value: 92.36231342167594 and parameters: {'x': -6.5850067138671875, 'y': -7}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,002] Trial 87 finished with value: 113.1648328428646 and parameters: {'x': 5.671404838562012, 'y': 9}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,004] Trial 88 finished with value: 57.54938823847624 and parameters: {'x': 6.967739105224609, 'y': -3}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,006] Trial 89 finished with value: 30.90162026335952 and parameters: {'x': -5.468237400054932, 'y': 1}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,008] Trial 90 finished with value: 64.14924470428377 and parameters: {'x': -0.386322021484375, 'y': -8}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,010] Trial 91 finished with value: 39.55273331664739 and parameters: {'x': 1.8848695755004883, 'y': 6}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,016] Trial 92 finished with value: 52.1793480588276 and parameters: {'x': 4.022356033325195, 'y': -6}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,019] Trial 93 finished with value: 77.87492446829947 and parameters: {'x': -3.7249059677124023, 'y': 8}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,021] Trial 94 finished with value: 75.70704801119064 and parameters: {'x': -8.64332389831543, 'y': -1}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,023] Trial 95 finished with value: 88.95178143581506 and parameters: {'x': 8.94157600402832, 'y': 3}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,025] Trial 96 finished with value: 108.73565252457047 and parameters: {'x': 9.150718688964844, 'y': -5}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,028] Trial 97 finished with value: 149.50150575995576 and parameters: {'x': -8.27656364440918, 'y': 9}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,030] Trial 98 finished with value: 10.258180059118104 and parameters: {'x': -3.2028393745422363, 'y': 0}. Best is trial 26 with value: 1.0072092889458872.\n[I 2025-04-01 16:35:38,033] Trial 99 finished with value: 32.61971787834773 and parameters: {'x': 4.076728820800781, 'y': 4}. Best is trial 26 with value: 1.0072092889458872.\n{'x': -0.08490753173828125, 'y': -1}\n\nOutput log without search_space:\n\n[I 2025-04-01 16:37:10,401] A new study created in memory with name: no-name-aa721abd-a305-42ec-b088-4eac67bd61fd\n[I 2025-04-01 16:37:10,403] Trial 0 finished with value: 87.29607271153357 and parameters: {'x': -2.50919762305275, 'y': 9}. Best is trial 0 with value: 87.29607271153357.\n[I 2025-04-01 16:37:10,408] Trial 1 finished with value: 0.06621428114249284 and parameters: {'x': -0.2573213577270508, 'y': 0}. Best is trial 1 with value: 0.06621428114249284.\n[I 2025-04-01 16:37:10,410] Trial 2 finished with value: 19.581905209620345 and parameters: {'x': 3.252984046936035, 'y': -3}. Best is trial 1 with value: 0.06621428114249284.\n[I 2025-04-01 16:37:15,201] Trial 3 finished with value: 4.3755888066892235 and parameters: {'x': -2.091790813319827, 'y': 0}. Best is trial 1 with value: 0.06621428114249284.\n[I 2025-04-01 16:37:18,341] Trial 4 finished with value: 0.06621714302871266 and parameters: {'x': -0.257326918585508, 'y': 0}. Best is trial 1 with value: 0.06621428114249284.\n[I 2025-04-01 16:37:21,309] Trial 5 finished with value: 81.06518425499416 and parameters: {'x': 0.2553120737336292, 'y': -9}. Best is trial 1 with value: 0.06621428114249284.\n[I 2025-04-01 16:37:24,239] Trial 6 finished with value: 0.08442886478422697 and parameters: {'x': 0.29056645502230116, 'y': 0}. Best is trial 1 with value: 0.06621428114249284.\n[I 2025-04-01 16:37:27,529] Trial 7 finished with value: 0.031856432919376676 and parameters: {'x': 0.17848370491273616, 'y': 0}. Best is trial 7 with value: 0.031856432919376676.\n[I 2025-04-01 16:37:30,655] Trial 8 finished with value: 0.0018794839269868314 and parameters: {'x': -0.043353015200638945, 'y': 0}. Best is trial 8 with value: 0.0018794839269868314.\n[I 2025-04-01 16:37:33,556] Trial 9 finished with value: 1.0000631292234912 and parameters: {'x': -0.007945390077971237, 'y': 1}. Best is trial 8 with value: 0.0018794839269868314.\n[I 2025-04-01 16:37:36,608] Trial 10 finished with value: 1.0017398621869356 and parameters: {'x': -0.04171165528884731, 'y': -1}. Best is trial 8 with value: 0.0018794839269868314.\n[I 2025-04-01 16:37:39,573] Trial 11 finished with value: 0.8509158614280606 and parameters: {'x': 0.9224510076031467, 'y': 0}. Best is trial 8 with value: 0.0018794839269868314.\n[I 2025-04-01 16:37:42,559] Trial 12 finished with value: 1.4513384447698874 and parameters: {'x': 0.671817270371853, 'y': -1}. Best is trial 8 with value: 0.0018794839269868314.\n[I 2025-04-01 16:37:45,390] Trial 13 finished with value: 0.001618385006661975 and parameters: {'x': -0.04022915617636014, 'y': 0}. Best is trial 13 with value: 0.001618385006661975.\n[I 2025-04-01 16:37:48,303] Trial 14 finished with value: 1.2518015573420158 and parameters: {'x': 0.5017983233750545, 'y': 1}. Best is trial 13 with value: 0.001618385006661975.\n[I 2025-04-01 16:37:51,197] Trial 15 finished with value: 59.83064321011919 and parameters: {'x': -6.620471524757069, 'y': 4}. Best is trial 13 with value: 0.001618385006661975.\n[I 2025-04-01 16:37:54,023] Trial 16 finished with value: 67.89567515655133 and parameters: {'x': 7.993477037969855, 'y': 2}. Best is trial 13 with value: 0.001618385006661975.\n[I 2025-04-01 16:37:56,895] Trial 17 finished with value: 0.001646255692271491 and parameters: {'x': 0.040574076604052135, 'y': 0}. Best is trial 13 with value: 0.001618385006661975.\n[I 2025-04-01 16:37:59,860] Trial 18 finished with value: 94.11012589060188 and parameters: {'x': 3.6207907824951544, 'y': 9}. Best is trial 13 with value: 0.001618385006661975.\n[I 2025-04-01 16:38:02,751] Trial 19 finished with value: 0.004139709435270662 and parameters: {'x': -0.06434057378723523, 'y': 0}. Best is trial 13 with value: 0.001618385006661975.\n[I 2025-04-01 16:38:05,708] Trial 20 finished with value: 3.0868448890405955 and parameters: {'x': -1.4445915993943048, 'y': -1}. Best is trial 13 with value: 0.001618385006661975.\n[I 2025-04-01 16:38:08,686] Trial 21 finished with value: 9.636067028475094e-05 and parameters: {'x': 0.009816347094757344, 'y': 0}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:11,524] Trial 22 finished with value: 0.00041016978289244244 and parameters: {'x': 0.02025264878707085, 'y': 0}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:14,424] Trial 23 finished with value: 0.0014000917248270617 and parameters: {'x': 0.03741779957222313, 'y': 0}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:17,340] Trial 24 finished with value: 58.51724730101938 and parameters: {'x': 3.085003614425659, 'y': 7}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:20,172] Trial 25 finished with value: 2.738148910948385 and parameters: {'x': 1.6547352993601077, 'y': 0}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:23,102] Trial 26 finished with value: 137.76187882087132 and parameters: {'x': 8.588473602501862, 'y': -8}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:25,989] Trial 27 finished with value: 0.005809721365060005 and parameters: {'x': 0.07622152822569228, 'y': 0}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:28,791] Trial 28 finished with value: 5.108680533423515 and parameters: {'x': 1.0529389979592905, 'y': -2}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:31,661] Trial 29 finished with value: 0.0006635551889576511 and parameters: {'x': 0.02575956499938714, 'y': 0}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:34,565] Trial 30 finished with value: 115.1452188988079 and parameters: {'x': -8.13297109910074, 'y': -7}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:37,497] Trial 31 finished with value: 36.22146588918112 and parameters: {'x': 0.47060162471151645, 'y': 6}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:40,291] Trial 32 finished with value: 1.565575645397141 and parameters: {'x': -0.7520476350585388, 'y': -1}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:43,189] Trial 33 finished with value: 2.450396846959496 and parameters: {'x': -1.204324228336994, 'y': 1}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:46,025] Trial 34 finished with value: 9.712692913407613 and parameters: {'x': -2.9517271068660147, 'y': 1}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:48,901] Trial 35 finished with value: 0.7257460144394305 and parameters: {'x': -0.8519072804240087, 'y': 0}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:51,794] Trial 36 finished with value: 10.466897077447197 and parameters: {'x': 2.5430094528819973, 'y': 2}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:54,625] Trial 37 finished with value: 0.000387848524282015 and parameters: {'x': -0.019693870221010774, 'y': 0}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:38:57,386] Trial 38 finished with value: 64.84940014311701 and parameters: {'x': -6.3126381286366335, 'y': -5}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:39:00,283] Trial 39 finished with value: 85.8924775393439 and parameters: {'x': 4.678939787958797, 'y': -8}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:39:03,193] Trial 40 finished with value: 10.503854005314953 and parameters: {'x': -1.2263172531261852, 'y': -3}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:39:06,044] Trial 41 finished with value: 28.53440099948068 and parameters: {'x': -1.8800002658193113, 'y': -5}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:39:08,968] Trial 42 finished with value: 0.001473477071447486 and parameters: {'x': 0.0383858967779507, 'y': 0}. Best is trial 21 with value: 9.636067028475094e-05.\n[I 2025-04-01 16:39:11,901] Trial 43 finished with value: 2.650858002662734e-05 and parameters: {'x': 0.005148648368904924, 'y': 0}. Best is trial 43 with value: 2.650858002662734e-05.\n[I 2025-04-01 16:39:14,709] Trial 44 finished with value: 8.857151426657968 and parameters: {'x': 2.2038946042535628, 'y': -2}. Best is trial 43 with value: 2.650858002662734e-05.\n[I 2025-04-01 16:39:17,668] Trial 45 finished with value: 5.203730318305035 and parameters: {'x': 1.0971464434181222, 'y': 2}. Best is trial 43 with value: 2.650858002662734e-05.\n[I 2025-04-01 16:39:21,350] Trial 46 finished with value: 2.6508579156204505e-05 and parameters: {'x': 0.005148648284375668, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:39:26,287] Trial 47 finished with value: 0.00043701532815174614 and parameters: {'x': -0.020904911579620378, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:39:31,039] Trial 48 finished with value: 0.00036555663349812766 and parameters: {'x': -0.019119535389180556, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:39:35,463] Trial 49 finished with value: 10.79142484765133 and parameters: {'x': 3.285030418070939, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:39:40,290] Trial 50 finished with value: 105.28504695387142 and parameters: {'x': 7.502336099767286, 'y': 7}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:39:44,812] Trial 51 finished with value: 0.022527163549285294 and parameters: {'x': -0.1500905178526788, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:39:49,722] Trial 52 finished with value: 107.33007068634281 and parameters: {'x': 9.916152010046176, 'y': 3}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:39:54,530] Trial 53 finished with value: 0.01342921684419811 and parameters: {'x': 0.11588449785971422, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:39:59,087] Trial 54 finished with value: 104.67396679788132 and parameters: {'x': -9.78130700867125, 'y': 3}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:04,284] Trial 55 finished with value: 105.77303675557651 and parameters: {'x': -4.977251928080044, 'y': 9}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:09,397] Trial 56 finished with value: 34.01156750877837 and parameters: {'x': 4.24400371215417, 'y': 4}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:14,254] Trial 57 finished with value: 0.0002479021525715667 and parameters: {'x': 0.01574490878257371, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:19,043] Trial 58 finished with value: 3.3876233299940875e-05 and parameters: {'x': 0.005820329311984063, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:23,888] Trial 59 finished with value: 25.04871099026803 and parameters: {'x': -4.587887421272239, 'y': -2}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:28,379] Trial 60 finished with value: 0.0011841064859292698 and parameters: {'x': 0.0344108483756107, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:33,098] Trial 61 finished with value: 10.2687930442682 and parameters: {'x': -1.1264071396560835, 'y': 3}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:37,987] Trial 62 finished with value: 76.01840303612826 and parameters: {'x': -3.4667568469865704, 'y': -8}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:41,477] Trial 63 finished with value: 3.1696493708616046e-05 and parameters: {'x': 0.005629963917168213, 'y': 0}. Best is trial 46 with value: 2.6508579156204505e-05.\n[I 2025-04-01 16:40:44,090] Trial 64 finished with value: 5.745007437864691e-07 and parameters: {'x': -0.0007579582731169765, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:40:46,853] Trial 65 finished with value: 0.003951507858628707 and parameters: {'x': -0.06286102018444106, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:40:49,566] Trial 66 finished with value: 0.000630680400038285 and parameters: {'x': 0.025113351031638233, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:40:52,351] Trial 67 finished with value: 0.001349768114183672 and parameters: {'x': 0.03673919043996032, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:40:55,186] Trial 68 finished with value: 141.02167102934646 and parameters: {'x': -6.404816236969369, 'y': -10}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:40:57,972] Trial 69 finished with value: 111.44502530985156 and parameters: {'x': -6.888034938199106, 'y': 8}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:00,724] Trial 70 finished with value: 10.85158539108449 and parameters: {'x': -3.1387235289340936, 'y': -1}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:03,486] Trial 71 finished with value: 33.21979859189145 and parameters: {'x': -4.92136145714694, 'y': 3}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:06,389] Trial 72 finished with value: 0.004630030277467062 and parameters: {'x': 0.0680443258285881, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:09,242] Trial 73 finished with value: 0.00402698227786356 and parameters: {'x': 0.063458508317353, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:11,968] Trial 74 finished with value: 0.000533644677186676 and parameters: {'x': 0.02310075057626215, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:14,927] Trial 75 finished with value: 0.006031741214975785 and parameters: {'x': -0.0776642853245672, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:17,737] Trial 76 finished with value: 17.313959648514846 and parameters: {'x': 1.1462807895602394, 'y': -4}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:21,710] Trial 77 finished with value: 114.28177405871672 and parameters: {'x': 9.913716460476198, 'y': -4}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:26,540] Trial 78 finished with value: 41.64000518487155 and parameters: {'x': 6.135145082626127, 'y': -2}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:31,381] Trial 79 finished with value: 98.97437743172753 and parameters: {'x': -9.948586705242485, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:34,225] Trial 80 finished with value: 60.0041477159114 and parameters: {'x': -7.681415736432406, 'y': -1}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:37,184] Trial 81 finished with value: 179.78035029445465 and parameters: {'x': 9.938830428901312, 'y': 9}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:40,214] Trial 82 finished with value: 18.642179859059105 and parameters: {'x': 4.20025949901421, 'y': -1}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:43,211] Trial 83 finished with value: 42.90323445933595 and parameters: {'x': 2.6274007039916754, 'y': -6}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:45,989] Trial 84 finished with value: 7.426747824616066e-05 and parameters: {'x': 0.008617858100836928, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:49,020] Trial 85 finished with value: 60.12041865011191 and parameters: {'x': 7.491356262394141, 'y': -2}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:52,030] Trial 86 finished with value: 34.82854771238608 and parameters: {'x': -3.1350514688575815, 'y': 5}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:55,042] Trial 87 finished with value: 0.0002869266381500812 and parameters: {'x': 0.016938909001174816, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:41:59,674] Trial 88 finished with value: 194.72991699547543 and parameters: {'x': -9.732929517646546, 'y': 10}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:04,825] Trial 89 finished with value: 103.38524805706464 and parameters: {'x': 1.8399043608472287, 'y': -10}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:09,775] Trial 90 finished with value: 0.0002801699529325217 and parameters: {'x': 0.016738278075492764, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:14,463] Trial 91 finished with value: 0.00014544711387567405 and parameters: {'x': 0.012060145682191159, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:19,253] Trial 92 finished with value: 37.7622175236923 and parameters: {'x': 5.810526441183475, 'y': 2}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:23,864] Trial 93 finished with value: 198.8877342541241 and parameters: {'x': -9.944231204780191, 'y': -10}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:28,310] Trial 94 finished with value: 0.0002071210736447342 and parameters: {'x': -0.014391701554879958, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:33,211] Trial 95 finished with value: 9.004373611531859 and parameters: {'x': -0.06613328611114912, 'y': -3}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:38,024] Trial 96 finished with value: 5.953877382731043e-06 and parameters: {'x': -0.0024400568400615268, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:42,527] Trial 97 finished with value: 25.006497689242774 and parameters: {'x': 0.08060824550115919, 'y': -5}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:47,146] Trial 98 finished with value: 6.048223520406065e-05 and parameters: {'x': -0.007777032544876011, 'y': 0}. Best is trial 64 with value: 5.745007437864691e-07.\n[I 2025-04-01 16:42:51,878] Trial 99 finished with value: 16.023249835080687 and parameters: {'x': -0.15247896602707822, 'y': 4}. Best is trial 64 with value: 5.745007437864691e-07.\n{'x': -0.0007579582731169765, 'y': 0}\n```\n\n### Steps to reproduce\n\n1. Run code with search_space\n2. Run code without search_space\n\nWith search_space:\n```python\nimport optuna\nimport optunahub\n\n\nmodule = optunahub.load_module(\"samplers/hebo\")\nHEBOSampler = module.HEBOSampler\n\n\ndef objective(trial: optuna.trial.Trial) -> float:\n    x = trial.suggest_float(\"x\", -10, 10)\n    y = trial.suggest_int(\"y\", -10, 10)\n    return x**2 + y**2\n\nsearch_space = {\n    \"x\": optuna.distributions.FloatDistribution(-10, 10),\n    \"y\": optuna.distributions.IntDistribution(-10, 10)\n}\n\nif __name__ == \"__main__\":\n    sampler = HEBOSampler(seed=42, search_space=search_space)\n    study = optuna.create_study(sampler=sampler)\n    study.optimize(objective, n_trials=100)\n    print(study.best_trial.params)\n```\n\nWithout search_space:\n```python\nimport optuna\nimport optunahub\n\n\nmodule = optunahub.load_module(\"samplers/hebo\")\nHEBOSampler = module.HEBOSampler\n\n\ndef objective(trial: optuna.trial.Trial) -> float:\n    x = trial.suggest_float(\"x\", -10, 10)\n    y = trial.suggest_int(\"y\", -10, 10)\n    return x**2 + y**2\n\nif __name__ == \"__main__\":\n    sampler = HEBOSampler(seed=42)\n    study = optuna.create_study(sampler=sampler)\n    study.optimize(objective, n_trials=100)\n    print(study.best_trial.params)\n```\n\nOptional code using original HEBO repository:\n\n```python\n\nimport numpy as np\nimport pandas as pd\nfrom hebo.design_space.design_space import DesignSpace\nfrom hebo.optimizers.hebo import HEBO\n\ndef objective(params: pd.DataFrame) -> np.ndarray:\n    x = params['x'].values\n    y = params['y'].values\n    return (x**2 + y**2).reshape(-1, 1)\n\nspace_config = [\n    {'name': 'x', 'type': 'num', 'lb': -10, 'ub': 10},\n    {'name': 'y', 'type': 'num', 'lb': -10, 'ub': 10}\n]\nspace = DesignSpace().parse(space_config)\n\noptimizer = HEBO(space)\n\nfor i in range(50):  # Number of iterations\n    suggestions = optimizer.suggest(n_suggestions=1)\n    objective_values = objective(suggestions)\n    optimizer.observe(suggestions, objective_values)\n    best_value = optimizer.y.min()\n    print(f\"Iteration {i + 1}: Best objective value = {best_value}\")\n\n````\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "y0z",
        "body": "@rafapm Thank you for your report!\n\nI've made a PR to fix this issue.\nhttps://github.com/optuna/optunahub-registry/pull/258"
      },
      {
        "user": "rafapm",
        "body": "@y0z thank you so much for addressing this in such a timely manner!"
      },
      {
        "user": "rafapm",
        "body": "@y0z Thank you once again for providing the fix. I’ve verified that it is working as expected. However, I haven't observed a noticeable improvement in speed. I ran the same code I shared previously multiple times, both with and without the search_space. In most of the test runs, the execution time remained similar, and in some cases, using a search_space was actually slower than not using it, while the reverse also occurred at times.\n\nI would greatly appreciate it if you could provide more details on the benefits of using a search_space, as the official documentation mentions that \"by specifying search_space, the sampling speed at each iteration becomes slightly quicker.\"\n\nLooking forward to your insights!"
      }
    ]
  },
  {
    "issue_number": 5806,
    "title": "Make error message for `create_study`'s `direction` easier to understand",
    "author": "not522",
    "state": "closed",
    "created_at": "2024-11-29T05:31:33Z",
    "updated_at": "2025-04-01T04:55:13Z",
    "labels": [
      "code-fix",
      "contribution-welcome"
    ],
    "body": "### Motivation\n\nUsers sometimes make the mistake of passing the list of directions to the `direction` argument. The error message for this mistake is hard to understand what is the problem.\n\n* Correct\n\n```python\noptuna.create_study(directions=[\"minimize\", \"minimize\"])\n```\n\n* Wrong\n\nUsing `direction` instead of `directions`\n\n```python\noptuna.create_study(direction=[\"minimize\", \"minimize\"])\n```\n\n```\nValueError: Please set either 'minimize' or 'maximize' to direction. You can also set the corresponding `StudyDirection` member.\n```\n\n### Suggestion\n\nWe can make the error message more helpful for the mistake. It can be something like \"for multi-objective optimization, using directions instead of direction\".\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "unKnownNG",
        "body": "Hello,\r\nI would like to take this issue. Can I know which file I have to change it for the error message. There are many files so it is confusing. \r\n\r\nThank you."
      },
      {
        "user": "not522",
        "body": "Thanks! It's `study/study.py`.\r\n\r\nhttps://github.com/optuna/optuna/blob/b29fdff87e46fd1c1e025e23af005c9a9ad80a11/optuna/study/study.py#L1254-L1257"
      }
    ]
  },
  {
    "issue_number": 5958,
    "title": "Optuna Trials Still Run Sequentially in Ray Tune Despite External Storage Configuration, Limiting Parallelization",
    "author": "Aricept094",
    "state": "open",
    "created_at": "2025-02-03T06:22:14Z",
    "updated_at": "2025-03-31T15:05:42Z",
    "labels": [
      "feature",
      "needs-discussion"
    ],
    "body": "### Expected behavior\n\nRay Tune with OptunaSearch, when configured with external storage, should execute multiple trials in parallel, effectively utilizing available CPU resources, similar to other Ray Tune search algorithms.\n\n### Environment\n\n- Optuna version: 4.2.0\n- Python version: 3.10.16\n- OS: Linux-6.6.36.3-microsoft-standard-WSL2-x86_64-with-glibc2.35\n- (Optional) Other libraries and their versions:\n    - Ray version: 2.41.0\n\n### Error messages, stack traces, or logs\n\n```shell\nNo specific error messages, stack traces, or logs are produced. The issue is with the observed behavior where trials are executed sequentially instead of in parallel, despite configurations intended for parallel execution. The Ray Tune dashboard shows trials queuing up and running one after another, rather than concurrently utilizing available resources.\n```\n\n### Steps to reproduce\n\n\n```python\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport ray\nfrom ray import tune\nfrom ray.tune.search.optuna import OptunaSearch\nfrom ray import train, tune\nimport optuna\n\ndef generate_data(n_samples=1000):\n    X = np.random.rand(n_samples, 5)\n    y = 2 * X[:, 0] + 3 * X[:, 1] - X[:, 2] + 0.5 * X[:, 3] - 1.5 * X[:, 4] + np.random.normal(0, 0.1, n_samples)\n    return X, y\n\ndef train_random_forest(config):\n    X, y = generate_data()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    rf = RandomForestRegressor(\n        n_estimators=config[\"n_estimators\"],\n    )\n\n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n\n    train.report({\"mean_squared_error\": mse,})\n\n\ndef main():\n    ray.init()\n\n    config = {\n        \"n_estimators\": tune.randint(10, 200),\n    }\n\n    analysis = tune.run(\n        train_random_forest,\n        config=config,\n        num_samples=100,\n        search_alg=OptunaSearch(storage=optuna.storages.RDBStorage(\"sqlite:///example.db\", heartbeat_interval=1), study_name=\"example_study\"), # Added external storage here\n        metric=\"mean_squared_error\",\n        mode=\"min\",\n        reuse_actors=True\n    )\n\n    print(\"Best config:\", analysis.best_config)\n    print(\"Best MSE:\", analysis.best_result[\"mean_squared_error\"])\n\nif __name__ == \"__main__\":\n    main()\n```\n\n\n### Additional context (optional)\n\nThis issue follows up on the previously closed issue #5611, where performance and parallelization limitations with Optuna in Ray Tune were initially reported due to the default in-memory storage. https://github.com/ray-project/ray/pull/48547 was intended to resolve this by enabling custom Optuna storage backends via the storage parameter in OptunaSearch.\n\nHowever, even after configuring external storage (RDB using SQLite in the provided example), trials continue to execute sequentially. This is contrary to the expected behavior and the parallel execution observed with other Ray Tune search algorithms. Attempts to control parallelism using max_concurrent_trials and set_max_concurrency() have been unsuccessful.\n\nAs a workaround, parallelization can be achieved by running multiple independent Python script instances using Optuna directly with external storage. However, this workaround introduces a new challenge with experiment tracking using Weights & Biases (WandB). When using WandB in this setup, each Python script instance registers as a separate WandB run. This results in fragmented and disjointed WandB logs, making it impossible to get a holistic view of the entire hyperparameter tuning process within a single, consolidated WandB run.\n\nThis WandB fragmentation issue is a primary reason for seeking a solution within Ray Tune. While I was able to parallelize Optuna using independent scripts, the lack of consolidated WandB tracking forced me to revert to using Ray Tune (without Optuna) to maintain proper experiment logging. Ideally, I would like to use Ray Tune with Optuna and have both parallel trial execution and a single, unified WandB run for the entire hyperparameter optimization process.\n\nTherefore, the core issue is the lack of parallel trial execution in Ray Tune when using OptunaSearch even with external storage configured, which significantly impacts performance and experiment efficiency. Guidance on achieving proper parallelization within Ray Tune with Optuna and potential solutions for the WandB fragmentation issue in the workaround scenario would be greatly appreciated.",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "Do you think making another study and adding all the trials from the study of interest to the new study solves your problem?\n\nBy doing so, you can see the entire study in one log in a posthoc manner.\n\nhttps://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.add_trials"
      },
      {
        "user": "Aricept094",
        "body": "Thank you for the suggestion! Yes, I've successfully implemented a solution using multiple studies and trial aggregation, though with some additional considerations to handle the Weights & Biases (WandB) logging effectively. Here's how I solved the parallelization and logging challenges\n\nI created two separate Python scripts to handle:\n\n1. Parallel Optuna optimization with distributed trials\n2. Post-processing to consolidate WandB logging\n\n1. Parallel Optuna Execution with WandB Logging\n\nThis script creates a shared Optuna study (using a shared SQLite storage) so that multiple independent processes can work on the same study. Each process runs several trials (targeting a total of 500 completed trials) and logs each trial with the WandB callback. The WandB runs are registered separately for each instance, which works well for parallel optimization but fragments the experiment log.\n\n```python \n\nimport numpy as np\nimport optuna\nfrom optuna.integration.wandb import WeightsAndBiasesCallback\nimport wandb\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ndef generate_data(n_samples=1000):\n    X = np.random.rand(n_samples, 5)\n    y = (2 * X[:, 0] + 3 * X[:, 1] - X[:, 2] +\n         0.5 * X[:, 3] - 1.5 * X[:, 4] +\n         np.random.normal(0, 0.1, n_samples))\n    return X, y\n\ndef train_random_forest(trial):\n    X, y = generate_data()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Suggest hyperparameters\n    n_estimators = trial.suggest_int(\"n_estimators\", 10, 200)\n    rf = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n    \n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    return mse\n\nif __name__ == \"__main__\":\n    # Ensure you are logged into WandB (via CLI using wandb login or via API key)\n    wandb.login()\n    \n    # Setup WandB callback with your project configuration\n    wandb_kwargs = {\"project\": \"wssw\"}\n    wandb_callback = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)\n    \n    # Callback to terminate after 500 completed trials (across all instances)\n    terminate_callback = optuna.study.MaxTrialsCallback(n_trials=500, \n                                                        states=(optuna.trial.TrialState.COMPLETE,))\n    \n    # Create a shared study using SQLite storage so that multiple processes work on the same study.\n    study = optuna.create_study(storage=\"sqlite:///wssw.db\",\n                                study_name=\"wssw\",\n                                load_if_exists=True,\n                                direction=\"minimize\")\n    \n    # Run optimization concurrently across processes.\n    study.optimize(train_random_forest, n_trials=800,\n                   callbacks=[terminate_callback, wandb_callback])\n    \n    best_trial = study.best_trial\n    print(\"Best Params:\", best_trial.params)\n    print(\"Best MSE:\", best_trial.value)\n\n```\n\n2. Converting and Consolidating WandB Offline Runs\n\nBecause WandB performs best when each Optuna trial is a separate run—and to avoid the rate-limiting issues encountered with simultaneous online requests—I implemented a second script to convert the 8 fragmented runs into one unified study. Initially, I attempted to parallelize the conversion process using multi-processing, but that led to too many simultaneous network requests (resulting in errors). Therefore, I switched to logging the conversion in offline mode.\n\nThis conversion script loads the merged study and then logs each trial as a separate WandB run in offline mode. Once all trials are logged, I sync them to WandB using a command-line tool. Here’s the conversion script:\n\n```python \nimport optuna\nimport wandb\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport subprocess\n\ndef log_trial(trial):\n    # Each process creates its own WandB run in offline mode to prevent rate limit issues.\n    run = wandb.init(\n        project=\"wsswo\",\n        name=f\"trial_{trial.number}\",\n        reinit=True,       # ensures a new run is created\n        job_type=\"trial\",\n        mode=\"offline\"     # log offline to avoid network calls\n    )\n    # Log trial parameters and metrics.\n    run.config.update(trial.params)\n    run.log({\n        \"objective\": trial.value,\n        \"metric\": trial.value,  # explicitly log metric\n        \"n_estimators\": trial.params.get(\"n_estimators\")\n    }, step=0)\n    run.finish()\n    return f\"Trial {trial.number} logged offline.\"\n\ndef parallel_log_trials(merged_study, max_workers=16):\n    trials = merged_study.trials\n    results = []\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Submit logging jobs concurrently.\n        future_to_trial = {executor.submit(log_trial, trial): trial for trial in trials}\n        for future in as_completed(future_to_trial):\n            result = future.result()\n            results.append(result)\n            print(result)\n    return results\n\ndef sync_offline_runs(sync_path=\"./wandb\"):\n    # Sync all offline-run directories to the cloud.\n    result = subprocess.run([\"wandb\", \"sync\", sync_path], capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Error syncing offline runs:\\n\", result.stderr)\n    else:\n        print(\"Offline runs synced successfully:\\n\", result.stdout)\n\ndef main():\n    # Load the merged study (with all trials from the parallel runs)\n    merged_study = optuna.load_study(\n        study_name=\"wssw\",\n        storage=\"sqlite:///wssw.db\"\n    )\n    # Log each trial as a separate offline WandB run.\n    parallel_log_trials(merged_study, max_workers=16)\n    print(\"Finished logging all trials offline to WandB.\")\n    \n    # Once logging is complete, sync the offline runs to WandB.\n    sync_offline_runs()  # Adjust sync_path if needed\n\nif __name__ == \"__main__\":\n    main()\n```\n\nFinally, I used the following command to sync the offline runs to a new WandB project (so that each Optuna trial appears as a separate run):\n\n```bash\nfind /home/aricept094/python/wandb -maxdepth 1 -type d -name 'offline-run-*' | parallel -j 16 wandb sync --project wsswo --include-offline {}\n\n```\n\nI appreciate your feedback and hope this detailed explanation clarifies the approach I took.\n\nI have a question regarding the potential impact of the fragmentation approach on hyperparameter tuning with Optuna. While merging multiple parallel runs into a consolidated study effectively addresses the WandB fragmentation issue, I’m a bit cautious about its overall efficiency. In theory, if the study’s storage remains consistent, the performance should mirror that of a single-process execution. \n\nThe current tests have been run on a simplified model configuration. I still need to Validate the approach on a more complex, full-scale model with a broader set of parameters.\n\nThank you agian"
      },
      {
        "user": "ishaan-mehta",
        "body": "Hi @Aricept094 — thank you for your detailed write-ups surrounding your problems and solutions! Just to confirm, does this mean that you have not yet been able to find a way to use Optuna via Ray Tune such that trials are run in parallel?"
      }
    ]
  },
  {
    "issue_number": 5388,
    "title": "Is there any plan for Maximal Update Parametrization support ?",
    "author": "starlitsky2010",
    "state": "closed",
    "created_at": "2024-04-11T06:18:24Z",
    "updated_at": "2025-03-30T23:05:43Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nSupport Maximal Update Parametrization as it could help get optimal HPs from small proxy models and apply them on large LLMs\r\nIs there any plan for adding this feature?\r\nMore details, please refer to : https://github.com/microsoft/mup\n\n### Description\n\nIt will acclerate the LLMs development as optimal HPs could be found by small proxy models and apply them on large LLMS. And this will save a lot of training cost. So there are quite a lot benefits for the LLM community.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "contramundum53",
        "body": "Thank you for your suggestion!\r\nMuP needs tightly coupled model implementation (i.e. you need MuReadout at appropriate place and also it has a special attention module) and optimizer (i.e. you need MuAdam/MuSGD). It's not something that can be implemented in Optuna and that users can forget everything about, but is more an application.\r\nYou need to carefully write your model & your training pipeline (correctly!) with the mup library, and after that Optuna can help with the hyperparameter tuning."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5349,
    "title": "Support for duckdb",
    "author": "jerome-f",
    "state": "closed",
    "created_at": "2024-03-20T20:09:40Z",
    "updated_at": "2025-03-25T23:06:01Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nSupport for duckdb as a RDB. Sqlalchemy already supports duckdb with duckdb-engine, it can easily be substituted as a drop in for sqlite. Avoids the database lock issue\n\n### Description\n\nsupport duckdb in optuna.storages \n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "not522",
        "body": "Thank you for your feedback! I'm not familiar with duckdb, but I think it already works using URL (`duckdb:///...`) as the storage url if the SQLAlchemy support is implemented."
      },
      {
        "user": "jerome-f",
        "body": "@not522 thanks for looking into it and yes sqlalchemy does support duckdb but I got errors in API call. duckdb is same convention as sqlite but allows for seamless integration with many python libraries (pandas, parquet I/O) and [avoids database lock](https://duckdb.org/docs/connect/concurrency) issue and it is more [efficient, fast and small disk](https://duckdblabs.github.io/db-benchmark/) footprint. I ran the following code \r\n\r\n```\r\nimport optuna\r\n\r\ndef objective(trial):\r\n    x = trial.suggest_float(\"x\", -10, 10)\r\n    return (x - 2) ** 2\r\nstorage = optuna.storages.RDBStorage(url=f\"duckdb:///test.db\")\r\nstudy = optuna.load_study(study_name=\"distributed-example\", storage=storage)\r\nstudy.optimize(objective, n_trials=100)\r\n\r\n```\r\n\r\nand it generates the following error which is related to the **SERIAL keyword in db creation**. In  **DuckDB the keyword INTEGER or BIGINT along with the AUTO_INCREMENT** keyword to achieve similar functionality as SERIAL. But I am not familiar with api calls to sqlalchemy so posted this here. \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nCatalogException                          Traceback (most recent call last)\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1971, in Connection._exec_single_context(self, dialect, context, statement, parameters)\r\n   1970     if not evt_handled:\r\n-> 1971         self.dialect.do_execute(\r\n   1972             cursor, str_statement, effective_parameters, context\r\n   1973         )\r\n   1975 if self._has_events or self.engine._has_events:\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/default.py:919, in DefaultDialect.do_execute(self, cursor, statement, parameters, context)\r\n    918 def do_execute(self, cursor, statement, parameters, context=None):\r\n--> 919     cursor.execute(statement, parameters)\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/duckdb_engine/__init__.py:163, in ConnectionWrapper.execute(self, statement, parameters, context)\r\n    162     else:\r\n--> 163         self.__c.execute(statement, parameters)\r\n    164 except RuntimeError as e:\r\n\r\nCatalogException: Catalog Error: Type with name SERIAL does not exist!\r\nDid you mean \"real\"?\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nProgrammingError                          Traceback (most recent call last)\r\nCell In[3], line 6\r\n      4     x = trial.suggest_float(\"x\", -10, 10)\r\n      5     return (x - 2) ** 2\r\n----> 6 storage = optuna.storages.RDBStorage(url=f\"duckdb:///test.db\")\r\n      7 study = optuna.load_study(study_name=\"distributed-example\", storage=storage)\r\n      8 study.optimize(objective, n_trials=100)\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/optuna/storages/_rdb/storage.py:229, in RDBStorage.__init__(self, url, engine_kwargs, skip_compatibility_check, heartbeat_interval, grace_period, failed_trial_callback, skip_table_creation)\r\n    225 self.scoped_session = sqlalchemy_orm.scoped_session(\r\n    226     sqlalchemy_orm.sessionmaker(bind=self.engine)\r\n    227 )\r\n    228 if not skip_table_creation:\r\n--> 229     models.BaseModel.metadata.create_all(self.engine)\r\n    231 self._version_manager = _VersionManager(self.url, self.engine, self.scoped_session)\r\n    232 if not skip_compatibility_check:\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/sql/schema.py:5825, in MetaData.create_all(self, bind, tables, checkfirst)\r\n   5801 def create_all(\r\n   5802     self,\r\n   5803     bind: _CreateDropBind,\r\n   5804     tables: Optional[_typing_Sequence[Table]] = None,\r\n   5805     checkfirst: bool = True,\r\n   5806 ) -> None:\r\n   5807     \"\"\"Create all tables stored in this metadata.\r\n   5808 \r\n   5809     Conditional by default, will not attempt to recreate tables already\r\n   (...)\r\n   5823 \r\n   5824     \"\"\"\r\n-> 5825     bind._run_ddl_visitor(\r\n   5826         ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables\r\n   5827     )\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/base.py:3255, in Engine._run_ddl_visitor(self, visitorcallable, element, **kwargs)\r\n   3248 def _run_ddl_visitor(\r\n   3249     self,\r\n   3250     visitorcallable: Type[Union[SchemaGenerator, SchemaDropper]],\r\n   3251     element: SchemaItem,\r\n   3252     **kwargs: Any,\r\n   3253 ) -> None:\r\n   3254     with self.begin() as conn:\r\n-> 3255         conn._run_ddl_visitor(visitorcallable, element, **kwargs)\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2461, in Connection._run_ddl_visitor(self, visitorcallable, element, **kwargs)\r\n   2449 def _run_ddl_visitor(\r\n   2450     self,\r\n   2451     visitorcallable: Type[Union[SchemaGenerator, SchemaDropper]],\r\n   2452     element: SchemaItem,\r\n   2453     **kwargs: Any,\r\n   2454 ) -> None:\r\n   2455     \"\"\"run a DDL visitor.\r\n   2456 \r\n   2457     This method is only here so that the MockConnection can change the\r\n   2458     options given to the visitor so that \"checkfirst\" is skipped.\r\n   2459 \r\n   2460     \"\"\"\r\n-> 2461     visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/sql/visitors.py:664, in ExternalTraversal.traverse_single(self, obj, **kw)\r\n    662 meth = getattr(v, \"visit_%s\" % obj.__visit_name__, None)\r\n    663 if meth:\r\n--> 664     return meth(obj, **kw)\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/sql/ddl.py:918, in SchemaGenerator.visit_metadata(self, metadata)\r\n    916 for table, fkcs in collection:\r\n    917     if table is not None:\r\n--> 918         self.traverse_single(\r\n    919             table,\r\n    920             create_ok=True,\r\n    921             include_foreign_key_constraints=fkcs,\r\n    922             _is_metadata_operation=True,\r\n    923         )\r\n    924     else:\r\n    925         for fkc in fkcs:\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/sql/visitors.py:664, in ExternalTraversal.traverse_single(self, obj, **kw)\r\n    662 meth = getattr(v, \"visit_%s\" % obj.__visit_name__, None)\r\n    663 if meth:\r\n--> 664     return meth(obj, **kw)\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/sql/ddl.py:956, in SchemaGenerator.visit_table(self, table, create_ok, include_foreign_key_constraints, _is_metadata_operation)\r\n    947 if not self.dialect.supports_alter:\r\n    948     # e.g., don't omit any foreign key constraints\r\n    949     include_foreign_key_constraints = None\r\n    951 CreateTable(\r\n    952     table,\r\n    953     include_foreign_key_constraints=(\r\n    954         include_foreign_key_constraints\r\n    955     ),\r\n--> 956 )._invoke_with(self.connection)\r\n    958 if hasattr(table, \"indexes\"):\r\n    959     for index in table.indexes:\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/sql/ddl.py:314, in ExecutableDDLElement._invoke_with(self, bind)\r\n    312 def _invoke_with(self, bind):\r\n    313     if self._should_execute(self.target, bind):\r\n--> 314         return bind.execute(self)\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1422, in Connection.execute(self, statement, parameters, execution_options)\r\n   1420     raise exc.ObjectNotExecutableError(statement) from err\r\n   1421 else:\r\n-> 1422     return meth(\r\n   1423         self,\r\n   1424         distilled_parameters,\r\n   1425         execution_options or NO_OPTIONS,\r\n   1426     )\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/sql/ddl.py:180, in ExecutableDDLElement._execute_on_connection(self, connection, distilled_params, execution_options)\r\n    177 def _execute_on_connection(\r\n    178     self, connection, distilled_params, execution_options\r\n    179 ):\r\n--> 180     return connection._execute_ddl(\r\n    181         self, distilled_params, execution_options\r\n    182     )\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1533, in Connection._execute_ddl(self, ddl, distilled_parameters, execution_options)\r\n   1528 dialect = self.dialect\r\n   1530 compiled = ddl.compile(\r\n   1531     dialect=dialect, schema_translate_map=schema_translate_map\r\n   1532 )\r\n-> 1533 ret = self._execute_context(\r\n   1534     dialect,\r\n   1535     dialect.execution_ctx_cls._init_ddl,\r\n   1536     compiled,\r\n   1537     None,\r\n   1538     exec_opts,\r\n   1539     compiled,\r\n   1540 )\r\n   1541 if self._has_events or self.engine._has_events:\r\n   1542     self.dispatch.after_execute(\r\n   1543         self,\r\n   1544         ddl,\r\n   (...)\r\n   1548         ret,\r\n   1549     )\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1850, in Connection._execute_context(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\r\n   1848     return self._exec_insertmany_context(dialect, context)\r\n   1849 else:\r\n-> 1850     return self._exec_single_context(\r\n   1851         dialect, context, statement, parameters\r\n   1852     )\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1990, in Connection._exec_single_context(self, dialect, context, statement, parameters)\r\n   1987     result = context._setup_result_proxy()\r\n   1989 except BaseException as e:\r\n-> 1990     self._handle_dbapi_exception(\r\n   1991         e, str_statement, effective_parameters, cursor, context\r\n   1992     )\r\n   1994 return result\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2357, in Connection._handle_dbapi_exception(self, e, statement, parameters, cursor, context, is_sub_exec)\r\n   2355 elif should_wrap:\r\n   2356     assert sqlalchemy_exception is not None\r\n-> 2357     raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\r\n   2358 else:\r\n   2359     assert exc_info[1] is not None\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1971, in Connection._exec_single_context(self, dialect, context, statement, parameters)\r\n   1969                 break\r\n   1970     if not evt_handled:\r\n-> 1971         self.dialect.do_execute(\r\n   1972             cursor, str_statement, effective_parameters, context\r\n   1973         )\r\n   1975 if self._has_events or self.engine._has_events:\r\n   1976     self.dispatch.after_cursor_execute(\r\n   1977         self,\r\n   1978         cursor,\r\n   (...)\r\n   1982         context.executemany,\r\n   1983     )\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/sqlalchemy/engine/default.py:919, in DefaultDialect.do_execute(self, cursor, statement, parameters, context)\r\n    918 def do_execute(self, cursor, statement, parameters, context=None):\r\n--> 919     cursor.execute(statement, parameters)\r\n\r\nFile /mambaforge/lib/python3.11/site-packages/duckdb_engine/__init__.py:163, in ConnectionWrapper.execute(self, statement, parameters, context)\r\n    161         self.__c.execute(statement)\r\n    162     else:\r\n--> 163         self.__c.execute(statement, parameters)\r\n    164 except RuntimeError as e:\r\n    165     if e.args[0].startswith(\"Not implemented Error\"):\r\n\r\nProgrammingError: (duckdb.duckdb.CatalogException) Catalog Error: Type with name SERIAL does not exist!\r\nDid you mean \"real\"?\r\n[SQL: \r\nCREATE TABLE studies (\r\n\tstudy_id SERIAL NOT NULL, \r\n\tstudy_name VARCHAR(512) NOT NULL, \r\n\tPRIMARY KEY (study_id)\r\n)\r\n\r\n]\r\n(Background on this error at: https://sqlalche.me/e/20/f405)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "not522",
        "body": "Sorry for my late response and thank you for your error report. In my understanding, this issue should be addressed on the duckdb side, and I think it's been discussed recently. ref:\r\n- https://github.com/duckdb/duckdb/issues/1768\r\n- https://github.com/Mause/duckdb_engine/issues/289\r\n- https://github.com/Mause/duckdb_engine/pull/866"
      }
    ]
  },
  {
    "issue_number": 6015,
    "title": "Artifact store should be usable without global variables",
    "author": "danielprecog",
    "state": "open",
    "created_at": "2025-03-19T07:45:10Z",
    "updated_at": "2025-03-24T07:13:23Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nRight now, the only way to use an artifact store is to define it as a global variable. This is bad style and restricts usage (for instance, the artifact store must be defined in the same file as the objective function).\n\nSee the [documentation](https://optuna.readthedocs.io/en/stable/reference/artifacts.html). \n\n\n### Description\n\nProposed feature:\nThere should be 2 additional arguments for Study:\n\nartifact_store_type \nartifact_store_name \n\nExample: if artifact_store_type is \"file\" and artifact_store_name is \"my-location\",\nthe study initialization should run and make available the file store FileSystemArtifactStore(base_path=\"my-location\")\n\n\n\nAn additional trial method trial.save_artifact that takes 2 inputs:\nlocal_file_name\nartifact_store_file_name\n\nand copies a local file to the relevant artifact store.\n\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "not522",
        "body": "Thank you for your feedback!\n\nHow about using classes like below? (ref: [How to define objective functions that have own arguments?](https://optuna.readthedocs.io/en/stable/faq.html#how-to-define-objective-functions-that-have-own-arguments))\n\n```python\nclass Objective:\n    def __init__(self, artifact_store):\n        self.artifact_store = artifact_store\n\n    def __call__(self, trial):\n        ... = trial.suggest_float(\"x\", -10, 10)\n        file_path = generate_example(...)\n        upload_artifact(\n            artifact_store=self.artifact_store,\n            file_path=file_path,\n            study_or_trial=trial,\n        )\n```"
      }
    ]
  },
  {
    "issue_number": 3083,
    "title": "Hyperband bracket assignment using hash in parallelization",
    "author": "donny8",
    "state": "closed",
    "created_at": "2021-11-05T01:51:53Z",
    "updated_at": "2025-03-23T07:45:23Z",
    "labels": [
      "bug"
    ],
    "body": "<!-- Please write a clear and concise description of what the bug is. -->\n\nI think there could be a problem when using Hyperband pruner in parallelization.\nIn Hyperband, each trial is assigned to the bracket by computing `hash(\"{}{}\".format(study.studyname, trial.number)`.\nHowever, the hash gives different value for the same input when executed on different terminal.\nA trial n, for example, could be assigned to the bracket 1 at process 1 and the bracket 2 at process 2 , which ruins the benefits of using Hyperband pruner instead of the SuccesiveHalving pruner.\n\n## Expected behavior\n\nA trial needs to be assigned to the identical bracket among each process.\n \n<!-- Please write a clear and concise description of what you expected to happen. -->\n\n## Environment\n\n<!--\nYou can get this information by typing the following:\n```\npython -c 'import optuna; print(optuna.__version__)'\npython -c 'import platform; print(platform.python_version())'\npython -c 'import platform; print(platform.platform())'\n```\n-->\n\n- Optuna version: 2.10.0\n- Python version: 3.6.9\n- OS: Linux\n\n## Error messages, stack traces, or logs\n\nI attached an custom-made log that I'd added in the SuccessiveHalving code, which is a part of the Hyperband.\nThe trials 1, 2, 3, 4 were included in bracket 1 at both process 1 and 2\nHowever, the trials 5, 7, 9 were included in the bracket only at process 2. \n\n**Trials for bracket 1 at process 1**\n![image](https://user-images.githubusercontent.com/55653250/140302703-a7ea8e8a-0f9d-4be1-ae1c-246686535c17.png)\n\n**Trials for the same bracket 1 at process 2**\n![image](https://user-images.githubusercontent.com/55653250/140302720-94e346e1-0984-4278-a278-005534efe9cc.png)\n\n\n## Steps to reproduce\n\n1. Add the following codes at the beginning of the **def _get_competing_values( )** in the **_successive_halving.py**\n```\nfor t in trials :\n    if rung_key in t.system_attrs:\n        compet = [t.system_attrs[rung_key]]\n        print('   Prune SH Get_competing_values: '+ rung_key, ' trial ',t.number,'competing ',compet)\n```\n2. Check if the trials consisting of a bracket differs among each process\n\n## Additional context (optional)\n\n<!-- Please add any other context or screenshots about the problem here. -->\nAs you can see [here](https://docs.python.org/3.3/using/cmdline.html), it was due to the update that hash randomization is turned on after python version 3.3.\nOf course, the randomization could be turned off by setting the PYTHONHASHSEED=0 when executing the python script.\nSince I did not see any warning or suggestion about this problem, I would like to ask if my assertion was legit.\n",
    "comments": [
      {
        "user": "hvy",
        "body": "I think you're right. It's clearly manifested when creating the bracket studies (a study which contains a subset of the trials) in https://github.com/optuna/optuna/blob/v2.10.0/optuna/pruners/_hyperband.py.\r\n\r\nLet me share a somewhat more limited but self contained example that demonstrates the issue that you're describing.\r\n\r\nRun the following script from more than one process and notice that the printed brackets are not the same. SQLite is required.\r\n\r\n```python\r\nfrom collections import defaultdict\r\n\r\nimport optuna as ot\r\n\r\n\r\ndef objective(t):\r\n    x = t.suggest_float(\"x\", 0, 1)\r\n\r\n    for step in range(10):\r\n        t.report(step=step, value=x * step)\r\n        if t.should_prune():\r\n            raise ot.TrialPruned\r\n\r\n    return x\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    study = ot.create_study(\r\n        study_name=\"test_hyperband_3083\",\r\n        storage=\"sqlite:///test_hyperband_3083.db\",\r\n        pruner=ot.pruners.HyperbandPruner(),  # You actually don't need to prune, for this demonstration.\r\n        load_if_exists=True,\r\n    )\r\n    study.optimize(objective, n_trials=10)\r\n\r\n    # Check bracket splits. Using private APIs, only use for testing.\r\n    brackets_to_trial_numbers = defaultdict(list)\r\n    for t in study.trials:\r\n        brackets_to_trial_numbers[study.pruner._get_bracket_id(study, t)].append(t.number)\r\n    print(dict(brackets_to_trial_numbers))\r\n```\r\n\r\nOne approach is to simply replace `hash` with a deterministic algorithm from `hashlib`.\r\n\r\nCc. @HideakiImamura "
      },
      {
        "user": "donny8",
        "body": "I'd executed the script above and assured that trials in the brackets keep changing in parallelization.\r\nI would also consider your suggestion to use `hashlib` instead of `hash`\r\nThank you very much, I really appreciate it."
      },
      {
        "user": "hvy",
        "body": "Thanks for the prompt response.\r\nHow about keeping the issue open until it's been fixed?"
      }
    ]
  },
  {
    "issue_number": 1484,
    "title": "Add option to abort hang trials",
    "author": "yorickreum",
    "state": "closed",
    "created_at": "2020-07-04T13:40:28Z",
    "updated_at": "2025-03-21T14:47:58Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "Sometimes, single trials of my training take excessively long or hang up completely. Then, it were reasonable for optuna to abort these trials and continue with the next trial. This can be implemented by timeouts for the trials.\r\n\r\n## Motivation / Problem\r\n\r\nI let hyperparamter studies run over the night. From time to time, they hang up because of a single trial – this is really annoying and costs me days. The current option, specifying timeouts for the whole study, does not help at all at here.\r\n",
    "comments": [
      {
        "user": "toshihikoyanase",
        "body": "Hello @yorickreum. Thank you for your feature request.\r\nThe execution time of objective functions may depend on suggested hyperparameter value, and I think timeout for each trial is useful.\r\n\r\nI talked with @sile and he suggested applying a timeout decorator to objective functions. I created a notebook to show this workaround. Please take a look at it.\r\n\r\nIt uses [`timeout-decorator`](https://pypi.org/project/timeout-decorator/) as follows:\r\n```python\r\n@timeout_decorator.timeout(5, timeout_exception=optuna.TrialPruned, use_signals=False)\r\ndef objective(trial):\r\n    x = trial.suggest_float(\"x\", 0, 1)\r\n    time.sleep(10)\r\n    return x\r\n\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=3)\r\n```\r\n\r\nhttps://colab.research.google.com/drive/17AaLdO2vBtsfEhBuBwEeRrg21Iwbm8Jg?usp=sharing\r\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "toshihikoyanase",
        "body": "Let me close this issue since it has no recent activities. Please feel free to reopen it or create a new issue."
      }
    ]
  },
  {
    "issue_number": 4653,
    "title": "PQ enum type `TrialValueType` is not comparable with `varchar`",
    "author": "daskol",
    "state": "closed",
    "created_at": "2023-05-04T16:30:10Z",
    "updated_at": "2025-03-16T18:23:36Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\r\n\r\nWith PostgreSQL 14/15 RDBS as a storage backend, it is impossible to perform hyperparameter optimization. The issue is that `optuna` uses enum types which should be explicitely casted to `varchars`.\r\n\r\n### Environment\r\n\r\nFailed on PQ 14/15.\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\nsqlalchemy.exc.ProgrammingError: (psycopg.errors.UndefinedFunction) operator does not exist: trialvaluetype = character varying\r\nLINE 3: ...$3::INTEGER ORDER BY CASE trial_values.value_type WHEN $4::V...\r\n                                                             ^\r\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\r\n[SQL: SELECT trials.trial_id AS trials_trial_id, trials.number AS trials_number, trials.study_id AS trials_study_id, trials.state AS trials_state, trials.datetime_start AS trials_datetime_start, trials.datetime_complete AS trials_datetime_complete \r\nFROM trials JOIN trial_values ON trials.trial_id = trial_values.trial_id \r\nWHERE trials.study_id = %(study_id_1)s::INTEGER AND trials.state = %(state_1)s AND trial_values.objective = %(objective_1)s::INTEGER ORDER BY CASE trial_values.value_type WHEN %(param_1)s::VARCHAR THEN %(param_2)s::INTEGER WHEN %(param_3)s::VARCHAR THEN %(param_4)s::INTEGER WHEN %(param_5)s::VARCHAR THEN %(param_6)s::INTEGER END ASC, trial_values.value ASC \r\n LIMIT %(param_7)s::INTEGER]\r\n[parameters: {'study_id_1': 1, 'state_1': 'COMPLETE', 'objective_1': 0, 'param_1': 'INF_NEG', 'param_2': -1, 'param_3': 'FINITE', 'param_4': 0, 'param_5': 'INF_POS', 'param_6': 1, 'param_7': 1}]\r\n(Background on this error at: https://sqlalche.me/e/20/f405)\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nJust use PostgreSQL 14/15 instead of 10 as in your CI.\r\n\r\n### Additional context (optional)\r\n\r\nIt is worth to note that `optuna` overrides a db driver exception with something senseless.\r\n\r\n```\r\noptuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. \r\n```",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Hi, could you share minimal reproducible codes with us? "
      },
      {
        "user": "daskol",
        "body": "Sorry, but no. I've spent half a day to find work around or solve the issue. It seems that fix introduced in #4422 does not work for newer PSQL. The bug was introduced in v3.1.1. In branch v3.1.0 everything works fine. It seems that other versions (e.g. v3.0.6) are buggy as well. This is kind of floating bug with keeping #4420 in mind."
      },
      {
        "user": "c-bata",
        "body": "@daskol Thank you for your report!\r\n\r\nI tried to reproduce the problem with PostgreSQL v15.2 and Optuna's master branch (rev: 41045cef), but found no issues.\r\n\r\n```\r\n$ pip freeze | grep optuna\r\n-e git+ssh://git@github.com/c-bata/optuna.git@41045cef7a8f5d0e5e4026bf0c5c649b269fc312#egg=optuna\r\n$ docker run -d --rm --platform linux/amd64 -p 5432:5432 -e POSTGRES_USER=root -e POSTGRES_PASSWORD=root -e POSTGRES_DB=optuna --name optuna-postgres postgres:15\r\n$ cat example.py\r\nimport optuna\r\n\r\ndef objective(trial: optuna.Trial) -> float:\r\n    x = trial.suggest_float(\"x\", -10, 10)\r\n    return x ** 2\r\n\r\nif __name__ == '__main__':\r\n    study = optuna.create_study(storage=\"postgresql+psycopg2://root:root@127.0.0.1/optuna\")\r\n    study.optimize(objective, n_trials=100)\r\n    print(study.best_trial)\r\n\r\n$ python example.py\r\n[I 2023-05-08 10:08:48,713] A new study created in RDB with name: no-name-9928f092-7af3-4482-923c-30755b3ebcec\r\n[I 2023-05-08 10:08:48,808] Trial 0 finished with value: 42.396053677841124 and parameters: {'x': -6.51122520558467}. Best is trial 0 with value: 42.396053677841124.\r\n[I 2023-05-08 10:08:48,849] Trial 1 finished with value: 1.0035347584146774 and parameters: {'x': 1.001765820146943}. Best is trial 1 with value: 1.0035347584146774.\r\n[I 2023-05-08 10:08:48,891] Trial 2 finished with value: 0.38741646241748234 and parameters: {'x': -0.6224278772817637}. Best is trial 2 with value: 0.38741646241748234.\r\n[I 2023-05-08 10:08:48,931] Trial 3 finished with value: 75.59820724281214 and parameters: {'x': -8.69472295376984}. Best is trial 2 with value: 0.38741646241748234.\r\n...\r\n\r\n$ docker exec -it fa674cd13cdf bash\r\nroot@fa674cd13cdf:/# psql optuna\r\npsql (15.2 (Debian 15.2-1.pgdg110+1))\r\nType \"help\" for help.\r\n\r\noptuna=# select * from studies;\r\n study_id |                  study_name\r\n----------+----------------------------------------------\r\n        1 | no-name-9928f092-7af3-4482-923c-30755b3ebcec\r\n(1 row)\r\n\r\noptuna=# select * from trials;\r\n trial_id | number | study_id |  state   |       datetime_start       |     datetime_complete\r\n----------+--------+----------+----------+----------------------------+----------------------------\r\n        1 |      0 |        1 | COMPLETE | 2023-05-08 10:08:48.731837 | 2023-05-08 10:08:48.770278\r\n        2 |      1 |        1 | COMPLETE | 2023-05-08 10:08:48.81335  | 2023-05-08 10:08:48.830148\r\n       46 |     45 |        1 | COMPLETE | 2023-05-08 10:08:50.725233 | 2023-05-08 10:08:50.745521\r\n        3 |      2 |        1 | COMPLETE | 2023-05-08 10:08:48.855296 | 2023-05-08 10:08:48.872277\r\n        4 |      3 |        1 | COMPLETE | 2023-05-08 10:08:48.896755 | 2023-05-08 10:08:48.912588\r\n       89 |     88 |        1 | COMPLETE | 2023-05-08 10:08:52.707945 | 2023-05-08 10:08:52.729369\r\n...\r\n```\r\n\r\nIf you could provide more detailed information or conditions, we might be able to fix the issue."
      }
    ]
  },
  {
    "issue_number": 5397,
    "title": "Duration of iteration increases as the number of iterations increase, with distributed optimization.",
    "author": "jt269",
    "state": "closed",
    "created_at": "2024-04-16T08:59:36Z",
    "updated_at": "2025-03-16T14:15:07Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nExpect that runtime of an iteration should not increase so dramatically over time, here is a chart showing the actual increase in iteration duration: \r\n![image](https://github.com/optuna/optuna/assets/135972473/036a3f27-9b99-4ec4-9592-e5944e3c886f)\r\nThe increase in iteration duration decreases the benefits of distributed optimization.\r\n\r\n### Environment\r\n\r\n- Optuna version: 3.6.1\r\n- Python version: 3.11\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\nNone\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\n1. Create an Optuna study with journal file storage for distributed optimisation.\r\n2. The study is launched by a subprocess loop which calls a separate python file with contains the objective function, this also loads the study and starts the optimisation. This creates multiple workers with the idea to maximise resource usage, in this example 4 workers are created.\r\n\r\n```python\r\n# #python code\r\n#parent code\r\n\r\nlock_obj = optuna.storages.JournalFileOpenLock(journal_path)\r\nstorage = optuna.storages.JournalStorage(\r\noptuna.storages.JournalFileStorage(journal_path, lock_obj=lock_obj),\r\n)\r\n\r\n# Create a study\r\nstudy = optuna.create_study(study_name=\"multi_cpu\", sampler=NSGAIISampler(), direction=\"maximize\", storage=storage, load_if_exists=True)\r\n\r\nprocs = [subprocess.Popen([sys.executable, script_to_execute, str(i), csv_path]) for i in range(cpu_num)]\r\n\r\nfor p in procs:\r\n    p.wait()\r\n\r\n# Save results\r\ntimestamp=datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\r\nf_name=f\"{_name}_results_{timestamp}.csv\"\r\ndf = study.trials_dataframe()\r\ndf.to_csv(f_name, index=False)\r\n\r\nprint(study.best_params)\r\nprint(study.best_value)\r\n\r\n#worker code\r\ndef fun(trial):\r\n\r\n    for i in range(num_vars):\r\n        var = trial.suggest_int(f\"x{i}\", lower_bounds_list[i], upper_bounds_list[i], step=1)\r\n\r\n    #send to external program and retrieve result\r\n \r\n    return objective_value\r\n\r\noptuna.logging.set_verbosity(optuna.logging.WARNING)\r\n\r\nlock_obj = optuna.storages.JournalFileOpenLock(url)\r\nstorage = optuna.storages.JournalStorage(\r\n    optuna.storages.JournalFileStorage(url, lock_obj=lock_obj),\r\n)\r\n\r\nstudy = optuna.load_study(\r\n    study_name=\"multi_cpu\", storage=storage\r\n)\r\n\r\nstudy.optimize(fun,n_trials=max_iter, timeout=max_time)\r\n\r\n```\r\n\r\n\r\n### Additional context (optional)\r\n\r\nI have tried various ways to fix this issue and I assume it has to do with the size of the storage as the optimization progresses. Things I have tried:\r\n\r\n1. Using RDB storage with postgresql, same result.\r\n2. Trying to delete older trials as they are no longer relevant to the sampler, loaded the journal as a JSON however the format is not easy to edit. The iteration time decreased if I limited the number of stored trials to 1000 however this caused issues with the dashboard and outputting to pandas dataframe. Also requires optimization to be conducted in a loop which is inconvenient.\r\n3. Trying to switch older trials to a FAIL state however this is not possible.\r\n\r\nThe issue does not appear when using a single worker launched from within the parent code. The increase in iteration duration removes the benefits of distributed optimization.\r\n\r\nI have to create workers using subprocess to be able to connect to multiple instances of the external program where the objective function is calculated.",
    "comments": [
      {
        "user": "nzw0301",
        "body": "As explained in https://optuna.readthedocs.io/en/stable/reference/samplers/index.html, evaluating the objective function takes more time on the number of trials for almost all samplers. So I'm not sure this is a bug. \r\n"
      },
      {
        "user": "jt269",
        "body": "Thanks for your reply, I am using the NSGAIII sampler which is not shown in that table but I assume it would have the same relationship as NSGAII. NSGAII time per trial should be independent of the number of trials completed. As mentioned above, if I use a single worker I do not get the same issue."
      },
      {
        "user": "nzw0301",
        "body": "Thanks. Right, but in `_collect_parent_population` of NSGAIII has for loop over trials, so I'm not sure this sampler is completely independent of the number of finished trials. Sorry I cannot answer clearly because I'm not familiar with this sampler..."
      }
    ]
  },
  {
    "issue_number": 961,
    "title": "How to change schema in postgresql to store the study.",
    "author": "i-Hun",
    "state": "closed",
    "created_at": "2020-02-26T09:06:44Z",
    "updated_at": "2025-03-16T13:29:47Z",
    "labels": [
      "question",
      "stale"
    ],
    "body": "I put postgresql connection string into a`storage`argument in `create_study` method and got an error, since optuna trying to create new table in a public schema, which I can not access. Could someone help me and say how to change the schema to another one?",
    "comments": [
      {
        "user": "toshihikoyanase",
        "body": "Optuna uses [`sqlalchemy`](https://www.sqlalchemy.org/) to handle RDB access, and I think you can change the schema using [database URLs](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls). For example, if your schema is `myschema`, the URL will be as follows:\r\n\r\n`postgres+psycopg2://username:password@localhost:32007/optuna?options=--search_path%3Dmyschema`"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "toshihikoyanase",
        "body": "@i-Hun \r\nLet me close this issue. Please feel free to re-open it or create new issues if you have further problems."
      }
    ]
  },
  {
    "issue_number": 5985,
    "title": "Thompson sampling for categorical variables",
    "author": "samueldmcdermott",
    "state": "closed",
    "created_at": "2025-02-18T17:02:25Z",
    "updated_at": "2025-03-11T14:45:32Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nI think a Thompson sampler would be beneficial for `optuna` by improving coverage of uncertain categorical variables. See [A Tutorial on Thompson Sampling](https://arxiv.org/abs/1707.02038) (I found Fig 2.2 to be an illuminating example) for details and motivations.\n\n### Description\n\nI have written (and used) a subclass of `optuna.samplers.BaseSampler` that implements Thompson sampling for a categorical variable and reverts to a `base_sampler` for other variables. My code introduces one new free parameter (`burn_in`, which runs sequentially for a predetermined number of attempts of the categories during a burn-in phase) and then supplements the sampler with a new dictionary `cat_dict` whose keys are the category choices and whose values are the values of the objective function.\n\nIt has worked well in a test setting that I've used it for (see images). The categories here are four Gaussians: `a` has the second highest mean, but is the broadest, and can attain the highest values; `b` has the highest mean, but is somewhat narrow; `c` and `d` are narrower and are mostly present as controls. The base sampler oversamples `b` and is unable to detect \"good\" fluctuations of `a`. The Thompson sampler samples more appropriately, and is able to observe upward fluctuations for `a`.\n\n![Image](https://github.com/user-attachments/assets/173bcdbb-db81-4939-bd99-3ae9629ad5aa)\n(results with base sampler ☝🏻)\n\n![Image](https://github.com/user-attachments/assets/9a13f563-b343-4e48-8f32-cc26a819e554)\n(results with Thompson sampler ☝🏻)\n\nThe main limitations are:\n- only works for a single categorical variable right now\n    - likely straightforward to change\n    - possible subtleties with nested categories\n- adding to `cat_dict` uses a custom `if` statement\n    - it would be preferable to use `before_trial` or `after_trial`\n    - I was unable to get those to work paradigmatically\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Cool! How about sending a PR to https://github.com/optuna/optunahub-registry, which would be more likely to be merged?"
      },
      {
        "user": "samueldmcdermott",
        "body": "PR over [there](https://github.com/optuna/optunahub-registry/pull/248) now! Feel free to take a look and close this if appropriate"
      },
      {
        "user": "nzw0301",
        "body": "Cool! Let me close this PR once."
      }
    ]
  },
  {
    "issue_number": 6002,
    "title": "S3-based Journal Backend",
    "author": "amitani",
    "state": "closed",
    "created_at": "2025-03-04T23:38:04Z",
    "updated_at": "2025-03-11T08:56:36Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nOur use case doesn't require high-frequency read/writes, and some latency when obtaining suggestions is acceptable. We can use RDS, Redis or NFS, but they all come with some infrastructure management burden. We already use S3 for data and artifacts, so it will be much simpler if we can store the study on AWS S3.\n\n\n### Description\n\nWith conditional writes and read-after-write consistency, AWS S3 provides enough mutual exclusion mechanisms to implement a `BaseJournalBackend` subclass.\n\nhttps://aws.amazon.com/about-aws/whats-new/2024/08/amazon-s3-conditional-writes/\nhttps://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "c-bata",
        "body": "Thank you again for your huge effort! Let me close this issue due to the reason I commented at https://github.com/optuna/optuna/pull/6003#issuecomment-2713208652."
      }
    ]
  },
  {
    "issue_number": 3099,
    "title": "Using constraints with ask/tell interface",
    "author": "alexis-intellegens",
    "state": "closed",
    "created_at": "2021-11-15T10:51:04Z",
    "updated_at": "2025-03-11T02:14:54Z",
    "labels": [
      "question"
    ],
    "body": "Using the NSGAII sampler with the ask/tell interface I'm looking for a simple way to incorporate constraints.\r\n\r\nI am currently using ask() to get trial IDs and suggestions, then computing the objective externally and using tell() to give them back. Where in this process should I compute and return the constraint evaluation?\r\n\r\nMaybe I can do something like:\r\n\r\n`optim.study.trials[0].set_user_attr(\"constraint\", 2)`\r\n\r\nNote: the above line of code doesn't seem to work at all. It doesn't throw an error but it also doesn't actually set user_attrs :(.\r\n\r\nFrom the examples it seems I can set an attribute? Should this be done before tell? Should I ask the study to set the constraint attribute for trial N, then return trial Ns objective?\r\n",
    "comments": [
      {
        "user": "himkt",
        "body": "Hi @alexis-intellegens, thank you for trying Optuna.\r\nI think you can set constraints to trial by invoking `set_user_attr` on `trial` retrieved by `ask`.\r\n\r\n```python\r\nstudy = optuna.create_study()\r\n\r\n# generate trial and suggest a hyperparameter\r\ntrial = study.ask()\r\nparam = trial.suggest(...)\r\n\r\n# set constraint status\r\ntrial.set_user_attr(\"constraint\", 2)\r\n\r\n# evaluate and report\r\nval_metric = evaluation(param)\r\nstudy.tell(trial, val_metric)\r\n```\r\n\r\n`study.trials` is a list of `FrozenTrial` and `set_user_attr` for `FrozenTrial` is effective in-place, which means it only updates an attribute of a `FrozenTrial` object.\r\n\r\nref. `note` in https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.FrozenTrial.html"
      },
      {
        "user": "alexis-intellegens",
        "body": "Cheers, I forgot you could return the trial in study.tell, I'll do that."
      },
      {
        "user": "F18",
        "body": "I have a follow-up question on this same topic. I am using the ask-and-tell interface to ask Optuna for a set of trial parameters (for a given batch size). I then store the trial parameters and numbers and pass them to the objective function to evaluate the entire batch of runs in one-shot (i.e. one call to `evaluation()`). This returns all the objectives and user attributes for the entire batch. \n\nAfter evaluation, I loop through the trial numbers to tell Optuna about the objective values associated with each trial number (using `int` values for the Trial, not `Trial` objects) . In this scenario, how can I also tell Optuna about the user attributes that were evaluated in the batch of runs?\n\n```Python\nstudy = optuna.create_study()\n\n# generate all trials and suggest a hyperparameter\ntrial_list = []\ntrial_numbers = []\nfor _ in range (batch_size):\n    trial = study.ask()\n    param = trial.suggest(...)\n    trial_list.append(trial.params)\n    trial_numbers.append(trial.number)\n\n# evaluate entire batch with a single call to evaluation()\nval_metric, user_attr_1, user_attr2 = evaluation(trial_numbers, trial_list)\n\n# report\nfor trial_number in trial_numbers:\n    study.tell(trial_number, val_metric[trial_number])\n```\n\nWhen reporting, I do not know how to access the trial objects in order to set `user_attr_1` and `user_attr_2`. Any suggestions?"
      }
    ]
  },
  {
    "issue_number": 2985,
    "title": "Tried to import 'plotly' but failed",
    "author": "Greco1899",
    "state": "closed",
    "created_at": "2021-10-05T10:04:32Z",
    "updated_at": "2025-03-10T15:44:54Z",
    "labels": [
      "bug"
    ],
    "body": "<!-- Please write a clear and concise description of what the bug is. -->\r\n\r\n## Expected behavior\r\nPlot of param importances of optimized study.\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n<!--\r\nYou can get this information by typing the following:\r\n```\r\npython -c 'import optuna; print(optuna.__version__)'\r\npython -c 'import platform; print(platform.python_version())'\r\npython -c 'import platform; print(platform.platform())'\r\n```\r\n-->\r\n\r\n- Optuna version: 2.9.1\r\n- Python version: 3.8.5\r\n- OS: Windows 10\r\n- Plotly: 5.3.1\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\nImportError: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n~\\Anaconda3\\lib\\site-packages\\optuna\\visualization\\_plotly_imports.py in <module>\r\n      6 with try_import() as _imports:  # NOQA\r\n----> 7     import plotly  # NOQA\r\n      8     from plotly import __version__ as plotly_version\r\n\r\nModuleNotFoundError: No module named 'plotly'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-46-ec7b157c78f5> in <module>\r\n----> 1 optuna.visualization.plot_param_importances(study)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\optuna\\visualization\\_param_importances.py in plot_param_importances(study, evaluator, params, target, target_name)\r\n     93     \"\"\"\r\n     94 \r\n---> 95     _imports.check()\r\n     96     _check_plot_args(study, target, target_name)\r\n     97 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\optuna\\_imports.py in check(self)\r\n     84         if self._deferred is not None:\r\n     85             exc_value, message = self._deferred\r\n---> 86             raise ImportError(message) from exc_value\r\n     87 \r\n     88 \r\n\r\nImportError: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1. Create objective, optimize study\r\n2. Run optuna.visualization.plot_param_importances(study)\r\n\r\n## Additional context (optional)\r\n\r\nThis issue here https://github.com/optuna/optuna/issues/1339 has been closed but others have continued reporting similar issues.",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Can you use plolty directly not via optuna?"
      },
      {
        "user": "fsmosca",
        "body": "I use the following for visualization.\r\n\r\n```\r\npip install plotly\r\npip install scikit-learn\r\npip install kaleido\r\n```\r\n\r\nPerhaps using matplotlib is simpler with `optuna.visualization.matplotlib.plot_param_importances(study)` and only need matplotlib. I have not tried it yet and I don't know what is the difference between matplotlib and plotly plus scikit-learn plus kaleido."
      },
      {
        "user": "Greco1899",
        "body": "> Can you use plolty directly not via optuna?\r\n\r\nYes I can.\r\n\r\n\r\n\r\n> I use the following for visualization.\r\n> \r\n> ```\r\n> pip install plotly\r\n> pip install scikit-learn\r\n> pip install kaleido\r\n> ```\r\n> \r\n> Perhaps using matplotlib is simpler with `optuna.visualization.matplotlib.plot_param_importances(study)` and only need matplotlib. I have not tried it yet and I don't know what is the difference between matplotlib and plotly plus scikit-learn plus kaleido.\r\n\r\nThanks for the suggestion, plotting directly with matplotlib works for me."
      }
    ]
  },
  {
    "issue_number": 5485,
    "title": "Specify the removing version on `convert_positional_args`",
    "author": "not522",
    "state": "open",
    "created_at": "2024-06-07T08:42:38Z",
    "updated_at": "2025-03-07T07:39:01Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\n`convert_positional_args` does not output the information for `deprecated_version` and `removed_version`. They are necessary to let users know when features will be removed.\n\n### Description\n\nAdding `deprecated_version` and `removed_version` arguments to `convert_positional_args`. They should be aligned with `deprecated_func` and `deprecated_class`.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "fusawa-yugo",
        "body": "I would like to take on it."
      }
    ]
  },
  {
    "issue_number": 5921,
    "title": "`JournalStorage` with `JournalFileSymlinkLock` may fail to resume",
    "author": "not522",
    "state": "closed",
    "created_at": "2025-01-09T09:06:31Z",
    "updated_at": "2025-03-07T03:03:50Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\n`JournalStorage` should be able to resume even if the process terminates unexpectedly. However, if the process terminates suddenly, a lock file may remain and it may not be able to resume.\n\n### Environment\n\n- Optuna version:4.2.0.dev\r\n- Python version:3.11.0\r\n- OS:macOS-15.1-x86_64-i386-64bit\r\n- (Optional) Other libraries and their versions:\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\nIf we encounter the above bug, the resumed process will sleep forever.\n```\n\n\n### Steps to reproduce\n\n1. Terminate the process suddenly (not Ctrl-C; it deletes the lock file correctly.)\r\n2. Resume the optimization process\r\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5948,
    "title": "`_pop_waiting_trial_id` for `InMemoryStorage` is slow",
    "author": "not522",
    "state": "closed",
    "created_at": "2025-01-28T06:00:52Z",
    "updated_at": "2025-03-06T02:25:05Z",
    "labels": [
      "code-fix"
    ],
    "body": "### Motivation\n\n`_pop_waiting_trial_id` for `InMemoryStorage` becomes slow with a large number of trials. We can increase the speed with caching the number of the previous trial with waiting state.\n\nThe main cause of this issue is below.\nhttps://github.com/optuna/optuna/blob/229fe58389a649ca90e7ee2205aeb3d648182235/optuna/storages/_in_memory.py#L380\n\nref: #4716\n\n\n### Suggestion\n\nWe can introduce something like `previous_waiting_trial_number` to cache the number of the previous waiting trial.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "sawa3030",
        "body": "I will work on this issue based on [the PR #5987](https://github.com/optuna/optuna/pull/5987)"
      }
    ]
  },
  {
    "issue_number": 5999,
    "title": "Add MARS (Mixed Adaptive Random Search) as a new sampler in Optuna",
    "author": "sametcopur",
    "state": "closed",
    "created_at": "2025-03-03T09:20:10Z",
    "updated_at": "2025-03-04T10:51:50Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nThe MARS (Mixed Adaptive Random Search) algorithm has shown promising results in black-box optimization tasks, as well as significant speed gains in certain benchmarks. Integrating MARS directly into Optuna would allow users to benefit from its adaptive noise scheduling, categorical handling, and integer/continuous variable support within the familiar Optuna API. This integration could improve exploration in high-dimensional or mixed-type search spaces and provide an additional powerful sampler option for Optuna’s community.\n\n### Description\n\nMARS is an iterative optimizer that starts by sampling points randomly, then progressively shifts toward exploiting the best-performing (elite) solutions, applying a cosine-annealed noise schedule. Key features include:\n\n- **Hybrid Variable Support**: Handles continuous, integer, and categorical variables (using softmax-based sampling for categorical).\n- **Adaptive Noise**: Uses a cosine-annealing strategy to decrease noise over time, balancing exploration and exploitation.\n- **Elite-Based Sampling**: Maintains a subset of best trials “elites” whose values get perturbed to propose new points.\n- **Stochastic Rounding**: Converts continuous values to integers in a probabilistic manner, reducing bias.\n- **Performance Gains**: Initial experiments suggest MARS can achieve faster or competitive performance compared to some existing Optuna samplers, especially in problems with limited budgets or certain high-dimensional settings.\n\nIf integrated as a **sampler** (e.g., `MARSsampler`) within Optuna, users could seamlessly switch to MARS for their optimization studies with minimal changes to existing code. This built-in support would encourage broader adoption and allow for further community-driven improvements.\n\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n**Additional Context (optional)**  \n- We have prepared a Python library called [marsopt](https://github.com/sibirbil/marsopt) that demonstrates the full implementation of MARS. It provides an interface similar to Optuna, making the code references straightforward for users already familiar with Optuna’s concepts.  \n- In our benchmarks against popular optimizers including **CMA-ES** and **Optuna’s TPE**, MARS delivered consistently excellent results. It also demonstrated substantially faster optimization overhead, particularly beneficial for large-scale or time-sensitive tuning tasks.  \n- For further details on the MARS algorithm, including performance plots, comparisons, and usage examples, please visit [our documentation](https://marsopt.readthedocs.io/en/latest/performance.html). We are happy to assist with any code contributions, tests, or design discussions needed to facilitate a seamless integration of MARS into Optuna’s existing sampler architecture.",
    "comments": [
      {
        "user": "y0z",
        "body": "Thank you for your proposal.\n\nHow about introducing MARSSampler into OptunaHub?\n- https://hub.optuna.org/\n\nYou can find more information about OptunaHub below.\n- [OptunaHub, a Feature-Sharing Platform for Optuna, Now Available in Official Release!](\nhttps://medium.com/optuna/optunahub-a-feature-sharing-platform-for-optuna-now-available-in-official-release-4b99efe9934d)\n- [optunahub-registry](https://github.com/optuna/optunahub-registry)\n- [Welcome to OptunaHub Registry’s documentation!](https://optuna.github.io/optunahub-registry/#welcome-to-optunahub-registry-s-documentation)"
      },
      {
        "user": "sametcopur",
        "body": "I’ve looked into OptunaHub, and while it seems like a suitable place to share samplers, I didn’t find it as user-friendly as Optuna itself. Specifically, I’m not a fan of how samplers are called, and the way sampler's dependencies are managed seems unclear (or maybe I just couldn’t find the right place to check). Additionally, accessing sampler's docstrings doesn’t seem as straightforward as I’d expect.\n\nI appreciate the opportunity, If I start using OptunaHub in the future, I’ll definitely add it. However, for now, I’m not planning to add it.\n\nClosing this issue now, thanks again."
      }
    ]
  },
  {
    "issue_number": 5986,
    "title": "\"AssertionError: Should not reach.\"",
    "author": "kuldeepdhaka",
    "state": "closed",
    "created_at": "2025-02-19T09:16:30Z",
    "updated_at": "2025-02-27T15:42:52Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nNot crash I guess\n\n### Environment\n\n- Optuna version: 4.2.1\n- Python version: 3.10\n- OS: Inside Docker container\n- (Optional) Other libraries and their versions:\n\n\n### Error messages, stack traces, or logs\n\n```shell\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 209, in _run_trial\n    frozen_trial = _tell_with_warning(\n  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_tell.py\", line 120, in _tell_with_warning\n    raise ValueError(f\"Cannot tell a {frozen_trial.state.name} trial.\")\nValueError: Cannot tell a FAIL trial.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/workspace/train.py\", line 411, in <module>\n    main()\n  File \"/workspace/train.py\", line 359, in main\n    study.optimize(objective, n_trials=CFG.n_folds_train, show_progress_bar=True, n_jobs=CFG.study_optimize_n_jobs, gc_after_trial=True)\n  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 100, in _optimize\n    f.result()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 239, in _run_trial\n    assert False, \"Should not reach.\"\nAssertionError: Should not reach.\n```\n\n### Steps to reproduce\n\nI do not know other than use the API.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Hi, could you share minimal reproducible codes with us?"
      },
      {
        "user": "kuldeepdhaka",
        "body": "@nzw0301 \n\n```python\nimport optuna\n\nstudy = optuna.create_study()\n\ndef objective(trial):\n    x = trial.suggest_int('x', 0, 3)\n\n    if x == 0:\n        # randomly failed\n        study.tell(trial, state=optuna.trial.TrialState.FAIL, skip_if_finished=True)\n\n    return (x - 2) ** 2\n\n\nstudy.optimize(objective, n_trials=100)\n\nstudy.best_params  # E.g. {'x': 2}\n```\n\n\n```\n[I 2025-02-19 11:05:32,203] A new study created in memory with name: no-name-e034d113-7f9c-4277-aa3d-de85580bc625\n[I 2025-02-19 11:05:32,217] Trial 0 finished with value: 1.0 and parameters: {'x': 1}. Best is trial 0 with value: 1.0.\n[I 2025-02-19 11:05:32,220] Trial 1 finished with value: 0.0 and parameters: {'x': 2}. Best is trial 1 with value: 0.0.\n[I 2025-02-19 11:05:32,221] Trial 2 finished with value: 1.0 and parameters: {'x': 1}. Best is trial 1 with value: 0.0.\n[I 2025-02-19 11:05:32,222] Trial 3 finished with value: 1.0 and parameters: {'x': 3}. Best is trial 1 with value: 0.0.\n[I 2025-02-19 11:05:32,223] Trial 4 finished with value: 1.0 and parameters: {'x': 1}. Best is trial 1 with value: 0.0.\n[I 2025-02-19 11:05:32,224] Trial 5 finished with value: 1.0 and parameters: {'x': 3}. Best is trial 1 with value: 0.0.\nTraceback (most recent call last):\n  File \"<...>/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 209, in _run_trial\n    frozen_trial = _tell_with_warning(\n        study=study,\n    ...<3 lines>...\n        suppress_warning=True,\n    )\n  File \"<...>/.venv/lib/python3.13/site-packages/optuna/study/_tell.py\", line 120, in _tell_with_warning\n    raise ValueError(f\"Cannot tell a {frozen_trial.state.name} trial.\")\nValueError: Cannot tell a FAIL trial.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<...>/replicate/try1.py\", line 15, in <module>\n    study.optimize(objective, n_trials=100)\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<...>/.venv/lib/python3.13/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n    ~~~~~~~~~^\n        study=self,\n        ^^^^^^^^^^^\n    ...<7 lines>...\n        show_progress_bar=show_progress_bar,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"<...>/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n    ~~~~~~~~~~~~~~~~~~~~^\n        study,\n        ^^^^^^\n    ...<8 lines>...\n        progress_bar=progress_bar,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"<...>/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n  File \"<...>/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 239, in _run_trial\n    assert False, \"Should not reach.\"\n           ^^^^^\nAssertionError: Should not reach.\n```"
      },
      {
        "user": "nzw0301",
        "body": "I think it is unexpected code because of calling tell method in the optimize method. Could you use either?"
      }
    ]
  },
  {
    "issue_number": 5988,
    "title": "LightGBMTuner results in a different best_score than the LGBM trained with the same parameters",
    "author": "RashidBakirov",
    "state": "closed",
    "created_at": "2025-02-20T00:12:57Z",
    "updated_at": "2025-02-20T03:39:34Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nI am running LightGBMTuner, which results in a different best_score than when I train LGBM using the parameters from LightGBMTuner. Is this expected?\n\n \n\n### Environment\n\n- Optuna version:4.2.1\n- Python version:3.11.11\n- OS:Linux-6.1.85+-x86_64-with-glibc2.35\n\n\n### Error messages, stack traces, or logs\n\n```shell\n[see code section]\n```\n\n### Steps to reproduce\n\n```python\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as lgbo\n\n\nparams = {\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",\n}\n\nclf_lgbo = lgbo.train(\n    params,\n    dtrain,\n    valid_sets=[dtrain, dval],\n    callbacks=[early_stopping(2), log_evaluation(2)],\n)\n\nprint(\"LGBM Tuner AUC: \", clf_lgbo .best_score)\n\nclf = lgb.train(\n    clf_lgbo .params,\n    dtrain,\n    valid_sets=[dtrain, dval],\n)\n\nprint(\"LGBM AUC: \", clf .best_score)\n```\n\nLGBM Tuner AUC:  defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('auc', 0.8854423141607803)]), 'valid_1': OrderedDict([('auc', 0.7886743450767841)])})\nLGBM AUC:  defaultdict(<class 'collections.OrderedDict'>, {'training': OrderedDict([('auc', 1.0)]), 'valid_1': OrderedDict([('auc', 0.5610885275519422)])})\n\n\n\n\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5350,
    "title": "Implement Delta Lake as storage backend",
    "author": "pfwnicks",
    "state": "closed",
    "created_at": "2024-03-21T09:24:52Z",
    "updated_at": "2025-02-19T23:05:49Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nDelta Lake is a very performant and robust storage system with [ACID](https://delta-io.github.io/delta-rs/how-delta-lake-works/delta-lake-acid-transactions/) (Atomic, Consistent, Isolated and Durable) transactions. Therefore it would be a good match and alternative to using an SQL table especially when computing in a parallel environment.\n\n### Description\n\nIt would be nice to have delta-rs: https://github.com/delta-io/delta-rs\r\n\r\nintegrated into the framework in some way or another. To start off, it could probably be achieved by implementing this in a similar way to the JournalFileStorage and JournalStorage implementation, but in the long run it would be nice to have this fully fledged into the storage backends available. Such that one could just provide a url perhaps with a local or remote path and some storage options to configure the local or remote path.\n\n### Alternatives (optional)\n\nTo start off with, it could of course be implemented as a similar implementation to JournalFileStorage, I am working on this already and will post some code when it is ready.\n\n### Additional context (optional)\n\nhttps://delta-io.github.io/delta-rs/how-delta-lake-works/delta-lake-acid-transactions/\r\n\r\nhttps://delta-io.github.io/delta-rs/",
    "comments": [
      {
        "user": "Ademord",
        "body": "@pfwnicks i come here to ask, since i am running into problems in a distributed tuning scenario, where sqlite is crashing im assuming because of shared access etc; have u tried \"JournalFileStorage\" and do u know if it helps?  what would be the difference to Delta Lake"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5949,
    "title": "`IntersectionSearchSpace` is slow",
    "author": "not522",
    "state": "closed",
    "created_at": "2025-01-28T06:07:49Z",
    "updated_at": "2025-02-19T09:24:17Z",
    "labels": [
      "code-fix"
    ],
    "body": "### Motivation\n\nThe time complexity of `IntersectionSearchSpace` is O(n_trials) for each trial. We can improve it to the constant time.\n\nWe filter trials here, but we can do it in the following for-loop.\nhttps://github.com/optuna/optuna/blob/229fe58389a649ca90e7ee2205aeb3d648182235/optuna/search_space/intersection.py#L29\n\n### Suggestion\n\nWe can move the filtering to the inside of the for-loop. The loop will finish immediately using `cached_trial_number`.\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5968,
    "title": "Remove `_StudyInfo`'s `param_distribution` in `_cached_storage.py`",
    "author": "not522",
    "state": "closed",
    "created_at": "2025-02-12T10:32:23Z",
    "updated_at": "2025-02-18T01:20:01Z",
    "labels": [
      "code-fix",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Motivation\n\nThe `_StudyInfo`'s `param_distribution` in `_cached_storage.py` is not used, so it can be removed.\n\n### Suggestion\n\nRemove `_StudyInfo`'s `param_distribution` in `_cached_storage.py`.\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5969,
    "title": "Remove `UnsupportedDistribution` in `testing/distributions.py`",
    "author": "not522",
    "state": "closed",
    "created_at": "2025-02-12T10:35:20Z",
    "updated_at": "2025-02-17T01:50:39Z",
    "labels": [
      "code-fix",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Motivation\n\n`UnsupportedDistribution` in `testing/distributions.py` is not used, so it can be removed.\n\n### Suggestion\n\nRemove `UnsupportedDistribution` in `testing/distributions.py`\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5967,
    "title": "Remove `_check_and_set_param_distribution` in `RDBStorage`",
    "author": "not522",
    "state": "closed",
    "created_at": "2025-02-12T10:28:08Z",
    "updated_at": "2025-02-14T03:53:45Z",
    "labels": [
      "code-fix",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Motivation\n\nThe `RDBStorage`'s `_check_and_set_param_distribution` is not used, so it can be removed.\n\n### Suggestion\n\nRemove `_check_and_set_param_distribution` in `RDBStorage`.\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5970,
    "title": "Remove `FirstTrialOnlyRandomSampler` in `testing/samplers.py`",
    "author": "not522",
    "state": "closed",
    "created_at": "2025-02-12T10:36:40Z",
    "updated_at": "2025-02-14T03:53:05Z",
    "labels": [
      "code-fix",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Motivation\n\n`FirstTrialOnlyRandomSampler` in `testing/samplers.py` is not used, so it can be removed.\n\n### Suggestion\n\nRemove `FirstTrialOnlyRandomSampler` in `testing/samplers.py`.\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5963,
    "title": "Inconsistency Optuna behavior subjected to Ctrl-C",
    "author": "vatthaphon",
    "state": "open",
    "created_at": "2025-02-12T03:04:06Z",
    "updated_at": "2025-02-12T14:15:54Z",
    "labels": [
      "feature",
      "needs-discussion"
    ],
    "body": "### Expected behavior\n\nIn the toy example, when n_jobs, for example, equals 2, I have to wait for the execution of the obj function to finish in order to make Ctrl-C effective. This behavior is different from the n_jobs = 1 case, with which Ctrl-C is raised as soon as I press it. I would expect that Ctrl-C should be consistent regardless of the number of n_jobs.\n\n### Environment\n\n- Optuna version: 4.0.0\n- Python version: 3.10.0\n- OS: Windows-10-10.0.19045-SP0\n- (Optional) Other libraries and their versions:\n\n\n### Error messages, stack traces, or logs\n\n```shell\nNone\n```\n\n### Steps to reproduce\n\n```python\nimport time\nimport optuna\n\ndef obj(trial):\n\n    trial_id = trial.number\n\n    print(f\"Start trial {trial_id}\")\n\n    for _ in range(3):\n        time.sleep(5)\n\n    return trial_id\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(study_name=\"test\", storage=\"sqlite:///test.db\", direction=\"maximize\", load_if_exists=True)\n\n    try:\n        study.optimize(obj, n_trials=4, n_jobs=2) # Press CTRL+C to stop the optimization. \n    except:\n        print(\"Ctrl-C\")\n```\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Thank you for creating the issue. I believe that this is not a bug in Optuna and it looks like FAQ ex.  \n https://stackoverflow.com/questions/4136632/how-to-kill-a-child-thread-with-ctrlc. Technically, we can stop the script by pressing `Ctrl-C` `n_jobs` times. So let me change the label."
      },
      {
        "user": "vatthaphon",
        "body": "> Thank you for creating the issue. I believe that this is not a bug in Optuna and it looks like FAQ ex. https://stackoverflow.com/questions/4136632/how-to-kill-a-child-thread-with-ctrlc. Technically, we can stop the script by pressing `Ctrl-C` `n_jobs` times. So let me change the label.\n\nThank you for the link. I am new to Optuna. Could you please guide me to some resources, with which I could implement the threading methods mentioned in your link with Optuna?"
      },
      {
        "user": "nzw0301",
        "body": "The implementation is found in https://github.com/optuna/optuna/blob/a1df0ca210f4b57324fdd567be9568be4f003106/optuna/study/_optimize.py."
      }
    ]
  },
  {
    "issue_number": 1835,
    "title": "Is there anyway to disable the ExperimentalWarnings",
    "author": "Madman600",
    "state": "closed",
    "created_at": "2020-09-15T18:12:45Z",
    "updated_at": "2025-02-08T09:51:15Z",
    "labels": [
      "question"
    ],
    "body": "I'm trying to output parameters for a multi objective study when a new best optimization is found but the ExperimentalWarnings are cluttering things up. Is there any way to turn these off whilst still keeping other log output?",
    "comments": [
      {
        "user": "c-bata",
        "body": "Hi @Madman600. I guess you can use `warnings.filterwarnings()` like:\r\n\r\n```python\r\nwarnings.filterwarnings(\"ignore\", category=ExperimentalWarnings, module=\"optuna.multi_objective\")\r\n```\r\n\r\nhttps://docs.python.org/3/library/warnings.html#warnings.filterwarnings\r\n"
      },
      {
        "user": "Madman600",
        "body": "Thanks. How can I turn logging for optuna off completely?"
      },
      {
        "user": "Madman600",
        "body": "Never mind I figured it out, I was calling disable_default_handler in a method where I create my logger instead of when the module is executed initially."
      }
    ]
  },
  {
    "issue_number": 2141,
    "title": "What happens when I add/remove parameters dynamically during an Optuna study?",
    "author": "gosuto-inzasheru",
    "state": "closed",
    "created_at": "2020-12-22T10:19:39Z",
    "updated_at": "2025-02-08T04:37:22Z",
    "labels": [
      "question"
    ],
    "body": "Cross-posted from https://stackoverflow.com/questions/65362133/what-happens-when-i-add-remove-parameters-dynamically-during-an-optuna-study:\r\n> Optuna's FAQ has a [clear answer](https://optuna.readthedocs.io/en/stable/faq.html#id10) when it comes to dynamically adjusting the range of parameter during a study: it poses no problem since each sampler is defined individually.\r\n>\r\n> But what about adding and/or removing parameters? Is Optuna able to handle such adjustments?\r\n>\r\n> One thing I noticed when doing this is that in the results dataframe these parameters get `nan` entries for other trials. Would there be any benefit to being able to set these `nan`s to their (default) value that they had when not being sampled? Is the study still sound with all these unknown values?\r\n\r\nI believe an underlying question is: is there dependence between the `trial.suggest_` functions?",
    "comments": [
      {
        "user": "HideakiImamura",
        "body": "Thanks for the question. Optuna internally supports two types of sampling: `optuna.samplers.BaseSampler.sample_independet` and `optuna.samplers.BaseSampler.sample_relative`. \r\n\r\nThe former `optuna.samplers.BaseSampler.sample_independet` is a method that samples independently on each parameter, and is not affected by the addition or removal of parameters. The added parameters are taken into account from the timing when they are added.\r\n\r\nThe latter `optuna.samplers.BaseSampler.sample_relative` is a method that samples by considering the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method `optuna.samplers.BaseSampler.infer_relative_search_space`. This may allow correlations to be considered for hyperparameters that have been added or removed, but this depends on the sampling algorithm, so there is no API for normal users to modify."
      },
      {
        "user": "HideakiImamura",
        "body": "Let me close this issue. @jorijnsmit please re-open this if you want. "
      },
      {
        "user": "gosuto-inzasheru",
        "body": "My question was answered, thank you @HideakiImamura 🙏"
      }
    ]
  },
  {
    "issue_number": 5959,
    "title": "[Feature Request]: Support for Normal Distribution in Optuna",
    "author": "shashank-iitbhu",
    "state": "open",
    "created_at": "2025-02-04T19:23:49Z",
    "updated_at": "2025-02-05T01:14:11Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\n- Some machine learning models benefit from hyperparameters sampled from a normal distribution.\n- Katib (Kubeflow’s AutoML framework) integrates with Optuna, and this feature could benefit that integration.\n\n### Description\n\nI wanted to check if Optuna has any plans to support normal (Gaussian) distribution for sampling hyperparameters. Currently, the supported distributions seem to be uniform and logUniform, but normal distribution could be useful for certain hyperparameter tuning scenarios.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "shashank-iitbhu",
        "body": "CC: @c-bata (Tagging you since you've contributed to both Optuna and Katib, would love to hear your thoughts!)"
      },
      {
        "user": "nabenabe0928",
        "body": "@shashank-iitbhu \nHi thank you for your post:)\nWhat about using the inverse transform method?\n\n```python\nfrom scipy.special import ndtri\n\n\nmu = ...\nsigma = ...\np = trial.suggest_float(\"p\", 0, 1)\nx = mu + sigma * ndtri(p)\n```\n\nWhat I can say here is that it's unlikely to provide something like `suggest_float(..., normal=True)` from our side. That's simply because most samplers need box constraints, i.e. to have lower and upper bounds for each parameter, but the normal distribution doesn't allow it. Namely, the parameter will be defined on `(-inf, inf)`. Plus, the normal distribution requires two parameters (mu and sigma) to define its shape, increasing the complexity of the UI.\n\nI would ask you to use the snippet I provided for your purpose 🙏 \n\nIf it solves your problem, please close this issue!"
      }
    ]
  },
  {
    "issue_number": 3542,
    "title": "Add explanation when `FrozenTrial.values` with `PRUNED` state use the last intermediate value as `values`",
    "author": "nzw0301",
    "state": "closed",
    "created_at": "2022-05-06T13:21:05Z",
    "updated_at": "2025-02-02T23:05:29Z",
    "labels": [
      "document",
      "needs-discussion",
      "stale"
    ],
    "body": "### What is an issue?\r\n\r\n I suppose this behaviour as in the title should be documented.",
    "comments": [
      {
        "user": "g-votte",
        "body": "Adding `needs-discussion` label since the expected behavior (`None`, last intermediate value, or undefined) has not been fully agreed even among maintainers.\r\n\r\nSee also this comment: https://github.com/optuna/optuna/blob/f00abac7d53bb29e53e7c8bd13e9b4be9e0f9d44/optuna/study/_tell.py#L162-L165"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5611,
    "title": "Optuna integration significantly reduces performance and limits number of parallel trials in Ray Tune",
    "author": "Aricept094",
    "state": "closed",
    "created_at": "2024-08-06T07:49:26Z",
    "updated_at": "2025-01-31T22:28:24Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nOptuna integration should maintain performance comparable to other search algorithms, utilize CPU resources efficiently, and allow for multiple trials as specified.\r\n\r\n### Environment\r\n\r\nRay version: 2.32.0\r\nPython version: 3.11.8\r\nOperating System: Linux-6.6.36.3-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\nOptuna version: 3.6.1\r\n\r\nDetailed OS Information:\r\nOS: Linux\r\nOS Version: https://github.com/ray-project/ray/pull/1 SMP PREEMPT_DYNAMIC Sat Jun 29 07:01:04 UTC 2024\r\nOS Release: 6.6.36.3-microsoft-standard-WSL2\r\nMachine: x86_64\r\nProcessor: x86_64\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\nNo specific error.\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nThe execution time for 100 trials shows a dramatic difference:\r\n\r\nWith Optuna: 41 seconds\r\nWithout Optuna: 9 seconds\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import mean_squared_error\r\nimport ray\r\nfrom ray import tune\r\nfrom ray.tune.search.optuna import OptunaSearch\r\nfrom ray import train, tune\r\n\r\ndef generate_data(n_samples=1000):\r\n    X = np.random.rand(n_samples, 5)\r\n    y = 2 * X[:, 0] + 3 * X[:, 1] - X[:, 2] + 0.5 * X[:, 3] - 1.5 * X[:, 4] + np.random.normal(0, 0.1, n_samples)\r\n    return X, y\r\n\r\ndef train_random_forest(config):\r\n    X, y = generate_data()\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\r\n    \r\n    rf = RandomForestRegressor(\r\n        n_estimators=config[\"n_estimators\"],\r\n    )\r\n    \r\n    rf.fit(X_train, y_train)\r\n    y_pred = rf.predict(X_test)\r\n    mse = mean_squared_error(y_test, y_pred)\r\n    \r\n    train.report({\"mean_squared_error\": mse,})\r\n\r\n\r\ndef main():\r\n    ray.init()\r\n    \r\n    config = {\r\n        \"n_estimators\": tune.randint(10, 200),\r\n    }\r\n        \r\n    analysis = tune.run(\r\n        train_random_forest,\r\n        config=config,\r\n        num_samples=100, \r\n        search_alg=OptunaSearch(),\r\n        metric=\"mean_squared_error\",\r\n        mode=\"min\",\r\n        reuse_actors=True\r\n    )\r\n    \r\n    print(\"Best config:\", analysis.best_config)\r\n    print(\"Best MSE:\", analysis.best_result[\"mean_squared_error\"])\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n### Additional context (optional)\r\n\r\nAlso referenced here in ray\r\n\r\n[](https://github.com/ray-project/ray/issues/46965)",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "Hi, thank you for your report!\r\nCould you tell me what you meant by `without Optuna` and how to run the code above without Optuna?"
      },
      {
        "user": "Aricept094",
        "body": "> Hi, thank you for your report!\n> \n> Could you tell me what you meant by `without Optuna` and how to run the code above without Optuna?\n\nHey , thanks for fast response. \n\nSorry for being a bit vague. \n\n\"With optuna\" I meant using OptunaSearch() as the search algorithm in the \"def main\" and without optuna I meant just removing the line so ray defaults to its base search algorithm. \n\nJust copy and pasting the code should work . \n\n"
      },
      {
        "user": "nabenabe0928",
        "body": "@Aricept094 \r\n\r\nThank you for the note! I will check the cause and get back to you:)\r\nBut usually, Optuna would not take a second to sample 100 configs, so it might be the issue from Ray.\r\nAnyways, I will check why it is slow right now."
      }
    ]
  },
  {
    "issue_number": 5947,
    "title": "grpcio version messed up by/with tensorboard and/or ahead of conda-forge",
    "author": "fredshone",
    "state": "closed",
    "created_at": "2025-01-27T10:38:10Z",
    "updated_at": "2025-01-31T07:49:15Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nI am getting a runtime error. eg:\n\nRunning `python3 -c 'import optuna; print(f\"- Optuna version:{optuna.__version__}\")'` is resulting in runtime error.\n\nRunning the first basic [example](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/001_first.html) is resulting in runtime error.\n\n### Environment\n\n- Optuna version: 4.2.0\n- Python version: 3.12.8\n- OS: Ubuntu 24.04.1 LTS (and also Windows 11)\n\n\n### Error messages, stack traces, or logs\n\n```shell\nRuntimeError: The grpc package installed is at version 1.67.1, but the generated code in api_pb2_grpc.py depends on grpcio>=1.68.1. Please upgrade your grpc module to grpcio>=1.68.1 or downgrade your generated code using grpcio-tools<=1.67.1.\n```\n\n### Steps to reproduce\n\n1. Using a conda-forge install of optuna and tensorboard `mamba install optuna tensorboard -c conda-forge`\n2. `python -c 'import optuna; optuna.__version__'` (for example).\n\n\n\n\n### Additional context (optional)\n\nAfter install, grpcio is version 1.67.1. But optuna wants >=1.68.1 according to [here](https://github.com/optuna/optuna/blob/master/optuna/storages/_grpc/auto_generated/api_pb2_grpc.py). I see this is auto generated by grpcio but when or where is beyond me.\n\nSeems to be a problem specifically with tensorboard keeping grpcio version below at 1.67.1. Installing optuna on its own is fine and as per error message, a pip install also fixes, ie `pip install grpcio==1.70.0`. \n\nI note that the [conda-forge version of grpcio](https://anaconda.org/conda-forge/grpcio) is at version 1.67.1 only.\n\nSo more of a conda annoyance than a bug. Also time to stop using tensorboard. but I hope still useful.",
    "comments": [
      {
        "user": "c-bata",
        "body": "Thank you for your report! We will address this issue in #5954 or #5915 :bow:"
      },
      {
        "user": "c-bata",
        "body": "[Announcement] For those encountering issues while trying to use Optuna's gRPC storage proxy in a conda environment, we are planning to address the failure of `import optuna` and release a patch version, v4.2.1. However, the gRPC storage proxy is still not usable with older versions of `grpcio`.\n\nIf you wish to use Optuna's gRPC storage proxy in a conda environment, please file an issue on the grpc's feedstock repository and install the latest grpcio version.\nhttps://github.com/conda-forge/grpc-cpp-feedstock"
      }
    ]
  },
  {
    "issue_number": 5925,
    "title": "Fix `StorageSupplier(\"grpc\")` to make unit tests robust.",
    "author": "c-bata",
    "state": "closed",
    "created_at": "2025-01-17T04:05:17Z",
    "updated_at": "2025-01-30T04:15:18Z",
    "labels": [
      "code-fix",
      "contribution-welcome"
    ],
    "body": "### Motivation\n\nIn [this workflow run](https://github.com/optuna/optuna/actions/runs/12802246427/job/35693009350), `test_run_trial_invoke_tell_with_suppressing_warning[grpc]` was once failed and succeeded on the second attempt.\n\n```\n=================================== FAILURES ===================================\n__________ test_run_trial_invoke_tell_with_suppressing_warning[grpc] ___________\n\nstorage_mode = 'grpc'\n\n    @pytest.mark.parametrize(\"storage_mode\", STORAGE_MODES)\n    def test_run_trial_invoke_tell_with_suppressing_warning(storage_mode: str) -> None:\n        def func_numerical(trial: Trial) -> float:\n            return trial.suggest_float(\"v\", 0, 10)\n    \n        with StorageSupplier(storage_mode) as storage:\n>           study = create_study(storage=storage)\n\ntests/study_tests/test_optimize.py:144: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/_convert_positional_args.py:83: in converter_wrapper\n    return func(**kwargs)  # type: ignore[call-arg]\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/study.py:1265: in create_study\n    study_id = storage.create_new_study(direction_objects, study_name)\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/storages/_grpc/client.py:99: in create_new_study\n    response = self._stub.CreateNewStudy(request)\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/grpc/_channel.py:1[181](https://github.com/optuna/optuna/actions/runs/12802246427/job/35693009350#step:9:182): in __call__\n    return _end_unary_response_blocking(state, call, False, None)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstate = <grpc._channel._RPCState object at 0x147eea6f0>\ncall = <grpc._cython.cygrpc.SegregatedCall object at 0x13776e300>\nwith_call = False, deadline = None\n\n    def _end_unary_response_blocking(\n        state: _RPCState,\n        call: cygrpc.SegregatedCall,\n        with_call: bool,\n        deadline: Optional[float],\n    ) -> Union[ResponseType, Tuple[ResponseType, grpc.Call]]:\n        if state.code is grpc.StatusCode.OK:\n            if with_call:\n                rendezvous = _MultiThreadedRendezvous(state, call, None, deadline)\n                return state.response, rendezvous\n            else:\n                return state.response\n        else:\n>           raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\nE           grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\nE           \tstatus = StatusCode.UNAVAILABLE\nE           \tdetails = \"sendmsg: Broken pipe (32)\"\nE           \tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"sendmsg: Broken pipe (32)\", grpc_status:14, created_time:\"2025-01-16T04:48:29.945491+00:00\"}\"\nE           >\n\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/grpc/_channel.py:1006: _InactiveRpcError\n```\n\n### Suggestion\n\nFixes the `StorageSupplier(\"grpc\")` to make unit tests robust.\nhttps://github.com/optuna/optuna/blob/master/optuna/testing/storages.py\n\n### Additional context (optional)\n\nThis issue is related to https://github.com/optuna/optuna/pull/5913",
    "comments": [
      {
        "user": "HideakiImamura",
        "body": "Let me close this issue since #5938 resolved this."
      }
    ]
  },
  {
    "issue_number": 5780,
    "title": "`BruteForceSampler` stops before suggesting all combinations in parallel optimization",
    "author": "not522",
    "state": "closed",
    "created_at": "2024-11-22T08:50:04Z",
    "updated_at": "2025-01-29T06:46:19Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\n`BruteForceSampler` should suggest all parameter combinations, but it stops unexpectedly before that in parallel optimization.\n\n### Environment\n\n- Optuna version:4.2.0.dev\r\n- Python version:3.9.4\r\n- OS:macOS-15.1-x86_64-i386-64bit\r\n- (Optional) Other libraries and their versions:\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\nSee the following MRE.\n```\n\n\n### Steps to reproduce\n\n```python\r\nimport time\r\nimport optuna\r\n\r\ndef objective(trial):\r\n    x = trial.suggest_int(\"x\", 0, 1)\r\n    time.sleep(x)\r\n    y = trial.suggest_int(\"y\", 0, 1)\r\n    return x + y\r\n\r\nsampler = optuna.samplers.BruteForceSampler(seed=42)\r\nstudy = optuna.create_study(sampler=sampler)\r\nstudy.optimize(objective, n_jobs=2)\r\n```\r\n\r\n```\r\n[I 2024-11-22 17:43:20,854] A new study created in memory with name: no-name-86ca63d9-e334-4237-a5a5-854dd2483f9f\r\n[I 2024-11-22 17:43:20,855] Trial 0 finished with value: 1.0 and parameters: {'x': 0, 'y': 1}. Best is trial 0 with value: 1.0.\r\n[I 2024-11-22 17:43:20,856] Trial 2 finished with value: 0.0 and parameters: {'x': 0, 'y': 0}. Best is trial 2 with value: 0.0.\r\n[I 2024-11-22 17:43:21,861] Trial 1 finished with value: 1.0 and parameters: {'x': 1, 'y': 0}. Best is trial 2 with value: 0.0.\r\n```\r\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2021,
    "title": "Optuna Suggests the Same Parameter Values in a lot of Trials (Duplicate Trials that Waste Time and Budget)",
    "author": "deathes",
    "state": "open",
    "created_at": "2020-11-14T11:56:57Z",
    "updated_at": "2025-01-28T17:16:22Z",
    "labels": [],
    "body": "Optuna TPESampler and RandomSampler try the same suggested integer values (possible floats and loguniforms as well) for any parameter more than once for some reason. I couldn't find a way to stop it from suggesting same values over over again. Out of 100 trials quite a few of them are just duplicates. Unique suggested value count ends up around 80-90 out of 100 trials. If I include more parameters for tuning, say 3, I even see all 3 of them getting the same values a few times in 100 trials.\r\n\r\nIt's like this:\r\n\r\n[I 2020-11-14 14:44:05,320] Trial 8 finished with value: 45910.54012028659 and parameters: {'min_data_in_leaf': 75}. Best is trial 4 with value: 45805.19030897498.\r\n[I 2020-11-14 14:44:07,876] Trial 9 finished with value: 45910.54012028659 and parameters: {'min_data_in_leaf': 75}. Best is trial 4 with value: 45805.19030897498.\r\n[I 2020-11-14 14:44:10,447] Trial 10 finished with value: 45831.75933279074 and parameters: {'min_data_in_leaf': 43}. Best is trial 4 with value: 45805.19030897498.\r\n[I 2020-11-14 14:44:13,502] Trial 11 finished with value: 46125.39810101329 and parameters: {'min_data_in_leaf': 4}. Best is trial 4 with value: 45805.19030897498.\r\n[I 2020-11-14 14:44:16,547] Trial 12 finished with value: 45910.54012028659 and parameters: {'min_data_in_leaf': 75}. Best is trial 4 with value: 45805.19030897498.\r\n\r\nExample code below: \r\n\r\n    def lgb_optuna(trial):\r\n\r\n        rmse = []\r\n\r\n        params = {\r\n            \"seed\": 42,\r\n            \"objective\": \"regression\",\r\n            \"metric\": \"rmse\",\r\n            \"verbosity\": -1,\r\n            \"boosting\": \"gbdt\",\r\n            \"num_iterations\":  1000,\r\n            'min_data_in_leaf':  trial.suggest_int('min_data_in_leaf', 1, 100)\r\n        }\r\n\r\n        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\r\n        for train_index, test_index in cv.split(tfd_train, tfd_train[:,-1]):\r\n            X_train, X_test = tfd_train[train_index], tfd_train[test_index]\r\n            y_train = X_train[:,-2].copy()\r\n            y_test = X_test[:,-2].copy()\r\n            \r\n            dtrain = lgb.Dataset(X_train[:,:-2], label=y_train)\r\n            dtest = lgb.Dataset(X_test[:,:-2], label=y_test)\r\n        \r\n            booster_gbm = lgb.train(params, dtrain, valid_sets=dtest, verbose_eval=False)\r\n\r\n            y_predictions = booster_gbm.predict(X_test[:,:-2])\r\n            final_mse = mean_squared_error(y_test, y_predictions)\r\n            final_rmse = np.sqrt(final_mse)\r\n            rmse.append(final_rmse)\r\n    \r\n         return np.mean(rmse)\r\n\r\n    study = optuna.create_study(sampler=TPESampler(seed=42), direction='minimize') \r\n    study.optimize(lgb_optuna, n_trials=100) ",
    "comments": [
      {
        "user": "crcrpar",
        "body": "Desirably, if the suggested parameters are the ones already suggested, the trial should be skipped for faster hyperparameter optimization, right?\r\n\r\nReproduced output: https://gist.github.com/crcrpar/aba308a1350bb4986276a6c87cf256cb"
      },
      {
        "user": "deathes",
        "body": "> Desirably, if the suggested parameters are the ones already suggested, the trial should be skipped for faster hyperparameter optimization, right?\r\n> \r\n> Reproduced output: https://gist.github.com/crcrpar/aba308a1350bb4986276a6c87cf256cb\r\n\r\nExactly. If the trial has already been suggested, it should be skipped for faster hyperparameter optimization. Or better yet, it shouldn't have been suggested in the first place. The sampler should check the history of suggested hyperparameter values first before suggesting new one."
      },
      {
        "user": "crcrpar",
        "body": "By its nature, TPESampler tends to sample similar values with the number of trials increasing since it narrows the search space based on Bayesian optimization with the number of trials increasing. So I think it's a bit hard to avoid the same values suggested.\r\n\r\nThat being said, we can deal with this case by checking the suggested values in the `objective` function and if they aren't new ones, terminate the trial.\r\n\r\n```python\r\ndef objective(trial):\r\n    some_param = trial.suggest_int(\"some_param\", 0, 100)\r\n    if some_param in param_history[\"some_param\"]:\r\n        raise optuna.exceptions.TrialPruned()\r\n    ...\r\n    # define your evaluation \r\n    return value\r\n```\r\nThe full code with this hack and its result is available: https://gist.github.com/crcrpar/c6d6fdf8112280654884353d6e68f6bb.\r\n\r\nNote that this solution might not be general enough to apply to other cases.\r\n\r\n@HideakiImamura would have some better ideas."
      }
    ]
  },
  {
    "issue_number": 5612,
    "title": "`ModuleNotFoundError` when  running tutorial",
    "author": "kAIto47802",
    "state": "open",
    "created_at": "2024-08-06T08:21:34Z",
    "updated_at": "2025-01-28T10:07:16Z",
    "labels": [
      "document"
    ],
    "body": "### What is an issue?\n\nThere are some libraries that need to be installed when running the tutorial. Following the [installation tutorial ](https://optuna.readthedocs.io/en/latest/installation.html) and simply running `pip install optuna` is sometimes insufficient, resulting in a `ModuleNotFoundError`.\r\n> [!NOTE]\r\n> In the CI environment, other libraries listed under `document` in `[project.optional-dependencies]` are installed, preventing the `ModuleNotFoundError` when generating the document.\r\n\r\nIt is clear that a `ModuleNotFoundError` occurs in lines where explicit imports, such as import torch, are made. However, it is less obvious when the error occurs due to libraries used internally by Optuna that do not require explicit imports. For example, the function `optuna.visualization.plot_param_importances()` in [Multi-objective Optimization tutorial](https://optuna.readthedocs.io/en/latest/tutorial/20_recipes/002_multi_objective.html) raises `ModuleNotFoundError` beacause it needs plotly, nbformat, and scikit-learn. It would be beneficial to add a note highlighting this.\r\n\r\nBased on my findings so far, this issue occurs in the following areas:\r\n- [Quick Visualization for Hyperparameter Optimization Analysis](https://optuna.readthedocs.io/en/latest/tutorial/10_key_features/005_visualization.html) needs plotly and nbformat\r\n- [Saving/Resuming Study with RDB Backend](https://optuna.readthedocs.io/en/latest/tutorial/20_recipes/001_rdb.html) needs pandas\r\n- [Multi-objective Optimization with Optuna](https://optuna.readthedocs.io/en/latest/tutorial/20_recipes/002_multi_objective.html) needs plotly, nbformat, and scikit-learn",
    "comments": []
  },
  {
    "issue_number": 5797,
    "title": "Fix a fragile unit test, `test_check_distribution_suggest_loguniform[sqlite3]`",
    "author": "c-bata",
    "state": "open",
    "created_at": "2024-11-28T04:11:22Z",
    "updated_at": "2025-01-23T07:21:59Z",
    "labels": [
      "code-fix"
    ],
    "body": "### Motivation\r\n\r\nIn [this workflow run](https://github.com/optuna/optuna/actions/runs/12058983729/job/33626763864?pr=5795), `test_check_distribution_suggest_loguniform[sqlite]` was once failed and succeeded on the second attempt.\r\n\r\n### Suggestion\r\n\r\nUpdate the `test_check_distribution_suggest_loguniform` function to ensure the unit test is non-fragile.\r\n\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "sawa3030",
        "body": "Hello @c-bata, I think it might be more robust to avoid counting warnings in the unit test. Just to check if there is warning comes up would be enough, I think. This is because diagnosing the root cause of such failures can be time-consuming and challenging."
      }
    ]
  },
  {
    "issue_number": 5860,
    "title": "Use a `TrialState.STALE`  instead of `TrialState.FAIL` when heartbeats time out ",
    "author": "agrinh",
    "state": "open",
    "created_at": "2024-12-11T12:40:06Z",
    "updated_at": "2025-01-23T06:34:54Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nWhen running distributed HPO trials it is useful to be able differentiate between trials that fail due to node pre-emption and a trial that leads to an unexpected exception.\n\n### Description\n\n- Add `TrialState.STALE`\r\n- Set this as the new status for timed out trials in `fail_stale_trials`\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "agrinh",
        "body": "Happy to contribute code if desired."
      },
      {
        "user": "sawa3030",
        "body": "Thank you for your suggestion. Refactoring `TrialState` is quite challenging because many parts of the existing Optuna codebase depend on it. However, if your goal is to handle stale trials, you can achieve this by adding a function to `failed_trial_callback()` in [RDBStorage.](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.storages.RDBStorage.html#optuna.storages.RDBStorage) This callback function is triggered right after stale trials are identified and marked as failed. Within this callback, you can implement custom logic to modify attributes in the study, as trials themselves are non-changeable when the `failed_trial_callback()` is called."
      }
    ]
  },
  {
    "issue_number": 5890,
    "title": "Trial should raise error if the `suggest` method to the same HP name is called more than once?",
    "author": "superleesa",
    "state": "open",
    "created_at": "2024-12-27T07:09:12Z",
    "updated_at": "2025-01-21T05:01:39Z",
    "labels": [
      "feature",
      "needs-discussion"
    ],
    "body": "### Motivation\n\nCurrent implementation does not raise error even if the value of the same HP name is suggested twice within the same trial, which feels weird to me (I think only 1 HP value suggestion for 1 trial makes most sense) ?. It is bug-prone from the user point of view because one might accidentally name different hp with the same name. Existing implementation seems to be checking that two distributions are compatible with each other if a HP with same name is called twice \r\n\r\nhttps://github.com/optuna/optuna/blob/cdcbaf3a91d39c42e41f4bb852ed01804619c1a7/optuna/trial/_trial.py#L618-L621\r\n\r\nHowever, i think this condition can be more strict and can just raise error immediately without checking their compatibility.\r\n\r\nI am asking this because when I had two different HPs but both float and similar distribution, I did not get error (and I had to start tuning overall again).\r\n\r\nI know that this is a breaking change to existing functionality because some people might be using trial.suggest more than once in thier code to expect to get the same value.  However, I thought users can instead call trial.suggest once and store the output into another variable and reuse that.\n\n### Description\n\nI want Optuna to simply raise error if the `Trial.suggest` method is called more than once with the same HP name.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "I understand your point very well, but since your suggestion is gonna break the backward compatibility, we cannot actually add the error raise.\r\nHowever, we can probably add a warning instead.\r\nWhat do you, @superleesa , think?"
      }
    ]
  },
  {
    "issue_number": 5931,
    "title": "issue: CUDA out of memory when using optuna to do hyperparam search for LLM LoRA",
    "author": "zwille014",
    "state": "closed",
    "created_at": "2025-01-20T05:06:20Z",
    "updated_at": "2025-01-20T05:22:06Z",
    "labels": [
      "question"
    ],
    "body": "### Expected behavior\n\nOur GPU has 24 memory in total. We load \"Llama-3.2-3B-instruct\" full precision every trial (It will consume approximately 12GB of GPU memory), and use torch.cuda.empty_cache() to clear CPU memory after each trial to avoid memory leak. It should not encounter any out-of-memory issues. It should complete all the hyperparameter search trials. Did optuna cause the problem? Is there any solution?\n\n### Environment\n\n- Optuna version: 4.1.0\n- Python version: 3.10.14\n- OS: Linux\n- (Optional) Other libraries and their versions:  transformers 4.45.2, peft 0.14.0\n\n\n### Error messages, stack traces, or logs\n\n```shell\nOutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 21.98 GiB of which 40.44 MiB is free. Including non-PyTorch memory, this process has 21.93 GiB memory in use. Of the allocated memory 20.93 GiB is allocated by PyTorch, and 715.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.\n```\n\n### Steps to reproduce\n\n```python\n\nmodel_path = \"Llama-3.2-3B-instruct\"\n\ndef objective(trial):\n    # Define the search space for hyperparameters\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-4)\n    per_device_train_batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n\n    # Define the model\n    cur_base_model = LlamaForTokenClassification.from_pretrained(\n        model_path\n    )\n    \n    cur_config = {\n        \"peft_type\": \"LORA\",\n        \"inference_mode\": False,\n        \"r\": trial.suggest_categorical('lora_r', [8, 16]),\n        \"lora_alpha\": 16,\n        \"lora_dropout\": 0.05,\n        \"bias\": \"none\",    \n        \"target_modules\": [\n            \"q_proj\",\n            \"v_proj\"\n        ]\n    }\n    \n    cur_peft_config = get_peft_config(cur_config)\n    cur_peft_model = PeftModelForTokenClassification(cur_base_model, cur_peft_config)\n    \n    hpo_training_args = TrainingArguments(\n        output_dir = 'trained',\n        learning_rate = 1e-4,\n        per_device_train_batch_size = per_device_train_batch_size,\n        per_device_eval_batch_size = 2,\n        num_train_epochs = 5,\n        logging_steps=1,\n        weight_decay = 0.001,\n        report_to=\"none\"\n    )\n\n    # Define the Trainer\n    trainer = Trainer(\n        model=cur_peft_model,\n        args=hpo_training_args,\n        train_dataset=tokenized_train.shard(index=1, num_shards=10),\n        eval_dataset=tokenized_eval.shard(index=1, num_shards=10),\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=lambda preds: eval_util.compute_metrics(preds, mappings)\n    )\n\n    # Train and evaluate\n    train_results = trainer.train()\n    eval_results = trainer.evaluate()\n    \n    res = eval_results['eval_loss']\n\n    # Manually clear GPU memory after each trial to avoid memory leaks\n    del cur_peft_model\n    torch.cuda.empty_cache()\n    \n    return res\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10, gc_after_trial=True, callbacks=[lambda study, trial: gc.collect()])  # 10 trials as an example\nprint(f\"Best hyperparameters: {study.best_params}\")\n```\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 4878,
    "title": "Can't upgrade MySQL database schema from v2.6.0.a → v3.0.0.a",
    "author": "a7brusco",
    "state": "closed",
    "created_at": "2023-08-21T12:51:58Z",
    "updated_at": "2025-01-19T23:05:47Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\n\nUpgrading the database schema from v2.6.0.a → v3.0.0.a with `optuna storage upgrade --storage <mysql_database>`\n\n### Environment\n\n- Optuna version: 3.3.0\r\n- Python version: 3.10.9\r\n- OS: Windows-10-10.0.19045-SP0\r\n- (Optional) Other libraries and their versions: `mysqlclient==2.2.0`\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\nINFO  [alembic.runtime.migration] Context impl MySQLImpl.\r\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\r\nINFO  [alembic.runtime.migration] Running upgrade v2.6.0.a -> v3.0.0.a, unify existing distributions to {int,float} distribution\r\nd:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\optuna\\distributions.py:598: FutureWarning: IntUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.IntDistribution` instead.\r\n  return cls(**json_dict[\"attributes\"])\r\nd:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\optuna\\distributions.py:598: FutureWarning: UniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.FloatDistribution` instead.\r\n  return cls(**json_dict[\"attributes\"])\r\nERROR [sqlalchemy.pool.impl.NullPool] Error closing cursor\r\nTraceback (most recent call last):\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\orm\\query.py\", line 2830, in __iter__\r\n    yield from result  # type: ignore\r\nGeneratorExit\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 2199, in _safe_close_cursor\r\n    cursor.close()\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 103, in close\r\n    self._discard()\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 89, in _discard\r\n    self._result.discard()\r\nMySQLdb.OperationalError: (2006, '')\r\nTraceback (most recent call last):\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1934, in _exec_single_context\r\n    self.dialect.do_executemany(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\dialects\\mysql\\mysqldb.py\", line 175, in do_executemany\r\n    rowcount = cursor.executemany(statement, parameters)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 250, in executemany\r\n    self.rowcount = sum(self.execute(query, arg) for arg in args)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 250, in <genexpr>\r\n    self.rowcount = sum(self.execute(query, arg) for arg in args)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 174, in execute\r\n    self._discard()\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 95, in _discard\r\n    while con.next_result() == 0:  # -1 means no more data.\r\nMySQLdb.ProgrammingError: (2014, \"Commands out of sync; you can't run this command now\")\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Python310\\lib\\runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"c:\\Users\\antoine.brusco\\.vscode\\extensions\\ms-python.python-2023.14.0\\pythonFiles\\lib\\python\\debugpy\\__main__.py\", line 39, in <module>\r\n    cli.main()\r\n  File \"c:\\Users\\antoine.brusco\\.vscode\\extensions\\ms-python.python-2023.14.0\\pythonFiles\\lib\\python\\debugpy/..\\debugpy\\server\\cli.py\", line 430, in main\r\n    run()\r\n  File \"c:\\Users\\antoine.brusco\\.vscode\\extensions\\ms-python.python-2023.14.0\\pythonFiles\\lib\\python\\debugpy/..\\debugpy\\server\\cli.py\", line 284, in run_file\r\n    runpy.run_path(target, run_name=\"__main__\")\r\n  File \"c:\\Users\\antoine.brusco\\.vscode\\extensions\\ms-python.python-2023.14.0\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py\", line 321, in run_path   \r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"c:\\Users\\antoine.brusco\\.vscode\\extensions\\ms-python.python-2023.14.0\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py\", line 135, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"c:\\Users\\antoine.brusco\\.vscode\\extensions\\ms-python.python-2023.14.0\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py\", line 124, in _run_code  \r\n    exec(code, run_globals)\r\n  File \"D:\\Git\\NeuralNetworkInternship\\src_v2\\apps\\tmp.py\", line 17, in <module>\r\n    storage.upgrade()\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\optuna\\storages\\_rdb\\storage.py\", line 956, in upgrade\r\n    self._version_manager.upgrade()\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\optuna\\storages\\_rdb\\storage.py\", line 1131, in upgrade\r\n    alembic_command.upgrade(config, \"head\")\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\alembic\\command.py\", line 382, in upgrade\r\n    script.run_env()\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\alembic\\script\\base.py\", line 578, in run_env\r\n    util.load_python_file(self.dir, \"env.py\")\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\alembic\\util\\pyfiles.py\", line 93, in load_python_file\r\n    module = load_module_py(module_id, path)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\alembic\\util\\pyfiles.py\", line 109, in load_module_py\r\n    spec.loader.exec_module(module)  # type: ignore\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\optuna\\storages\\_rdb\\alembic\\env.py\", line 79, in <module>\r\n    run_migrations_online()\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\optuna\\storages\\_rdb\\alembic\\env.py\", line 73, in run_migrations_online\r\n    context.run_migrations()\r\n  File \"<string>\", line 8, in run_migrations\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\alembic\\runtime\\environment.py\", line 922, in run_migrations\r\n    self.get_context().run_migrations(**kw)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\alembic\\runtime\\migration.py\", line 624, in run_migrations\r\n    step.migration_fn(**kw)\r\n  File \"D:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\Lib\\site-packages\\optuna\\storages\\_rdb\\alembic\\versions\\v3.0.0.a.py\", line 169, in upgrade\r\n    raise e\r\n  File \"D:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\Lib\\site-packages\\optuna\\storages\\_rdb\\alembic\\versions\\v3.0.0.a.py\", line 162, in upgrade\r\n    persist(session, distributions)\r\n  File \"D:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\Lib\\site-packages\\optuna\\storages\\_rdb\\alembic\\versions\\v3.0.0.a.py\", line 141, in persist\r\n    session.bulk_save_objects(distributions)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 4412, in bulk_save_objects\r\n    self._bulk_save_mappings(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 4582, in _bulk_save_mappings\r\n    with util.safe_reraise():\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 147, in __exit__\r\n    raise exc_value.with_traceback(exc_tb)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 4563, in _bulk_save_mappings\r\n    bulk_persistence._bulk_update(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\orm\\bulk_persistence.py\", line 332, in _bulk_update\r\n    persistence._emit_update_statements(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py\", line 909, in _emit_update_statements\r\n    c = connection.execute(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1412, in execute\r\n    return meth(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\sql\\elements.py\", line 515, in _execute_on_connection\r\n    return connection._execute_clauseelement(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1635, in _execute_clauseelement\r\n    ret = self._execute_context(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1844, in _execute_context\r\n    return self._exec_single_context(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1984, in _exec_single_context\r\n    self._handle_dbapi_exception(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 2339, in _handle_dbapi_exception\r\n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1934, in _exec_single_context\r\n    self.dialect.do_executemany(\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\sqlalchemy\\dialects\\mysql\\mysqldb.py\", line 175, in do_executemany\r\n    rowcount = cursor.executemany(statement, parameters)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 250, in executemany\r\n    self.rowcount = sum(self.execute(query, arg) for arg in args)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 250, in <genexpr>\r\n    self.rowcount = sum(self.execute(query, arg) for arg in args)\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 174, in execute\r\n    self._discard()\r\n  File \"d:\\Git\\NeuralNetworkInternship\\.venv\\optuna_env\\lib\\site-packages\\MySQLdb\\cursors.py\", line 95, in _discard\r\n    while con.next_result() == 0:  # -1 means no more data.\r\nsqlalchemy.exc.ProgrammingError: (MySQLdb.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\")\r\n[SQL: UPDATE trial_params SET distribution_json=%s WHERE trial_params.param_id = %s]\r\n[parameters: [('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 1, \"low\": 2, \"high\": 10}}', 1), ('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 1, \r\n\"low\": 2, \"high\": 10}}', 2), ('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 1, \"low\": 2, \"high\": 10}}', 3), ('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 1, \"low\": 2, \"high\": 10}}', 4), ('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 1, \"low\": 2, \"high\": 10}}', 5), ('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 1, \"low\": 2, \"high\": 10}}', 6), ('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 1, \"low\": 2, \"high\": 10}}', 7), ('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 1, \"low\": 2, \"high\": 10}}', 8)  ... displaying 10 of 5000 total bound parameter sets ...  ('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 2, \"low\": 20, \"high\": 512}}', 4999), ('{\"name\": \"IntDistribution\", \"attributes\": {\"log\": false, \"step\": 2, \"low\": 20, \"high\": 22050}}', 5000)]]\r\n(Background on this error at: https://sqlalche.me/e/20/f405)\n```\n\n\n### Steps to reproduce\n\n1. Upgrade the database schema using either the CLI (`optuna storage upgrade --storage <URL>`) or the code below\r\n```python\r\nfrom optuna.storages import RDBStorage\r\n\r\nuser = <db_user> # all permissions granted\r\npassword = <user_pswd>\r\nhost = <db_url>\r\nport = <usually_3306>\r\ndb = <name_of_db>\r\n\r\noptuna_db = f\"mysql://{user}:{password}@{host}:{port}/{db}\"\r\nstorage = RDBStorage(optuna_db, skip_compatibility_check=True)\r\nstorage.upgrade()\r\n```\r\n\n\n### Additional context (optional)\n\nI'm using a fairly heavy database, since I've been using Optuna for some time.\r\nI backed up my database before trying to upgrade it, and the `.sql` file weights 500 MB of data.\r\nI also tried to update the database progressively (starting with Optuna version 3.0.0 until the last one) but the result is the same for Optuna 3.0.0.",
    "comments": [
      {
        "user": "a7brusco",
        "body": "Find a bit of time to debug that !\r\n\r\nAfter a lot of tries, I've managed to migrate the database (quick remember, my database is pretty huge).\r\n\r\nI've commented the block of code here: https://github.com/optuna/optuna/blob/c21fe6253343950f31d9ae0bc13be384236335e6/optuna/storages/_rdb/alembic/versions/v3.0.0.a.py#L161-L163\r\n\r\nI suppose during the migration to the schema 3.0.0a, I've reached the 5000 rows to update (the 5000 corresponds to the max `BATCH_SIZE` value set at the beginning of the script).\r\nI didn't really understand why that first batch couldn't be updated properly, tho.\r\n\r\nCommenting those lines above disabled the batch update process, the migration is done on all rows simultaneously, so it's a bit long (like around 5 min) but it's working."
      },
      {
        "user": "KirtiPriya07",
        "body": "do u still need help?"
      },
      {
        "user": "a7brusco",
        "body": "Not really, I've got it working with the small workaround I've described."
      }
    ]
  },
  {
    "issue_number": 4508,
    "title": "Use `__future__.annotations` everywhere in the Optuna code base",
    "author": "HideakiImamura",
    "state": "open",
    "created_at": "2023-03-10T12:01:15Z",
    "updated_at": "2025-01-16T01:50:21Z",
    "labels": [
      "code-fix"
    ],
    "body": "### Motivation\r\n\r\nOptuna drops Python 3.6 from v3.1, so we can use `__future__.annotations`, which simplifies the code base. See [PEP 563](https://peps.python.org/pep-0563/), [PEP584](https://peps.python.org/pep-0584/), [PEP 585](https://peps.python.org/pep-0585/), and [PEP 604](https://peps.python.org/pep-0604/) for more details. This issue suggests to use the module and simplifies the code base.\r\n\r\n### Suggestion\r\n\r\nUse `__future__.annotations` for each file and simplify the type annotations. The list of classes whose type annotations can be simplified is [here](https://peps.python.org/pep-0585/#implementation). The list of files where the `__future__.annotations` can be used is as follows. In order to reduce review costs and to encourage more contributors to work on it, please, as a rule, fix one file per PR.\r\n\r\n> [!NOTE]\r\n> All the files were resolved, thank you everyone for the contributions!\r\n> We are now waiting for Python 3.8 drop to address the files below.\r\n\r\n### Files that cannot be updated now\r\n\r\n> [!IMPORTANT]\r\n> Currently (23 Dec 2024), Optuna supports Python 3.8, making it impossible to reduce some typing.\r\n> For example, `|` cannot be used in a type declaration and we need to use `typing.Union` instead.\r\n> These typings can only be eliminated after we drop Python 3.8 support.\r\n> Please note that any other type annotations can be replaced as long as we have the line `from __future__ import annotations`.\r\n\r\nThe following files cannot be updated as long as we support Python 3.8.\r\n\r\n- [ ] optuna/samplers/_grid.py\r\n    - Cannot reduce `Union` in the type definition.\r\n- [ ] optuna/samplers/_cmaes.py\r\n    - Cannot reduce `Union` in the type definition.\r\n- [ ] optuna/samplers/_tpe/probability_distributions.py\r\n    - Cannot reduce `Union` in the type definition.\r\n- [ ] optuna/study/study.py\r\n    - Cannot reduce `Callable` and `Sequence` in the type definition.\r\n- [ ] optuna/_typing.py\r\n    - Cannot reduce `Union` in the type definition.\r\n- [ ] optuna/trial/_frozen.py\r\n    - Cannot reduce `Dict` in the casting.\r\n- [ ] optuna/distributions.py\r\n    - Cannot reduce `Union` in the type definition.\r\n- [ ] tests/study_tests/test_study.py\r\n    - Cannot reduce `Callable` in the type definition.\r\n\r\n### Additional context (optional)\r\n\r\nThe above list is generated by the following script.\r\n\r\n<details>\r\n<summary>script</summary>\r\n\r\n```python\r\nimport os\r\nimport pathlib\r\n\r\n\r\nPATTERNS = [\r\n    \"from typing import Union\",\r\n    \"from typing import DefaultDict\",\r\n    \"from typing import Optional\",\r\n    \"from typing import Tuple\",\r\n    \"from typing import Iterator\",\r\n    \"from typing import List\",\r\n    \"from typing import Dict\",\r\n    \"from typing import Set\",\r\n    \"from typing import Container\",\r\n    \"from typing import Type\",\r\n    \"from typing import FrozenSet\",\r\n    \"from typing import Callable\",\r\n    \"from typing import Sequence\",\r\n    \"from typing import ValuesView\",\r\n]\r\n\r\n\r\ndef get_filenames_to_be_simplified(dir_path):\r\n    ret = []\r\n    for f in os.listdir(dir_path):\r\n        file_path = os.path.join(dir_path, f)\r\n        if not os.path.isfile(file_path):\r\n            ret.extend(get_filenames_to_be_simplified(file_path))\r\n        else:\r\n            try:\r\n                with open(file_path) as fd:\r\n                    contents = fd.read()\r\n                    if any([s in contents for s in PATTERNS]):\r\n                        ret.append(str(file_path))\r\n            except UnicodeDecodeError as e:\r\n                pass\r\n    return ret\r\n\r\n\r\ndef main():\r\n    dirs = [\"optuna\", \"tests\", \"benchmarks\"]\r\n\r\n    for dir_name in dirs:\r\n        filenames = get_filenames_to_be_simplified(pathlib.Path(dir_name))\r\n        for filename in filenames:\r\n            print(f\"- [ ] {filename}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "harupy",
        "body": "@HideakiImamura I believe the following command is all we need to close this issue:\r\n\r\n```\r\npip install -U ruff && echo -n '\\n[tool.ruff.isort]\\nrequired-imports = [\"from __future__ import annotations\"]\\n' >> pyproject.toml && ruff --fix --select UP007,UP006,F401,I002 optuna tests\r\n```"
      },
      {
        "user": "harupy",
        "body": "A breakdown of the command:\r\n\r\n- `pip install -U ruff`: Installs [ruff](https://github.com/charliermarsh/ruff)\r\n- `UP007`: Fixes `Union[x, y]` to `x | y`\r\n- `UP006`: Fixes collection types (e.g. `List[str]` -> `list[str]`)\r\n- `F401`: Removes unused imports such as `from typing import List`\r\n- `I002`: Enforces `from __future__ import annotations`"
      },
      {
        "user": "not522",
        "body": "@harupy Thank you for your great suggestion!\r\n\r\n----\r\n\r\nPEP 604 seems too early for Optuna yet. It was introduced in Python 3.10."
      }
    ]
  },
  {
    "issue_number": 685,
    "title": "a method for deleting trials",
    "author": "dwiel",
    "state": "closed",
    "created_at": "2019-11-08T18:53:49Z",
    "updated_at": "2025-01-15T20:13:43Z",
    "labels": [
      "feature"
    ],
    "body": "It should be possible to delete individual trials from studies.\r\n\r\n## Motivation\r\n\r\nThere are many reasons you might want to remove a trial from a study. Some examples include:\r\n\r\n- a trial has been determined to be invalid\r\n- old trials are no longer relevant\r\n- in an evolutionary search where trials represent individuals",
    "comments": [
      {
        "user": "sile",
        "body": "Thank you for your proposal!\r\n\r\nWe plan to introduce an API that allows users to change the state of a trial as follows:\r\n```python\r\nfor trial in study.trials:\r\n    if \"the trial is invalid\":\r\n        # [New API]\r\n        # Sets the state of the trials to `FAIL`.\r\n        # Failed trials aren't considered when sampling new parameters.\r\n        study.set_trial_state(trial, optuna.structs.TrialState.FAIL)\r\n```\r\n\r\nDoes this API satisfy your requirements?"
      },
      {
        "user": "dwiel",
        "body": "Yeah I think that should do the trick, thanks!"
      },
      {
        "user": "louisabraham",
        "body": "Hi, great thread!\r\n\r\n- Do you still consider adding this method?\r\n\r\n- How are FAIL-ed trials handled by the samplers? Is is really equivalent to deleting the trials?\r\n\r\n**EDIT**: they are just ignored in most samplers that test for `structs.TrialState.COMPLETE`. Maybe it should be documented.\r\n\r\n- I think it would still be useful to have a delete_trial method to clean a history but the implementation complexity and the database corruption risks might not be worth it"
      }
    ]
  },
  {
    "issue_number": 5347,
    "title": "Replace the hyperlinks of integration files to ones in `optuna-integration` or move them to integration docs",
    "author": "nzw0301",
    "state": "open",
    "created_at": "2024-03-19T17:33:41Z",
    "updated_at": "2025-01-13T23:23:30Z",
    "labels": [
      "document"
    ],
    "body": "### What is an issue?\n\nLinks of integration module in https://optuna.readthedocs.io/en/stable/reference/integration.html to corresponding `optuna/optuna`'s files, which are almost empty thanks to the integration migration (see https://github.com/optuna/optuna/issues/4484 for more details). So I think the current links are not informative and I'd like to suggest replace the link with new locations in https://github.com/optuna/optuna-integration if this page remains. Otherwise, I'd suggest moving this page, at least the list, to integration docs. \r\n\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5321,
    "title": "Nested studies for Nested Cross-Validation Support",
    "author": "Jose-Verdu-Diaz",
    "state": "closed",
    "created_at": "2024-03-15T08:53:10Z",
    "updated_at": "2025-01-12T23:05:36Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nWhen training a model using a nested cross-validation approach, different studies are created for each split of the outer cross-validation. However, it would be interesting to organize all these studies into a \"Parent Study\" showing the overall result of the experiment (i.e Mean Accuracy of all outer splits).\n\n### Description\n\nThis \"Parent Study\" could work as a wrapper of all children studies, showing averaged metrics from all the children studies and giving an overview of the Nested Cross-Validation results. Therefore, The Parent Study may be implemented using a separate wrapper class containing a list of children and user-defined averaged results.\r\n\r\nIn the optuna-dashboard, the Parent Study could be displayed as a folder or as a dropdown in the navbar.\n\n### Alternatives (optional)\n\nThe current straight-forward approach is to collect all outer split results into a single study and create Attributes showing the results. This is suboptimal, as all the different splits are shown together in the training and hyperparameter graphs, hiding the fact that every outer split is completely independent and making the graphs unusable.\r\n\r\nAnother solution is to build scripts to edit the optuna study database, which forces the user to abandon the optuna framework.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Thank you for feature request. Do you think https://github.com/optuna/optuna/issues/5118 is the same as this request?"
      },
      {
        "user": "Jose-Verdu-Diaz",
        "body": "@nzw0301 Thank you for your reply. #5118 seems to have a confusing objective, and the answers seem to solve the issue using features already implemented in Optuna.\r\n\r\nThe proposed feature here might also help in #5118 , but I would consider this as an independent issue. See below a diagram of the proposed structure. The role of Parent Wrapper is to allow the comparison between different runs of nested cross-validation, keeping the study database table organized and the optuna-dashboard tidy.  Maybe we can adopt the term \"**Experiment**\" for this new class?\r\n\r\n![optuna-nestedcv](https://github.com/optuna/optuna/assets/21974160/ee16aeef-7c32-4c00-aaa5-3c87bbea8659)\r\n\r\nThe results of individual Studies are irrelevant when assessing the results of nested cross-validation, as we are interested in the overall performance of all studies (i.e mean accuracy of the best model of each Study). The \"Experiment\" class could implement a method `set_metric()` or equivalent to manually set the overall performance metric of the experiment. This Parent Wrapper could be used for different uses than nested cross-validation, allowing users to organize studies into groups.\r\n\r\nI think users should not be forced to create an Experiment to be able to create Studies, as this would alter current implementations with Optuna and many users won't need Experiments at all. Instead, an Experiment object could be created and passed when creating Study objects. The Study database table could then store a Foreign Key to the parent Experiment. See below an example usage:\r\n\r\n```python\r\nimport optuna\r\nimport numpy as np\r\nfrom sklearn.model_selection import KFold, cross_val_score\r\n\r\n'''\r\nSuppose that `X` and `y` are the input features and\r\ninput labels, respectively.\r\n\r\nSuppose that `pipeline` is an sklearn Pipeline.\r\n'''\r\n\r\nsearch_spaces = {\r\n    'param_1': optuna.distributions.FloatDistribution(0.001, 0.4, log=True),\r\n    'param_2': optuna.distributions.IntDistribution(10, 500),\r\n}\r\n\r\nstorage = 'sqlite:///example-storage.db'\r\n\r\n# Here, we create the new Experiment object\r\nexperiment = Experiment(\r\n    experiment_name = 'nestedCV-1',\r\n    storage = storage\r\n)\r\n\r\nouter_scores = []\r\ncv_outer = KFold()\r\nfor split, (train_idx, test_idx) in enumerate(cv_outer.split(X, y)):\r\n\r\n    study = optuna.create_study(\r\n        study_name=f'study_{split}', \r\n        storage=storage, \r\n        direction='maximize',\r\n        experiment=experiment # We assign this study to the experiment object we defined\r\n    )\r\n\r\n    X_train, X_test = X[train_idx], X[test_idx]\r\n    y_train, y_test = y[train_idx], y[test_idx]\r\n\r\n    def objective(trial):\r\n        trial_params = {key: trial._suggest(key, dist) for key, dist in search_spaces.items()}\r\n\r\n        pipeline.set_params(**trial_params)\r\n        cv_inner = KFold()   \r\n        scores = cross_val_score(pipeline, X_train, y_train, cv=cv_inner)\r\n        return scores.mean()\r\n    \r\n    study.optimize(objective, n_trials=100)\r\n\r\n    best_params = study.best_params\r\n    pipeline.set_params(**best_params)\r\n    pipeline.fit(X_train, y_train)\r\n    test_score = pipeline.score(X_test, y_test)\r\n    outer_scores.append(test_score)\r\n\r\nexperiment.set_metric('Mean Accuracy', np.mean(outer_scores)) # We add the overall performance metric (similar to `set_user_attr()`)\r\n```\r\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5556,
    "title": "Prohibit set as a return of `sample_relative`",
    "author": "nabenabe0928",
    "state": "closed",
    "created_at": "2024-07-09T10:12:12Z",
    "updated_at": "2025-01-10T03:43:57Z",
    "labels": [
      "feature",
      "needs-discussion"
    ],
    "body": "### Motivation\r\n\r\nThe current implementation allows samplers to return non-dict objects from `sample_relative`.\r\nAlthough it should be taken care by the developers, it is possible to return `set` instead of `dict`  without noticing it like this (for more details, please defer **Code Example** below):\r\n\r\n```python\r\n        # This is correct.\r\n        # return {param_name: param_value for param_name, param_value in zip(search_space, sample)}\r\n        # This is wrong (the following became a set instead of dict).\r\n        return {param_value for param_name, param_value in zip(search_space, sample)}\r\n```\r\n\r\nCurrently, samplers fall back to `sample_indepent` silently, but this is a bit buggy if users would like to customize samplers.\r\n\r\n<details>\r\n\r\n<summary>Code Example</summary>\r\n\r\n```python\r\nfrom __future__ import annotations\r\n\r\nfrom typing import Any\r\n\r\nimport numpy as np\r\n\r\nimport optuna\r\n\r\n\r\nclass AroundTopTrialSampler(optuna.samplers.BaseSampler):\r\n    def __init__(self, seed: int = 42, n_startup_trials: int = 10, top_quantile: float = 0.1):\r\n        assert 0 <= top_quantile <= 1\r\n        assert n_startup_trials >= 2\r\n        self._rng = np.random.RandomState(seed)\r\n        self._top_quantile = top_quantile\r\n        self._n_startup_trials = n_startup_trials\r\n        self._uniform_sampler = optuna.samplers.RandomSampler(seed=self._rng.randint(1 << 30))\r\n\r\n    def infer_relative_search_space(\r\n        self, study: optuna.Study, trial: optuna.trial.FrozenTrial\r\n    ) -> dict[str, optuna.distributions.BaseDistribution]:\r\n        # This is a boilerplate.\r\n        return optuna.search_space.intersection_search_space(study.get_trials(deepcopy=False))\r\n\r\n    def sample_independent(\r\n        self,\r\n        study: optuna.Study,\r\n        trial: optuna.trial.FrozenTrial,\r\n        param_name: str,\r\n        param_distribution: optuna.distributions.BaseDistribution,\r\n    ) -> Any:\r\n        return self._uniform_sampler.sample_independent(\r\n            study, trial, param_name, param_distribution\r\n        )\r\n\r\n    def sample_relative(\r\n        self,\r\n        study: optuna.Study,\r\n        trial: optuna.trial.FrozenTrial,\r\n        search_space: dict[str, optuna.distributions.BaseDistribution],\r\n    ) -> dict[str, Any]:\r\n        if any(\r\n            not isinstance(dist, optuna.distributions.FloatDistribution) or dist.log or dist.step\r\n            for dist in search_space.values()\r\n        ):\r\n            # Raise an error if search_space is not supported by your sampler.\r\n            raise ValueError(f\"{self.__class__.__name__} supports only simple FloatDistribution.\")\r\n\r\n        # Get complete trials. Each trial has one of the following states:\r\n        # RUNNING, WAITING, COMPLETE, PRUNED, FAIL\r\n        complete_trials = study.get_trials(\r\n            deepcopy=False, states=(optuna.trial.TrialState.COMPLETE,)\r\n        )\r\n        if len(complete_trials) < self._n_startup_trials:\r\n            # Initial sampling.\r\n            return {\r\n                param_name: self._uniform_sampler.sample_independent(\r\n                    study, trial, param_name, dist\r\n                ) for param_name, dist in search_space.items()\r\n            }\r\n\r\n        is_minimization = study.direction == optuna.study.StudyDirection.MINIMIZE\r\n        loss_values = [t.value if is_minimization else -t.value for t in complete_trials]\r\n        params = [t.params for t in complete_trials]\r\n        return self._sample(search_space, params, loss_values)\r\n\r\n    def _sample(\r\n        self,\r\n        search_space: dict[str, optuna.distributions.BaseDistribution],\r\n        params: list[dict[str, Any]],\r\n        loss_values: list[float],\r\n    ) -> dict[str, Any]:\r\n        # search_space has the search range information for each parameter.\r\n        lows = np.asarray([search_space[param_name].low for param_name in search_space])\r\n        highs = np.asarray([search_space[param_name].high for param_name in search_space])\r\n\r\n        n_top_trials = max(int(np.ceil(len(params) * self._top_quantile)), 2)\r\n        top_indices = np.argsort(loss_values)[:n_top_trials]\r\n\r\n        top_params = [[params[i][param_name] for param_name in search_space] for i in top_indices]\r\n        top_mean = np.mean(top_params, axis=0)\r\n        top_std = np.std(top_params, axis=0)\r\n        sample = np.clip(\r\n            self._rng.multivariate_normal(mean=top_mean, cov=np.diag(top_std)), lows, highs\r\n        )\r\n        # This is correct.\r\n        # return {param_name: param_value for param_name, param_value in zip(search_space, sample)}\r\n        # This is wrong (the following became a set instead of dict).\r\n        return {param_value for param_name, param_value in zip(search_space, sample)}\r\n\r\n\r\ndef sphere(trial: optuna.Trial) -> float:\r\n    X = np.asarray([trial.suggest_float(f\"x{i}\", -5, 5) for i in range(5)])\r\n    return np.sum(X ** 2)\r\n\r\n\r\nstudy = optuna.create_study(sampler=AroundTopTrialSampler(seed=42, top_quantile=0.1))\r\nstudy.optimize(sphere, n_trials=20)\r\n\r\n```\r\n\r\n</details>\r\n\r\n### Description\r\n\r\nWe can simply check if `sample_relative` returned a set or not by adding an assertion [here](https://github.com/optuna/optuna/blob/master/optuna/trial/_trial.py#L75).\r\n\r\n```python\r\nassert isinstance(self._relative_params, set), \"sample_relative returned set instead of dict.\"\r\n```",
    "comments": []
  },
  {
    "issue_number": 3676,
    "title": "The `optuna.samplers._tpe.sampler._calculate_weights_below_for_multi_objective` should consider `+/-inf`",
    "author": "HideakiImamura",
    "state": "closed",
    "created_at": "2022-06-15T03:41:57Z",
    "updated_at": "2025-01-10T03:43:04Z",
    "labels": [
      "bug",
      "description-checked"
    ],
    "body": "### Expected behavior\r\n\r\nShould be resolved after #3674.\r\n\r\nThe `optuna.samplers._tpe.sampler._calculate_weights_below_for_multi_objective` should consider `+/-inf`.\r\n\r\nCurrently, the `nan`'s list is returned when the trials include `+/-inf` as objective values, but it should return the list of floats. This might be fixed in #3674.\r\n\r\n### Environment\r\n\r\n- Optuna version: current master [d3c53e0](https://github.com/optuna/optuna/commit/d3c53e053a18341405cdfc9587d4525fdce7494d)\r\n- Python version: 3.9\r\n- OS: mac, linux\r\n- (Optional) Other libraries and their versions:\r\n\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\n(venv) mamu@HideakinoMacBook-puro 3662 % python fuga.py\r\n/Users/mamu/Dev/pfn/optuna/optuna/_hypervolume/utils.py:14: RuntimeWarning: invalid value encountered in subtract\r\n  return float(np.abs(np.prod(point1 - point2)))\r\n[nan nan nan]\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\n```python\r\nimport optuna\r\nimport numpy as np\r\n\r\n\r\n# +/-inf objective values.\r\nweights_below = optuna.samplers._tpe.sampler._calculate_weights_below_for_multi_objective(\r\n    {\"x\": np.array([1.0, 2.0, 3.0, 4.0], dtype=float)},\r\n    [\r\n        (0, [-float(\"inf\"), -float(\"inf\")]),\r\n        (0, [0.0, -float(\"inf\")]),\r\n        (0, [-float(\"inf\"), 0.0]),\r\n        (0, [float(\"inf\"), float(\"inf\")]),\r\n    ],\r\n    np.array([0, 1, 2]),\r\n    None\r\n)\r\nprint(weights_below)\r\n\r\n```\r\n\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "sidshrivastav",
        "body": "@HideakiImamura hey, I tried to reproduce the issue, I think its fixed now."
      },
      {
        "user": "HideakiImamura",
        "body": "Ops. Really?\r\n\r\n@contramundum53 Do you have any opinions this?\r\n"
      },
      {
        "user": "contramundum53",
        "body": "It isn't fixed in my environment 🤔 \r\n```\r\nPython 3.9.12 (main, May  8 2022, 17:57:49)\r\n[Clang 13.1.6 (clang-1316.0.21.2)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import optuna\r\n>>> import numpy as np\r\n>>> optuna.__version__\r\n'3.0.0rc0.dev'\r\n>>>\r\n>>>\r\n>>> # +/-inf objective values.\r\n>>> weights_below = optuna.samplers._tpe.sampler._calculate_weights_below_for_multi_objective(\r\n...     {\"x\": np.array([1.0, 2.0, 3.0, 4.0], dtype=float)},\r\n...     [\r\n...         (0, [-float(\"inf\"), -float(\"inf\")]),\r\n...         (0, [0.0, -float(\"inf\")]),\r\n...         (0, [-float(\"inf\"), 0.0]),\r\n...         (0, [float(\"inf\"), float(\"inf\")]),\r\n...     ],\r\n...     np.array([0, 1, 2]),\r\n...     None)\r\n>>> print(weights_below)\r\n[nan nan nan]\r\n>>>\r\n```"
      }
    ]
  },
  {
    "issue_number": 5203,
    "title": "Speed up `_hypervolume.WFG`",
    "author": "HideakiImamura",
    "state": "open",
    "created_at": "2024-01-26T06:53:55Z",
    "updated_at": "2025-01-10T03:37:01Z",
    "labels": [
      "enhancement",
      "contribution-welcome"
    ],
    "body": "### What is an issue?\n\nOptuna has implemented the _hypervolume.WFG class for calculating hypervolume, but this implementation is naive and slow. Speeding this up would be of great benefit in multi-objective optimization. For example, https://ieeexplore.ieee.org/document/1583625 could be a candidate for the algorithm.",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "Just in case, I am already working on a faster implementation of WFG.\r\nhttps://github.com/nabenabe0928/optuna/compare/enhance/speed-up-non-dominated-sort...nabenabe0928:optuna:enhance/speed-up-wfg"
      },
      {
        "user": "nabenabe0928",
        "body": "As mentioned [here](https://github.com/optuna/optuna/pull/5504#issuecomment-2196548251),  hypervolume computation for 3D or more requires a special data structure.\r\n\r\nSo if anyone would like to contribute to this issue, please note that it may take a longer time to merge your PR as your PR needs to add an additional dependency or implementation of a special data structure.\r\n\r\nHere is an example of HV3D.\r\n\r\n- https://github.com/optuna/optuna/compare/master...nabenabe0928:add-hv3d?expand=1"
      },
      {
        "user": "nabenabe0928",
        "body": "Once we introduce the box decomposition, we can replace the WFG with it for a small number of objectives (up to 4 or 5 objectives)\r\n\r\n- https://github.com/optuna/optuna/compare/master...nabenabe0928:temp/add-log-ehvi?expand=1"
      }
    ]
  },
  {
    "issue_number": 5909,
    "title": "Adding secondary indexes in foreign key for SQL database tables",
    "author": "korntewin",
    "state": "closed",
    "created_at": "2024-12-31T12:37:47Z",
    "updated_at": "2025-01-10T01:33:36Z",
    "labels": [
      "code-fix"
    ],
    "body": "### Motivation\n\nHello, I really appreciate the Optuna framework (especially the Optuna dashboard 🥹), and currently using it for my personal project.\r\n\r\nMy use case involves a single study generating a large number of trials (each trial is very quick to compute). In this scenario, the Optuna dashboard becomes very slow, primarily due to the querying time.\r\n\r\nI investigated the [alembic migration code](https://github.com/optuna/optuna/tree/master/optuna/storages/_rdb/alembic/versions) and found that there are no secondary indexes on the foreign keys. This lack of indexing causes slowness when looking up tables using these foreign keys.\r\n\n\n### Suggestion\n\nConsider adding the secondary index for every foreign keys for fast lookup in the alembic migration code.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "korntewin",
        "body": "Oh, my bad. After carefully checking, there was already necessary secondary indexes and by querying database directly it is very fast. \r\n\r\nThe root cause seems to be backend server implementation which I solved by developing my web app using `dioxus` in Rust (from 1 minute loading time to milli seconds interval). \r\n\r\nFeel free to close this issue 🥹."
      },
      {
        "user": "not522",
        "body": "Thanks. Let me close this issue."
      }
    ]
  },
  {
    "issue_number": 5814,
    "title": "`retry_history` method in `RetryFailedTrialCallback` is not tested",
    "author": "not522",
    "state": "closed",
    "created_at": "2024-12-02T08:17:31Z",
    "updated_at": "2025-01-10T00:30:01Z",
    "labels": [
      "test",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Motivation\n\nThere are no tests for the `retry_history` method.\n\n### Suggestion\n\nThis test should be located in `tests/storages_tests/test_callbacks.py`.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "iamarunbrahma",
        "body": "I will create a PR for this one"
      },
      {
        "user": "HideakiImamura",
        "body": "@not522 Is this issue closed?"
      },
      {
        "user": "HideakiImamura",
        "body": "Let me close this issue since this was resolved in #5865."
      }
    ]
  },
  {
    "issue_number": 5815,
    "title": "Add `value` for `--format` option in CLI",
    "author": "not522",
    "state": "closed",
    "created_at": "2024-12-02T09:23:12Z",
    "updated_at": "2025-01-10T00:29:36Z",
    "labels": [
      "feature",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Motivation\n\nThe `value` format has been implemented for `study-names` subcommand, but other subcommands don't have the `value` format.\r\n\r\nhttps://github.com/optuna/optuna/blob/b29fdff87e46fd1c1e025e23af005c9a9ad80a11/optuna/cli.py#L395\r\n\r\nhttps://github.com/optuna/optuna/blob/b29fdff87e46fd1c1e025e23af005c9a9ad80a11/optuna/cli.py#L426\n\n### Description\n\nI think just adding `value` to the `--format` option will work.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "iamarunbrahma",
        "body": "I will create a PR for this one"
      },
      {
        "user": "HideakiImamura",
        "body": "Let me close this issue since this was resolved in #5822."
      }
    ]
  },
  {
    "issue_number": 5805,
    "title": "Add 404 Not Found pages in the document",
    "author": "not522",
    "state": "closed",
    "created_at": "2024-11-29T05:04:33Z",
    "updated_at": "2025-01-09T10:25:34Z",
    "labels": [
      "document",
      "contribution-welcome"
    ],
    "body": "### What is an issue?\n\nThe Optuna document uses the default 404 page, but it is too simple and helpless. (For example, try to access https://optuna.readthedocs.io/wrong_url.)\r\n[The official document of Read the Docs](https://docs.readthedocs.io/en/stable/reference/404-not-found.html) recommends custom 404 pages. I think that the [sphinx-notfound-page](https://pypi.org/project/sphinx-notfound-page) extension is good for Optuna.",
    "comments": []
  },
  {
    "issue_number": 5919,
    "title": "Default to BaseSampler for load_study",
    "author": "joansigh",
    "state": "closed",
    "created_at": "2025-01-08T19:13:46Z",
    "updated_at": "2025-01-09T03:24:34Z",
    "labels": [
      "question"
    ],
    "body": "\r\n\r\n\"Thanks for your reply. I found out what the issue was, If you load a study using \"optuna.study.load_study\" the settings for the study are not loaded and they revert to default. Which in this case meant the sampler was changing from NSGAIII to the BaseSampler\"\r\n\r\nThis comment is from this issue: https://github.com/optuna/optuna/issues/5397\r\n\r\n\r\n\r\nI am looking at the codebase here: https://github.com/optuna/optuna/blob/755195c3bfc25d181e3f9d248974f3bf2eac5efc/optuna/study/study.py#L1295\r\n\r\nI am unable to find in the code base where BaseSampler is set as the default.\r\n\r\n\r\n\r\nIn what files is BaseSampler set as the default? ",
    "comments": []
  },
  {
    "issue_number": 5917,
    "title": "Code Hangs during the implementation of DDP ",
    "author": "Mukil07",
    "state": "closed",
    "created_at": "2025-01-07T10:23:17Z",
    "updated_at": "2025-01-07T12:20:57Z",
    "labels": [],
    "body": "### Don't use GitHub Issues to ask support questions.\n\nThe code works perfectly fine for single gpu and also with multigpu (DDP). But while using optuna, the code hangs until the GPU processes gets killed at this line `model = DDP(model)`. \r\n\r\nI have followed the same procedure followed by this example \r\nhttps://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_distributed_simple.py\r\n\r\nIm trying to tune only the learning rate and not the model. Also using custom dataloader, and train loops. ",
    "comments": []
  },
  {
    "issue_number": 5297,
    "title": "Add a way to make the behavior of Optuna Terminator deterministic.",
    "author": "eukaryo",
    "state": "closed",
    "created_at": "2024-03-06T06:31:11Z",
    "updated_at": "2024-12-31T23:05:41Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nI want a way to make the behavior of Terminator completely deterministic.\n\n### Description\n\nAdd an optional argument to fix the PRNG-seed of Terminator and make the bahavior deterministic\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5286,
    "title": "Add examples of newly added feature to the documents",
    "author": "Alnusjaponica",
    "state": "closed",
    "created_at": "2024-02-29T02:43:56Z",
    "updated_at": "2024-12-25T23:06:01Z",
    "labels": [
      "document",
      "stale"
    ],
    "body": "### What is an issue?\n\n## Motivation\r\n\r\nFollow up https://github.com/optuna/optuna/pull/4986 and https://github.com/optuna/optuna/pull/4871\r\n\r\n## Details\r\n\r\nUse newly added feature in the examples written in docstrings. To be specific,\r\nhttps://github.com/optuna/optuna/blob/2f279f321664bf8793914f442ca13b1b97495b2b/optuna/visualization/_param_importances.py#L123-L144 sould be rewritten or have new examples.\r\n\r\nAlso, one of the tasks described in https://github.com/optuna/optuna/pull/4871#issue-1848196133 seems not resolved.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5873,
    "title": "The explanation of how to optimize the acquisition function in `GPSampler` is outdated",
    "author": "HideakiImamura",
    "state": "open",
    "created_at": "2024-12-22T08:14:13Z",
    "updated_at": "2024-12-22T08:14:18Z",
    "labels": [
      "document",
      "contribution-welcome"
    ],
    "body": "### What is an issue?\n\n## Motivations\r\nThe explanation of how to optimize the acquisition function in `GPSampler` is outdated.\r\n\r\n[Currently](https://github.com/optuna/optuna/commit/bf5519e9b6856d71c83d9b533cd738541c35d2ab), it is described as `Quasi-Monte Carlo (QMC) sampling to optimize the acquisition function.` in the [document](https://optuna.readthedocs.io/en/latest/reference/samplers/generated/optuna.samplers.GPSampler.html).\r\n\r\nHowever, current implementation to optimize the acquisition function is as follows. See https://github.com/optuna/optuna/blob/master/optuna/_gp/optim_mixed.py for more details.\r\n- Using Quasi Monte-Carlo (QMC) sampling, initial values are sampled and multiple local searches are conducted to return the result with the best acquisition function value. The generation of initial values involves using QMC sampling to select multiple initial values, and then performing a roulette selection from these values to favor those with better acquisition function values.\r\n- Local search is performed using the L-BFGS-B method for continuous variables, linear search for discrete variables, and exhaustive search for categorical variables. Notably, the linear search method applied for discrete variables optimizes a function that returns the linearly interpolated value of the evaluation values of the nearest points on the grid to a given x, and the closest value on the grid to the optimal solution is then returned.\r\n- During the local search, the convergence is determined by continuously checking if the acquisition function value has improved. If no improvement is observed, it is quickly determined that convergence has been reached and the search is terminated.\r\n\r\n## What should we do?\r\nUpdate the document of `GPSampler` to align the current implementation. I think we need to succinctly summarize the key points from the information described above, or actual source codes.",
    "comments": []
  },
  {
    "issue_number": 5829,
    "title": "Hide methods and attributes derived from `Enum.IntEnum` from the `optuna.study.StudyDirection` documentation.",
    "author": "y0z",
    "state": "closed",
    "created_at": "2024-12-05T05:06:54Z",
    "updated_at": "2024-12-19T05:04:18Z",
    "labels": [
      "document",
      "contribution-welcome"
    ],
    "body": "### What is an issue?\r\n\r\nIn the [`optuna.study.StudyDirection`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.StudyDirection.html#optuna-study-studydirection) documentation, some methods and attributes derived from `Enum.IntEnum` are shown.\r\nHowever, it is not helpful to users (e.g., there will be no situation where the user wants to know the `bit_length` of `StudyDirection.MINIMIZE`). Therefore, we want to hide the information about these elements from the doc.\r\n\r\n<img width=\"1114\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5cf2892a-e3a0-4c38-b061-c8b78079936b\">",
    "comments": [
      {
        "user": "y0z",
        "body": "Based on the discussion at https://github.com/optuna/optuna/pull/5863, we cannot find a simple solution.\r\nTherefore, I close this issue at the moment."
      }
    ]
  },
  {
    "issue_number": 5830,
    "title": "Hide methods and attributes derived from `Enum.IntEnum` from the `optuna.trial.TrialState` documentation.",
    "author": "y0z",
    "state": "closed",
    "created_at": "2024-12-05T05:20:45Z",
    "updated_at": "2024-12-19T05:04:09Z",
    "labels": [
      "document",
      "contribution-welcome"
    ],
    "body": "### What is an issue?\r\n\r\nThis issue is similar to https://github.com/optuna/optuna/issues/5829.\r\n\r\nIn the [optuna.trial.TrialState](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.TrialState.html) documentation, some methods and attributes derived from `Enum.IntEnum` are shown.\r\nHowever, it is not helpful to users. Therefore, we want to hide the information about these elements from the doc.\r\n\r\n<img width=\"814\" alt=\"image\" src=\"https://github.com/user-attachments/assets/60bccb0d-61ad-441c-87ea-9c2f2850a092\">\r\n",
    "comments": [
      {
        "user": "fusawa-yugo",
        "body": "I'd like to take on the issue."
      },
      {
        "user": "y0z",
        "body": "Based on the discussion at https://github.com/optuna/optuna/pull/5863, we cannot find a simple solution.\r\nTherefore, I close this issue at the moment."
      }
    ]
  },
  {
    "issue_number": 5868,
    "title": "No module named 'cmaes'",
    "author": "HuXiLiFeng",
    "state": "closed",
    "created_at": "2024-12-19T03:39:48Z",
    "updated_at": "2024-12-19T03:42:24Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nI just want to run a test code using cmaesSampler, but generate a error\n\n### Environment\n\n- Optuna version: Optuna version:4.1.0\r\n- Python version: Python version:3.10.9\r\n- OS:OS:Linux-5.15.0-78-generic-x86_64-with-glibc2.35\r\n- (Optional) Other libraries and their versions:\r\n-- optuna                                   4.1.0\r\n-- optuna-dashboard                         0.17.0\r\n-- optuna-integration                       4.1.0\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\n[I 2024-12-19 11:37:29,725] A new study created in memory with name: no-name-b5e1963a-8c37-4e91-9e17-2cbe074219e4\r\n[I 2024-12-19 11:37:29,726] Trial 0 finished with value: 0.5227885610219444 and parameters: {'x': 0.7230411890217212, 'y': 0}. Best is trial 0 with value: 0.5227885610219444.\r\n[W 2024-12-19 11:37:29,726] Trial 1 failed with parameters: {} because of the following error: ModuleNotFoundError(\"No module named 'cmaes'\").\r\nTraceback (most recent call last):\r\n  File \"/home/tomas/miniconda3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\r\n    value_or_values = func(trial)\r\n  File \"/home/tomas/project/tmk_mining/event_extraction/auto_merge/test.py\", line 5, in objective\r\n    x = trial.suggest_float(\"x\", -1, 1)\r\n  File \"/home/tomas/miniconda3/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 161, in suggest_float\r\n    suggested_value = self._suggest(name, distribution)\r\n  File \"/home/tomas/miniconda3/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 627, in _suggest\r\n    elif self._is_relative_param(name, distribution):\r\n  File \"/home/tomas/miniconda3/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 659, in _is_relative_param\r\n    if name not in self.relative_params:\r\n  File \"/home/tomas/miniconda3/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 71, in relative_params\r\n    self._relative_params = self.study.sampler.sample_relative(\r\n  File \"/home/tomas/miniconda3/lib/python3.10/site-packages/optuna/samplers/_cmaes.py\", line 417, in sample_relative\r\n    optimizer = self._init_optimizer(\r\n  File \"/home/tomas/miniconda3/lib/python3.10/site-packages/optuna/samplers/_cmaes.py\", line 690, in _init_optimizer\r\n    return cmaes.CMA(\r\n  File \"/home/tomas/miniconda3/lib/python3.10/site-packages/optuna/_imports.py\", line 131, in __getattr__\r\n    return getattr(self._load(), item)\r\n  File \"/home/tomas/miniconda3/lib/python3.10/site-packages/optuna/_imports.py\", line 126, in _load\r\n    module = importlib.import_module(self._name)\r\n  File \"/home/tomas/miniconda3/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'cmaes'\r\n```\n```\n\n\n### Steps to reproduce\n\n\r\n```python\r\nimport optuna\r\n\r\n\r\ndef objective(trial):\r\n    x = trial.suggest_float(\"x\", -1, 1)\r\n    y = trial.suggest_int(\"y\", -1, 1)\r\n    return x**2 + y\r\n\r\n\r\nsampler = optuna.samplers.CmaEsSampler()\r\nstudy = optuna.create_study(sampler=sampler)\r\nstudy.optimize(objective, n_trials=20)\r\n```\r\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5804,
    "title": "Use `study.ask` in tests instead of `create_new_trial`",
    "author": "not522",
    "state": "closed",
    "created_at": "2024-11-29T04:55:35Z",
    "updated_at": "2024-12-14T05:11:58Z",
    "labels": [
      "code-fix",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Motivation\r\n\r\nThe tests sometimes use lengthy implementation to create new trials, `Trial(study, study._storage.create_new_trial(study._study_id))`, it can be replaced with `study.ask()`.\r\n\r\nhttps://github.com/optuna/optuna/blob/4e4a7761b6cbb5e4dd6897f0846a6c33e72cceab/tests/trial_tests/test_trial.py#L41\r\n\r\n### Suggestion\r\n\r\nThe codes are in the following file:\r\n\r\n- [x] [trial_tests/test_trial.py](https://github.com/optuna/optuna/blob/master/tests/trial_tests/test_trial.py) @unKnownNG \r\n- [x] [pruners_tests/test_percentile.py](https://github.com/optuna/optuna/blob/master/tests/pruners_tests/test_percentile.py) @fusawa-yugo \r\n- [ ] [pruners_tests/test_successive_halving.py](https://github.com/optuna/optuna/blob/master/tests/pruners_tests/test_successive_halving.py)\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "unKnownNG",
        "body": "Hello,\r\nI would like to take on this issue. I have referred to the issue, just simply changing `trial = Trial(study, study._storage.create_new_trial(study._study_id))` to `study.ask()` is enough?? There is `study = create_study(storage=storage, sampler=sampler)` prior to the line `trial = Trial(study, study._storage.create_new_trial(study._study_id))` , should I change that also?? There are 59 line, should all the line be changed??\r\n"
      },
      {
        "user": "not522",
        "body": "`study = create_study(storage=storage, sampler=sampler)` follows Optuna's usual manner. So, please leave it as it is."
      },
      {
        "user": "fusawa-yugo",
        "body": "I'd like to take pruners_tests/test_percentile.py."
      }
    ]
  },
  {
    "issue_number": 5862,
    "title": "Tracking Issue: free-threaded Python support",
    "author": "porink0424",
    "state": "open",
    "created_at": "2024-12-13T07:20:41Z",
    "updated_at": "2024-12-13T07:23:11Z",
    "labels": [
      "feature",
      "needs-discussion"
    ],
    "body": "## Motivation\r\n\r\nThis issue aims to track and summarize Optuna’s support for the experimental free-threaded mode (Python v3.13t) introduced in CPython 3.13.\r\n\r\n## Description\r\n\r\nThe current status of Optuna’s compatibility with v3.13t is as follows:\r\n\r\n### Sampler\r\n\r\n| Sampler           | Support for 3.13t | Remarks                                                                                                            |\r\n|-------------------|-------------------|--------------------------------------------------------------------------------------------------------------------|\r\n| RandomSampler     | Yes               | -                                                                                                                  |\r\n| NSGAIISampler     | Yes               | -                                                                                                                  |\r\n| TPESampler        | Yes               | Dependent on NumPy, but NumPy already supports 3.13t                                                               |\r\n| GridSampler       | Yes               | Same as above                                                                                                      |\r\n| NSGAIIISampler    | Yes               | Same as above                                                                                                      |\r\n| BruteForceSampler | No                | Despite the pure Python implementation, a `RuntimeError: dictionary changed size during iteration` was confirmed to occur when running in multi-threaded mode on Python 3.13t. The cause is currently under investigation ([Link](https://gist.github.com/porink0424/e43886aa85f7962cf4814703e1db99af) to reproduce the error without pytest-freethreaded). |\r\n| CmaEsSampler      | Yes               | Dependent on NumPy and cmaes, but both already support 3.13t                                                       |\r\n| GPSampler         | No                | Dependent on PyTorch, which is not yet supported on PyPI                                                           |\r\n| QMCSampler        | No                | Dependent on SciPy, which is not yet supported on PyPI                                                             |\r\n\r\n### Storage\r\n\r\n| Storage              | Support for 3.13t | Remarks                                                                                                                                                                                                                                                                                                                              |\r\n|-----------------------|-------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\n| InMemoryStorage      | Yes               | -                                                                                                                                                                                                                                                                                                                                    |\r\n| RDBStorage (sqlite)  | Partial           | `sqlite3.OperationalError` can occur, but this is not due to GIL. The issue arises because sqlite does not support high-level parallel processing. Errors can be resolved by reducing the number of threads (i.e., decreasing the values of `n_jobs` or `--threads`) or by extending the sqlite timeout. However, Optuna does not recommend using sqlite in cases where concurrent processing is performed with many threads ([reference](https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-solve-the-error-that-occurs-when-performing-parallel-optimization-with-sqlite3)). |\r\n| RDBStorage (MySQL)   | No                | Dependent on cryptography, which is not yet supported on PyPI                                                                                                                                                                                                                                                                       |\r\n| JournalStorage (File) | Yes              | -                                                                                                                                                                                                                                                                                                                                    |\r\n| JournalStorage (Redis) | No              | Despite the pure Python implementation, a `RuntimeError: dictionary changed size during iteration` was confirmed to occur when running in multi-threaded mode on Python 3.13t. The cause is currently under investigation ([Link](https://gist.github.com/porink0424/8b974b2d638cffc176ab38212bf0e391) to reproduce the error without pytest-freethreaded).                                                                                     |\r\n\r\n### Optuna-Integration\r\n\r\nThe current status has not yet been verified and requires further investigation.\r\n\r\n### Action Items\r\n\r\nThe possible action items are as follows (but may not be limited to) :\r\n- [ ] Verify the Optuna-Integration’s compatibility with v3.13t\r\n- [ ] Add unit tests for v3.13t\r\n",
    "comments": []
  },
  {
    "issue_number": 5248,
    "title": "UnicodeDecodeError in optuna 3.5.0 with db.sqlite3 storage",
    "author": "HDembinski",
    "state": "closed",
    "created_at": "2024-02-13T15:27:08Z",
    "updated_at": "2024-12-11T23:06:07Z",
    "labels": [
      "enhancement",
      "stale"
    ],
    "body": "### Expected behavior\r\n\r\nAs newbie on optuna, I wrongly tried to use this call in the CLI, where `db.sqlite3` is a file on my drive in the same folder.\r\n```\r\noptuna --storage db.sqlite3 study-names\r\n```\r\n\r\nThis call currently gives a complicated error message about a UnicodeDecodeError that does not indicate the true mistake; I should have used the full URL `sqlite:///db.sqlite3`. For me it is not obvious that I have to use this URL, and I expected this simpler call to work. I propose two solutions, with preference in that order:\r\n1) For user convenience, make the call above work: optuna detects that db.sqlite3 is the path to a filename and handles this call correctly.\r\n2) Optuna should detect that the storage is given without an URL prefix and complain with an error message that points the user towards this issue.",
    "comments": [
      {
        "user": "nzw0301",
        "body": "According to the first line of the log message, I suspect the CLI reads the SQLite file as journal storage. Could you try either way\r\n\r\n```zsh\r\noptuna --storage sqlite:///db.sqlite3 study-names\r\n```\r\n\r\nor \r\n\r\n```zsh\r\noptuna --storage db.sqlite3 --storage-class RDBStorage study-names\r\n```\r\nEDIT: sorry the second example does not work. Could you try only the first one?"
      },
      {
        "user": "HDembinski",
        "body": "Thank you for the fix, the first line works. Still, I think this a UX bug. Please provide an error message for this case which points the user in the right direction or make the CLI smarter to handle the call `optuna --storage db.sqlite3 study-names` correctly. You should not expect that your users are familiar with the details of sqlite."
      },
      {
        "user": "nzw0301",
        "body": "Thank you for your input. Could you update the title and description? Then I will re-open this issue and change the label to a more appropriate one."
      }
    ]
  },
  {
    "issue_number": 5816,
    "title": "CLI for empty DB raises `ValueError`",
    "author": "not522",
    "state": "closed",
    "created_at": "2024-12-02T09:50:53Z",
    "updated_at": "2024-12-09T07:29:34Z",
    "labels": [
      "bug",
      "contribution-welcome"
    ],
    "body": "### Expected behavior\n\nCLI for empty DB should output empty result, but the current implementation raises `ValueError`.\n\n### Environment\n\n- Optuna version:4.2.0.dev\r\n- Python version:3.13.0\r\n- OS:macOS-15.1-x86_64-i386-64bit-Mach-O\r\n- (Optional) Other libraries and their versions:\n\n### Error messages, stack traces, or logs\n\n```shell\nSee below.\n```\n\n\n### Steps to reproduce\n\nFor empty DB (`tmp.db` does not exist before the command), the `optuna studies` command raises `ValueError`.\r\n\r\n```bash\r\n$ optuna --storage sqlite:///tmp.db studies\r\nTraceback (most recent call last):\r\n  File \"/Users/naotomizuno/.pyenv/versions/optuna-3.13.0/bin/optuna\", line 8, in <module>\r\n    sys.exit(main())\r\n             ~~~~^^\r\n  File \"/Users/naotomizuno/optuna/optuna/cli.py\", line 991, in main\r\n    return args.handler(args)\r\n           ~~~~~~~~~~~~^^^^^^\r\n  File \"/Users/naotomizuno/optuna/optuna/cli.py\", line 463, in take_action\r\n    _format_output(\r\n    ~~~~~~~~~~~~~~^\r\n        records, self._study_list_header, parsed_args.format, parsed_args.flatten\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    )\r\n    ^\r\n  File \"/Users/naotomizuno/optuna/optuna/cli.py\", line 258, in _format_output\r\n    return _dump_table(values, header).strip()\r\n           ~~~~~~~~~~~^^^^^^^^^^^^^^^^\r\n  File \"/Users/naotomizuno/optuna/optuna/cli.py\", line 222, in _dump_table\r\n    max_width = max(len(header[column]), max(row[column].width() for row in rows))\r\n                                         ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: max() iterable argument is empty\r\n```\r\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5226,
    "title": "TypeError: '<' not supported between instances of 'NoneType' and 'int'",
    "author": "yancychy",
    "state": "closed",
    "created_at": "2024-02-05T18:13:31Z",
    "updated_at": "2024-12-05T23:06:19Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Don't use GitHub Issues to ask support questions.\n\nHi, when I try to run optuna on parallel mode, I have 4 CPUs on the node. I tried to set n_jobs=-1, but it failed with following errors. It only work when I set n_jobs=1. \r\nPython: 3.10\r\nOptuna: 3.5.0\r\n\r\n```python\r\n[W 2024-02-05 13:12:09,752] Trial 68 failed with parameters: {} because of the following error: TypeError(\"'<' not supported between instances of 'NoneType' and 'int'\").\r\nTraceback (most recent call last):\r\n  File \"/home/cheny48/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\r\n    value_or_values = func(trial)\r\n  File \"/gpfs/gsfs12/users/cheny48/TFproject/fly/foo_tmp.py\", line 11, in objective\r\n    x = trial.suggest_int(\"tx\", -10, 10)\r\n  File \"/home/cheny48/.local/lib/python3.10/site-packages/optuna/_convert_positional_args.py\", line 83, in converter_wrapper\r\n    return func(**kwargs)\r\n  File \"/home/cheny48/.local/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 326, in suggest_int\r\n    suggested_value = int(self._suggest(name, distribution))\r\n  File \"/home/cheny48/.local/lib/python3.10/site-packages/optuna/trial/_trial.py\", line 635, in _suggest\r\n    param_value = self.study.sampler.sample_independent(\r\n  File \"/home/cheny48/.local/lib/python3.10/site-packages/optuna/samplers/_tpe/sampler.py\", line 427, in sample_independent\r\n    trials = study._get_trials(deepcopy=False, states=states, use_cache=True)\r\n  File \"/home/cheny48/.local/lib/python3.10/site-packages/optuna/study/study.py\", line 274, in _get_trials\r\n    self._thread_local.cached_all_trials = self._storage.get_all_trials(\r\n  File \"/home/cheny48/.local/lib/python3.10/site-packages/optuna/storages/_cached_storage.py\", line 234, in get_all_trials\r\n    trials = list(sorted(trials.values(), key=lambda t: t.number))\r\nTypeError: '<' not supported between instances of 'NoneType' and 'int'\r\n```\r\n\r\nMy example code: \r\n\r\n```python\r\nimport optuna\r\nimport sys\r\nfrom optuna.trial import TrialState\r\n\r\njobID = sys.argv[1]\r\n\r\nurlName= 'mysql://root@localhost:55555/example0'\r\n\r\ndef objective(trial):\r\n    x = trial.suggest_int(\"tx\", -10, 10)\r\n    print(x)\r\n    return x**2\r\n\r\nif __name__ == \"__main__\":\r\n    study = optuna.create_study(direction=\"maximize\", study_name= 'studyID',\r\n                                storage=urlName, load_if_exists=True)#,\r\n\r\n    study.optimize(objective, n_trials=5, n_jobs=-1)\r\n\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "yancychy",
        "body": "I just followed the code in this [link](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html#easy-parallelization)\r\n"
      },
      {
        "user": "contramundum53",
        "body": "@yancychy Thank you for your bug report!\r\nHmm...I couldn't reproduce this error. (The above code worked normally, after setting url to my database.) "
      },
      {
        "user": "yancychy",
        "body": "Thanks. It's 3.5.0. It works when n_jobs=1 which needs very long time to finish. I hope to do paralle to reduce the running time. "
      }
    ]
  },
  {
    "issue_number": 5231,
    "title": "Sorting Error in find_max_value_trial()",
    "author": "unolife",
    "state": "closed",
    "created_at": "2024-02-07T06:27:00Z",
    "updated_at": "2024-12-03T23:06:06Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\n\nI just wanna use rdb, when I use optuna, but there is some problem\r\n\r\nIn optuna/storages/_rdb/models.py class TrialModel, function find_max_value_trial, \r\ncase in the query makes problem :\r\n`case({\"INF_NEG\": -1, \"FINITE\": 0, \"INF_POS\": 1},value=TrialValueModel.value_type,)`\r\nif I annotate this part, it works well.\r\n\r\n```\r\n@classmethod\r\n    def find_max_value_trial(\r\n        cls, study_id: int, objective: int, session: orm.Session\r\n    ) -> \"TrialModel\":\r\n        trial = (\r\n            session.query(cls)\r\n            .filter(cls.study_id == study_id)\r\n            .filter(cls.state == TrialState.COMPLETE)\r\n            .join(TrialValueModel)\r\n            .filter(TrialValueModel.objective == objective)\r\n            .order_by(\r\n                desc(\r\n                    case(\r\n                        {\"INF_NEG\": -1, \"FINITE\": 0, \"INF_POS\": 1},\r\n                        value=TrialValueModel.value_type,\r\n                    )\r\n                ),\r\n                desc(TrialValueModel.value),\r\n            )\r\n            .limit(1)\r\n            .one_or_none()\r\n        )\r\n        if trial is None:\r\n            raise ValueError(NOT_FOUND_MSG)\r\n        return trial\r\n```\r\n\n\n### Environment\n\n- Optuna version: 3.4.0\r\n- Python version: 3.10.10\r\n- OS: MacOS Ventura 13.5.2\r\n- (Optional) Other libraries and their versions:\r\nscikit-learn: 1.3.2\r\ncatboost:1.2.2\n\n### Error messages, stack traces, or logs\n\n```shell\nTraceback (most recent call last):\r\n  File \"/Users/warren/projects/automl/optuna_rdb_test.py\", line 20, in <module>\r\n    study.optimize(objective, n_trials=10)\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/optuna/study/study.py\", line 451, in optimize\r\n    _optimize(\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 76, in _optimize\r\n    _optimize_sequential(\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 173, in _optimize_sequential\r\n    frozen_trial = _run_trial(study, func, catch)\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 234, in _run_trial\r\n    study._log_completed_trial(frozen_trial)\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/optuna/study/study.py\", line 1105, in _log_completed_trial\r\n    best_trial = self.best_trial\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/optuna/study/study.py\", line 157, in best_trial\r\n    return copy.deepcopy(self._storage.get_best_trial(self._study_id))\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/optuna/storages/_cached_storage.py\", line 183, in get_best_trial\r\n    return self._backend.get_best_trial(study_id)\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py\", line 915, in get_best_trial\r\n    trial = models.TrialModel.find_max_value_trial(study_id, 0, session)\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/optuna/storages/_rdb/models.py\", line 214, in find_max_value_trial\r\n    case(\r\n  File \"<string>\", line 2, in case\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/sqlalchemy/sql/elements.py\", line 2755, in __init__\r\n    whenlist = [\r\n  File \"/Users/warren/miniforge3/envs/hfcss/lib/python3.10/site-packages/sqlalchemy/sql/elements.py\", line 2764, in <listcomp>\r\n    for (c, r) in whens\r\nValueError: too many values to unpack (expected 2)\n```\n\n\n### Steps to reproduce\n\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import train_test_split\r\nfrom catboost import CatBoostClassifier\r\nimport optuna\r\n\r\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n\r\ndef objective(trial):\r\n    params = {\r\n        'depth': trial.suggest_int('depth', 3, 10),\r\n        'verbose': False,\r\n    }\r\n    model = CatBoostClassifier(**params)\r\n    model.fit(X_train, y_train)\r\n    score = model.score(X_test, y_test)\r\n    return score\r\nstudy = optuna.create_study(direction='maximize', study_name='catboost', storage='sqlite:///test.db', load_if_exists=True)\r\nstudy.optimize(objective, n_trials=10)\r\nbest_params = study.best_params\r\nprint(\"Best params:\", best_params)\r\nbest_model = CatBoostClassifier(**best_params)\r\nbest_model.fit(X_train, y_train)\r\ntest_score = best_model.score(X_test, y_test)\r\nprint(\"Test set accuracy:\", test_score)\r\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Hmm, I could not reproduce the error; the script worked fine without any error. \r\n"
      },
      {
        "user": "nzw0301",
        "body": "Does this happen with\r\n\r\n```python\r\nstudy = optuna.create_study(direction='maximize', study_name='catboost', storage='sqlite:///test.db', load_if_exists=False)\r\n```\r\n?\r\n\r\nI'm not sure but I suspect the database contains old optuna's data or broken info."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 3669,
    "title": "Add recommended setting for batch optimization",
    "author": "knshnb",
    "state": "open",
    "created_at": "2022-06-14T10:16:19Z",
    "updated_at": "2024-12-02T14:22:59Z",
    "labels": [
      "document",
      "contribution-welcome"
    ],
    "body": "### What is an issue?\n\nThere are several recommended settings for efficient batch optimization, such as the followings.\r\n\r\n- set `popsize` to a multiple of batch size in `CmaEsSampler` (supported by #3649).\r\n- set `constant_liar=True` in `TPESampler`.\r\n\r\nWe can add these points in [the tutorial of batch optimization](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#batch-optimization).",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5605,
    "title": "Enbabling deleting trials ",
    "author": "MLfreakPy",
    "state": "open",
    "created_at": "2024-08-01T14:24:19Z",
    "updated_at": "2024-12-02T05:37:59Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\r\n\r\nHi together, \r\n\r\nso often some trials go bad in the process. They destroy the full optimization (e.g., giving 0.0 loss because of NaN values). Mistakenly these (bad) trials are considered the best based on loss metric. There can be many reasons for bad trials.\r\n\r\nIn any case, please enable the deletion of specific trials!! Otherwise a new study has to be started - loosing hours of precious trial runs. \r\n\r\n### Description\r\n\r\nThis is a highly requested features by many users.\r\n\r\nSee also: https://github.com/optuna/optuna/issues/685\r\n\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "contramundum53",
        "body": "Current workaround is to create a new study, copy all correct trials and optionally delete the old study. You can resume optimization from there without any impact on performance.\r\nSee also: https://stackoverflow.com/questions/74907041/how-can-i-change-an-optunas-trial-result/74919486#74919486.\r\n\r\n```\r\nold_trials = old_study.get_trials(deepcopy=False)\r\ncorrect_trials = [optuna.trial.create_trial(\r\n        params=t.params,\r\n        distributions=t.distributions,\r\n        value=t.value,\r\n    ) for t in old_trials if keep_this_trial(t)]\r\nnew_study = optuna.create_study(...)\r\nnew_study.add_trials(correct_trials)\r\n```\r\n"
      },
      {
        "user": "contramundum53",
        "body": "The current design to not allow changing a finished trial is intentional. Finished trials will be cached everywhere during distributed optimization, due to performance reasons (otherwise every new trial would need to fetch the whole optimization history from storage to check whether any of the old trials got deleted). The only way to safely clear those cache is to force all workers to reload the whole study, and copying a study exactly reflects this operation.\r\n\r\nWe are open to discussions as to making this interface more user-friendly."
      }
    ]
  },
  {
    "issue_number": 5000,
    "title": "Support Python 3.12",
    "author": "not522",
    "state": "closed",
    "created_at": "2023-10-04T03:03:52Z",
    "updated_at": "2024-11-29T04:45:49Z",
    "labels": [
      "feature",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Motivation\r\n\r\nPython 3.12 released on 2023-10-02. https://www.python.org/downloads/release/python-3120/\r\nThis is a tracking issue to support the new Python.\r\n\r\n### Description\r\n\r\n- [x] Add Python 3.12 to the CI of the unit tests.\r\n- [x] Add Python 3.12 to the CI of the unit tests for integrations.\r\n- [x] Add Python 3.12 to the CI of the Docker build.\r\n- [x] Add Python 3.12 to the CI of [optuna-examples](https://github.com/optuna/optuna-examples/).\r\n- [x] Use Python 3.12 in the speed benchmark.\r\n- [x] Add branch protection rules.\r\n- [x] Add badge.\r\n- [x] Update classifiers in `pyproject.toml`.\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\nSee also the 3.11 tracking issue. #3964",
    "comments": [
      {
        "user": "not522",
        "body": "Current status:\r\nOptuna already supports Python 3.12, but some CIs need to be updated to use it.\r\nMany integration modules on `optuna-integration` also work with Python 3.12, but some are not yet supported."
      },
      {
        "user": "HideakiImamura",
        "body": "This issue is still open. We would like to add a CI to build a Docker image with Python 3.12. This is contribution welcome."
      }
    ]
  },
  {
    "issue_number": 5776,
    "title": "Update kaleido ",
    "author": "nzw0301",
    "state": "open",
    "created_at": "2024-11-19T02:01:48Z",
    "updated_at": "2024-11-25T17:30:33Z",
    "labels": [
      "code-fix",
      "contribution-welcome"
    ],
    "body": "### Motivation\n\nSee https://github.com/optuna/optuna/pull/5771\n\n### Suggestion\n\nUpdate kaleido version and install chrome or chromium independently\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "willdavidson05",
        "body": "Hi @nzw0301 as shown in #5770 where installing Chromium wasn't enough to fix the CI error. Would adding something like an init() function to set the browser path or improving the CI setup be the right direction? Happy to help, but wanted to check if this aligns with what’s needed or if there’s ongoing work I should consider. Thanks!"
      },
      {
        "user": "nzw0301",
        "body": "Hello @willdavidson05 thank you for your comment. I'm unfamiliar with this, but adding a few lines of command sounds great. I think no one is working on this, so your help is more than appreciated. "
      },
      {
        "user": "willdavidson05",
        "body": "@nzw0301 thank you for this! I looked into this more and tried adding the Chromium installation to the test workflow similar to your approach in #5771(detailed in #5788). However, after digging I realized that the dependency issue comes from Kaleido versions >=0.4 being yanked from PyPi(metioned https://github.com/plotly/Kaleido/issues/230). Due to this I decided to close my PR, but curious what you think about this implementation."
      }
    ]
  },
  {
    "issue_number": 5209,
    "title": "Non-deterministic test: float comparisons in some tests make them flaky",
    "author": "pkoz",
    "state": "closed",
    "created_at": "2024-01-28T20:27:25Z",
    "updated_at": "2024-11-24T23:05:55Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\n\nI found the following tests randomly failing in the GitHub Actions: \r\n- `TestLightGBMTuner.test_tune_best_score_reproducibility`\r\n- `TestLightGBMTunerCV.test_tune_best_score_reproducibility`\r\n- `test_optimize_parallel_timeout`\r\n\r\n### Expected behavior\r\n\r\nTests should be deterministic.\r\n\r\n### Suggestion:\r\n\r\nWe can fix assertions like:\r\n\r\nhttps://github.com/optuna/optuna/blob/073abfc520725688434b512212e910a395dd4f99/tests/integration_tests/lightgbm_tuner_tests/test_optimize.py#L766\r\n\r\nby using `pytest.approx` that accepts numbers with a tolerance (default relative tolerance: `1e-6`)\r\n\r\n```python\r\n        assert best_score_second_try == pytest.approx(best_score_first_try)\r\n```\n\n### Environment\n\n- Optuna version: `3.6.0.dev`\r\n- Python version: `3.10.11`\r\n- OS: `macOS-14.2.1-arm64-arm-64bit`\r\n- (Optional) Other libraries and their versions: n/a\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\n>           assert first_trial.value == second_trial.value\r\nE           AssertionError: assert 0.21086425862654534 == 0.21086425862654531\r\nE            +  where 0.21086425862654534 = FrozenTrial(number=27, state=1, values=[0.21086425862654534], datetime_start=datetime.datetime(2024, 1, 27, 23, 47, 9,...alse, low=0.4, step=None), 'bagging_freq': IntDistribution(high=7, log=False, low=1, step=1)}, trial_id=27, value=None).value\r\nE            +  and   0.21086425862654531 = FrozenTrial(number=27, state=1, values=[0.21086425862654531], datetime_start=datetime.datetime(2024, 1, 27, 23, 47, 10...alse, low=0.4, step=None), 'bagging_freq': IntDistribution(high=7, log=False, low=1, step=1)}, trial_id=27, value=None).value\n```\n\n\n### Steps to reproduce\n\nBy the nature of the problem, there is no deterministic way to observe the problem.\r\n\r\nPlease take a look at [this job log](https://github.com/optuna/optuna/actions/runs/7681878115/job/20935516656#step:10:101) to see the example of the failed run.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5774,
    "title": "Axis tick scale in `plot_contour` with Matplotlib backend",
    "author": "sulan",
    "state": "closed",
    "created_at": "2024-11-18T11:02:02Z",
    "updated_at": "2024-11-22T05:55:27Z",
    "labels": [
      "bug",
      "contribution-welcome",
      "good first issue"
    ],
    "body": "### Expected behavior\n\nWhen I plot with `optuna.visualization.matplotlib.plot_contour`, the horizontal and vertical ticks are set for each subfigure.\r\n\r\nThe subfigures on the diagonal are empty (as it should be), but the ticks are always scaled linearly. This is inconsistent with the off-diagonal subfigures in the corresponding columns/rows, where the axis ticks are logarithmic for logarithmic parameters.\r\n\r\nThis is only a problem (annoyance) in the first row and the last column, since the tick labels (which are rendered on the left and the bottom of the figure) do not correspond to the actual subfigures they should be describing.\n\n### Environment\n\n- Optuna version: 4.0.0\r\n- Python version: 3.12.7\r\n- OS: Linux-6.11.7-arch1-1-x86_64-with-glibc2.40\r\n- (Optional) Other libraries and their versions:\r\n    - Matplotlib: 3.9.2\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\n-\n```\n\n\n### Steps to reproduce\n\n1. Create a study with at least 3 parameters that has logarithmic scale\r\n2. Load and plot:\r\n\r\n```python\r\nimport optuna\r\nstudy = optuna.load_study(...)\r\nimport optuna.visualization.matplotlib as vis\r\nvis.plot_contour(study)\r\nimport matplotlib.pyplot as plt\r\nplt.show()\r\n```\r\n\n\n### Additional context (optional)\n\nThis bug does not appear with the Plotly backend.",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Hi, could you share minimal reproducible codes with us?"
      },
      {
        "user": "sulan",
        "body": "Yes, sorry:\r\n\r\n```python\r\nimport optuna\r\nimport matplotlib.pyplot as plt\r\nimport optuna.visualization.matplotlib as vis\r\n    \r\ndef f(trial):\r\n    trial.suggest_float('a', 1, 10, log=True)\r\n    trial.suggest_float('b', 100, 1000, log=True)\r\n    trial.suggest_float('c', 1000, 10_000, log=True)\r\n    return 1.0\r\n    \r\nstudy = optuna.create_study()\r\nstudy.optimize(f, n_trials=100)\r\nvis.plot_contour(study)\r\nplt.show()\r\n```"
      },
      {
        "user": "not522",
        "body": "Thank you for your bug report.\r\n\r\nThe following early return seems to be the cause of this bug. We need to fix the logic of `_generate_contour_subplot`.\r\n\r\nhttps://github.com/optuna/optuna/blob/101b1e81841d26ee3bae17268cb9e89679629a77/optuna/visualization/matplotlib/_contour.py#L275-L277"
      }
    ]
  },
  {
    "issue_number": 5182,
    "title": "ArangoDB as an optional DB.",
    "author": "PhilipMay",
    "state": "closed",
    "created_at": "2024-01-12T20:28:13Z",
    "updated_at": "2024-11-20T23:05:51Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nAdditionaly to those DB options that come with SQLAlchemy I would like to use ArangoDB as an option. This is because ArangoDB is accessable via HTTP and easy to secure with an reverse proxy and HTTPS. It causes no problem with company proxies that might only allow HTTP and HTTPS traffic.\r\n\r\n### Description\r\n\r\nsee above\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "not522",
        "body": "Thank you for your suggestion!\r\nCurrently, we have no plan for additional DB support, but in my opinion, the implementation as 3rd party library is welcome.\r\nI think that it can be implemented easily using the journal storage. It just needs to implement several methods. (Please see [`JournalRedisStorage`](https://github.com/optuna/optuna/blob/master/optuna/storages/_journal/redis.py).)"
      },
      {
        "user": "PhilipMay",
        "body": "Thanks @not522 \r\nThat looks easy.\r\n\r\nTwo questions:\r\n\r\nWhere would you suggest me to put this code? My own project or are there Optuna-contrib projects you would suggest?\r\n\r\nWould you provide feedback and some guidance for me - if needed?\r\n\r\nThanks\r\nPhilip"
      },
      {
        "user": "not522",
        "body": "1. We welcome a PR submission to [optuna-integration](https://github.com/optuna/optuna-integration).\r\n2. Yes!\r\n\r\nI'm wondering if we need accounts or licenses to use ArangoDB. If we need them for testing or CI, it might be a little difficult to add the ArangoDB integration into optuna-integration."
      }
    ]
  },
  {
    "issue_number": 5762,
    "title": "Reducing storage file size for CMA-ES with margin",
    "author": "hrntsm",
    "state": "open",
    "created_at": "2024-11-13T12:04:14Z",
    "updated_at": "2024-11-16T12:55:52Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nI often use CMA-ES with steps of about 0.01, so I often use the with margin option set to True.\r\nWhen the with margin option is set to True, the storage journal file size may be much larger than normal, is it possible to make it smaller?\n\n### Description\n\nAs an example, the following code is shown below, which, when executed \"with margin\" set to True, results in a log file size of about 40 MB. On the other hand, if “with margin” is set to false, the log file size is about 50KB.\r\nIn this example, there are 12 trials, but in reality, I want to run about several hundred trials, so the file size will be several hundred MB, which will complicate the visualization of the results and handling in the dashboard.\r\nIs there any possibility to reduce the file size, or is there any knowledge that if step is about 0.01, setting with margin to false will not affect the results?\r\n\r\n```py\r\nimport optuna\r\nimport optuna.storages.journal\r\n\r\n\r\ndef objective(trial):\r\n    variables: list[float] = []\r\n    for i in range(10):\r\n        variables.append(trial.suggest_float(f\"x{i}\", -30, 30, step=0.001))\r\n    return sum(variables)\r\n\r\n\r\nsampler = optuna.samplers.CmaEsSampler(with_margin=True, seed=42)\r\nstorage = optuna.storages.JournalStorage(\r\n    optuna.storages.journal.JournalFileBackend(\"./journal.log\")\r\n)\r\nstudy = optuna.create_study(sampler=sampler, storage=storage)\r\nstudy.optimize(objective, n_trials=12)\r\n```\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "If the optimiser state in the journal file is not used for visualisation or dashboard, manually removing such lines can reduce the file size as a workaround because when I look at `journal.log`, many lines look like\r\n\r\n```\r\n{\"op_code\":9,\"worker_id\":\"65a3ccbc-a211-4773-a271-1f7ae52b8912-8244481856\",\"trial_id\":65,\"system_attr\":{\"cmawm:optimizer:18726\":\"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\"}}\r\n\r\n```"
      },
      {
        "user": "nzw0301",
        "body": "I skimmed the definition of CMA-ES w/ margin, and actually, this issue in the shared script comes from the too-fine discrete search space, which might not be suitable for `CmaEsSampler` as summarised in https://optuna.readthedocs.io/en/stable/reference/samplers/index.html 's `Categorical parameters` row. Thus I suggest using continuous search space: removing `step` instead of `step=0.001`. What do you think? "
      },
      {
        "user": "nzw0301",
        "body": "To optuna core dev team, let me share my thoughts on this.\r\n\r\nWhen I investigated this storage heaviness of CMA-ESwM, I checked the optimizer's attributes' lengths: \r\n\r\n```python\r\n# load `optimizer` saved at  https://github.com/optuna/optuna/blob/d3ee4740b18bdec95313e6cd077cc313e59e5bdc/optuna/samplers/_cmaes.py#L484C42-L484C51\r\nfor k, v in optimizer.__dict__.items():\r\n    print(k, len(pickle.dumps(v).hex()))\r\n```\r\n\r\nThe output is as follows:\r\n\r\n```\r\n_cma 7170\r\n_n_max_resampling 10\r\n_discrete_idx 456\r\n_continuous_idx 296\r\n_continuous_space 300\r\n_n_zdim 10\r\nmargin 42\r\nz_space 9600486\r\nz_lim 9600326\r\nz_lim_low 9600486\r\nz_lim_up 9600486\r\nm_z_lim_low 456\r\nm_z_lim_up 456\r\n_A 456\r\n```\r\n\r\nAs I implied in [the previous comment](https://github.com/optuna/optuna/issues/5762#issuecomment-2480456145), `z_`-ish attributes are for discrete search space https://github.com/CyberAgentAILab/cmaes/blob/main/cmaes/_cmawm.py#L140-L166. If the team works on this, I suppose that Optuna already stores distributions, so we may not need to store these storage-heavy attributes. Instead, we restore them from distributions. \r\n\r\nBut I suspect that this CMA ESwM-specific treatment introduces additional complexity to the code. In addition, this sampler isn't good at optimising discrete search space, so simply adding some docstring to warn this might be enough to avoid this unexpected storage issue."
      }
    ]
  },
  {
    "issue_number": 5439,
    "title": "Inconsistent Dynamic Parameter Handling in Optuna Ask-Tell Interface with Conditional Trials",
    "author": "Djkmarco",
    "state": "open",
    "created_at": "2024-05-09T13:10:00Z",
    "updated_at": "2024-11-15T14:59:43Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nI'm encountering unexpected behavior when using the Optuna ask-tell interface with dynamic search spaces and conditional trials. The issue manifests as the dynamic parameter being treated as a separate parameter at each iteration, even though it's intended to be the same parameter across iterations.\r\nWithin the ask-tell interface, I anticipate that the ask-tell interface would recognize the dynamic parameter as a single entity throughout the optimization process. This implies that the value provided by the tell method should be used for subsequent ask calls with the same parameter name, regardless of the trial iteration.\r\nThis behavior isn't confined to the Optuna dashboard; it also affects core Optuna functionality. For instance, if I reload a study and attempt a new optimization using study.optimize, the conditional trial's dynamic parameter isn't carried over to subsequent trials.\r\n\r\n![image](https://github.com/optuna/optuna/assets/111493577/7a8ffec2-1478-479e-9487-7a1e5019d08f)\r\n\r\n\r\n### Environment\r\n\r\n- Optuna version: 3.6.1\r\n- Python version: 3.10.11\r\n- OS :Windows-10-10.0.22631-SP0\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\nNo Error Messages\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nExample 1\r\n\r\n```python\r\nimport optuna\r\nfrom numpy import random\r\n\r\nstudy = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=42),\r\n                            direction='minimize',\r\n                            study_name='nsga3',\r\n                            storage='sqlite:///example.db',\r\n                            load_if_exists=True)\r\n\r\nfor i in range(10):\r\n    trial = study.ask()\r\n    x = trial.suggest_float('x', -10, 10)\r\n    y = trial.suggest_int('y', 0, i)\r\n    result = (x - 2) ** 2 + y\r\n    study.tell(trial, result)\r\n```\r\n\r\nExample 2\r\n\r\n```python\r\nimport optuna\r\nfrom numpy import random\r\n\r\nstudy = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=42),\r\n                            direction='minimize',\r\n                            study_name='nsga3',\r\n                            storage='sqlite:///example.db',\r\n                            load_if_exists=True)\r\n\r\nfor i in range(10):\r\n    x = optuna.distributions.IntDistribution(-10, 10)\r\n    y = optuna.distributions.IntDistribution(0, i)\r\n    distribution = {'x': x, 'y': y}\r\n    trial = study.ask(distribution)\r\n    result = (trial.params['x'] - 2) ** 2 + trial.params['y']\r\n    study.tell(trial, result)\r\n```\r\n\r\nExample 3\r\n\r\n```python\r\nimport optuna\r\nfrom numpy import random\r\n\r\n\r\nstorage = optuna.storages.InMemoryStorage()\r\n\r\nstudy = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=42),\r\n                            direction='minimize',\r\n                            study_name='nsga3',\r\n                            storage=storage)\r\n\r\nfor i in range(3):\r\n    x = optuna.distributions.IntDistribution(-10, 10)\r\n    y = optuna.distributions.IntDistribution(0, i)\r\n    distribution = {'x': x, 'y': y}\r\n    trial = study.ask(distribution)\r\n    result = (trial.params['x'] - 2) ** 2 + trial.params['y']\r\n    study.tell(trial, result)\r\n\r\nstudy2 = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=42),\r\n                             direction='minimize',\r\n                             study_name='nsga3',\r\n                             storage=storage,\r\n                             load_if_exists=True)\r\n\r\nfor j in range(10):\r\n    x = optuna.distributions.IntDistribution(-10, 10)\r\n    y = optuna.distributions.IntDistribution(0, 10)\r\n    distribution = {'x': x, 'y': y}\r\n    trial = study2.ask(distribution)\r\n    result = (trial.params['x'] - 2) ** 2 + trial.params['y']\r\n    study2.tell(trial, result)\r\n\r\nstudy3 = optuna.copy_study(from_study_name='nsga3', from_storage=storage, to_storage=\"sqlite:///example.db\")\r\n```\r\n\r\nEvidence this affects following steps\r\n\r\n```python\r\nimport optuna\r\n\r\nstorage = optuna.storages.InMemoryStorage()\r\n\r\npartial_sampler = optuna.samplers.PartialFixedSampler({'y':0}, optuna.samplers.NSGAIIISampler(seed=42,\r\n                                                                                              population_size=5,\r\n                                                                                              mutation_prob=0,\r\n                                                                                              crossover_prob=0.9,\r\n                                                                                              crossover=optuna.samplers.nsgaii.UniformCrossover()))\r\n\r\nstudy = optuna.create_study(sampler=partial_sampler,\r\n                            direction='minimize',\r\n                            study_name='nsga3',\r\n                            storage=storage)\r\n\r\nfor i in range(10):\r\n    x = optuna.distributions.IntDistribution(-10, 10)\r\n    y = optuna.distributions.IntDistribution(0, i)\r\n    distribution = {'x': x, 'y': y}\r\n    trial = study.ask(distribution)\r\n    result = (trial.params['x'] - 2) ** 2 + trial.params['y']\r\n    study.tell(trial, result)\r\n\r\nstudy2 = optuna.create_study(sampler=optuna.samplers.NSGAIIISampler(seed=42,\r\n                                                                    population_size=5,\r\n                                                                    mutation_prob=0,\r\n                                                                    crossover_prob=0.9,\r\n                                                                    crossover=optuna.samplers.nsgaii.UniformCrossover(),\r\n                                                                    ),\r\n                             direction='minimize',\r\n                             study_name='nsga3',\r\n                             storage=storage,\r\n                             load_if_exists=True)\r\n\r\nfor j in range(10):\r\n    x = optuna.distributions.IntDistribution(-10, 10)\r\n    y = optuna.distributions.IntDistribution(0, 10)\r\n    distribution = {'x': x, 'y': y}\r\n    trial = study2.ask(distribution)\r\n    result = (trial.params['x'] - 2) ** 2 + trial.params['y']\r\n    study2.tell(trial, result)\r\n\r\nstudy3 = optuna.copy_study(from_study_name='nsga3', from_storage=storage, to_storage=\"sqlite:///example.db\")\r\n```\r\n\r\nGives results:\r\n![image](https://github.com/optuna/optuna/assets/111493577/dc96acbc-b4ab-4d4b-b0ad-b4a2f59f0a5c)\r\n\r\nExpected y for trials from 10 to 19 to be 0 (as mutation probability is 0)\r\n\r\nSetting in the first loop:\r\n\r\n```python\r\ny = optuna.distributions.IntDistribution(0, 10)\r\n```\r\n\r\ncorrectly returns:\r\n![image](https://github.com/optuna/optuna/assets/111493577/f8f056c8-ba9f-4673-9c60-6a078935a41e)\r\n\r\n\r\n### Additional context (optional)\r\n\r\nIt is important ask-tell interface can deal with conditional search space, as it is one of the most usefull application of such interface itself.\r\nThank you for your assistance!",
    "comments": [
      {
        "user": "rodrigoscoelho",
        "body": "Any progress on this? "
      }
    ]
  },
  {
    "issue_number": 5732,
    "title": "Use Python 3.12 to build the documentation in CI",
    "author": "c-bata",
    "state": "closed",
    "created_at": "2024-10-29T08:37:10Z",
    "updated_at": "2024-11-14T04:42:49Z",
    "labels": [
      "document",
      "contribution-welcome"
    ],
    "body": "## Motivation\r\n\r\nThe Optuna documentation is currently built on GitHub Actions using Python 3.8, which has officially reached end-of-life (EOL). The build environment should be updated to a non-EOL Python version, specifically Python 3.9 or later.\r\n\r\n## Description\r\n\r\nAn attempt to upgrade the Python version from 3.8 to 3.12 was made in [PR #5728](https://github.com/optuna/optuna/pull/5728), but it resulted in CI failures ([error logs](https://github.com/optuna/optuna/actions/runs/11549638785/job/32143038062)). This issue aims to update the Python version used in the GitHub Actions workflow to a non-EOL version (preferably Python 3.9 or newer).",
    "comments": []
  },
  {
    "issue_number": 5183,
    "title": "user attribute keep best",
    "author": "deadsoul44",
    "state": "closed",
    "created_at": "2024-01-13T09:50:50Z",
    "updated_at": "2024-11-11T23:05:36Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nUsually we want to keep the best model from optimization. It is not straightforward to do this with the current api.\n\n### Description\n\nAdd an option to keep a user attribute for only the best iteration. \r\n```\r\ndef objective(trial):\r\n    gbm = ...\r\n    trial.set_user_attr(key=\"best_booster\", value=gbm, keep_best=True)\r\n```\r\n\r\nBest booster will only be available for the best trial.\r\nThis will work:\r\n```\r\nbest_model=study.best_trial.user_attrs[\"best_booster\"]\r\n```\r\nThis won't:\r\n```\r\nbest_model=study.trials[0].user_attrs[\"best_booster\"]\r\n```\r\nThis way we won't save all the unnecassry models and keep only the best.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "How about using `study`'s user attribute and callback as answered by https://stackoverflow.com/a/63365355/2406562? "
      },
      {
        "user": "deadsoul44",
        "body": "I also commented there. I couldn't understand the point of that answer. The purpose is not to save an object in memory for every trial. "
      },
      {
        "user": "nzw0301",
        "body": "See also the original answer https://stackoverflow.com/a/62164601/2406562. How about this?"
      }
    ]
  },
  {
    "issue_number": 5750,
    "title": "Pruning checks performed at incorrect steps by SuccessiveHalvingPruner",
    "author": "mrvnthss",
    "state": "open",
    "created_at": "2024-11-07T14:56:08Z",
    "updated_at": "2024-11-07T14:56:08Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nWhen instantiating a ``SuccessiveHalvingPruner`` instance with ``min_resource = 10`` and all other parameters left to their default values, I would expect the pruning checks to be performed after 10, 40, 160, ... steps. Instead, the pruning checks happen after 11, 41, 161, ... steps have been completed.\r\n\r\nThe same applies to other combinations of ``min_resource`` and ``reduction_factor``, of course. For example, setting ``min_resource = 20`` and ``reduction_factor = 2``, the pruning checks are performed after 21, 41, 81, ... steps have been completed.\n\n### Environment\n\n- Optuna version: 4.0.0\r\n- Python version: 3.11.9\r\n- OS: macOS-15.0.1-arm64-arm-64bit\n\n### Error messages, stack traces, or logs\n\n```shell\n[I 2024-11-07 15:37:31,637] A new study created in memory with name: no-name-59a336b0-eb30-4dbc-bfd0-b1d203d25b8c\r\n[I 2024-11-07 15:37:31,701] Trial 0 finished with value: 0.6842105263157895 and parameters: {'alpha': 0.5583527238178998}. Best is trial 0 with value: 0.6842105263157895.\r\n[I 2024-11-07 15:37:31,760] Trial 1 finished with value: 0.6842105263157895 and parameters: {'alpha': 0.8933291665361408}. Best is trial 0 with value: 0.6842105263157895.\r\n[I 2024-11-07 15:37:31,818] Trial 2 finished with value: 0.9210526315789473 and parameters: {'alpha': 0.005589525165252018}. Best is trial 2 with value: 0.9210526315789473.\r\n[I 2024-11-07 15:37:31,846] Trial 3 pruned. \r\n[I 2024-11-07 15:37:31,853] Trial 4 pruned. \r\n[I 2024-11-07 15:37:31,862] Trial 5 pruned. \r\n[I 2024-11-07 15:37:31,869] Trial 6 pruned. \r\n[I 2024-11-07 15:37:31,877] Trial 7 pruned. \r\n[I 2024-11-07 15:37:31,885] Trial 8 pruned. \r\n[I 2024-11-07 15:37:31,893] Trial 9 pruned. \r\n[I 2024-11-07 15:37:31,954] Trial 10 finished with value: 0.7894736842105263 and parameters: {'alpha': 0.08461143581683261}. Best is trial 2 with value: 0.9210526315789473.\r\n[I 2024-11-07 15:37:31,963] Trial 11 pruned. \r\n[I 2024-11-07 15:37:31,972] Trial 12 pruned. \r\n[I 2024-11-07 15:37:31,981] Trial 13 pruned. \r\n[I 2024-11-07 15:37:32,009] Trial 14 pruned. \r\n[I 2024-11-07 15:37:32,018] Trial 15 pruned. \r\n[I 2024-11-07 15:37:32,027] Trial 16 pruned. \r\n[I 2024-11-07 15:37:32,056] Trial 17 pruned. \r\n[I 2024-11-07 15:37:32,065] Trial 18 pruned. \r\n[I 2024-11-07 15:37:32,090] Trial 19 pruned. \r\nTrial 3 pruned after 41 steps.\r\nTrial 4 pruned after 11 steps.\r\nTrial 5 pruned after 11 steps.\r\nTrial 6 pruned after 11 steps.\r\nTrial 7 pruned after 11 steps.\r\nTrial 8 pruned after 11 steps.\r\nTrial 9 pruned after 11 steps.\r\nTrial 11 pruned after 11 steps.\r\nTrial 12 pruned after 11 steps.\r\nTrial 13 pruned after 11 steps.\r\nTrial 14 pruned after 41 steps.\r\nTrial 15 pruned after 11 steps.\r\nTrial 16 pruned after 11 steps.\r\nTrial 17 pruned after 41 steps.\r\nTrial 18 pruned after 11 steps.\r\nTrial 19 pruned after 41 steps.\n```\n\n\n### Steps to reproduce\n\n1. Copy example from the [SuccessiveHalvingPruner](https://optuna.readthedocs.io/en/v4.0.0/reference/generated/optuna.pruners.SuccessiveHalvingPruner.html)'s documentation and additionally import ``TrialState`` from ``optuna.trial``\r\n2. Set ``min_resource`` to 10 when instantiating the ``SuccessiveHalvingPruner`` instance\r\n3. Print out total number of completed steps of pruned trials\r\n```python\r\nimport numpy as np\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.linear_model import SGDClassifier\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nimport optuna\r\nfrom optuna.trial import TrialState\r\n\r\nX, y = load_iris(return_X_y=True)\r\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)\r\nclasses = np.unique(y)\r\n\r\n\r\ndef objective(trial):\r\n    alpha = trial.suggest_float(\"alpha\", 0.0, 1.0)\r\n    clf = SGDClassifier(alpha=alpha)\r\n    n_train_iter = 100\r\n\r\n    for step in range(n_train_iter):\r\n        clf.partial_fit(X_train, y_train, classes=classes)\r\n\r\n        intermediate_value = clf.score(X_valid, y_valid)\r\n        trial.report(intermediate_value, step)\r\n\r\n        if trial.should_prune():\r\n            raise optuna.TrialPruned()\r\n\r\n    return clf.score(X_valid, y_valid)\r\n\r\n\r\nstudy = optuna.create_study(\r\n    direction=\"maximize\", pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=10)\r\n)\r\nstudy.optimize(objective, n_trials=20)\r\n\r\ntrials = study.get_trials()\r\npruned_trials = [trial for trial in trials if trial.state == TrialState.PRUNED]\r\ntotal_steps = [(trial.number, len(trial.intermediate_values)) for trial in pruned_trials]\r\nfor (trial_nr, steps) in total_steps:\r\n    print(f\"Trial {trial_nr} pruned after {steps} steps.\")\r\n```\r\n\n\n### Additional context (optional)\n\nIt's clear where this behavior comes from: The ``prune`` method of the ``SuccessiveHalvingPruner`` class implements the following check:\r\n```python\r\nif step < rung_promotion_step:\r\n    return False\r\n```\r\nHere, ``rung_promotion_step`` is computed correctly, and is an integer (1-based, if you wish). At the same time, ``step`` is a 0-based integer (this is explicitly mentioned [here](https://optuna.readthedocs.io/en/v4.0.0/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.report)–\"Note that pruners assume that step **starts at zero**.\"–and implicitly suggested by all examples in the documentation.). Hence, the first time the above check is violated (and the logic of the SHA is executed) happens when ``step == rung_promotion_step``. At this point, a total of ``step + 1`` steps (e.g., epochs) have already been completed since step is 0-based, leading to the behavior detailed in this issue.\r\n\r\nThe straightforward way to fix this would be to simply change the check mentioned above to\r\n```python\r\nif step + 1 < rung_promotion_step:\r\n    return False\r\n```\r\nThis is first violated when ``step + 1 == rung_promotion_step``, or equivalently ``step == rung_promotion_step - 1``, correctly accounting for the fact that step is 0-based.",
    "comments": []
  },
  {
    "issue_number": 5743,
    "title": "Update `actions/upload-artifact` to v4 from v3",
    "author": "nzw0301",
    "state": "closed",
    "created_at": "2024-11-02T16:02:14Z",
    "updated_at": "2024-11-07T03:19:57Z",
    "labels": [
      "code-fix",
      "contribution-welcome"
    ],
    "body": "### Motivation\r\n\r\nAs announced by https://github.blog/changelog/2024-04-16-deprecation-notice-v3-of-the-artifact-actions/, we have not been able to use `actions/upload-artifact@v3` since 30 November 2024.\r\n\r\n### Suggestion\r\n\r\nUpdate the version in https://github.com/optuna/optuna/blob/master/.github/workflows/sphinx-build.yml to v4 and update the workflow if necessary.\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5463,
    "title": "Cannot build the document on Mac",
    "author": "HideakiImamura",
    "state": "open",
    "created_at": "2024-05-28T06:54:17Z",
    "updated_at": "2024-10-31T11:32:10Z",
    "labels": [
      "bug",
      "document"
    ],
    "body": "### What is an issue?\n\n## Problem\r\nI cannot build the document on my MacBook, and my colleagues also cannot. It might be the problem due to the dependencies, but I have no exact idea how to fix this. Concretely, the build stops at the execution of `optuna.visualization.matplotlib.plot_terminator_improvement`.\r\n\r\n## What should be done\r\n The following command should be finished without any errors.\r\n```bash\r\ncd docs\r\nmake html\r\n```\r\n\r\n## Environment\r\nChip: Apple M2\r\nOS: Sonoma 14.4.1\r\nPython: 3.9.6\r\nOptuna: current master https://github.com/optuna/optuna/commit/98d44447cb557a7654ea92f5ea94059c2d2fc2b1\r\nSphinx : 7.3.7\r\n\r\n## Log\r\n\r\n```\r\n(venv) mamu@hideakis-mbp docs % make html\r\nRunning Sphinx v7.3.7\r\nUsing Sphinx-Gallery to convert rst text blocks to markdown for .ipynb files.\r\n[autosummary] generating autosummary for: faq.rst, index.rst, installation.rst, license_thirdparty.rst, privacy.rst, reference/artifacts.rst, reference/cli.rst, reference/distributions.rst, reference/exceptions.rst, reference/generated/optuna.TrialPruned.rst, ..., tutorial/20_recipes/007_optuna_callback.rst, tutorial/20_recipes/008_specify_params.rst, tutorial/20_recipes/009_ask_and_tell.rst, tutorial/20_recipes/010_reuse_best_trial.rst, tutorial/20_recipes/011_journal_storage.rst, tutorial/20_recipes/012_artifact_tutorial.rst, tutorial/20_recipes/013_wilcoxon_pruner.rst, tutorial/20_recipes/index.rst, tutorial/20_recipes/sg_execution_times.rst, tutorial/index.rst\r\n... (omitted)\r\n[I 2024-05-28 15:49:13,556] Trial 29 finished with value: 0.3988888888888889 and parameters: {'reg_alpha': 3.678682178991996, 'reg_lambda': 0.3774170267659814, 'num_leaves': 149, 'colsample_bytree': 0.41211293837374274, 'subsample': 0.6490900435795488, 'subsample_freq': 7, 'min_child_samples': 70}. Best is trial 27 with value: 0.3314285714285714.\r\n<string>:27: ExperimentalWarning:\r\n\r\nplot_terminator_improvement is experimental (supported from v3.2.0). The interface can change in the future.\r\n\r\n/Users/mamu/Work-and-Study/optuna/optuna/visualization/_terminator_improvement.py:131: ExperimentalWarning:\r\n\r\nRegretBoundEvaluator is experimental (supported from v3.2.0). The interface can change in the future.\r\n\r\n/Users/mamu/Work-and-Study/optuna/optuna/visualization/_terminator_improvement.py:136: ExperimentalWarning:\r\n\r\nCrossValidationErrorEvaluator is experimental (supported from v3.2.0). The interface can change in the future.\r\n\r\n  0%|                                                                                     | 0/30 [00:00<?, ?it/s]make: *** [html] Segmentation fault: 11\r\n/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```",
    "comments": [
      {
        "user": "HideakiImamura",
        "body": "@gen740 tells me that if we call `make clean`, then the build hangs."
      },
      {
        "user": "gen740",
        "body": "It seems that this issue has been unexpectedly resolved due to the changes in #5575."
      },
      {
        "user": "nzw0301",
        "body": "I cannot also build the document on Mac.\r\n\r\nThe warning message are as follows:\r\n\r\n```\r\n...\r\nbuilding [html]: targets for 0 source files that are out of date\r\nupdating environment: 0 added, 2 changed, 0 removed\r\nreading sources... [100%] reference/samplers/generated/optuna.samplers.intersection_search_space\r\nWARNING: autodoc: failed to import class 'IntersectionSearchSpace' from module 'optuna.samplers'; the following exception was raised:\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/util/inspect.py\", line 397, in safe_getattr\r\n    return getattr(obj, name, *defargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: module 'optuna.samplers' has no attribute 'IntersectionSearchSpace'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/ext/autodoc/importer.py\", line 207, in import_object\r\n    obj = attrgetter(obj, mangled_name)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/ext/autodoc/__init__.py\", line 343, in get_attr\r\n    return autodoc_attrgetter(self.env.app, obj, name, *defargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/ext/autodoc/__init__.py\", line 2835, in autodoc_attrgetter\r\n    return safe_getattr(obj, name, *defargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/util/inspect.py\", line 413, in safe_getattr\r\n    raise AttributeError(name) from exc\r\nAttributeError: IntersectionSearchSpace\r\n [autodoc.import_object]\r\nWARNING: autodoc: failed to import function 'intersection_search_space' from module 'optuna.samplers'; the following exception was raised:\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/util/inspect.py\", line 397, in safe_getattr\r\n    return getattr(obj, name, *defargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: module 'optuna.samplers' has no attribute 'intersection_search_space'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/ext/autodoc/importer.py\", line 207, in import_object\r\n    obj = attrgetter(obj, mangled_name)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/ext/autodoc/__init__.py\", line 343, in get_attr\r\n    return autodoc_attrgetter(self.env.app, obj, name, *defargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/ext/autodoc/__init__.py\", line 2835, in autodoc_attrgetter\r\n    return safe_getattr(obj, name, *defargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/optuna-312/lib/python3.12/site-packages/sphinx/util/inspect.py\", line 413, in safe_getattr\r\n    raise AttributeError(name) from exc\r\nAttributeError: intersection_search_space\r\n [autodoc.import_object]\r\nlooking for now-outdated files... none found\r\npickling environment... done\r\nchecking consistency... /Users/nzw0301/Documents/oss/optuna/docs/source/reference/samplers/generated/optuna.samplers.IntersectionSearchSpace.rst: WARNING: document isn't included in any toctree\r\n/Users/nzw0301/Documents/oss/optuna/docs/source/reference/samplers/generated/optuna.samplers.intersection_search_space.rst: WARNING: document isn't included in any toctree\r\ndone\r\npreparing documents... done\r\ncopying assets...\r\n```\r\n\r\n\r\n\r\n- python: 3.12.0\r\n- Optuna: https://github.com/optuna/optuna/commit/a5d9e9650ddaffe5c29331cc6a023895656870e6\r\n- Mac: M3 MBP\r\n- OS: Sonoma 14.6.1\r\n\r\n\r\nDocs related dependencies:\r\n\r\n```\r\n pip list | grep -i sphi\r\nSphinx                             8.1.3\r\nsphinx-copybutton                  0.5.2\r\nsphinx-gallery                     0.18.0\r\nsphinx-plotly-directive            0.1.3\r\nsphinx-rtd-theme                   3.0.1\r\nsphinxcontrib-applehelp            1.0.8\r\nsphinxcontrib-devhelp              1.0.6\r\nsphinxcontrib-htmlhelp             2.1.0\r\nsphinxcontrib-jquery               4.1\r\nsphinxcontrib-jsmath               1.0.1\r\nsphinxcontrib-qthelp               1.0.7\r\nsphinxcontrib-serializinghtml      1.1.10\r\n```"
      }
    ]
  },
  {
    "issue_number": 5205,
    "title": "Add CI settings to check `optuna.visualization.matplotlib` module is available without `plotly`",
    "author": "c-bata",
    "state": "closed",
    "created_at": "2024-01-26T09:49:12Z",
    "updated_at": "2024-10-31T09:23:42Z",
    "labels": [
      "feature",
      "contribution-welcome"
    ],
    "body": "### Motivation\n\nAdd CI settings to prevent bugs like https://github.com/optuna/optuna/pull/5133 \n\n### Description\n\nAdd GitHub action workflow to trigger `pytest tests/visualization_tests/matplotlib_tests/` only when pull requests contain changes in `optuna/visualization/**.py`.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Let me close this issue because #5263 addressed this issue."
      }
    ]
  },
  {
    "issue_number": 4586,
    "title": "Drop Python 3.7 Support",
    "author": "nzw0301",
    "state": "closed",
    "created_at": "2023-04-07T04:42:57Z",
    "updated_at": "2024-10-30T05:36:29Z",
    "labels": [
      "compatibility"
    ],
    "body": "### Motivation\r\n\r\nThe end of life of python 3.7 is coming soon. Specifically, the date is `27 Jun 2023` according to https://endoflife.date/python. \r\n\r\nA few critical dependencies, such as Numpy and PyTorch, have already dropped support python 3.7, so at least the integration module requiring PyTorch would not work soon. \r\n\r\n\r\n\r\n### Description\r\n\r\nI'd like to know the opinions from core-team's decision, so please replace this field to clarify the tasks to resolve this issue.\r\n\r\n### Alternatives (optional)\r\n\r\n- https://github.com/optuna/optuna/issues/3021\r\n- https://github.com/optuna/optuna/issues/4484\r\n\r\n\r\n### Additional context (optional)\r\n\r\n",
    "comments": [
      {
        "user": "not522",
        "body": "Thank you for your notification. For now, we continue to support Python 3.7 and do not force users to upgrade their Python. When it becomes a barrier to development, we will consider stopping support."
      },
      {
        "user": "c-bata",
        "body": "We internally discussed this topic again and decided to stop integration tests with Python 3.7 (refs #4784)."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5708,
    "title": "Update an FAQ entry on model preservation using Optuna Artifact.",
    "author": "toshihikoyanase",
    "state": "closed",
    "created_at": "2024-10-16T07:05:47Z",
    "updated_at": "2024-10-30T04:48:38Z",
    "labels": [
      "document",
      "contribution-welcome"
    ],
    "body": "### What is an issue?\n\n## Motivation\r\n\r\nThe current FAQ entry guides saving models trained in objective functions, but it suggests using trial numbers in file names to associate models with trials. \r\n\r\nhttps://optuna.readthedocs.io/en/stable/faq.html#how-to-save-machine-learning-models-trained-in-objective-functions\r\n\r\nHowever, Optuna's built-in [ArtifactStore](https://optuna.readthedocs.io/en/stable/reference/artifacts.html), specifically the [FileSystemArtifactStore](https://optuna.readthedocs.io/en/stable/reference/artifacts.html#optuna.artifacts.FileSystemArtifactStore), offers a more robust and flexible solution for storing and retrieving models.\r\n\r\n\r\n## Suggestion\r\n\r\nWe propose revising the FAQ entry to utilize Optuna's ArtifactStore to provide up-to-date guide for users. Here's a suggestion for the updated FAQ entry:\r\n\r\n- Replace the description of using trial numbers with ArtifactStore.\r\n- Update the example code to use ArtifactStore. We can use `upload_artifact`, `get_all_artifact_meta`, and `download_artifact` for saving, listing, and retrieving models.\r\n\r\nThe example code is available in the [ArtifactStore reference](https://optuna.readthedocs.io/en/stable/reference/artifacts.html) and the [ArtifactStore tutorial](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/012_artifact_tutorial.html).\r\n",
    "comments": []
  },
  {
    "issue_number": 4850,
    "title": "Add Log Scale Option for Objective Value in optuna.visualization Plots",
    "author": "corentinlger",
    "state": "closed",
    "created_at": "2023-08-01T14:54:27Z",
    "updated_at": "2024-10-29T17:01:28Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nAdd an option to optuna.visualization functions to enable log_scale plotting of objective values.\r\n\n\n### Description\n\nCurrently, when using optuna.visualization to plot optimization results, it is possible to use log-scaled axes for parameters suggested by Optuna using the log=True option. However, there is no built-in option to apply the same log scale for the objective function values. This can lead to difficulties in visualizing results when the objective values span a wide range, making it challenging to distinguish values that are close to zero.\r\n\r\n### Feature Request:\r\n\r\nI propose adding a new argument (let's call it \"objective_log_scale\" for example) to the plotting functions in optuna.visualization. This new argument would allow users to choose whether to display the objective function values using a log scale or not. By enabling the log scale option for the objective value, users can gain a more insightful understanding of the optimization process, especially when dealing with objective values that have a wide range of values.\r\n\r\n### Example Use Case:\r\n\r\nConsider a scenario where the objective values range from approximately 10¹ to 10⁻⁴. In the current implementation, it becomes challenging to distinguish the values that are close to zero in the plots. However, when using a custom plot with a log scale applied to the objective values, users can see the differences even with a lr going from 0.1 to 1. This can provide valuable insights into the optimization process.\r\n\r\nHere is an illustration with an optuna slice_plot and another custom plot (the values are not exactly the same but come from a random hyper parameter search on the same problem).\r\n\r\n![image](https://github.com/optuna/optuna/assets/111868204/defd6768-04c1-49fc-b02b-f08e034f0daa)\r\n\r\n![image](https://github.com/optuna/optuna/assets/111868204/600023c1-132a-451e-b55c-0a1a4f13668b)\r\n\r\nDo you think it would be useful to add this kind of feature to the visualization methods ? \r\n\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Personally, I don't think this is necessary. Since Optuna's visualisation methods return an object that users can edit, changing the scale can be done easily without introducing such a new argument, which makes the code and test more complicated."
      },
      {
        "user": "corentinlger",
        "body": "Ok thanks for your response ! "
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5409,
    "title": "Enhance `plot_parallel_coordinate()` by eliminating redundant for-loops",
    "author": "c-bata",
    "state": "closed",
    "created_at": "2024-04-22T08:59:48Z",
    "updated_at": "2024-10-28T04:52:18Z",
    "labels": [
      "code-fix",
      "contribution-welcome"
    ],
    "body": "### Motivation\n\nIn parallel coordinate plot, `_is_log_scale()`, `_is_categorical()`, and `_is_numerical()` functions are invoked.\r\n\r\nhttps://github.com/optuna/optuna/blob/fcffbeab86859c32c3c646947f4fd764860c922b/optuna/visualization/_parallel_coordinate.py#L213-L237\r\n\r\nEach one of these functions iterates over the trials, which is clearly redundant.\r\n\r\nhttps://github.com/optuna/optuna/blob/fcffbeab86859c32c3c646947f4fd764860c922b/optuna/visualization/_utils.py#L72-L98\n\n### Suggestion\n\nMaking `plot_parallel_coorinate()` faster by removing wasted for-loop.\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "karthikkurella",
        "body": "@c-bata Please take a look at the PR and review the code-fix."
      },
      {
        "user": "kAIto47802",
        "body": "I will take over [the PR](https://github.com/optuna/optuna/pull/5547) (I will checkout another branch from the branch created by @nzw0301 ) and would like to solve this issue!"
      },
      {
        "user": "kAIto47802",
        "body": "This issue was resolved by\r\n\r\n- #5587.\r\n\r\nHowever, the additional issue\r\n\r\n- #5571\r\n\r\nrelated to this is still unresolved. (I'm currently not working on this.)"
      }
    ]
  },
  {
    "issue_number": 2820,
    "title": "Why Optuna getting stuck after certain number of trials",
    "author": "mostafiz67",
    "state": "closed",
    "created_at": "2021-07-23T14:33:10Z",
    "updated_at": "2024-10-26T03:22:20Z",
    "labels": [
      "question",
      "stale"
    ],
    "body": "I am trying to do hypertuning using `Optuna`. The dataset is the `MovieLense` (1M). In one script I have `Lasso`, `Ridge` and `Knn`. Optuna is working fine for the Lasso and Ridge but getting stuck for the Knn. \r\n\r\nYou can see the trials for the Ridge model tuning was done at `2021-07-22 18:33:53`. Later a new study was created for the Knn at `2021-07-22 18:33:53`. Now (at the time of posting) it is `2021-07-23 11:07:48` but there was no trial for the `Knn`.   \r\n\r\n```\r\n^[[32m[I 2021-07-22 18:33:53,959]^[[0m Trial 199 finished with value: -1.1917496039282074 and parameters: {'alpha': 3.553292157377711e-07, 'solver': 'sag', 'normalize': False}. Best is trial 71 with value: -1.1917485424789929.^[[0m\r\n^[[32m[I 2021-07-22 18:33:53,961]^[[0m A new study created in memory with name: no-name-208652b3-68ec-4464-a2ae-5afefa9bf133^[[0m\r\n```\r\n\r\nThe same thing is happening with the `SVR` model (you can see optuna stuck after 84 number trial at `2021-07-23 05:13:40`)\r\n```\r\n^[[32m[I 2021-07-23 05:13:37,907]^[[0m Trial 83 finished with value: -1.593471166487258 and parameters: {'C': 834.9834466420455, 'epsilon': 99.19181748590665, 'kernel': 'linear', 'norm': 'minmax'}. Best is trial 61 with value: -1.553044709891868.^[[0m\r\n^[[32m[I 2021-07-23 05:13:40,261]^[[0m Trial 84 finished with value: -1.593471166487258 and parameters: {'C': 431.4022584640214, 'epsilon': 2.581688694428477, 'kernel': 'linear', 'norm': 'minmax'}. Best is trial 61 with value: -1.553044709891868.^[[0m\r\n```\r\nCould you tell me why Optuna is getting stuck and how can I solve the issues?\r\n\r\n\r\n### Environment\r\n\r\n<!--\r\nYou can get this information by typing the following:\r\n```\r\npython -c 'import optuna; print(optuna.__version__)'\r\npython -c 'import platform; print(platform.python_version())'\r\npython -c 'import platform; print(platform.platform())'\r\n```\r\n-->\r\n\r\n- Optuna version: 2.8.0\r\n- Python version: 3.8\r\n- OS: Linux CentOS 7\r\n- (Optional) Other libraries and their versions: Scikit Learn, Pandas, and (most common libraries)\r\n\r\n\r\n### Reproducible examples\r\n\r\nThe code I am using for hypertuning\r\n\r\n```\r\ndef tune(objective):\r\n    study = optuna.create_study(direction=\"maximize\")\r\n    study.optimize(objective, n_trials=200, n_jobs=40)\r\n    params = study.best_params\r\n    return params\r\n\r\ndef knn_objective(X_train: DataFrame, y_train: DataFrame, cv_method: kfolds) -> Callable[[Trial], float]:\r\n    def objective(trial: Trial) -> float:\r\n        args: Dict = dict(\r\n            n_neighbors=trial.suggest_int(\"n_neighbors\", 2, 40, 1),\r\n            weights=trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\r\n            metric=trial.suggest_categorical(\"metric\", [\"euclidean\", \"manhattan\", \"mahalanobis\"]),\r\n        )\r\n        estimator = KNeighborsRegressor(**args)\r\n        scores = cross_validate(\r\n            estimator, X=X_train, y=y_train, scoring=\"neg_mean_squared_error\", cv=cv_method, n_jobs=-1\r\n        )\r\n        return float(np.mean(scores[\"test_score\"]))\r\n\r\n    return objective\r\n```",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Could you share the reproducible code?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "nzw0301",
        "body": "Unfortunately, we could not get further information for investigation for a while. Let me close this issue, but please feel free to reopen if you give the reproducible code."
      }
    ]
  },
  {
    "issue_number": 3450,
    "title": "[RFC] Enable trial report multiple intermediate values by `trial.report`",
    "author": "nzw0301",
    "state": "open",
    "created_at": "2022-04-06T10:58:22Z",
    "updated_at": "2024-10-25T14:37:21Z",
    "labels": [
      "feature",
      "needs-discussion"
    ],
    "body": "### Motivation\r\n\r\nRequested by https://github.com/optuna/optuna/issues/2792\r\n\r\nWith this change, users will be able to implement multi-objective pruning algorithms easily. In addition, storing intermediate values for multi-objective optimisation is supposed to be useful to analyse the trials after optimisation even though Optuna's pruning algorithms do not support multi-objective functions.\r\n\r\n\r\n### Description\r\n\r\nAccept multiple intermediate values: `trial.report(value=[1.0, 2.0] step=1)` or `trial.report(values=[1.0, 2.0] step=1)`.\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "HideakiImamura",
        "body": "I think it would be nice to allow the multiple intermediate values. I would like to know other opinions."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5664,
    "title": "Sampler getting stuck non-optimally in search space?",
    "author": "muhlbach",
    "state": "open",
    "created_at": "2024-09-10T12:43:37Z",
    "updated_at": "2024-10-25T09:10:00Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nIt seems that the `TPESampler` can get stuck and repeatedly sample parameters that result in `NaN`/`None` without ability to move away from these parameters. I cannot reproduce the error systematically but it happens once in a while using XGBoost with these parameters:\r\n```python\r\nparams = {\r\n            \"n_estimators\": IntDistribution(low=100, high=2000, step=100, log=False),\r\n            \"max_depth\": IntDistribution(low=2, high=15, step=1, log=False),\r\n            \"learning_rate\": FloatDistribution(low=0.001, high=0.3, log=False),  # log=True\r\n            \"reg_alpha\": FloatDistribution(low=0.001, high=10, log=True),  # log=True\r\n            \"reg_lambda\": FloatDistribution(low=0.001, high=100, log=True),  # log=True\r\n            \"subsample\": FloatDistribution(low=0.5, high=1.0, log=False),\r\n            \"colsample_bytree\": FloatDistribution(low=0.3, high=1.0, log=False),\r\n            \"min_child_weight\": IntDistribution(low=5, high=200, step=5, log=False),\r\n        }\r\n```\r\nAm I suing the `error_score=np.nan` and `catch=FloatingPointError` incorrectly? \r\n\r\n\r\n### Environment\r\n\r\n- Optuna version:3.6.1\r\n- Python version:3.12.3\r\n- OS:Windows-10-10.0.19045-SP0\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\n10:01:15 [I 2024-09-10 10:01:14,515] A new study created in memory with name: no-name-ea338d4e-d2f0-4b6a-b4dc-d5df894fec91\r\n10:10:51 [I 2024-09-10 10:10:46,866] Trial 0 finished with value: -0.18997005963218971 and parameters: {'n_estimators': 1700, 'max_depth': 14, 'learning_rate': 0.1375090273534775, 'reg_alpha': 0.004058997725367004, 'reg_lambda': 0.22260852762554037, 'subsample': 0.46057821404523164, 'colsample_bytree': 0.9518107769686431, 'min_child_weight': 75}. Best is trial 0 with value: -0.18997005963218971.\r\n10:17:42 [I 2024-09-10 10:17:29,351] Trial 1 finished with value: -0.010669558606033303 and parameters: {'n_estimators': 1700, 'max_depth': 8, 'learning_rate': 0.07971345451351762, 'reg_alpha': 4.589928354954093, 'reg_lambda': 14.419464546755341, 'subsample': 0.6935565539982141, 'colsample_bytree': 0.6171303954206171, 'min_child_weight': 115}. Best is trial 1 with value: -0.010669558606033303.\r\n10:22:49 [I 2024-09-10 10:22:40,079] Trial 2 finished with value: -0.044669134991376314 and parameters: {'n_estimators': 1100, 'max_depth': 12, 'learning_rate': 0.07947372526387049, 'reg_alpha': 0.14514535429795028, 'reg_lambda': 74.97732987559624, 'subsample': 0.3914031291130755, 'colsample_bytree': 0.7229053888690472, 'min_child_weight': 150}. Best is trial 1 with value: -0.010669558606033303.\r\n10:25:41 [I 2024-09-10 10:25:28,931] Trial 3 finished with value: -0.06932224204604909 and parameters: {'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.26525853204532984, 'reg_alpha': 0.0018184252177571635, 'reg_lambda': 0.16065847278791215, 'subsample': 0.5842136203797257, 'colsample_bytree': 0.2762531232812062, 'min_child_weight': 65}. Best is trial 1 with value: -0.010669558606033303.\r\n10:30:17 [I 2024-09-10 10:30:11,089] Trial 4 finished with value: -0.2320931034803132 and parameters: {'n_estimators': 2000, 'max_depth': 6, 'learning_rate': 0.2892834990042613, 'reg_alpha': 0.021208341501352656, 'reg_lambda': 4.8311227339979705, 'subsample': 0.6779472214429496, 'colsample_bytree': 0.9558532158605845, 'min_child_weight': 15}. Best is trial 1 with value: -0.010669558606033303.\r\n10:37:53 [I 2024-09-10 10:37:48,025] Trial 5 finished with value: -0.3450577348960157 and parameters: {'n_estimators': 1900, 'max_depth': 10, 'learning_rate': 0.2232109457519439, 'reg_alpha': 0.006748231077491931, 'reg_lambda': 0.024615794354667278, 'subsample': 0.4485352141510567, 'colsample_bytree': 0.707622743964222, 'min_child_weight': 165}. Best is trial 1 with value: -0.010669558606033303.\r\n10:42:00 [I 2024-09-10 10:41:48,965] Trial 6 finished with value: -0.027345607874212308 and parameters: {'n_estimators': 1500, 'max_depth': 3, 'learning_rate': 0.20953977603757537, 'reg_alpha': 0.161122302760904, 'reg_lambda': 11.834629562437705, 'subsample': 0.8353162083158374, 'colsample_bytree': 0.4422175179562352, 'min_child_weight': 40}. Best is trial 1 with value: -0.010669558606033303.\r\n10:43:51 [I 2024-09-10 10:43:45,244] Trial 7 finished with value: 0.0024711636741346077 and parameters: {'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.19193682738334275, 'reg_alpha': 1.5134204966755875, 'reg_lambda': 0.3867427501219411, 'subsample': 0.49559896862233926, 'colsample_bytree': 0.3947339636040147, 'min_child_weight': 110}. Best is trial 7 with value: 0.0024711636741346077.\r\n10:51:42 [I 2024-09-10 10:51:41,349] Trial 8 finished with value: -0.1660809020711997 and parameters: {'n_estimators': 1800, 'max_depth': 12, 'learning_rate': 0.27793525427222865, 'reg_alpha': 0.08866838004424586, 'reg_lambda': 0.24029061044506006, 'subsample': 0.9827000119310183, 'colsample_bytree': 0.3460044928308177, 'min_child_weight': 195}. Best is trial 7 with value: 0.0024711636741346077.\r\n10:54:34 [I 2024-09-10 10:54:27,220] Trial 9 finished with value: -0.11521976069910256 and parameters: {'n_estimators': 1100, 'max_depth': 5, 'learning_rate': 0.21074635135718878, 'reg_alpha': 0.7075127925125327, 'reg_lambda': 3.990251868105303, 'subsample': 0.29048421616565484, 'colsample_bytree': 0.7505633648538987, 'min_child_weight': 200}. Best is trial 7 with value: 0.0024711636741346077.\r\n10:54:34 [W 2024-09-10 10:54:27,496] Trial 10 failed with parameters: {} because of the following error: FloatingPointError('underflow encountered in _ndtri_exp_single (vectorized)').\r\n10:54:34 Traceback (most recent call last):\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\r\n10:54:34     value_or_values = func(trial)\r\n10:54:34                       ^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 214, in __call__\r\n10:54:34     params = self._get_params(trial)\r\n10:54:34              ^^^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 325, in _get_params\r\n10:54:34     name: trial._suggest(name, distribution)\r\n10:54:34           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\trial\\_trial.py\", line 629, in _suggest\r\n10:54:34     param_value = self.study.sampler.sample_independent(\r\n10:54:34                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 447, in sample_independent\r\n10:54:34     return self._sample(study, trial, {param_name: param_distribution})[param_name]\r\n10:54:34            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 487, in _sample\r\n10:54:34     samples_below = mpe_below.sample(self._rng.rng, self._n_ei_candidates)\r\n10:54:34                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py\", line 81, in sample\r\n10:54:34     sampled = self._mixture_distribution.sample(rng, size)\r\n10:54:34               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\probability_distributions.py\", line 65, in sample\r\n10:54:34     samples = _truncnorm.rvs(\r\n10:54:34               ^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\_truncnorm.py\", line 215, in rvs\r\n10:54:34     return ppf(percentiles, a, b) * scale + loc\r\n10:54:34            ^^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\_truncnorm.py\", line 194, in ppf\r\n10:54:34     out[case_left] = ppf_left(q_left, a[case_left], b[case_left])\r\n10:54:34                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\_truncnorm.py\", line 182, in ppf_left\r\n10:54:34     return _ndtri_exp(log_Phi_x)\r\n10:54:34            ^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\_truncnorm.py\", line 170, in _ndtri_exp\r\n10:54:34     return np.frompyfunc(_ndtri_exp_single, 1, 1)(y).astype(float)\r\n10:54:34            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n10:54:34 FloatingPointError: underflow encountered in _ndtri_exp_single (vectorized)\r\n10:54:34 [W 2024-09-10 10:54:27,499] Trial 10 failed with value None.\r\n10:56:40 [I 2024-09-10 10:56:26,326] Trial 11 finished with value: -0.005724771331084982 and parameters: {'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.024649026977378752, 'reg_alpha': 8.686842626761925, 'reg_lambda': 0.005563717165219818, 'subsample': 0.2604880783626039, 'colsample_bytree': 0.47731777962309624, 'min_child_weight': 115}. Best is trial 7 with value: 0.0024711636741346077.\r\n10:58:16 [I 2024-09-10 10:58:03,002] Trial 12 finished with value: -76.25463561629338 and parameters: {'n_estimators': 100, 'max_depth': 2, 'learning_rate': 0.002111990181721174, 'reg_alpha': 5.97626864763605, 'reg_lambda': 0.002125604190117196, 'subsample': 0.2524308777117293, 'colsample_bytree': 0.486401356164175, 'min_child_weight': 115}. Best is trial 7 with value: 0.0024711636741346077.\r\n11:00:08 [I 2024-09-10 10:59:55,648] Trial 13 finished with value: -0.01526518754084294 and parameters: {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.1463304236953738, 'reg_alpha': 1.2122940756445504, 'reg_lambda': 0.0011603424047401695, 'subsample': 0.35528286074005655, 'colsample_bytree': 0.4910629315582275, 'min_child_weight': 140}. Best is trial 7 with value: 0.0024711636741346077.\r\n11:01:59 [I 2024-09-10 11:01:57,477] Trial 14 finished with value: 0.001405276345087303 and parameters: {'n_estimators': 500, 'max_depth': 2, 'learning_rate': 0.016060723476221263, 'reg_alpha': 8.745223791225987, 'reg_lambda': 0.014495122877716522, 'subsample': 0.2031350958336356, 'colsample_bytree': 0.38392240540684064, 'min_child_weight': 85}. Best is trial 7 with value: 0.0024711636741346077.\r\n11:04:20 [I 2024-09-10 11:04:12,565] Trial 15 finished with value: -0.008397760581131064 and parameters: {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.09814173693718731, 'reg_alpha': 1.2717282711504283, 'reg_lambda': 0.02772146784873597, 'subsample': 0.5388121718718919, 'colsample_bytree': 0.2583261981875801, 'min_child_weight': 80}. Best is trial 7 with value: 0.0024711636741346077.\r\n11:06:27 [I 2024-09-10 11:06:21,342] Trial 16 finished with value: -0.05416356592761046 and parameters: {'n_estimators': 700, 'max_depth': 4, 'learning_rate': 0.18290418523200333, 'reg_alpha': 1.8518712438525688, 'reg_lambda': 0.8643973198256812, 'subsample': 0.2103481768198327, 'colsample_bytree': 0.606455333530921, 'min_child_weight': 90}. Best is trial 7 with value: 0.0024711636741346077.\r\n11:08:33 [I 2024-09-10 11:08:26,391] Trial 17 finished with value: 0.005311400663346066 and parameters: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.046391838620829184, 'reg_alpha': 0.5268210072572383, 'reg_lambda': 0.04435879306910267, 'subsample': 0.7791723443126173, 'colsample_bytree': 0.38278902032095263, 'min_child_weight': 50}. Best is trial 17 with value: 0.005311400663346066.\r\n11:11:09 [I 2024-09-10 11:11:05,262] Trial 18 finished with value: 0.003616616210599277 and parameters: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.043802940565508486, 'reg_alpha': 0.3542989912394958, 'reg_lambda': 1.0004316854150197, 'subsample': 0.7997138701262596, 'colsample_bytree': 0.35895744795827367, 'min_child_weight': 50}. Best is trial 17 with value: 0.005311400663346066.\r\n11:14:01 [I 2024-09-10 11:13:53,480] Trial 19 finished with value: -0.00014211181783271853 and parameters: {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.04890588905948621, 'reg_alpha': 0.38370420498388436, 'reg_lambda': 0.08045793642529564, 'subsample': 0.844593699233772, 'colsample_bytree': 0.5381976297484903, 'min_child_weight': 5}. Best is trial 17 with value: 0.005311400663346066.\r\n11:18:22 [I 2024-09-10 11:18:17,793] Trial 20 finished with value: -0.014826681921361828 and parameters: {'n_estimators': 1300, 'max_depth': 10, 'learning_rate': 0.05004572794407963, 'reg_alpha': 0.03105864467235086, 'reg_lambda': 1.2731069293985098, 'subsample': 0.8244641910779109, 'colsample_bytree': 0.3374862971496377, 'min_child_weight': 45}. Best is trial 17 with value: 0.005311400663346066.\r\n11:21:13 [I 2024-09-10 11:21:13,081] Trial 21 finished with value: -0.03002182584796942 and parameters: {'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.11189896213758986, 'reg_alpha': 0.3164027750003232, 'reg_lambda': 0.06221522794079191, 'subsample': 0.9615100164808865, 'colsample_bytree': 0.5633000980374495, 'min_child_weight': 35}. Best is trial 17 with value: 0.005311400663346066.\r\n11:23:05 [I 2024-09-10 11:23:02,622] Trial 22 finished with value: -0.023918047735005443 and parameters: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.17300870877527535, 'reg_alpha': 0.44483407793112906, 'reg_lambda': 0.8290852141150747, 'subsample': 0.7150893261574482, 'colsample_bytree': 0.3993948263261181, 'min_child_weight': 55}. Best is trial 17 with value: 0.005311400663346066.\r\n11:24:56 [I 2024-09-10 11:24:52,113] Trial 23 finished with value: 0.0076487107260690135 and parameters: {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.04940925450697702, 'reg_alpha': 2.47731894548485, 'reg_lambda': 0.5128540538601576, 'subsample': 0.7505462476338058, 'colsample_bytree': 0.3099952550662981, 'min_child_weight': 25}. Best is trial 23 with value: 0.0076487107260690135.\r\n11:27:02 [I 2024-09-10 11:26:53,168] Trial 24 finished with value: 0.009405124905217166 and parameters: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.04713587937948018, 'reg_alpha': 3.2807638080063874, 'reg_lambda': 1.3995676590239676, 'subsample': 0.7702057448102747, 'colsample_bytree': 0.30314087325870115, 'min_child_weight': 25}. Best is trial 24 with value: 0.009405124905217166.\r\n11:29:09 [I 2024-09-10 11:29:01,287] Trial 25 finished with value: 0.007513439980736308 and parameters: {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.07542545607286495, 'reg_alpha': 2.4397748076948527, 'reg_lambda': 3.2771414252446545, 'subsample': 0.754181603173294, 'colsample_bytree': 0.279665894601023, 'min_child_weight': 25}. Best is trial 24 with value: 0.009405124905217166.\r\n11:31:15 [I 2024-09-10 11:31:01,473] Trial 26 finished with value: 0.007917615104654496 and parameters: {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.06946305445365424, 'reg_alpha': 2.912448613491123, 'reg_lambda': 2.8078074859219035, 'subsample': 0.9114505918197824, 'colsample_bytree': 0.2958603246810116, 'min_child_weight': 25}. Best is trial 24 with value: 0.009405124905217166.\r\n11:33:36 [I 2024-09-10 11:33:32,367] Trial 27 finished with value: 0.003411537894701777 and parameters: {'n_estimators': 600, 'max_depth': 5, 'learning_rate': 0.11804917211221655, 'reg_alpha': 3.406003832260802, 'reg_lambda': 31.809930487912624, 'subsample': 0.9146199543363627, 'colsample_bytree': 0.3254930256220847, 'min_child_weight': 5}. Best is trial 24 with value: 0.009405124905217166.\r\n11:35:28 [I 2024-09-10 11:35:27,228] Trial 28 finished with value: 0.008942023964594703 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.06992567075338592, 'reg_alpha': 3.5331955627480873, 'reg_lambda': 2.404147844470027, 'subsample': 0.8919164674961959, 'colsample_bytree': 0.25363160299509613, 'min_child_weight': 20}. Best is trial 24 with value: 0.009405124905217166.\r\n11:38:19 [I 2024-09-10 11:38:09,609] Trial 29 finished with value: -0.0015003747178826965 and parameters: {'n_estimators': 900, 'max_depth': 5, 'learning_rate': 0.09376131019414345, 'reg_alpha': 0.8569666506779596, 'reg_lambda': 2.103602586851088, 'subsample': 0.8863007597445797, 'colsample_bytree': 0.25644409733822676, 'min_child_weight': 25}. Best is trial 24 with value: 0.009405124905217166.\r\n11:41:25 [I 2024-09-10 11:41:15,946] Trial 30 finished with value: -0.015755656874046342 and parameters: {'n_estimators': 300, 'max_depth': 15, 'learning_rate': 0.12869959660372696, 'reg_alpha': 4.99968980421572, 'reg_lambda': 9.975872280377569, 'subsample': 0.633940309694496, 'colsample_bytree': 0.841809544165268, 'min_child_weight': 70}. Best is trial 24 with value: 0.009405124905217166.\r\n11:45:32 [I 2024-09-10 11:45:27,604] Trial 31 finished with value: -0.01475795810659647 and parameters: {'n_estimators': 1300, 'max_depth': 9, 'learning_rate': 0.06707214570406113, 'reg_alpha': 0.06378985711260042, 'reg_lambda': 6.767363014048414, 'subsample': 0.9486228549807845, 'colsample_bytree': 0.42956812612783013, 'min_child_weight': 60}. Best is trial 24 with value: 0.009405124905217166.\r\n11:47:23 [I 2024-09-10 11:47:13,887] Trial 32 finished with value: 0.005142164284669681 and parameters: {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.02903582018205072, 'reg_alpha': 2.6148062772742335, 'reg_lambda': 0.4117497138620986, 'subsample': 0.8861173027082039, 'colsample_bytree': 0.3159376383087238, 'min_child_weight': 25}. Best is trial 24 with value: 0.009405124905217166.\r\n11:49:45 [I 2024-09-10 11:49:30,201] Trial 33 finished with value: 0.00778319094090546 and parameters: {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.06715860569474333, 'reg_alpha': 3.353088971073043, 'reg_lambda': 2.2638089051996104, 'subsample': 0.7427592313627511, 'colsample_bytree': 0.2858299497069965, 'min_child_weight': 15}. Best is trial 24 with value: 0.009405124905217166.\r\n11:51:36 [I 2024-09-10 11:51:35,953] Trial 34 finished with value: 0.006926617355720949 and parameters: {'n_estimators': 600, 'max_depth': 3, 'learning_rate': 0.09307421897571805, 'reg_alpha': 3.979828648894999, 'reg_lambda': 35.303227998975345, 'subsample': 0.8779018343125211, 'colsample_bytree': 0.2883681115037744, 'min_child_weight': 15}. Best is trial 24 with value: 0.009405124905217166.\r\n11:53:42 [I 2024-09-10 11:53:35,921] Trial 35 finished with value: 0.006678995361620421 and parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.06796006778736532, 'reg_alpha': 9.277289233684817, 'reg_lambda': 1.8294676943521027, 'subsample': 0.6494024003879391, 'colsample_bytree': 0.2543735675578804, 'min_child_weight': 35}. Best is trial 24 with value: 0.009405124905217166.\r\n11:55:19 [I 2024-09-10 11:55:18,990] Trial 36 finished with value: 0.004411783140162356 and parameters: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.06354153171085007, 'reg_alpha': 5.4609780403032655, 'reg_lambda': 27.965769904882603, 'subsample': 0.7242343068107765, 'colsample_bytree': 0.30709175266633904, 'min_child_weight': 15}. Best is trial 24 with value: 0.009405124905217166.\r\n11:57:40 [I 2024-09-10 11:57:33,776] Trial 37 finished with value: -0.015077234909149362 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.0965573966176837, 'reg_alpha': 0.0013045837859765382, 'reg_lambda': 2.3773128249099678, 'subsample': 0.9373663199088111, 'colsample_bytree': 0.9024844782420711, 'min_child_weight': 5}. Best is trial 24 with value: 0.009405124905217166.\r\n11:59:46 [I 2024-09-10 11:59:32,115] Trial 38 finished with value: -0.019361426249078106 and parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.15748766160369806, 'reg_alpha': 0.22625559127483905, 'reg_lambda': 16.952323282036545, 'subsample': 0.9942879131203826, 'colsample_bytree': 0.439597978296263, 'min_child_weight': 35}. Best is trial 24 with value: 0.009405124905217166.\r\n12:01:38 [I 2024-09-10 12:01:35,985] Trial 39 finished with value: -0.01633498811766315 and parameters: {'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.0061721483562373475, 'reg_alpha': 0.005144946650235849, 'reg_lambda': 0.1240869421730969, 'subsample': 0.5930916870283173, 'colsample_bytree': 0.36342308859937056, 'min_child_weight': 65}. Best is trial 24 with value: 0.009405124905217166.\r\n12:04:14 [I 2024-09-10 12:04:08,344] Trial 40 finished with value: 0.008526817730823727 and parameters: {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.029315839665427758, 'reg_alpha': 0.9955394959601305, 'reg_lambda': 4.860445519121875, 'subsample': 0.6813960278719081, 'colsample_bytree': 0.6767691808472179, 'min_child_weight': 130}. Best is trial 24 with value: 0.009405124905217166.\r\n12:08:05 [I 2024-09-10 12:07:54,920] Trial 41 finished with value: 0.0026923064253944196 and parameters: {'n_estimators': 1300, 'max_depth': 9, 'learning_rate': 0.033272040060350616, 'reg_alpha': 0.7545080496224342, 'reg_lambda': 7.020283565040536, 'subsample': 0.7998484112611823, 'colsample_bytree': 0.6644301095599315, 'min_child_weight': 140}. Best is trial 24 with value: 0.009405124905217166.\r\n12:08:05 [W 2024-09-10 12:07:54,940] Trial 42 failed with parameters: {'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.08320452582508608, 'reg_alpha': 1.6717129223260627, 'reg_lambda': 87.65667556476141} because of the following error: FloatingPointError('underflow encountered in exp').\r\n12:08:05 Traceback (most recent call last):\r\n12:08:05   ~\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\r\n12:08:05     value_or_values = func(trial)\r\n12:08:05                       ^^^^^^^^^^^\r\n12:08:05   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 214, in __call__\r\n12:08:05     params = self._get_params(trial)\r\n12:08:05              ^^^^^^^^^^^^^^^^^^^^^^^\r\n12:08:05   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 325, in _get_params\r\n12:08:05     name: trial._suggest(name, distribution)\r\n12:08:05           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:08:05   ~\\Lib\\site-packages\\optuna\\trial\\_trial.py\", line 629, in _suggest\r\n12:08:05     param_value = self.study.sampler.sample_independent(\r\n12:08:05                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:08:05   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 447, in sample_independent\r\n12:08:05     return self._sample(study, trial, {param_name: param_distribution})[param_name]\r\n12:08:05            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:08:05   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 488, in _sample\r\n12:08:05     acq_func_vals = self._compute_acquisition_func(samples_below, mpe_below, mpe_above)\r\n12:08:05                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:08:05   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 528, in _compute_acquisition_func\r\n12:08:05     log_likelihoods_above = mpe_above.log_pdf(samples)\r\n12:08:05                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:08:05   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py\", line 86, in log_pdf\r\n12:08:05     return self._mixture_distribution.log_pdf(transformed_samples)\r\n12:08:05            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:08:05   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\probability_distributions.py\", line 121, in log_pdf\r\n12:08:05     return np.log(np.exp(weighted_log_pdf - max_[:, None]).sum(axis=1)) + max_\r\n12:08:05                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:08:05 FloatingPointError: underflow encountered in exp\r\n12:08:05 [W 2024-09-10 12:07:54,941] Trial 42 failed with value None.\r\n12:10:27 [I 2024-09-10 12:10:20,457] Trial 43 finished with value: -0.0022679745646995153 and parameters: {'n_estimators': 900, 'max_depth': 5, 'learning_rate': 0.08377318604153461, 'reg_alpha': 1.7232848957191431, 'reg_lambda': 3.8102696444255395, 'subsample': 0.6976284047494112, 'colsample_bytree': 0.7445414604299656, 'min_child_weight': 130}. Best is trial 24 with value: 0.009405124905217166.\r\n12:10:27 [W 2024-09-10 12:10:20,477] Trial 44 failed with parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.017812312827070256, 'reg_alpha': 1.061161424314795, 'reg_lambda': 1.5391922416756485} because of the following error: FloatingPointError('underflow encountered in exp').\r\n12:10:27 Traceback (most recent call last):\r\n12:10:27   ~\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\r\n12:10:27     value_or_values = func(trial)\r\n12:10:27                       ^^^^^^^^^^^\r\n12:10:27   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 214, in __call__\r\n12:10:27     params = self._get_params(trial)\r\n12:10:27              ^^^^^^^^^^^^^^^^^^^^^^^\r\n12:10:27   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 325, in _get_params\r\n12:10:27     name: trial._suggest(name, distribution)\r\n12:10:27           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:10:27   ~\\Lib\\site-packages\\optuna\\trial\\_trial.py\", line 629, in _suggest\r\n12:10:27     param_value = self.study.sampler.sample_independent(\r\n12:10:27                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:10:27   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 447, in sample_independent\r\n12:10:27     return self._sample(study, trial, {param_name: param_distribution})[param_name]\r\n12:10:27            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:10:27   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 488, in _sample\r\n12:10:27     acq_func_vals = self._compute_acquisition_func(samples_below, mpe_below, mpe_above)\r\n12:10:27                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:10:27   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 528, in _compute_acquisition_func\r\n12:10:27     log_likelihoods_above = mpe_above.log_pdf(samples)\r\n12:10:27                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:10:27   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py\", line 86, in log_pdf\r\n12:10:27     return self._mixture_distribution.log_pdf(transformed_samples)\r\n12:10:27            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:10:27   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\probability_distributions.py\", line 121, in log_pdf\r\n12:10:27     return np.log(np.exp(weighted_log_pdf - max_[:, None]).sum(axis=1)) + max_\r\n12:10:27                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:10:27 FloatingPointError: underflow encountered in exp\r\n12:10:27 [W 2024-09-10 12:10:20,477] Trial 44 failed with value None.\r\n12:13:18 [I 2024-09-10 12:13:06,308] Trial 45 finished with value: 0.0025315289981590328 and parameters: {'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.06033587654963292, 'reg_alpha': 3.4747350950246223, 'reg_lambda': 1.576525561669667, 'subsample': 0.6300580509756624, 'colsample_bytree': 0.8189534112805277, 'min_child_weight': 100}. Best is trial 24 with value: 0.009405124905217166.\r\n12:13:18 [W 2024-09-10 12:13:06,328] Trial 46 failed with parameters: {'n_estimators': 700, 'max_depth': 12, 'learning_rate': 0.032661748037195196, 'reg_alpha': 1.1213743299824717, 'reg_lambda': 5.302014957673832} because of the following error: FloatingPointError('underflow encountered in exp').\r\n12:13:18 Traceback (most recent call last):\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\r\n12:13:18     value_or_values = func(trial)\r\n12:13:18                       ^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 214, in __call__\r\n12:13:18     params = self._get_params(trial)\r\n12:13:18              ^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 325, in _get_params\r\n12:13:18     name: trial._suggest(name, distribution)\r\n12:13:18           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\trial\\_trial.py\", line 629, in _suggest\r\n12:13:18     param_value = self.study.sampler.sample_independent(\r\n12:13:18                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 447, in sample_independent\r\n12:13:18     return self._sample(study, trial, {param_name: param_distribution})[param_name]\r\n12:13:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 488, in _sample\r\n12:13:18     acq_func_vals = self._compute_acquisition_func(samples_below, mpe_below, mpe_above)\r\n12:13:18                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 528, in _compute_acquisition_func\r\n12:13:18     log_likelihoods_above = mpe_above.log_pdf(samples)\r\n12:13:18                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py\", line 86, in log_pdf\r\n12:13:18     return self._mixture_distribution.log_pdf(transformed_samples)\r\n12:13:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\probability_distributions.py\", line 121, in log_pdf\r\n12:13:18     return np.log(np.exp(weighted_log_pdf - max_[:, None]).sum(axis=1)) + max_\r\n12:13:18                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18 FloatingPointError: underflow encountered in exp\r\n12:13:18 [W 2024-09-10 12:13:06,328] Trial 46 failed with value None.\r\n12:13:18 [W 2024-09-10 12:13:06,355] Trial 47 failed with parameters: {'n_estimators': 700, 'max_depth': 4, 'learning_rate': 0.03344881096466124, 'reg_alpha': 0.9683785733842344, 'reg_lambda': 76.49358739507885, 'subsample': 0.6689777262492278, 'colsample_bytree': 0.6765861986442967} because of the following error: FloatingPointError('underflow encountered in exp').\r\n12:13:18 Traceback (most recent call last):\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\r\n12:13:18     value_or_values = func(trial)\r\n12:13:18                       ^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 214, in __call__\r\n12:13:18     params = self._get_params(trial)\r\n12:13:18              ^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 325, in _get_params\r\n12:13:18     name: trial._suggest(name, distribution)\r\n12:13:18           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\trial\\_trial.py\", line 629, in _suggest\r\n12:13:18     param_value = self.study.sampler.sample_independent(\r\n12:13:18                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 447, in sample_independent\r\n12:13:18     return self._sample(study, trial, {param_name: param_distribution})[param_name]\r\n12:13:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 488, in _sample\r\n12:13:18     acq_func_vals = self._compute_acquisition_func(samples_below, mpe_below, mpe_above)\r\n12:13:18                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 528, in _compute_acquisition_func\r\n12:13:18     log_likelihoods_above = mpe_above.log_pdf(samples)\r\n12:13:18                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py\", line 86, in log_pdf\r\n12:13:18     return self._mixture_distribution.log_pdf(transformed_samples)\r\n12:13:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\probability_distributions.py\", line 121, in log_pdf\r\n12:13:18     return np.log(np.exp(weighted_log_pdf - max_[:, None]).sum(axis=1)) + max_\r\n12:13:18                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18 FloatingPointError: underflow encountered in exp\r\n12:13:18 [W 2024-09-10 12:13:06,356] Trial 47 failed with value None.\r\n12:13:18 [W 2024-09-10 12:13:06,367] Trial 48 failed with parameters: {'n_estimators': 700, 'max_depth': 12} because of the following error: FloatingPointError('underflow encountered in exp').\r\n12:13:18 Traceback (most recent call last):\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\r\n12:13:18     value_or_values = func(trial)\r\n12:13:18                       ^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 214, in __call__\r\n12:13:18     params = self._get_params(trial)\r\n12:13:18              ^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 325, in _get_params\r\n12:13:18     name: trial._suggest(name, distribution)\r\n12:13:18           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\trial\\_trial.py\", line 629, in _suggest\r\n12:13:18     param_value = self.study.sampler.sample_independent(\r\n12:13:18                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 447, in sample_independent\r\n12:13:18     return self._sample(study, trial, {param_name: param_distribution})[param_name]\r\n12:13:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 488, in _sample\r\n12:13:18     acq_func_vals = self._compute_acquisition_func(samples_below, mpe_below, mpe_above)\r\n12:13:18                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 528, in _compute_acquisition_func\r\n12:13:18     log_likelihoods_above = mpe_above.log_pdf(samples)\r\n12:13:18                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py\", line 86, in log_pdf\r\n12:13:18     return self._mixture_distribution.log_pdf(transformed_samples)\r\n12:13:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\probability_distributions.py\", line 121, in log_pdf\r\n12:13:18     return np.log(np.exp(weighted_log_pdf - max_[:, None]).sum(axis=1)) + max_\r\n12:13:18                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18 FloatingPointError: underflow encountered in exp\r\n12:13:18 [W 2024-09-10 12:13:06,367] Trial 48 failed with value None.\r\n12:13:18 [W 2024-09-10 12:13:06,378] Trial 49 failed with parameters: {'n_estimators': 700, 'max_depth': 4} because of the following error: FloatingPointError('underflow encountered in exp').\r\n12:13:18 Traceback (most recent call last):\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\r\n12:13:18     value_or_values = func(trial)\r\n12:13:18                       ^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 214, in __call__\r\n12:13:18     params = self._get_params(trial)\r\n12:13:18              ^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna_integration\\sklearn\\sklearn.py\", line 325, in _get_params\r\n12:13:18     name: trial._suggest(name, distribution)\r\n12:13:18           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\trial\\_trial.py\", line 629, in _suggest\r\n12:13:18     param_value = self.study.sampler.sample_independent(\r\n12:13:18                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 447, in sample_independent\r\n12:13:18     return self._sample(study, trial, {param_name: param_distribution})[param_name]\r\n12:13:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 488, in _sample\r\n12:13:18     acq_func_vals = self._compute_acquisition_func(samples_below, mpe_below, mpe_above)\r\n12:13:18                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py\", line 528, in _compute_acquisition_func\r\n12:13:18     log_likelihoods_above = mpe_above.log_pdf(samples)\r\n12:13:18                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\parzen_estimator.py\", line 86, in log_pdf\r\n12:13:18     return self._mixture_distribution.log_pdf(transformed_samples)\r\n12:13:18            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18   ~\\Lib\\site-packages\\optuna\\samplers\\_tpe\\probability_distributions.py\", line 121, in log_pdf\r\n12:13:18     return np.log(np.exp(weighted_log_pdf - max_[:, None]).sum(axis=1)) + max_\r\n12:13:18                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n12:13:18 FloatingPointError: underflow encountered in exp\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nNot systematically reproducible.\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "This looks similar to https://github.com/optuna/optuna/issues/2639#issuecomment-908828116. Could you try the workaround shared by the related issue?"
      },
      {
        "user": "muhlbach",
        "body": "Yes, I will try that. Note that the CMA-ES sampler does not have this issue."
      }
    ]
  },
  {
    "issue_number": 5607,
    "title": "Option to use `enqueue_trial` along with the ask-and-tell interface",
    "author": "SBlokhuizen",
    "state": "open",
    "created_at": "2024-08-02T09:55:15Z",
    "updated_at": "2024-10-25T00:28:18Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\r\n\r\nAs far as I can tell, it is not possible yet to use the `enqueue_trial` function alongside the ask-and-tell interface. I would like to supply an initial set of parameters to use for the first trial of an optimization run, while using the ask-and-tell interface. \r\n\r\n\r\n### Description\r\n\r\nIt could look something like this (adapted from the ask-and-tell [docs](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html)). \r\n\r\n```python\r\nstudy = optuna.create_study(direction=\"maximize\")\r\n\r\n# initial enqueued trial\r\ntrial = study.ask()\r\n# Similar to calling enqueue_trial\r\ntrial.set_params({\"solver\": \"lbfgs\", \"C\": 10.0)\r\nclf = LogisticRegression(C=C, solver=solver)\r\nclf.fit(X_train, y_train)\r\nval_accuracy = clf.score(X_test, y_test)\r\nstudy.tell(trial, val_accuracy)\r\n\r\nn_trials = 10\r\nfor _ in range(n_trials):\r\n    trial = study.ask()  # `trial` is a `Trial` and not a `FrozenTrial`.\r\n\r\n    C = trial.suggest_float(\"C\", 1e-7, 10.0, log=True)\r\n    solver = trial.suggest_categorical(\"solver\", (\"lbfgs\", \"saga\"))\r\n\r\n    clf = LogisticRegression(C=C, solver=solver)\r\n    clf.fit(X_train, y_train)\r\n    val_accuracy = clf.score(X_test, y_test)\r\n\r\n    study.tell(trial, val_accuracy)  # tell the pair of trial and objective value\r\n```\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "jacobnzw",
        "body": "To add more context to the problem @SBlokhuizen described above, here is what I found to work if I wanna \"enqueue\" a baseline trial\r\n\r\n```python\r\nimport optuna\r\n\r\nstudy = optuna.create_study(study_name=\"test\")\r\n\r\n# evaluate objective; has built-in defaults param values, no need to explicitly set them\r\nvalue = some_objective_func()\r\n\r\n# For the record, create Optuna trial with default parameter values.\r\ndefault_param_values = {\r\n    \"theta\": 0.1\r\n}\r\n# IMPORTANT: The distributions specified here must match those used in other trials,\r\n# otherwise parameter importance plots will show empty.\r\ndefault_distributions = {\r\n    \"theta\": optuna.distributions.FloatDistribution(0.0, 1.0)\r\n}\r\nbaseline_trial = optuna.trial.create_trial(\r\n    state=optuna.trial.TrialState.COMPLETE, \r\n    params=default_param_values, \r\n    distributions=default_distributions, \r\n    value=value\r\n)\r\nbaseline_trial.set_user_attr(\"note\", \"Baseline trial\")   # mark the trial as baseline\r\nstudy.add_trial(baseline_trial)\r\n```\r\nNote, that if you're tempted, like me, to set the distribution limits to the default value itself, like so\r\n```python\r\ndefault_distributions = {\r\n    \"theta\": optuna.distributions.FloatDistribution(0.1, 0.1)\r\n}\r\n```\r\nbecause you think it makes no sense letting the limits be loose, you will get empty parameter importance plots in the `optuna-dashboard` and the `optuna.importance.get_param_importances(study)` will show empty (because of [this line](https://github.com/optuna/optuna/blob/1925ca1a051b3bf11819478b65d830de614774b2/optuna/search_space/intersection.py#L51))."
      },
      {
        "user": "faze-geek",
        "body": "@jacobnzw Hi, I saw your workaround here.\r\nCan you look into this [issue](https://github.com/optuna/optuna/issues/5710) where I want to enqueue the best weight from one study as the baseline trial for another study."
      },
      {
        "user": "robbiemu",
        "body": "this is important, the current structure does not allow proper separation of concerns (if I have a single function to get my trials and another to run the study, the runner has to know about the internal state of the creation function ie is it a baseline)"
      }
    ]
  },
  {
    "issue_number": 5724,
    "title": "optim too slow on AWS EC2 instance",
    "author": "raamana",
    "state": "closed",
    "created_at": "2024-10-23T15:26:36Z",
    "updated_at": "2024-10-24T05:23:41Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nfirst of all, I love the library and **thank you** for open sourcing and maintaining it. \r\n\r\nIssue:\r\nI am optimizing a forecasting model from Darts with Optuna and the individual trials finish in about 2 mins on M3 Max Macbook pro ( Number of Cores: 14 (10 performance and 4 efficiency)) for a wide variety of hyper param configurations. When I run the exact same thing on an EC2 instance (type `m8g.8xlarge` with 32 vCPU and 128 GB RAM), and they never finish even when given 30 mins per trial (accounting for CPU vs GPU differences). What's going on?\r\n\r\nI am using a timeout decorator as suggested to prune long-running or hungup trials, and on EC2 instance, every single trial is getting pruned, even with a different predictive model. \r\n\r\n```python\r\n@timeout_decorator.timeout(max_run_time_per_trial,  # in seconds\r\n                           timeout_exception=optuna.TrialPruned,  # trial result will be noted as pruned\r\n                           use_signals=True)  # needed for process-based parallelization with n_jobs=1\r\ndef objective(trial):\r\n```\r\n\r\nI know this might not be optuna-specific issue but I would appreciate any constructive feedback and pointers. \r\n\r\n### Environment\r\n\r\n- Optuna version: 4.0.0 (both places)\r\n\r\n- Local Python version: 3.12.4\r\n- Local OS: macOS-14.6.1-arm64-arm-64bit\r\n\r\n- EC2 instance OS: Linux-6.8.0-1016-aws-aarch64-with-glibc2.39\r\n- EC2 Python version:3.12.3\r\n\r\n\r\n### Error messages, stack traces, or logs\r\n\r\nno errors or warnings - trials just keep running!\r\n\r\n\r\n### Steps to reproduce\r\n\r\ni am not allowed to share code or data but I can try to write down a simplest example to reproduce the issue that does not involve any corporate restrictions, but I would like your feedback on the above before I develop this example\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5171,
    "title": "NSGAIISampler AssertionError()",
    "author": "FlorinAndrei",
    "state": "closed",
    "created_at": "2023-12-28T20:56:30Z",
    "updated_at": "2024-10-23T23:05:54Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\n\nI was trying to use Optuna for feature selection for a regression model. Whether a feature is selected or not, that's a categorical variable from `[0, 1]`. Each trial, the objective function does the equivalent of:\r\n\r\n```python\r\nfeature_flags = [trial.suggest_categorical(f, [0, 1]) for f in features]\r\n```\r\n\r\nThe sampler is invoked like this:\r\n\r\n```python\r\nfs_sampler = optuna.samplers.NSGAIISampler(seed=0)\r\n```\r\n\r\nThere are dozens of features, and I want to run thousands of trials in a study.\r\n\r\nAll this works well with some of the other samplers: TPESampler, CmaEsSampler, QMCSampler. I just use `trial.suggest_int(f, 0, 1)` with these samplers.\r\n\r\nThe study is single-objective.\r\n\r\nFull code and data here:\r\n\r\nhttps://github.com/FlorinAndrei/misc/tree/master/github_issues/nsgaiisampler_assertion_error\n\n### Environment\n\n- Optuna version:3.5.0\r\n- Python version:3.11.7\r\n- OS:Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\n- (Optional) Other libraries and their versions: Ubuntu 22.04\n\n### Error messages, stack traces, or logs\n\n```shell\n[W 2023-12-28 12:30:51,722] Trial 3313 failed with parameters: {} because of the following error: AssertionError().\r\nTraceback (most recent call last):\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\r\n    value_or_values = func(trial)\r\n                      ^^^^^^^^^^^\r\n  File \"/tmp/ipykernel_7493/1947895776.py\", line 49, in <lambda>\r\n  File \"/tmp/ipykernel_7493/1947895776.py\", line 8, in fs_objective\r\n  File \"/tmp/ipykernel_7493/1947895776.py\", line 8, in <listcomp>\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 410, in suggest_categorical\r\n    return self._suggest(name, CategoricalDistribution(choices=choices))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 631, in _suggest\r\n    elif self._is_relative_param(name, distribution):\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 663, in _is_relative_param\r\n    if name not in self.relative_params:\r\n                   ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 73, in relative_params\r\n    self._relative_params = self.study.sampler.sample_relative(\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/nsgaii/_sampler.py\", line 264, in sample_relative\r\n    return self._child_generation_strategy(study, search_space, parent_population)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/nsgaii/_child_generation_strategy.py\", line 73, in __call__\r\n    child_params = perform_crossover(\r\n                   ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/nsgaii/_crossover.py\", line 102, in perform_crossover\r\n    parents = _select_parents(crossover, study, parent_population, rng, dominates)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/nsgaii/_crossover.py\", line 129, in _select_parents\r\n    parent = _select_parent(\r\n             ^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/nsgaii/_crossover.py\", line 148, in _select_parent\r\n    if dominates(candidate0, candidate1, study.directions):\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/study/_multi_objective.py\", line 79, in _dominates\r\n    assert values1 is not None\r\n           ^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n```\n```\n\n\n### Steps to reproduce\n\nJust run the Jupyter notebook. I suggest you use the MySQL backend for Optuna.\r\n\r\nhttps://github.com/FlorinAndrei/misc/tree/master/github_issues/nsgaiisampler_assertion_error\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5172,
    "title": "NSGAIIISampler multiple errors",
    "author": "FlorinAndrei",
    "state": "closed",
    "created_at": "2023-12-28T21:14:08Z",
    "updated_at": "2024-10-23T23:05:52Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\n\nI was trying to use Optuna for feature selection for a regression model. Whether a feature is selected or not, that's a categorical variable from [0, 1]. Each trial, the objective function does the equivalent of:\r\n\r\n```python\r\nfeature_flags = [trial.suggest_categorical(f, [0, 1]) for f in features]\r\n```\r\n\r\nThe sampler is invoked like this:\r\n\r\n```python\r\nfs_sampler = optuna.samplers.NSGAIIISampler(seed=0)\r\n```\r\n\r\nThere are dozens of features, and I want to run thousands of trials in a study.\r\n\r\nAll this works well with some of the other samplers: TPESampler, CmaEsSampler, QMCSampler. I just use `trial.suggest_int(f, 0, 1)` with these samplers.\r\n\r\nThe study is single-objective.\r\n\r\nFull code and data here:\r\n\r\nhttps://github.com/FlorinAndrei/misc/tree/master/github_issues/nsgaiiisampler_multiple_errors\n\n### Environment\n\n- Optuna version:3.5.0\r\n- Python version:3.11.7\r\n- OS:Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\n- (Optional) Other libraries and their versions: Ubuntu 22.04\n\n### Error messages, stack traces, or logs\n\n```shell\nIt's a lot. Let me try and copy/paste some of it - or please refer to the notebook on github.\r\n\r\n\r\n[cut a lot of divide-by-zero error lines]\r\n\r\nBest trial: 1170. Best value: 23735.1:   21%|██        | 01:02/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   21%|██▏       | 01:04/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   21%|██▏       | 01:04/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   21%|██▏       | 01:04/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   21%|██▏       | 01:04/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:05/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:05/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:06/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:06/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:06/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:06/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:06/05:00/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\n/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_elite_population_selection_strategy.py:191: RuntimeWarning: divide by zero encountered in divide\r\n  intercepts_inv = 1 / np.max(objective_matrix, axis=0)\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:06/05:00\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:06/05:00[W 2023-12-28 13:00:51,835] Trial 3634 failed with parameters: {} because of the following error: AssertionError().\r\nTraceback (most recent call last):\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\r\n    value_or_values = func(trial)\r\n                      ^^^^^^^^^^^\r\n  File \"/tmp/ipykernel_7954/1192155253.py\", line 49, in <lambda>\r\n  File \"/tmp/ipykernel_7954/1192155253.py\", line 8, in fs_objective\r\n  File \"/tmp/ipykernel_7954/1192155253.py\", line 8, in <listcomp>\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 410, in suggest_categorical\r\n    return self._suggest(name, CategoricalDistribution(choices=choices))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 631, in _suggest\r\n    elif self._is_relative_param(name, distribution):\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 663, in _is_relative_param\r\n    if name not in self.relative_params:\r\n                   ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 73, in relative_params\r\n    self._relative_params = self.study.sampler.sample_relative(\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/_nsgaiii/_sampler.py\", line 184, in sample_relative\r\n    return self._child_generation_strategy(study, search_space, parent_population)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/nsgaii/_child_generation_strategy.py\", line 73, in __call__\r\n    child_params = perform_crossover(\r\n                   ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/nsgaii/_crossover.py\", line 102, in perform_crossover\r\n    parents = _select_parents(crossover, study, parent_population, rng, dominates)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/nsgaii/_crossover.py\", line 129, in _select_parents\r\n    parent = _select_parent(\r\n             ^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/samplers/nsgaii/_crossover.py\", line 148, in _select_parent\r\n    if dominates(candidate0, candidate1, study.directions):\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/florin/.local/lib/python3.11/site-packages/optuna/study/_multi_objective.py\", line 78, in _dominates\r\n    assert values0 is not None\r\n           ^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n[W 2023-12-28 13:00:51,837] Trial 3634 failed with value None.\r\nBest trial: 1170. Best value: 23735.1:   22%|██▏       | 01:07/05:00\r\n```\n```\n\n\n### Steps to reproduce\n\nJust run the Jupyter notebook. I suggest you use the MySQL backend for Optuna.\r\n\r\nhttps://github.com/FlorinAndrei/misc/tree/master/github_issues/nsgaiiisampler_multiple_errors\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5721,
    "title": "Missing trial parameter",
    "author": "rustamzh",
    "state": "closed",
    "created_at": "2024-10-23T13:01:50Z",
    "updated_at": "2024-10-23T14:43:43Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nIn a study with ~160 trials, only 47 trials have a parameter present, in all others it is missing in the database. The parameter is essential for running the model.\r\n\r\ndefined as\r\n\r\n`d_rounds=trial.suggest_int(\"d_rounds\", 1, 10)`\r\n\r\nThere are no if statements in the code.\r\n\r\nAfter checking the parameter table, only value '2' is missing among sampled values.\r\n\r\nRight now trials are present in all 4 states with the missing parameter.\n\n### Environment\n\n- Optuna version:4.0.0\r\n- Python version:3.8.18\r\n- OS:Linux-5.15.0-113-generic-x86_64-with-glibc2.10\n\n### Error messages, stack traces, or logs\n\n```shell\nNo errors\n```\n\n\n### Steps to reproduce\n\nIt can be hard reproduce due to randomness and lack of any errors\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Hi, could you share minimal reproducible codes with us? https://stackoverflow.com/help/minimal-reproducible-example"
      },
      {
        "user": "nzw0301",
        "body": "Even though reproducing the results is difficult, we can't help you with the current issue description. Can you share the code even if it is not reproducible?"
      },
      {
        "user": "rustamzh",
        "body": "Hi,\r\n\r\nThe main difficulty in running the code is the input data, which I cannot share. \r\nHere is the HO code. The mode code from DoppelGANger directory can be found by cloning this repo: https://github.com/fjxmlzn/DoppelGANger\r\n\r\n<details>\r\n\r\n<summary>Code</summary>\r\n\r\n```python\r\n\r\nimport gc\r\nimport optuna\r\nimport os\r\nfrom sqlalchemy.pool import NullPool\r\nimport typer\r\nfrom functools import partial\r\nfrom optuna.samplers import TPESampler\r\nimport sys\r\nimport warnings\r\nimport tensorflow as tf\r\n\r\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"DoppelGANger\"))\r\n\r\nfrom gan import output\r\nsys.modules[\"output\"] = output\r\n\r\nfrom gan.doppelganger import DoppelGANger\r\nfrom gan.util import add_gen_flag, normalize_per_sample, renormalize_per_sample\r\nfrom gan.load_data import load_data\r\nfrom gan.network import DoppelGANgerGenerator, Discriminator, AttrDiscriminator\r\nimport tensorflow as tf\r\nfrom output import Output, OutputType, Normalization\r\nimport ot\r\nEPOCHS = 2500\r\n\r\n\r\nimport numpy as np\r\nfrom sklearn.cluster import KMeans\r\nfrom scipy.stats import norm\r\n\r\nclass MSWD:\r\n\r\n    def __init__(self, training_data, mswd_n_proj, *args, **kwargs) -> None:\r\n        self.training_data = training_data\r\n        self.mswd_n_proj = mswd_n_proj\r\n        self.training_data_size = len(training_data)\r\n        self._construct_projections()\r\n\r\n    def _construct_projections(self):\r\n        d = np.prod(self.training_data.shape[1:])\r\n        self.projections = ot.sliced.get_random_projections(d, self.mswd_n_proj, seed=42)\r\n\r\n    def evaluate(self, query_samples) -> float:\r\n        return {\r\n            \"NDB\": ot.max_sliced_wasserstein_distance(self.training_data.reshape(self.training_data.shape[0], -1), query_samples.reshape(query_samples.shape[0], -1), p=2, projections=self.projections)\r\n        }\r\n\r\nclass DoppelGANgerOptuna(DoppelGANger):\r\n    \r\n    def __init__(self, val_ndb_metric: NDB, trial, num_real_attribute, warmup_epochs=0, **kwargs):\r\n        self.val_ndb_metric = val_ndb_metric\r\n        self.trial = trial\r\n        self.num_real_attribute = num_real_attribute\r\n        self.warmup_epochs = warmup_epochs\r\n        super().__init__(**kwargs)\r\n    \r\n    def build(self):\r\n        try:\r\n            sys.stdout = open(os.devnull, 'w')\r\n            super().build()\r\n        finally:\r\n            sys.stdout = sys.__stdout__\r\n    \r\n    def generate_samples(self, val_examples):\r\n        real_attribute_input_noise = self.gen_attribute_input_noise(\r\n                val_examples)\r\n        addi_attribute_input_noise = self.gen_attribute_input_noise(\r\n            val_examples)\r\n        feature_input_noise = self.gen_feature_input_noise(\r\n            val_examples, self.sample_time)\r\n\r\n        feature_input_data = self.gen_feature_input_data_free(\r\n            val_examples)\r\n        data_feature, attributes, data_gen_flag, lengths = self.sample_from(\r\n            real_attribute_input_noise, addi_attribute_input_noise,\r\n            feature_input_noise, feature_input_data)\r\n                    \r\n        features, attributes = renormalize_per_sample(\r\n                data_feature, attributes, self.data_feature_outputs,\r\n                self.data_attribute_outputs, data_gen_flag,\r\n                num_real_attribute=self.num_real_attribute)\r\n    \r\n        full_timeseries_g = features[data_gen_flag.sum(axis=1) == data_gen_flag.shape[1]]\r\n        if full_timeseries_g.shape[0] < 1:\r\n            return\r\n        return full_timeseries_g\r\n    \r\n    def compute_ndb(self):\r\n        # self.discriminate_from() for disc\r\n        if self.val_ndb_metric:\r\n            train_size = self.val_ndb_metric.training_data_size\r\n\r\n            val_examples = int(train_size+0.1*train_size)\r\n            \r\n            full_timeseries_g = self.generate_samples(val_examples)\r\n            if full_timeseries_g is None:\r\n                return\r\n\r\n            full_timeseries_g_torch = full_timeseries_g[:train_size]\r\n\r\n            results = self.val_ndb_metric.evaluate(full_timeseries_g_torch.reshape(full_timeseries_g_torch.shape[0], -1))\r\n            return results[\"NDB\"]\r\n\r\n    def visualize(self, epoch_id, batch_id, global_id):\r\n        if self.val_ndb_metric:\r\n            ndb = self.compute_ndb()\r\n            if not ndb:\r\n                return\r\n            \r\n            with warnings.catch_warnings():\r\n                warnings.simplefilter(\"ignore\")\r\n                self.trial.report(ndb, step=epoch_id)\r\n            if not self.trial.should_prune() or epoch_id <= self.warmup_epochs:\r\n                return\r\n            raise optuna.TrialPruned(f\"Trial was pruned at epoch {epoch_id}.\")\r\n\r\n\r\ndef objective(trial: optuna.trial.Trial, data_dir, batch_size) -> float:\r\n    tf.reset_default_graph()\r\n    # 1, 2, 3, 4, 6, 8, 9, 12, 16, 18, 24, 32, 36, 48, 72, 96, 144 and 288\r\n    sample_len_idx = trial.suggest_int(\"sample_len_idx\", 0, 17)\r\n    sample_len = [1, 2, 3, 4, 6, 8, 9, 12, 16, 18, 24, 32, 36, 48, 72, 96, 144, 288][sample_len_idx]\r\n\r\n    params = dict(\r\n        d_rounds=trial.suggest_int(\"d_rounds\", 1, 10),\r\n        g_rounds=1,\r\n        \r\n        d_gp_coe=trial.suggest_float(\"d_gp_coe\", 1.0, 30.0),\r\n        attr_d_gp_coe=trial.suggest_float(\"attr_d_gp_coe\", 1.0, 30.0),\r\n        g_attr_d_coe=trial.suggest_float(\"g_attr_d_coe\", 0.1, 10.0),\r\n        \r\n    )\r\n\r\n    (data_feature, data_attribute,\r\n    data_gen_flag,\r\n    data_feature_outputs, data_attribute_outputs) = \\\r\n        load_data(data_dir)\r\n    \r\n    num_real_attribute = len(data_attribute_outputs)\r\n\r\n    data_feature = data_feature[:2*batch_size]\r\n    data_attribute = data_attribute[:2*batch_size]\r\n    data_gen_flag = data_gen_flag[:2*batch_size]\r\n    \r\n    (val_data_feature, _,\r\n    _,_,_) = \\\r\n        load_data(data_dir, flag=\"val\")\r\n        \r\n    ndb = MSWD(training_data=val_data_feature.reshape(len(val_data_feature), -1), mswd_n_proj=100, number_of_bins=100, whitening=True)\r\n\r\n    (data_feature, data_attribute, data_attribute_outputs,\r\n    real_attribute_mask) = \\\r\n        normalize_per_sample(\r\n            data_feature, data_attribute, data_feature_outputs,\r\n            data_attribute_outputs)\r\n    \r\n    data_feature, data_feature_outputs = add_gen_flag(\r\n        data_feature, data_gen_flag, data_feature_outputs, sample_len)\r\n    \r\n    output_dir = f\"dg_optim/DG_{trial.number}\"\r\n    \r\n    generator = DoppelGANgerGenerator(\r\n        feed_back=False,\r\n        noise=True,\r\n        feature_outputs=data_feature_outputs,\r\n        attribute_outputs=data_attribute_outputs,\r\n        real_attribute_mask=real_attribute_mask,\r\n        feature_num_layers=params[\"generator_feature_num_layers\"],\r\n        sample_len=sample_len)\r\n    discriminator = Discriminator()\r\n    attr_discriminator = AttrDiscriminator()\r\n\r\n    checkpoint_dir = \"./\"+output_dir+\"/checkpoint\"\r\n    if not os.path.exists(checkpoint_dir):\r\n        os.makedirs(checkpoint_dir)\r\n    sample_dir = \"./\"+output_dir+\"/sample\"\r\n    if not os.path.exists(sample_dir):\r\n        os.makedirs(sample_dir)\r\n    time_path = \"./\"+output_dir+\"/time.txt\"\r\n    epoch = EPOCHS\r\n    vis_freq = 2\r\n    d_rounds = params[\"d_rounds\"]\r\n    g_rounds = params[\"g_rounds\"]\r\n    d_gp_coe = params[\"d_gp_coe\"]\r\n    attr_d_gp_coe = params[\"attr_d_gp_coe\"]\r\n    g_attr_d_coe = params[\"g_attr_d_coe\"]\r\n    extra_checkpoint_freq = 999999\r\n    num_packing = 1\r\n    \r\n    # OOM handling adapted from https://github.com/facebookresearch/fairseq/blob/50a671f78d0c8de0392f924180db72ac9b41b801/fairseq/trainer.py#L283\r\n    try:\r\n        run_config = tf.ConfigProto()\r\n        with tf.Session(config=run_config) as sess:\r\n            gan = DoppelGANgerOptuna(\r\n                val_ndb_metric=ndb,\r\n                trial=trial,\r\n                num_real_attribute = num_real_attribute,\r\n                sess=sess,\r\n                checkpoint_dir=checkpoint_dir,\r\n                sample_dir=sample_dir,\r\n                time_path=time_path,\r\n                epoch=epoch,\r\n                batch_size=batch_size,\r\n                data_feature=data_feature,\r\n                data_attribute=data_attribute,\r\n                real_attribute_mask=real_attribute_mask,\r\n                data_gen_flag=data_gen_flag,\r\n                sample_len=sample_len,\r\n                data_feature_outputs=data_feature_outputs,\r\n                data_attribute_outputs=data_attribute_outputs,\r\n                vis_freq=vis_freq,\r\n                vis_num_sample=1000, # Not used\r\n                generator=generator,\r\n                discriminator=discriminator,\r\n                attr_discriminator=attr_discriminator,\r\n                d_gp_coe=d_gp_coe,\r\n                attr_d_gp_coe=attr_d_gp_coe,\r\n                g_attr_d_coe=g_attr_d_coe,\r\n                d_rounds=d_rounds,\r\n                g_rounds=g_rounds,\r\n                num_packing=num_packing,\r\n                extra_checkpoint_freq=extra_checkpoint_freq)\r\n            gan.build()\r\n            gan.train()\r\n            return gan.compute_ndb()\r\n    except RuntimeError as e:\r\n        if 'out of memory' in str(e) or 'share variable' in str(e):\r\n            if 'out of memory' in str(e):\r\n                print('| WARNING: ran out of memory, Fail trial')\r\n            elif 'share variable' in str(e):\r\n                print('| WARNING: problem with variable sharing, Fail trial')        \r\n            del gan\r\n            gc.collect()\r\n            raise optuna.TrialPruned()\r\n        else:\r\n            raise e\r\n\r\n\r\ndef main(study_name:str, data_dir:str, batch_size:int=1000, n_iter:int=300):\r\n    pruner = optuna.pruners.HyperbandPruner(\r\n        min_resource=200, max_resource=EPOCHS, reduction_factor=2\r\n    )\r\n    storage = optuna.storages.RDBStorage(\r\n        url=os.environ[\"OPTUNA_URL\"],\r\n        engine_kwargs={\"poolclass\": NullPool}, heartbeat_interval=60, grace_period=120,\r\n    )\r\n\r\n    study = optuna.create_study(\r\n        load_if_exists=True,\r\n        study_name=study_name,\r\n        storage=storage,\r\n        direction=\"minimize\",\r\n        pruner=pruner,\r\n        sampler=TPESampler(multivariate=True, constant_liar=True)\r\n    )\r\n    bound_objective = partial(objective, data_dir=data_dir,batch_size=batch_size)\r\n    study.optimize(\r\n            bound_objective,\r\n            n_trials=n_iter,\r\n            callbacks=[\r\n                optuna.study.MaxTrialsCallback(\r\n                    n_iter,\r\n                    states=(optuna.trial.TrialState.COMPLETE,),\r\n                )\r\n            ],\r\n        )\r\n\r\n\r\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\r\n\r\n    print(\"Best trial:\")\r\n    trial = study.best_trial\r\n\r\n    print(\"  Value: {}\".format(trial.value))\r\n\r\n    print(\"  Params: \")\r\n    for key, value in trial.params.items():\r\n        print(\"    {}: {}\".format(key, value))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    app = typer.Typer(add_completion=False, pretty_exceptions_show_locals = False)\r\n    app.command()(main)\r\n    app()\r\n\r\n```\r\n\r\n</details>"
      }
    ]
  },
  {
    "issue_number": 5717,
    "title": "Optuna Multinode Multi GPU training using transfromers trainer",
    "author": "rajeshitshoulders",
    "state": "closed",
    "created_at": "2024-10-22T00:28:35Z",
    "updated_at": "2024-10-23T13:18:50Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nHi, we are trying fine-tune training using transformers_hyperparameter_search for opt-350m model  to find the best trail for Learning Rate and Batch size. we are able to run the fine-tune trainin with Ray Hyperparameter tunning, but we wanted to test it with just troch frame work, hence we converted and tested code works well with single node with 8GPU. we converted the same code to run 15xGPU using 2 nodes with 8 gpu's each using ```accelerate launch``` in the pytorch:23.12-py3\r\n\r\nwe are getting below error \r\nRuntimeError: Detected mismatch between collectives on ranks. Rank 3 is running collective: CollectiveFingerPrint(SequenceNumber=0, OpType=BROADC\r\nAST, TensorShape=[3150], TensorDtypes=Long, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), require\r\ns_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))), but Rank 8 is running collective: CollectiveFingerPrint(Sequenc\r\neNumber=0, OpType=BROADCAST, TensorShape=[3154], TensorDtypes=Long, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=St\r\nrided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))).Collectives differ in the following aspe\r\ncts:   Tensor Tensor shapes: 3150vs 3154\r\n\r\nWe tried same code using Slurm with the example https://github.com/huggingface/accelerate/blob/main/examples/slurm/submit_multinode.sh, we are getting below error\r\n\r\n    work = default_pg.broadcast([tensor], opts)\r\ntorch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1727, internal error - ple\r\nase report this issue to the NCCL developers, NCCL version 2.19.3\r\nncclInternalError: Internal check failed.\r\nLast error:\r\n\r\nAfter lot of research, we are in doubt, Optuna Hyperparameter search does work with transformers library.\r\n\r\nwe would like to know Optuna can be used with accelerate + transforms + slurm\r\n\n\n### Environment\n\n\r\n- Optuna version: 4.0.0\r\n- Python version: 3.10.12\r\n- OS: Ubuntu / Docker Container nvcr.io/nvidia/pytorch:23.12-py3\r\n- (Optional) Other libraries and their versions: transformers==4.36.2 accelerate==0.27.2 deepspeed==0.14.0\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\nwork = default_pg.broadcast([tensor], opts)\r\ntorch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1727, internal error - ple\r\nase report this issue to the NCCL developers, NCCL version 2.19.3\r\nncclInternalError: Internal check failed.\r\nLast error:\n```\n\n\n### Steps to reproduce\n\n1.\r\n2.\r\n3.\r\n```python\r\n# python code\r\n```\r\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "rajeshitshoulders",
        "body": "transformers hp code block\r\n\r\ndef model_init(trial):\r\n    gc.collect()\r\n    torch.cuda.empty_cache()\r\n    #torch.cuda.set_device(int(os.environ['LOCAL_RANK']))\r\n    #dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)\r\n    return prepare_model()\r\n\r\ntraining_args = transformers.TrainingArguments(\r\n        args.output_dir,\r\n        logging_steps=1,\r\n        save_strategy=save_strategy,\r\n        save_steps=(steps_per_epoch * args.epochs) if args.save_model else float('inf'),\r\n        #max_steps=steps_per_epoch * args.epochs,\r\n        max_steps=10,\r\n        #learning_rate=args.learning_rate,\r\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\r\n        weight_decay=0.01,\r\n        warmup_steps=0,\r\n        label_names=[\"input_ids\", \"attention_mask\"],\r\n        push_to_hub=False,\r\n        report_to=\"none\",\r\n        disable_tqdm=True,\r\n        gradient_checkpointing=True,\r\n        #per_device_train_batch_size=args.batch_size,\r\n        eval_accumulation_steps=4,\r\n)\r\n\r\n\r\ntrainer = transformers.Trainer(\r\n        model=None,\r\n        model_init=model_init,\r\n        args=training_args,\r\n        train_dataset=tokenized_train_dataset,\r\n        eval_dataset=tokenized_val_dataset if args.do_eval else None,\r\n        #compute_metrics=compute_metrics if args.do_eval else None,\r\n        tokenizer=tokenizer,\r\n        data_collator=data_collator,\r\n        preprocess_logits_for_metrics=prep,\r\n)\r\n\r\n\r\n\r\ndef hp_space(trial):\r\n    # Define hyperparameters to be tuned\r\n    return {\r\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.000001, 0.001, log=True),\r\n            #\"batch_size\": trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64]),\r\n            #\"batch_size\": trial.suggest_int(\"batch_size\", 8, 128),\r\n            \"per_device_train_batch_size\": trial.suggest_int(\"per_device_train_batch_size\", 32, 256),\r\n            #\"max_steps\": trial.suggest_int(\"max_steps\", 2*10, 10*10),\r\n        }\r\n\r\ndef run_hp_search_optuna(trainer, n_trials, direction, **kwargs):\r\n    import optuna\r\n\r\n    def _objective(trial):\r\n        trainer.objective = None\r\n        trainer.train(trial=trial)\r\n\r\n        # memory management\r\n        del trainer.model\r\n        gc.collect()\r\n        torch.cuda.empty_cache()\r\n\r\n        # Evaluate if needed\r\n        if getattr(trainer, \"objective\", None) is None:\r\n            metrics = trainer.evaluate()\r\n            trainer.objective = trainer.compute_objective(metrics)\r\n\r\n        return trainer.objective\r\n\r\n\r\ntrainer.run_hp_search_optuna = run_hp_search_optuna\r\n#best_trials = trainer.hyperparameter_search(backend='optuna',direction=\"maximize\", storage='sqlite:///optuna-opt-study.db', hp_space=hp_space, n_trials=5)\r\n#best_trials = trainer.hyperparameter_search(\r\nbest_trails = trainer.hyperparameter_search(\r\n    direction=\"minimize\",\r\n    storage='sqlite:///optuna-opt-study.db',\r\n    backend=\"optuna\",\r\n    hp_space=hp_space,\r\n    n_trials=20,\r\n#    compute_objective=optuna_hp_space,\r\n)\r\n\r\n\r\nRan code:\r\naccelerate launch --config_file ./configs/deepspeed_2node_client.yaml opt/llm_sft_optuna2.py  --model_name facebook/opt-350m --output_dir o_t --epochs ${EPOCH} --batch_size 4 --save_model False --dataset_path yizhongw/self_instruct   --do_eval True --train_size 1500 --max_length 1500"
      }
    ]
  },
  {
    "issue_number": 5531,
    "title": "show trend in plot_optimization_history",
    "author": "agdiiura",
    "state": "open",
    "created_at": "2024-06-27T09:38:32Z",
    "updated_at": "2024-10-17T02:35:04Z",
    "labels": [
      "feature",
      "needs-discussion"
    ],
    "body": "### Motivation\r\n\r\nIn `plot_optimization_history`, showing some indicators of the optimization trend might be useful to understand if convergence occurs.\r\n\r\n### Description\r\n\r\nA possible solution is to show a smooth indicator of the optimization value, such as the rolling median or the rolling mean of the last N trials. This is already done in other tools, such as Tensorboard.\r\n\r\nHere is a simple example.\r\n<img width=\"610\" alt=\"CatturaOptuna\" src=\"https://github.com/optuna/optuna/assets/16191679/69c76ba2-99ff-47a4-9836-1f156254becd\">\r\n\r\n\r\nThe following snippet can be included in `_get_optimization_history_plot`  to obtain the output\r\n\r\n```python\r\n\r\n    import pandas as pd\r\n\r\n   [...]\r\n\r\n    values = pd.DataFrame(feasible_trial_values, index=feasible_trial_numbers)\r\n    rm = values.rolling(rolling_window, min_periods=1).median().values.ravel()\r\n   \r\n    traces.append(\r\n        go.Scatter(\r\n            x=feasible_trial_numbers,\r\n            y=rm,\r\n            mode=\"lines\",\r\n            name=\"Rolling Median\",\r\n        )\r\n    )\r\n    \r\n```\r\n\r\nhere I used `pandas` (optional dependency) because the rolling median is already implemented. \r\nThe number of records `rolling_window` can be an optional parameter or a fixed value, e.g. `sqrt(len(feasible_trial_numbers))`.\r\n\r\nTo show a smooth curve the `sagvol_filter` function implemented in `scipy` can be used, see this SO question https://stackoverflow.com/questions/20618804/how-to-smooth-a-curve-for-a-dataset\r\n\r\n\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "dominik-pichler",
        "body": "@agdiiura Hello :) I can take a look at this. "
      },
      {
        "user": "y0z",
        "body": "Thank you for your proposal.\r\n\r\nI'd like to point out that introducing this feature into Optuna's plot_optimization_history requires discussion.\r\nThere are several approaches to calculating the trend, and it is not obvious which one to adopt.\r\nIn addition, introducing a new argument to the function to turn the trend display on and off would lead to API complexity.\r\nTherefore, this change would take time.\r\n\r\nSo, I think it would be better for you to introduce the implementation of your proposal to [OptunaHub](https://hub.optuna.org/).\r\nOptunaHub is a platform for sharing third-party features for Optuna.\r\n- [Announcing OptunaHub 0.1.0-β](https://medium.com/optuna/announcing-optunahub-0-1-0-%CE%B2-69b35bb3e95e)\r\n- [How to Implement Your Own Visualization Function](https://optuna.github.io/optunahub-registry/recipes/004_visualization.html)\r\n- [How to Register Your Algorithm with OptunaHub](https://optuna.github.io/optunahub-registry/recipes/002_registration.html)\r\n\r\nIn OptunaHub, you can easily publish your visualization function, and users can use it immediately.\r\nWhat do you think?"
      },
      {
        "user": "agdiiura",
        "body": "Hi, I see your point. \r\nI will look at OptunaHub (it seems cool btw ) and I'll try to push something in the next few days"
      }
    ]
  },
  {
    "issue_number": 3050,
    "title": "Increasing memory during study leading to OOM",
    "author": "jmakov",
    "state": "closed",
    "created_at": "2021-10-27T23:16:00Z",
    "updated_at": "2024-10-16T20:08:04Z",
    "labels": [
      "question"
    ],
    "body": "Hi. First, thanks for this project! A couple of observations and I'm not sure if it's a memory leak or it's just how Optuna works:\r\n\r\n1. study runs, OOM: I'm distributing optuna workers with ray (code below) and am observing increasing memory consumption of Optuna workers while the study runs (increases ~400MB for 20k trials per Optuna process). This becomes problematic and nodes are crashing with OOM. The objective function run takes about 500MB RAM (and this is constant) and for some reason the memory amount (of the optimization process) increases (also with calling gc.collect, so we assume it's connected with Optuna). Is this as intended or is there a memory leak?\r\n2. study resumes, OOM: Studies are created with `load_if_exists=True`. If we run a study from beginning (all previous DBs erased)  to e.g. 100k trials, the memory usage is e.g. 3/4 of node's available RAM. But if I interrupt the optimization processes on the node (ctrl+c, processes end) and start again (as said, with `load_if_exists=True`), then the memory increases til OOM. So I can only resume from e.g. 50k trials but not from e.g. 100k (although, as said, the study ran w/o problems till e.g. 100k trials). \r\n3. study resumes, OOM earlier with specific samplers: For samplers like `CmaEsSampler(use_separable_cma=True)` we get OOM already with e.g. 50k trials which leaves us to believe that some samplers need more memory to resume. Is this behavior as intended?\r\n4. high network traffic: With the default sampler we observe 50-90Mbps of network traffic between PostgreSQL and the node. With `CmaEsSampler(use_separable_cma=True)` about 120Mbps (constant traffic). Our objective/tunable function takes about 1s to compute. Is such high network traffic expected?\r\n\r\n## Information for reproducibility (optional)\r\n### Environment\r\n- Optuna version: 2.10.0\r\n- Python version: 3.7.10\r\n- OS: Ubuntu 21.10 (Linux-5.13.0-20-generic-x86_64-with-debian-11.0)\r\n- (Optional) Other libraries and their versions:\r\n```yaml\r\nname: puma-lab\r\nchannels:\r\n  - pyviz\r\n  - conda-forge\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=conda_forge\r\n  - _openmp_mutex=4.5=1_gnu\r\n  - abseil-cpp=20210324.2=h9c3ff4c_0\r\n  - alembic=1.7.3=pyhd8ed1ab_0\r\n  - alsa-lib=1.2.3=h516909a_0\r\n  - anyio=3.3.0=py37h89c1867_0\r\n  - argcomplete=1.12.3=pyhd8ed1ab_2\r\n  - argon2-cffi=20.1.0=py37h5e8e339_2\r\n  - arrow-cpp=5.0.0=py37hdf48254_5_cpu\r\n  - async_generator=1.10=py_0\r\n  - attrs=21.2.0=pyhd8ed1ab_0\r\n  - autopage=0.4.0=pyhd8ed1ab_0\r\n  - aws-c-cal=0.5.11=h95a6274_0\r\n  - aws-c-common=0.6.2=h7f98852_0\r\n  - aws-c-event-stream=0.2.7=h3541f99_13\r\n  - aws-c-io=0.10.5=hfb6a706_0\r\n  - aws-checksums=0.1.11=ha31a3da_7\r\n  - aws-sdk-cpp=1.8.186=hb4091e7_3\r\n  - babel=2.9.1=pyh44b312d_0\r\n  - backcall=0.2.0=pyh9f0ad1d_0\r\n  - backports=1.0=py_2\r\n  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0\r\n  - backports.zoneinfo=0.2.1=py37h5e8e339_4\r\n  - bleach=4.1.0=pyhd8ed1ab_0\r\n  - bokeh=2.3.3=py37h89c1867_0\r\n  - brotlipy=0.7.0=py37h5e8e339_1001\r\n  - bzip2=1.0.8=h7f98852_4\r\n  - c-ares=1.17.2=h7f98852_0\r\n  - ca-certificates=2021.10.8=ha878542_0\r\n  - certifi=2021.10.8=py37h89c1867_0\r\n  - cffi=1.14.6=py37hc58025e_0\r\n  - chardet=4.0.0=py37h89c1867_1\r\n  - charset-normalizer=2.0.0=pyhd8ed1ab_0\r\n  - click=8.0.1=py37h89c1867_0\r\n  - clickhouse-cityhash=1.0.2.3=py37h3340039_2\r\n  - clickhouse-driver=0.2.1=py37h5e8e339_0\r\n  - cliff=3.9.0=pyhd8ed1ab_0\r\n  - cloudpickle=2.0.0=pyhd8ed1ab_0\r\n  - cmaes=0.8.2=pyh44b312d_0\r\n  - cmd2=2.2.0=py37h89c1867_0\r\n  - colorama=0.4.4=pyh9f0ad1d_0\r\n  - colorcet=2.0.6=pyhd8ed1ab_0\r\n  - colorlog=6.4.1=py37h89c1867_0\r\n  - conda=4.10.3=py37h89c1867_1\r\n  - conda-package-handling=1.7.3=py37h5e8e339_0\r\n  - cramjam=2.3.1=py37h5e8e339_1\r\n  - cryptography=3.4.7=py37h5d9358c_0\r\n  - cycler=0.10.0=py_2\r\n  - cytoolz=0.11.0=py37h5e8e339_3\r\n  - dask=2021.9.0=pyhd8ed1ab_0\r\n  - dask-core=2021.9.0=pyhd8ed1ab_0\r\n  - datashader=0.13.0=pyh6c4a22f_0\r\n  - datashape=0.5.4=py_1\r\n  - dbus=1.13.6=h48d8840_2\r\n  - debugpy=1.4.1=py37hcd2ae1e_0\r\n  - decorator=5.1.0=pyhd8ed1ab_0\r\n  - defusedxml=0.7.1=pyhd8ed1ab_0\r\n  - distributed=2021.9.0=py37h89c1867_0\r\n  - entrypoints=0.3=py37hc8dfbb8_1002\r\n  - expat=2.4.1=h9c3ff4c_0\r\n  - fastparquet=0.7.1=py37hb1e94ed_0\r\n  - filelock=3.0.12=pyh9f0ad1d_0\r\n  - fontconfig=2.13.1=hba837de_1005\r\n  - freetype=2.10.4=h0708190_1\r\n  - fsspec=2021.8.1=pyhd8ed1ab_0\r\n  - gettext=0.19.8.1=h0b5b191_1005\r\n  - gflags=2.2.2=he1b5a44_1004\r\n  - gitdb=4.0.7=pyhd8ed1ab_0\r\n  - gitpython=3.1.23=pyhd8ed1ab_1\r\n  - glib=2.68.4=h9c3ff4c_0\r\n  - glib-tools=2.68.4=h9c3ff4c_0\r\n  - glog=0.5.0=h48cff8f_0\r\n  - greenlet=1.1.1=py37hcd2ae1e_0\r\n  - grpc-cpp=1.40.0=h850795e_0\r\n  - gst-plugins-base=1.18.5=hf529b03_0\r\n  - gstreamer=1.18.5=h76c114f_0\r\n  - heapdict=1.0.1=py_0\r\n  - holoviews=1.14.5=py_0\r\n  - hvplot=0.7.3=py_0\r\n  - icu=68.1=h58526e2_0\r\n  - idna=3.1=pyhd3deb0d_0\r\n  - importlib-metadata=4.8.1=py37h89c1867_0\r\n  - importlib_metadata=4.8.1=hd8ed1ab_0\r\n  - importlib_resources=5.2.2=pyhd8ed1ab_0\r\n  - ipykernel=6.4.1=py37h6531663_0\r\n  - ipympl=0.7.0=pyhd8ed1ab_0\r\n  - ipython=7.27.0=py37h6531663_0\r\n  - ipython_genutils=0.2.0=py_1\r\n  - ipywidgets=7.6.5=pyhd8ed1ab_0\r\n  - jbig=2.1=h7f98852_2003\r\n  - jedi=0.18.0=py37h89c1867_2\r\n  - jinja2=3.0.1=pyhd8ed1ab_0\r\n  - joblib=1.0.1=pyhd8ed1ab_0\r\n  - jpeg=9d=h36c2ea0_0\r\n  - json5=0.9.5=pyh9f0ad1d_0\r\n  - jsonschema=3.2.0=py37hc8dfbb8_1\r\n  - jupyter-server-mathjax=0.2.3=pyhd8ed1ab_0\r\n  - jupyter_client=7.0.2=pyhd8ed1ab_0\r\n  - jupyter_contrib_core=0.3.3=py_2\r\n  - jupyter_contrib_nbextensions=0.5.1=py37hc8dfbb8_1\r\n  - jupyter_core=4.7.1=py37h89c1867_0\r\n  - jupyter_highlight_selected_word=0.2.0=py37h89c1867_1002\r\n  - jupyter_latex_envs=1.4.6=py37h89c1867_1001\r\n  - jupyter_nbextensions_configurator=0.4.1=py37h89c1867_2\r\n  - jupyter_server=1.11.0=pyhd8ed1ab_0\r\n  - jupyterlab=3.1.11=pyhd8ed1ab_0\r\n  - jupyterlab-git=0.32.2=pyhd8ed1ab_0\r\n  - jupyterlab_pygments=0.1.2=pyh9f0ad1d_0\r\n  - jupyterlab_server=2.8.1=pyhd8ed1ab_0\r\n  - jupyterlab_widgets=1.0.2=pyhd8ed1ab_0\r\n  - kiwisolver=1.3.2=py37h2527ec5_0\r\n  - krb5=1.19.2=hcc1bbae_0\r\n  - lcms2=2.12=hddcbb42_0\r\n  - ld_impl_linux-64=2.36.1=hea4e1c9_2\r\n  - lerc=2.2.1=h9c3ff4c_0\r\n  - libarchive=3.5.2=hccf745f_0\r\n  - libblas=3.9.0=11_linux64_openblas\r\n  - libbrotlicommon=1.0.9=h7f98852_5\r\n  - libbrotlidec=1.0.9=h7f98852_5\r\n  - libbrotlienc=1.0.9=h7f98852_5\r\n  - libcblas=3.9.0=11_linux64_openblas\r\n  - libclang=11.1.0=default_ha53f305_1\r\n  - libcurl=7.78.0=h2574ce0_0\r\n  - libdeflate=1.7=h7f98852_5\r\n  - libedit=3.1.20191231=he28a2e2_2\r\n  - libev=4.33=h516909a_1\r\n  - libevent=2.1.10=hcdb4288_3\r\n  - libffi=3.3=h58526e2_2\r\n  - libgcc-ng=11.1.0=hc902ee8_8\r\n  - libgfortran-ng=11.1.0=h69a702a_8\r\n  - libgfortran5=11.1.0=h6c583b3_8\r\n  - libglib=2.68.4=h3e27bee_0\r\n  - libgomp=11.1.0=hc902ee8_8\r\n  - libiconv=1.16=h516909a_0\r\n  - liblapack=3.9.0=11_linux64_openblas\r\n  - libllvm11=11.1.0=hf817b99_2\r\n  - libnghttp2=1.43.0=h812cca2_0\r\n  - libogg=1.3.4=h7f98852_1\r\n  - libopenblas=0.3.17=pthreads_h8fe5266_1\r\n  - libopus=1.3.1=h7f98852_1\r\n  - libpng=1.6.37=h21135ba_2\r\n  - libpq=13.3=hd57d9b9_0\r\n  - libprotobuf=3.16.0=h780b84a_0\r\n  - libsodium=1.0.18=h36c2ea0_1\r\n  - libsolv=0.7.19=h780b84a_5\r\n  - libssh2=1.10.0=ha56f1ee_0\r\n  - libstdcxx-ng=11.1.0=h56837e0_8\r\n  - libta-lib=0.4.0=h516909a_0\r\n  - libthrift=0.14.2=he6d91bd_1\r\n  - libtiff=4.3.0=hf544144_1\r\n  - libutf8proc=2.6.1=h7f98852_0\r\n  - libuuid=2.32.1=h7f98852_1000\r\n  - libuv=1.42.0=h7f98852_0\r\n  - libvorbis=1.3.7=h9c3ff4c_0\r\n  - libwebp-base=1.2.1=h7f98852_0\r\n  - libxcb=1.13=h7f98852_1003\r\n  - libxkbcommon=1.0.3=he3ba5ed_0\r\n  - libxml2=2.9.12=h72842e0_0\r\n  - libxslt=1.1.33=h15afd5d_2\r\n  - llvmlite=0.37.0=py37h9d7f4d0_0\r\n  - locket=0.2.0=py_2\r\n  - lxml=4.6.3=py37h77fd288_0\r\n  - lz4-c=1.9.3=h9c3ff4c_1\r\n  - lzo=2.10=h516909a_1000\r\n  - mako=1.1.5=pyhd8ed1ab_0\r\n  - mamba=0.15.3=py37h7f483ca_0\r\n  - markdown=3.3.4=pyhd8ed1ab_0\r\n  - markupsafe=2.0.1=py37h5e8e339_0\r\n  - matplotlib=3.4.3=py37h89c1867_0\r\n  - matplotlib-base=3.4.3=py37h1058ff1_0\r\n  - matplotlib-inline=0.1.3=pyhd8ed1ab_0\r\n  - mistune=0.8.4=py37h5e8e339_1004\r\n  - msgpack-python=1.0.2=py37h2527ec5_1\r\n  - multipledispatch=0.6.0=py_0\r\n  - mysql-common=8.0.25=ha770c72_2\r\n  - mysql-libs=8.0.25=hfa10184_2\r\n  - nb_conda_kernels=2.3.1=py37h89c1867_0\r\n  - nbclassic=0.3.1=pyhd8ed1ab_1\r\n  - nbclient=0.5.4=pyhd8ed1ab_0\r\n  - nbconvert=6.1.0=py37h89c1867_0\r\n  - nbdime=3.1.0=pyhd8ed1ab_0\r\n  - nbformat=5.1.3=pyhd8ed1ab_0\r\n  - ncurses=6.2=h58526e2_4\r\n  - nest-asyncio=1.5.1=pyhd8ed1ab_0\r\n  - notebook=6.4.3=pyha770c72_0\r\n  - nspr=4.30=h9c3ff4c_0\r\n  - nss=3.69=hb5efdd6_0\r\n  - numba=0.54.0=py37h2d894fd_0\r\n  - numpy=1.20.3=py37h038b26d_1\r\n  - olefile=0.46=pyh9f0ad1d_1\r\n  - openjpeg=2.4.0=hb52868f_1\r\n  - openssl=1.1.1l=h7f98852_0\r\n  - optuna=2.10.0=pyhd8ed1ab_0\r\n  - orc=1.6.10=h58a87f1_0\r\n  - packaging=21.0=pyhd8ed1ab_0\r\n  - pandas=1.3.2=py37he8f5f7f_0\r\n  - pandoc=2.14.2=h7f98852_0\r\n  - pandocfilters=1.4.2=py_1\r\n  - panel=0.12.1=py_0\r\n  - param=1.11.1=pyh6c4a22f_0\r\n  - parquet-cpp=1.5.1=1\r\n  - parso=0.8.2=pyhd8ed1ab_0\r\n  - partd=1.2.0=pyhd8ed1ab_0\r\n  - patsy=0.5.2=pyhd8ed1ab_0\r\n  - pbr=5.6.0=pyhd8ed1ab_0\r\n  - pcre=8.45=h9c3ff4c_0\r\n  - pexpect=4.8.0=py37hc8dfbb8_1\r\n  - pickle5=0.0.11=py37h5e8e339_0\r\n  - pickleshare=0.7.5=py37hc8dfbb8_1002\r\n  - pillow=8.3.2=py37h0f21c89_0\r\n  - pip=21.2.4=pyhd8ed1ab_0\r\n  - prettytable=2.2.0=pyhd8ed1ab_0\r\n  - prometheus_client=0.11.0=pyhd8ed1ab_0\r\n  - prompt-toolkit=3.0.20=pyha770c72_0\r\n  - psutil=5.8.0=py37h5e8e339_1\r\n  - pthread-stubs=0.4=h36c2ea0_1001\r\n  - ptyprocess=0.7.0=pyhd3deb0d_0\r\n  - pyarrow=5.0.0=py37h58331f5_5_cpu\r\n  - pycosat=0.6.3=py37h5e8e339_1006\r\n  - pycparser=2.20=pyh9f0ad1d_2\r\n  - pyct=0.4.6=py_0\r\n  - pyct-core=0.4.6=py_0\r\n  - pygments=2.10.0=pyhd8ed1ab_0\r\n  - pykalman=0.9.5=py_1\r\n  - pyopenssl=20.0.1=pyhd8ed1ab_0\r\n  - pyparsing=2.4.7=pyh9f0ad1d_0\r\n  - pyperclip=1.8.2=pyhd8ed1ab_2\r\n  - pyqt=5.12.3=py37h89c1867_7\r\n  - pyqt-impl=5.12.3=py37he336c9b_7\r\n  - pyqt5-sip=4.19.18=py37hcd2ae1e_7\r\n  - pyqtchart=5.12=py37he336c9b_7\r\n  - pyqtwebengine=5.12.1=py37he336c9b_7\r\n  - pyrsistent=0.17.3=py37h5e8e339_2\r\n  - pysocks=1.7.1=py37h89c1867_3\r\n  - python=3.7.10=hffdb5ce_100_cpython\r\n  - python-dateutil=2.8.2=pyhd8ed1ab_0\r\n  - python_abi=3.7=2_cp37m\r\n  - pytz=2021.1=pyhd8ed1ab_0\r\n  - pyviz_comms=2.1.0=py_0\r\n  - pyyaml=5.4.1=py37h5e8e339_1\r\n  - pyzmq=22.2.1=py37h336d617_0\r\n  - qt=5.12.9=hda022c4_4\r\n  - re2=2021.09.01=h9c3ff4c_0\r\n  - readline=8.1=h46c0cb4_0\r\n  - redis-py=3.5.3=pyh9f0ad1d_0\r\n  - reproc=14.2.3=h7f98852_0\r\n  - reproc-cpp=14.2.3=h9c3ff4c_0\r\n  - requests=2.26.0=pyhd8ed1ab_0\r\n  - requests-unixsocket=0.2.0=py_0\r\n  - ruamel_yaml=0.15.80=py37h5e8e339_1004\r\n  - s2n=1.0.10=h9b69904_0\r\n  - scikit-learn=0.24.2=py37hf0f1638_1\r\n  - send2trash=1.8.0=pyhd8ed1ab_0\r\n  - setproctitle=1.1.10=py37h5e8e339_1004\r\n  - setuptools=58.0.4=py37h89c1867_0\r\n  - six=1.16.0=pyh6c4a22f_0\r\n  - smmap=3.0.5=pyh44b312d_0\r\n  - snappy=1.1.8=he1b5a44_3\r\n  - sniffio=1.2.0=py37h89c1867_1\r\n  - sortedcontainers=2.4.0=pyhd8ed1ab_0\r\n  - sqlalchemy=1.4.25=py37h5e8e339_0\r\n  - sqlite=3.36.0=h9cd32fc_1\r\n  - statsmodels=0.12.2=py37hb1e94ed_0\r\n  - stevedore=3.4.0=py37h89c1867_0\r\n  - ta-lib=0.4.19=py37ha21ca33_2\r\n  - tabulate=0.8.9=pyhd8ed1ab_0\r\n  - tblib=1.7.0=pyhd8ed1ab_0\r\n  - tensorboardx=2.4=pyhd8ed1ab_0\r\n  - terminado=0.12.1=py37h89c1867_0\r\n  - testpath=0.5.0=pyhd8ed1ab_0\r\n  - threadpoolctl=2.2.0=pyh8a188c0_0\r\n  - thrift=0.13.0=py37hcd2ae1e_2\r\n  - tk=8.6.11=h27826a3_1\r\n  - toolz=0.11.1=py_0\r\n  - tornado=6.1=py37h5e8e339_1\r\n  - tqdm=4.62.2=pyhd8ed1ab_0\r\n  - traitlets=5.1.0=pyhd8ed1ab_0\r\n  - typing_extensions=3.10.0.0=pyha770c72_0\r\n  - tzdata=2021a=he74cb21_1\r\n  - tzlocal=3.0=py37h89c1867_2\r\n  - urllib3=1.26.6=pyhd8ed1ab_0\r\n  - wcwidth=0.2.5=pyh9f0ad1d_2\r\n  - webencodings=0.5.1=py_1\r\n  - websocket-client=0.57.0=py37h89c1867_4\r\n  - wheel=0.37.0=pyhd8ed1ab_1\r\n  - widgetsnbextension=3.5.1=py37h89c1867_4\r\n  - xarray=0.19.0=pyhd8ed1ab_1\r\n  - xeus=2.0.0=h7d0c39e_0\r\n  - xeus-python=0.13.0=py37h4b46df4_1\r\n  - xeus-python-shell=0.1.5=pyhd8ed1ab_0\r\n  - xorg-libxau=1.0.9=h7f98852_0\r\n  - xorg-libxdmcp=1.1.3=h7f98852_0\r\n  - xz=5.2.5=h516909a_1\r\n  - yaml=0.2.5=h516909a_0\r\n  - zeromq=4.3.4=h9c3ff4c_1\r\n  - zict=2.0.0=py_0\r\n  - zipp=3.5.0=pyhd8ed1ab_0\r\n  - zlib=1.2.11=h516909a_1010\r\n  - zstandard=0.15.2=py37h5e8e339_0\r\n  - zstd=1.5.0=ha95c52a_0\r\n  - pip:\r\n    - absl-py==0.13.0\r\n    - aiohttp==3.7.4.post0\r\n    - aiohttp-cors==0.7.0\r\n    - aioredis==1.3.1\r\n    - async-timeout==3.0.1\r\n    - autograd==1.3\r\n    - bayesian-optimization==1.2.0\r\n    - blessed==1.19.0\r\n    - blessings==1.7\r\n    - cachetools==4.2.2\r\n    - cma==2.7.0\r\n    - colorful==0.5.4\r\n    - cython==0.29.24\r\n    - future==0.18.2\r\n    - google-api-core==1.31.2\r\n    - google-auth==1.35.0\r\n    - google-auth-oauthlib==0.4.6\r\n    - googleapis-common-protos==1.53.0\r\n    - gpustat==0.6.0\r\n    - gpy==1.10.0\r\n    - gpytorch==1.5.1\r\n    - grpcio==1.40.0\r\n    - hebo==0.1.0\r\n    - hiredis==2.0.0\r\n    - multidict==5.1.0\r\n    - nevergrad==0.4.3.post8\r\n    - nvidia-ml-py3==7.352.0\r\n    - oauthlib==3.1.1\r\n    - opencensus==0.7.13\r\n    - opencensus-context==0.1.2\r\n    - paramz==0.9.5\r\n    - protobuf==3.17.3\r\n    - psycopg2==2.9.1\r\n    - py-spy==0.3.9\r\n    - pyasn1==0.4.8\r\n    - pyasn1-modules==0.2.8\r\n    - pydantic==1.8.2\r\n    - pymoo==0.4.2.2\r\n    - ray==1.7.1\r\n    - requests-oauthlib==1.3.0\r\n    - rsa==4.7.2\r\n    - scipy==1.5.4\r\n    - sklearn==0.0\r\n    - tensorboard==2.6.0\r\n    - tensorboard-data-server==0.6.1\r\n    - tensorboard-plugin-wit==1.8.0\r\n    - torch==1.9.1\r\n    - werkzeug==2.0.1\r\n    - yarl==1.6.3\r\n```\r\n### Error messages, stack traces, or logs\r\n\r\n### Steps to reproduce\r\n\r\n1. run study\r\n2. observe memory increasing on the node (the objective/tunable function consumes a constant amount)\r\n3. OOM after a few hours\r\n\r\n### Reproducible examples\r\n\r\n```python\r\nimport optuna\r\nimport ray\r\n\r\n\r\n@ray.remote\r\ndef optimize_optuna(df_data_in_object_store, labels_in_object_store, study_name, feature_name, pg_user, pg_pass,\r\n                    pg_srv_ip):\r\n    import optuna\r\n    from feature import factory  # local module\r\n\r\n    # disable all outputs\r\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\r\n    optuna.logging.disable_default_handler()\r\n\r\n    # this crashes the node with OOM when resuming from a \"large\" number of trials e.g. 100k (uses +32GB)\r\n    # sampler = optuna.samplers.CmaEsSampler(use_separable_cma=True)\r\n\r\n    study = optuna.create_study(\r\n        study_name=study_name,\r\n        direction=\"maximize\",\r\n        # sampler=sampler,\r\n        load_if_exists=True,\r\n        storage=f\"postgresql://{pg_user}:{pg_pass}@{pg_srv_ip}/{study_name}\")\r\n\r\n    # get our trainable class from a local module\r\n    trainable = factory.get_feature_cls(feature_name)(asks_in_object_store, labels_in_object_store)\r\n\r\n    study.optimize(\r\n        trainable, n_trials=None, timeout=None, gc_after_trial=False, show_progress_bar=False, \r\n        callbacks=None)\r\n\r\nif __name__ == \"__main__\":\r\n    # create the DB for a new study or do noting if already exist\r\n    subprocess.run(\r\n            [\"psql\", \"-U\", \"myrole\", \"-d\", \"postgres\", \"-h\", POSTGRESQL_SRV_IP, \"-c\", \r\n            f\"CREATE DATABASE {study_name}\"],\r\n            stdout=subprocess.DEVNULL,\r\n            stderr=subprocess.DEVNULL)\r\n    \r\n    # load data\r\n    df_labeled = ...\r\n    df_data = ...\r\n    \r\n    # connect to local ray cluster, propagate local env so we can call local \r\n    # modules from the remote function\r\n    ray.init(\r\n        runtime_env={\"working_dir\": \"src\", \"excludes\": [\"*.txt\", \"bin\", \"doc\", \"examples\", \"notebooks\"]}, \r\n        address=\"auto\", \r\n        _redis_password=\"xxx\")\r\n    \r\n    # put data in every node's shared memory so all processes running on that node can  \r\n    # access a single object in memory (instead  of each process having it's own copy)\r\n    ref_data = ray.put(df_data)\r\n    ref_labeled = ray.put(df_labeled)\r\n    \r\n    resources_cluster = ray.cluster_resources()\r\n    workers_available = int(resources_cluster[\"CPU\"])\r\n\r\n    # send tasks to the local cluster\r\n    for _ in range(workers_available):\r\n        optimize_optuna.remote(\r\n            ref_data,\r\n            ref_labeled,\r\n            study_name,\r\n            feature_name,\r\n            constants.Env.POSTGRESQL_USER,\r\n            db_pass,\r\n            POSTGRESQL_SRV_IP\r\n        )\r\n\r\n    # block until the user interrupts\r\n    while True:\r\n        time.sleep(3600*24*31*356)\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "himkt",
        "body": "Hi @jmakov, sorry for the late response.\r\n\r\n`CmaEsSampler` needs to fetch completed trials when sampling so I think it's not memory leak but data of tons of trials becomes too large (also, Optuna stores sampled parameters in JSON format and it also could be huge if you have many parameters to search).\r\n\r\nIf the problem also occurs with a sampler using the optimization history such as `TPESampler` and doesn't occur with a sampler that doesn't depend on the history such as `RandomSampler`, I think the behavior is not a bug."
      },
      {
        "user": "azagajewski",
        "body": "Hi @himkt, it appears not to be a feature of the adaptive samplers - I just ran a hyperparameter optimization on a deep learning model using RandomSampler and MedianPruner, and the memory leak over trials is the same as TPESampler. It's a bit hard to reproduce since this ran for 3 hrs on a 4xGPU AWS node, but the rough code structure is as follows:\r\n\r\n\r\n```\r\ndef objective(trial, strategy, **kwargs):\r\n   \r\n   keras.backend.clear_session()\r\n   \r\n   architecture = trial.suggest_categorical(\"arch\", list(...)) \r\n   learning_rate = trial.suggest_float(\"learning_rate\", low, high, log=True) \r\n   \r\n   with strategy.scope():\r\n      model = build_model(architecture, learning_rate)\r\n      prune = [TFKerasPruningCallback(trial,'val_loss')]\r\n      history = model.fit((callbacks=prune)\r\n      return(history.history[\"val_loss\"][-1]\r\n\r\n\r\nstudy = optuna.create_study(direction=\"minimize\",storage=db_URL, study_name=study_name, load_if_exists=True,sampler=optuna.samplers.RandomSampler(), pruner=optuna.pruners.MedianPruner(n_startup_trials=10,n_warmup_steps=3))\r\n\r\nobjective_wrapper = lambda trial: optuna_objective(trial, strategy=strategy)\r\n\r\nstudy.optimize(objective_wrapper, n_trials=trials, gc_after_trial=True)\r\n```\r\n\r\nThe `clear_session()` correctly flushes the GPU between trials and clears distribution settings, `with.strategy.scope()` recreates the distribution settings to use multiple GPUs, `gc_after_trial` reduces the memory leak but does not eliminate entirely. The RDB is a postgres on a remote server. \r\n\r\nHere is the diagnostic showing real and virtual memory increasing over the completion of several trials with the random sampler and median pruner - the process continues until the OS refuses to create new optuna threads due to the virtual memory being too high, leading to `LLVM ERROR: pthread_create failed: Resource temporarily unavailable`\r\n\r\n![optuna_memory_leak](https://github.com/optuna/optuna/assets/44504386/6f1741ae-403e-42f8-93f7-687076a4d7c7)\r\n"
      },
      {
        "user": "steveepreston",
        "body": "I confirm this issue.\r\nMemory in-use stacking trail by trail, until it raise out of memory (OOM) error.\r\n\r\nplacing these inside `objective(trial)` also not helped:\r\n```python\r\nK.clear_session()\r\ngc.collect()\r\n```"
      }
    ]
  },
  {
    "issue_number": 1318,
    "title": "Optimization starts from scratch after switching sampler from TPE to CMA-ES",
    "author": "danpolus",
    "state": "closed",
    "created_at": "2020-06-03T07:57:43Z",
    "updated_at": "2024-10-16T10:41:48Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "The following code performs 500 optimization trials with TPE sampler and then another 500 optimization trials with CMA-ES sampler. Sometimes the optimization continues from the same point after the sampler switch and sometimes it starts from scratch. \r\n\r\nsmp = optuna.samplers.TPESampler()\r\nstudy = optuna.create_study(sampler=smp)\r\nstudy.optimize(objective, n_trials=500)\r\nstudy.sampler = optuna.integration.CmaEsSampler()\r\nstudy.optimize(objective, n_trials=500)\r\n\r\n![Figure_1 TPE CMA combined params 2](https://user-images.githubusercontent.com/28355894/83610866-cce64600-a588-11ea-9fe2-ffa7564bbd9e.png)\r\n\r\n## Expected behavior\r\n\r\nI expect the optimization process to take in account the optimization achievements so far, after switching the sampler\r\n\r\n## Environment\r\n\r\n- Optuna version: 1.2.0\r\n- Python version: 3.7\r\n- OS: windows 10\r\n\r\n",
    "comments": [
      {
        "user": "toshihikoyanase",
        "body": "Thank you for your report.\r\n\r\n> Sometimes the optimization continues from the same point after the sampler switch and sometimes it starts from scratch.\r\n\r\nHmm, `optuna.integration.CmaEsSampler` does not keep any states between trials and uses the entire optimization history to suggest the next parameter values (c.f., https://github.com/optuna/optuna/blob/master/optuna/integration/cma.py#L213-L216). I created a notebook to check the behavior of `CmaEsSampler` with a simple quadratic function:\r\nhttps://colab.research.google.com/drive/1VB5lUYtFKyaqG-Xz9iHYGdUSekbgAMaG#scrollTo=sj5BSMcAitHl\r\n\r\nAs shown in the following figure, the restarting strategy makes the variance of CMA-ES small. Please compare the trials 50-99 in the first figure with the trials 0-49 in the second figure. If CMA-ES does not use the optimization history of TPE, they should be similar plots.\r\n\r\nTPE then CMA (fifty-fifty)\r\n![image](https://user-images.githubusercontent.com/3255979/83621835-2c187a80-a5ca-11ea-913b-9e5ae7582ee3.png)\r\n\r\nCMA only\r\n![image](https://user-images.githubusercontent.com/3255979/83621951-510ced80-a5ca-11ea-9ef3-0dd459417ab4.png)\r\n\r\nI guess the CMA-ES is not suitable for your objective function. Have you ever tried `optuna.integration.CmaEsSampler` without `TPESampler`?"
      },
      {
        "user": "danpolus",
        "body": "Thanks for your fast reply!\r\nIndeed, there might be a problem with my error value. Many times it's NaN. I set it to np.inf or to a very large number (10 times bigger than the maximum possible error). So this is what the objective function returns.  Can you propose a better strategy?\r\nRegarding using CMA only, sometimes it explores the entire parameter range and sometimes get \"stacked\" in a very narrow range. \r\nIn general, my main problem is that I can't get a consistent optimization behaviour. Sometimes it works as expected and sometimes not."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5710,
    "title": "Is it possible to enqueue the best trial achieved from TPESampler into CmaEsSampler",
    "author": "faze-geek",
    "state": "closed",
    "created_at": "2024-10-16T10:30:09Z",
    "updated_at": "2024-10-16T10:33:04Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\r\n\r\nHi, in my codebase, I am running 2 separate studies.\r\n1. **Scale phase** - Here we just optimize 1 parameter the multiplier. (Let's say `m` for a linear combination of indicators).\r\n2. **Search phase** - Here we fine tune the multiplicative weights for all indicators (Let's say `w1, w2 ...wn` for optimizing \r\n`w1*f1 +/- w2*f2 +/- .....wn*fn`).\r\n\r\nHow can I get the best multiplier from the scale and pass that as the initial base line trial (or defualt weight value say) for all multiplicative weights for the search (i.e. `(w1 = m , w2 =m ,.....wn=m)`). \r\nActually I am optimizing a linear combination and my multiplier optimizes to a great extent with TPE  but then I shift to **search** and I do not have any context of the best trial in **scale**.\r\n\r\nThat leads to a almost new optimization problem where CmaEs is expected to work for a huge deficit instead of the optimal trial of TPE.\r\n\r\n\r\n### Description\r\n\r\nThis is my current code. Kindly ignore the irrelevant parts.\r\n\r\n```\r\nvalidate_args_and_return_types\r\ndef setup_experiment_configs(\r\n        experiment_config: ExperimentConfig,\r\n        strategy_library_config: StrategyLibraryConfig,\r\n        strategy_hash_config_dict: Optional[dict[str, dict[str, tuple[StrategyConfig, optuna.trial.Trial, int]]]],\r\n        strategy_studies: Optional[dict[str, optuna.Study]],\r\n        scale_report: dict[str, (float, float)],\r\n        mode: str\r\n\r\n) -> tuple[dict[str, optuna.Study], dict[str, dict[str, tuple[StrategyConfig, optuna.trial.Trial, int]]]]:\r\n\r\n    global SCALE_STUDY_CREATED, SEARCH_STUDY_CREATED, batch_counter\r\n    print(divide_strategy_config(strategy_library_config)[0].keys())\r\n    strategy_config = StrategyConfig(config=divide_strategy_config(strategy_library_config)[0])\r\n\r\n    if strategy_hash_config_dict is None:\r\n        strategy_hash_config_dict: dict[str, dict[str, tuple[StrategyConfig, optuna.trial.Trial]]] = defaultdict(dict)\r\n    if strategy_studies is None:\r\n        strategy_studies: dict[str, optuna.Study] = dict()\r\n\r\n    for strategy in experiment_config.get(ExperimentConfig.Types().STRATEGIES):\r\n        print(f\"Setting up strategy study for {strategy} mode: {mode} scale_report: {scale_report}\")\r\n\r\n        if mode == WorkFlow().SEARCH_MODEL:\r\n            strategy_config.update({StrategyConfig.Types().MULTIPLIER: scale_report[strategy][0]\r\n                                    })\r\n            (pathlib.Path(experiment_config.experiment_dir()) / \"strategy\").mkdir(exist_ok=True, parents=True)\r\n            if not SEARCH_STUDY_CREATED:\r\n                batch_counter = 0\r\n                (pathlib.Path(experiment_config.experiment_dir()) / \"strategy\" / strategy).mkdir(parents=True,\r\n                                                                                                exist_ok=True)\r\n                study_storage = optuna.storages.RDBStorage(f\"postgresql://postgres:billion@/\"\r\n                                                           f\"{experiment_config.get(ExperimentConfig.Types().USER)}\")\r\n                print(f\"Study storage is: {study_storage}\")\r\n\r\n                strategy_studies[strategy] = optuna.create_study(direction=\"maximize\",\r\n                                                                 study_name=f\"{experiment_config.get(ExperimentConfig.Types().EXPERIMENT_NAME)}_{strategy}_{mode}\",\r\n                                                                 storage=study_storage,\r\n                                                                 sampler=CmaEsSampler(),\r\n                                                                 load_if_exists=True)\r\n                SEARCH_STUDY_CREATED = True\r\n\r\n        if mode == WorkFlow().SCALE_MODEL:\r\n            if not SCALE_STUDY_CREATED:\r\n                batch_counter = 0\r\n                (pathlib.Path(experiment_config.experiment_dir()) / \"strategy\" / strategy).mkdir(parents=True,\r\n                                                                                                 exist_ok=True)\r\n                study_storage = optuna.storages.RDBStorage(f\"postgresql://postgres:billion@/\"\r\n                                                           f\"{experiment_config.get(ExperimentConfig.Types().USER)}\")\r\n                print(f\"Study storage is: {study_storage}\")\r\n                print(\"Adding TPE Sampler for scale model\")\r\n                strategy_studies[strategy] = optuna.create_study(direction=\"maximize\",\r\n                                                                 study_name=f\"{experiment_config.get(ExperimentConfig.Types().EXPERIMENT_NAME)}_{strategy}_{mode}\",\r\n                                                                 storage=study_storage,\r\n                                                                 sampler=TPESampler(),\r\n                                                                 load_if_exists=True)\r\n                SCALE_STUDY_CREATED = True\r\n\r\n        strategy_hash_config_dict[strategy] = setup_strategy_studies(\r\n            experiment_config,\r\n            strategy,\r\n            strategy_config,\r\n            strategy_hash_config_dict[strategy],\r\n            strategy_studies[strategy],\r\n            mode,\r\n        )\r\n\r\n    return strategy_studies, strategy_hash_config_dict\r\n    ```\r\n\r\n### Alternatives (optional)\r\n\r\nAlso realize that the parameters to be optimized in both studies are different.\r\nHow is it possible to pass the multiplier list as the base line trial. Do I need to shift to TPE sampler totally ?\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5118,
    "title": "Nested study",
    "author": "R-N",
    "state": "closed",
    "created_at": "2023-11-16T22:30:03Z",
    "updated_at": "2024-10-15T23:05:34Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nSimultaneously optimizing a general hyperparameter set, such as activation functions, and dataset-specific hyperparameter set, such as hidden dimension size. Doing the search in parallel for different datasets.\r\n\r\n### Description\r\n\r\nI want to be able to simultaneously optimize a general hyperparameter set, such as activation functions or architectural options, and dataset-specific hyperparameter set, such as hidden dimension size. Doing the search in parallel for different datasets. \r\n\r\n### Alternatives (optional)\r\n\r\nI can't simply nest study.optimize because it does its own loop. But maybe with the dataset-specific study as outer loop, wrapping the objective as partial with sampled parameters, then do the general study as inner loop with n=1, storing the value in an object to retrieve later for the outer loop. Or reversing the inner-outer study. Would that work? Does Optuna allow nesting study.optimize calls? This also means each study will be completely oblivious of each other. Is that fine?\r\n\r\nAs another alternative, I can add a prefix/suffix to the dataset-specific hyperparameters and conditionally sample them. However this means they can't be sampled multivariately. It will also clutter the study, specially when analyzed with Optuna Dashboard.\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "R-N",
        "body": "Or maybe what I need is just a function to \"return\" the objective value so I can just do the loop manually and \"return\" the value to the two studies. "
      },
      {
        "user": "R-N",
        "body": "Oh wow I just realized it exists now with study.ask and study.tell! Guess I'll just try this. "
      },
      {
        "user": "R-N",
        "body": "Oh I just realized the different datasets will have different objective value scale and distribution. This won't work unless it's handled, and I don't have any idea how or if it's even possible at all. \r\n\r\nMaybe it can be done at sampler level when it separates the good and bad parameters."
      }
    ]
  },
  {
    "issue_number": 5284,
    "title": "Multi-criteria decision making ",
    "author": "Mahdibrahim",
    "state": "open",
    "created_at": "2024-02-28T13:34:00Z",
    "updated_at": "2024-10-13T18:44:24Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nIn solving multi-objective hyperparameter optimization , the pareto front solution will be the output.\r\nIn order to select the optimal solution among the paretos, multi-criteria decision making analysis should be performed (in case of 2 objectives and more).\n\n### Description\n\nIn order to develop MCDM tool, lot of MCDA algorithm exist (TOPSIS, ELECTRE I,...)\r\nMost algorithms exist on PyMCDM which was developed recently. \r\nThis will allow programmers to select the most optimal model upon several criteria and objective weights. \n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "Mr-Saab-29",
        "body": "Hey @Mahdibrahim,\r\n\r\nWithin a university project, my team and I would like to contribute to this issue. If you have any more detailed specifications or hints, we would be glad to get them from you.\r\nThanks for your work !"
      }
    ]
  },
  {
    "issue_number": 5375,
    "title": "Support multi-objective CMA-ES (MOCMAES) sampling algorithm",
    "author": "hnanacc",
    "state": "open",
    "created_at": "2024-03-29T11:17:13Z",
    "updated_at": "2024-10-10T11:02:30Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\r\n\r\nOptuna currently supports only single-objective CMA-ES for sampling, it would be useful to have multi-objective CMA-ES as a lot of tasks need to consider multiple objectives to be optimized together.\r\n\r\nThis would also be helpful for other HPO/AutoML frameworks that depend on Optuna for their functionality. This issue is particularly motivated from the need to [support MOCMAES](https://github.com/kubeflow/katib/issues/2078) in [kubeflow/katib](https://github.com/kubeflow/katib).\r\n\r\n### Description\r\n\r\nSimilar to other multi-objective sampling algorithms, MOCMAES will be able to work with multiple objectives.\r\n\r\n```python\r\n# multi-objective function to be optimized.\r\ndef multi_objective(trial):\r\n    ...\r\n    return objective_1, objective_2\r\n\r\n# use CMA-ES sampler with multi-ojective.\r\nsampler = optuna.samplers.CmaEsSampler()\r\n\r\n# specify directions for each objective.\r\nstudy = optuna.create_study(sampler=sampler, directions=['maximize', 'minimize'])\r\n\r\n# execute the optimization process with multi-objective.\r\nstudy.optimize(multi_objective, n_trials=1000, timeout=1000)\r\n\r\n# visualize pareto-front\r\noptuna.visualization.plot_pareto_front(study, target_names=[\"objective_1\", \"objective_2\"])\r\n```\r\n\r\n### Alternatives (optional)\r\n\r\nCurrently, there are no decent solutions to MOCMAES, which are also maintained in the open-source domain.\r\n\r\nSome relevant solutions are [chocolate](https://github.com/AIworx-Labs/chocolate) and [pycomocma](https://github.com/CMA-ES/pycomocma) which were both last committed 4 years ago.\r\n\r\nAn option would be add pycomocma as an optuna-integration and maintain it further.\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "c-bata",
        "body": "@hnanacc (cc: @tenzen-y) Thank you for your feature request!\r\n\r\n@nomuramasahir0, the maintainer of `cmaes` (https://github.com/CyberAgentAILab/cmaes), is planning to work on implementing MOCMAES. However, we do not have a definite schedule for the completion of this task yet. Would it be possible for you (and Katib) to wait until our implementation is completed?"
      },
      {
        "user": "tenzen-y",
        "body": "> @hnanacc (cc: @tenzen-y) Thank you for your feature request!\r\n> \r\n> @nomuramasahir0, the maintainer of `cmaes` (https://github.com/CyberAgentAILab/cmaes), is planning to work on implementing MOCMAES. However, we do not have a definite schedule for the completion of this task yet. Would it be possible for you (and Katib) to wait until our implementation is completed?\r\n\r\n@hnanacc @c-bata Thank you for pointing this out! Yes, we (katib) can wait for the implementation.\r\nIt might be better to create a feature request issue in the `cmaes` (https://github.com/CyberAgentAILab/cmaes) repository to ask `cmaes` maintainers to raise the priority of MOCMAES. WDYT?\r\n\r\ncc: @andreyvelich"
      },
      {
        "user": "y0z",
        "body": "@hnanacc @tenzen-y \r\n\r\nMO-CMA-ES is now available on [OptunaHub](https://hub.optuna.org/)!\r\n- [Multi-objective CMA-ES (MO-CMA-ES) Sampler](https://hub.optuna.org/samplers/mocma/)\r\n\r\nYou can use `MoCmaSampler` as follows.\r\n```python\r\nimport optunahub\r\n\r\nsampler = optunahub.load_module(\"samplers/mocma\").MoCmaSampler(popsize=100, seed=42)\r\n```"
      }
    ]
  },
  {
    "issue_number": 3715,
    "title": "Trial 2 failed, because the value None could not be cast to float.",
    "author": "manza-ari",
    "state": "closed",
    "created_at": "2022-06-23T09:41:52Z",
    "updated_at": "2024-10-09T18:22:00Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nI want to find optimal milestones (SGD optimizer) for my model but facing above error \r\n\r\n`\r\n# OPTUNA Learning Rate and Optimizer \r\n\r\n                optuna_milestone_min =  optuna_trial.suggest_int('milestones' , 1, 30, step = 5)\r\n                optuna_milestone_max =  optuna_trial.suggest_int('milestones' , 1, 30, step = 5 )\r\n                \r\n                \r\n                # Loss, criterion and scheduler (re)initialization\r\n                criterion      = nn.CrossEntropyLoss(reduction='none')\r\n                optim_backbone = optim.SGD(models.parameters(), lr=LR, \r\n                    momentum=MOMENTUM, weight_decay=WDECAY)                             #OPTUNA Learning Rate suggestions \r\n    \r\n                sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=[optuna_milestone_min, optuna_milestone_max])\r\n                optimizers =  optim_backbone\r\n                schedulers = sched_backbone\r\n`\n\n### Environment\n\n- Optuna version:  2.10.0\r\n- Python version: 3.8\r\n- OS: Ubuntu \r\n- (Optional) Other libraries and their versions:\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\nTrial 2 failed, because the value None could not be cast to float.\n```\n\n\n### Steps to reproduce\n\n1.\r\n2.\r\n3.\r\n```pmodels      = resnet18\r\n                \r\n                torch.backends.cudnn.benchmark = True\r\n                models = torch.nn.DataParallel(models, device_ids=[0])\r\n                \r\n                # OPTUNA Learning Rate and Optimizer \r\n\r\n                optuna_milestone_min =  optuna_trial.suggest_int('milestones' , 1, 30, step = 5)\r\n                optuna_milestone_max =  optuna_trial.suggest_int('milestones' , 1, 30, step = 5 )\r\n                \r\n                \r\n                # Loss, criterion and scheduler (re)initialization\r\n                criterion      = nn.CrossEntropyLoss(reduction='none')\r\n                optim_backbone = optim.SGD(models.parameters(), lr=LR, \r\n                    momentum=MOMENTUM, weight_decay=WDECAY)                             #OPTUNA Learning Rate suggestions \r\n    \r\n                sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=[optuna_milestone_min, optuna_milestone_max])\r\n                optimizers =  optim_backbone\r\n                schedulers = sched_backbone\r\n                \r\n```\r\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "I'm sure that this is not the bug of Optuna; the objective function returns `None` instead of `float` as in the message. \r\n\r\nSince the code is reproducible and the objective function is not in the issue description, we could not investigate this issue. However could you take a look at the following comments:\r\n\r\n1. Use unique param name for min/max milestone. `optuna_milestone_max =  optuna_trial.suggest_int('milestones' , 1, 30, step = 5 )` should be `optuna_milestone_max =  optuna_trial.suggest_int('milestones_max' , 1, 30, step = 5 )` (min too). \r\n2. As in the docs of [`MultiStepLR`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html), `milestones` should increase. In the current implementation, `milestones` might decrease. \r\n"
      },
      {
        "user": "manza-ari",
        "body": "Thank you so much for your response!\r\nYeah, right. I agree with you, I did the following changes so OPTUNA can suggest an optimal value for the start and the end of milestones for the optimizer.  but still facing the same error. \r\n`\r\noptuna_milestone_min =  optuna_trial.suggest_int('milestones_min' , 10, 15, step = 5)\r\noptuna_milestone_max =  optuna_trial.suggest_int('milestones_max' , 15, 30, step = 5 )\r\n`\r\nI want your comment on one more thing. I want to ask OPTUNA for an optimal initial learning rate as well as the reduction during the given milestone[initial, end] as given here https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html     for some 300 Epochs\r\n`\r\n# OPTUNA Learning Rate and Optimizer \r\noptuna_initial_lr = optuna_trial.suggest_float('lr', 1e-3, 1e-1, log=True)\r\noptuna_reductions = optuna_trial.suggest_int('num_reduction', 1, 4)\r\n         for i in range(optuna_reductions):\r\n              i = optuna_trial.suggest_int(\"when_{}\".format(i), 50*(i-1), 300, step=50)       \r\n`\r\n\r\n\r\n\r\n"
      },
      {
        "user": "okra01",
        "body": "I'm facing the same problem now. May I ask did you solved it?"
      }
    ]
  },
  {
    "issue_number": 5697,
    "title": "trial_id to reset to zero for every study",
    "author": "loewenm",
    "state": "closed",
    "created_at": "2024-10-08T15:06:28Z",
    "updated_at": "2024-10-08T17:36:42Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\r\n\r\nUsage of a single database to house multiple studies results in an ever increasing trial_id number. We'd like to see trial_id be specific to a particular study instead of to the entire database. \r\n\r\nIn other words, why is the trial_id number shared across studies?\r\n\r\n### Description\r\n\r\nWhen using a single database to house multiple studies, the usage of study._trial_id results in an ever increasing trial_id value across studies.\r\n\r\nFor example: \r\nSuppose I have a study _ABC_ with 100 trials in it and then start a new study called _DEF_ with another 100 trials in it.\r\nThis will result in study _ABC_'s trial_id's ranging [0, 100) and study _DEF_'s trial_id's ranging [100, 200).\r\n\r\n### Alternatives (optional)\r\n\r\nPlease have trial_id's as a property of each individual study instead of the entire database.\r\n\r\n### Additional context (optional)\r\n\r\nI am extracting the trial_id as a property of the trial via the following:\r\n\r\n```\r\nstudy = optuna.create_study(params)\r\nstudy.optimize(lambda trial: optuna_objective(trial), n_trials=100)\r\n\r\ndef optuna_objective(trial, **kwargs):\r\n    params = ModelConfig(\r\n        batch_size=2 ** trial.suggest_int(\"batch_size\", 4, 11),\r\n    )\r\n\r\n    valid_results = train_model(params, trial_id=trial._trial_id)   # <<< HERE!!! trial._trial_id\r\n\r\n    return valid_results[\"objective\"]\r\n```\r\n\r\nWhat's strange is that if I return the study's trials via 'study.trials_dataframe()', the correct (intended) trial_id's are displayed...\r\n\r\nSo, somewhere, someone has accounted for trial_id numbers being shared across studies, but didn't correct the root cause.\r\n\r\nEdit: Or is there a better way to extract the trial_id of a study's trial during runtime that matches study.trials_dataframe()?",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "Did you check `trial.number`?\r\nThat's exactly what you want."
      },
      {
        "user": "loewenm",
        "body": "Ah! Thank you for this.\r\n\r\nPerhaps the naming convention should be consistent across documentation and parameters to reduce confusion in the future. A clear description of trial._number and trial._trial_id and how it's referenced in both Optuna and Optuna Dashboard would be helpful. I say this because nearly everything mentions what should be _trial._number_ as _trial._trial_id_.\r\n\r\nedit: It's apparently trial._number"
      }
    ]
  },
  {
    "issue_number": 5202,
    "title": "Document how each pruner handles `nan`",
    "author": "HideakiImamura",
    "state": "open",
    "created_at": "2024-01-26T06:50:53Z",
    "updated_at": "2024-10-08T10:15:29Z",
    "labels": [
      "document",
      "contribution-welcome"
    ],
    "body": "### What is an issue?\n\nOptuna has various pruners, and their handling of nan in intermediate values is different from each other. Checking the source code and documenting each handling would be beneficial for users.\r\n\r\n- [ ] optuna.pruners.MedianPruner\r\n- [x] optuna.pruners.NopPruner\r\n- [ ] optuna.pruners.PatientPruner\r\n- [ ] optuna.pruners.PercentilePruner\r\n- [ ] optuna.pruners.SuccessiveHalvingPruner\r\n- [ ] optuna.pruners.HyperbandPruner\r\n- [x] optuna.pruners.ThresholdPruner\r\n\r\n",
    "comments": [
      {
        "user": "Prabhat-Thapa45",
        "body": "@HideakiImamura I would like to work on this. I quickly checked the code base and understood why it was unnecessary for two pruners( nop and threshold). I will look into others and make a PR out of it."
      },
      {
        "user": "HideakiImamura",
        "body": "@Prabhat-Thapa45 Thank you for your work. I'm looking forward to your contribution. Please feel free to ask me if you have any questions."
      },
      {
        "user": "Prabhat-Thapa45",
        "body": "I will create pr soon please do provide feedback if needed there so far it seems fine for me. @HideakiImamura "
      }
    ]
  },
  {
    "issue_number": 5571,
    "title": "Enhance `optuna/optuna/visualization` by eliminating redundant for-loops and repetitive code",
    "author": "kAIto47802",
    "state": "open",
    "created_at": "2024-07-12T03:08:52Z",
    "updated_at": "2024-10-03T02:15:45Z",
    "labels": [
      "code-fix"
    ],
    "body": "### Motivation\r\n\r\nIn the `optuna/optuna/visualization`,\r\n-  The functions `_is_log_scale()`, `_is_categorical()`, and `_is_numerical()`, which iterates over the trials, are called several times redundantly. This problem exists in:\r\n    - `_get_parallel_coordinate_info()` in `_parallel_coordinate.py` as described in #5409 \r\n    - `_get_axis_info()` in `_contour.py`\r\n    - `_get_axis_info()` in `_rank.py`\r\nhttps://github.com/optuna/optuna/blob/fcffbeab86859c32c3c646947f4fd764860c922b/optuna/visualization/_contour.py#L385-L413\r\n- Also, the function `_get_axis_info()`, which iterates over the trials, are called several times.  This exists in\r\n    - `_get_contour_subplot_info()` in `_contour.py`\r\n    - `_get_rank_subplot_info()` in `_rank.py`\r\nhttps://github.com/optuna/optuna/blob/fcffbeab86859c32c3c646947f4fd764860c922b/optuna/visualization/_contour.py#L328-L342\r\n  \r\nTherefore, if the performance is improved in #5409, it should also be improved in these locations.\r\n \r\n- Additionally, the function `_get_axis_info()`  in `_contour.py` and `_rank.py` are mostly the same, so they should be refactored.\r\nhttps://github.com/optuna/optuna/blob/fcffbeab86859c32c3c646947f4fd764860c922b/optuna/visualization/_contour.py#L383-L447\r\nhttps://github.com/optuna/optuna/blob/fcffbeab86859c32c3c646947f4fd764860c922b/optuna/visualization/_rank.py#L244-L291\r\n\r\n\r\n### Suggestion\r\n\r\nIn the `optuna/optuna/visualization`,\r\n- Remove redundant for-loops not only in `_get_parallel_coordinate_info()` in `_parallel_coordinate.py` but also in:\r\n  - `_get_axis_info()` in `_contour.py`\r\n  - `_get_axis_info()` in `_rank.py`\r\n- Remove redundant for-loops in \r\n    - `_get_contour_subplot_info()` in `_contour.py`\r\n    - `_get_rank_subplot_info()` in `_rank.py`\r\n- Refactor the duplicate part of `_get_axis_info()` in `_contour.py` and `_rank.py` by separating them into `_util.py`\r\n\r\nNote that these removing of the for-loops should only be done if the performance improved in the fix in #5409 .\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "RektPunk",
        "body": "Can I give this a try?"
      },
      {
        "user": "Prabhat-Thapa45",
        "body": "Can I get to contribute here @RektPunk @kAIto47802 if things are not taken"
      },
      {
        "user": "RektPunk",
        "body": "I'm still waiting too, I think they're working on https://github.com/optuna/optuna/issues/5409 ;)"
      }
    ]
  },
  {
    "issue_number": 5646,
    "title": "Optuna cannot multithread on sklearn Pipeline with multiple ColumnTransformers referencing column names",
    "author": "vkarakcheev",
    "state": "open",
    "created_at": "2024-08-28T09:18:58Z",
    "updated_at": "2024-09-27T09:56:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nOptuna optimization executed on sklearn `Pipeline` with _multiple_ `ColumnTransformer`s crashes when `n_jobs` ≠ 1 and the `transformers` argument of `ColumnTransformer`s references column _names_. But works fine when `n_jobs` = 1 _or_ when `transformers` argument of `ColumnTransformer`s references column _indices_ _or_ when there's only one `ColumnTransformer` in the `Pipeline`.\r\n\r\n### Environment\r\n\r\n- OS: Windows-10-10.0.19045-SP0\r\n- Python version: 3.11.7\r\n- Optuna version: 3.6.1\r\n- Sklearn version: 1.5.0\r\n- Pandas version: 2.1.4\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\n[W 2024-08-28 12:09:53,378] Trial 0 failed with parameters: {'est__alpha': 2.3247620494647703, 'est__l1_ratio': 0.6976340871751063} because of the following error: ValueError('Specifying the columns using strings is only supported for dataframes.').\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 338, in _get_column_indices\r\n    all_columns = X.columns\r\n                  ^^^^^^^^^\r\nAttributeError: 'numpy.ndarray' object has no attribute 'columns'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\r\n    value_or_values = func(trial)\r\n                      ^^^^^^^^^^^\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Temp\\ipykernel_11588\\885295757.py\", line 46, in objective\r\n    model.fit(X, y)\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\r\n    return fit_method(estimator, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 472, in fit\r\n    Xt = self._fit(X, y, routed_params)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 409, in _fit\r\n    X, fitted_transformer = fit_transform_one_cached(\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\r\n    return self.func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1329, in _fit_transform_one\r\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 313, in wrapped\r\n    data_to_wrap = f(self, X, *args, **kwargs)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\r\n    return fit_method(estimator, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 969, in fit_transform\r\n    self._validate_column_callables(X)\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 536, in _validate_column_callables\r\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\r\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\SC13015\\AppData\\Local\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 340, in _get_column_indices\r\n    raise ValueError(\r\nValueError: Specifying the columns using strings is only supported for dataframes.\r\n[W 2024-08-28 12:09:53,382] Trial 0 failed with value None.\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nfrom sklearn import set_config\r\nfrom sklearn.base import clone\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\r\nfrom sklearn.linear_model import ElasticNet\r\n\r\nfrom optuna import create_study\r\n\r\niris = load_iris()\r\nX = pd.DataFrame(iris['data'], columns=iris['feature_names'])\r\ny = pd.Series(iris['target']).rename('iris type')\r\n\r\nset_config(transform_output='pandas')\r\n\r\ntransf_params = dict(\r\n    remainder='passthrough',\r\n    verbose_feature_names_out=False,\r\n    force_int_remainder_cols=False,\r\n)\r\n\r\n# Works only when n_jobs=1\r\nsc1_cols = ['sepal length (cm)', 'sepal width (cm)']\r\nsc2_cols = ['petal length (cm)', 'petal width (cm)']\r\n\r\n# Works with any n_jobs\r\n# sc1_cols = [0, 1]\r\n# sc2_cols = [2, 3]\r\n\r\npipe = Pipeline([\r\n    ('scl', ColumnTransformer([('sc1', StandardScaler(), sc1_cols)], **transf_params)), \r\n    ('sc2', ColumnTransformer([('sc2', MinMaxScaler(), sc2_cols)], **transf_params)), \r\n    ('est', ElasticNet())\r\n])\r\n\r\ndef objective(trial):\r\n    params = dict(\r\n        est__alpha=trial.suggest_float('est__alpha', 1e-3, 1e3, log=True),\r\n        est__l1_ratio=trial.suggest_float('est__l1_ratio', 0, 1),\r\n    )\r\n    \r\n    model = clone(pipe).set_params(**params)\r\n    model.fit(X, y)\r\n\r\n    return model.score(X, y)\r\n    \r\nstudy = create_study(direction='maximize')\r\nstudy.optimize(objective, n_trials=1, n_jobs=-1, show_progress_bar=True)\r\n```\r\n\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "kAIto47802",
        "body": "@vkarakcheev \r\n\r\nThank you for reporting.\r\nHowever, this doesn't seem to be an issue with Optuna itself; it appears to be a problem with scikit-learn. The same error occurs in the following code, which does not using Optuna:\r\n\r\n```py\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\nimport pandas as pd\r\nfrom sklearn import set_config\r\nfrom sklearn.base import clone\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\r\nfrom sklearn.linear_model import ElasticNet\r\n\r\niris = load_iris()\r\nX = pd.DataFrame(iris['data'], columns=iris['feature_names'])\r\ny = pd.Series(iris['target']).rename('iris type')\r\n\r\nset_config(transform_output='pandas')\r\n\r\ntransf_params = dict(\r\n    remainder='passthrough',\r\n    verbose_feature_names_out=False,\r\n    force_int_remainder_cols=False,\r\n)\r\n\r\n# Works only when n_jobs=1\r\nsc1_cols = ['sepal length (cm)', 'sepal width (cm)']\r\nsc2_cols = ['petal length (cm)', 'petal width (cm)']\r\n\r\npipe = Pipeline([\r\n    ('scl', ColumnTransformer([('sc1', StandardScaler(), sc1_cols)], **transf_params)), \r\n    ('sc2', ColumnTransformer([('sc2', MinMaxScaler(), sc2_cols)], **transf_params)), \r\n    ('est', ElasticNet())\r\n])\r\n\r\ndef f():\r\n    params = dict(\r\n        est__alpha=0.01,\r\n        est__l1_ratio=0.5,\r\n    )\r\n    \r\n    model = clone(pipe).set_params(**params)\r\n    model.fit(X, y)\r\n\r\n    return model.score(X, y)\r\n\r\nwith ThreadPoolExecutor(max_workers=8) as executor:\r\n    results = []\r\n    for _ in range(8):\r\n        results.append(executor.submit(f))\r\n\r\nprint(results[0].result())\r\n```\r\n\r\n\r\nAlso, as shown in [this tutorial](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html), Optuna supports parallel optimization by running multiple optimization scripts with an RDB or journal storage.\r\nIf you follow this approach, such errors should not occur."
      }
    ]
  },
  {
    "issue_number": 5299,
    "title": "Improvement to multivariate TPESampler when the union search space is different from the intersection search space",
    "author": "larsbratholm",
    "state": "open",
    "created_at": "2024-03-07T15:10:29Z",
    "updated_at": "2024-09-27T08:31:26Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nDuring hyper-parameter optimization I often want to find the best of a few different methods, all of which can have a different set of parameters. To model this with the multivariate TPESampler we're required to use the group decomposition, which samples each group independently of each other. E.g. in a simple objective like the following:\r\n```\r\ndef objective(trial):\r\n    x = trial.suggest_categorical(\"x\", [\"A\", \"B\"])\r\n    t = trial.suggest_float(\"t\", -2, 2)\r\n    if x == \"A\":\r\n        y = trial.suggest_float(\"y\", 1, 2)\r\n        return (y - t)**2\r\n    else:\r\n        z = trial.suggest_float(\"z\", -2, 1)\r\n        return (z - t)**2\r\n```\r\nthe model will struggle to converge as t is heavily correlated with y and z, but they are modeled independently of each other. This is a very simple example, but highlights some issues with correlation between groups.\n\n### Description\n\nThe cleanest way I could think off to avoid the group decomposition is to replace the intersection search space used in the multivariate TPESampler with a union search space. The internal representation of 'missing' parameters can be set to e.g. 'nan' which is then handled in the ParzenEstimator and _MixtureOfProductDistribution. \r\nDuring sampling, we can just set a parameter in the sample to 'nan' if the active index had 'nan' values as well. In the likelihood estimate for categorical parameters, we can just introduce 'nan' as an additional category. For integers and floats, we can multiply the univariate distribution with a catagorical one, estimating the likelihood of the parameter being sampled or not.\r\nOf course due to priors, this can result in some required parameters not being sampled, but these can be sampled with an independent tpe sampler.\r\n\r\nThis doesn't require huge changes in the code, and shouldn't change anything on the user end. If it performs well across tests, it should be able to replace the default {multivariate=True, group=False} behavior, as the behavior is only different from the current one when the intersection search space is different from the union search space.\n\n### Alternatives (optional)\n\nAlternatively we can continue using the group decomposition, but instead determine the parameter hierarchy in the objective. I.e. in the above example, (x, t) is always sampled, so we can draw n_ei_candidates samples for them first using all trials. Then based on their values, we can either sample y or z from the subset of trials that contain these values. This requires some classifier which can either be given as user input, or be determined automatically by a simple decision tree. Then once all the required parameters have been sampled, we can choose the best sample from their collective likelihoods.\r\nThis only adds in correlation between the parameters that are part of the objective decision tree, but does help the sampler avoid getting stuck in local minima. However, the changes doesn't mesh well with the TPE sampler code, and would likely have to be implemented as a separate sampler.\n\n### Additional context (optional)\n\nI made implementations of both approaches here: https://github.com/larsbratholm/optuna_tpe_suggestion/tree/main\r\nas well as a benchmark on a slightly more complex objective shown here: https://github.com/larsbratholm/optuna_tpe_suggestion/blob/main/run_benchmark.py#L19\r\n\r\nThe geometric mean loss over 128 runs for RandomSampler, TPE, multivariate TPE, the non-group multivariate approach suggested, as well as the alternative hierarchical tpe sampler are shown below\r\n![benchmark](https://github.com/optuna/optuna/assets/2844319/d5aa5fc8-2a55-44e5-9656-997bd269aee1)\r\n\r\nBoth methods perform well on the objective, but the objective features high correlation and a local minima that is easy to get stuck in, so might not be representative overall?\r\n\r\nAre either of these approaches something that would be welcome as a contribution to Optuna? And are there other objectives that have been used to benchmark the group decomposed TPESampler that would be helpful to test on?",
    "comments": [
      {
        "user": "AndreasGerken",
        "body": "Thanks @larsbratholm. I'm struggling with a complex optimization for my PhD since some month and found your post yesterday. It finally works and you helped me to get the optimization stable and to solve the issue! You're awesome for implementing and sharing this sampler. It should definitely be part of the main repository!\r\n\r\nHave a great day, thank you so much! Andi"
      }
    ]
  },
  {
    "issue_number": 5663,
    "title": "GPSampler's suggestion fails when used inside `torch.no_grad()` context manager",
    "author": "kAIto47802",
    "state": "closed",
    "created_at": "2024-09-10T09:26:07Z",
    "updated_at": "2024-09-26T02:08:47Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\n\nCurrently, GPSampler's `sample_relative` fails when used inside `torch.no_grad()` context manager, resulting the default initial kernel params to be used.\r\nSpecifically, the `torch.no_grad()` context manager, when used outside of Optuna, affects the PyTorch gradient calculations within GPSampler and raises a error, causing `sample_relative` to be fail.\r\n\r\nThe expected behavior is:\r\n\r\n- GPSampler work correctly even when  the `torch.no_grad()` is active.\n\n### Environment\n\n- Optuna version: 4.0.0\r\n- Python version: 3.11.0\r\n- OS: MacOS, Ubuntu\r\n- torch: 2.4.0\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\n[I 2024-09-03 07:44:07,293] A new study created in memory with name: no-name-2ddc8765-755f-4bca-8a7a-fd0f3249e467\r\n[I 2024-09-03 07:44:07,322] Trial 0 finished with value: 0.32613587379455566 and parameters: {'n_hidden': 3}. Best is trial 0 with value: 0.32613587379455566.\r\n[W 2024-09-03 07:44:07,560] The optimization of kernel_params failed: element 0 of tensors does not require grad and does not have a grad_fn\r\nThe default initial kernel params will be used instead.\r\n[I 2024-09-03 07:44:07,801] Trial 1 finished with value: -0.08969029039144516 and parameters: {'n_hidden': 10}. Best is trial 1 with value: -0.08969029039144516.\n```\n\n\n### Steps to reproduce\n\nThe following code yields the error above:\r\n\r\n```python\r\nimport optuna\r\nimport torch\r\nfrom torch import nn\r\n\r\nclass Net(nn.Module):\r\n   def __init__(self, n_input, n_output, n_hidden):\r\n       super(Net, self).__init__()\r\n       self.fc1 = nn.Linear(n_input, n_hidden)\r\n       self.fc2 = nn.Linear(n_hidden, n_output)\r\n\r\n   def forward(self, x):\r\n       x = torch.relu(self.fc1(x))\r\n       x = self.fc2(x)\r\n       return x\r\n\r\ndef objective(trial: optuna.Trial) -> float:\r\n   with torch.no_grad():\r\n       n_input = 10\r\n       n_output = 1\r\n       n_hidden = trial.suggest_int('n_hidden', 1, 10)\r\n       net = Net(n_input, n_output, n_hidden)\r\n       x = torch.randn(1, n_input)\r\n       y = net(x)\r\n   return y.item()\r\n\r\nsampler = optuna.samplers.GPSampler(n_startup_trials=1)\r\nstudy = optuna.create_study(sampler=sampler)\r\nstudy.optimize(objective, n_trials=2)\r\n```\r\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "kAIto47802",
        "body": "I will work on this issue."
      },
      {
        "user": "kAIto47802",
        "body": "The cause of the issue is as follows:\r\n\r\nhttps://github.com/optuna/optuna/blob/558f215e5644226fb1a55f09ab4b6ab3d28b57f9/optuna/_gp/gp.py#L175-L194\r\n\r\nIn the `loss_func` within `_fit_kernel_params` function in `optuna/_gp/gp.py`, \r\n\r\nhttps://github.com/optuna/optuna/blob/558f215e5644226fb1a55f09ab4b6ab3d28b57f9/optuna/_gp/gp.py#L177\r\n\r\nis set. \r\n\r\nHowever, inside the `torch.no_grad()` context, operations like `torch.exp(raw_params_tensor[:n_params])` do not result in the output having `requires_grad=True` and the loss\r\n\r\nhttps://github.com/optuna/optuna/blob/558f215e5644226fb1a55f09ab4b6ab3d28b57f9/optuna/_gp/gp.py#L187-L189\r\n\r\ndose not have the gradient information.\r\n\r\nThis leads to a failure in performing backpropagation at\r\n\r\nhttps://github.com/optuna/optuna/blob/558f215e5644226fb1a55f09ab4b6ab3d28b57f9/optuna/_gp/gp.py#L190\r\n\r\n."
      },
      {
        "user": "kAIto47802",
        "body": "One possible solution of this is to set grad enabled with in the `loss_func`:\r\n\r\n```py\r\ndef loss_func(raw_params: np.ndarray) -> tuple[float, np.ndarray]:\r\n    raw_params_tensor = torch.from_numpy(raw_params)\r\n    raw_params_tensor.requires_grad_(True)\r\n    with torch.enable_grad():\r\n        params = KernelParamsTensor(\r\n            inverse_squared_lengthscales=torch.exp(raw_params_tensor[:n_params]),\r\n            kernel_scale=torch.exp(raw_params_tensor[n_params]),\r\n            noise_var=(\r\n                torch.tensor(minimum_noise, dtype=torch.float64)\r\n                if deterministic_objective\r\n                else torch.exp(raw_params_tensor[n_params + 1]) + minimum_noise\r\n            ),\r\n        )\r\n        loss = -marginal_log_likelihood(\r\n            torch.from_numpy(X), torch.from_numpy(Y), torch.from_numpy(is_categorical), params\r\n        ) - log_prior(params)\r\n        loss.backward()  # type: ignore\r\n        # scipy.minimize requires all the gradients to be zero for termination.\r\n        raw_noise_var_grad = raw_params_tensor.grad[n_params + 1]  # type: ignore\r\n        assert not deterministic_objective or raw_noise_var_grad == 0\r\n    return loss.item(), raw_params_tensor.grad.detach().numpy()\r\n```\r\n\r\n> [!NOTE]\r\n> The context manager [`torch.enable_grad()`](https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#enable_grad) preserves whether the mode was originally set to compute gradient information, and when `__exit__()` is called, it restores the original state."
      }
    ]
  },
  {
    "issue_number": 5678,
    "title": "how to identify the brackets resulting from hyperband",
    "author": "solegalli",
    "state": "closed",
    "created_at": "2024-09-20T15:50:20Z",
    "updated_at": "2024-09-20T16:22:40Z",
    "labels": [],
    "body": "### Don't use GitHub Issues to ask support questions.\n\nHey guys! Thank you very much for this awesome library.\r\n\r\nI've got a question regarding hyperband.\r\n\r\nThe number of brackets in hyperband is automatically defined based on **min_resource** and **max_resource**.\r\n\r\nFrom the original paper, if maximum resource is 81 and minimum resource is 1, the resource being an epoch, then there will be 5 brackets. The first one with 81 initial configurations, the second with 27, the third with 9 and then 6 and 5, so that the last one, which is randomized search takes place on 5 configurations.\r\n\r\nBut in Optuna, there is one additional parameter that we pass to the `optimize` function: the `n_trials`. \r\n\r\nSo if I set this:\r\n\r\n```\r\nstudy = optuna.create_study(\r\n    sampler=optuna.samplers.RandomSampler(),\r\n    \r\n    pruner=optuna.pruners.HyperbandPruner(\r\n        min_resource=1,\r\n        max_resource=81,\r\n        reduction_factor=3,\r\n    ),\r\n    \r\n    direction=\"maximize\",\r\n\r\n)\r\n\r\nstudy.optimize(objective, n_trials=30)\r\n```\r\n\r\nwhich I believe is equivalent to the original paper, except for the 30 trials, I don't get the expected results, probably because of the n_trials. Is my reasoning correct?\r\n\r\nI expected 81+27+9+6=5, but instead I got 30 evaluations.\r\n\r\nHow does n_trials interact with min and max resource to set the brackets and the initial configurations per brackets? and should I then calculate the n_trials to more or less examine as many configurations as I think I need?\r\n\r\nAnd the final question: how do I identify the brackets in `study.trials_dataframe()`? What I did is the following:\r\n\r\n```\r\nv = [v for v in r.columns if 'rung' in v]\r\n\r\n30-r[v].isnull().sum()\r\n```\r\n\r\nWhich resulted in this:\r\n\r\n```\r\nsystem_attrs_completed_rung_0    26\r\nsystem_attrs_completed_rung_1    10\r\nsystem_attrs_completed_rung_2     2\r\ndtype: int64\r\n```\r\n and then I concluded that 4 brackets were examined. Is that so?",
    "comments": []
  },
  {
    "issue_number": 5642,
    "title": "GPSampler fails with `Bracketing values (xa, xb, xc) do not fulfill this requirement: (f(xb) < f(xa)) and (f(xb) < f(xc))`",
    "author": "cartisan",
    "state": "open",
    "created_at": "2024-08-27T08:15:35Z",
    "updated_at": "2024-09-17T13:18:40Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nI'm trying out the GPSampler to hyperopt my ML pipeline, which is basically a non-deterministic, multivariate objective with real, integer and categorical parameters, and a conditional search space. The TPSampler can handle this space fine. With the GPSampler, I occasionally (8 out of 100 runs) get the error detailed in the stacktrace below. It occurs several trials in, sometimes 20 sometimes 40.\r\n\r\nIf I understand this error message correctly, it means that the GPSampler fails to find a useful minimum in the sampling region that it could use to explore. In such a case I would expect it to issue a warning and proceed the sampling with randomly selected parameters, instead of erroring out.\r\n\r\n### Environment\r\n\r\n- Optuna version: 3.6.1\r\n- Python version: 3.11.4\r\n- OS: Linux-6.8.0-40-generic-x86_64-with-glibc2.35\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/srv/my_code/__main__.py\", line 469, in <module>\r\n    fire.Fire(CLI)\r\n  File \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/srv/my_code/__main__.py\", line 185, in endpoint_forecast_hpo\r\n    performance, runtime = hpo.hpo_endpoint(\r\n                           ^^^^^^^^^^^^^^^^^\r\n  File \"/srv/my_code/routines/forecast/hpo.py\", line 313, in hpo_endpoint\r\n    hpo_result = core_hpo_api.optimize_hyperparameters(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/srv/my_code/core/api/hpo.py\", line 122, in optimize_forecast_model_schema_hyperparameters\r\n    optimization_result, optimized_params, aborted = hyperparameter_optimizer.optimize(\r\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/srv/my_code/core/hpo/hyperopt.py\", line 724, in optimize\r\n    self.study.optimize(\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/study/study.py\", line 451, in optimize\r\n    _optimize(\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 62, in _optimize\r\n    _optimize_sequential(\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 159, in _optimize_sequential\r\n    frozen_trial = _run_trial(study, func, catch)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 247, in _run_trial\r\n    raise func_err\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\r\n    value_or_values = func(trial)\r\n                      ^^^^^^^^^^^\r\n  File \"/srv/my_code/core/hpo/hyperopt.py\", line 568, in objective\r\n    params = self.space.suggest_values(trial)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/srv/my_code/core/hpo/space.py\", line 315, in suggest_values\r\n    suggested_values[parameter.name] = parameter.suggest_value(trial)\r\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/srv/my_code/core/hpo/space.py\", line 69, in suggest_value\r\n    return trial.suggest_int(\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_convert_positional_args.py\", line 83, in converter_wrapper\r\n    return func(**kwargs)\r\n           ^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 326, in suggest_int\r\n    suggested_value = int(self._suggest(name, distribution))\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 625, in _suggest\r\n    elif self._is_relative_param(name, distribution):\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 657, in _is_relative_param\r\n    if name not in self.relative_params:\r\n                   ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 73, in relative_params\r\n    self._relative_params = self.study.sampler.sample_relative(\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/samplers/_gp/sampler.py\", line 197, in sample_relative\r\n    normalized_param = self._optimize_acqf(\r\n                       ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/samplers/_gp/sampler.py\", line 124, in _optimize_acqf\r\n    normalized_params, _acqf_val = optim_mixed.optimize_acqf_mixed(\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_gp/optim_mixed.py\", line 316, in optimize_acqf_mixed\r\n    x, f = local_search_mixed(acqf_params, x_warmstart, tol=tol)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_gp/optim_mixed.py\", line 255, in local_search_mixed\r\n    (best_normalized_params, best_fval, updated) = _local_search_discrete(\r\n                                                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_gp/optim_mixed.py\", line 182, in _local_search_discrete\r\n    return _discrete_line_search(\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_gp/optim_mixed.py\", line 146, in _discrete_line_search\r\n    res = so.minimize_scalar(\r\n          ^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/scipy/optimize/_minimize.py\", line 908, in minimize_scalar\r\n    return _minimize_scalar_brent(fun, bracket, args, **options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/scipy/optimize/_optimize.py\", line 2606, in _minimize_scalar_brent\r\n    brent.optimize()\r\n  File \"/usr/local/lib/python3.11/site-packages/scipy/optimize/_optimize.py\", line 2377, in optimize\r\n    xa, xb, xc, fa, fb, fc, funcalls = self.get_bracket_info()\r\n                                       ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/scipy/optimize/_optimize.py\", line 2361, in get_bracket_info\r\n    raise ValueError(\r\nValueError: Bracketing values (xa, xb, xc) do not fulfill this requirement: (f(xb) < f(xa)) and (f(xb) < f(xc))\r\n```\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n```python\r\n# python code\r\n```\r\n\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "@cartisan Would you be able to give me the logging file when it fails?\r\n\r\nAn example of the logging:\r\n\r\n```python\r\n[I 2024-08-27 10:29:31,604] Trial 0 finished with value: 0.022523642746275215 and parameters: {'x': 0.15007878846217815}. Best is trial 0 with value: 0.022523642746275215.\r\n[I 2024-08-27 10:29:31,604] Trial 1 finished with value: 0.017645641479613693 and parameters: {'x': -0.13283689803519838}. Best is trial 1 with value: 0.017645641479613693.\r\n[I 2024-08-27 10:29:31,605] Trial 2 finished with value: 0.7218967899776574 and parameters: {'x': -0.84964509648303}. Best is trial 1 with value: 0.017645641479613693.\r\n[I 2024-08-27 10:29:31,605] Trial 3 finished with value: 0.007185638603422745 and parameters: {'x': 0.08476814616011574}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,605] Trial 4 finished with value: 0.7828524861224834 and parameters: {'x': 0.8847895151517582}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,605] Trial 5 finished with value: 0.13334149750729005 and parameters: {'x': -0.3651595507545846}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,606] Trial 6 finished with value: 0.6648832396626668 and parameters: {'x': -0.8154037280161692}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,606] Trial 7 finished with value: 0.21869417969985172 and parameters: {'x': -0.46764749512838377}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,606] Trial 8 finished with value: 0.0750729906695263 and parameters: {'x': -0.2739945084660025}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,606] Trial 9 finished with value: 0.06740440122039634 and parameters: {'x': -0.25962357601033914}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,642] Trial 10 finished with value: 0.10032036985659154 and parameters: {'x': 0.316733910177915}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,646] Trial 11 finished with value: 0.1491831039571356 and parameters: {'x': 0.3862422865988855}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,648] Trial 12 finished with value: 0.3775959539261121 and parameters: {'x': 0.6144883676084618}. Best is trial 3 with value: 0.007185638603422745.\r\n[I 2024-08-27 10:29:31,650] Trial 13 finished with value: 0.00021373309259190406 and parameters: {'x': -0.014619613284622274}. Best is trial 13 with value: 0.00021373309259190406.\r\n[I 2024-08-27 10:29:31,652] Trial 14 finished with value: 0.0023603741712872888 and parameters: {'x': 0.048583682150360824}. Best is trial 13 with value: 0.00021373309259190406.\r\n[I 2024-08-27 10:29:31,653] Trial 15 finished with value: 0.2591613183742695 and parameters: {'x': 0.5090788920926397}. Best is trial 13 with value: 0.00021373309259190406.\r\n[I 2024-08-27 10:29:31,655] Trial 16 finished with value: 0.33341433350885435 and parameters: {'x': -0.5774204131383427}. Best is trial 13 with value: 0.00021373309259190406.\r\n[I 2024-08-27 10:29:31,657] Trial 17 finished with value: 0.015842014885353188 and parameters: {'x': 0.12586506618340607}. Best is trial 13 with value: 0.00021373309259190406.\r\n[I 2024-08-27 10:29:31,658] Trial 18 finished with value: 0.006416665432784487 and parameters: {'x': -0.0801040912362439}. Best is trial 13 with value: 0.00021373309259190406.\r\n[I 2024-08-27 10:29:31,660] Trial 19 finished with value: 0.6675181140945129 and parameters: {'x': 0.8170178174890147}. Best is trial 13 with value: 0.00021373309259190406.\r\n\r\n\r\n```\r\n\r\n`GPSampler` may fail when your pipeline returns an infinite value.\r\n\r\nhttps://github.com/optuna/optuna/blob/ff309dc17c5da47d515ee7ce3a84dce9db85011c/optuna/_gp/optim_mixed.py#L146-L153"
      },
      {
        "user": "cartisan",
        "body": "@nabenabe0928 Here is one of the logs, is it sufficient for you with the parameters removed? Let me know if not, then I'll come up with something to anonymize them.\r\n\r\n```\r\n[I 2024-08-23 08:26:14,720] Trial 0 finished with value: 1.9982947150455423 and parameters: \r\n[I 2024-08-23 08:29:45,532] Trial 1 finished with value: 2.2420623914856654 and parameters: \r\n[I 2024-08-23 08:37:36,720] Trial 2 finished with value: 2.326639955969506 and parameters: \r\n[I 2024-08-23 08:43:27,348] Trial 3 finished with value: 2.0651854373827176 and parameters:\r\n[I 2024-08-23 08:46:32,486] Trial 4 finished with value: 2.383566288021314 and parameters:\r\n[I 2024-08-23 08:53:05,770] Trial 5 finished with value: 2.508985525963295 and parameters:\r\n[I 2024-08-23 08:56:35,932] Trial 6 finished with value: 2.098528531761311 and parameters:\r\n[I 2024-08-23 09:00:08,452] Trial 7 finished with value: 2.283702573966803 and parameters: \r\n[I 2024-08-23 09:09:00,922] Trial 8 finished with value: 2.1517287510657197 and parameters: \r\n[I 2024-08-23 09:16:38,362] Trial 9 finished with value: 2.7528310201324624 and parameters: \r\n[I 2024-08-23 09:23:03,584] Trial 10 finished with value: 2.653029505115957 and parameters: \r\n[I 2024-08-23 09:30:38,656] Trial 11 finished with value: 3.2037251275107503 and parameters: \r\n[I 2024-08-23 09:38:18,222] Trial 12 finished with value: 2.0671085864362775 and parameters:\r\n[I 2024-08-23 09:41:39,484] Trial 13 finished with value: 2.304300354526567 and parameters:\r\n[I 2024-08-23 09:47:36,295] Trial 14 finished with value: 2.173515679199308 and parameters:\r\n[I 2024-08-23 09:51:15,240] Trial 15 finished with value: 2.06825637990649 and parameters: \r\n[I 2024-08-23 09:58:02,241] Trial 16 finished with value: 2.086730780240595 and parameters:\r\n[I 2024-08-23 10:05:00,451] Trial 17 finished with value: 2.0045831919948935 and parameters:\r\n[I 2024-08-23 10:11:32,204] Trial 18 finished with value: 2.0093571965728905 and parameters:\r\n[I 2024-08-23 10:18:53,263] Trial 19 finished with value: 2.1993706852480917 and parameters:\r\n[I 2024-08-23 10:24:06,403] Trial 20 finished with value: 2.3019584236760267 and parameters:\r\n[I 2024-08-23 10:26:37,552] Trial 21 finished with value: 2.391499456083707 and parameters:\r\n[I 2024-08-23 10:32:45,623] Trial 22 finished with value: 1.997459736879597 and parameters:\r\n[I 2024-08-23 10:39:10,926] Trial 23 finished with value: 2.0282148430774956 and parameters:\r\n[I 2024-08-23 10:46:23,122] Trial 24 finished with value: 2.199803939215562 and parameters: \r\n[I 2024-08-23 10:53:30,931] Trial 25 finished with value: 2.110191834067339 and parameters:\r\n[I 2024-08-23 11:00:12,324] Trial 26 finished with value: 2.1454411250908088 and parameters:\r\n[I 2024-08-23 11:06:25,749] Trial 27 finished with value: 2.066355047736335 and parameters:\r\n[I 2024-08-23 11:12:45,052] Trial 28 finished with value: 2.0196136377188942 and parameters:\r\n[I 2024-08-23 11:19:58,945] Trial 29 finished with value: 2.191075805928475 and parameters:\r\n[W 2024-08-23 11:19:59,947] Trial 30 failed with parameters: {} because of the following error: ValueError('Bracketing values (xa, xb, xc) do not fulfill this requirement: (f(xb) < f(xa)) and (f(xb) < f(xc))').\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\r\n    value_or_values = func(trial)\r\n                      ^^^^^^^^^^^\r\n  File \"/srv/prism_ada/core/hpo/hyperopt.py\", line 568, in objective\r\n    params = self.space.suggest_values(trial)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/srv/prism_ada/core/hpo/space.py\", line 315, in suggest_values\r\n    suggested_values[parameter.name] = parameter.suggest_value(trial)\r\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/srv/prism_ada/core/hpo/space.py\", line 69, in suggest_value\r\n    return trial.suggest_int(\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_convert_positional_args.py\", line 83, in converter_wrapper\r\n    return func(**kwargs)\r\n           ^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 326, in suggest_int\r\n    suggested_value = int(self._suggest(name, distribution))\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 625, in _suggest\r\n    elif self._is_relative_param(name, distribution):\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 657, in _is_relative_param\r\n    if name not in self.relative_params:\r\n                   ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/trial/_trial.py\", line 73, in relative_params\r\n    self._relative_params = self.study.sampler.sample_relative(\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/samplers/_gp/sampler.py\", line 197, in sample_relative\r\n    normalized_param = self._optimize_acqf(\r\n                       ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/samplers/_gp/sampler.py\", line 124, in _optimize_acqf\r\n    normalized_params, _acqf_val = optim_mixed.optimize_acqf_mixed(\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_gp/optim_mixed.py\", line 316, in optimize_acqf_mixed\r\n    x, f = local_search_mixed(acqf_params, x_warmstart, tol=tol)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_gp/optim_mixed.py\", line 255, in local_search_mixed\r\n    (best_normalized_params, best_fval, updated) = _local_search_discrete(\r\n                                                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_gp/optim_mixed.py\", line 182, in _local_search_discrete\r\n    return _discrete_line_search(\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/optuna/_gp/optim_mixed.py\", line 146, in _discrete_line_search\r\n    res = so.minimize_scalar(\r\n          ^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/scipy/optimize/_minimize.py\", line 908, in minimize_scalar\r\n    return _minimize_scalar_brent(fun, bracket, args, **options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/scipy/optimize/_optimize.py\", line 2606, in _minimize_scalar_brent\r\n    brent.optimize()\r\n  File \"/usr/local/lib/python3.11/site-packages/scipy/optimize/_optimize.py\", line 2377, in optimize\r\n    xa, xb, xc, fa, fb, fc, funcalls = self.get_bracket_info()\r\n                                       ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/scipy/optimize/_optimize.py\", line 2361, in get_bracket_info\r\n    raise ValueError(\r\nValueError: Bracketing values (xa, xb, xc) do not fulfill this requirement: (f(xb) < f(xa)) and (f(xb) < f(xc))\r\n[W 2024-08-23 11:19:59,950] Trial 30 failed with value None.\r\n```\r\nMy pipeline should never return an infinite value as its training loss, I use a fail value of 9999 for failure cases, so that would be the maximum the objective function returns."
      },
      {
        "user": "nabenabe0928",
        "body": "@cartisan \r\nIt is hard to reproduce your error only with the objective values, so it would be very helpful if you could provide me with your anonymized search space and its suggested parameters."
      }
    ]
  },
  {
    "issue_number": 4354,
    "title": "Enrich CLI documentation",
    "author": "cross32768",
    "state": "closed",
    "created_at": "2023-01-20T06:08:13Z",
    "updated_at": "2024-09-16T23:05:34Z",
    "labels": [
      "document",
      "stale"
    ],
    "body": "### What is an issue?\n\nIn optuna v3.0.0, `optuna.cli` page has rich documentation: https://optuna.readthedocs.io/en/v3.0.0/reference/cli.html .\r\nHowever, in optuna v3.1.0, `optuna.cli` page only has induction to `optuna help` command: https://optuna.readthedocs.io/en/stable/reference/cli.html .\r\n\r\nThis is a side effect of removing dependency to cliff in PR https://github.com/optuna/optuna/pull/4100 .\r\nI propose to enrich CLI documentation again.\r\n\r\nI consider to write CLI documentation by hand and create PR.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5125,
    "title": "Parallelize GPU without a database, just use the .txt file you save trials to.",
    "author": "luke-mcdermott-mi",
    "state": "closed",
    "created_at": "2023-11-19T01:22:43Z",
    "updated_at": "2024-09-15T23:05:46Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\n\nGPU parallelization is not simple. I cannot set up a sql database on my cluster as I do not have admin access, and there is no genuine reason why I should have to create a database to store simple information thats for search.\n\n### Description\n\nInstead of a database, just use a .txt file that holds the hyperparameters + any metrics you need. \r\n\r\n```\r\nwith open(\"./optuna_trials.txt\", \"a\") as file:\r\n        file.write(f\"Trial {trial.number}, Mean Distance: {mean_distance}, BOPs: {bops}, Inference time: {inference_time}, Validation Loss: {validation_loss}, Param Count: {param_count}, Hyperparams: {trial.params}\\n\")\r\n```\r\n    \r\nI use this to save my results. I can reload certain trials just by using trial.params. Why is not used to parallelize/sync GPUs?\r\n\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Did you check https://optuna.readthedocs.io/en/latest/reference/generated/optuna.storages.JournalStorage.html?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 372,
    "title": "Dynamic Value space !",
    "author": "Enderdead",
    "state": "closed",
    "created_at": "2019-04-01T15:39:46Z",
    "updated_at": "2024-09-14T02:30:46Z",
    "labels": [],
    "body": "Why don't support dynamic value space if parameters have the same type ?\r\nIt can be usefull  if I need a distribution depending on other dynamic parameter.\r\n\r\n",
    "comments": [
      {
        "user": "sile",
        "body": "Except categorical parameters, we already support dynamic value space. An example is found in [examples/lightgbm_simpl.py](https://github.com/pfnet/optuna/blob/master/examples/lightgbm_simple.py#L50)."
      },
      {
        "user": "Enderdead",
        "body": "I'm sorry but in your example you just have different generated arguments. \r\nExample :\r\n```\r\nif param['optimizer'] == 'SGD':\r\n        param['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\r\n\r\nif param['optimizer'] == 'Adam':\r\n        param['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-4, 1.0)\r\n\r\n ```\r\nIt's impossible to do that and I don't understand why ?"
      },
      {
        "user": "sile",
        "body": "Sorry for the lack of explanation.\r\n[examples/lightgbm_simple.py](https://github.com/pfnet/optuna/blob/master/examples/lightgbm_simple.py#L50) contains the following code and the value of  `high` argument of `trial.suggest_uniform('other_rate', ...)` changes dynamically during a study:\r\n\r\n\r\n```python\r\nif param['boosting_type'] == 'goss':\r\n   param['top_rate'] = trial.suggest_uniform('top_rate', 0.0, 1.0)\r\n   param['other_rate'] = trial.suggest_uniform('other_rate', 0.0, 1.0 - param['top_rate'])\r\n```\r\n\r\nAs well, I believe the example code you wrote in the above comment can work.\r\n(If you have errors when executing the code, it would be appreciated if you could let me know the detailed error log)"
      }
    ]
  },
  {
    "issue_number": 5119,
    "title": "Clarifying the documentation of the step parameter in the report method",
    "author": "wtld",
    "state": "closed",
    "created_at": "2023-11-17T09:00:37Z",
    "updated_at": "2024-09-12T23:06:45Z",
    "labels": [
      "document",
      "stale"
    ],
    "body": "The documentation (https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.report) does not indicate that reporting of intermediate results must be continuous or not? Can I, for example, report steps 10, 20, 30, etc.? Can I report steps 10, 20, 30, etc. in one trial and 5, 10, 15, 20, 25 in another? Will pruners (especially MedianPruner) work correctly in this case?",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5667,
    "title": "How to determine the best number of trials?",
    "author": "kkckk1110",
    "state": "closed",
    "created_at": "2024-09-12T01:25:19Z",
    "updated_at": "2024-09-12T04:21:32Z",
    "labels": [],
    "body": "### Don't use GitHub Issues to ask support questions.\n\nHello, I am using Optuna to optimize the parameters and find it really helpful. However, I have a question that how to determine the best number of trials to search? I found that, a larger number of trials can reduce error overall but may subject to overfitting problems. But the results depend on the dataset and do not show a universal rule. Are there any golden rules or recommendations I can apply to finetune the \"trial\" parameter? Thanks a lot.",
    "comments": []
  },
  {
    "issue_number": 5280,
    "title": "Fixing potential unbound local variable in `Study._optimize_sequential`",
    "author": "lumip",
    "state": "open",
    "created_at": "2024-02-26T19:03:15Z",
    "updated_at": "2024-09-11T02:37:12Z",
    "labels": [
      "bug"
    ],
    "body": "### Motivation\r\n\r\n`Study._optimize_sequential` encounters an issue if `_run_trial` errors and callbacks are present, as seen in the following relevant code block:\r\n\r\n```python\r\n        try:\r\n            frozen_trial = _run_trial(study, func, catch)\r\n        finally:\r\n            # The following line mitigates memory problems that can be occurred in some\r\n            # environments (e.g., services that use computing containers such as GitHub Actions).\r\n            # Please refer to the following PR for further details:\r\n            # https://github.com/optuna/optuna/pull/325.\r\n            if gc_after_trial:\r\n                gc.collect()\r\n\r\n        if callbacks is not None:\r\n            for callback in callbacks:\r\n                callback(study, frozen_trial)\r\n```\r\n\r\nIf `_run_trial` raises, `frozen_trial` is never set, leading to an `UnboundLocalError: local variable 'frozen_trial' referenced before assignment` error.\r\n\r\n### Suggestion\r\n\r\nConsider moving the callback loop into the try block? Not sure if there might be some side effects in moving this before the garbage collection, however.\r\n\r\n```python\r\n        try:\r\n            frozen_trial = _run_trial(study, func, catch)\r\n    \r\n            if callbacks is not None:\r\n                for callback in callbacks:\r\n                    callback(study, frozen_trial)\r\n        finally:\r\n            # The following line mitigates memory problems that can be occurred in some\r\n            # environments (e.g., services that use computing containers such as GitHub Actions).\r\n            # Please refer to the following PR for further details:\r\n            # https://github.com/optuna/optuna/pull/325.\r\n            if gc_after_trial:\r\n                gc.collect()\r\n```\r\n\r\n### Additional context (optional)\r\n\r\nThis really is a bug report, I saw this as a cascading error after the file based journal encountered a `json.decoder.JSONDecodeError`. However, the bug report template is asking for so many details irrelevant to this that filling this template is the more feasible alternative.",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Could you share the minimal reproducible code to see the error with us?"
      },
      {
        "user": "lumip",
        "body": "I don't have anything minimal right now but I'll try to come up with something"
      },
      {
        "user": "lumip",
        "body": "It seems I was hasty with the above code change suggestion: when `_run_trial` does encounter an error/exception, it would of course raise out of the `_optimize_sequential` call as well and never access the `frozen_trial`, so the fix suggested above wouldn't achieve anything. In fact, it's not where the error occurred and I don't know anymore why I was looking at that spot in the code for a fix - I guess I was just being stupid.\r\n\r\nHere is the relevant snippet from the logs of one of my failing runs\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"pyenv/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 220, in _run_trial\r\n    frozen_trial = study._storage.get_trial(trial._trial_id)\r\n  File \"pyenv/lib/python3.9/site-packages/optuna/storages/_journal/storage.py\", line 369, in get_trial\r\n    self._sync_with_backend()\r\n  File \"pyenv/lib/python3.9/site-packages/optuna/storages/_journal/storage.py\", line 149, in _sync_with_backend\r\n    logs = self._backend.read_logs(self._replay_result.log_number_read)\r\n  File \"pyenv/lib/python3.9/site-packages/optuna/storages/_journal/file.py\", line 177, in read_logs\r\n    raise last_decode_error\r\n  File \"pyenv/lib/python3.9/site-packages/optuna/storages/_journal/file.py\", line 191, in read_logs\r\n    logs.append(json.loads(line))\r\n  File \"pyenv/lib/python3.9/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"pyenv/lib/python3.9/json/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"pyenv/lib/python3.9/json/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"search_hyperparams.py\", line 179, in <module>\r\n    study.optimize(trial_inference, n_trials=args.num_trials, gc_after_trial=True)\r\n  File \"pyenv/lib/python3.9/site-packages/optuna/study/study.py\", line 451, in optimize\r\n    _optimize(\r\n  File \"pyenv/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 66, in _optimize\r\n    _optimize_sequential(\r\n  File \"pyenv/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 163, in _optimize_sequential\r\n    frozen_trial = _run_trial(study, func, catch)\r\n  File \"pyenv/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 223, in _run_trial\r\n    if frozen_trial.state == TrialState.COMPLETE:\r\nUnboundLocalError: local variable 'frozen_trial' referenced before assignment\r\n```\r\n\r\nThe error originates within `JournalStorage._sync_with_backend`, probably due to file system issues on the cluster I was running this, and can be reproduced with the following code:\r\n\r\n```python\r\nfrom typing import Optional, Sequence\r\nimport optuna\r\nfrom json import JSONDecodeError\r\nfrom optuna.storages._journal.base import BaseJournalLogStorage\r\n\r\nfrom optuna.trial import TrialState\r\n\r\ndef trial_fn(trial: optuna.Trial):\r\n    foo = trial.suggest_float(\"foo\", 0., 1.)\r\n    return foo\r\n    \r\nclass ErroneousJournalStorage(optuna.storages.JournalStorage):\r\n\r\n    def __init__(self, log_storage: BaseJournalLogStorage) -> None:\r\n        self.trial_state_values_set = False\r\n        super().__init__(log_storage)\r\n\r\n    def set_trial_state_values(self, trial_id: int, state: TrialState, values: Optional[Sequence[float]] = None) -> bool:\r\n        self.trial_state_values_set = True\r\n        return super().set_trial_state_values(trial_id, state, values)\r\n\r\n    def _sync_with_backend(self) -> None:\r\n        if self.trial_state_values_set:\r\n            raise JSONDecodeError(\"some json error\", \"foo\", 0)\r\n        return super()._sync_with_backend()\r\n\r\n\r\nsampler = optuna.samplers.TPESampler()\r\nstorage = ErroneousJournalStorage(\r\n    optuna.storages.JournalFileStorage(\"foo\"),\r\n)\r\n\r\nstudy = optuna.create_study(storage=storage, sampler=sampler)\r\nstudy.optimize(trial_fn, n_trials=100, gc_after_trial=True)\r\n```\r\n\r\n### Related problem\r\nAnother issue I was seeing appears to be when `set_trial_state_values` itself fails (instead of the `_sync_with_backend` call after it). In that case the completed trial is never written and the block\r\n```python\r\n    try:\r\n        frozen_trial = _tell_with_warning(\r\n            study=study,\r\n            trial=trial,\r\n            value_or_values=value_or_values,\r\n            state=state,\r\n            suppress_warning=True,\r\n        )\r\n    except Exception:\r\n        frozen_trial = study._storage.get_trial(trial._trial_id)\r\n        raise\r\n```\r\nin `_run_trial` reloads it from the journal with state `RUNNING`. This causes the if statements in the following `finally` block to fall into the `Should not reach` else-branch (in line 243,244), where the error is not handled, and therefore results in a crash.\r\n\r\nThis can be reproduced by using the above code with a modified \r\n```python\r\nclass ErroneousJournalStorage(optuna.storages.JournalStorage):\r\n\r\n    def set_trial_state_values(self, trial_id: int, state: TrialState, values: Optional[Sequence[float]] = None) -> bool:\r\n        raise JSONDecodeError(\"some json error\", \"foo\", 0)\r\n\r\n```"
      }
    ]
  },
  {
    "issue_number": 3522,
    "title": "Parallelization using Mysql with the error of “\"Can't connect to local MySQL server through socket '/tmp/mysql.sock'”",
    "author": "201520906050",
    "state": "closed",
    "created_at": "2022-04-28T12:02:41Z",
    "updated_at": "2024-09-10T08:10:22Z",
    "labels": [
      "question"
    ],
    "body": "### Environment\r\n\r\n- Optuna version: 2.10.0\r\n- Python version: 3.7.12\r\n- OS: Linux-5.4.0-109-generic-x86_64-with-debian-buster-sid\r\n- (Optional) Other libraries and their versions: stabe-baseline3,  highway-env, mysql  Ver 14.14 Distrib 5.7.37, for Linux (x86_64) using  EditLine wrapper\r\n\r\n\r\n### Description\r\n\r\n Glad to see such an extraordinary awesome tools for tuning hyperparameter ! I followed the tutorial [https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html](url) and tried the easy parallelization, but either I run my code through PyCharm or terminal output the error：\r\n`sqlalchemy.exc.OperationalError: (MySQLdb._exceptions.OperationalError) (2002, \"Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)\")\r\n`\r\nI installed Mysql through`sudo apt update`and`sudo apt install mysql-server`and Mysql worked fine, so i'm confused whether it's the problem of Optuna or Mysql or even something else. I pasted my whole code here for you to reproduce\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\n/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/ale_py/roms/__init__.py:94: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\r\n  _RESOLVED_ROMS = _resolve_roms()\r\n\r\nBad key \"text.kerning_factor\" on line 4 in\r\n/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\r\nYou probably need to get an updated matplotlibrc file from\r\nhttp://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\r\nor from the matplotlib source distribution\r\nTraceback (most recent call last):\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 3250, in _wrap_pool_connect\r\n    return fn()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 310, in connect\r\n    return _ConnectionFairy._checkout(self)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 868, in _checkout\r\n    fairy = _ConnectionRecord.checkout(pool)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 476, in checkout\r\n    rec = pool._do_get()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/impl.py\", line 146, in _do_get\r\n    self._dec_overflow()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py\", line 72, in __exit__\r\n    with_traceback=exc_tb,\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/util/compat.py\", line 207, in raise_\r\n    raise exception\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/impl.py\", line 143, in _do_get\r\n    return self._create_connection()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 256, in _create_connection\r\n    return _ConnectionRecord(self)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 371, in __init__\r\n    self.__connect()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 666, in __connect\r\n    pool.logger.debug(\"Error on connect(): %s\", e)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py\", line 72, in __exit__\r\n    with_traceback=exc_tb,\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/util/compat.py\", line 207, in raise_\r\n    raise exception\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 661, in __connect\r\n    self.dbapi_connection = connection = pool._invoke_creator(self)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/create.py\", line 590, in connect\r\n    return dialect.connect(*cargs, **cparams)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 597, in connect\r\n    return self.dbapi.connect(*cargs, **cparams)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/MySQLdb/__init__.py\", line 130, in Connect\r\n    return Connection(*args, **kwargs)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/MySQLdb/connections.py\", line 185, in __init__\r\n    super().__init__(*args, **kwargs2)\r\nMySQLdb._exceptions.OperationalError: (2002, \"Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)\")\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"dqn_highway_optuna.py\", line 197, in <module>\r\n    storage=\"mysql://root@localhost/optuna\")\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/optuna/study/study.py\", line 1136, in create_study\r\n    storage = storages.get_storage(storage)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/optuna/storages/__init__.py\", line 31, in get_storage\r\n    return _CachedStorage(RDBStorage(storage))\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/optuna/storages/_rdb/storage.py\", line 183, in __init__\r\n    models.BaseModel.metadata.create_all(self.engine)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/sql/schema.py\", line 4786, in create_all\r\n    ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 3116, in _run_ddl_visitor\r\n    with self.begin() as conn:\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 3032, in begin\r\n    conn = self.connect(close_with_result=close_with_result)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 3204, in connect\r\n    return self._connection_cls(self, close_with_result=close_with_result)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 96, in __init__\r\n    else engine.raw_connection()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 3283, in raw_connection\r\n    return self._wrap_pool_connect(self.pool.connect, _connection)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 3254, in _wrap_pool_connect\r\n    e, dialect, self\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 2101, in _handle_dbapi_exception_noconnection\r\n    sqlalchemy_exception, with_traceback=exc_info[2], from_=e\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/util/compat.py\", line 207, in raise_\r\n    raise exception\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 3250, in _wrap_pool_connect\r\n    return fn()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 310, in connect\r\n    return _ConnectionFairy._checkout(self)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 868, in _checkout\r\n    fairy = _ConnectionRecord.checkout(pool)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 476, in checkout\r\n    rec = pool._do_get()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/impl.py\", line 146, in _do_get\r\n    self._dec_overflow()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py\", line 72, in __exit__\r\n    with_traceback=exc_tb,\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/util/compat.py\", line 207, in raise_\r\n    raise exception\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/impl.py\", line 143, in _do_get\r\n    return self._create_connection()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 256, in _create_connection\r\n    return _ConnectionRecord(self)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 371, in __init__\r\n    self.__connect()\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 666, in __connect\r\n    pool.logger.debug(\"Error on connect(): %s\", e)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py\", line 72, in __exit__\r\n    with_traceback=exc_tb,\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/util/compat.py\", line 207, in raise_\r\n    raise exception\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 661, in __connect\r\n    self.dbapi_connection = connection = pool._invoke_creator(self)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/create.py\", line 590, in connect\r\n    return dialect.connect(*cargs, **cparams)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 597, in connect\r\n    return self.dbapi.connect(*cargs, **cparams)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/MySQLdb/__init__.py\", line 130, in Connect\r\n    return Connection(*args, **kwargs)\r\n  File \"/home/pami/anaconda3/envs/RL/lib/python3.7/site-packages/MySQLdb/connections.py\", line 185, in __init__\r\n    super().__init__(*args, **kwargs2)\r\nsqlalchemy.exc.OperationalError: (MySQLdb._exceptions.OperationalError) (2002, \"Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)\")\r\n(Background on this error at: https://sqlalche.me/e/14/e3q8)\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\n1. You can either run this code through IDE or terminal.\r\n2. stable-baseline3 is a framework of reinforcement learning, you can either install it or not since this is not the point of error.\r\n3. highway-env is a gym environment.\r\n```python\r\n\"\"\" Optuna example that optimizes the hyperparameters of\r\na reinforcement learning agent using DQN implementation from Stable-Baselines3\r\non highway-environment： highway-v0 scene .\r\nThis is a simplified version of what can be found in https://github.com/DLR-RM/rl-baselines3-zoo.\r\n\r\n\r\nfrom typing import Any\r\nfrom typing import Dict\r\n\r\nimport gym\r\nimport optuna\r\nfrom optuna.pruners import HyperbandPruner, MedianPruner\r\nfrom optuna.samplers import TPESampler\r\nfrom stable_baselines3 import DQN\r\nfrom stable_baselines3.common.callbacks import EvalCallback\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nfrom stable_baselines3.common.callbacks import CallbackList\r\nfrom stable_baselines3.common.monitor import Monitor\r\nfrom stable_baselines3.common.vec_env import DummyVecEnv\r\nfrom tqdm import tqdm, trange\r\nfrom optuna.visualization import plot_slice\r\n\r\nimport highway_env\r\n\r\nN_TRIALS = 100\r\nN_STARTUP_TRIALS = 5\r\nN_EVALUATIONS = 2\r\nN_TIMESTEPS = int(4e4)\r\nEVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\r\nN_EVAL_EPISODES = 3\r\n\r\nENV_ID = \"CartPole-v1\"\r\n\r\nDEFAULT_HYPERPARAMS = {\r\n    \"policy\": \"MlpPolicy\",\r\n    \"env\": ENV_ID,\r\n}\r\n\r\nlog_dir = \"../dqn_log/\"\r\n\r\n\r\ndef sample_dqn_params(trial: optuna.Trial) -> Dict[str, Any]:\r\n    \"\"\"\r\n    Sampler for DQN hyperparams.\r\n\r\n    :param trial:\r\n    :return:\r\n    \"\"\"\r\n    # param define\r\n    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\r\n    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\r\n    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 256, 512])\r\n    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(5e4), int(1e5), int(1e6)])\r\n    exploration_final_eps = trial.suggest_uniform(\"exploration_final_eps\", 0, 0.2)\r\n    exploration_fraction = trial.suggest_uniform(\"exploration_fraction\", 0, 0.5)\r\n    target_update_interval = trial.suggest_categorical(\"target_update_interval\", [1, 1000, 5000, 10000, 15000, 20000])\r\n    learning_starts = trial.suggest_categorical(\"learning_starts\", [0, 1000, 5000, 10000, 20000])\r\n\r\n    train_freq = trial.suggest_categorical(\"train_freq\", [1, 4, 8, 16, 128, 256, 1000])\r\n    subsample_steps = trial.suggest_categorical(\"subsample_steps\", [1, 2, 4, 8])\r\n    gradient_steps = max(train_freq // subsample_steps, 1)\r\n\r\n    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\", \"medium\"])\r\n\r\n    net_arch = {\"tiny\": [64], \"small\": [64, 64], \"medium\": [256, 256]}[net_arch]\r\n\r\n    hyperparams = {\r\n        \"gamma\": gamma,\r\n        \"learning_rate\": learning_rate,\r\n        \"batch_size\": batch_size,\r\n        \"buffer_size\": buffer_size,\r\n        \"train_freq\": train_freq,\r\n        \"gradient_steps\": gradient_steps,\r\n        \"exploration_fraction\": exploration_fraction,\r\n        \"exploration_final_eps\": exploration_final_eps,\r\n        \"target_update_interval\": target_update_interval,\r\n        \"learning_starts\": learning_starts,\r\n        \"policy_kwargs\": dict(net_arch=net_arch),\r\n    }\r\n\r\n    # if trial.using_her_replay_buffer:\r\n    #     hyperparams = sample_her_params(trial, hyperparams)\r\n\r\n    return hyperparams\r\n\r\n\r\nclass TrialEvalCallback(EvalCallback):\r\n    \"\"\"Callback used for evaluating and reporting a trial.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            eval_env: gym.Env,\r\n            trial: optuna.Trial,\r\n            n_eval_episodes: int = 10,\r\n            eval_freq: int = 10000,\r\n            deterministic: bool = True,\r\n            verbose: int = 0,\r\n    ):\r\n\r\n        super().__init__(\r\n            eval_env=eval_env,\r\n            n_eval_episodes=n_eval_episodes,\r\n            eval_freq=eval_freq,\r\n            deterministic=deterministic,\r\n            verbose=verbose,\r\n        )\r\n        self.trial = trial\r\n        self.eval_idx = 0\r\n        self.is_pruned = False\r\n\r\n    def _on_step(self) -> bool:\r\n        COUNT = 0\r\n        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\r\n            super()._on_step()\r\n            self.eval_idx += 1\r\n            self.trial.report(self.last_mean_reward, self.eval_idx)\r\n            # Prune trial if need\r\n            if self.trial.should_prune():\r\n                self.is_pruned = True\r\n                return False\r\n        return True\r\n\r\n\r\ndef objective(trial: optuna.Trial) -> float:\r\n    kwargs = DEFAULT_HYPERPARAMS.copy()\r\n    # Sample hyperparameters\r\n    kwargs.update(sample_dqn_params(trial))\r\n    # Create the RL model\r\n    # model = DQN(**kwargs)\r\n    model = DQN(**kwargs, tensorboard_log='../tensorboard/dqn_highway_tensorboard/')\r\n\r\n    # Create env used for evaluation\r\n    eval_env = gym.make(ENV_ID)\r\n\r\n\r\n    eval_env = Monitor(eval_env, log_dir)\r\n    eval_env = DummyVecEnv([lambda: eval_env])\r\n\r\n    # model.load(\"dqn_highway.zip\", env=eval_env)\r\n\r\n    # Create the callback that will periodically evaluate\r\n    # and report the performance\r\n    eval_callback = TrialEvalCallback(\r\n        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\r\n    )\r\n    auto_save_callback = SaveOnBestTraingRewardCallback(check_freq=1000, log_dir=log_dir)\r\n\r\n    nan_encountered = False\r\n    try:\r\n        with ProgressBarManager(total_timesteps=N_TIMESTEPS) as progress_callback:\r\n            # model.learn(N_TIMESTEPS, callback=[eval_callback, progress_callback])\r\n            model.learn(N_TIMESTEPS, callback=eval_callback)\r\n    except AssertionError as e:\r\n        # Sometimes, random hyperparams can generate NaN\r\n        print(e)\r\n        nan_encountered = True\r\n    finally:\r\n        # Free memory\r\n        model.env.close()\r\n        eval_env.close()\r\n\r\n    # Tell the optimizer that the trial failed\r\n    if nan_encountered:\r\n        return float(\"nan\")\r\n\r\n    if eval_callback.is_pruned:\r\n        raise optuna.exceptions.TrialPruned()\r\n\r\n    return eval_callback.last_mean_reward\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # Set pytorch num threads to 1 for faster training\r\n    torch.set_num_threads(1)\r\n    sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\r\n    # Do not prune before 1/3 of the max budget is used\r\n\r\n    # pruner = HyperbandPruner(min_resource=1, max_resource=N_TIMESTEPS, reduction_factor=3)\r\n    pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3)\r\n    # error starts here\r\n    study = optuna.create_study(study_name='distributed-example', sampler=sampler, pruner=pruner, direction=\"maximize\",\r\n                                storage=\"mysql://root@localhost/optuna\")\r\n    study = optuna.load_study(\r\n        study_name=\"distributed-example\", storage=\"mysql://root@localhost/optuna\"\r\n    )\r\n    try:\r\n        study.optimize(objective, n_trials=N_TRIALS, timeout=600, )\r\n        # plot the which hyperparameter matters the most\r\n        fig = optuna.visualization.plot_param_importances(study)\r\n        # plot the which hyperparamter that Oputna tried the most frequent\r\n        # fig = optuna.visualization.plot_slice(study)\r\n\r\n        fig.show()\r\n\r\n    except KeyboardInterrupt:\r\n        pass\r\n\r\n    print(\"Number of finished trials: \", len(study.trials))\r\n\r\n    print(\"Best trial:\")\r\n    trial = study.best_trial\r\n\r\n    print(\"  Value: \", trial.value)\r\n\r\n    print(\"  Params: \")\r\n    for key, value in trial.params.items():\r\n        print(\"    {}: {}\".format(key, value))\r\n\r\n    print(\"  User attrs:\")\r\n    for key, value in trial.user_attrs.items():\r\n        print(\"    {}: {}\".format(key, value))\r\n```\r\n",
    "comments": [
      {
        "user": "201520906050",
        "body": "And i think the problem is the creation of the study, as i  typed in terminal：\r\n`optuna create-study --study-name \"distributed-example\" --storage \"mysql://root@localhost/optuna\"`\r\nand the traceback were like that：\r\n\r\n> [E 2022-04-28 22:28:42,149] (MySQLdb._exceptions.OperationalError) (2002, \"Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)\")\r\n"
      },
      {
        "user": "himkt",
        "body": "> I installed Mysql throughsudo apt updateandsudo apt install mysql-serverand Mysql worked fine\r\n\r\n@201520906050 Could you please share with us to connect MySQL with `mysql` command? Does it work?"
      },
      {
        "user": "201520906050",
        "body": "> > I installed Mysql throughsudo apt updateandsudo apt install mysql-serverand Mysql worked fine\r\n> \r\n> @201520906050 Could you please share with us to connect MySQL with `mysql` command? Does it work?\r\n\r\nJust `sudo mysql`is fine or `mysql -u root -p` and it works. And I also tried another example using pymysql to connect MySQL and it also worked."
      }
    ]
  },
  {
    "issue_number": 5113,
    "title": " No exception when passing invalid data to .enqueue_trial",
    "author": "qnbhd",
    "state": "closed",
    "created_at": "2023-11-14T09:14:50Z",
    "updated_at": "2024-09-09T23:06:54Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Expected behavior\r\n\r\nIt is expected that some exception will be raised when passing obviously incorrect data (not matching the `Dict[str, Any]` type) to `study.enqueue_trial`. \r\n\r\n### Environment\r\n\r\n- Optuna version:3.4.0\r\n- Python version:3.9.6\r\n- OS:macOS-13.0.1-arm64-arm-64bit\r\n\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\n[I 2023-11-14 12:09:55,858] A new study created in memory with name: no-name-7cf052d2-a932-4b47-bdae-130b5491e38b\r\n0 5\r\n1 4\r\n2 2\r\n3 0\r\n4 0\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\n```python\r\nimport optuna\r\n\r\nstudy = optuna.create_study(direction=\"maximize\")\r\nstudy.enqueue_trial({\"foo\": 5})  # OK\r\nstudy.enqueue_trial([{\"foo\": 15}]) # Exception expected\r\nstudy.enqueue_trial([1, 1, 3]) # Exception expected\r\n\r\nfor _ in range(5):\r\n    trial = study.ask()\r\n    foo = trial.suggest_int(\"foo\", 0, 10)\r\n    print(trial._trial_id, foo)  # noqa\r\n    study.tell(trial, 0.1)\r\n```\r\n\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Thank for your your input. As we can see in the example, `study` cannot know the parameter space defined in the objective when `enqueue_trial` is called. ~In addition, I do not expect `study.enqueue_trial([{\"foo\": 15}])` shows some error message because Optuna supports dynamic search space, which contradicts with causing some exceptions here.~, Sorry, I misunderstood your example, the main point here is a list of dict, not dynamic space.\r\n\r\nI agree that showing some warning or error messages seems useful.\r\n\r\nThus I remove `bug` label from this issue."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5104,
    "title": "Generic type-hints for suggesters",
    "author": "MartinBernstorff",
    "state": "closed",
    "created_at": "2023-11-10T14:34:51Z",
    "updated_at": "2024-09-08T23:06:19Z",
    "labels": [
      "code-fix",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nCurrently, if I call `trial.suggest_cateogorical(\"name\", [1,2,3])`, the return type is a `CategoricalChoiceType`, which is insufficiently bounded. E.g. the input is a `Sequence[int]`, so the output should be `int`.\r\n\r\n### Suggestion\r\n\r\nIf `CategoricalChoiceType` was type-hinted as a TypeVar with bounds, the return type would be an `int`, which is what I would expect.\r\n\r\n### Additional context (optional)\r\n\r\nI'm super happy to draft a PR, but wanted to clear it with you guys first!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5658,
    "title": "2 tests fail",
    "author": "yurivict",
    "state": "closed",
    "created_at": "2024-09-03T08:12:01Z",
    "updated_at": "2024-09-07T03:42:25Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nn/a\r\n\r\n### Environment\r\n\r\n- Optuna version: 4.0.0\r\n- Python version: 3.11\r\n- OS: FreeBSD 14.1\r\n- (Optional) Other libraries and their versions:\r\n\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```shell\r\n========================================================================================= FAILURES ==========================================================================================\r\n_____________________________________________________________________________ test_deprecation_class_decorator ______________________________________________________________________________\r\n\r\n    def test_deprecation_class_decorator() -> None:\r\n        deprecated_version = \"1.1.0\"\r\n        removed_version = \"3.0.0\"\r\n        decorator_deprecation = _deprecated.deprecated_class(deprecated_version, removed_version)\r\n        assert callable(decorator_deprecation)\r\n    \r\n        decorated_class = decorator_deprecation(_Sample)\r\n        assert decorated_class.__name__ == \"_Sample\"\r\n        assert decorated_class.__init__.__name__ == \"__init__\"\r\n>       assert decorated_class.__doc__ == _deprecated._DEPRECATION_NOTE_TEMPLATE.format(\r\n            d_ver=deprecated_version, r_ver=removed_version\r\n        )\r\nE       AssertionError: assert '.. warning::...tag/v1.1.0.\\n' == '\\n\\n.. warni...tag/v1.1.0.\\n'\r\nE         \r\nE         - \r\nE         + .. warning::\r\nE         +     Deprecated in v1.1.0. This feature will be removed in the future. The removal of this\r\nE         +     feature is currently scheduled for v3.0.0, but this schedule is subject to change.\r\nE         +     See https://github.com/optuna/optuna/releases/tag/v1.1.0.\r\nE           ...\r\nE         \r\nE         ...Full output truncated (4 lines hidden), use '-vv' to show\r\n\r\ntests/test_deprecated.py:87: AssertionError\r\n_____________________________________________________________________________ test_experimental_class_decorator _____________________________________________________________________________\r\n\r\n    def test_experimental_class_decorator() -> None:\r\n        version = \"1.1.0\"\r\n        decorator_experimental = _experimental.experimental_class(version)\r\n        assert callable(decorator_experimental)\r\n    \r\n        decorated_class = decorator_experimental(_Sample)\r\n        assert decorated_class.__name__ == \"_Sample\"\r\n        assert decorated_class.__init__.__name__ == \"__init__\"\r\n>       assert decorated_class.__doc__ == _experimental._EXPERIMENTAL_NOTE_TEMPLATE.format(ver=version)\r\nE       AssertionError: assert '.. note::\\n ...tag/v1.1.0.\\n' == '\\n\\n.. note:...tag/v1.1.0.\\n'\r\nE         \r\nE         - \r\nE         + .. note::\r\nE         +     Added in v1.1.0 as an experimental feature. The interface may change in newer versions\r\nE         +     without prior notice. See https://github.com/optuna/optuna/releases/tag/v1.1.0.\r\nE           \r\nE           .. note::\r\nE               Added in v1.1.0 as an experimental feature. The interface may change in newer versions\r\nE               without prior notice. See https://github.com/optuna/optuna/releases/tag/v1.1.0.\r\n\r\ntests/test_experimental.py:79: AssertionError\r\n========================================================================= 2 failed, 370 passed in 301.25s (0:05:01) =========================================================================\r\n*** Error code 1\r\n```\r\n\r\n\r\n\r\n### Steps to reproduce\r\n\r\npytest\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Hmm, I could not reproduce the error with the latest stable Optuna (4.0.0) and the latest comment: https://github.com/optuna/optuna/commit/d00b9509b461c24dd0b2dbb2ad8561973d4ad929 w/ my macOS. Could you explain more about what you'd like to do with pytest? To send a pull request, you can ignore the error on your local machine since they are just tests if the introduced changes are not related to this test file."
      },
      {
        "user": "nzw0301",
        "body": "Since this is not reproducible and no additional update so far, let me close this issue."
      }
    ]
  },
  {
    "issue_number": 1339,
    "title": "Plotly not install errror, though installed",
    "author": "cmeier303",
    "state": "closed",
    "created_at": "2020-06-08T14:53:50Z",
    "updated_at": "2024-09-05T16:03:31Z",
    "labels": [
      "stale"
    ],
    "body": "Hi All, \r\n\r\nI have Plotly installed and I am getting an error from optuna that it isn't. My version of plotly is 4.8.1.  Is there a solution for this?  If I should install an older version of plotly, which version should it be?\r\n\r\nThanks!\r\nChris\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-308-04cb7fde5fca> in <module>\r\n----> 1 optuna.visualization.plot_intermediate_values(study)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\optuna\\visualization\\intermediate_values.py in plot_intermediate_values(study)\r\n     68     \"\"\"\r\n     69 \r\n---> 70     _check_plotly_availability()\r\n     71     return _get_intermediate_plot(study)\r\n     72 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\optuna\\visualization\\utils.py in _check_plotly_availability()\r\n     38             \"Plotly can be installed by executing `$ pip install plotly`. \"\r\n     39             \"For further information, please refer to the installation guide of plotly. \"\r\n---> 40             \"(The actual import error is as follows: \" + str(plotly_imports._import_error) + \")\"\r\n     41         )\r\n     42 \r\n\r\nImportError: Plotly is not available. Please install plotly to use this feature. Plotly can be installed by executing `$ pip install plotly`. For further information, please refer to the installation guide of plotly. (The actual import error is as follows: No module named 'plotly')\r\n",
    "comments": [
      {
        "user": "HideakiImamura",
        "body": "Thanks for the question. Could you give me your OS information and results of the following commands in the python interpreter?\r\n```bash\r\n$ import plotly\r\n$ plotly.__version__\r\n```"
      },
      {
        "user": "HideakiImamura",
        "body": "You already told me about the version of plotly. Sorry for the confusion.\r\n\r\nWe haven't investigated Optuna's behavior on Windows environment. Looking at the log above, it looks like you are using Windows OS. Can I ask for more information for the OS version?\r\n\r\nAlso, looking at the logs above, I believe you are creating an environment using Anaconda. Is this a virtual environment? If so, may I ask what you used to create your virtual environment?  (`pyenv`, `virtualenv`, or `conda`?)\r\n\r\nAlso, as you can see on [this page](https://stackoverflow.com/questions/41060382/using-pip-to-install-packages-to-anaconda-environment), in Anaconda's environment, it is better to do `conda install` than `pip install`. Can you do `conda install plotly` instead? (You may need to rebuild your virtual environment, if necessary."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5662,
    "title": "Multiobjective pruner",
    "author": "Xixaus",
    "state": "closed",
    "created_at": "2024-09-05T12:08:46Z",
    "updated_at": "2024-09-05T12:39:46Z",
    "labels": [],
    "body": "### Don't use GitHub Issues to ask support questions.\n\nIs there a pruner in Optuna for multi-objective optimization? If so, how can I use it? I'm seeking guidance on implementing a pruner that can simultaneously handle multiple objectives. Any examples or documentation would be greatly appreciated. Thanks!",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Please see https://github.com/optuna/optuna/issues/3450"
      }
    ]
  },
  {
    "issue_number": 5301,
    "title": "Add a new class called `StudyJoblib`",
    "author": "cheginit",
    "state": "open",
    "created_at": "2024-03-10T04:41:19Z",
    "updated_at": "2024-09-04T08:24:36Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\r\n\r\nThe current parallelization with Python's built-in `ThreadPool` is very limited. By adding a new class called `StudyJoblib` and using `Study` as the base-class, we can just replace the `.optimize` and use `joblib` instead. We can then add a new arg to `create_study` called, for example, `use_joblib` that will create a `StudyJoblib` instead of `Study`. My implementation does not add `joblib` as a new dep, rather it's a soft dep that if `use_joblib=True`, will check if it's installed and throws an exception if not installed.\r\n\r\nI can submit a PR if interested.\r\n\r\n### Description\r\n\r\nI have already implemented this and tested it for my use-case. It works nicely and runs much faster than the default method. This is what I have:\r\n\r\n```python\r\nclass StudyJoblib(Study):\r\n    def __init__(\r\n        self,\r\n        study_name: str,\r\n        storage: str | storages.BaseStorage,\r\n        sampler: BaseSampler | None = None,\r\n        pruner: pruners.BasePruner | None = None,\r\n    ) -> None:\r\n        super().__init__(study_name, storage, sampler, pruner)\r\n\r\n    def optimize(\r\n        self,\r\n        func: ObjectiveFuncType,\r\n        n_trials: int | None = None,\r\n        timeout: float | None = None,\r\n        n_jobs: int = 1,\r\n        catch: Iterable[type[Exception]] | type[Exception] = (),\r\n        callbacks: list[Callable[[\"Study\", FrozenTrial], None]] | None = None,\r\n        gc_after_trial: bool = False,\r\n        show_progress_bar: bool = False,\r\n    ) -> None:\r\n        \"\"\"Optimize an objective function.\"\"\"\r\n        if joblib is None:\r\n            raise ImportError(\r\n                \"Please install joblib to use `Study.optimize` method with `use_joblib=True`. \"\r\n                \"You can install joblib with `pip install joblib`.\"\r\n            )\r\n        if not isinstance(catch, tuple):\r\n            raise TypeError(\r\n                \"The catch argument is of type '{}' but must be a tuple.\".format(type(catch).__name__)\r\n            )\r\n\r\n        if n_trials is None:\r\n            raise ValueError(\"The n_trials argument must be specified.\")\r\n\r\n        if self._thread_local.in_optimize_loop:\r\n            raise RuntimeError(\"Nested invocation of `Study.optimize` method isn't allowed.\")\r\n\r\n        self._stop_flag = False\r\n\r\n        if not isinstance(n_jobs, int):\r\n            raise ValueError(\"The n_jobs argument must be an integer.\")\r\n\r\n        with joblib.Parallel(n_jobs=n_jobs, require=\"sharedmem\") as parllel:\r\n            parllel(\r\n                joblib.delayed(_optimize_sequential)(\r\n                    self,\r\n                    func,\r\n                    1,\r\n                    timeout,\r\n                    catch,\r\n                    callbacks,\r\n                    gc_after_trial,\r\n                    True,\r\n                    None,\r\n                    None,\r\n                ) for _ in range(n_trials)\r\n            )\r\n\r\ndef create_study(\r\n    *,\r\n    storage: str | storages.BaseStorage | None = None,\r\n    sampler: BaseSampler | None = None,\r\n    pruner: pruners.BasePruner | None = None,\r\n    study_name: str | None = None,\r\n    direction: str | StudyDirection | None = None,\r\n    load_if_exists: bool = False,\r\n    directions: Sequence[str | StudyDirection] | None = None,\r\n    use_joblib: bool = False,\r\n) -> Study | StudyJoblib:\r\n    \"\"\"Create a new :class:`~optuna.study.Study` or :class:`~optuna.study.StudyJoblib`.\"\"\"\r\n    if direction is None and directions is None:\r\n        directions = [\"minimize\"]\r\n    elif direction is not None and directions is not None:\r\n        raise ValueError(\"Specify only one of `direction` and `directions`.\")\r\n    elif direction is not None:\r\n        directions = [direction]\r\n    elif directions is not None:\r\n        directions = list(directions)\r\n    else:\r\n        assert False\r\n\r\n    if len(directions) < 1:\r\n        raise ValueError(\"The number of objectives must be greater than 0.\")\r\n    elif any(\r\n        d not in [\"minimize\", \"maximize\", StudyDirection.MINIMIZE, StudyDirection.MAXIMIZE]\r\n        for d in directions\r\n    ):\r\n        raise ValueError(\r\n            \"Please set either 'minimize' or 'maximize' to direction. You can also set the \"\r\n            \"corresponding `StudyDirection` member.\"\r\n        )\r\n\r\n    direction_objects = [\r\n        d if isinstance(d, StudyDirection) else StudyDirection[d.upper()] for d in directions\r\n    ]\r\n\r\n    storage = storages.get_storage(storage)\r\n    try:\r\n        study_id = storage.create_new_study(direction_objects, study_name)\r\n    except DuplicatedStudyError:\r\n        if load_if_exists:\r\n            assert study_name is not None\r\n\r\n            logger = optuna.logging.get_logger(\"optuna\")\r\n\r\n            logger.info(\r\n                \"Using an existing study with name '{}' instead of \"\r\n                \"creating a new one.\".format(study_name)\r\n            )\r\n            study_id = storage.get_study_id_from_name(study_name)\r\n        else:\r\n            raise\r\n\r\n    if sampler is None and len(direction_objects) > 1:\r\n        sampler = NSGAIISampler()\r\n\r\n    study_name = storage.get_study_name_from_id(study_id)\r\n    if use_joblib:\r\n        if importlib.util.find_spec(\"joblib\") is None:\r\n            raise ImportError(\r\n                \"Please install joblib to use `use_joblib=True`. \"\r\n                \"You can install joblib with `pip install joblib`.\"\r\n            )\r\n        return StudyJoblib(study_name=study_name, storage=storage, sampler=sampler, pruner=pruner)\r\n\r\n    return Study(study_name=study_name, storage=storage, sampler=sampler, pruner=pruner)\r\n```\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "cheginit",
        "body": "Never mind, I came up with a more elegant solution for using `joblib`. I'm posting the code here for people who may want to use this approach. I can also add this as an example to the Optuna examples repo, if interested:\r\n\r\n```python\r\nimport contextlib\r\nimport warnings\r\nfrom dataclasses import dataclass\r\nfrom pathlib import Path\r\nfrom typing import Any, Literal, cast\r\n\r\nimport optuna\r\nfrom optuna.exceptions import DuplicatedStudyError, ExperimentalWarning\r\nfrom optuna.pruners import BasePruner, HyperbandPruner\r\nfrom optuna.samplers import BaseSampler, TPESampler\r\nfrom optuna.storages import JournalFileStorage, JournalStorage\r\nfrom optuna.study import MaxTrialsCallback, Study\r\nfrom optuna.trial import Trial, TrialState\r\n\r\n\r\n@dataclass\r\nclass StudyConfig:\r\n    study_name: str\r\n    sampler: BaseSampler\r\n    pruner: BasePruner\r\n    directions: list[Literal[\"minimize\", \"maximize\"]]\r\n    storage: JournalStorage\r\n    n_trials: int\r\n    n_cores: int = 1\r\n    log_path: Path = Path(\"optuna_journal.log\")\r\n    study_path: Path = Path(\"optuna_study.pkl\")\r\n\r\n    @property\r\n    def study_args(self) -> dict[str, Any]:\r\n        return {\r\n            \"study_name\": self.study_name,\r\n            \"sampler\": self.sampler,\r\n            \"pruner\": self.pruner,\r\n            \"directions\": self.directions,\r\n            \"storage\": self.storage,\r\n        }\r\n\r\n\r\ndef objective(trial: Trial) -> float:\r\n    x = trial.suggest_float(\"x\", -100, 100)\r\n    y = trial.suggest_categorical(\"y\", [-1, 0, 1])\r\n    return x**2 + y\r\n\r\n\r\ndef optimize(study_cfg: StudyConfig, worker_id: int) -> None:\r\n    study = optuna.create_study(**study_cfg.study_args, load_if_exists=True)\r\n    n_trials = study_cfg.n_trials // study_cfg.n_cores\r\n    n_trials += study_cfg.n_cores - (study_cfg.n_trials % study_cfg.n_cores)\r\n    study.optimize(\r\n        objective,\r\n        n_trials=n_trials,\r\n        callbacks=[MaxTrialsCallback(study_cfg.n_trials, states=(TrialState.COMPLETE,))],\r\n    )\r\n    if worker_id == 0:\r\n        with study_cfg.study_path.open(\"wb\") as f:\r\n            pickle.dump(study, f)\r\n\r\n\r\nn_trials = 6000\r\nn_cores = 12\r\nlog_path = Path(\"optuna_journal.log\")\r\nlog_path.unlink(missing_ok=True)\r\nPath(f\"{log_path}.lock\").unlink(missing_ok=True)\r\nstudy_path = Path(\"optuna_study.pkl\")\r\nstudy_path.unlink(missing_ok=True)\r\n\r\nwith warnings.catch_warnings():\r\n    warnings.simplefilter(\"ignore\", ExperimentalWarning)\r\n    study_cfg = StudyConfig(\r\n        \"test\",\r\n        TPESampler(seed=42),\r\n        HyperbandPruner(),\r\n        [\"minimize\"],\r\n        JournalStorage(JournalFileStorage(str(log_path))),\r\n        n_trials,\r\n        n_cores,\r\n        log_path,\r\n        study_path,\r\n    )\r\n    with contextlib.suppress(DuplicatedStudyError):\r\n        _ = optuna.create_study(**study_cfg.study_args)\r\nwhile study_cfg.n_trials >= min(100, study_cfg.n_trials):\r\n    try:\r\n        _ = joblib.Parallel(n_jobs=n_cores)(\r\n            joblib.delayed(optimize)(study_cfg, i) for i in range(n_cores)\r\n        )\r\n    except Exception:\r\n        Path(f\"{log_path}.lock\").unlink(missing_ok=True)\r\n        study_cfg.n_trials //= 2\r\n    else:\r\n        break\r\nwith study_cfg.study_path.open(\"rb\") as f:\r\n    study = cast(\"Study\", pickle.load(f))\r\nbest_params = study.best_trial.params\r\nbest_params\r\n```\r\n\r\nNote that, sometimes, depending on the number of trials, it fails with `Error: did not possess lock`, that's why I added that while-loop."
      }
    ]
  },
  {
    "issue_number": 5647,
    "title": "allow to get trial by number from study without O(N) scan over all trials",
    "author": "bionicles",
    "state": "closed",
    "created_at": "2024-08-28T16:05:05Z",
    "updated_at": "2024-08-28T17:38:00Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\r\n\r\nif you need to get a trial object you already created, the only option is an inefficient scan over all the trials in a given study, or to get it from storage, by `trial._trial_id` and not by `trial.number`, but `Study.tell` requires `trial.number` and not `_trial_id`, so you need to store both `trial._trial_id` and `trial.number` to work with custom optimization loops, and you need to remember which one to use to get vs tell\r\n\r\n### Description\r\n\r\nstudy could have a method to `get_trial_by_number` that quickly (O(N)) returns the trial with a given number. then users would not need to store both `_trial_id` and `number` and remember to use `storage.get_trial(_trial_id)` vs `study.tell(trial_number)`\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "bionicles",
        "body": "sorry, i might be confused and this is a non-issue\r\n\r\nis the trial number just an index into the list of trials on the study object?\r\ndoes loading a study eagerly repopulate the whole trial list?\r\n\r\nif so, then loading and re-loading the same study will likely be slow but we can just do \r\n\r\nstudy.trials[trial_number] to get the trial ? "
      },
      {
        "user": "bionicles",
        "body": "guess so, ill try to cache studies so i dont remake them all the time, problem is i have many studies each with many trials, and trials having ids and numbers, it's a bit confusing, ill close this"
      }
    ]
  },
  {
    "issue_number": 5633,
    "title": "Inappropriate behavior of `plot_rank()` in `optuna.visualization`  when `None` values exist in trials",
    "author": "kAIto47802",
    "state": "closed",
    "created_at": "2024-08-23T09:49:33Z",
    "updated_at": "2024-08-28T01:16:01Z",
    "labels": [
      "bug"
    ],
    "body": "### Expected behavior\r\n\r\nCurrently, if `None` value exists in trials, the function `plot_rank()` in `optuna.visualization` shows inappropriate behavior:\r\n\r\n- When using `plotly` (i.e., using `optuna.visualization.plot_rank`):\r\n\r\n   - trial with `None` value will be ignored and not plotted.\r\n\r\n- When using `matplotlib` (i.e., using `optuna.visualization.matplotlib.plot_rank`):\r\n\r\n   - an error occurs and no plot will be made.\r\n\r\nThe expected behavior is as follows:\r\n\r\n- a plot with all trials is made even if the trials contain `None` values (for both `plotly` and `matplotlib`).\r\n\r\n### Environment\r\n\r\n- Optuna version: master branch\r\n- Python version: any\r\n- OS: any\r\n\r\n\r\n### Error messages, stack traces, or logs\r\n\r\nThe error message when using `matplotlib` is follows:\r\n\r\n```py\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[3], line 10\r\n      7 study = optuna.create_study()\r\n      8 study.optimize(objective, n_trials=20)\r\n---> 10 fig = optuna.visualization.matplotlib.plot_rank(study)\r\n     11 fig.show()\r\n\r\nFile ~/Documents/2_PFN/pfn-workspace/optuna/optuna/_experimental.py:82, in experimental_func.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\r\n     71 @functools.wraps(func)\r\n     72 def wrapper(*args: Any, **kwargs: Any) -> \"FT\":\r\n     73     warnings.warn(\r\n     74         \"{} is experimental (supported from v{}). \"\r\n     75         \"The interface can change in the future.\".format(\r\n   (...)\r\n     79         stacklevel=2,\r\n     80     )\r\n---> 82     return func(*args, **kwargs)\r\n\r\nFile ~/Documents/2_PFN/pfn-workspace/optuna/optuna/visualization/matplotlib/_rank.py:98, in plot_rank(study, params, target, target_name)\r\n     93 _logger.warning(\r\n     94     \"Output figures of this Matplotlib-based `plot_rank` function would be different from \"\r\n     95     \"those of the Plotly-based `plot_rank`.\"\r\n     96 )\r\n     97 info = _get_rank_info(study, params, target, target_name)\r\n...\r\n     94         \", \".join(names[:-1]) + \" or \" + names[-1]\r\n     95         if len(names) > 1 else names[0],\r\n     96         type_name(type(v))))\r\n\r\nTypeError: 'value' must be an instance of str or bytes, not a None\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nThe following code yields the error above:\r\n\r\n```py\r\ndef objective(trial: optuna.Trial) -> float:\r\n    x = trial.suggest_float(\"x\", -100, 100)\r\n    trial.suggest_categorical(\"y\", [\"foo\", None])\r\n    return x**2\r\n\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=20)\r\n\r\nfig = optuna.visualization.matplotlib.plot_rank(study)\r\nfig.show()\r\n```\r\n\r\n\r\n\r\n### Additional context (optional)\r\n\r\nThis issue is related to this PR:\r\n\r\n- #5599 .\r\n\r\nThis PR simply suppressed the issue reported here and left TODO comments saying that this issue should be addressed in the future. The another issue is resolved in this PR:\r\n\r\n- #5630 .\r\n\r\n\r\nOne solution about this issue is convert `None` value to the string `\"None\"` before the plot is made.\r\nSpecifically, `None` value in `info.xs` and `info.ys` in `optuna.visualization._get_rank_subplot` and `optuna.visualization.matplotlib._add_rank_subplot` should be converted before they being passed to these functions.\r\n\r\nThe current code can be found here:\r\n\r\n- for `optuna.visualization.plot_rank`:\r\n    https://github.com/optuna/optuna/blob/1f38f73efae96d4d0099142efc794ea802f864a0/optuna/visualization/_rank.py#L301-L303\r\n\r\n- and for `optuna.visualization.matplotlib.plot_rank`:\r\n\r\n   https://github.com/optuna/optuna/blob/1f38f73efae96d4d0099142efc794ea802f864a0/optuna/visualization/matplotlib/_rank.py#L163\r\n\r\n.\r\n\r\nThe conversion can simply be done by these code:\r\n\r\n```py\r\n[\"None\" if x is None else x for y in info.xs]\r\n[\"None\" if y is None else y for y in info.ys]\r\n```\r\n\r\nand these values should be passed in these functions.\r\n\r\n",
    "comments": []
  },
  {
    "issue_number": 5036,
    "title": "Migrate from `threading.local` to `contextvars`",
    "author": "contramundum53",
    "state": "closed",
    "created_at": "2023-10-13T11:21:29Z",
    "updated_at": "2024-08-26T23:05:59Z",
    "labels": [
      "code-fix",
      "needs-discussion",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nhttps://github.com/optuna/optuna/blob/3fcbf8839c760916cc0cf38fb545f526d5efe85d/optuna/study/study.py#L56 uses `threading.local`, but `contextvars` can make the code more sophisticated (https://peps.python.org/pep-0567/#converting-code-that-uses-threading-local).\r\n\r\n### Suggestion\r\n\r\nUse `contextvars` instead of `threading.local`.\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "sousu4",
        "body": "I will work on this issue."
      },
      {
        "user": "SG5",
        "body": "Hey @sousu4 I just saw your comment. I started working on this issue 3 days ago. Let me know if you want to pass it to me"
      },
      {
        "user": "sousu4",
        "body": "HEY @SG5 OK,  Please proceed."
      }
    ]
  },
  {
    "issue_number": 4944,
    "title": "Add example thumbnails of `optuna.visualization` features",
    "author": "Alnusjaponica",
    "state": "closed",
    "created_at": "2023-09-25T03:59:48Z",
    "updated_at": "2024-08-26T06:24:09Z",
    "labels": [
      "document",
      "needs-discussion",
      "optuna.visualization"
    ],
    "body": "### What is an issue?\r\n\r\n## Motivation\r\nEnhance the document's visual appeal for better comprehension.\r\n\r\n## Detail\r\nThe primary goal of this issue is to provide a comprehensive overview of examples for each visualization feature on the [visualization document top](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html). The inclusion of visual examples significantly improves the understanding of the intended purpose for each plot function.\r\n\r\nWhile this may require some discussion, one straightforward method to incorporate visual explanations is by adding thumbnails with links to examples, similar to [`matplotlib.pyplot.scatter`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html#examples-using-matplotlib-pyplot-scatter):\r\n<img width=\"808\" alt=\"matplotlib_example\" src=\"https://github.com/optuna/optuna/assets/50256998/04367e6b-b8ea-4253-aa79-4985f3c30097\">\r\n\r\nThe challenge lies in avoiding code duplication when using [sphinx gallery](https://sphinx-gallery.github.io/stable/configuration.html#examples-using-numpy-exp). It is essential to note that generating figures automatically would be ideal to prevent an increase in maintenance costs associated with this change.",
    "comments": [
      {
        "user": "Cupcha",
        "body": "Hello ! We can work on it."
      },
      {
        "user": "Alnusjaponica",
        "body": "I apologize for my delayed reply. We appreciate your proposal and eagerly anticipate your PR!"
      },
      {
        "user": "guillaumeferreol",
        "body": "Hello @Alnusjaponica, I am working with Cupcha on this issue. \r\nI saw that in the documentation there already is a tutorial with visualization examples of different plot types. There also are a few examples on the documentation page of each plot type. \r\nWhat exactly did you have in mind ? For each plot type replace the example currently included by a thumbnail (like in matplotlib) linking to a specific tutorial (and include all the thumbnails on the `optuna.visualization` documentation) ?\r\n"
      }
    ]
  },
  {
    "issue_number": 5631,
    "title": "Optuna conditional search space definition does not hit on Google Search easily",
    "author": "nabenabe0928",
    "state": "open",
    "created_at": "2024-08-22T05:09:05Z",
    "updated_at": "2024-08-22T05:09:20Z",
    "labels": [
      "document",
      "needs-discussion"
    ],
    "body": "### What is an issue?\n\nRelated to the following issue:\r\n- https://github.com/optuna/optuna/issues/5611\r\n\r\nBasically, [this page](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html) (or any new page related to this) should hit, but this website hits when we search with `optuna dynamic search space`.\r\n\r\nProbably, we should modify the corresponding pages so that they include `conditional search space` and `conditional parameters` as they are more widely used terminologies.",
    "comments": []
  },
  {
    "issue_number": 4936,
    "title": "pg8000 driver for postgresql databases has limits in default settings when using a study optimization",
    "author": "gituser789",
    "state": "closed",
    "created_at": "2023-09-22T13:10:33Z",
    "updated_at": "2024-08-20T23:06:16Z",
    "labels": [
      "bug",
      "stale"
    ],
    "body": "### Expected behavior\n\nWhen running a postgresql-database with the `pg8000` DB API in the default settings like this\r\n```\r\ndatabase_url = f\"postgresql+pg8000://user:password@127.0.0.1:port/mydb\"\r\n```\r\nthe database runs into trouble loading a study with more than 32767 trials when performing e.g. a NSGAIII optimization.\r\n\r\nThere seems so be a limit in the DB API `pg8000`. Adapting the `Cursor.setinputsizes` in `pg8000` could solve the issue https://github.com/tlocke/pg8000/issues/120 (i did not verify this).\r\n\r\nWhat solves the issue, is using another database API `psycopg2` in the default settings:\r\n```\r\ndatabase_url = f\"postgresql+psycopg2://user:password@127.0.0.1:port/mydb\"\r\n```\r\nDue to optuna users like me are may no experts in setting up databases, it might be useful to fix this by optuna (if possible). Otherwise, a hint in the documentation would be nice. I can adapt the documentation and send a PR, but want to listen to your feedback first.\r\n\n\n### Environment\n\n- Optuna version: 3.3.0\r\n- Python version: 3.11\r\n- OS: debian 10 and Manjaro (current version of rolling release)\r\n\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\nstruct.error: 'h' format requires -32768 <= number <= 32767\r\n```\n```\n\n\n### Steps to reproduce\n\nPerform a study with more than 32767 trials using the `PG8000` database API for the postgresql database\r\n```\r\ndatabase_url = f\"postgresql+pg8000://user:password@127.0.0.1:port/mydb\"\r\n```\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "HideakiImamura",
        "body": "@gituser789 Does this problem still occur with the latest Optuna v3.4?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 4946,
    "title": "Add FAQ section about cross validation with pruning",
    "author": "nzw0301",
    "state": "closed",
    "created_at": "2023-09-25T09:31:56Z",
    "updated_at": "2024-08-20T23:06:14Z",
    "labels": [
      "document",
      "contribution-welcome",
      "stale"
    ],
    "body": "### What is an issue?\n\nI sometimes see users struggling to combine pruning and cross-validation. It would be nice to have a FAQ section.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5073,
    "title": "Unify the behavior of the terminator and its visualization function",
    "author": "HideakiImamura",
    "state": "closed",
    "created_at": "2023-10-25T07:22:13Z",
    "updated_at": "2024-08-20T23:06:11Z",
    "labels": [
      "feature",
      "contribution-welcome",
      "stale"
    ],
    "body": "### Motivation\n\nWe are developing `optuna.terminator` as an automatic termination module, and have prepared `optuna.visualization.plot_terminator_improvement` for its visualization. However, there are inconsistencies in their behavior, and I propose that we align them.\n\n### Description\n\nThe `optuna.terminator.Terminator` class is initialized with `improvement_evaluator`, `error_evaluator`, and `min_n_trials`. If `improvement_evaluator` is an instance of `BestValueStagnationEvaluator` and `error_evaluator` is `None`, `error_evaluator` is initialized with `StaticErrorEvaluator(constant=0)` ([ref](https://github.com/optuna/optuna/blob/master/optuna/terminator/terminator.py#L115)). `plot_terminator_improvement` is not set up like this. The latter should be made to match the former.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 5624,
    "title": "Fail if the function `plot_contour()` specifies a pair of parameters that cannot coexist",
    "author": "kAIto47802",
    "state": "closed",
    "created_at": "2024-08-13T10:06:27Z",
    "updated_at": "2024-08-20T09:07:21Z",
    "labels": [
      "bug",
      "needs-discussion"
    ],
    "body": "### Expected behavior\r\n\r\nCurrently, if a pair of parameters, which cannot co-exist in `trial.params`, is specified, it will fail with an unfriendly error message.\r\nNote that when `params=None` is specified, as all pairs are used in the plot, anyways the function fails.\r\nThere are two possible solutions to this:\r\n\r\n- Raise an error saying that the plot cannot be performed, or\r\n- Return an empty plot without an error.\r\n\r\nPlease see the additional context for more information.\r\n\r\n### Environment\r\n\r\n- Optuna version: master branch\r\n- Python version: any\r\n- OS: any\r\n\r\n\r\n### Error messages, stack traces, or logs\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\nCell In[8], line 5\r\n      2 study = optuna.create_study(sampler=sampler)\r\n      3 study.optimize(objective_single_dynamic_with_categorical, n_trials=30)\r\n----> 5 fig = optuna.visualization.plot_contour(study, params=[\"x1\", \"x2\"])\r\n      6 fig.show()\r\n\r\nFile ~/Documents/2_PFN/pfn-workspace/optuna/optuna/visualization/_contour.py:122, in plot_contour(study, params, target, target_name)\r\n    120 _imports.check()\r\n    121 info = _get_contour_info(study, params, target, target_name)\r\n--> 122 return _get_contour_plot(info)\r\n\r\nFile ~/Documents/2_PFN/pfn-workspace/optuna/optuna/visualization/_contour.py:140, in _get_contour_plot(info)\r\n    138 y_param = sorted_params[1]\r\n    139 sub_plot_info = sub_plot_infos[0][0]\r\n--> 140 sub_plots = _get_contour_subplot(sub_plot_info, reverse_scale, target_name)\r\n    141 figure = go.Figure(data=sub_plots, layout=layout)\r\n    142 figure.update_xaxes(title_text=x_param, range=sub_plot_info.xaxis.range)\r\n\r\nFile ~/Documents/2_PFN/pfn-workspace/optuna/optuna/visualization/_contour.py:228, in _get_contour_subplot(info, reverse_scale, target_name)\r\n    225 xys = np.array(list(info.z_values.keys()))\r\n    226 zs = np.array(list(info.z_values.values()))\r\n--> 228 z_values[xys[:, 1], xys[:, 0]] = zs\r\n    230 if len(x_indices) < 2 or len(y_indices) < 2:\r\n    231     return go.Contour(), go.Scatter(), go.Scatter()\r\n\r\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\r\n```\r\n\r\n\r\n### Steps to reproduce\r\n\r\nThe following code yields the error above:\r\n\r\n```python\r\nimport optuna\r\n\r\ndef objective(trial: optuna.Trial) -> float:\r\n    category = trial.suggest_categorical(\"category\", [\"foo\", \"bar\"])\r\n    if category == \"foo\":\r\n        return (trial.suggest_float(\"x1\", 0, 10) - 2) ** 2\r\n    else:\r\n        return -((trial.suggest_float(\"x2\", -10, 0) + 5) ** 2)\r\n\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=30)\r\n\r\nfig = optuna.visualization.plot_contour(study, params=[\"x1\", \"x2\"])\r\nfig.show()\r\n```\r\n\r\n> [!NOTE]\r\n> In the code above, since `x1` and `x2` cannot be defined at the same time, the function values cannot be defined in the space composed of `x1` and `x2` and contour cannot be plot.\r\n\r\n### Additional context (optional)\r\n\r\nThis issue is related to the PR:\r\n\r\n- #5599 \r\n\r\nThis PR simlpy suppressed the issue reported here and left TODO comments saying that this issue should be addressed in the future.\r\n\r\nhttps://github.com/optuna/optuna/blob/1925ca1a051b3bf11819478b65d830de614774b2/tests/visualization_tests/test_visualizations.py#L90-L92\r\n\r\nIn these lines, if the specified parameter pair cannot exist at the same time, `z_values` becomes empty.\r\n\r\nhttps://github.com/optuna/optuna/blob/1925ca1a051b3bf11819478b65d830de614774b2/optuna/visualization/_contour.py#L341-L344\r\n\r\nAn empty `info.z_values` causes an error.\r\n\r\nhttps://github.com/optuna/optuna/blob/1925ca1a051b3bf11819478b65d830de614774b2/optuna/visualization/_contour.py#L228\r\n\r\nAs mentioned earlier, one solution is to return an empty plot without an error, meaning that the current code \r\n\r\nhttps://github.com/optuna/optuna/blob/1925ca1a051b3bf11819478b65d830de614774b2/optuna/visualization/_contour.py#L203-L231\r\n\r\nshould be fixed like:\r\n\r\n```python\r\ndef _get_contour_subplot(\r\n    info: _SubContourInfo,\r\n    reverse_scale: bool,\r\n    target_name: str = \"Objective Value\",\r\n) -> tuple[\"Contour\", \"Scatter\", \"Scatter\"]:\r\n    x_indices = info.xaxis.indices\r\n    y_indices = info.yaxis.indices\r\n\r\n    if len(x_indices) < 2 or len(y_indices) < 2 or len(info.z_values) == 0:\r\n        return go.Contour(), go.Scatter(), go.Scatter()\r\n```\r\n\r\n> [!NOTE]\r\n> As long as the early return is defined before this line, any change is acceptable.\r\n> ```python\r\n> z_values[xys[:, 1], xys[:, 0]] = zs\r\n> ```",
    "comments": [
      {
        "user": "nabenabe0928",
        "body": "Thank you for raising the issue!\r\nWe can simply address this issue by Solution 2, i.e., to return an empty plot, with warning:)"
      },
      {
        "user": "not522",
        "body": "This issue has been resolved by #5630."
      }
    ]
  },
  {
    "issue_number": 5035,
    "title": "Bring `_CachedStorage` into `RDBStorage`",
    "author": "contramundum53",
    "state": "closed",
    "created_at": "2023-10-13T11:08:19Z",
    "updated_at": "2024-08-19T23:06:27Z",
    "labels": [
      "code-fix",
      "needs-discussion",
      "stale"
    ],
    "body": "### Motivation\r\n\r\n`_CachedStorage` is only used from `RDBStorage`. It can be moved into `RDBStorage`.\r\n\r\n### Suggestion\r\n\r\nMove `_CachedStorage` (https://github.com/optuna/optuna/blob/3fcbf8839c760916cc0cf38fb545f526d5efe85d/optuna/storages/_cached_storage.py#L38) into `RDBStorage`.\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "BitVivek",
        "body": "I want to work on it , I am new as contributer , can I start working on it ?"
      },
      {
        "user": "contramundum53",
        "body": "Of course, thank you!"
      },
      {
        "user": "BitVivek",
        "body": "I have a question regarding the integration of _CachedStorage into RDBStorage. With this integration, all methods would first check data from the cache memory before querying the RDBStorage.Currently, the _CachedStorage class is integrated with RDBStorage. However, considering the potential for changing storage mechanisms in the future, is it advisable to proceed with this integration?"
      }
    ]
  },
  {
    "issue_number": 5199,
    "title": "Problem with running Optuna study because Lightning cannot be imported (not found)",
    "author": "Mballerina",
    "state": "closed",
    "created_at": "2024-01-24T13:25:36Z",
    "updated_at": "2024-08-19T03:59:28Z",
    "labels": [
      "code-fix"
    ],
    "body": "### Motivation\n\nMac M1 with pytorch Lightning prevents optuna study.optimize() from running without adaptation of code.\n\n### Suggestion\n\nCode needs to be adapted so that Lightning Trainer defaults to 'auto': \r\n\r\nPYTORCH_ENABLE_MPS_FALLBACK=1\r\n\r\nimport warnings\r\n\r\nfrom packaging import version\r\n\r\nimport optuna\r\nfrom optuna.storages._cached_storage import _CachedStorage\r\nfrom optuna.storages._rdb.storage import RDBStorage\r\n\r\n\r\n# Define key names of `Trial.system_attrs`.\r\n_PRUNED_KEY = \"ddp_pl:pruned\"\r\n_EPOCH_KEY = \"ddp_pl:epoch\"\r\n\r\n\r\nwith optuna._imports.try_import() as _imports:\r\n    import pytorch_lightning as pl\r\n    from pytorch_lightning import LightningModule\r\n    from pytorch_lightning import Trainer   # here trainer only accepts mps and cpu, but optimization cannot handle mps always so 'auto'\r\n    from pytorch_lightning.callbacks import Callback\r\n    from pytorch_lightning.strategies import ParallelStrategy\r\n\r\nif not _imports.is_successful():\r\n    Callback = object  # type: ignore  # NOQA\r\n    LightningModule = object  # type: ignore  # NOQA\r\n    Trainer = object  # type: ignore  # NOQA\r\n\r\ndef _is_ddp_backend(strategy):\r\n    \"\"\"Check for DDP training strategy.\r\n\r\n    This may be too inclusive a check, but I haven't tested it comprehensively.\r\n    Derived from https://github.com/Lightning-AI/lightning/blob/e57f12851fe68ccfb190b368e173d5f4fad67a41/src/lightning/pytorch/trainer/connectors/accelerator_connector.py#L223\r\n    \"\"\"\r\n    is_ddp_str = isinstance(strategy, str) and \"ddp\" in strategy\r\n    is_deepspeed_str = isinstance(strategy, str) and \"deepspeed\" in strategy\r\n    return isinstance(strategy, ParallelStrategy) or is_ddp_str or is_deepspeed_str\r\n\r\nif version.parse(pl.__version__) >= version.parse(\"1.8.0\"):\r\n\r\n    class PyTorchLightningPruningCallback(Callback):\r\n        \"\"\"PyTorch Lightning callback to prune unpromising trials.\r\n\r\n        See `the example <https://github.com/optuna/optuna-examples/blob/\r\n        main/pytorch/pytorch_lightning_simple.py>`__\r\n        if you want to add a pruning callback which observes accuracy.\r\n\r\n        Args:\r\n            trial:\r\n                A :class:`~optuna.trial.Trial` corresponding to the current evaluation of the\r\n                objective function.\r\n            monitor:\r\n                An evaluation metric for pruning, e.g., ``val_loss`` or\r\n                ``val_acc``. The metrics are obtained from the returned dictionaries from e.g.\r\n                ``pytorch_lightning.LightningModule.training_step`` or\r\n                ``pytorch_lightning.LightningModule.validation_epoch_end`` and the names thus depend on\r\n                how this dictionary is formatted.\r\n\r\n        .. note::\r\n            For the distributed data parallel training, the version of PyTorchLightning needs to be\r\n            higher than or equal to v1.5.0. In addition, :class:`~optuna.study.Study` should be\r\n            instantiated with RDB storage.\r\n        \"\"\"\r\n\r\n        def __init__(self, trial: optuna.trial.Trial, monitor: str) -> None:\r\n            _imports.check()\r\n            super().__init__()\r\n\r\n            self._trial = trial\r\n            self.monitor = monitor\r\n            self.is_ddp_backend = False\r\n\r\n        def setup(self, trainer: Trainer(accelerator='auto'), pl_module: LightningModule, stage: str) -> None:\r\n            # Fix\r\n            self.is_ddp_backend = _is_ddp_backend(trainer._accelerator_connector.strategy)\r\n            if self.is_ddp_backend:\r\n                if not (\r\n                    isinstance(self._trial.study._storage, _CachedStorage)\r\n                    and isinstance(self._trial.study._storage._backend, RDBStorage)\r\n                ):\r\n                    raise ValueError(\r\n                        \"optuna.integration.PyTorchLightningPruningCallback\"\r\n                        \" supports only optuna.storages.RDBStorage in DDP.\"\r\n                    )\r\n\r\n        def on_validation_end(self, trainer: Trainer(accelerator='auto'), pl_module: LightningModule) -> None:\r\n\r\n            # When the trainer calls `on_validation_end` for sanity check,\r\n            # do not call `trial.report` to avoid calling `trial.report` multiple times\r\n            # at epoch 0. The related page is\r\n            # https://github.com/PyTorchLightning/pytorch-lightning/issues/1391.\r\n            if trainer.sanity_checking:\r\n                return\r\n\r\n            epoch = pl_module.current_epoch\r\n\r\n            current_score = trainer.callback_metrics.get(self.monitor)\r\n            if current_score is None:\r\n                message = (\r\n                    \"The metric '{}' is not in the evaluation logs for pruning. \"\r\n                    \"Please make sure you set the correct metric name.\".format(self.monitor)\r\n                )\r\n                warnings.warn(message)\r\n                return\r\n\r\n            should_stop = False\r\n            if trainer.is_global_zero:\r\n                self._trial.report(current_score.item(), step=epoch)\r\n                should_stop = self._trial.should_prune()\r\n            should_stop = trainer.strategy.broadcast(should_stop)\r\n            if not should_stop:\r\n                return\r\n\r\n            if not self.is_ddp_backend:\r\n                message = \"Trial was pruned at epoch {}.\".format(epoch)\r\n                raise optuna.TrialPruned(message)\r\n            else:\r\n                # Stop every DDP process if global rank 0 process decides to stop.\r\n                trainer.should_stop = True\r\n                if trainer.is_global_zero:\r\n                    self._trial.storage.set_trial_system_attr(self._trial._trial_id, _PRUNED_KEY, True)\r\n                    self._trial.storage.set_trial_system_attr(self._trial._trial_id, _EPOCH_KEY, epoch)\r\n\r\n        def on_fit_end(self, trainer: Trainer(accelerator='auto'), pl_module: LightningModule) -> None:\r\n            if not self.is_ddp_backend:\r\n                return\r\n\r\n            # Because on_validation_end is executed in spawned processes,\r\n            # _trial.report is necessary to update the memory in main process, not to update the RDB.\r\n            _trial_id = self._trial._trial_id\r\n            _study = self._trial.study\r\n            _trial = _study._storage._backend.get_trial(_trial_id)  # type: ignore\r\n            _trial_system_attrs = _study._storage.get_trial_system_attrs(_trial_id)\r\n            is_pruned = _trial_system_attrs.get(_PRUNED_KEY)\r\n            epoch = _trial_system_attrs.get(_EPOCH_KEY)\r\n            intermediate_values = _trial.intermediate_values\r\n            for step, value in intermediate_values.items():\r\n                self._trial.report(value, step=step)\r\n\r\n            if is_pruned:\r\n                message = \"Trial was pruned at epoch {}.\".format(epoch)\r\n                raise optuna.TrialPruned(message)\r\n\r\nelse:\r\n    PyTorchLightningPruningCallback = optuna.integration.PyTorchLightningPruningCallback\n\n### Additional context (optional)\n\nAdditionally I am working with tcn the arguments need to be passed for when weight_norm (and other non mps ) functions are included;\r\n\r\n# GPU/MPS/CPU: detect if a GPU is available\r\n    if torch.cuda.is_available():\r\n        pl_trainer_kwargs = {\r\n            \"accelerator\": \"gpu\",  #used to be gpu\r\n            \"gpus\": -1,\r\n            \"auto_select_gpus\": True,\r\n            \"callbacks\": callbacks,\r\n            #\"callbacks\": [TFMProgressBar(enable_train_bar_only=True)],  #disable progress bars for all model stages except training.\r\n        }\r\n        num_workers = 4\r\n    elif torch.cuda.is_available()==False:\r\n        pl_trainer_kwargs = {\r\n            \"accelerator\": \"auto\",  #detect if MPS can be used if no GPU\r\n            \"callbacks\": callbacks}\r\n        num_workers = 0\r\n    else:\r\n        pl_trainer_kwargs = {\r\n            \"accelerator\": \"cpu\", #fallback on CPU\r\n            \"callbacks\": callbacks}\r\n        num_workers = 0",
    "comments": [
      {
        "user": "nzw0301",
        "body": "Could you fix the code snippet to run your script?  "
      },
      {
        "user": "AndrewJGroves",
        "body": "Hi, I got the same problem running this example for darts \r\nhttps://github.com/unit8co/darts/blob/master/examples/17-hyperparameter-optimization.ipynb\r\n"
      },
      {
        "user": "nzw0301",
        "body": "@AndrewJGroves Could you share the minimal reproducible code? The shared notebook is too long."
      }
    ]
  },
  {
    "issue_number": 4818,
    "title": "Support more automatic termination algorithms (`optuna.terminator`)",
    "author": "toshihikoyanase",
    "state": "closed",
    "created_at": "2023-07-20T07:53:00Z",
    "updated_at": "2024-08-18T23:05:52Z",
    "labels": [
      "feature",
      "contribution-welcome",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nOptuna terminator was added in Optuna v3.2 to automatically terminate optimization based on a method proposed in the research paper \"[Automatic Termination for Hyperparameter Optimization](https://www.amazon.science/publications/automatic-termination-for-hyperparameter-optimization)\" [1].\r\n\r\nCurrently, the terminator only supports the regret-bound based method mentioned in the paper. However, the paper also describes some alternative approaches, such as EI and PI threshold-based methods, which are more convenient as they do not require cross-validation.\r\n\r\n### Description\r\n\r\nI suggest adding support for EI and PI threshold-based methods in the Optuna terminator. These methods are easy to use and would be useful for users who do not want to perform cross-validation. The [paper](https://www.amazon.science/publications/automatic-termination-for-hyperparameter-optimization) suggests using pre-defined thresholds for these methods and provides some examples.\r\n\r\n> Threshold for Expected improvement (EI; Nguyen et al., 2017): stopping BO once EI drops below a pre-defined threshold. Choosing a threshold crucially depends on a problem at hand, e.g., values studied by the original paper result in too aggressive stopping across a range of our experiments. We thus extend it with a more finer grained grid resulting into {10−9, 10−13, 10−17}.\r\n\r\n> Threshold for Probability of improvement (PI; Lorenz et al., 2016): stopping BO once PI drops below a pre-defined threshold. Similar to EI baseline, we tune the threshold and use {10−5, 10−9, 10−13}.\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\n- The tracking issue of Optuna terminator https://github.com/optuna/optuna/issues/4398",
    "comments": [
      {
        "user": "toshihikoyanase",
        "body": "@smygw72 will work on a part of this PR regarding the Conv-i termination algorithm."
      },
      {
        "user": "smygw72",
        "body": "I'm planning to implement a new evaluator class named `ExpectedImprovementEvaluator` with `botorch.acquisition.analytic.ExpectedImprovement`."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 5061,
    "title": "`GridSampler` currently does not support Sequences as a type of categorical feature",
    "author": "Koen1999",
    "state": "closed",
    "created_at": "2023-10-20T13:38:31Z",
    "updated_at": "2024-08-15T23:05:31Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nI would like to perform grid search, where one of the parameters is categorical. The type of the categorical feature is given by `Sequence[str]`.\r\n\r\nMore specifically, I want to supply the following to the `GridSampler`as `search_space`:\r\n`{'groupby': (('src',), ('dst',), ('src', 'dst',), ('src_dst',), ('src', 'src_dst',), ('dst', 'src_dst',), ('src', 'dst', 'src_dst',)),}`\r\n\r\n### Description\r\n\r\nI think `GridSampler` should support this feature, since a `Sequence` can be considered as a slightly more complex type of categorical parameter.\r\n\r\nI do think one constraint to be put on the Sequence is that it should be hashable. Otherwise, comparison of grid search spaces will fail.\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\nThis also triggered a downstream error in `optuna-dashboard`. Please see: https://github.com/optuna/optuna-dashboard/issues/665\r\n\r\nThis also related to the `CategoricalChoiceType` and `CategoricalDistribution`, which do not support this behavior either at the moment.",
    "comments": [
      {
        "user": "Koen1999",
        "body": "I just discovered there are several related older issues that were never addressed: https://github.com/optuna/optuna/issues/3292 https://github.com/optuna/optuna/issues/2341 https://github.com/optuna/optuna/issues/905 https://github.com/optuna/optuna/issues/3093#issuecomment-968075749\r\nTagged to keep all the issues somewhat together."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      }
    ]
  },
  {
    "issue_number": 3210,
    "title": "Change parameter value after trial",
    "author": "eafpres",
    "state": "closed",
    "created_at": "2021-12-24T22:53:53Z",
    "updated_at": "2024-08-15T02:49:02Z",
    "labels": [
      "question"
    ],
    "body": "I have an optimization of an xgboost model. I use early stopping, and the initial iters is set by an optuna sampler. I would like to update the iters value of the trial after the trial so that the optimizer \"sees\" the actual iterations used vs just the max.  I cannot find a way to do that.\r\n\r\n### Environment\r\n\r\nOS Ubuntu 20.04\r\nOptuna 2.10.0\r\nPython 3.8\r\n\r\n\r\n",
    "comments": [
      {
        "user": "fsmosca",
        "body": "What do you mean by iters? Can you show a minimal example code of what you are trying to achieve?"
      },
      {
        "user": "eafpres",
        "body": "@fsmosca:  Here is my objective function:\r\n\r\n```\r\n# train xgboost model\r\n#\r\ndef train_xgboost(trial,\r\n                  train_X,\r\n                  train_Y,\r\n                  val_X,\r\n                  val_Y,\r\n\t\t\t\t  early_stopping,\r\n                  study):\r\n#\r\n  objective = 'reg:gamma'\r\n  eval_metric = 'gamma-deviance'\r\n#\r\n  n_iters = int(trial.suggest_discrete_uniform('n_iters', 10, 1000, 10))\r\n#\r\n  model = xgb.XGBRegressor(objective = objective,\r\n                           eval_metric = eval_metric)\r\n#\r\n  eval_set = [(val_X, val_Y)]\r\n#\r\n  model.fit(train_X,\r\n            train_Y,\r\n            eval_set = eval_set,\r\n            early_stopping_rounds = early_stopping,\r\n            verbose = 0)\r\n#\r\n  results = model.evals_result()['validation_0'][eval_metric]\r\n  result = results[-1]\r\n  trial.set_user_attr('act_iters', len(results))\r\n#\r\n  return result\r\n```\r\n\r\nSo the maximum rounds of boosting are set by n_iters in the trial.suggest() method, but the actual rounds is also affected by early stopping.  The early_stopping parameter tells the method to stop training if early_stopping_rounds pass without improvement in the objective.  \r\n\r\nSo my issue is in the overall optimization, for example using TPE optimizer, it is using the maximum values not the actual values, so whatever effect the number of iterations has interacting with other hyperparameters is lost.\r\n\r\nSo I wanted to update the n_iters value to 'act_iters\" which is determined from the actual stopping round.\r\n\r\nI know I can do this another way--set the max to a large value, then use\r\nn_iters = int(trial.suggest_discrete_uniform('n_iters', act_iters, act_iters, 1) which will force it to act_iters.  However, I actually want the maximum to be varied, so this loses some of the functionality I have now.  If I could change the value of n_iters before it is stored back to the experiments which are used for the optimization, that would be a bit closer to what I want.\r\n\r\nNote the for minimum repro I removed most of the actual hyperparameters used in the overall optimization; there are several more than n_iters in the actual code."
      },
      {
        "user": "fsmosca",
        "body": "> So my issue is in the overall optimization, for example using TPE optimizer, it is using the maximum values not the actual values, so whatever effect the number of iterations has interacting with other hyperparameters is lost.\r\n\r\nSorry but your issue in unclear to me.\r\n\r\nHave a look at the example below tell me where is the problem.\r\n```python\r\nstop_round = 20\r\npct = 0.1  # stop_round is 10% of min_iter\r\nmin_iter = int(stop_round / pct) = 20/0.1 = 200\r\n\r\nn_iters = int(trial.suggest_discrete_uniform('n_iters', min_iter, 1000, 10))\r\nmodel = xgb.XGBRegressor(n_estimators=n_iters, ...)\r\nmodel.fit(early_stopping_rounds=stop_round, ...)\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 5622,
    "title": "Fix the expired links caused by the directory name change in `optuna-examples`",
    "author": "kAIto47802",
    "state": "closed",
    "created_at": "2024-08-09T08:01:59Z",
    "updated_at": "2024-08-14T07:49:23Z",
    "labels": [
      "document"
    ],
    "body": "### What is an issue?\r\n\r\nThe directory structure of optuna-examples related to basic and faq has changed in several pull requests, such as  https://github.com/optuna/optuna-examples/pull/277, causing some links to expire. The expired link is:\r\n- https://github.com/optuna/optuna/blob/master/optuna/pruners/_successive_halving.py#L23",
    "comments": []
  },
  {
    "issue_number": 5616,
    "title": "`report_cross_validation_scores()` in combination with `TerminatorCallback()`  needs `torch` dependency",
    "author": "thomasATbayer",
    "state": "closed",
    "created_at": "2024-08-07T07:26:11Z",
    "updated_at": "2024-08-14T01:40:25Z",
    "labels": [
      "feature"
    ],
    "body": "### Motivation\n\nThere is the really nice feature that allows the tuning to stop earlier if there has been no variance in the objective for some iterations. But this needs a torch dependency. This makes python environments quite huge (several GB) and also introduces a large number of extra packages.\n\n### Description\n\nSo would be great if report_cross_validation_scores() in combination with TerminatorCallback() would run without a torch dependency.\n\n### Alternatives (optional)\n\n_No response_\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "kAIto47802",
        "body": "@thomasATbayer \r\nAre you trying to run in a Linux environment?\r\n`pip install torch` will install the CUDA version, which is much larger than the CPU-only version.\r\nAs the terminator (and any other Optuna features) only depends on the CPU-only version, it is totally no problem to install the CPU-only version.\r\nSo could you try the CPU-only install?\r\n\r\n```bash\r\n$ pip install torch --index-url https://download.pytorch.org/whl/cpu\r\n```"
      },
      {
        "user": "nabenabe0928",
        "body": "@thomasATbayer \r\nDid the response by @kAIto47802 resolve your issue?\r\nIf so, I would like to close this issue. "
      },
      {
        "user": "thomasATbayer",
        "body": "> @thomasATbayer Did the response by @kAIto47802 resolve your issue? If so, I would like to close this issue.\r\n\r\nYes, many thanks for the help. That should solve the issue."
      }
    ]
  },
  {
    "issue_number": 5620,
    "title": "Using Optuna with Matlab",
    "author": "ozilxu",
    "state": "closed",
    "created_at": "2024-08-08T10:10:53Z",
    "updated_at": "2024-08-08T10:34:44Z",
    "labels": [],
    "body": "### Don't use GitHub Issues to ask support questions.\n\nHello, may I ask if this toolbox can be used in the matlab? Thanks!",
    "comments": []
  },
  {
    "issue_number": 5617,
    "title": "Run optimization iteratively",
    "author": "zzzrbx",
    "state": "closed",
    "created_at": "2024-08-07T11:25:11Z",
    "updated_at": "2024-08-07T13:40:17Z",
    "labels": [],
    "body": "### Don't use GitHub Issues to ask support questions.\r\n\r\nIs it possible to run trials iteratively, for example:\r\nfor i in range(0, num_experiments):\r\n    # do something. \r\n    # get run new trial. \r\n    # get results. \r\n    # do something else. \r\n",
    "comments": []
  },
  {
    "issue_number": 5013,
    "title": "JournalStorage: Checkpoints / Compression",
    "author": "Turakar",
    "state": "closed",
    "created_at": "2023-10-10T08:26:40Z",
    "updated_at": "2024-08-06T23:06:00Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nMy JournalStorage has grown to 172 MB in size, also because I store training logs in the user attributes. In general, every JournalStorage will grow to a large size during longer operation. Transferring the JournalStorage from the HPC cluster to my laptop begins to be a nuisance due to this. Sever-based storage are no alternative for me, as I cannot start a central server on the HPC cluster. It might be good to consider checkpointing / switching to a faster or more compact message format to reduce the Journal size.\r\n\r\n\r\n\r\n### Description\r\n\r\n**Checkpointing** Currently, all operations are written to a single file. For checkpointing to work, one process should acquire an exclusive lock on this file, re-write its contents into a compressed format, and then release the lock. This would decrease file size and loading time. All other processes need to recognize this event and update their position in the file accordingly.\r\n\r\n**Message format** Currently, all messages are stored in JSON arrays. We might consider adding something like [msgpack](https://github.com/msgpack/msgpack-python). I think that the [streaming unpacker](https://github.com/msgpack/msgpack-python#streaming-unpacking) might improve performance significantly (If the actual processing of the read  data is not dominating the runtime, that is. I did not profile it, yet.).\r\n\r\n### Alternatives (optional)\r\n\r\n_No response_\r\n\r\n### Additional context (optional)\r\n\r\nFurther things to consider:\r\n\r\n- How to handle past data which is not relevant anymore? Currently, a deleted study is not actually deleted from the JournalStorage, but it is only marked as deleted. In the compressed variant, one may choose to delete this data forever, or keep it somewhere. I prefer the former, as this mimics how the other storages work.\r\n- When should we compress the storage? Maybe based on file size?",
    "comments": [
      {
        "user": "c-bata",
        "body": "Hi @Turakar, thanks for your suggestions!\r\n\r\nWe actually did contemplate using binary serialization formats like msgpack or protobuf when we first introduced JournalStorage. However, we chose to stick with JSON Lines for its easier maintenance and lesser dependencies.\r\n\r\nThat being said, your ideas for improving and enhancing the performance are very intriguing. Instead of integrating all these changes directly into Optuna, how about we make them available as a third-party library? The functionality sounds quite beneficial, and if it becomes a library, we could even feature it in our official documentation or blog posts.\r\n\r\nRegarding your last question, it's worth noting that JournalStorage includes a Snapshot feature. Similar to JournalRedisStorage, we can implement save_snapshot() and load_snapshot() methods, which would mean we wouldn't have to review the logs from the start. Though the feature was introduced for speed-up rather than compression, it might be possible to discard past logs up to a certain point during the save_snapshot() method.\r\n\r\nhttps://github.com/optuna/optuna/blob/c21fe6253343950f31d9ae0bc13be384236335e6/optuna/storages/_journal/redis.py#L92-L97"
      },
      {
        "user": "Turakar",
        "body": "It seems like snapshots are only implemented for Redis, which does require a central server.\r\n\r\nI would be open to the idea of a library, as I generally think that there is still quite some potential for how we keep central storage in NFS HPC systems in Python. However, I currently do not have the capacity to implement it."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  },
  {
    "issue_number": 4124,
    "title": "Add a function that decides the likely-best parameter from the trials executed so far",
    "author": "contramundum53",
    "state": "closed",
    "created_at": "2022-11-06T00:54:29Z",
    "updated_at": "2024-08-04T09:32:23Z",
    "labels": [
      "feature",
      "stale"
    ],
    "body": "### Motivation\r\n\r\nCurrently, Optuna only has the interface for returning the trial that resulted in the best value after the study is over. This is not good when the objective function is noisy and the number of trials are small, since a bad parameter can result in the best value by chance. Instead, we should use some algorithm to decide the parameter that would most likely yield the best result in the evaluation run.\r\n\r\n\r\n### Description\r\n\r\n\r\nEach black-box optimization algorithm usually has some natural way to do that. Examples include:\r\n* For GP, the natural way would be returning the parameter with highest expected value in the surrogate GP model.\r\n* For TPE, the natural way would be returning the parameter with highest probability density in \"below\" distribution.\r\n* For CMA-ES, the natural way would be returning the mean value of the normal distribution.\r\n\r\nAlthough these algorithms are inspired by each sampler, these algorithms can be used independently from the sampler. Thus, we could implement these algorithms as free-standing functions.\r\n\r\n\r\n### Alternatives (optional)\r\n\r\nAlternatively, we could add a function in applicable samplers.\r\n\r\n### Additional context (optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed automatically because it had not seen any recent activity. If you want to discuss it, you can reopen it freely."
      },
      {
        "user": "shindannin",
        "body": "I strongly support this idea. In environments with limited computational resources, we can't afford to perform extensive calculations to remove noise from noisy functions. Therefore, having functionality like this would be helpful.\r\nAdditionally, if internal features like CMA-ES sample are already available, implementation would be relatively straightforward."
      }
    ]
  },
  {
    "issue_number": 5604,
    "title": "Multi-objective Optimization with Optuna",
    "author": "krishdotn1",
    "state": "closed",
    "created_at": "2024-08-01T07:09:43Z",
    "updated_at": "2024-08-01T07:31:05Z",
    "labels": [
      "question"
    ],
    "body": "### Expected behavior\n\nI found on doc that multi-object optimization is possible.\r\n![image](https://github.com/user-attachments/assets/e9673190-65f4-4b7c-b0f6-3981c23e012f)\r\n\r\nbut it show me an error \r\n![image](https://github.com/user-attachments/assets/520dd207-2e4b-4fe9-9cc9-352f397210fc)\r\n\r\nbelow is my actual code. \r\n\r\nstudy = optuna.create_study(\r\n    storage=\"sqlite:///./db.sqlite3\", \r\n    study_name=tb_time,\r\n    direction=[\"maximize\", \"minimize\"],\r\n    pruner=optuna.pruners.MedianPruner()\r\n)\r\nstudy.optimize(objective, n_trials=100, n_jobs=8)\n\n### Environment\n\n- Optuna version:\r\n- Python version:\r\n- OS:\r\n- (Optional) Other libraries and their versions:\r\n\n\n### Error messages, stack traces, or logs\n\n```shell\nValueError                                Traceback (most recent call last)\r\nCell In[7], line 1\r\n----> 1 study = optuna.create_study(\r\n      2     storage=\"sqlite:///./db.sqlite3\", \r\n      3     study_name=tb_time,\r\n      4     direction=[\"maximize\", \"minimize\"],\r\n      5     pruner=optuna.pruners.MedianPruner()\r\n      6 )\r\n      7 study.optimize(objective, n_trials=1, n_jobs=n_cpu)\r\n\r\nFile c:\\Users\\krish\\anaconda3\\envs\\finrl\\lib\\site-packages\\optuna\\_convert_positional_args.py:83, in convert_positional_args.<locals>.converter_decorator.<locals>.converter_wrapper(*args, **kwargs)\r\n     77     raise TypeError(\r\n     78         f\"{func.__name__}() got multiple values for arguments {duplicated_kwds}.\"\r\n     79     )\r\n     81 kwargs.update(inferred_kwargs)\r\n---> 83 return func(**kwargs)\r\n\r\nFile c:\\Users\\krish\\anaconda3\\envs\\finrl\\lib\\site-packages\\optuna\\study\\study.py:1240, in create_study(storage, sampler, pruner, study_name, direction, load_if_exists, directions)\r\n   1235     raise ValueError(\"The number of objectives must be greater than 0.\")\r\n   1236 elif any(\r\n   1237     d not in [\"minimize\", \"maximize\", StudyDirection.MINIMIZE, StudyDirection.MAXIMIZE]\r\n   1238     for d in directions\r\n   1239 ):\r\n-> 1240     raise ValueError(\r\n...\r\n   1246     d if isinstance(d, StudyDirection) else StudyDirection[d.upper()] for d in directions\r\n   1247 ]\r\n   1249 storage = storages.get_storage(storage)\r\n\r\nValueError: Please set either 'minimize' or 'maximize' to direction. You can also set the corresponding `StudyDirection` member.\n```\n\n\n### Steps to reproduce\n\n1.\r\n2.\r\n3.\r\n```python\r\n# python code\r\n```\r\n\n\n### Additional context (optional)\n\n_No response_",
    "comments": [
      {
        "user": "y0z",
        "body": "Thank you for your comment.\r\n\r\nI found two issues in your code.\r\n```python\r\nstudy = optuna.create_study(\r\nstorage=\"sqlite:///./db.sqlite3\",\r\nstudy_name=tb_time,\r\ndirection=[\"maximize\", \"minimize\"],\r\npruner=optuna.pruners.MedianPruner()\r\n)\r\nstudy.optimize(objective, n_trials=100, n_jobs=8)\r\n```\r\n\r\n1. `direction=[\"maximize\", \"minimize\"],` should be `directions=[\"maximize\", \"minimize\"],` (cf. [reference](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html#optuna.study.create_study))\r\n2. Pruners cannot be used for multi-objective optimization because it is not supported at the moment (cf. [reference](https://optuna.readthedocs.io/en/stable/reference/pruners.html#module-optuna.pruners), #3450).\r\n\r\nPlease try again after resolving these issues."
      },
      {
        "user": "krishdotn1",
        "body": "Thank you! \r\nit's work. :)"
      }
    ]
  },
  {
    "issue_number": 4916,
    "title": "Fix the documentation warnings with Sphinx `nitpicky`: `torch.distributed.ProcessGroup` is not linked to the pytorch document",
    "author": "HideakiImamura",
    "state": "closed",
    "created_at": "2023-09-11T07:22:43Z",
    "updated_at": "2024-07-28T23:06:34Z",
    "labels": [
      "document",
      "contribution-welcome",
      "stale"
    ],
    "body": "### What is an issue?\n\nDerivative of https://github.com/optuna/optuna/issues/4213.\r\n\r\nBuilding the documentation using [Sphinx's nitpicky](https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-nitpicky), we can observe several warnings.\r\n\r\nIn this issue, I will suggest the problem reported [here](https://github.com/optuna/optuna/pull/4290#discussion_r1095256156). \r\n\r\n#### Repro\r\n\r\n1. Add the `-n` (`nitpicky=True`) option in `docs/Makefile`\r\n\r\n```diff\r\ndiff --git a/docs/Makefile b/docs/Makefile\r\nindex 4bdf05f33..579749286 100644\r\n--- a/docs/Makefile\r\n+++ b/docs/Makefile\r\n@@ -2,7 +2,7 @@\r\n #\r\n\r\n # You can set these variables from the command line.\r\n-SPHINXOPTS    = -W --keep-going\r\n+SPHINXOPTS    = -W --keep-going -n\r\n SPHINXBUILD   = sphinx-build\r\n SPHINXPROJ    = Optuna\r\n SOURCEDIR     = source\r\n```\r\n\r\n2. Build the documentation and observe the warnings.\r\n\r\n```console\r\n$ pip install -e '.[document]'\r\n$ cd docs\r\n$ make html\r\n# Notice list of warnings.\r\n```\r\n\r\n3. (Optional) Debug/check references in the build.\r\n\r\n```console\r\n$ open build/html/index.html\r\n```",
    "comments": [
      {
        "user": "hamster-86",
        "body": "A document linked to `torch.distributed.ProcessGroup` does not exist."
      },
      {
        "user": "nzw0301",
        "body": "I think this issue mentions [this section](https://optuna.readthedocs.io/en/latest/reference/generated/optuna.integration.TorchDistributedTrial.html#optuna.integration.TorchDistributedTrial), where `torch.distributed.ProcessGroup` and `ProcessGroup` are not linked correctly."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has not seen any recent activity."
      }
    ]
  }
]