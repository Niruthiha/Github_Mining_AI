[
  {
    "issue_number": 4180,
    "title": "Add some iteration method on a dataset column (specific for inference)",
    "author": "Narsil",
    "state": "closed",
    "created_at": "2022-04-19T09:15:45Z",
    "updated_at": "2025-06-17T13:08:50Z",
    "labels": [
      "enhancement"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is.\r\n\r\nCurrently, `dataset[\"audio\"]` will load EVERY element in the dataset in RAM, which can be quite big for an audio dataset.\r\nHaving an iterator (or sequence) type of object, would make inference with `transformers` 's `pipeline` easier to use and not so memory hungry.\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\nFor a non breaking change:\r\n\r\n```python\r\nfor audio in dataset.iterate(\"audio\"):\r\n    # {\"array\": np.array(...), \"sampling_rate\":...}\r\n```\r\n\r\nFor a  breaking change solution (not necessary), changing the type of `dataset[\"audio\"]` to a sequence type so that\r\n\r\n```python\r\npipe = pipeline(model=\"...\")\r\nfor out in pipe(dataset[\"audio\"]):\r\n    # {\"text\":....}\r\n```\r\ncould work\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n```python\r\ndef iterate(dataset, key):\r\n    for item in dataset:\r\n        yield dataset[key]\r\n\r\nfor out in pipeline(iterate(dataset, \"audio\")):\r\n    # {\"array\": ...}\r\n```\r\n\r\nThis works but requires the helper function which feels slightly clunky.\r\n\r\n**Additional context**\r\nAdd any other context about the feature request here.\r\n\r\nThe context is actually to showcase better integration between  `pipeline` and `datasets` in the Quicktour demo: https://github.com/huggingface/transformers/pull/16723/files\r\n\r\n@lhoestq \r\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Thanks for the suggestion ! I agree it would be nice to have something directly in `datasets` to do something as simple as that\r\n\r\ncc @albertvillanova @mariosasko @polinaeterna What do you think if we have something similar to pandas `Series` that wouldn't bring everything in memory when doing `dataset[\"audio\"]` ? Currently it returns a list with all the decoded audio data in memory.\r\n\r\nIt would be a breaking change though, since `isinstance(dataset[\"audio\"], list)` wouldn't work anymore, but we could implement a `Sequence` so that `dataset[\"audio\"][0]` still works and only loads one item in memory.\r\n\r\nYour alternative suggestion with `iterate` is also sensible, though maybe less satisfactory in terms of experience IMO"
      },
      {
        "user": "albertvillanova",
        "body": "I agree that current behavior (decoding all audio file sin the dataset when accessing `dataset[\"audio\"]`) is not useful, IMHO. Indeed in our docs, we are constantly warning our collaborators not to do that.\r\n\r\nTherefore I upvote for a \"useful\" behavior of `dataset[\"audio\"]`. I don't think the breaking change is important in this case, as I guess no many people use it with its current behavior. Therefore, for me it seems reasonable to return a generator (instead of an in-memeory list) for \"special\" features, like Audio/Image.\r\n\r\n@lhoestq on the other hand I don't understand your proposal about Pandas-like... "
      },
      {
        "user": "mariosasko",
        "body": "I recall I had the same idea while working on the `Image` feature, so I agree implementing something similar to `pd.Series` that lazily brings elements in memory would be beneficial."
      }
    ]
  },
  {
    "issue_number": 648,
    "title": "offset overflow when multiprocessing batched map on large datasets.",
    "author": "richarddwang",
    "state": "closed",
    "created_at": "2020-09-19T02:15:11Z",
    "updated_at": "2025-06-17T12:56:07Z",
    "labels": [
      "bug"
    ],
    "body": "It only happened when \"multiprocessing\" + \"batched\" + \"large dataset\" at the same time.\r\n\r\n```\r\ndef bprocess(examples):\r\n  examples['len'] = []\r\n  for text in examples['text']:\r\n    examples['len'].append(len(text))\r\n  return examples\r\nwiki.map(brpocess, batched=True, num_proc=8)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nRemoteTraceback                           Traceback (most recent call last)\r\nRemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 153, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/yisiang/datasets/src/datasets/fingerprint.py\", line 163, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 1486, in _map_single\r\n    batch = self[i : i + batch_size]\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 1071, in __getitem__\r\n    format_kwargs=self._format_kwargs,\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 972, in _getitem\r\n    data_subset = self._data.take(indices_array)\r\n  File \"pyarrow/table.pxi\", line 1145, in pyarrow.lib.Table.take\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/pyarrow/compute.py\", line 268, in take\r\n    return call_function('take', [data, indices], options)\r\n  File \"pyarrow/_compute.pyx\", line 298, in pyarrow._compute.call_function\r\n  File \"pyarrow/_compute.pyx\", line 192, in pyarrow._compute.Function.call\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nArrowInvalid                              Traceback (most recent call last)\r\n in \r\n     30   owt = datasets.load_dataset('/home/yisiang/datasets/datasets/openwebtext/openwebtext.py', cache_dir='./datasets')['train']\r\n     31   print('load/create data from OpenWebText Corpus for ELECTRA')\r\n---> 32   e_owt = ELECTRAProcessor(owt, apply_cleaning=False).map(cache_file_name=f\"electra_owt_{c.max_length}.arrow\")\r\n     33   dsets.append(e_owt)\r\n     34 \r\n\r\n~/Reexamine_Attention/electra_pytorch/_utils/utils.py in map(self, **kwargs)\r\n    126       writer_batch_size=10**4,\r\n    127       num_proc=num_proc,\r\n--> 128       **kwargs\r\n    129     )\r\n    130 \r\n\r\n~/hugdatafast/hugdatafast/transform.py in my_map(self, *args, **kwargs)\r\n     21     if not cache_file_name.endswith('.arrow'): cache_file_name += '.arrow'\r\n     22     if '/' not in cache_file_name: cache_file_name = os.path.join(self.cache_directory(), cache_file_name)\r\n---> 23   return self.map(*args, cache_file_name=cache_file_name, **kwargs)\r\n     24 \r\n     25 @patch\r\n\r\n~/datasets/src/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1285                 logger.info(\"Spawning {} processes\".format(num_proc))\r\n   1286                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]\r\n-> 1287                 transformed_shards = [r.get() for r in results]\r\n   1288                 logger.info(\"Concatenating {} shards from multiprocessing\".format(num_proc))\r\n   1289                 result = concatenate_datasets(transformed_shards)\r\n\r\n~/datasets/src/datasets/arrow_dataset.py in (.0)\r\n   1285                 logger.info(\"Spawning {} processes\".format(num_proc))\r\n   1286                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]\r\n-> 1287                 transformed_shards = [r.get() for r in results]\r\n   1288                 logger.info(\"Concatenating {} shards from multiprocessing\".format(num_proc))\r\n   1289                 result = concatenate_datasets(transformed_shards)\r\n\r\n~/miniconda3/envs/ml/lib/python3.7/multiprocessing/pool.py in get(self, timeout)\r\n    655             return self._value\r\n    656         else:\r\n--> 657             raise self._value\r\n    658 \r\n    659     def _set(self, i, obj):\r\n\r\nArrowInvalid: offset overflow while concatenating arrays\r\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "This should be fixed with #645 "
      },
      {
        "user": "lhoestq",
        "body": "Feel free to re-open if it still occurs"
      },
      {
        "user": "ntoxeg",
        "body": "This has just happened to me while working with a large (65GB) Parquet dataset.\n```\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/app/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n[rank0]:     launch()\n[rank0]:   File \"/app/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n[rank0]:     run_exp()\n[rank0]:   File \"/app/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n[rank0]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n[rank0]:   File \"/app/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in _training_function\n[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n[rank0]:   File \"/app/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n[rank0]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/app/LLaMA-Factory/src/llamafactory/data/loader.py\", line 306, in get_dataset\n[rank0]:     dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/app/LLaMA-Factory/src/llamafactory/data/loader.py\", line 184, in _get_merged_dataset\n[rank0]:     datasets[dataset_name] = _load_single_dataset(dataset_attr, model_args, data_args, training_args)\n[rank0]:                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/app/LLaMA-Factory/src/llamafactory/data/loader.py\", line 164, in _load_single_dataset\n[rank0]:     return align_dataset(dataset, dataset_attr, data_args, training_args)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/app/LLaMA-Factory/src/llamafactory/data/converter.py\", line 279, in align_dataset\n[rank0]:     return dataset.map(\n[rank0]:            ^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 557, in wrapper\n[rank0]:     out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n[rank0]:                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3171, in map\n[rank0]:     for rank, done, content in iflatmap_unordered(\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py\", line 728, in iflatmap_unordered\n[rank0]:     [async_result.get(timeout=0.05) for async_result in async_results]\n[rank0]:      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/multiprocess/pool.py\", line 774, in get\n[rank0]:     raise self._value\n[rank0]: pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\n```"
      }
    ]
  },
  {
    "issue_number": 7619,
    "title": "`from_list` fails while `from_generator` works for large datasets",
    "author": "abdulfatir",
    "state": "open",
    "created_at": "2025-06-17T10:58:55Z",
    "updated_at": "2025-06-17T11:07:30Z",
    "labels": [],
    "body": "### Describe the bug\n\nI am constructing a large time series dataset and observed that first constructing a list of entries and then using `Dataset.from_list` led to a crash as the number of items became large. However, this is not a problem when using `Dataset.from_generator`.\n\n### Steps to reproduce the bug\n\n#### Snippet A (crashes)\n\n```py\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport datasets\n\n\ndef data_generator():\n    for i in tqdm(range(10_000_000)):\n        length = np.random.randint(2048)\n        series = np.random.rand(length)\n        yield {\"target\": series, \"item_id\": str(i), \"start\": np.datetime64(\"2000\", \"ms\")}\n\ndata_list = list(data_generator())\nds = datasets.Dataset.from_list(data_list)\n```\nThe last line crashes with\n```\nArrowInvalid: Value 2147483761 too large to fit in C integer type\n```\n\n#### Snippet B (works)\n\n```py\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport datasets\n\n\ndef data_generator():\n    for i in tqdm(range(10_000_000)):\n        length = np.random.randint(2048)\n        series = np.random.rand(length)\n        yield {\"target\": series, \"item_id\": str(i), \"start\": np.datetime64(\"2000\", \"ms\")}\n\nds = datasets.Dataset.from_generator(data_generator)\n```\n\n### Expected behavior\n\nI expected both the approaches to work or to fail similarly. \n\n### Environment info\n\n```\n- `datasets` version: 3.6.0\n- Platform: Linux-6.8.0-1029-aws-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- `huggingface_hub` version: 0.32.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2025.3.0\n```",
    "comments": []
  },
  {
    "issue_number": 7086,
    "title": "load_dataset ignores cached datasets and tries to hit HF Hub, resulting in API rate limit errors",
    "author": "tginart",
    "state": "open",
    "created_at": "2024-08-02T18:12:23Z",
    "updated_at": "2025-06-16T18:43:29Z",
    "labels": [],
    "body": "### Describe the bug\n\nI have been running lm-eval-harness a lot which has results in an API rate limit. This seems strange, since all of the data should be cached locally. I have in fact verified this.\n\n### Steps to reproduce the bug\n\n1. Be Me\r\n2. Run `load_dataset(\"TAUR-Lab/MuSR\")`\r\n3. Hit rate limit error\r\n4. Dataset is in .cache/huggingface/datasets\r\n5. ???\n\n### Expected behavior\n\nWe should not run into API rate limits if we have cached the dataset\n\n### Environment info\n\ndatasets 2.16.0\r\npython 3.10.4",
    "comments": [
      {
        "user": "luisgalan",
        "body": "I'm having the same issue - running into rate limits when doing hyperparameter tuning even though the dataset is supposed to be cached. I feel like this behaviour should at the very least be documented, but honestly you should just not be running into rate limits in the first place when the dataset is cached. It even happens when specifying a specific revision for the dataset, in which case AFAIK there should be no reason to be doing API requests if it's already cached (besides maybe a quick hash check but hitting rate limits for that in ~200 requests across 10 hours of use seems a bit ridiculous)."
      }
    ]
  },
  {
    "issue_number": 7599,
    "title": "My already working dataset (when uploaded few months ago) now is ignoring metadata.jsonl",
    "author": "JuanCarlosMartinezSevilla",
    "state": "closed",
    "created_at": "2025-06-06T18:59:00Z",
    "updated_at": "2025-06-16T15:18:00Z",
    "labels": [],
    "body": "### Describe the bug\n\nHi everyone, I uploaded my dataset https://huggingface.co/datasets/PRAIG/SMB a few months ago while I was waiting for a conference acceptance response. Without modifying anything in the dataset repository now the Dataset viewer is not rendering the metadata.jsonl annotations, neither it is being downloaded when using load_dataset. Can you please help? Thank you in advance.\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\nds = load_dataset(\"PRAIG/SMB\")\nds = ds[\"train\"]\n\n### Expected behavior\n\n It is expected to have all the metadata available in the jsonl file. Fields like: \"score_id\", \"original_width\", \"original_height\", \"regions\"... among others.\n\n### Environment info\n\ndatasets==3.6.0, python 3.13.3 (but he problem is already in the huggingface dataset page)",
    "comments": [
      {
        "user": "JuanCarlosMartinezSevilla",
        "body": "Maybe its been a recent update, but i can manage to load the metadata.jsonl separately from the images with:\n\n```\nmetadata = load_dataset(\"PRAIG/SMB\", split=\"train\", data_files=[\"*.jsonl\"])\nimages = load_dataset(\"PRAIG/SMB\", split=\"train\")\n```\nDo you know it this is an expected behaviour? This makes my dataset viewer to only load the images without the labeling of metadata.jsonl.\n\nThanks"
      },
      {
        "user": "lhoestq",
        "body": "Hi ! this is because we now expect the metadata file to be inside the directory named after the split \"train\" (this way each split can have its own metadata and can be loaded independently)\n\nYou can fix that by configuring it explicitly in the dataset's README.md header:\n\n```yaml\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path:\n    - \"train/**/*.png\"\n    - \"metadata.jsonl\"\n```\n\n(or by moving the metadata.jsonl in train/ but in this case you also have to modify the content of the JSONL to fix the relative paths to the images)"
      },
      {
        "user": "JuanCarlosMartinezSevilla",
        "body": "Thank you very much, dataset viewer is already working as expected!!"
      }
    ]
  },
  {
    "issue_number": 7617,
    "title": "Unwanted column padding in nested lists of dicts",
    "author": "qgallouedec",
    "state": "closed",
    "created_at": "2025-06-15T22:06:17Z",
    "updated_at": "2025-06-16T13:43:31Z",
    "labels": [],
    "body": "```python\nfrom datasets import Dataset\n\ndataset = Dataset.from_dict({\n    \"messages\": [\n        [\n            {\"a\": \"...\",},\n            {\"b\": \"...\",},\n        ],\n    ]\n})\nprint(dataset[0])\n```\n\nWhat I get:\n```\n{'messages': [{'a': '...', 'b': None}, {'a': None, 'b': '...'}]}\n```\n\nWhat I want:\n\n```\n{'messages': [{'a': '...'}, {'b': '...'}]}\n```\n\nIs there an easy way to automatically remove these auto-filled null/none values?\n\nIf not, I probably need a recursive none exclusion function, don't I?\n\nDatasets 3.6.0",
    "comments": [
      {
        "user": "qgallouedec",
        "body": "Answer from @lhoestq:\n\n> No\n> This is because Arrow and Parquet a columnar format: they require a fixed type for each column. So if you have nested dicts, each item should have the same subfields\n\nThe way around I found is the handle it after sampling with this function:\n\n```python\ndef remove_padding(example):\n    if isinstance(example, list):\n        return [remove_padding(value) if isinstance(value, (dict, list)) else value for value in example]\n    elif isinstance(example, Mapping):\n        return {\n            key: remove_padding(value) if isinstance(value, (dict, list)) else value\n            for key, value in example.items()\n            if value is not None\n        }\n    else:\n        raise TypeError(\"Input must be a list or a dictionary.\")\n\n# Example:\nexample = next(iter(dataset))\nexample = remove_padding(example)\n```"
      }
    ]
  },
  {
    "issue_number": 6152,
    "title": "FolderBase Dataset automatically resolves under current directory when data_dir is not specified",
    "author": "npuichigo",
    "state": "open",
    "created_at": "2023-08-16T04:38:09Z",
    "updated_at": "2025-06-16T07:54:09Z",
    "labels": [
      "good first issue"
    ],
    "body": "### Describe the bug\n\nFolderBase Dataset automatically resolves under current directory when data_dir is not specified.\r\n\r\nFor example:\r\n```\r\nload_dataset(\"audiofolder\")\r\n```\r\ntakes long time to resolve and collect data_files from current directory. But I think it should reach out to this line for error handling https://github.com/huggingface/datasets/blob/cb8c5de5145c7e7eee65391cb7f4d92f0d565d62/src/datasets/packaged_modules/folder_based_builder/folder_based_builder.py#L58-L59\n\n### Steps to reproduce the bug\n\n```\r\nload_dataset(\"audiofolder\")\r\n```\n\n### Expected behavior\n\nError report\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.15\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3",
    "comments": [
      {
        "user": "npuichigo",
        "body": "@lhoestq "
      },
      {
        "user": "lhoestq",
        "body": "Makes sense, I guess this can be fixed in the load_dataset_builder method.\r\nIt concerns every packaged builder I think (see values in `_PACKAGED_DATASETS_MODULES`)"
      },
      {
        "user": "npuichigo",
        "body": "I think the behavior is related to these lines, which short circuited the error handling.\r\nhttps://github.com/huggingface/datasets/blob/664a1cb72ea1e6ef7c47e671e2686ca4a35e8d63/src/datasets/load.py#L946-L952\r\n\r\nSo should data_dir be checked here or still delegating to actual `DatasetModule`? In that case, how to properly set `data_files` here."
      }
    ]
  },
  {
    "issue_number": 7612,
    "title": "Provide an option of robust dataset iterator with error handling",
    "author": "wwwjn",
    "state": "open",
    "created_at": "2025-06-13T00:40:48Z",
    "updated_at": "2025-06-13T20:21:55Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nAdding an option to skip corrupted data samples. Currently the datasets behavior is throwing errors if the data sample if corrupted and let user aware and handle the data corruption. When I tried to try-catch the error at user level, the iterator will raise StopIteration when I called next() again. \n\nThe way I try to do error handling is: (This doesn't work, unfortunately)\n\n```\n        # Load the dataset with streaming enabled\n        dataset = load_dataset(\n            \"pixparse/cc12m-wds\", split=\"train\", streaming=True\n        )\n        # Get an iterator from the dataset\n        iterator = iter(dataset)\n\n        while True:\n            try:\n                # Try to get the next example\n                example = next(iterator)\n                \n                # Try to access and process the image\n                image = example[\"jpg\"]\n                pil_image = Image.fromarray(np.array(image))\n                pil_image.verify()  # Verify it's a valid image file\n\n            except StopIteration:  # Code path 1\n                print(\"\\nStopIteration was raised! Reach the end of dataset\")\n                raise StopIteration\n\n            except Exception as e:  # Code path 2\n                errors += 1\n                print(\"Error! Skip this sample\")\n                cotinue\n            else:\n                successful += 1\n```\n\nThis is because the `IterableDataset` already throws an error (reaches Code path 2). And if I continue call next(), it will hit Code path 1. This is because the inner iterator of `IterableDataset`([code](https://github.com/huggingface/datasets/blob/89bd1f971402acb62805ef110bc1059c38b1c8c6/src/datasets/iterable_dataset.py#L2242)) as been stopped, so calling next() on it will raise StopIteration. \n\nSo I can not skip the corrupted data sample in this way. Would also love to hear any suggestions about creating a robust dataloader.\n\nThanks for your help in advance!\n\n\n### Motivation\n\n## Public dataset corruption might be common\nA lot of users would use public dataset, and the public dataset might contains some corrupted data, especially for dataset with image / video etc.  I totally understand it's dataset owner and user's responsibility to ensure the data integrity / run data cleaning or preprocessing, but it would be easier for developers who would use the dataset\n\n## Use cases\nFor example, a robust dataloader would be easy for users who want to try quick tests on different dataset, and chose one dataset which fits their needs. So user could use IterableDataloader with `stream=True` to use the dataset easily without downloading and removing corrupted data samples from the dataset.\n\n\n### Your contribution\n\nThe error handling might not trivial and might need more careful design. ",
    "comments": []
  },
  {
    "issue_number": 7607,
    "title": "Video and audio decoding with torchcodec",
    "author": "TyTodd",
    "state": "open",
    "created_at": "2025-06-11T07:02:30Z",
    "updated_at": "2025-06-13T19:38:41Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nPytorch is migrating video processing to torchcodec and it's pretty cool. It would be nice to migrate both the audio and video features to use torchcodec instead of torchaudio/video.\n\n### Motivation\n\nMy use case is I'm working on a multimodal AV model, and what's nice about torchcodec is I can extract the audio tensors directly from MP4 files. Also, I can easily resample video data to whatever fps I like on the fly. I haven't found an easy/efficient way to do this with torchvision.\n\n### Your contribution\n\nI’m modifying the Video dataclass to use torchcodec in place of the current backend, starting from a stable commit for a project I’m working on. If it ends up working well, I’m happy to open a PR on main.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Good idea ! let me know if you have any question or if I can help"
      },
      {
        "user": "TyTodd",
        "body": "@lhoestq Almost finished, but I'm having trouble understanding this test case.\nThis is how it looks originally. The `map` function is called, and then `with_format` is called. According to the test case example[\"video\"] is supposed to be a VideoReader. However, according to the [docs](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.with_format) its supposed to be the type passed into  `with_format` (numpy in this case). My implementation with VideoDecoder currently does the latter, is that correct, or should it be a VideoDecoder object instead?\n```\n@require_torchvision\ndef test_dataset_with_video_map_and_formatted(shared_datadir):\n    from torchvision.io import VideoReader\n\n    video_path = str(shared_datadir / \"test_video_66x50.mov\")\n    data = {\"video\": [video_path]}\n    features = Features({\"video\": Video()})\n    dset = Dataset.from_dict(data, features=features)\n    dset = dset.map(lambda x: x).with_format(\"numpy\")\n    example = dset[0]\n    assert isinstance(example[\"video\"], VideoReader)\n    # assert isinstance(example[\"video\"][0], np.ndarray)\n\n    # from bytes\n    with open(video_path, \"rb\") as f:\n        data = {\"video\": [f.read()]}\n    dset = Dataset.from_dict(data, features=features)\n    dset = dset.map(lambda x: x).with_format(\"numpy\")\n    example = dset[0]\n    assert isinstance(example[\"video\"], VideoReader)\n    # assert isinstance(example[\"video\"][0], np.ndarray)\n\n```"
      },
      {
        "user": "lhoestq",
        "body": "Hi ! It's maybe more convenient for users to always have a VideoDecoder, since they might only access a few frames and not the full video. So IMO it's fine to always return a VideoDecoder (maybe later we can extend the VideoDecoder to return other types of tensors than numpy arrays though ? 👀 it's not crucial for now though)"
      }
    ]
  },
  {
    "issue_number": 7597,
    "title": "Download datasets from a private hub in 2025",
    "author": "DanielSchuhmacher",
    "state": "closed",
    "created_at": "2025-06-06T07:55:19Z",
    "updated_at": "2025-06-13T13:46:00Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nIn the context of a private hub deployment, customers would like to use load_dataset() to load datasets from their hub, not from the public hub. This doesn't seem to be configurable at the moment and it would be nice to add this feature.\n\nThe obvious workaround is to clone the repo first and then load it from local storage, but this adds an extra step. It'd be great to have the same experience regardless of where the hub is hosted.\n\nThis issue was raised before here: https://github.com/huggingface/datasets/issues/3679\n@juliensimon\n\n### Motivation\n\nnone\n\n### Your contribution\n\nnone",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! First, and in the general case, Hugging Face does offer to host private datasets, and with a subscription you can even choose the region in which the repositories are hosted (US, EU)\n\nThen if you happen to have a private deployment, you can set the HF_ENDPOINT environment variable (same as in https://github.com/huggingface/transformers/issues/38634)"
      },
      {
        "user": "DanielSchuhmacher",
        "body": "Thank you @lhoestq. Works as described!"
      }
    ]
  },
  {
    "issue_number": 7570,
    "title": "Dataset lib seems to broke after fssec lib update",
    "author": "sleepingcat4",
    "state": "closed",
    "created_at": "2025-05-15T11:45:06Z",
    "updated_at": "2025-06-13T00:44:27Z",
    "labels": [],
    "body": "### Describe the bug\n\nI am facing an issue since today where HF's dataset is acting weird and in some instances failure to recognise a valid dataset entirely, I think it is happening due to recent change in `fsspec` lib as using this command fixed it for me in one-time: `!pip install -U datasets huggingface_hub fsspec`\n\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\n\ndef download_hf():\n  dataset_name = input(\"Enter the dataset name: \")\n  subset_name = input(\"Enter subset name: \")\n  ds = load_dataset(dataset_name, name=subset_name)\n  for split in ds:\n    ds[split].to_pandas().to_csv(f\"{subset_name}.csv\", index=False)\n\ndownload_hf()\n\n### Expected behavior\n\n```\nDownloading readme: 100%\n 1.55k/1.55k [00:00<00:00, 121kB/s]\nDownloading data files: 100%\n 1/1 [00:00<00:00,  2.06it/s]\n\nDownloading data:   0%|          | 0.00/54.2k [00:00<?, ?B/s]\nDownloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 121kB/s]\nExtracting data files: 100%\n 1/1 [00:00<00:00, 35.17it/s]\nGenerating test split: \n 140/0 [00:00<00:00, 2628.62 examples/s]\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n[<ipython-input-2-12ab305b0e77>](https://localhost:8080/#) in <cell line: 0>()\n      8     ds[split].to_pandas().to_csv(f\"{subset_name}.csv\", index=False)\n      9 \n---> 10 download_hf()\n\n2 frames\n[/usr/local/lib/python3.11/dist-packages/datasets/builder.py](https://localhost:8080/#) in as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\n   1171         is_local = not is_remote_filesystem(self._fs)\n   1172         if not is_local:\n-> 1173             raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")\n   1174         if not os.path.exists(self._output_dir):\n   1175             raise FileNotFoundError(\n\nNotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.\n```\nOR\n```\nTraceback (most recent call last):\n  File \"e:\\Fuck\\download-data\\mcq_dataset.py\", line 10, in <module>\n    download_hf()\n  File \"e:\\Fuck\\download-data\\mcq_dataset.py\", line 6, in download_hf\n    ds = load_dataset(dataset_name, name=subset_name)\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py\", line 2606, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py\", line 2277, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py\", line 1917, in dataset_module_factory    \n    raise e1 from None\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py\", line 1867, in dataset_module_factory    \n    raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e\ndatasets.exceptions.DatasetNotFoundError: Dataset 'dataset repo_id' doesn't exist on the Hub or cannot be accessed.\n```\n\n### Environment info\n\ncolab and 3.10 local system",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi, can you try updating `datasets` ? Colab still installs `datasets` 2.x by default, instead of 3.x\n\nIt would be cool to also report this to google colab, they have a GitHub repo for this IIRC"
      },
      {
        "user": "sleepingcat4",
        "body": "@lhoestq I have updated it to `datasets==3.6.0` and now there's an entirely different issue on colab while locally its fine. \n\n```\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\nREADME.md: 100%\n 2.88k/2.88k [00:00<00:00, 166kB/s]\nsuno.jsonl.zst: 100%\n 221M/221M [00:05<00:00, 48.6MB/s]\nGenerating train split: \n 18633/0 [00:01<00:00, 13018.92 examples/s]\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n[/usr/local/lib/python3.11/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\n   1870                     try:\n-> 1871                         writer.write_table(table)\n   1872                     except CastError as cast_error:\n\n17 frames\nTypeError: Couldn't cast array of type\nstruct<id: string, type: string, infill: bool, source: string, continue_at: double, infill_dur_s: double, infill_end_s: double, infill_start_s: double, include_future_s: double, include_history_s: double, infill_context_end_s: double, infill_context_start_s: int64>\nto\n{'id': Value(dtype='string', id=None), 'type': Value(dtype='string', id=None), 'infill': Value(dtype='bool', id=None), 'source': Value(dtype='string', id=None), 'continue_at': Value(dtype='float64', id=None), 'include_history_s': Value(dtype='float64', id=None)}\n\nThe above exception was the direct cause of the following exception:\n\nDatasetGenerationError                    Traceback (most recent call last)\n[/usr/local/lib/python3.11/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\n   1896             if isinstance(e, DatasetGenerationError):\n   1897                 raise\n-> 1898             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\n   1899 \n   1900         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\n\nDatasetGenerationError: An error occurred while generating the dataset\n```"
      },
      {
        "user": "sleepingcat4",
        "body": "@lhoestq opps sorry the dataset was in .zst which was causing this error rather than being a datasets library fault. After upgrading dataset version Colab is working fine. "
      }
    ]
  },
  {
    "issue_number": 7611,
    "title": "Code example for dataset.add_column() does not reflect correct way to use function",
    "author": "shaily99",
    "state": "open",
    "created_at": "2025-06-12T19:42:29Z",
    "updated_at": "2025-06-12T19:42:29Z",
    "labels": [],
    "body": "https://github.com/huggingface/datasets/blame/38d4d0e11e22fdbc4acf373d2421d25abeb43439/src/datasets/arrow_dataset.py#L5925C10-L5925C10\n\nThe example seems to suggest that dataset.add_column() can add column inplace, however, this is wrong -- it cannot. It returns a new dataset with the column added to it.",
    "comments": []
  },
  {
    "issue_number": 7610,
    "title": "i cant confirm email",
    "author": "lykamspam",
    "state": "open",
    "created_at": "2025-06-12T18:58:49Z",
    "updated_at": "2025-06-12T18:58:49Z",
    "labels": [],
    "body": "### Describe the bug\n\nThis is dificult, I cant confirm email because I'm not get any email!\nI cant post forum because I cant confirm email!\nI can send help desk because... no exist on web page.\n\nparagraph 44\n\n### Steps to reproduce the bug\n\nrthjrtrt\n\n### Expected behavior\n\newtgfwetgf\n\n### Environment info\n\nsdgfswdegfwe",
    "comments": []
  },
  {
    "issue_number": 4981,
    "title": "Can't create a dataset with `float16` features",
    "author": "dconathan",
    "state": "open",
    "created_at": "2022-09-15T21:03:24Z",
    "updated_at": "2025-06-12T11:47:42Z",
    "labels": [
      "bug"
    ],
    "body": "## Describe the bug\r\nI can't create a dataset with `float16` features.\r\n\r\nI understand from the traceback that this is a `pyarrow` error, but I don't see anywhere in the `datasets` documentation about how to successfully do this.  Is it actually supported?  I've tried older versions of `pyarrow` as well with the same exact error.\r\n\r\nThe bug seems to arise from `datasets` casting the values to `double` and then `pyarrow` doesn't know how to convert those back to `float16`... does that sound right?  Is there a way to bypass this since it's not necessary in the `numpy` and `torch` cases?\r\n\r\nThanks!\r\n\r\n\r\n## Steps to reproduce the bug\r\nAll of the following raise the following error with the same exact (as far as I can tell) traceback:\r\n```python\r\nArrowNotImplementedError: Unsupported cast from double to halffloat using function cast_half_float\r\n```\r\n```python\r\nfrom datasets import Dataset, Features, Value\r\nDataset.from_dict({\"x\": [0.0, 1.0, 2.0]}, features=Features(x=Value(\"float16\")))\r\n\r\nimport numpy as np\r\nDataset.from_dict({\"x\": np.arange(3, dtype=np.float16)}, features=Features(x=Value(\"float16\")))\r\n\r\nimport torch\r\nDataset.from_dict({\"x\": torch.arange(3).to(torch.float16)}, features=Features(x=Value(\"float16\")))\r\n```\r\n\r\n## Expected results\r\nA dataset with `float16` features is successfully created.\r\n\r\n## Actual results\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nArrowNotImplementedError                  Traceback (most recent call last)\r\nCell In [14], line 1\r\n----> 1 Dataset.from_dict({\"x\": [1.0, 2.0, 3.0]}, features=Features(x=Value(\"float16\")))\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:870, in Dataset.from_dict(cls, mapping, features, info, split)\r\n    865     mapping = features.encode_batch(mapping)\r\n    866 mapping = {\r\n    867     col: OptimizedTypedSequence(data, type=features[col] if features is not None else None, col=col)\r\n    868     for col, data in mapping.items()\r\n    869 }\r\n--> 870 pa_table = InMemoryTable.from_pydict(mapping=mapping)\r\n    871 if info.features is None:\r\n    872     info.features = Features({col: ts.get_inferred_type() for col, ts in mapping.items()})\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/datasets/table.py:750, in InMemoryTable.from_pydict(cls, *args, **kwargs)\r\n    734 @classmethod\r\n    735 def from_pydict(cls, *args, **kwargs):\r\n    736     \"\"\"\r\n    737     Construct a Table from Arrow arrays or columns\r\n    738 \r\n   (...)\r\n    748         :class:`datasets.table.Table`:\r\n    749     \"\"\"\r\n--> 750     return cls(pa.Table.from_pydict(*args, **kwargs))\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/table.pxi:3648, in pyarrow.lib.Table.from_pydict()\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/table.pxi:5174, in pyarrow.lib._from_pydict()\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/array.pxi:343, in pyarrow.lib.asarray()\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/array.pxi:231, in pyarrow.lib.array()\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/array.pxi:110, in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/datasets/arrow_writer.py:197, in TypedSequence.__arrow_array__(self, type)\r\n    192     # otherwise we can finally use the user's type\r\n    193     elif type is not None:\r\n    194         # We use cast_array_to_feature to support casting to custom types like Audio and Image\r\n    195         # Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\r\n    196         # We only do it if trying_type is False - since this is what the user asks for.\r\n--> 197         out = cast_array_to_feature(out, type, allow_number_to_str=not self.trying_type)\r\n    198     return out\r\n    199 except (TypeError, pa.lib.ArrowInvalid) as e:  # handle type errors and overflows\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/datasets/table.py:1683, in _wrap_for_chunked_arrays.<locals>.wrapper(array, *args, **kwargs)\r\n   1681     return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1682 else:\r\n-> 1683     return func(array, *args, **kwargs)\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/datasets/table.py:1853, in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1851     return array_cast(array, get_nested_type(feature), allow_number_to_str=allow_number_to_str)\r\n   1852 elif not isinstance(feature, (Sequence, dict, list, tuple)):\r\n-> 1853     return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n   1854 raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/datasets/table.py:1683, in _wrap_for_chunked_arrays.<locals>.wrapper(array, *args, **kwargs)\r\n   1681     return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1682 else:\r\n-> 1683     return func(array, *args, **kwargs)\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/datasets/table.py:1762, in array_cast(array, pa_type, allow_number_to_str)\r\n   1760     if pa.types.is_null(pa_type) and not pa.types.is_null(array.type):\r\n   1761         raise TypeError(f\"Couldn't cast array of type {array.type} to {pa_type}\")\r\n-> 1762     return array.cast(pa_type)\r\n   1763 raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{pa_type}\")\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/array.pxi:919, in pyarrow.lib.Array.cast()\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/compute.py:389, in cast(arr, target_type, safe, options)\r\n    387     else:\r\n    388         options = CastOptions.safe(target_type)\r\n--> 389 return call_function(\"cast\", [arr], options)\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/_compute.pyx:560, in pyarrow._compute.call_function()\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/_compute.pyx:355, in pyarrow._compute.Function.call()\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/error.pxi:144, in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nFile ~/scratch/scratch-env-39/.venv/lib/python3.9/site-packages/pyarrow/error.pxi:121, in pyarrow.lib.check_status()\r\n\r\nArrowNotImplementedError: Unsupported cast from double to halffloat using function cast_half_float\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.4.0\r\n- Platform: macOS-12.5.1-arm64-arm-64bit\r\n- Python version: 3.9.13\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.4\r\n",
    "comments": [
      {
        "user": "albertvillanova",
        "body": "Hi @dconathan, thanks for reporting.\r\n\r\nWe rely on Arrow as a backend, and as far as I know currently support for `float16` in Arrow is not fully implemented in Python (C++), hence the `ArrowNotImplementedError` you get.\r\n\r\nSee, e.g.: https://arrow.apache.org/docs/status.html?highlight=float16#data-types"
      },
      {
        "user": "dconathan",
        "body": "Thanks for the link…. didn’t realize arrow didn’t support it yet.  Should it be removed from https://huggingface.co/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Value until Arrow supports it?"
      },
      {
        "user": "albertvillanova",
        "body": "Yes, you are right: maybe we should either remove it from our docs or add a comment explaining the issue.\r\n\r\nThe thing is that in Arrow it is partially supported: you can create `float16` values, but you can't cast them from/to other types. And current implementation of `Value` always tries to perform a cast from `float64` to `float16`."
      }
    ]
  },
  {
    "issue_number": 6790,
    "title": "PyArrow 'Memory mapping file failed: Cannot allocate memory' bug",
    "author": "lasuomela",
    "state": "open",
    "created_at": "2024-04-07T19:25:39Z",
    "updated_at": "2025-06-12T07:31:44Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nHello,\r\n\r\nI've been struggling with a problem using Huggingface datasets caused by PyArrow memory allocation. I finally managed to solve it, and thought to document it since similar issues have been raised here before (https://github.com/huggingface/datasets/issues/5710, https://github.com/huggingface/datasets/issues/6176).\r\n\r\nIn my case, I was trying to load ~70k dataset files from disk using `datasets.load_from_disk(data_path)` (meaning 70k repeated calls to load_from_disk). This triggered an (uninformative) exception around 64k loaded files:\r\n```\r\n  File \"pyarrow/io.pxi\", line 1053, in pyarrow.lib.memory_map\r\n  File \"pyarrow/io.pxi\", line 1000, in pyarrow.lib.MemoryMappedFile._open\r\n  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\r\nOSError: Memory mapping file failed: Cannot allocate memory\r\n```\r\nDespite system RAM usage being very low. After a lot of digging around, I discovered that my Ubuntu machine had a limit on the maximum number of memory mapped files in `/proc/sys/vm/max_map_count` set to 65530, which was causing my data loader to crash. Increasing the limit in the file (`echo <new_mmap_size> | sudo tee /proc/sys/vm/max_map_count`) made the issue go away.\r\n\r\nWhile this isn't a bug as such in either Datasets or PyArrow, this behavior can be very confusing to users. Maybe this should be mentioned in documentation? I suspect the other issues raised here about memory mapping OOM errors could actually be consequence of system configuration.\r\n\r\nBr,\r\nLauri\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport tqdm\r\n\r\n# Write some data to disk\r\narr = pa.array(np.arange(100))\r\nschema = pa.schema([\r\n    pa.field('nums', arr.type)\r\n])\r\nwith pa.OSFile('arraydata.arrow', 'wb') as sink:\r\n    with pa.ipc.new_file(sink, schema=schema) as writer:\r\n        batch = pa.record_batch([arr], schema=schema)\r\n        writer.write(batch)\r\n\r\n# Number of times to open the memory map\r\nnums = 70000\r\n\r\n# Read the data back\r\narrays = [pa.memory_map('arraydata.arrow', 'r') for _ in tqdm.tqdm(range(nums))]\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo errors.\r\n\r\n### Environment info\r\n\r\ndatasets: 2.18.0\r\npyarrow: 15.0.0",
    "comments": [
      {
        "user": "jxmorris12",
        "body": "Thanks for a very clean explanation. This happened to me too, and I don't have sudo access to update the value. I wonder if there might be another workaround."
      },
      {
        "user": "lasuomela",
        "body": "One option is to just have more data in each file - /proc/sys/vm/max_map_count limits the maximum number of concurrently open files, but I don't know if the size of a single file is restricted in any way. E.g. 5000 files with 1GB each is 5TB of data. https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.concatenate_datasets can come in handy."
      },
      {
        "user": "WelkinYang",
        "body": "> One option is to just have more data in each file - /proc/sys/vm/max_map_count limits the maximum number of concurrently open files, but I don't know if the size of a single file is restricted in any way. E.g. 5000 files with 1GB each is 5TB of data. https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.concatenate_datasets can come in handy.\n\nI still got the same error even if i only got 25k arrow files and i found this error will raise when each process of \"load_from_disk\" was allocated vram more than 128T"
      }
    ]
  },
  {
    "issue_number": 7600,
    "title": "`push_to_hub` is not concurrency safe (dataset schema corruption)",
    "author": "sharvil",
    "state": "open",
    "created_at": "2025-06-07T17:28:56Z",
    "updated_at": "2025-06-11T14:14:04Z",
    "labels": [],
    "body": "### Describe the bug\n\nConcurrent processes modifying and pushing a dataset can overwrite each others' dataset card, leaving the dataset unusable.\n\nConsider this scenario:\n- we have an Arrow dataset\n- there are `N` configs of the dataset\n- there are `N` independent processes operating on each of the individual configs (e.g. adding a column, `new_col`)\n- each process calls `push_to_hub` on their particular config when they're done processing\n- all calls to `push_to_hub` succeed\n- the `README.md` now has some configs with `new_col` added and some with `new_col` missing\n\nAny attempt to load a config (using `load_dataset`) where `new_col` is missing will fail because of a schema mismatch between `README.md` and the Arrow files. Fixing the dataset requires updating `README.md` by hand with the correct schema for the affected config. In effect, `push_to_hub` is doing a `git push --force` (I found this behavior quite surprising).\n\nWe have hit this issue every time we run processing jobs over our datasets and have to fix corrupted schemas by hand.\n\nReading through the code, it seems that specifying a [`parent_commit`](https://github.com/huggingface/huggingface_hub/blob/v0.32.4/src/huggingface_hub/hf_api.py#L4587) hash around here https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_dataset.py#L5794 would get us to a normal, non-forced git push, and avoid schema corruption. I'm not familiar enough with the code to know how to determine the commit hash from which the in-memory dataset card was loaded.\n\n### Steps to reproduce the bug\n\nSee above.\n\n### Expected behavior\n\nConcurrent edits to disjoint configs of a dataset should never corrupt the dataset schema.\n\n### Environment info\n\n- `datasets` version: 2.20.0\n- Platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35\n- Python version: 3.10.14\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.2\n- `fsspec` version: 2023.9.0",
    "comments": [
      {
        "user": "sharvil",
        "body": "@lhoestq can you please take a look? I've submitted a PR that fixes this issue. Thanks."
      },
      {
        "user": "lhoestq",
        "body": "Thanks for the ping ! As I said in https://github.com/huggingface/datasets/pull/7605 there is maybe a more general approach using retries :)"
      }
    ]
  },
  {
    "issue_number": 6330,
    "title": "Latest fsspec==2023.10.0 issue with streaming datasets",
    "author": "ZachNagengast",
    "state": "closed",
    "created_at": "2023-10-22T20:57:10Z",
    "updated_at": "2025-06-09T22:00:16Z",
    "labels": [],
    "body": "### Describe the bug\n\nLoading a streaming dataset with this version of fsspec fails with the following error:\r\n\r\n`NotImplementedError: Loading a streaming dataset cached in a LocalFileSystem is not supported yet.`\r\n\r\nI suspect the issue is with this PR \r\n\r\nhttps://github.com/fsspec/filesystem_spec/pull/1381\n\n### Steps to reproduce the bug\n\n1. Upgrade fsspec to version `2023.10.0`\r\n2. Attempt to load a streaming dataset e.g. `load_dataset(\"laion/gpt4v-emotion-dataset\", split=\"train\", streaming=True)`\r\n3. Observe the following exception:\r\n\r\n```\r\n  File \"/opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages/datasets/load.py\", line 2146, in load_dataset\r\n    return builder_instance.as_streaming_dataset(split=split)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages/datasets/builder.py\", line 1318, in as_streaming_dataset\r\n    raise NotImplementedError(\r\nNotImplementedError: Loading a streaming dataset cached in a LocalFileSystem is not supported yet.\r\n```\n\n### Expected behavior\n\nShould stream the dataset as normal.\n\n### Environment info\n\ndatasets@main\r\nfsspec==2023.10.0",
    "comments": [
      {
        "user": "humpydonkey",
        "body": "I also encountered a similar error below.\r\nAppreciate the team could shed some light on this issue.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n[/home/ubuntu/work/EveryDream2trainer/prepare_dataset.ipynb](https://vscode-remote+ssh-002dremote-002braspberry-002dg5-002e4x.vscode-resource.vscode-cdn.net/home/ubuntu/work/EveryDream2trainer/prepare_dataset.ipynb) Cell 1 line 4\r\n      [1](vscode-notebook-cell://ssh-remote%2Braspberry-g5.4x/home/ubuntu/work/EveryDream2trainer/prepare_dataset.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0) from datasets import load_dataset, load_dataset\r\n      [3](vscode-notebook-cell://ssh-remote%2Braspberry-g5.4x/home/ubuntu/work/EveryDream2trainer/prepare_dataset.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2) # ds = load_dataset(\"parquet\", data_dir=\"/home/ubuntu/work/EveryDream2trainer/datasets/monse_v1/data\")\r\n----> [4](vscode-notebook-cell://ssh-remote%2Braspberry-g5.4x/home/ubuntu/work/EveryDream2trainer/prepare_dataset.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3) ds = load_dataset(\"Raspberry-ai/monse-v1\")\r\n\r\nFile [/opt/conda/envs/everydream/lib/python3.10/site-packages/datasets/load.py:1804](https://vscode-remote+ssh-002dremote-002braspberry-002dg5-002e4x.vscode-resource.vscode-cdn.net/opt/conda/envs/everydream/lib/python3.10/site-packages/datasets/load.py:1804), in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   1800 # Build dataset for splits\r\n   1801 keep_in_memory = (\r\n   1802     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   1803 )\r\n-> 1804 ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n   1805 # Rename and cast features to match task schema\r\n   1806 if task is not None:\r\n\r\nFile [/opt/conda/envs/everydream/lib/python3.10/site-packages/datasets/builder.py:1108](https://vscode-remote+ssh-002dremote-002braspberry-002dg5-002e4x.vscode-resource.vscode-cdn.net/opt/conda/envs/everydream/lib/python3.10/site-packages/datasets/builder.py:1108), in DatasetBuilder.as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\r\n   1106 is_local = not is_remote_filesystem(self._fs)\r\n   1107 if not is_local:\r\n-> 1108     raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")\r\n   1109 if not os.path.exists(self._output_dir):\r\n   1110     raise FileNotFoundError(\r\n   1111         f\"Dataset {self.name}: could not find data in {self._output_dir}. Please make sure to call \"\r\n   1112         \"builder.download_and_prepare(), or use \"\r\n   1113         \"datasets.load_dataset() before trying to access the Dataset object.\"\r\n   1114     )\r\n\r\nNotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.\r\n```\r\n\r\nCode to reproduce the issue:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"Raspberry-ai/monse-v1\")\r\n```\r\n\r\n\r\nDependencies:\r\n```\r\nPackage                   Version\r\n------------------------- ------------\r\nabsl-py                   2.0.0\r\naccelerate                0.23.0\r\naiohttp                   3.8.4\r\naiosignal                 1.3.1\r\nantlr4-python3-runtime    4.9.3\r\nanyio                     4.0.0\r\nappdirs                   1.4.4\r\nargon2-cffi               23.1.0\r\nargon2-cffi-bindings      21.2.0\r\narrow                     1.3.0\r\nasttokens                 2.4.0\r\nasync-lru                 2.0.4\r\nasync-timeout             4.0.3\r\nattrs                     23.1.0\r\nBabel                     2.13.0\r\nbackcall                  0.2.0\r\nbeautifulsoup4            4.12.2\r\nbitsandbytes              0.41.1\r\nbleach                    6.1.0\r\nbraceexpand               0.1.7\r\ncachetools                5.3.1\r\ncertifi                   2023.7.22\r\ncffi                      1.16.0\r\ncharset-normalizer        3.3.1\r\nclick                     8.1.7\r\ncmake                     3.27.7\r\ncolorama                  0.4.6\r\ncomm                      0.1.4\r\ncompel                    1.1.6\r\ndatasets                  2.11.0\r\ndebugpy                   1.8.0\r\ndecorator                 5.1.1\r\ndefusedxml                0.7.1\r\ndiffusers                 0.18.0\r\ndill                      0.3.6\r\ndocker-pycreds            0.4.0\r\ndowg                      0.3.1\r\neinops                    0.7.0\r\neinops-exts               0.0.4\r\nexceptiongroup            1.1.3\r\nexecuting                 2.0.0\r\nfastjsonschema            2.18.1\r\nfilelock                  3.12.4\r\nfqdn                      1.5.1\r\nfrozenlist                1.4.0\r\nfsspec                    2023.10.0\r\nftfy                      6.1.1\r\ngitdb                     4.0.11\r\nGitPython                 3.1.40\r\ngoogle-auth               2.23.3\r\ngoogle-auth-oauthlib      1.1.0\r\ngrpcio                    1.59.0\r\nhuggingface-hub           0.18.0\r\nidna                      3.4\r\nimportlib-metadata        6.8.0\r\ninflection                0.5.1\r\nipykernel                 6.25.2\r\nipython                   8.16.1\r\nisoduration               20.11.0\r\njedi                      0.19.1\r\nJinja2                    3.1.2\r\njoblib                    1.3.2\r\njson5                     0.9.14\r\njsonpointer               2.4\r\njsonschema                4.19.1\r\njsonschema-specifications 2023.7.1\r\njupyter_client            8.4.0\r\njupyter_core              5.4.0\r\njupyter-events            0.8.0\r\njupyter-lsp               2.2.0\r\njupyter_server            2.8.0\r\njupyter_server_terminals  0.4.4\r\njupyterlab                4.0.7\r\njupyterlab-pygments       0.2.2\r\njupyterlab_server         2.25.0\r\nlightning-utilities       0.9.0\r\nlion-pytorch              0.1.2\r\nlit                       17.0.3\r\nMarkdown                  3.5\r\nMarkupSafe                2.1.3\r\nmatplotlib-inline         0.1.6\r\nmistune                   3.0.2\r\nmore-itertools            10.1.0\r\nmpmath                    1.3.0\r\nmultidict                 6.0.4\r\nmultiprocess              0.70.14\r\nmypy-extensions           1.0.0\r\nnbclient                  0.8.0\r\nnbconvert                 7.9.2\r\nnbformat                  5.9.2\r\nnest-asyncio              1.5.8\r\nnetworkx                  3.2\r\nnltk                      3.8.1\r\nnotebook_shim             0.2.3\r\nnumpy                     1.23.5\r\noauthlib                  3.2.2\r\nomegaconf                 2.2.3\r\nopen-clip-torch           2.22.0\r\nopen-flamingo             2.0.0\r\noverrides                 7.4.0\r\npackaging                 23.2\r\npandas                    2.1.1\r\npandocfilters             1.5.0\r\nparso                     0.8.3\r\npathtools                 0.1.2\r\npexpect                   4.8.0\r\npickleshare               0.7.5\r\nPillow                    10.1.0\r\npip                       23.3.1\r\nplatformdirs              3.11.0\r\nprometheus-client         0.17.1\r\nprompt-toolkit            3.0.39\r\nprotobuf                  3.20.1\r\npsutil                    5.9.6\r\nptyprocess                0.7.0\r\npure-eval                 0.2.2\r\npyarrow                   13.0.0\r\npyasn1                    0.5.0\r\npyasn1-modules            0.3.0\r\npycparser                 2.21\r\npyDeprecate               0.3.2\r\nPygments                  2.16.1\r\npynvml                    11.4.1\r\npyparsing                 3.1.1\r\npyre-extensions           0.0.29\r\npython-dateutil           2.8.2\r\npython-json-logger        2.0.7\r\npytorch-lightning         1.6.5\r\npytz                      2023.3.post1\r\nPyYAML                    6.0.1\r\npyzmq                     25.1.1\r\nreferencing               0.30.2\r\nregex                     2023.10.3\r\nrequests                  2.31.0\r\nrequests-oauthlib         1.3.1\r\nresponses                 0.18.0\r\nrfc3339-validator         0.1.4\r\nrfc3986-validator         0.1.1\r\nrpds-py                   0.10.6\r\nrsa                       4.9\r\nsafetensors               0.4.0\r\nscipy                     1.11.3\r\nSend2Trash                1.8.2\r\nsentencepiece             0.1.98\r\nsentry-sdk                1.32.0\r\nsetproctitle              1.3.3\r\nsetuptools                68.2.2\r\nsix                       1.16.0\r\nsmmap                     5.0.1\r\nsniffio                   1.3.0\r\nsoupsieve                 2.5\r\nstack-data                0.6.3\r\nsympy                     1.12\r\ntensorboard               2.15.0\r\ntensorboard-data-server   0.7.1\r\nterminado                 0.17.1\r\ntimm                      0.9.8\r\ntinycss2                  1.2.1\r\ntokenizers                0.13.3\r\ntomli                     2.0.1\r\ntorch                     2.0.1+cu118\r\ntorchmetrics              1.2.0\r\ntorchvision               0.15.2+cu118\r\ntornado                   6.3.3\r\ntqdm                      4.66.1\r\ntraitlets                 5.11.2\r\ntransformers              4.29.2\r\ntriton                    2.0.0\r\ntypes-python-dateutil     2.8.19.14\r\ntyping_extensions         4.8.0\r\ntyping-inspect            0.9.0\r\ntzdata                    2023.3\r\nuri-template              1.3.0\r\nurllib3                   2.0.7\r\nwandb                     0.15.12\r\nwcwidth                   0.2.8\r\nwebcolors                 1.13\r\nwebdataset                0.2.62\r\nwebencodings              0.5.1\r\nwebsocket-client          1.6.4\r\nWerkzeug                  3.0.0\r\nwheel                     0.41.2\r\nxformers                  0.0.20\r\nxxhash                    3.4.1\r\nyarl                      1.9.2\r\nzipp                      3.17.0\r\n```"
      },
      {
        "user": "ZachNagengast",
        "body": "@humpydonkey FWIW setting fsspec down to 2023.9.2 fixed the issue\r\n\r\n`pip install fsspec==2023.9.2`"
      },
      {
        "user": "humpydonkey",
        "body": "got it, thanks @ZachNagengast "
      }
    ]
  },
  {
    "issue_number": 7573,
    "title": "No Samsum dataset",
    "author": "IgorKasianenko",
    "state": "open",
    "created_at": "2025-05-20T09:54:35Z",
    "updated_at": "2025-06-09T08:58:24Z",
    "labels": [],
    "body": "### Describe the bug\n\nhttps://huggingface.co/datasets/Samsung/samsum dataset not found error 404\n\nOriginated from https://github.com/meta-llama/llama-cookbook/issues/948\n\n### Steps to reproduce the bug\n\ngo to website https://huggingface.co/datasets/Samsung/samsum\nsee the error\n\nalso downloading it with python throws \n```\nCouldn't find 'Samsung/samsum' on the Hugging Face Hub either: FileNotFoundError: Samsung/samsum@f00baf5a7d4abfec6820415493bcb52c587788e6/samsum.py (repository not found)\n```\n\n### Expected behavior\n\nDataset exists\n\n### Environment info\n\n```\n- `datasets` version: 3.2.0\n- Platform: macOS-15.4.1-arm64-arm-64bit\n- Python version: 3.12.2\n- `huggingface_hub` version: 0.26.5\n- PyArrow version: 16.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0\n```",
    "comments": [
      {
        "user": "SP1029",
        "body": "According to the following https://huggingface.co/posts/seawolf2357/424129432408590, as of now the dataset seems to be inaccessible.\n\n@IgorKasianenko, would https://huggingface.co/datasets/knkarthick/samsum suffice for your purpose?\n"
      },
      {
        "user": "IgorKasianenko",
        "body": "Thanks @SP1029 for the update!\nThat will work for now, using it as replacement. Is there a officially recommended way to maintain the CC licensed dataset under the organization account? \nFeel free to close this issue"
      }
    ]
  },
  {
    "issue_number": 5665,
    "title": "Feature request: IterableDataset.push_to_hub",
    "author": "NielsRogge",
    "state": "closed",
    "created_at": "2023-03-23T09:53:04Z",
    "updated_at": "2025-06-06T16:13:22Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\r\n\r\nIt'd be great to have a lazy push to hub, similar to the lazy loading we have with `IterableDataset`.\r\n\r\nSuppose you'd like to filter [LAION](https://huggingface.co/datasets/laion/laion400m) based on certain conditions, but as LAION doesn't fit into your disk, you'd like to leverage streaming:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"laion/laion400m\", streaming=True, split=\"train\")\r\n```\r\nThen you could filter the dataset based on certain conditions:\r\n```\r\nfiltered_dataset = dataset.filter(lambda example: example['HEIGHT'] > 400)\r\n```\r\n\r\nIn order to persist this dataset and push it back to the hub, one currently needs to first load the entire filtered dataset on disk and then push:\r\n\r\n```\r\nfrom datasets import Dataset\r\n\r\nDataset.from_generator(filtered_dataset.__iter__).push_to_hub(...)\r\n```\r\nIt would be great if we can instead lazy push to the data to the hub (basically stream the data to the hub), not being limited by our disk size:\r\n```\r\nfiltered_dataset.push_to_hub(\"my-filtered-dataset\")\r\n```\r\n\r\n### Motivation\r\n\r\nThis feature would be very useful for people that want to filter huge datasets without having to load the entire dataset or a filtered version thereof on their local disk.\r\n\r\n### Your contribution\r\n\r\nHappy to test out a PR :)",
    "comments": [
      {
        "user": "ducha-aiki",
        "body": "+1"
      },
      {
        "user": "phineas-pta",
        "body": "+1"
      },
      {
        "user": "Jourdelune",
        "body": "+1, should be possible now? :) https://huggingface.co/blog/xethub-joins-hf"
      }
    ]
  },
  {
    "issue_number": 7594,
    "title": "Add option to ignore keys/columns when loading a dataset from jsonl(or any other data format)",
    "author": "avishaiElmakies",
    "state": "open",
    "created_at": "2025-06-05T11:12:45Z",
    "updated_at": "2025-06-05T12:58:12Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nHi, I would like the option to ignore keys/columns when loading a dataset from files (e.g. jsonl).\n\n### Motivation\n\nI am working on a dataset which is built on jsonl. It seems the dataset is unclean and a column has different types in each row. I can't clean this or remove the column (It is not my data and it is too big for me to clean and save on my own hardware). \nI would like the option to just ignore this column when using `load_dataset`, since i don't need it.\nI tried to look if this is already possible but couldn't find a solution. if there is I would love some help. If it is not currently possible, I would love this feature\n\n### Your contribution\n\nI don't think I can help this time, unfortunately.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Good point, I'd be in favor of having the `columns` argument in `JsonConfig` (and the others) to align with `ParquetConfig` to let users choose which columns to load and ignore the rest"
      },
      {
        "user": "avishaiElmakies",
        "body": "Is it possible to ignore columns when using parquet? "
      },
      {
        "user": "lhoestq",
        "body": "Yes, you can pass `columns=...` to load_dataset to select which columns to load, and it is passed to `ParquetConfig` :)"
      }
    ]
  },
  {
    "issue_number": 7561,
    "title": "NotImplementedError: <class 'datasets.iterable_dataset.RepeatExamplesIterable'> doesn't implement num_shards yet",
    "author": "cyanic-selkie",
    "state": "closed",
    "created_at": "2025-05-07T15:05:42Z",
    "updated_at": "2025-06-05T12:41:30Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen using `.repeat()` on an `IterableDataset`, this error gets thrown. There is [this thread](https://discuss.huggingface.co/t/making-an-infinite-iterabledataset/146192/5) that seems to imply the fix is trivial, but I don't know anything about this codebase, so I'm opening this issue rather than attempting to open a PR.\n\n### Steps to reproduce the bug\n\n1. Create an `IterableDataset`.\n2. Call `.repeat(None)` on it.\n3. Wrap it in a pytorch `DataLoader`\n4. Iterate over it.\n\n### Expected behavior\n\nThis should work normally.\n\n### Environment info\n\ndatasets: 3.5.0",
    "comments": []
  },
  {
    "issue_number": 7591,
    "title": "Add num_proc parameter to push_to_hub",
    "author": "SwayStar123",
    "state": "open",
    "created_at": "2025-06-04T13:19:15Z",
    "updated_at": "2025-06-04T13:19:23Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nA number of processes parameter to the dataset.push_to_hub method\n\n### Motivation\n\nShards are currently uploaded serially which makes it slow for many shards, uploading can be done in parallel and much faster\n",
    "comments": []
  },
  {
    "issue_number": 7590,
    "title": "`Sequence(Features(...))` causes PyArrow cast error in `load_dataset` despite correct schema.",
    "author": "AHS-uni",
    "state": "open",
    "created_at": "2025-05-29T22:53:36Z",
    "updated_at": "2025-06-04T13:13:08Z",
    "labels": [],
    "body": "### Description\n\nWhen loading a dataset with a field declared as a list of structs using `Sequence(Features(...))`, `load_dataset` incorrectly infers the field as a plain `struct<...>` instead of a `list<struct<...>>`. This leads to the following error:\n\n```\nArrowNotImplementedError: Unsupported cast from list<item: struct<id: string, data: string>> to struct using function cast_struct\n```\n\nThis occurs even when the `features` schema is explicitly provided and the dataset format supports nested structures natively (e.g., JSON, JSONL).\n\n---\n\n###  Minimal Reproduction\n\n[Colab Link.](https://colab.research.google.com/drive/1FZPQy6TP3jVd4B3mYKyfQaWNuOAvljUq?usp=sharing)\n\n#### Dataset\n\n```python\ndata = [\n    {\n        \"list\": [\n            {\"id\": \"example1\", \"data\": \"text\"},\n        ]\n    },\n]\n```\n\n#### Schema\n\n```python\nfrom datasets import Features, Sequence, Value\n\nitem = Features({\n    \"id\":   Value(\"string\"),\n    \"data\": Value(\"string\"),\n})\n\nfeatures = Features({\n    \"list\": Sequence(item),\n})\n```\n\n---\n\n###  Tested File Formats\n\nThe same schema was tested across different formats:\n\n| Format    | Method                      | Result              |\n| --------- | --------------------------- | ------------------- |\n| JSONL     | `load_dataset(\"json\", ...)` |  Arrow cast error  |\n| JSON      | `load_dataset(\"json\", ...)` |  Arrow cast error  |\n| In-memory | `Dataset.from_list(...)`    | Works as expected |\n\nThe issue seems not to be in the schema or the data, but in how `load_dataset()` handles the `Sequence(Features(...))` pattern when parsing from files (specifically JSON and JSONL).\n\n---\n\n### Expected Behavior\n\nIf `features` is explicitly defined as:\n\n```python\nFeatures({\"list\": Sequence(Features({...}))})\n```\n\nThen the data should load correctly across all backends — including from JSON and JSONL — without any Arrow casting errors. This works correctly when loading from memory via `Dataset.from_list`.\n\n---\n\n###  Environment\n\n* `datasets`: 3.6.0\n* `pyarrow`: 20.0.0\n* Python: 3.12.10\n* OS: Ubuntu 24.04.2 LTS\n* Notebook: \\[Colab test notebook available]\n\n---\n",
    "comments": [
      {
        "user": "Flink-ddd",
        "body": "Hi @lhoestq \n\nCould you help confirm whether this qualifies as a bug?\n\nIt looks like the issue stems from how `Sequence(Features(...))` is interpreted as a plain struct during schema inference, which leads to a mismatch when casting with PyArrow (especially with nested structs inside lists). From the description, this seems like an inconsistency with expected behavior.\n\nIf confirmed, I’d be happy to take a shot at investigating and potentially submitting a fix.\n\nAlso looping in @AHS-uni  — could you kindly share a minimal JSONL example that reproduces this?\n\nThanks!"
      },
      {
        "user": "AHS-uni",
        "body": "Hello @Flink-ddd \n\nI updated the minimal example and included both JSON and JSONL minimal examples in the Colab notebook. \n\nHere is the minimal  JSON file for convenience (can't upload JSONL files).\n\n[mini.json](https://github.com/user-attachments/files/20535145/mini.json)\n\nI've also found a number of issues which describe a similar problem:\n\n[7569](https://github.com/huggingface/datasets/issues/7569) (Open)\n[7137](https://github.com/huggingface/datasets/issues/7137) (Open)\n[7501](https://github.com/huggingface/datasets/issues/7501) (Closed)\n[2434](https://github.com/huggingface/datasets/issues/2434) (Closed)\n\nThe closed issues don't really address the problem (IMO). [7501](https://github.com/huggingface/datasets/issues/7501) provides a workaround (using a Python list instead of `Sequence`), but it seem precarious. "
      },
      {
        "user": "lhoestq",
        "body": "Hi ! `Sequence({...})` corresponds to a struct of lists ([docs](https://huggingface.co/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.Features)). This come from Tensorflow Datasets.\n\nIf you want to use a list of structs, you should use `[{...}]`, e.g.\n\n```python\nitem = {\n    \"id\":   Value(\"string\"),\n    \"data\": Value(\"string\"),\n}\n\nfeatures = Features({\n    \"list\": [item],\n})\n```"
      }
    ]
  },
  {
    "issue_number": 7588,
    "title": "ValueError: Invalid pattern: '**' can only be an entire path component [Colab]",
    "author": "wkambale",
    "state": "closed",
    "created_at": "2025-05-27T13:46:05Z",
    "updated_at": "2025-05-30T13:22:52Z",
    "labels": [],
    "body": "### Describe the bug\n\nI have a dataset on HF [here](https://huggingface.co/datasets/kambale/luganda-english-parallel-corpus) that i've previously used to train a translation model [here](https://huggingface.co/kambale/pearl-11m-translate).\n\nnow i changed a few hyperparameters to increase number of tokens for the model, increase Transformer layers, and all\n\nhowever, when i try to load the dataset, this error keeps coming up.. i have tried everything.. i have re-written the code a hundred times, and this keep coming up\n\n### Steps to reproduce the bug\n\nImports:\n\n```bash\n!pip install datasets huggingface_hub fsspec\n```\n\nPython code:\n\n```python\nfrom datasets import load_dataset\n\nHF_DATASET_NAME = \"kambale/luganda-english-parallel-corpus\"\n\n# Load the dataset\ntry:\n    if not HF_DATASET_NAME or HF_DATASET_NAME == \"YOUR_HF_DATASET_NAME\":\n        raise ValueError(\n            \"Please provide a valid Hugging Face dataset name.\"\n        )\n\n    dataset = load_dataset(HF_DATASET_NAME)\n\n# Omitted code as the error happens on the line above \n\nexcept ValueError as ve:\n    print(f\"Configuration Error: {ve}\")\n    raise\nexcept Exception as e:\n    print(f\"An error occurred while loading the dataset '{HF_DATASET_NAME}': {e}\")\n    raise e\n```\n\nnow, i have tried going through this [issue](https://github.com/huggingface/datasets/issues/6737) and nothing helps \n\n### Expected behavior\n\nloading the dataset successfully and perform splits (train, test, validation)\n\n### Environment info\n\nfrom the imports, i do not install specific versions of these libraries, so the latest or available version is installed\n\n* `datasets` version: latest\n* `Platform`: Google Colab\n* `Hardware`: NVIDIA A100 GPU\n* `Python` version: latest\n* `huggingface_hub` version: latest\n* `fsspec` version: latest",
    "comments": [
      {
        "user": "zero-point",
        "body": "Could you please run the following code snippet in your environment and share the exact output? This will help check for any compatibility issues within the env itself. \n\n```\nimport datasets\nimport huggingface_hub\nimport fsspec\n\nprint(\"datasets version:\", datasets.__version__)\nprint(\"huggingface_hub version:\", huggingface_hub.__version__)\nprint(\"fsspec version:\", fsspec.__version__)\n```"
      },
      {
        "user": "wkambale",
        "body": "```bash\ndatasets version: 2.14.4\nhuggingface_hub version: 0.31.4\nfsspec version: 2025.3.2\n```"
      },
      {
        "user": "CleitonOERocha",
        "body": "Version 2.14.4 is not the latest version available, in fact it is from August 08, 2023 (you can check here: https://pypi.org/project/datasets/#history)\n\nUse pip install datasets==3.6.0 to install a more recent version (from May 7, 2025)\n\nI also had the same problem with Colab, after updating to the latest version it was solved.\n\nI hope it helps"
      }
    ]
  },
  {
    "issue_number": 7363,
    "title": "ImportError: To support decoding images, please install 'Pillow'.",
    "author": "jamessdixon",
    "state": "open",
    "created_at": "2025-01-08T02:22:57Z",
    "updated_at": "2025-05-28T14:56:53Z",
    "labels": [],
    "body": "### Describe the bug\n\nFollowing this tutorial locally using a macboko and VSCode: https://huggingface.co/docs/diffusers/en/tutorials/basic_training\r\nThis line of code: for i, image in enumerate(dataset[:4][\"image\"]):\r\nthrows: ImportError: To support decoding images, please install 'Pillow'.\r\n\r\nPillow is installed.\n\n### Steps to reproduce the bug\n\nRun the tutorial\r\n\n\n### Expected behavior\n\nImages should be rendered\n\n### Environment info\n\nMacBook, VSCode",
    "comments": [
      {
        "user": "ruidazeng",
        "body": "what's your `pip show Pillow` output"
      },
      {
        "user": "chansinhui",
        "body": "same issue.. my pip show Pillow output as below:\n\n```\nName: pillow\nVersion: 11.1.0\nSummary: Python Imaging Library (Fork)\nHome-page: https://python-pillow.github.io/\nAuthor: \nAuthor-email: \"Jeffrey A. Clark\" <aclark@aclark.net>\nLicense: MIT-CMU\nLocation: [/opt/homebrew/lib/python3.10/site-packages](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/lib/python3.10/site-packages)\nRequires: \nRequired-by:\n```"
      },
      {
        "user": "Shunying",
        "body": "I encountered the same problem on Ubuntu system, my pip show Pillow output as below:\n\n```\nName: pillow\nVersion: 10.4.0\nSummary: Python Imaging Library (Fork)\nHome-page: https://python-pillow.org/\nAuthor: \nAuthor-email: \"Jeffrey A. Clark\" <[aclark@aclark.net](mailto:aclark@aclark.net)>\nLicense: HPND\nLocation: /home/shunying/.local/lib/python3.8/site-packages\nRequires: \nRequired-by: \n```\n\nWell, solved this by specifying the pip version to my conda virtual environment :)"
      }
    ]
  },
  {
    "issue_number": 7577,
    "title": "arrow_schema is not compatible with list",
    "author": "jonathanshen-upwork",
    "state": "closed",
    "created_at": "2025-05-21T16:37:01Z",
    "updated_at": "2025-05-26T18:49:51Z",
    "labels": [],
    "body": "### Describe the bug\n\n```\nimport datasets\nf = datasets.Features({'x': list[datasets.Value(dtype='int32')]})\nf.arrow_schema\n\nTraceback (most recent call last):\n  File \"datasets/features/features.py\", line 1826, in arrow_schema\n    return pa.schema(self.type).with_metadata({\"huggingface\": json.dumps(hf_metadata)})\n                     ^^^^^^^^^\n  File \"datasets/features/features.py\", line 1815, in type\n    return get_nested_type(self)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"datasets/features/features.py\", line 1252, in get_nested_type\n    return pa.struct(\n           ^^^^^^^^^^\n  File \"pyarrow/types.pxi\", line 5406, in pyarrow.lib.struct\n  File \"pyarrow/types.pxi\", line 3890, in pyarrow.lib.field\n  File \"pyarrow/types.pxi\", line 5918, in pyarrow.lib.ensure_type\nTypeError: DataType expected, got <class 'list'>\n```\n\nThe following works\n```\nf = datasets.Features({'x': datasets.LargeList(datasets.Value(dtype='int32'))})\n```\n\n### Expected behavior\n\naccording to https://github.com/huggingface/datasets/blob/458f45a22c3cc9aea5f442f6f519333dcfeae9b9/src/datasets/features/features.py#L1765 python list should be a valid type specification for features\n\n### Environment info\n\n- `datasets` version: 3.5.1\n- Platform: macOS-15.5-arm64-arm-64bit\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Thanks for reporting, I'll look into it"
      },
      {
        "user": "lhoestq",
        "body": "Actually it looks like you just forgot parenthesis:\n\n```diff\n- f = datasets.Features({'x': list[datasets.Value(dtype='int32')]})\n+ f = datasets.Features({'x': list([datasets.Value(dtype='int32')])})\n```\n\nor simply using the `[ ]` syntax:\n\n```python\nf = datasets.Features({'x':[datasets.Value(dtype='int32')]})\n```\n\nI'm closing this issue if you don't mind"
      },
      {
        "user": "jonathanshen-upwork",
        "body": "Ah is that what the syntax is? I don't think I was able to find an actual example of it so I assumed it was in the same way that you specify types eg. `list[int]`. This is good to know, thanks."
      }
    ]
  },
  {
    "issue_number": 7580,
    "title": "Requesting a specific split (eg: test) still downloads all (train, test, val) data when streaming=False.",
    "author": "s3pi",
    "state": "open",
    "created_at": "2025-05-22T11:08:16Z",
    "updated_at": "2025-05-26T18:40:31Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen using load_dataset() from the datasets library (in load.py), specifying a particular split (e.g., split=\"train\") still results in downloading data for all splits when streaming=False. This happens during the builder_instance.download_and_prepare() call.\nThis behavior leads to unnecessary bandwidth usage and longer download times, especially for large datasets, even if the user only intends to use a single split.\n\n### Steps to reproduce the bug\n\ndataset_name = \"skbose/indian-english-nptel-v0\"\ndataset = load_dataset(dataset_name, token=hf_token, split=\"test\")\n\n\n### Expected behavior\n\nOptimize the download logic so that only the required split is downloaded when streaming=False when a specific split is provided.\n\n### Environment info\n\nDataset: skbose/indian-english-nptel-v0\nPlatform: M1 Apple Silicon\nPython verison: 3.12.9\ndatasets>=3.5.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! There was a PR open to improve this: https://github.com/huggingface/datasets/pull/6832 \nbut it hasn't been continued so far.\n\nIt would be a cool improvement though !"
      }
    ]
  },
  {
    "issue_number": 7574,
    "title": "Missing multilingual directions in IWSLT2017 dataset's processing script",
    "author": "andy-joy-25",
    "state": "open",
    "created_at": "2025-05-21T09:53:17Z",
    "updated_at": "2025-05-26T18:36:38Z",
    "labels": [],
    "body": "### Describe the bug\n\nHi, \n\nUpon using `iwslt2017.py` in `IWSLT/iwslt2017` on the Hub for loading the datasets, I am unable to obtain the datasets for the language pairs `de-it`, `de-ro`, `de-nl`, `it-de`, `nl-de`, and `ro-de` using it. These 6 pairs do not show up when using `get_dataset_config_names()` to obtain the list of all the configs present in `IWSLT/iwslt2017`. This should not be the case since as mentioned in their original paper (please see https://aclanthology.org/2017.iwslt-1.1.pdf), the authors specify that \"_this year we proposed the multilingual translation between any pair of languages from {Dutch, English, German, Italian, Romanian}..._\" and because these datasets are indeed present in `data/2017-01-trnmted/texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.zip`.\n\nBest Regards,\nAnand\n\n### Steps to reproduce the bug\n\nCheck the output of `get_dataset_config_names(\"IWSLT/iwslt2017\", trust_remote_code=True)`: only 24 language pairs are present and the following 6 config names are absent: `iwslt2017-de-it`, `iwslt2017-de-ro`, `iwslt2017-de-nl`, `iwslt2017-it-de`, `iwslt2017-nl-de`, and `iwslt2017-ro-de`.\n\n### Expected behavior\n\nThe aforementioned 6 language pairs should also be present and hence, all these 6 language pairs' IWSLT2017 datasets must also be available for further use. \n\nI would suggest removing `de` from the `BI_LANGUAGES` list and moving it over to the `MULTI_LANGUAGES` list instead in `iwslt2017.py` to account for all the 6 missing language pairs (the same `de-en` dataset is present in both `data/2017-01-trnmted/texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.zip` and `data/2017-01-trnted/texts/de/en/de-en.zip` but the `de-ro`, `de-nl`, `it-de`, `nl-de`, and `ro-de` datasets are only present in  `data/2017-01-trnmted/texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.zip`: so, its unclear why the following comment: _`# XXX: Artificially removed DE from here, as it also exists within bilingual data`_ has been added as `L71` in `iwslt2017.py`). The `README.md` file in `IWSLT/iwslt2017`must then be re-created using `datasets-cli test path/to/iwslt2017.py --save_info --all_configs` to pass all split size verification checks for the 6 new language pairs which were previously non-existent. \n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Linux-6.8.0-56-generic-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- `huggingface_hub` version: 0.30.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0",
    "comments": [
      {
        "user": "andy-joy-25",
        "body": "I have opened 2 PRs on the Hub: `https://huggingface.co/datasets/IWSLT/iwslt2017/discussions/7` and `https://huggingface.co/datasets/IWSLT/iwslt2017/discussions/8` to resolve this issue"
      },
      {
        "user": "lhoestq",
        "body": "cool ! I pinged the owners of the dataset on HF to merge your PRs :)"
      }
    ]
  },
  {
    "issue_number": 7583,
    "title": "load_dataset type stubs reject List[str] for split parameter, but runtime supports it",
    "author": "hierr",
    "state": "closed",
    "created_at": "2025-05-25T02:33:18Z",
    "updated_at": "2025-05-26T18:29:58Z",
    "labels": [],
    "body": "### Describe the bug\n\nThe [load_dataset](https://huggingface.co/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset) method accepts a `List[str]` as the split parameter at runtime, however, the current type stubs restrict the split parameter to `Union[str, Split, None]`. This causes type checkers like Pylance to raise `reportArgumentType` errors when passing a list of strings, even though it works as intended at runtime.\n\n### Steps to reproduce the bug\n\n1. Use load_dataset with multiple splits e.g.:\n```\nfrom datasets import load_dataset\n\nds_train, ds_val, ds_test = load_dataset(\n    \"Silly-Machine/TuPyE-Dataset\",\n    \"binary\",\n    split=[\"train[:75%]\", \"train[75%:]\", \"test\"]\n)\n```\n2. Observe that code executes correctly at runtime and Pylance raises `Argument of type \"List[str]\" cannot be assigned to parameter \"split\" of type \"str | Split | None\"`\n\n### Expected behavior\n\nThe type stubs for [load_dataset](https://huggingface.co/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset) should accept `Union[str, Split, List[str], None]` or more specific overloads for the split parameter to correctly represent runtime behavior. \n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39\n- Python version: 3.12.7\n- `huggingface_hub` version: 0.32.0\n- PyArrow version: 20.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2025.3.0",
    "comments": []
  },
  {
    "issue_number": 7584,
    "title": "Add LMDB format support",
    "author": "trotsky1997",
    "state": "open",
    "created_at": "2025-05-26T07:10:13Z",
    "updated_at": "2025-05-26T18:23:37Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nAdd LMDB format support for large memory-mapping files\n\n### Motivation\n\nAdd LMDB format support for large memory-mapping files\n\n### Your contribution\n\nI'm trying to add it",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Can you explain what's your use case ? Is it about converting LMDB to Dataset objects (i.e. converting to Arrow) ?"
      }
    ]
  },
  {
    "issue_number": 7586,
    "title": "help is appreciated",
    "author": "rajasekarnp1",
    "state": "open",
    "created_at": "2025-05-26T14:00:42Z",
    "updated_at": "2025-05-26T18:21:57Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nhttps://github.com/rajasekarnp1/neural-audio-upscaler/tree/main\n\n### Motivation\n\n ai model develpment and audio \n\n### Your contribution\n\n   ai model develpment and audio ",
    "comments": [
      {
        "user": "lhoestq",
        "body": "how is this related to this repository ?"
      }
    ]
  },
  {
    "issue_number": 7018,
    "title": "`load_dataset` fails to load dataset saved by `save_to_disk`",
    "author": "sliedes",
    "state": "open",
    "created_at": "2024-07-01T12:19:19Z",
    "updated_at": "2025-05-24T05:21:12Z",
    "labels": [],
    "body": "### Describe the bug\n\nThis code fails to load the dataset it just saved:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\nMODEL = \"google-bert/bert-base-cased\"\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\r\n\r\ndataset = load_dataset(\"yelp_review_full\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\r\ntokenized_datasets.save_to_disk(\"dataset\")\r\n\r\ntokenized_datasets = load_dataset(\"dataset/\")  # raises\r\n```\r\n\r\nIt raises `ValueError: Couldn't infer the same data file format for all splits. Got {NamedSplit('train'): ('arrow', {}), NamedSplit('test'): ('json', {})}`.\r\n\r\nI believe this bug is caused by the [logic that tries to infer dataset format](https://github.com/huggingface/datasets/blob/9af8dd3de7626183a9a9ec8973cebc672d690400/src/datasets/load.py#L556). It counts the most common file extension. However, a small dataset can fit in a single `.arrow` file and have two JSON metadata files, causing the format to be inferred as JSON:\r\n\r\n```shell\r\n$ ls -l dataset/test\r\n-rw-r--r-- 1 sliedes sliedes 191498784 Jul  1 13:55 data-00000-of-00001.arrow\r\n-rw-r--r-- 1 sliedes sliedes      1730 Jul  1 13:55 dataset_info.json\r\n-rw-r--r-- 1 sliedes sliedes       249 Jul  1 13:55 state.json\r\n```\n\n### Steps to reproduce the bug\n\nExecute the code above.\n\n### Expected behavior\n\nThe dataset is loaded successfully.\n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-6.9.3-arch1-1-x86_64-with-glibc2.39\r\n- Python version: 3.12.4\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0\r\n",
    "comments": [
      {
        "user": "happyTonakai",
        "body": "In my case the error was:\r\n```\r\nValueError: You are trying to load a dataset that was saved using `save_to_disk`. Please use `load_from_disk` instead.\r\n```\r\nDid you try `load_from_disk`?"
      },
      {
        "user": "ManuelFay",
        "body": "More generally, any reason there is no API consistency between save_to_disk and push_to_hub ? \r\n\r\nWould be nice to be able to save_to_disk and then upload manually to the hub and load_dataset (which works in some situations but not all)..."
      },
      {
        "user": "kfarivar",
        "body": "I have the exact same problem !"
      }
    ]
  },
  {
    "issue_number": 7381,
    "title": "Iterating over values of a column in the IterableDataset",
    "author": "TopCoder2K",
    "state": "closed",
    "created_at": "2025-01-28T13:17:36Z",
    "updated_at": "2025-05-22T18:00:04Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nI would like to be able to iterate (and re-iterate if needed) over a column of an `IterableDataset` instance. The following example shows the supposed API:\n```python\ndef gen():\n    yield {\"text\": \"Good\", \"label\": 0}\n    yield {\"text\": \"Bad\", \"label\": 1}\n\nds = IterableDataset.from_generator(gen)\ntexts = ds[\"text\"]\n\nfor v in texts:\n    print(v)  # Prints \"Good\" and \"Bad\"\n\nfor v in texts:\n    print(v)  # Prints \"Good\" and \"Bad\" again\n```\n\n### Motivation\n\nIn the real world problems, huge NNs like Transformer are not always the best option, so there is a need to conduct experiments with different methods. While 🤗Datasets is perfectly adapted to 🤗Transformers, it may be inconvenient when being used with other libraries. The ability to retrieve a particular column is the case (e.g., gensim's FastText [requires](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.train) only lists of strings, not dictionaries).\nWhile there are ways to achieve the desired functionality, they are not good ([forum](https://discuss.huggingface.co/t/how-to-iterate-over-values-of-a-column-in-the-iterabledataset/135649)). It would be great if there was a built-in solution.\n\n### Your contribution\n\nTheoretically, I can submit a PR, but I have very little knowledge of the internal structure of 🤗Datasets, so some help may be needed.\nMoreover, I can only work on weekends, since I have a full-time job. However, the feature does not seem to be popular, so there is no need to implement it as fast as possible.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "I'd be in favor of that ! I saw many people implementing their own iterables that wrap a dataset just to iterate on a single column, that would make things more practical.\n\nKinda related: https://github.com/huggingface/datasets/issues/5847"
      },
      {
        "user": "TopCoder2K",
        "body": "(For anyone's information, I'm going on vacation for the next 3 weeks, so the work is postponed. If anyone can implement this feature within the next 4 weeks, go ahead :) )\n\nUPD from 04/06/25:\nI'm planning to start work on the feature in early May."
      },
      {
        "user": "TopCoder2K",
        "body": "#self-assign"
      }
    ]
  },
  {
    "issue_number": 7569,
    "title": "Dataset creation is broken if nesting a dict inside a dict inside a list",
    "author": "TimSchneider42",
    "state": "open",
    "created_at": "2025-05-13T21:06:45Z",
    "updated_at": "2025-05-20T19:25:15Z",
    "labels": [],
    "body": "### Describe the bug\n\nHey,\n\nI noticed that the creation of datasets with `Dataset.from_generator` is broken if dicts and lists are nested in a certain way and a schema is being passed. See below for details.\n\nBest,\nTim\n\n### Steps to reproduce the bug\n\nRuning this code:\n\n```python\nfrom datasets import Dataset, Features, Sequence, Value\n\n\ndef generator():\n    yield {\n        \"a\": [{\"b\": {\"c\": 0}}],\n    }\n\n\nfeatures = Features(\n    {\n        \"a\": Sequence(\n            feature={\n                \"b\": {\n                    \"c\": Value(\"int32\"),\n                },\n            },\n            length=1,\n        )\n    }\n)\n\ndataset = Dataset.from_generator(generator, features=features)\n```\n\nleads to \n```\nGenerating train split: 1 examples [00:00, 540.85 examples/s]\nTraceback (most recent call last):\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1635, in _prepare_split_single\n    num_examples, num_bytes = writer.finalize()\n                              ^^^^^^^^^^^^^^^^^\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 657, in finalize\n    self.write_examples_on_file()\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 510, in write_examples_on_file\n    self.write_batch(batch_examples=batch_examples)\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 629, in write_batch\n    pa_table = pa.Table.from_arrays(arrays, schema=schema)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/table.pxi\", line 4851, in pyarrow.lib.Table.from_arrays\n  File \"pyarrow/table.pxi\", line 1608, in pyarrow.lib._sanitize_arrays\n  File \"pyarrow/array.pxi\", line 399, in pyarrow.lib.asarray\n  File \"pyarrow/array.pxi\", line 1004, in pyarrow.lib.Array.cast\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/pyarrow/compute.py\", line 405, in cast\n    return call_function(\"cast\", [arr], options, memory_pool)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/_compute.pyx\", line 598, in pyarrow._compute.call_function\n  File \"pyarrow/_compute.pyx\", line 393, in pyarrow._compute.Function.call\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowNotImplementedError: Unsupported cast from fixed_size_list<item: struct<c: int32>>[1] to struct using function cast_struct\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/user/test/tools/hf_test2.py\", line 23, in <module>\n    dataset = Dataset.from_generator(generator, features=features)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 1114, in from_generator\n    ).read()\n      ^^^^^^\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/io/generator.py\", line 49, in read\n    self.builder.download_and_prepare(\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 925, in download_and_prepare\n    self._download_and_prepare(\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1649, in _download_and_prepare\n    super()._download_and_prepare(\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1001, in _download_and_prepare\n    self._prepare_split(split_generator, **prepare_split_kwargs)\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1487, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1644, in _prepare_split_single\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n\nProcess finished with exit code 1\n```\n\n### Expected behavior\n\nI expected this code not to lead to an error. \n\nI have done some digging and figured out that the problem seems to be the `get_nested_type` function in `features.py`, which, for whatever reason, flips Sequences and dicts whenever it encounters a dict inside of a sequence. This seems to be necessary, as disabling that flip leads to another error. However, by keeping that flip enabled for the highest level and disabling it for all subsequent levels, I was able to work around this problem. Specifically, by patching `get_nested_type` as follows, it works on the given example (emphasis on the `level` parameter I added):\n\n```python\ndef get_nested_type(schema: FeatureType, level=0) -> pa.DataType:\n    \"\"\"\n    get_nested_type() converts a datasets.FeatureType into a pyarrow.DataType, and acts as the inverse of\n        generate_from_arrow_type().\n\n    It performs double-duty as the implementation of Features.type and handles the conversion of\n        datasets.Feature->pa.struct\n    \"\"\"\n    # Nested structures: we allow dict, list/tuples, sequences\n    if isinstance(schema, Features):\n        return pa.struct(\n            {key: get_nested_type(schema[key], level = level + 1) for key in schema}\n        )  # Features is subclass of dict, and dict order is deterministic since Python 3.6\n    elif isinstance(schema, dict):\n        return pa.struct(\n            {key: get_nested_type(schema[key], level = level + 1) for key in schema}\n        )  # however don't sort on struct types since the order matters\n    elif isinstance(schema, (list, tuple)):\n        if len(schema) != 1:\n            raise ValueError(\"When defining list feature, you should just provide one example of the inner type\")\n        value_type = get_nested_type(schema[0], level = level + 1)\n        return pa.list_(value_type)\n    elif isinstance(schema, LargeList):\n        value_type = get_nested_type(schema.feature, level = level + 1)\n        return pa.large_list(value_type)\n    elif isinstance(schema, Sequence):\n        value_type = get_nested_type(schema.feature, level = level + 1)\n        # We allow to reverse list of dict => dict of list for compatibility with tfds\n        if isinstance(schema.feature, dict) and level == 1:\n            data_type = pa.struct({f.name: pa.list_(f.type, schema.length) for f in value_type})\n        else:\n            data_type = pa.list_(value_type, schema.length)\n        return data_type\n\n    # Other objects are callable which returns their data type (ClassLabel, Array2D, Translation, Arrow datatype creation methods)\n    return schema()\n```\n\nI have honestly no idea what I am doing here, so this might produce other issues for different inputs.\n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Linux-6.8.0-59-generic-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n\nAlso tested it with 3.5.0, same result.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! That's because Séquence is a type that comes from tensorflow datasets and inverts lists and focus when doing Séquence(dict).\n\nInstead you should use a list. In your case\n```python\nfeatures = Features({\n    \"a\": [{\"b\": {\"c\": Value(\"string\")}}]\n})\n```"
      },
      {
        "user": "TimSchneider42",
        "body": "Hi,\n\nThanks for the swift reply! Could you quickly clarify a couple of points?\n\n1. Is there any benefit in using Sequence over normal lists? Especially for longer lists (in my case, up to 256 entries)\n2. When exactly can I use Sequence? If there is a maximum of one level of dictionaries inside, then it's always fine?\n3. When creating the data in the generator, do I need to swap lists and dicts manually, or does that happen automatically?\n\nAlso, the documentation does not seem to mention this limitation of the Sequence type anywhere and encourages users to use it [here](https://huggingface.co/docs/datasets/en/about_dataset_features). In fact, I did not even know that just using a Python list was an option. Maybe the documentation can be improved to mention the limitations of Sequence and highlight that lists can be used instead.\n\nThanks a lot in advance!\n\nBest,\nTim"
      }
    ]
  },
  {
    "issue_number": 7515,
    "title": "`concatenate_datasets` does not preserve Pytorch format for IterableDataset",
    "author": "francescorubbo",
    "state": "closed",
    "created_at": "2025-04-15T04:36:34Z",
    "updated_at": "2025-05-19T15:07:38Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen concatenating datasets with `concatenate_datasets`, I would expect the resulting combined dataset to be in the same format as the inputs (assuming it's consistent). This is indeed the behavior when combining `Dataset`, but not when combining `IterableDataset`. Specifically, when applying `concatenate_datasets` to a list of `IterableDataset` in Pytorch format (i.e. using `.with_format(Pytorch)`), the output `IterableDataset` is not in Pytorch format.\n\n### Steps to reproduce the bug\n\n```\nimport datasets\nds = datasets.Dataset.from_dict({\"a\": [1,2,3]})\niterable_ds = ds.to_iterable_dataset()\ndatasets.concatenate_datasets([ds.with_format(\"torch\")]) # <- this preserves Pytorch format\ndatasets.concatenate_datasets([iterable_ds.with_format(\"torch\")]) # <- this does NOT preserves Pytorch format\n```\n\n### Expected behavior\n\nPytorch format should be preserved when combining IterableDataset in Pytorch format.\n\n### Environment info\n\ndatasets==3.5.0, Python 3.11.11, torch==2.2.2",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Oh indeed it would be cool to return the same format in that case. Would you like to submit a PR ? The function that does the concatenation is here:\n\nhttps://github.com/huggingface/datasets/blob/90e5bf8a8599b625d6103ee5ac83b98269991141/src/datasets/iterable_dataset.py#L3375-L3380"
      },
      {
        "user": "francescorubbo",
        "body": "Thank you for the pointer, @lhoestq ! See #7522 "
      }
    ]
  },
  {
    "issue_number": 7510,
    "title": "Incompatibile dill version (0.3.9) in datasets 2.18.0 - 3.5.0",
    "author": "JGrel",
    "state": "open",
    "created_at": "2025-04-14T07:22:44Z",
    "updated_at": "2025-05-19T14:54:04Z",
    "labels": [],
    "body": "### Describe the bug\n\nDatasets 2.18.0 - 3.5.0 has a dependency on dill < 0.3.9. This causes errors with dill >= 0.3.9.\n\nCould you please take a look into it and make it compatible?\n\n### Steps to reproduce the bug\n\n1. Install setuptools >= 2.18.0\n2. Install dill >=0.3.9\n3. Run pip check\n4. Output:\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 2.18.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n\n### Expected behavior\n\nPip install both libraries without any errors\n\n### Environment info\n\nDatasets version: 2.18 - 3.5\nPython: 3.11",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! We can bump `dill` to 0.3.9 if we make sure it's deterministic and doesn't break the caching mechanism in `datasets`.\n\nWould you be interested in opening a PR ? Then we can run the CI to see if it works"
      },
      {
        "user": "JGrel",
        "body": "Hi!. Yeah I can do it. Should I make any changes besides dill versions?"
      },
      {
        "user": "lhoestq",
        "body": "There are probably some usage of internal functions from `dill` that we'll need to update in `datasets`\n\nIf you run `pytest tests/test_fingerprint.py` you should already have a good idea of what works and what doesn't.\nBut feel free to open a PR anyway, this way we can run the full CI and see the results\n"
      }
    ]
  },
  {
    "issue_number": 7495,
    "title": "Columns in the dataset obtained though load_dataset do not correspond to the one in the dataset viewer since 3.4.0",
    "author": "bruno-hays",
    "state": "open",
    "created_at": "2025-04-02T17:01:11Z",
    "updated_at": "2025-05-19T13:54:16Z",
    "labels": [],
    "body": "### Describe the bug\n\nI have noticed that on my dataset named [BrunoHays/Accueil_UBS](https://huggingface.co/datasets/BrunoHays/Accueil_UBS), since the version 3.4.0, every column except audio is missing when I load the dataset.\n\nInterestingly, the dataset viewer still shows the correct columns\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import load_dataset\nds = load_dataset(\"BrunoHays/Accueil_UBS\", streaming=True)\nprint(next(iter(ds[\"test\"])).keys())\n```\nWith datasets >= 3.4.0: \n-> dict_keys(['audio'])\nWith datasets == 3.3.2:\n-> dict_keys(['audio', 'id', 'speaker', 'sentence', 'raw_sentence', 'start_timestamp', 'end_timestamp', 'overlap'])\n\n\n### Expected behavior\n\nAll the columns should be present\n\n### Environment info\n\n- `datasets` version: 3.3.2\n- Platform: macOS-14.6.1-x86_64-i386-64bit\n- Python version: 3.10.15\n- `huggingface_hub` version: 0.30.1\n- PyArrow version: 16.1.0\n- Pandas version: 1.5.3\n- `fsspec` version: 2023.10.0\n",
    "comments": [
      {
        "user": "phoebecd",
        "body": "Hi, the dataset viewer shows all the possible columns and their types, but `load_dataset()` iterates through all the columns that you defined. It seems that you only have one column (‘audio’) defined in your dataset because when I ran `print(ds.column_names)`, the only name I got was “audio”. You need to clearly define all the other features of the dataset as columns to enable your original code to work. Furthermore, you can run this code to print out all the features of your dataset: \n```python\nfrom datasets import load_dataset_builder\nds_builder = load_dataset_builder(\"BrunoHays/Accueil_UBS\")\nprint(ds_builder.info.features)\n```\n"
      }
    ]
  },
  {
    "issue_number": 6903,
    "title": "Add the option of saving in parquet instead of arrow ",
    "author": "arita37",
    "state": "open",
    "created_at": "2024-05-16T13:35:51Z",
    "updated_at": "2025-05-19T12:14:14Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nIn dataset.save_to_disk('/path/to/save/dataset'),\r\n\r\nadd the option to save in parquet format\r\n\r\ndataset.save_to_disk('/path/to/save/dataset', format=\"parquet\"),\r\n\r\nbecause arrow is not used for Production Big data.... (only parquet)\r\n\r\n\n\n### Motivation\n\nbecause arrow is not used for Production Big data.... (only parquet)\n\n### Your contribution\n\nI can do the testing !",
    "comments": [
      {
        "user": "Dref360",
        "body": "I think [`Dataset.to_parquet`](https://huggingface.co/docs/datasets/v1.10.2/package_reference/main_classes.html#datasets.Dataset.to_parquet) is what you're looking for.\r\n\r\nLet me know if I'm wrong "
      },
      {
        "user": "arita37",
        "body": "No, it does not save the metadata json.\r\n\r\nWe have to recode all meta json load/save\r\nwith another custome functions.\r\n\r\nsave_to_disk\r\nand load should have option with\r\n“Parquet” instead of “arrow”\r\n\r\nsince “arrow” is never user for production \r\n(only parquet).\r\n\r\nThanks !\r\n\r\n> On May 17, 2024, at 5:38, Frédéric Branchaud-Charron ***@***.***> wrote:\r\n> \r\n> ﻿\r\n> I think Dataset.to_parquet is what you're looking for.\r\n> \r\n> Let me know if I'm wrong\r\n> \r\n> —\r\n> Reply to this email directly, view it on GitHub, or unsubscribe.\r\n> You are receiving this because you authored the thread.\r\n"
      },
      {
        "user": "lhoestq",
        "body": "You can use `to_parquet` and `ds.info.write_to_directory()` to save the dataset info"
      }
    ]
  },
  {
    "issue_number": 7568,
    "title": "`IterableDatasetDict.map()` call removes `column_names` (in fact info.features)",
    "author": "mombip",
    "state": "open",
    "created_at": "2025-05-13T15:45:42Z",
    "updated_at": "2025-05-19T12:09:48Z",
    "labels": [],
    "body": "When calling `IterableDatasetDict.map()`, each split’s `IterableDataset.map()` is invoked without a `features` argument. While omitting the argument isn’t itself incorrect, the implementation then sets `info.features = features`, which destroys the original `features` content. Since `IterableDataset.column_names` relies on `info.features`, it ends up broken (`None`).\n\n**Reproduction**\n\n1.  Define an IterableDatasetDict with a non-None features schema.\n2.  my_iterable_dataset_dict contains \"text\" column.\n3. Call:\n```Python\nnew_dict = my_iterable_dataset_dict.map(\n    function=my_fn,\n    with_indices=False,\n    batched=True,\n    batch_size=16,\n)\n```\n4. Observe\n```Python\nnew_dict[\"train\"].info.features  # {'text': Value(dtype='string', id=None)}\nnew_dict[\"train\"].column_names   # ['text']\n```\n5. Call:\n```Python\nnew_dict = my_iterable_dataset_dict.map(\n    function=my_fn,\n    with_indices=False,\n    batched=True,\n    batch_size=16,\n    remove_columns=[\"foo\"]\n)\n```\n6. Observe:\n```Python\nnew_dict[\"train\"].info.features  # → None\nnew_dict[\"train\"].column_names   # → None\n```\n5. Internally, in dataset_dict.py this loop omits features ([code](https://github.com/huggingface/datasets/blob/b9efdc64c3bfb8f21f8a4a22b21bddd31ecd5a31/src/datasets/dataset_dict.py#L2047C5-L2056C14)):\n```Python\nfor split, dataset in self.items():\n    dataset_dict[split] = dataset.map(\n        function=function,\n        with_indices=with_indices,\n        input_columns=input_columns,\n        batched=batched,\n        batch_size=batch_size,\n        drop_last_batch=drop_last_batch,\n        remove_columns=remove_columns,\n        fn_kwargs=fn_kwargs,\n        # features omitted → defaults to None\n    )\n```\n7. Then inside IterableDataset.map() ([code](https://github.com/huggingface/datasets/blob/b9efdc64c3bfb8f21f8a4a22b21bddd31ecd5a31/src/datasets/iterable_dataset.py#L2619C1-L2622C37)) correct `info.features` is replaced by features which is None:\n```Python\ninfo = self.info.copy()\ninfo.features = features  # features is None here\nreturn IterableDataset(..., info=info, ...)\n```\n\n**Suggestion**\nIt looks like this replacement was added intentionally but maybe should be done only if `features` is `not None`.\n\n**Workarround:**\n`SFTTrainer` calls `dataset.map()` several times and then fails on `NoneType` when iterating `dataset.column_names`.\nI decided to write this patch - works form me.\n\n```python\ndef patch_iterable_dataset_map():\n    _orig_map = IterableDataset.map\n\n    def _patched_map(self, *args, **kwargs):\n        if \"features\" not in kwargs or kwargs[\"features\"] is None:\n            kwargs[\"features\"] = self.info.features\n        return _orig_map(self, *args, **kwargs)\n\n    IterableDataset.map = _patched_map\n```\n\n\n\n\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! IterableDataset doesn't know what's the output of the function you pass to map(), so it's not possible to know in advance the features of the output dataset.\n\nThere is a workaround though: either do `ds = ds.map(..., features=features)`, or you can do `ds = ds._resolve_features()` which iterates on the first rows to infer the dataset features."
      },
      {
        "user": "mombip",
        "body": "Thank you. I understand that “IterableDataset doesn't know what's the output of the function”—that’s true, but:\n\nUnfortunately, the workaround you proposed **doesn’t solve** the problem. `ds.map()` is called multiple times by third-party code (i.e. `SFTTrainer`). To apply your approach, I would have to modify external library code. That’s why I decided to patch the _class_ rather than update `dataset` _objects_ (in fact, updating the object after `map()` was my initial approach, but then I realized I’m not the only one mapping an already-mapped dataset.)\n\nAs a user, I expected that after mapping I would get a new dataset with the correct column names. If, for some reason, that can’t be the default behavior, I would expect an argument—i.e. `auto_resolve_features: bool = False` — to control how my dataset is mapped if following mapping operation are called.\n\nIt’s also problematic that `column_names` are tied to `features`, which is even more confusing and forces you to inspect the source code to understand what’s going on.\n\n**New version of workaround:**\n```python\ndef patch_iterable_dataset_map():\n    _orig_map = IterableDataset.map\n\n    def _patched_map(self, *args, **kwargs):\n        ds = _orig_map(self, *args, **kwargs)\n        return ds._resolve_features()\n\n    IterableDataset.map = _patched_map\n```"
      },
      {
        "user": "lhoestq",
        "body": "I see, maybe `.resolve_features()` should be called by default in this case in the SFTTrainer ? (or pass `features=` if the data processing always output the same features)\n\nWe can even support a new parameter `features=\"infer\"` if it would be comfortable to not use internal methods in SFTTrainer"
      }
    ]
  },
  {
    "issue_number": 3847,
    "title": "Datasets' cache not re-used",
    "author": "gejinchen",
    "state": "open",
    "created_at": "2022-03-07T19:55:15Z",
    "updated_at": "2025-05-19T11:58:55Z",
    "labels": [
      "bug"
    ],
    "body": "## Describe the bug\r\nFor most tokenizers I have tested (e.g. the RoBERTa tokenizer), the data preprocessing cache are not fully reused in the first few runs, although their `.arrow` cache files are in the cache directory.\r\n\r\n## Steps to reproduce the bug\r\nHere is a reproducer. The GPT2 tokenizer works perfectly with caching, but not the RoBERTa tokenizer in this example.\r\n```python\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\nraw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\r\n# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\r\ntext_column_name = \"text\"\r\ncolumn_names = raw_datasets[\"train\"].column_names\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\r\n\r\ntokenized_datasets = raw_datasets.map(\r\n    tokenize_function,\r\n    batched=True,\r\n    remove_columns=column_names,\r\n    load_from_cache_file=True,\r\n    desc=\"Running tokenizer on every text in dataset\",\r\n)\r\n```\r\n\r\n## Expected results\r\nNo tokenization would be required after the 1st run. Everything should be loaded from the cache.\r\n\r\n## Actual results\r\nTokenization for some subsets are repeated at the 2nd and 3rd run. Starting from the 4th run, everything are loaded from cache.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Ubuntu 18.04.6 LTS\r\n- Python version: 3.6.9\r\n- PyArrow version: 6.0.1\r\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "<s>I think this is because the tokenizer is stateful and because the order in which the splits are processed is not deterministic. Because of that, the hash of the tokenizer may change for certain splits, which causes issues with caching.\r\n\r\nTo fix this we can try making the order of the splits deterministic for map.</s>"
      },
      {
        "user": "lhoestq",
        "body": "Actually this is not because of the order of the splits, but most likely because the tokenizer used to process the second split is in a state that has been modified by the first split.\r\n\r\nTherefore after reloading the first split from the cache, then the second split can't be reloaded since the tokenizer hasn't seen the first split (and therefore is considered a different tokenizer).\r\n\r\nThis is a bit trickier to fix, we can explore fixing this next week maybe"
      },
      {
        "user": "lhoestq",
        "body": "Sorry didn't have the bandwidth to take care of this yet - will re-assign when I'm diving into it again !"
      }
    ]
  },
  {
    "issue_number": 1627,
    "title": "`Dataset.map` disable progress bar",
    "author": "Nickil21",
    "state": "closed",
    "created_at": "2020-12-23T17:53:42Z",
    "updated_at": "2025-05-16T16:36:24Z",
    "labels": [],
    "body": "I can't find anything to turn off the `tqdm` progress bars while running a preprocessing function using `Dataset.map`. I want to do akin to `disable_tqdm=True` in the case of `transformers`. Is there something like that?",
    "comments": [
      {
        "user": "mariosasko",
        "body": "Progress bar can be disabled like this:\r\n```python\r\nfrom datasets.utils.logging import set_verbosity_error\r\nset_verbosity_error()\r\n```\r\n\r\nThere is this line in `Dataset.map`:\r\n```python\r\nnot_verbose = bool(logger.getEffectiveLevel() > WARNING)\r\n```\r\n\r\nSo any logging level higher than `WARNING` turns off the progress bar."
      },
      {
        "user": "akeyhero",
        "body": "From the linked issues above, an up-to-date solution is:\r\n\r\n```python\r\nfrom datasets.utils.logging import disable_progress_bar\r\ndisable_progress_bar()\r\n```\r\n\r\nhttps://github.com/huggingface/datasets/blob/c6e08fcfc3a04e53430c26fa7c07da4cb18d977d/src/datasets/utils/logging.py#L233-L236"
      },
      {
        "user": "brynhayder",
        "body": "Why not have a parameter in the function such as `progress: bool = True`?"
      }
    ]
  },
  {
    "issue_number": 7567,
    "title": "interleave_datasets seed with multiple workers",
    "author": "jonathanasdf",
    "state": "open",
    "created_at": "2025-05-12T22:38:27Z",
    "updated_at": "2025-05-15T20:39:37Z",
    "labels": [],
    "body": "### Describe the bug\n\nUsing interleave_datasets with multiple dataloader workers and a seed set causes the same dataset sampling order across all workers.\n\nShould the seed be modulated with the worker id?\n\n### Steps to reproduce the bug\n\nSee above\n\n### Expected behavior\n\nSee above\n\n### Environment info\n\n- `datasets` version: 3.5.1\n- Platform: macOS-15.4.1-arm64-arm-64bit\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! It's already the case IIRC: the effective seed looks like `seed + worker_id`. Do you have a reproducible example ?"
      },
      {
        "user": "jonathanasdf",
        "body": "here is an example with shuffle\n\n```\nimport itertools\nimport datasets\nimport multiprocessing\nimport torch.utils.data\n\n\ndef gen(shard):\n  worker_info = torch.utils.data.get_worker_info()\n  for i in range(10):\n    yield {'value': i, 'worker_id': worker_info.id}\n\n\ndef main():\n  ds = datasets.IterableDataset.from_generator(gen, gen_kwargs={'shard': list(range(8))})\n  ds = ds.shuffle(buffer_size=100, seed=1234)\n  dataloader = torch.utils.data.DataLoader(ds, batch_size=None, num_workers=8)\n  for i, ex in enumerate(itertools.islice(dataloader, 50)):\n    print(i, ex)\n\n\nif __name__ == '__main__':\n  multiprocessing.set_start_method('spawn')\n  main()\n```\n\n```\npython test.py\n0 {'value': 8, 'worker_id': 0}\n1 {'value': 8, 'worker_id': 1}\n2 {'value': 8, 'worker_id': 2}\n3 {'value': 8, 'worker_id': 3}\n4 {'value': 8, 'worker_id': 4}\n5 {'value': 8, 'worker_id': 5}\n6 {'value': 8, 'worker_id': 6}\n7 {'value': 8, 'worker_id': 7}\n8 {'value': 9, 'worker_id': 0}\n9 {'value': 9, 'worker_id': 1}\n10 {'value': 9, 'worker_id': 2}\n11 {'value': 9, 'worker_id': 3}\n12 {'value': 9, 'worker_id': 4}\n13 {'value': 9, 'worker_id': 5}\n14 {'value': 9, 'worker_id': 6}\n15 {'value': 9, 'worker_id': 7}\n16 {'value': 5, 'worker_id': 0}\n17 {'value': 5, 'worker_id': 1}\n18 {'value': 5, 'worker_id': 2}\n19 {'value': 5, 'worker_id': 3}\n```"
      },
      {
        "user": "jonathanasdf",
        "body": "With `interleave_datasets`\n\n```\nimport itertools\nimport datasets\nimport multiprocessing\nimport torch.utils.data\n\n\ndef gen(shard, value):\n  while True:\n    yield {'value': value}\n\n\ndef main():\n  ds = [\n    datasets.IterableDataset.from_generator(gen, gen_kwargs={'shard': list(range(8)), 'value': i})\n    for i in range(10)\n  ]\n  ds = datasets.interleave_datasets(ds, probabilities=[1 / len(ds)] * len(ds), seed=1234)\n  dataloader = torch.utils.data.DataLoader(ds, batch_size=None, num_workers=8)\n  for i, ex in enumerate(itertools.islice(dataloader, 50)):\n    print(i, ex)\n\n\nif __name__ == '__main__':\n  multiprocessing.set_start_method('spawn')\n  main()\n```\n\n```\npython test.py\n0 {'value': 9}\n1 {'value': 9}\n2 {'value': 9}\n3 {'value': 9}\n4 {'value': 9}\n5 {'value': 9}\n6 {'value': 9}\n7 {'value': 9}\n8 {'value': 3}\n9 {'value': 3}\n10 {'value': 3}\n11 {'value': 3}\n12 {'value': 3}\n13 {'value': 3}\n14 {'value': 3}\n15 {'value': 3}\n16 {'value': 9}\n17 {'value': 9}\n18 {'value': 9}\n19 {'value': 9}\n20 {'value': 9}\n21 {'value': 9}\n22 {'value': 9}\n23 {'value': 9}\n```"
      }
    ]
  },
  {
    "issue_number": 1796,
    "title": "Filter on dataset too much slowww",
    "author": "ayubSubhaniya",
    "state": "open",
    "created_at": "2021-01-30T04:09:19Z",
    "updated_at": "2025-05-15T13:19:55Z",
    "labels": [],
    "body": "I have a dataset with 50M rows.\r\nFor pre-processing, I need to tokenize this and filter rows with the large sequence.\r\n\r\nMy tokenization took roughly 12mins. I used `map()` with batch size 1024 and multi-process with 96 processes.\r\n\r\nWhen I applied the `filter()` function it is taking too much time. I need to filter sequences based on a boolean column.\r\nBelow are the variants I tried.\r\n1. filter() with batch size 1024, single process (takes roughly 3 hr)\r\n2. filter() with batch size 1024, 96 processes (takes 5-6 hrs ¯\\\\\\_(ツ)\\_/¯)\r\n3. filter() with loading all data in memory, only a single boolean column (never ends).\r\n\r\nCan someone please help?\r\n\r\nBelow is a sample code for small dataset.\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ndataset = dataset.map(lambda x: {'flag': random.randint(0,1)==1})\r\n\r\ndef _amplify(data):\r\n        return data\r\n\r\ndataset = dataset.filter(_amplify, batch_size=1024, keep_in_memory=False, input_columns=['flag'])\r\n```\r\n",
    "comments": [
      {
        "user": "ayubSubhaniya",
        "body": "When I use the filter on the arrow table directly, it works like butter. But I can't find a way to update the table in `Dataset` object.\r\n\r\n```\r\nds_table = dataset.data.filter(mask=dataset['flag'])\r\n```"
      },
      {
        "user": "ayubSubhaniya",
        "body": "@thomwolf @lhoestq can you guys please take a look and recommend some solution."
      },
      {
        "user": "lhoestq",
        "body": "Hi ! Currently the filter method reads the dataset batch by batch to write a new, filtered, arrow file on disk. Therefore all the reading + writing can take some time.\r\nUsing a mask directly on the arrow table doesn't do any read or write operation therefore it's way quicker.\r\n\r\nReplacing the old table by the new one should do the job:\r\n```python\r\ndataset._data = dataset._data.filter(...)\r\n```\r\n\r\nNote: this is a **workaround** and in general users shouldn't have to do that. In particular if you did some `shuffle` or `select` before that then it would not work correctly since the indices mapping (index from `__getitem__` -> index in the table) would not be valid anymore. But if you haven't done any `shuffle`, `select`, `shard`, `train_test_split` etc. then it should work.\r\n\r\nIdeally it would be awesome to update the filter function to allow masking this way !\r\nIf you would like to give it a shot I will be happy to help :) "
      }
    ]
  },
  {
    "issue_number": 6256,
    "title": "load_dataset() function's cache_dir does not seems to work",
    "author": "andyzhu",
    "state": "closed",
    "created_at": "2023-09-24T15:34:06Z",
    "updated_at": "2025-05-14T10:08:53Z",
    "labels": [],
    "body": "### Describe the bug\n\ndatasets version: 2.14.5\r\n\r\nwhen trying to run the following command\r\ntrec = load_dataset('trec', split='train[:1000]', cache_dir='/path/to/my/dir')\r\n\r\nI keep getting error saying the command does not have permission to the default cache directory on my macbook pro machine. \r\n\r\nIt seems the cache_dir parameter cannot change the dataset saving directory from the default \r\n\r\nwhat ever explained in the https://huggingface.co/docs/datasets/cache does not seem to work\r\n\n\n### Steps to reproduce the bug\n\ndatasets version: 2.14.5\r\n\r\nwhen trying to run the following command\r\ntrec = load_dataset('trec', split='train[:1000]', cache_dir='/path/to/my/dir')\r\n\r\nI keep getting error saying the command does not have permission to the default cache directory on my macbook pro machine. \r\n\r\nIt seems the cache_dir parameter cannot change the dataset saving directory from the default \r\n\r\nwhat ever explained in the https://huggingface.co/docs/datasets/cache does not seem to work\n\n### Expected behavior\n\nthe dataset should be saved to the cache_dir points to \n\n### Environment info\n\ndatasets version: 2.14.5\r\nmacos X: Ventura 13.4.1 (c)",
    "comments": [
      {
        "user": "mariosasko",
        "body": "Can you share the error message?\r\n\r\nAlso, it would help if you could check whether `huggingface_hub`'s download behaves the same:\r\n```python\r\nfrom huggingface_hub import snapshot_download\r\nsnapshot_download(\"trec\", repo_type=\"dataset\", cache_dir='/path/to/my/dir)\r\n```\r\n\r\nIn the next major release, we aim to switch to `huggingface_hub` for file download/caching, but we could align the `cache_dir`'s `umask` behavior earlier than this if their solution works for your use case."
      },
      {
        "user": "srinjoym-cerebras",
        "body": "@mariosasko, Hey , is there any update on this? "
      },
      {
        "user": "Dipeshtamboli",
        "body": "Yes, I am still facing the same issue.\r\n\r\n```python\r\nfrom huggingface_hub import snapshot_download\r\nsnapshot_download(\"trec\", repo_type=\"dataset\", cache_dir='/path/to/my/dir)\r\n```\r\n\r\nBut this is working for me.\r\n\r\nThanks."
      }
    ]
  },
  {
    "issue_number": 7546,
    "title": "Large memory use when loading large datasets to a ZFS pool",
    "author": "FredHaa",
    "state": "closed",
    "created_at": "2025-05-01T14:43:47Z",
    "updated_at": "2025-05-13T13:30:09Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen I load large parquet based datasets from the hub like `MLCommons/peoples_speech` using `load_dataset`, all my memory (500GB) is used and isn't released after loading, meaning that the process is terminated by the kernel if I try to load an additional dataset. This makes it impossible to train models using multiple large datasets.\n\n### Steps to reproduce the bug\n\n`uv run --with datasets==3.5.1 python`\n\n```python\nfrom datasets import load_dataset\nload_dataset('MLCommons/peoples_speech', 'clean')\nload_dataset('mozilla-foundation/common_voice_17_0', 'en')\n```\n\n### Expected behavior\n\nI would expect that a lot less than 500GB of RAM would be required to load the dataset, or at least that the RAM usage would be cleared as soon as the dataset is loaded (and thus reside as a memory mapped file) such that other datasets can be loaded.\n\n### Environment info\n\nI am currently using the latest datasets==3.5.1 but I have had the same problem with multiple other versions.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! datasets are memory mapped from disk, so they don't fill out your RAM. Not sure what's the source of your memory issue.\n\nWhat kind of system are you using ? and what kind of disk ?"
      },
      {
        "user": "FredHaa",
        "body": "Well, the fact of the matter is that my RAM is getting filled out by running the given example, as shown in [this video](https://streamable.com/usb0ql).\n\nMy system is a GPU server running Ubuntu. The disk is a SATA SSD attached to the server using a backplane. It is formatted with ZFS, mounted in /cache, and my HF_HOME is set to /cache/hf\n\nI really need this fixed, so I am more than willing to test out various suggestions you might have, or write a PR if we can figure out what is going on."
      },
      {
        "user": "lhoestq",
        "body": "I'm not super familiar with ZFS, but it looks like it loads the data in memory when the files are memory mapped, which is an issue.\n\nMaybe it's a caching mechanism ? Since `datasets` accesses every memory mapped file to read a small part (the metadata of the arrow record batches), maybe ZFS brings the whole files in memory for quicker subsequent reads. This is an antipattern when it comes to lazy loading datasets of that size though"
      }
    ]
  },
  {
    "issue_number": 7444,
    "title": "Excessive warnings when resuming an IterableDataset+buffered shuffle+DDP.",
    "author": "dhruvdcoder",
    "state": "open",
    "created_at": "2025-03-11T16:34:39Z",
    "updated_at": "2025-05-13T09:41:03Z",
    "labels": [],
    "body": "### Describe the bug\n\nI have a large dataset that I shared into 1024 shards and save on the disk during pre-processing. During training, I load the dataset using load_from_disk() and convert it into an iterable dataset, shuffle it and split the shards to different DDP nodes using the recommended method.\n\nHowever, when the training is resumed mid-epoch, I get thousands of identical warning messages:\n\n```\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\n```\n\n\n\n### Steps to reproduce the bug\n\n1. Run a multi-node training job using the following python script and interrupt the training after a few seconds to save a mid-epoch checkpoint.\n\n```python\n#!/usr/bin/env python\nimport os\nimport time\nfrom typing import Dict, List\nimport torch\nimport lightning as pl\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom datasets.distributed import split_dataset_by_node\nimport datasets\nfrom transformers import AutoTokenizer\nfrom more_itertools import flatten, chunked\nfrom torchdata.stateful_dataloader import StatefulDataLoader\nfrom lightning.pytorch.callbacks.on_exception_checkpoint import (\n    OnExceptionCheckpoint,\n)\n\ndatasets.logging.set_verbosity_debug()\n\n\ndef dummy_generator():\n    # Generate 60 examples: integers from $0$ to $59$\n    # 64 sequences of different lengths\n    dataset = [\n        list(range(3, 10)),\n        list(range(10, 15)),\n        list(range(15, 21)),\n        list(range(21, 27)),\n        list(range(27, 31)),\n        list(range(31, 36)),\n        list(range(36, 45)),\n        list(range(45, 50)),\n    ]\n    for i in range(8):\n        for j, ids in enumerate(dataset):\n            yield {\"token_ids\": [idx + i * 50 for idx in ids]}\n\n\ndef group_texts(\n    examples: Dict[str, List[List[int]]],\n    block_size: int,\n    eos_token_id: int,\n    bos_token_id: int,\n    pad_token_id: int,\n) -> Dict[str, List[List[int]]]:\n    real_block_size = block_size - 2  # make space for bos and eos\n    # colapse the sequences into a single list of tokens and then create blocks of real_block_size\n    input_ids = []\n    attention_mask = []\n    for block in chunked(flatten(examples[\"token_ids\"]), real_block_size):\n        s = [bos_token_id] + list(block) + [eos_token_id]\n        ls = len(s)\n        attn = [True] * ls\n        s += [pad_token_id] * (block_size - ls)\n        attn += [False] * (block_size - ls)\n        input_ids.append(s)\n        attention_mask.append(attn)\n\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n\n\ndef collate_fn(batch):\n    return {\n        \"input_ids\": torch.tensor(\n            [item[\"input_ids\"] for item in batch], dtype=torch.long\n        ),\n        \"attention_mask\": torch.tensor(\n            [item[\"attention_mask\"] for item in batch], dtype=torch.long\n        ),\n    }\n\n\nclass DummyModule(pl.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n        # A dummy linear layer (not used for actual computation)\n        self.layer = torch.nn.Linear(1, 1)\n        self.ds = None\n        self.prepare_data_per_node = False\n\n    def on_train_start(self):\n        # This hook is called once training begins on each process.\n        print(f\"[Rank {self.global_rank}] Training started.\", flush=True)\n        self.data_file = open(f\"data_{self.global_rank}.txt\", \"w\")\n\n    def on_train_end(self):\n        self.data_file.close()\n\n    def training_step(self, batch, batch_idx):\n        # Print batch information to verify data loading.\n        time.sleep(5)\n        # print(\"batch\", batch, flush=True)\n        print(\n            f\"\\n[Rank {self.global_rank}] Training step, epoch {self.trainer.current_epoch}, batch {batch_idx}: {batch['input_ids']}\",\n            flush=True,\n        )\n        self.data_file.write(\n            f\"[Rank {self.global_rank}] Training step, epoch {self.trainer.current_epoch}, batch {batch_idx}: {batch['input_ids']}\\n\"\n        )\n        # Compute a dummy loss (here, simply a constant tensor)\n        loss = torch.tensor(0.0, requires_grad=True)\n        return loss\n\n    def on_train_epoch_start(self):\n        epoch = self.trainer.current_epoch\n        print(\n            f\"[Rank {self.global_rank}] Training epoch {epoch} started.\",\n            flush=True,\n        )\n        self.data_file.write(\n            f\"[Rank {self.global_rank}] Training epoch {epoch} started.\\n\"\n        )\n\n    def configure_optimizers(self):\n        # Return a dummy optimizer.\n        return torch.optim.SGD(self.parameters(), lr=0.001)\n\n\nclass DM(pl.LightningDataModule):\n    def __init__(self):\n        super().__init__()\n        self.ds = None\n        self.prepare_data_per_node = False\n\n    def set_epoch(self, epoch: int):\n        self.ds.set_epoch(epoch)\n\n    def prepare_data(self):\n        # download the dataset\n        dataset = Dataset.from_generator(dummy_generator)\n        # save the dataset\n        dataset.save_to_disk(\"dataset\", num_shards=4)\n\n    def setup(self, stage: str):\n        # load the dataset\n        ds = datasets.load_from_disk(\"dataset\").to_iterable_dataset(\n            num_shards=4\n        )\n        ds = ds.map(\n            group_texts,\n            batched=True,\n            batch_size=5,\n            fn_kwargs={\n                \"block_size\": 5,\n                \"eos_token_id\": 1,\n                \"bos_token_id\": 0,\n                \"pad_token_id\": 2,\n            },\n            remove_columns=[\"token_ids\"],\n        ).shuffle(seed=42, buffer_size=8)\n        ds = split_dataset_by_node(\n            ds,\n            rank=self.trainer.global_rank,\n            world_size=self.trainer.world_size,\n        )\n        self.ds = ds\n\n    def train_dataloader(self):\n        print(\n            f\"[Rank {self.trainer.global_rank}] Preparing train_dataloader...\",\n            flush=True,\n        )\n        rank = self.trainer.global_rank\n        print(\n            f\"[Rank {rank}] Global rank: {self.trainer.global_rank}\",\n            flush=True,\n        )\n        world_size = self.trainer.world_size\n        print(f\"[Rank {rank}] World size: {world_size}\", flush=True)\n        return StatefulDataLoader(\n            self.ds,\n            batch_size=2,\n            num_workers=2,\n            collate_fn=collate_fn,\n            drop_last=True,\n            persistent_workers=True,\n        )\n\n\nif __name__ == \"__main__\":\n    print(\"Starting Lightning training\", flush=True)\n    # Optionally, print some SLURM environment info for debugging.\n    print(f\"SLURM_NNODES: {os.environ.get('SLURM_NNODES', '1')}\", flush=True)\n\n    # Determine the number of nodes from SLURM (defaulting to 1 if not set)\n    num_nodes = int(os.environ.get(\"SLURM_NNODES\", \"1\"))\n    model = DummyModule()\n    dm = DM()\n    on_exception = OnExceptionCheckpoint(\n        dirpath=\"checkpoints\",\n        filename=\"on_exception\",\n    )\n\n    # Configure the Trainer to use distributed data parallel (DDP).\n    trainer = pl.Trainer(\n        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n        devices=1,\n        strategy=(\n            \"ddp\" if num_nodes > 1 else \"auto\"\n        ),  # Use DDP strategy for multi-node training.\n        num_nodes=num_nodes,\n        max_epochs=2,\n        logger=False,\n        enable_checkpointing=True,\n        num_sanity_val_steps=0,\n        enable_progress_bar=False,\n        callbacks=[on_exception],\n    )\n\n    # resume (uncomment to resume)\n    # trainer.fit(model, datamodule=dm, ckpt_path=\"checkpoints/on_exception.ckpt\")\n    # train\n\n    trainer.fit(model, datamodule=dm)\n```\n\n\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=pl_ddp_test\n#SBATCH --nodes=2                     # Adjust number of nodes as needed\n#SBATCH --ntasks-per-node=1           # One GPU (process) per node\n#SBATCH --cpus-per-task=3             # At least as many dataloader workers as required\n#SBATCH --gres=gpu:1                  # Request one GPU per node\n#SBATCH --time=00:10:00               # Job runtime (adjust as needed)\n#SBATCH --partition=gpu-preempt               # Partition or queue name\n#SBATCH -o script.out\n\n# Disable Python output buffering.\nexport PYTHONUNBUFFERED=1\n\necho \"SLURM job starting on $(date)\"\necho \"Running on nodes: $SLURM_NODELIST\"\necho \"Current directory: $(pwd)\"\nls -l\n\n# Launch the script using srun so that each process starts the Lightning module.\nsrun script.py\n```\n2. Uncomment the \"resume\" line (second to last) and comment the original `trainer.fit` call (last line). \nIt will produce the following log.\n```\n[Rank 0] Preparing train_dataloader...\n[Rank 0] Global rank: 0\n[Rank 0] World size: 2\n[Rank 1] Preparing train_dataloader...\n[Rank 1] Global rank: 1\n[Rank 1] World size: 2\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nAssigning 2 shards (or data sources) of the dataset to each node.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#0 dataloader worker#1, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#0 dataloader worker#0, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nnode#0 dataloader worker#1, ': Finished iterating over 1/1 shards.\nnode#0 dataloader worker#0, ': Finished iterating over 1/1 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\n[Rank 0] Training started.\n[Rank 0] Training epoch 0 started.\n[Rank 0] Training epoch 1 started.\nAssigning 2 shards (or data sources) of the dataset to each node.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#0 dataloader worker#1, ': Starting to iterate over 1/2 shards.\nnode#0 dataloader worker#0, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#1 dataloader worker#1, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#1 dataloader worker#0, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nnode#0 dataloader worker#1, ': Finished iterating over 1/1 shards.\nnode#0 dataloader worker#0, ': Finished iterating over 1/1 shards.\n`Trainer.fit` stopped: `max_epochs=2` reached.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nnode#1 dataloader worker#1, ': Finished iterating over 1/1 shards.\nnode#1 dataloader worker#0, ': Finished iterating over 1/1 shards.\n[Rank 1] Training started.\n[Rank 1] Training epoch 0 started.\n[Rank 1] Training epoch 1 started.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#1 dataloader worker#1, ': Starting to iterate over 1/2 shards.\nnode#1 dataloader worker#0, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nnode#1 dataloader worker#0, ': Finished iterating over 1/1 shards.\nnode#1 dataloader worker#1, ': Finished iterating over 1/1 shards.\n```\n\nI'm also attaching the relevant state_dict to make sure that the state is being checkpointed as expected.\n\n```\n{'_iterator_finished': True,\n                                     '_snapshot': {'_last_yielded_worker_id': 1,\n                                                   '_main_snapshot': {'_IterableDataset_len_called': None,\n                                                                      '_base_seed': 3992758080362545099,\n                                                                      '_index_sampler_state': {'samples_yielded': 64},\n                                                                      '_num_workers': 2,\n                                                                      '_sampler_iter_state': None,\n                                                                      '_sampler_iter_yielded': 32,\n                                                                      '_shared_seed': None},\n                                                   '_snapshot_step': 32,\n                                                   '_worker_snapshots': {'worker_0': {'dataset_state': {'ex_iterable': {'shard_example_idx': 0,\n                                                                                                                        'shard_idx': 1},\n                                                                                                        'num_examples_since_previous_state': 0,\n                                                                                                        'previous_state': {'shard_example_idx': 0,\n                                                                                                                           'shard_idx': 1},\n                                                                                                        'previous_state_example_idx': 33},\n                                                                                      'fetcher_state': {'dataset_iter_state': None,\n                                                                                                        'fetcher_ended': False},\n                                                                                      'worker_id': 0},\n                                                                         'worker_1': {'dataset_state': {'ex_iterable': {'shard_example_idx': 0,\n                                                                                                                        'shard_idx': 1},\n                                                                                                        'num_examples_since_previous_state': 0,\n                                                                                                        'previous_state': {'shard_example_idx': 0,\n                                                                                                                           'shard_idx': 1},\n                                                                                                        'previous_state_example_idx': 33},\n                                                                                      'fetcher_state': {'dataset_iter_state': None,\n                                                                                                        'fetcher_ended': False},\n                                                                                      'worker_id': 1}}},\n                                     '_steps_since_snapshot': 0}\n```\n\n### Expected behavior\n\nSince I'm following all the recommended steps, I don't expect to see any warning when resuming. Am I doing something wrong? Also, can someone explain why I'm seeing 20 identical messages in the log in this reproduction setting? I'm trying to understand why I see thousands of these messages with the actual dataset. \n\nOne more surprising thing I noticed in the logs is the change in a number of shards per worker. In the following messages, the denominator changes from 2 to 1.  \n\n```\nnode#1 dataloader worker#1, ': Starting to iterate over 1/2 shards.\n...\nnode#1 dataloader worker#1, ': Finished iterating over 1/1 shards.\n```\n\n### Environment info\n\npython: 3.11.10\ndatasets: 3.3.2\nlightning: 2.3.1\n\n",
    "comments": [
      {
        "user": "wydwww",
        "body": "I had a similar issue when loading the saved iterable dataset state to fast-forward to the mid-train location before resuming. This happened when I shuffled a concatenated dataset. A `iterable_data_state_dict.json` file was saved during checkpointing in the Trainer with:\n```\ndef _save_rng_state(self, output_dir):\n    super()._save_rng_state(output_dir)\n    if self.args.should_save:\n        with open(os.path.join(output_dir, f'iterable_data_state_dict.json'), 'w', encoding='utf-8') as fo:\n            json.dump(self.train_dataset.state_dict(), fo, ensure_ascii=False)\n```\nThen when resuming training, I use `load_state_dict` to get the dataset state:\n```\nif training_args.resume_from_checkpoint:\n    if isinstance(training_args.resume_from_checkpoint, bool):\n        resume_from_checkpoint = get_last_checkpoint(training_args.output_dir)\n    else:\n        resume_from_checkpoint = training_args.resume_from_checkpoint\n    last_ckpt_iterable_data_state_dict_file_path = os.path.join(resume_from_checkpoint, f'iterable_data_state_dict.json')\n    if not training_args.ignore_data_skip:\n        raise ValueError(f'Please set `ignore_data_skip`=True to skip tokenization.')\n    with open(last_ckpt_iterable_data_state_dict_file_path, 'r', encoding='utf-8') as f:\n        train_dataset_state_dict = json.load(f)\n        train_dataset.load_state_dict(train_dataset_state_dict)\n        print(f'Loaded train_dataset state from {last_ckpt_iterable_data_state_dict_file_path}')\n```\n\nThen code works fine before I shuffled a subset of the training data to:\n```\nmath_dataset = concatenate_datasets([A, B]).to_iterable_dataset()\nshuffled_math_dataset = math_dataset.shuffle(seed=42, buffer_size=1000000)\n```\n\nOther than the warning, a real problem is that the loss bumped after loading a ckpt:\n\n<img width=\"400\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c8944e81-9df9-4857-82de-6ab9ebc1b066\" />"
      }
    ]
  },
  {
    "issue_number": 7551,
    "title": "Issue with offline mode and partial dataset cached",
    "author": "nrv",
    "state": "open",
    "created_at": "2025-05-04T16:49:37Z",
    "updated_at": "2025-05-13T03:18:43Z",
    "labels": [],
    "body": "### Describe the bug\n\nHi, \n\na issue related to #4760 here when loading a single file from a dataset, unable to access it in offline mode afterwards\n\n### Steps to reproduce the bug\n\n```python\nimport os\n# os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\nos.environ[\"HF_TOKEN\"] = \"xxxxxxxxxxxxxx\"\n\nimport datasets\n\ndataset_name = \"uonlp/CulturaX\"\ndata_files = \"fr/fr_part_00038.parquet\"\n\nds = datasets.load_dataset(dataset_name, split='train', data_files=data_files)\nprint(f\"Dataset loaded   : {ds}\")\n```\nOnce the file has been cached, I rerun with the HF_HUB_OFFLINE activated an get this error : \n```\nValueError: Couldn't find cache for uonlp/CulturaX for config 'default-1e725f978350254e'\nAvailable configs in the cache: ['default-2935e8cdcc21c613']\n```\n\n### Expected behavior\n\nShould be able to access the previously cached files\n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-5.4.0-215-generic-x86_64-with-glibc2.31\n- Python version: 3.12.0\n- `huggingface_hub` version: 0.27.0\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2024.3.1\n",
    "comments": [
      {
        "user": "nrv",
        "body": "It seems the problem comes from builder.py / create_config_id()\n\nOn the first call, when the cache is empty we have\n```\nconfig_kwargs = {'data_files': {'train': ['hf://datasets/uonlp/CulturaX@6a8734bc69fefcbb7735f4f9250f43e4cd7a442e/fr/fr_part_00038.parquet']}}\n```\nleading to config_id beeing 'default-2935e8cdcc21c613'\n\nthen, on the second call, \n```\nconfig_kwargs = {'data_files': 'fr/fr_part_00038.parquet'}\n```\nthus explaining why the hash is not the same, despite having the same parameter when calling load_dataset : data_files=\"fr/fr_part_00038.parquet\""
      },
      {
        "user": "nrv",
        "body": "Same behavior with version 3.5.1"
      },
      {
        "user": "YanshekWoo",
        "body": "Same issue when loading `google/IndicGenBench_flores_in` with `dataset==2.21.0` and `dataset==3.6.0` ."
      }
    ]
  },
  {
    "issue_number": 7433,
    "title": "`Dataset.map` ignores existing caches and remaps when ran with different `num_proc`",
    "author": "ringohoffman",
    "state": "closed",
    "created_at": "2025-03-03T05:51:26Z",
    "updated_at": "2025-05-12T15:14:09Z",
    "labels": [],
    "body": "### Describe the bug\n\nIf you `map` a dataset and save it to a specific `cache_file_name` with a specific `num_proc`, and then call map again with that same existing `cache_file_name` but a different `num_proc`, the dataset will be re-mapped.\n\n### Steps to reproduce the bug\n\n1. Download a dataset\n```python\nimport datasets\n\ndataset = datasets.load_dataset(\"ylecun/mnist\")\n```\n\n```\nGenerating train split: 100%|██████████| 60000/60000 [00:00<00:00, 116429.85 examples/s]\nGenerating test split: 100%|██████████| 10000/10000 [00:00<00:00, 103310.27 examples/s]\n```\n\n2. `map` and cache it with a specific `num_proc`\n\n```python\ncache_file_name=\"./cache/train.map\"\ndataset[\"train\"].map(lambda x: x, cache_file_name=cache_file_name, num_proc=2)\n```\n\n```\nMap (num_proc=2): 100%|██████████| 60000/60000 [00:01<00:00, 53764.03 examples/s]\n```\n\n3. `map` it with a different `num_proc` and the same `cache_file_name` as before\n\n```python\ndataset[\"train\"].map(lambda x: x, cache_file_name=cache_file_name, num_proc=3)\n```\n\n```\nMap (num_proc=3): 100%|██████████| 60000/60000 [00:00<00:00, 65377.12 examples/s]\n```\n\n### Expected behavior\n\nIf I specify an existing `cache_file_name`, I don't expect using a different `num_proc` than the one that was used to generate it to cause the dataset to have be be re-mapped.\n\n### Environment info\n\n```console\n$ datasets-cli env\n\n- `datasets` version: 3.3.2\n- Platform: Linux-5.15.0-131-generic-x86_64-with-glibc2.35\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.29.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "This feels related: https://github.com/huggingface/datasets/issues/3044"
      },
      {
        "user": "ringohoffman",
        "body": "@lhoestq This comment specifically, I agree:\n\n* https://github.com/huggingface/datasets/issues/3044#issuecomment-1239877570\n\n> Almost a year later and I'm in a similar boat. Using custom fingerprints and when using multiprocessing the cached datasets are saved with a template at the end of the filename (something like \"000001_of_000008\" for every process of num_proc). So if in the next time you run the script you set num_proc to a different number, the cache cannot be used.\n> \n> Is there any way to get around this? I am processing a huge dataset so I do the processing on one machine and then transfer the processed data to another in its cache dir but currently that's not possible due to num_proc mismatch.\n\n"
      }
    ]
  },
  {
    "issue_number": 7548,
    "title": "Python 3.13t (free threads) Compat",
    "author": "Qubitium",
    "state": "open",
    "created_at": "2025-05-02T09:20:09Z",
    "updated_at": "2025-05-12T15:11:32Z",
    "labels": [],
    "body": "### Describe the bug\n\nCannot install `datasets` under `python 3.13t` due to dependency on `aiohttp` and aiohttp cannot be built for free-threading python. \n\nThe `free threading` support issue in `aiothttp` is active since August 2024! Ouch.\n\nhttps://github.com/aio-libs/aiohttp/issues/8796#issue-2475941784\n\n`pip install dataset`\n\n```bash\n(vm313t) root@gpu-base:~/GPTQModel# pip install datasets\nWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/datasets/                                                                                                                                             \nCollecting datasets                                                                                                                                                                          \n  Using cached datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /root/vm313t/lib/python3.13t/site-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (2.2.5)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Using cached pyarrow-20.0.0-cp313-cp313t-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nCollecting pandas (from datasets)\n  Using cached pandas-2.2.3-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\nRequirement already satisfied: requests>=2.32.2 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (4.67.1)\nCollecting xxhash (from datasets)\n  Using cached xxhash-3.5.0-cp313-cp313t-linux_x86_64.whl\nCollecting multiprocess<0.70.17 (from datasets)\n  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nCollecting aiohttp (from datasets)\n  Using cached aiohttp-3.11.18.tar.gz (7.7 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: huggingface-hub>=0.24.0 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /root/vm313t/lib/python3.13t/site-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (6.0.2)\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\nCollecting attrs>=17.3.0 (from aiohttp->datasets)\n  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n  Using cached frozenlist-1.6.0-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n  Using cached multidict-6.4.3-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp->datasets)\n  Using cached propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n  Using cached yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\nRequirement already satisfied: idna>=2.0 in /root/vm313t/lib/python3.13t/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /root/vm313t/lib/python3.13t/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /root/vm313t/lib/python3.13t/site-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /root/vm313t/lib/python3.13t/site-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /root/vm313t/lib/python3.13t/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\nCollecting python-dateutil>=2.8.2 (from pandas->datasets)\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting pytz>=2020.1 (from pandas->datasets)\n  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata>=2022.7 (from pandas->datasets)\n  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets)\n  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\nUsing cached datasets-3.5.1-py3-none-any.whl (491 kB)\nUsing cached dill-0.3.8-py3-none-any.whl (116 kB)\nUsing cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\nUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\nUsing cached multidict-6.4.3-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\nUsing cached yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (404 kB)\nUsing cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nUsing cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nUsing cached attrs-25.3.0-py3-none-any.whl (63 kB)\nUsing cached frozenlist-1.6.0-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\nUsing cached propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\nUsing cached pyarrow-20.0.0-cp313-cp313t-manylinux_2_28_x86_64.whl (42.2 MB)\nUsing cached pandas-2.2.3-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\nUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\nUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\nUsing cached six-1.17.0-py2.py3-none-any.whl (11 kB)\nUsing cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\nBuilding wheels for collected packages: aiohttp\n  Building wheel for aiohttp (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n  \n  × Building wheel for aiohttp (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [156 lines of output]\n      *********************\n      * Accelerated build *\n      *********************\n      /tmp/pip-build-env-wjqi8_7w/overlay/lib/python3.13t/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n      !!\n      \n              ********************************************************************************\n              Please consider removing the following classifiers in favor of a SPDX license expression:\n      \n              License :: OSI Approved :: Apache Software License\n      \n              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n              ********************************************************************************\n      \n      !!\n        self._finalize_license_expression()\n      running bdist_wheel\n      running build\n      running build_py\n      creating build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/typedefs.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http_parser.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client_reqrep.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client_ws.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_app.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http_websocket.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/resolver.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/tracing.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http_writer.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http_exceptions.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/log.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/__init__.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_runner.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/worker.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/connector.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client_exceptions.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_middlewares.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/tcp_helpers.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_response.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_server.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_request.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_urldispatcher.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_exceptions.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/formdata.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/streams.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/multipart.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_routedef.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_ws.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/payload.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client_proto.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_log.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/base_protocol.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/payload_streamer.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_fileresponse.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/test_utils.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/cookiejar.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/compression_utils.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/hdrs.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/helpers.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/pytest_plugin.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_protocol.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/abc.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      creating build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/__init__.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/writer.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/models.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/reader.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/reader_c.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/helpers.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/reader_py.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      running egg_info\n      writing aiohttp.egg-info/PKG-INFO\n      writing dependency_links to aiohttp.egg-info/dependency_links.txt\n      writing requirements to aiohttp.egg-info/requires.txt\n      writing top-level names to aiohttp.egg-info/top_level.txt\n      reading manifest file 'aiohttp.egg-info/SOURCES.txt'\n      reading manifest template 'MANIFEST.in'\n      warning: no files found matching 'aiohttp' anywhere in distribution\n      warning: no files found matching '*.pyi' anywhere in distribution\n      warning: no previously-included files matching '*.pyc' found anywhere in distribution\n      warning: no previously-included files matching '*.pyd' found anywhere in distribution\n      warning: no previously-included files matching '*.so' found anywhere in distribution\n      warning: no previously-included files matching '*.lib' found anywhere in distribution\n      warning: no previously-included files matching '*.dll' found anywhere in distribution\n      warning: no previously-included files matching '*.a' found anywhere in distribution\n      warning: no previously-included files matching '*.obj' found anywhere in distribution\n      warning: no previously-included files found matching 'aiohttp/*.html'\n      no previously-included directories found matching 'docs/_build'\n      adding license file 'LICENSE.txt'\n      writing manifest file 'aiohttp.egg-info/SOURCES.txt'\n      copying aiohttp/_cparser.pxd -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/_find_header.pxd -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/_headers.pxi -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/_http_parser.pyx -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/_http_writer.pyx -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/py.typed -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      creating build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/_cparser.pxd.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/_find_header.pxd.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/_http_parser.pyx.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/_http_writer.pyx.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/hdrs.py.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/_websocket/mask.pxd -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/mask.pyx -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/reader_c.pxd -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      creating build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket/.hash\n      copying aiohttp/_websocket/.hash/mask.pxd.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket/.hash\n      copying aiohttp/_websocket/.hash/mask.pyx.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket/.hash\n      copying aiohttp/_websocket/.hash/reader_c.pxd.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket/.hash\n      running build_ext\n      building 'aiohttp._websocket.mask' extension\n      creating build/temp.linux-x86_64-cpython-313t/aiohttp/_websocket\n      x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fno-omit-frame-pointer -mno-omit-leaf-frame-pointer -fstack-protector-strong -fstack-clash-protection -Wformat -Werror=format-security -fcf-protection -fPIC -I/root/vm313t/include -I/usr/include/python3.13t -c aiohttp/_websocket/mask.c -o build/temp.linux-x86_64-cpython-313t/aiohttp/_websocket/mask.o\n      aiohttp/_websocket/mask.c:1864:80: error: unknown type name ‘__pyx_vectorcallfunc’; did you mean ‘vectorcallfunc’?\n       1864 | static CYTHON_INLINE PyObject *__Pyx_PyVectorcall_FastCallDict(PyObject *func, __pyx_vectorcallfunc vc, PyObject *const *args, size_t nargs, PyObject *kw);\n            |                                                                                ^~~~~~~~~~~~~~~~~~~~\n            |                                                                                vectorcallfunc\n      aiohttp/_websocket/mask.c: In function ‘__pyx_f_7aiohttp_10_websocket_4mask__websocket_mask_cython’:\n      aiohttp/_websocket/mask.c:2905:3: warning: ‘Py_OptimizeFlag’ is deprecated [-Wdeprecated-declarations]\n       2905 |   if (unlikely(__pyx_assertions_enabled())) {\n            |   ^~\n      In file included from /usr/include/python3.13t/Python.h:76,\n                       from aiohttp/_websocket/mask.c:16:\n      /usr/include/python3.13t/cpython/pydebug.h:13:37: note: declared here\n         13 | Py_DEPRECATED(3.12) PyAPI_DATA(int) Py_OptimizeFlag;\n            |                                     ^~~~~~~~~~~~~~~\n      aiohttp/_websocket/mask.c: At top level:\n      aiohttp/_websocket/mask.c:4846:69: error: unknown type name ‘__pyx_vectorcallfunc’; did you mean ‘vectorcallfunc’?\n       4846 | static PyObject *__Pyx_PyVectorcall_FastCallDict_kw(PyObject *func, __pyx_vectorcallfunc vc, PyObject *const *args, size_t nargs, PyObject *kw)\n            |                                                                     ^~~~~~~~~~~~~~~~~~~~\n            |                                                                     vectorcallfunc\n      aiohttp/_websocket/mask.c:4891:80: error: unknown type name ‘__pyx_vectorcallfunc’; did you mean ‘vectorcallfunc’?\n       4891 | static CYTHON_INLINE PyObject *__Pyx_PyVectorcall_FastCallDict(PyObject *func, __pyx_vectorcallfunc vc, PyObject *const *args, size_t nargs, PyObject *kw)\n            |                                                                                ^~~~~~~~~~~~~~~~~~~~\n            |                                                                                vectorcallfunc\n      aiohttp/_websocket/mask.c: In function ‘__Pyx_CyFunction_CallAsMethod’:\n      aiohttp/_websocket/mask.c:5580:6: error: unknown type name ‘__pyx_vectorcallfunc’; did you mean ‘vectorcallfunc’?\n       5580 |      __pyx_vectorcallfunc vc = __Pyx_CyFunction_func_vectorcall(cyfunc);\n            |      ^~~~~~~~~~~~~~~~~~~~\n            |      vectorcallfunc\n      aiohttp/_websocket/mask.c:1954:45: warning: initialization of ‘int’ from ‘vectorcallfunc’ {aka ‘struct _object * (*)(struct _object *, struct _object * const*, long unsigned int,  struct _object *)’} makes integer from pointer without a cast [-Wint-conversion]\n       1954 | #define __Pyx_CyFunction_func_vectorcall(f) (((PyCFunctionObject*)f)->vectorcall)\n            |                                             ^\n      aiohttp/_websocket/mask.c:5580:32: note: in expansion of macro ‘__Pyx_CyFunction_func_vectorcall’\n       5580 |      __pyx_vectorcallfunc vc = __Pyx_CyFunction_func_vectorcall(cyfunc);\n            |                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      aiohttp/_websocket/mask.c:5583:16: warning: implicit declaration of function ‘__Pyx_PyVectorcall_FastCallDict’ [-Wimplicit-function-declaration]\n       5583 |         return __Pyx_PyVectorcall_FastCallDict(func, vc, &PyTuple_GET_ITEM(args, 0), (size_t)PyTuple_GET_SIZE(args), kw);\n            |                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      aiohttp/_websocket/mask.c:5583:16: warning: returning ‘int’ from a function with return type ‘PyObject *’ {aka ‘struct _object *’} makes pointer from integer without a cast [-Wint-conversion]\n       5583 |         return __Pyx_PyVectorcall_FastCallDict(func, vc, &PyTuple_GET_ITEM(args, 0), (size_t)PyTuple_GET_SIZE(args), kw);\n            |                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for aiohttp\nFailed to build aiohttp                                                                                                                                                                      \nERROR: Failed to build installable wheels for some pyproject.toml based projects (aiohttp)\n```\n\n### Steps to reproduce the bug\n\nSee above\n\n### Expected behavior\n\nInstall\n\n### Environment info\n\nUbuntu 24.04",
    "comments": [
      {
        "user": "Qubitium",
        "body": "Update: `datasets` use `aiohttp` for data streaming and from what I understand data streaming is useful for large datasets that do not fit in memory and/or multi-modal datasets like image/audio where you only what the actual binary bits to fed in as needed. \n\nHowever, there are also many cases where aiohttp will never be used. Text datasets that are not huge, relative to machine spec, and non-multi-modal datasets. \n\nGetting `aiohttp` fixed for `free threading` appeals to be a large task that is not going to be get done in a quick manner. It may be faster to make `aiohttp` optional and not forced build. Otherwise, testing python 3.13t is going to be a painful install. \n\nI have created a fork/branch that temp disables aiohttp import so non-streaming usage of datasets can be tested under python 3.13.t:\n\nhttps://github.com/Qubitium/datasets/tree/disable-aiohttp-depend"
      },
      {
        "user": "lhoestq",
        "body": "We are mostly relying on `huggingface_hub` which uses `requests` to stream files from Hugging Face, so maybe we can move aiohttp to optional dependencies now. Would it solve your issue ? Btw what do you think of `datasets` in the free-threading setting ?"
      },
      {
        "user": "Qubitium",
        "body": "> We are mostly relying on `huggingface_hub` which uses `requests` to stream files from Hugging Face, so maybe we can move aiohttp to optional dependencies now. Would it solve your issue ? Btw what do you think of `datasets` in the free-threading setting ?\n\nI am testing transformers + dataset (simple text dataset usage) + GPTQModel for quantization and there were no issues encountered with python 3.13t but my test-case is the base-bare minimal test-case since dataset is not sharded, fully in-memory, text-only, small, not used for training.  \n\nOn the technical side, dataset is almost always 100% read-only so there should be zero locking issues but I have not checked the dataset internals so there may be cases where streaming, sharding, and/or cases where datset memory/states are updated needs a per dataset `threading.lock`. \n\nSo yes, making `aiohttp` optional will definitely solve my issue. There is also a companion (datasets and tokenizers usually go hand-in-hand) issue with `Tokenizers` as well but that's simple enough with package version update: https://github.com/huggingface/tokenizers/pull/1774\n"
      }
    ]
  },
  {
    "issue_number": 7526,
    "title": "Faster downloads/uploads with Xet storage",
    "author": "lhoestq",
    "state": "open",
    "created_at": "2025-04-18T14:46:42Z",
    "updated_at": "2025-05-12T12:09:09Z",
    "labels": [],
    "body": "![Image](https://github.com/user-attachments/assets/6e247f4a-d436-4428-a682-fe18ebdc73a9)\n\n## Xet is out !\n\nOver the past few weeks, Hugging Face’s [Xet Team](https://huggingface.co/xet-team) took a major step forward by [migrating the first Model and Dataset repositories off LFS and to Xet storage](https://huggingface.co/posts/jsulz/911431940353906).\n\nSee more information on the HF blog: https://huggingface.co/blog/xet-on-the-hub\n\nYou can already enable Xet on Hugging Face account to benefit from faster downloads and uploads :)\n\nWe finalized an official integration with the `huggingface_hub` library that means you get the benefits of Xet without any significant changes to your current workflow.\n\n\n## Previous versions of `datasets`\n\nFor older versions of `datasets` you might see this warning in `push_to_hub()`:\n\n```\nUploading files as bytes or binary IO objects is not supported by Xet Storage.\n```\n\nThis means the `huggingface_hub` + Xet integration isn't enabled for your version of `datasets`.\n\nYou can fix this by updating to `datasets>=3.6.0` and `huggingface_hub>=0.31.0`\n\n```\npip install -U datasets huggingface_hub\n```\n\n## The future\n\nStay tuned for more Xet optimizations, especially on [Xet-optimized Parquet](https://huggingface.co/blog/improve_parquet_dedupe)\n\n",
    "comments": []
  },
  {
    "issue_number": 7566,
    "title": "terminate called without an active exception; Aborted (core dumped)",
    "author": "alexey-milovidov",
    "state": "open",
    "created_at": "2025-05-11T23:05:54Z",
    "updated_at": "2025-05-11T23:05:54Z",
    "labels": [],
    "body": "### Describe the bug\n\nI use it as in the tutorial here: https://huggingface.co/docs/datasets/stream, and it ends up with abort.\n\n### Steps to reproduce the bug\n\n1. `pip install datasets`\n2. \n```\n$ cat main.py \n#!/usr/bin/env python3\n\nfrom datasets import load_dataset\n\ndataset = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)\nprint(next(iter(dataset)))\n```\n3. `chmod +x main.py`\n```\n$ ./main.py \nREADME.md: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43.1k/43.1k [00:00<00:00, 7.04MB/s]\nResolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25868/25868 [00:05<00:00, 4859.26it/s]\nResolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25868/25868 [00:00<00:00, 54773.56it/s]\n{'text': \"How AP reported in all formats from tornado-stricken regionsMarch 8, 2012\\nWhen the first serious bout of tornadoes of 2012 blew through middle America in the middle of the night, they touched down in places hours from any AP bureau. Our closest video journalist was Chicago-based Robert Ray, who dropped his plans to travel to Georgia for Super Tuesday, booked several flights to the cities closest to the strikes and headed for the airport. He’d decide once there which flight to take.\\nHe never got on board a plane. Instead, he ended up driving toward Harrisburg, Ill., where initial reports suggested a town was destroyed. That decision turned out to be a lucky break for the AP. Twice.\\nRay was among the first journalists to arrive and he confirmed those reports -- in all formats. He shot powerful video, put victims on the phone with AP Radio and played back sound to an editor who transcribed the interviews and put the material on text wires. He then walked around the devastation with the Central Regional Desk on the line, talking to victims with the phone held so close that editors could transcribe his interviews in real time.\\nRay also made a dramatic image of a young girl who found a man’s prosthetic leg in the rubble, propped it up next to her destroyed home and spray-painted an impromptu sign: “Found leg. Seriously.”\\nThe following day, he was back on the road and headed for Georgia and a Super Tuesday date with Newt Gingrich’s campaign. The drive would take him through a stretch of the South that forecasters expected would suffer another wave of tornadoes.\\nTo prevent running into THAT storm, Ray used his iPhone to monitor Doppler radar, zooming in on extreme cells and using Google maps to direct himself to safe routes. And then the journalist took over again.\\n“When weather like that occurs, a reporter must seize the opportunity to get the news out and allow people to see, hear and read the power of nature so that they can take proper shelter,” Ray says.\\nSo Ray now started to use his phone to follow the storms. He attached a small GoPro camera to his steering wheel in case a tornado dropped down in front of the car somewhere, and took video of heavy rain and hail with his iPhone. Soon, he spotted a tornado and the chase was on. He followed an unmarked emergency vehicle to Cleveland, Tenn., where he was first on the scene of the storm's aftermath.\\nAgain, the tornadoes had struck in locations that were hours from the nearest AP bureau. Damage and debris, as well as a wickedly violent storm that made travel dangerous, slowed our efforts to get to the news. That wasn’t a problem in Tennessee, where our customers were well served by an all-formats report that included this text story.\\n“CLEVELAND, Tenn. (AP) _ Fierce wind, hail and rain lashed Tennessee for the second time in three days, and at least 15 people were hospitalized Friday in the Chattanooga area.”\\nThe byline? Robert Ray.\\nFor being adept with technology, chasing after news as it literally dropped from the sky and setting a standard for all-formats reporting that put the AP ahead on the most competitive news story of the day, Ray wins this week’s $300 Best of the States prize.\\n© 2013 The Associated Press. All rights reserved. Terms and conditions apply. See AP.org for details.\", 'id': '<urn:uuid:d66bc6fe-8477-4adf-b430-f6a558ccc8ff>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://%20jwashington@ap.org/Content/Press-Release/2012/How-AP-reported-in-all-formats-from-tornado-stricken-regions', 'date': '2013-05-18T05:48:54Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.9721424579620361, 'token_count': 717}\nterminate called without an active exception\nAborted (core dumped)\n```\n\n### Expected behavior\n\nI'm not a proficient Python user, so it might be my own error, but even in that case, the error message should be better.\n\n### Environment info\n\n`Successfully installed datasets-3.6.0 dill-0.3.8 hf-xet-1.1.0 huggingface-hub-0.31.1 multiprocess-0.70.16 requests-2.32.3 xxhash-3.5.0`\n\n```\n$ cat /etc/lsb-release \nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=22.04\nDISTRIB_CODENAME=jammy\nDISTRIB_DESCRIPTION=\"Ubuntu 22.04.4 LTS\"\n```",
    "comments": []
  },
  {
    "issue_number": 7470,
    "title": "Is it possible to shard a single-sharded IterableDataset?",
    "author": "jonathanasdf",
    "state": "closed",
    "created_at": "2025-03-21T04:33:37Z",
    "updated_at": "2025-05-09T22:51:46Z",
    "labels": [],
    "body": "I thought https://github.com/huggingface/datasets/pull/7252 might be applicable but looking at it maybe not.\n\nSay we have a process, eg. a database query, that can return data in slightly different order each time. So, the initial query needs to be run by a single thread (not to mention running multiple times incurs more cost too). But the results are also big enough that we don't want to materialize it entirely and instead stream it with an IterableDataset.\n\nBut after we have the results we want to split it up across workers to parallelize processing.\n\nIs something like this possible to do?\n\nHere's a failed attempt. The end result should be that each of the shards has unique data, but unfortunately with this attempt the generator gets run once in each shard and the results end up with duplicates...\n\n```\nimport random\nimport datasets\n\n\ndef gen():\n  print('RUNNING GENERATOR!')\n  items = list(range(10))\n  random.shuffle(items)\n  yield from items\n\n\nds = datasets.IterableDataset.from_generator(gen)\n\nprint('dataset contents:')\nfor item in ds:\n  print(item)\nprint()\n\nprint('dataset contents (2):')\nfor item in ds:\n  print(item)\nprint()\n\n\nnum_shards = 3\n\n\ndef sharded(shard_id):\n  for i, example in enumerate(ds):\n    if i % num_shards in shard_id:\n      yield example\n\n\nds1 = datasets.IterableDataset.from_generator(\n  sharded, gen_kwargs={'shard_id': list(range(num_shards))}\n)\n\nfor shard in range(num_shards):\n  print('shard', shard)\n  for item in ds1.shard(num_shards, shard):\n    print(item)\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Maybe you can look for an option in your dataset to partition your data based on a deterministic filter ? For example each worker could stream the data based on `row.id % num_shards` or something like that ?"
      },
      {
        "user": "jonathanasdf",
        "body": "So the recommendation is to start out with multiple shards initially and re-sharding after is not expected to work? :(\n\nWould something like the following work? Some DiskCachingIterableDataset, where worker 0 streams from the datasource, but also writes to disk, and all of the other workers read from what worker 0 wrote? Then that would produce a stream with a deterministic order and we can subsample."
      },
      {
        "user": "lhoestq",
        "body": "To be honest it would be cool to support native multiprocessing in `IterableDataset.map` so you can parallelize any specific processing step without having to rely on a torch Dataloader. What do you think ?\n\nrelated: https://github.com/huggingface/datasets/issues/7193 https://github.com/huggingface/datasets/issues/3444 \noriginal issue: https://github.com/huggingface/datasets/issues/2642\n\nAlternatively the DiskCachingIterableDataset idea works, just note that to make it work with a torch Dataloader with num_workers>0 you'll need:\n1. to make your own `torch.utils.data.IterableDataset` and have rank=0 stream the data and share them with the other workers (either via disk as suggested or IPC)\n2. take into account that`datasets.IterableDataset` will yield 0 examples for ranks with id>0 if there is only one shard, but in your case it's ok since you'd only stream from rank=0"
      }
    ]
  },
  {
    "issue_number": 7534,
    "title": "TensorFlow RaggedTensor Support (batch-level)",
    "author": "Lundez",
    "state": "open",
    "created_at": "2025-04-24T13:14:52Z",
    "updated_at": "2025-05-08T14:13:47Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nHi,\n\nCurrently datasets does not support RaggedTensor output on batch-level.\nWhen building a Object Detection Dataset (with TensorFlow) I need to enable RaggedTensors as that's how BBoxes & classes are expected from the Keras Model POV.\n\nCurrently there's a error thrown saying that \"Nested Data is not supported\".\n\nIt'd be very helpful if this was fixed! :)\n\n### Motivation\n\nEnabling Object Detection pipelines for TensorFlow.\n\n### Your contribution\n\nWith guidance I'd happily help making the PR.\n\nThe current implementation with DataCollator and later enforcing `np.array` is the problematic part (at the end of `np_get_batch` in `tf_utils.py`). As `numpy` don't support \"Raggednes\"",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Keras doesn't support other inputs other than tf.data.Dataset objects ? it's a bit painful to have to support and maintain this kind of integration\n\nIs there a way to use a `datasets.Dataset` with outputs formatted as tensors / ragged tensors instead ? like in https://huggingface.co/docs/datasets/use_with_tensorflow#dataset-format"
      },
      {
        "user": "Lundez",
        "body": "I'll give it a try when I get the time. But quite sure I already tested the `with_format` approach.\n\nKeras when using TF as backend converts the datasets into `tf.data.Dataset`, much like you do."
      }
    ]
  },
  {
    "issue_number": 7554,
    "title": "datasets downloads and generates all splits, even though a single split is requested (for dataset with loading script)",
    "author": "sei-eschwartz",
    "state": "closed",
    "created_at": "2025-05-06T14:43:38Z",
    "updated_at": "2025-05-07T14:53:45Z",
    "labels": [],
    "body": "### Describe the bug\n\n`datasets` downloads and generates all splits, even though a single split is requested.  [This](https://huggingface.co/datasets/jordiae/exebench) is the dataset in question.  It uses a loading script.  I am not 100% sure that this is a bug, because maybe with loading scripts `datasets` must actually process all the splits?  But I thought loading scripts were designed to avoid this.\n\n### Steps to reproduce the bug\n\nSee [this notebook](https://colab.research.google.com/drive/14kcXp_hgcdj-kIzK0bCG6taE-CLZPVvq?usp=sharing)\n\nOr:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset('jordiae/exebench', split='test_synth', trust_remote_code=True)\n```\n\n### Expected behavior\n\nI expected only the `test_synth` split to be downloaded and processed.\n\n### Environment info\n\n- `datasets` version: 3.5.1\n- Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n- Python version: 3.11.12\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2025.3.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! there has been some effort on allowing to download only a subset of splits in https://github.com/huggingface/datasets/pull/6832 but no one has been continuing this work so far. This would be a welcomed contribution though\n\nAlso note that loading script are often unoptimized, and we recommend using datasets in standard formats like Parquet instead.\n\nBtw there is a CLI tool to convert a loading script to parquet:\n\n```\ndatasets-cli convert_to_parquet <dataset-name> --trust_remote_code\n```"
      },
      {
        "user": "sei-eschwartz",
        "body": "Closing in favor of #6832 "
      }
    ]
  },
  {
    "issue_number": 7517,
    "title": "Image Feature in Datasets Library Fails to Handle bytearray Objects from Spark DataFrames",
    "author": "giraffacarp",
    "state": "closed",
    "created_at": "2025-04-15T11:29:17Z",
    "updated_at": "2025-05-07T14:17:30Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen using `IterableDataset.from_spark()` with a Spark DataFrame containing image data, the `Image` feature class fails to properly process this data type, causing an `AttributeError: 'bytearray' object has no attribute 'get'`\n\n### Steps to reproduce the bug\n\n1. Create a Spark DataFrame with a column containing image data as bytearray objects\n2. Define a Feature schema with an Image feature\n3. Create an IterableDataset using `IterableDataset.from_spark()`\n4. Attempt to iterate through the dataset\n\n```\nfrom pyspark.sql import SparkSession\nfrom datasets import Dataset, IterableDataset, Features, Image, Value\n\n# initialize spark\nspark = SparkSession.builder.appName(\"MinimalRepro\").getOrCreate()\n\n# create spark dataframe\ndata = [(0, open(\"image.png\", \"rb\").read())]\ndf = spark.createDataFrame(data, \"idx: int, image: binary\")\n\n# convert to dataset\nfeatures = Features({\"idx\": Value(\"int64\"), \"image\": Image()})\nds = Dataset.from_spark(df, features=features)\nds_iter = IterableDataset.from_spark(df, features=features)\n\n# iterate\nprint(next(iter(ds)))\nprint(next(iter(ds_iter)))\n```\n\n### Expected behavior\n\nThe features should work on `IterableDataset` the same way they work on `Dataset`\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: macOS-15.3.2-arm64-arm-64bit\n- Python version: 3.12.7\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! The `Image()` type accepts either\n- a `bytes` object containing the image bytes\n- a `str` object containing the image path\n- a `PIL.Image` object\n\nbut it doesn't support `bytearray`, maybe you can convert to `bytes` beforehand ?"
      },
      {
        "user": "giraffacarp",
        "body": "Hi @lhoestq, \nconverting to bytes is certainly possible and would work around the error. However, the core issue is that `Dataset` and `IterableDataset` behave differently with the features.\n\nI’d be happy to work on a fix for this issue."
      },
      {
        "user": "lhoestq",
        "body": "I see, that's an issue indeed. Feel free to ping me if I can help with reviews or any guidance\n\nIf it can help, the code that takes a Spark DataFrame and iterates on the rows for `IterableDataset` is here: \n\nhttps://github.com/huggingface/datasets/blob/6a96bf313085d7538a999b929a550e14e1d406c9/src/datasets/packaged_modules/spark/spark.py#L49-L53"
      }
    ]
  },
  {
    "issue_number": 7527,
    "title": "Auto-merge option for `convert-to-parquet`",
    "author": "klamike",
    "state": "open",
    "created_at": "2025-04-18T16:03:22Z",
    "updated_at": "2025-05-07T12:47:02Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nAdd a command-line option, e.g. `--auto-merge-pull-request` that enables automatic merging of the commits created by the `convert-to-parquet` tool.\n\n### Motivation\n\nLarge datasets may result in dozens of PRs due to the splitting mechanism. Each of these has to be manually accepted via the website.\n\n### Your contribution\n\nHappy to look into submitting a PR if this is of interest to maintainers.",
    "comments": [
      {
        "user": "klamike",
        "body": "Alternatively, there could be an option to switch from submitting PRs to just committing changes directly to `main`."
      },
      {
        "user": "lhoestq",
        "body": "Why not, I'd be in favor of `--merge-pull-request` to call `HfApi().merge_pull_request()` at the end of the conversion :) feel free to open a PR if you'd like"
      },
      {
        "user": "klamike",
        "body": "#self-assign"
      }
    ]
  },
  {
    "issue_number": 7457,
    "title": "Document the HF_DATASETS_CACHE env variable",
    "author": "LSerranoPEReN",
    "state": "closed",
    "created_at": "2025-03-17T12:24:50Z",
    "updated_at": "2025-05-06T15:54:39Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nHello,\n\nI have a use case where my team is sharing models and dataset in shared directory to avoid duplication.\nI noticed that the [cache documentation for datasets](https://huggingface.co/docs/datasets/main/en/cache) only mention the `HF_HOME` environment variable but never the `HF_DATASETS_CACHE`.\n\nIt should be nice to add `HF_DATASETS_CACHE` to datasets documentation if it's an intended feature.\nIf it's not, I think a depreciation warning would be appreciated.\n\n### Motivation\n\nThis variable is fully working and similar to what `HF_HUB_CACHE` does for models, so it's nice to know that this exists. This seems to be a quick change to implement.\n\n### Your contribution\n\nI could contribute since this is only affecting a small portion of the documentation",
    "comments": [
      {
        "user": "junjun3518",
        "body": "Strongly agree to this, in addition, I am also suffering to change the cache location similar to other issues (since I changed the environmental variables).\nhttps://github.com/huggingface/datasets/issues/6886"
      },
      {
        "user": "lhoestq",
        "body": "`HF_DATASETS_CACHE` should be documented there indeed, feel free to open a PR :) "
      },
      {
        "user": "Harry-Yang0518",
        "body": "Hey, I’d love to work on this issue! Could you assign it to me?"
      }
    ]
  },
  {
    "issue_number": 7475,
    "title": "IterableDataset's state_dict shard_example_idx is always equal to the number of samples in a shard",
    "author": "bruno-hays",
    "state": "closed",
    "created_at": "2025-03-25T13:58:07Z",
    "updated_at": "2025-05-06T14:22:19Z",
    "labels": [],
    "body": "### Describe the bug\n\nI've noticed a strange behaviour with Iterable state_dict: the value of shard_example_idx is always equal to the amount of samples in a shard.\n\n\n### Steps to reproduce the bug\n\nI am reusing the example from the doc\n```python\nfrom datasets import Dataset\n\nds = Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=1)\nstate_dict = None\n# Iterate through the dataset and print examples\nfor idx, example in enumerate(ds):\n    print(example)\n    if idx == 2:\n        state_dict = ds.state_dict()\n        print(\"checkpoint\")\n        break\nprint(state_dict)\n```\nReturns:\n```\n{'a': 0}\n{'a': 1}\ncheckpoint\n{'examples_iterable': {'shard_idx': 0, 'shard_example_idx': 6, 'type': 'ArrowExamplesIterable'}, 'epoch': 0}\n```\n\n\n### Expected behavior\n\nshard_example_idx should be 2 instead of 6\nIf we run with num_shards=2, then shard_example_idx is 3 instead of 2 and so on.\n\n### Environment info\n\n- `datasets` version: 3.4.1\n- Platform: macOS-14.6.1-arm64-arm-64bit\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.29.3\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0",
    "comments": [
      {
        "user": "Harry-Yang0518",
        "body": "Hey, I’d love to work on this issue but I am a beginner, can I work it with you?"
      },
      {
        "user": "bruno-hays",
        "body": "Hello. I'm sorry but I don't have much time to get in the details for now.\nHave you managed to reproduce the issue with the code provided ?\nIf you want to work on it, you can self-assign and ask @lhoestq for directions"
      },
      {
        "user": "Harry-Yang0518",
        "body": "Hi Bruno, I am trying to reproduce it this later in this week and let you know what I found."
      }
    ]
  },
  {
    "issue_number": 7538,
    "title": "`IterableDataset` drops samples when resuming from a checkpoint",
    "author": "mariosasko",
    "state": "closed",
    "created_at": "2025-04-27T19:34:49Z",
    "updated_at": "2025-05-06T14:04:05Z",
    "labels": [
      "bug"
    ],
    "body": "When resuming from a checkpoint, `IterableDataset` will drop samples if `num_shards % world_size == 0` and the underlying example supports `iter_arrow` and needs to be formatted. \n\nIn that case, the `FormattedExamplesIterable` fetches a batch of samples from the child iterable's `iter_arrow` and yields them one by one (after formatting). However, the child increments the `shard_example_idx` counter (in its `iter_arrow`) before returning the batch for the whole batch size, which leads to a portion of samples being skipped if the iteration (of the parent iterable) is stopped mid-batch. \n\nPerhaps one way to avoid this would be by signalling the child iterable which samples (within the chunk) are processed by the parent and which are not, so that it can adjust the `shard_example_idx` counter accordingly. This would also mean the chunk needs to be sliced when resuming, but this is straightforward to implement.\n\nThe following is a minimal reproducer of the bug:\n```python\nfrom datasets import Dataset\nfrom datasets.distributed import split_dataset_by_node\n\nds = Dataset.from_dict({\"n\": list(range(24))})\nds = ds.to_iterable_dataset(num_shards=4)\n\nworld_size = 4\nrank = 0    \nds_rank = split_dataset_by_node(ds, rank, world_size)\n\nit = iter(ds_rank)\nexamples = []\nfor idx, example in enumerate(it):\n    examples.append(example)\n    if idx == 2:\n        state_dict = ds_rank.state_dict()\n        break\n\nds_rank.load_state_dict(state_dict)\nit_resumed = iter(ds_rank)\nexamples_resumed = examples[:]\n\nfor example in it:\n    examples.append(example)\n\nfor example in it_resumed:\n    examples_resumed.append(example)\n\nprint(\"ORIGINAL ITER EXAMPLES:\", examples)\nprint(\"RESUMED ITER EXAMPLES:\", examples_resumed)\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Thanks for reporting ! I fixed the issue using RebatchedArrowExamplesIterable before the formatted iterable"
      }
    ]
  },
  {
    "issue_number": 7531,
    "title": "Deepspeed reward training hangs at end of training with Dataset.from_list",
    "author": "Matt00n",
    "state": "open",
    "created_at": "2025-04-21T17:29:20Z",
    "updated_at": "2025-05-06T13:30:41Z",
    "labels": [],
    "body": "There seems to be a weird interaction between Deepspeed, the Dataset.from_list method and trl's RewardTrainer. On a multi-GPU setup (10 A100s), training always hangs at the very end of training until it times out. The training itself works fine until the end of training and running the same script with Deepspeed on a single GPU works without hangig. The issue persisted across a wide range of Deepspeed configs and training arguments. The issue went away when storing the exact same dataset as a JSON and using `dataset = load_dataset(\"json\", ...)`. Here is my training script:\n\n```python\nimport pickle\nimport os\nimport random\nimport warnings\n\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom trl import RewardConfig, RewardTrainer, ModelConfig\n\n####################################### Reward model #################################################\n\n# Explicitly set arguments\nmodel_name_or_path = \"Qwen/Qwen2.5-1.5B\"\noutput_dir = \"Qwen2-0.5B-Reward-LoRA\"\nper_device_train_batch_size = 2\nnum_train_epochs = 5\ngradient_checkpointing = True\nlearning_rate = 1.0e-4\nlogging_steps = 25\neval_strategy = \"steps\"\neval_steps = 50\nmax_length = 2048\ntorch_dtype = \"auto\"\ntrust_remote_code = False\n\nmodel_args = ModelConfig(\n    model_name_or_path=model_name_or_path,\n    model_revision=None,\n    trust_remote_code=trust_remote_code,\n    torch_dtype=torch_dtype,\n    lora_task_type=\"SEQ_CLS\", # Make sure task type is seq_cls\n)\n\ntraining_args = RewardConfig(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    num_train_epochs=num_train_epochs,\n    gradient_checkpointing=gradient_checkpointing,\n    learning_rate=learning_rate,\n    logging_steps=logging_steps,\n    eval_strategy=eval_strategy,\n    eval_steps=eval_steps,\n    max_length=max_length,\n    gradient_checkpointing_kwargs=dict(use_reentrant=False),\n    center_rewards_coefficient = 0.01,\n    fp16=False,\n    bf16=True,\n    save_strategy=\"no\",\n    dataloader_num_workers=0,\n    # deepspeed=\"./configs/deepspeed_config.json\",\n)\n\n################\n# Model & Tokenizer\n################\n\nmodel_kwargs = dict(\n    revision=model_args.model_revision,\n    use_cache=False if training_args.gradient_checkpointing else True,\n    torch_dtype=model_args.torch_dtype,\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    model_args.model_name_or_path, use_fast=True\n)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_args.model_name_or_path, num_labels=1, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n)\n# Align padding tokens between tokenizer and model\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# If post-training a base model, use ChatML as the default template\nif tokenizer.chat_template is None:\n    model, tokenizer = setup_chat_format(model, tokenizer)\n\nif model_args.use_peft and model_args.lora_task_type != \"SEQ_CLS\":\n    warnings.warn(\n        \"You are using a `task_type` that is different than `SEQ_CLS` for PEFT. This will lead to silent bugs\"\n        \" Make sure to pass --lora_task_type SEQ_CLS when using this script with PEFT.\",\n        UserWarning,\n    )\n\n##############\n# Load dataset\n##############\n\n\nwith open('./prefs.pkl', 'rb') as fh:\n    loaded_data = pickle.load(fh)\n\nrandom.shuffle(loaded_data)\n\ndataset = []\nfor a_wins, a, b in loaded_data:\n    if a_wins == 0:\n        a, b = b, a\n    dataset.append({'chosen': a, 'rejected': b})\n\n\ndataset = Dataset.from_list(dataset)\n\n# Split the dataset into training and evaluation sets\ntrain_eval_split = dataset.train_test_split(test_size=0.15, shuffle=True, seed=42)\n\n# Access the training and evaluation datasets\ntrain_dataset = train_eval_split['train']\neval_dataset = train_eval_split['test']\n\n\n##########\n# Training\n##########\n\n\ntrainer = RewardTrainer(\n    model=model,\n    processing_class=tokenizer,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\ntrainer.train()\n```\n\nReplacing `dataset = Dataset.from_list(dataset)` with \n```python\nwith open('./prefs.json', 'w') as fh:\n    json.dump(dataset, fh)\ndataset = load_dataset(\"json\", data_files=\"./prefs.json\", split='train')\n```\nresolves the issue.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! How big is the dataset ? if you load it using `from_list`, the dataset lives in memory and has to be copied to every gpu process, which can be slow.\n\nIt's fasted if you load it from JSON files from disk, because in that case the dataset in converted to Arrow and loaded from disk using memory mapping. Memory mapping allows to quickly reload the dataset in other processes.\n\nMaybe we can change `from_list` and other methods to always use the disk though, instead of loading in memory, WDYT ?"
      }
    ]
  },
  {
    "issue_number": 7528,
    "title": "Data Studio Error: Convert JSONL incorrectly",
    "author": "zxccade",
    "state": "open",
    "created_at": "2025-04-19T13:21:44Z",
    "updated_at": "2025-05-06T13:18:38Z",
    "labels": [],
    "body": "### Describe the bug\n\nHi there,\n\nI uploaded a dataset here https://huggingface.co/datasets/V-STaR-Bench/V-STaR, but I found that Data Studio incorrectly convert the \"bboxes\" value for the whole dataset. Therefore, anyone who downloaded the dataset via the API would get the wrong \"bboxes\" value in the data file.\n\nCould you help me address the issue?\n\nMany thanks,\n\n\n\n### Steps to reproduce the bug\n\nThe JSONL file of [V_STaR_test_release.jsonl](https://huggingface.co/datasets/V-STaR-Bench/V-STaR/blob/main/V_STaR_test_release.jsonl) has the correct values of every \"bboxes\" for each sample.\n\nBut in the Data Studio, we can see that the values of \"bboxes\" have changed, and load the dataset via API will also get the wrong values.\n\n### Expected behavior\n\nFix the bug to correctly download my dataset.\n\n### Environment info\n\n- `datasets` version: 2.16.1\n- Platform: Linux-5.14.0-427.22.1.el9_4.x86_64-x86_64-with-glibc2.34\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.29.3\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2023.10.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Your JSONL file is incompatible with Arrow / Parquet. Indeed in Arrow / Parquet every dict should have the same keys, while in your dataset the bboxes have varying keys.\n\nThis causes the Data Studio to treat the bboxes as if each row was missing the keys from other rows.\n\nFeel free to take a look at the docs on object segmentation to see how to format a dataset with bboxes: https://huggingface.co/docs/datasets/object_detection"
      }
    ]
  },
  {
    "issue_number": 7537,
    "title": "`datasets.map(..., num_proc=4)` multi-processing fails",
    "author": "faaany",
    "state": "open",
    "created_at": "2025-04-25T01:53:47Z",
    "updated_at": "2025-05-06T13:12:08Z",
    "labels": [],
    "body": "The following code fails in python 3.11+ \n\n```python\ntokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])  \n```\nError log:\n```bash\nTraceback (most recent call last):\nFile \"/usr/local/lib/python3.12/dist-packages/multiprocess/process.py\", line 315, in _bootstrap\nself.run()\nFile \"/usr/local/lib/python3.12/dist-packages/multiprocess/process.py\", line 108, in run\nself._target(*self._args, **self._kwargs)\nFile \"/usr/local/lib/python3.12/dist-packages/multiprocess/pool.py\", line 114, in worker\ntask = get()\n^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/multiprocess/queues.py\", line 371, in get\nreturn _ForkingPickler.loads(res)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/dill/_dill.py\", line 327, in loads\nreturn load(file, ignore, **kwds)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/dill/_dill.py\", line 313, in load\nreturn Unpickler(file, ignore=ignore, **kwds).load()\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/dill/_dill.py\", line 525, in load\nobj = StockUnpickler.load(self)\n^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/dill/_dill.py\", line 659, in _create_code\nif len(args) == 16: return CodeType(*args)\n^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n```\n\nAfter upgrading dill to the latest 0.4.0 with \"pip install --upgrade dill\", it can pass. So it seems that there is a compatibility issue between dill 0.3.4 and python 3.11+, because python 3.10 works fine. \n\n\nIs the dill deterministic issue mentioned in https://github.com/huggingface/datasets/blob/main/setup.py#L117) still valid? Any plan to unpin? \n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "related: https://github.com/huggingface/datasets/issues/7510\n\nwe need to do more tests to see if latest `dill` is deterministic"
      }
    ]
  },
  {
    "issue_number": 7536,
    "title": "[Errno 13] Permission denied: on `.incomplete` file",
    "author": "ryan-clancy",
    "state": "closed",
    "created_at": "2025-04-24T20:52:45Z",
    "updated_at": "2025-05-06T13:05:01Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen downloading a dataset, we frequently hit the below Permission Denied error. This looks to happen (at least) across datasets in HF, S3, and GCS.\n\nIt looks like the `temp_file` being passed [here](https://github.com/huggingface/datasets/blob/main/src/datasets/utils/file_utils.py#L412) can sometimes be created with `000` permissions leading to the permission denied error (the user running the code is still the owner of the file). Deleting that particular file and re-running the code with 0 changes will usually succeed.\n\nIs there some race condition happening with the [umask](https://github.com/huggingface/datasets/blob/main/src/datasets/utils/file_utils.py#L416), which is process global, and the [file creation](https://github.com/huggingface/datasets/blob/main/src/datasets/utils/file_utils.py#L404)?\n\n```\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.12/site-packages/datasets/load.py:2084: in load_dataset\n    builder_instance.download_and_prepare(\n.venv/lib/python3.12/site-packages/datasets/builder.py:925: in download_and_prepare\n    self._download_and_prepare(\n.venv/lib/python3.12/site-packages/datasets/builder.py:1649: in _download_and_prepare\n    super()._download_and_prepare(\n.venv/lib/python3.12/site-packages/datasets/builder.py:979: in _download_and_prepare\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\n.venv/lib/python3.12/site-packages/datasets/packaged_modules/folder_based_builder/folder_based_builder.py:120: in _split_generators\n    downloaded_files = dl_manager.download(files)\n.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:159: in download\n    downloaded_path_or_paths = map_nested(\n.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:514: in map_nested\n    _single_map_nested((function, obj, batched, batch_size, types, None, True, None))\n.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:382: in _single_map_nested\n    return [mapped_item for batch in iter_batched(data_struct, batch_size) for mapped_item in function(batch)]\n.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:206: in _download_batched\n    return thread_map(\n.venv/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:69: in thread_map\n    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n.venv/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:51: in _executor_map\n    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n.venv/lib/python3.12/site-packages/tqdm/std.py:1181: in __iter__\n    for obj in iterable:\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/_base.py:619: in result_iterator\n    yield _result_or_cancel(fs.pop())\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/_base.py:317: in _result_or_cancel\n    return fut.result(timeout)\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/_base.py:449: in result\n    return self.__get_result()\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/_base.py:401: in __get_result\n    raise self._exception\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/thread.py:59: in run\n    result = self.fn(*self.args, **self.kwargs)\n.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:229: in _download_single\n    out = cached_path(url_or_filename, download_config=download_config)\n.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py:206: in cached_path\n    output_path = get_from_cache(\n.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py:412: in get_from_cache\n    fsspec_get(url, temp_file, storage_options=storage_options, desc=download_desc, disable_tqdm=disable_tqdm)\n.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py:331: in fsspec_get\n    fs.get_file(path, temp_file.name, callback=callback)\n.venv/lib/python3.12/site-packages/fsspec/asyn.py:118: in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n.venv/lib/python3.12/site-packages/fsspec/asyn.py:103: in sync\n    raise return_result\n.venv/lib/python3.12/site-packages/fsspec/asyn.py:56: in _runner\n    result[0] = await coro\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <s3fs.core.S3FileSystem object at 0x7f27c18b2e70>\nrpath = '<my-bucket>/<my-prefix>/img_1.jpg'\nlpath = '/home/runner/_work/_temp/hf_cache/downloads/6c97983efa4e24e534557724655df8247a0bd04326cdfc4a95b638c11e78222d.incomplete'\ncallback = <datasets.utils.file_utils.TqdmCallback object at 0x7f27c00cdbe0>\nversion_id = None, kwargs = {}\n_open_file = <function S3FileSystem._get_file.<locals>._open_file at 0x7f27628d1120>\nbody = <StreamingBody at 0x7f276344fa80 for ClientResponse at 0x7f27c015fce0>\ncontent_length = 521923, failed_reads = 0, bytes_read = 0\n\n    async def _get_file(\n        self, rpath, lpath, callback=_DEFAULT_CALLBACK, version_id=None, **kwargs\n    ):\n        if os.path.isdir(lpath):\n            return\n        bucket, key, vers = self.split_path(rpath)\n    \n        async def _open_file(range: int):\n            kw = self.req_kw.copy()\n            if range:\n                kw[\"Range\"] = f\"bytes={range}-\"\n            resp = await self._call_s3(\n                \"get_object\",\n                Bucket=bucket,\n                Key=key,\n                **version_id_kw(version_id or vers),\n                **kw,\n            )\n            return resp[\"Body\"], resp.get(\"ContentLength\", None)\n    \n        body, content_length = await _open_file(range=0)\n        callback.set_size(content_length)\n    \n        failed_reads = 0\n        bytes_read = 0\n    \n        try:\n>           with open(lpath, \"wb\") as f0:\nE           PermissionError: [Errno 13] Permission denied: '/home/runner/_work/_temp/hf_cache/downloads/6c97983efa4e24e534557724655df8247a0bd04326cdfc4a95b638c11e78222d.incomplete'\n\n.venv/lib/python3.12/site-packages/s3fs/core.py:1355: PermissionError\n```\n\n### Steps to reproduce the bug\n\nI believe this is a race condition and cannot reliably re-produce it, but it happens fairly frequently in our GitHub Actions tests and can also be re-produced (with lesser frequency) on cloud VMs.\n\n### Expected behavior\n\nThe dataset loads properly with no permission denied error.\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Linux-5.10.0-34-cloud-amd64-x86_64-with-glibc2.31\n- Python version: 3.12.10\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "It must be an issue with umask being used by multiple threads indeed. Maybe we can try to make a thread safe function to apply the umask (using filelock for example)"
      },
      {
        "user": "ryan-clancy",
        "body": "> It must be an issue with umask being used by multiple threads indeed. Maybe we can try to make a thread safe function to apply the umask (using filelock for example)\n\n@lhoestq is this something which can go in a 3.5.1 release?"
      },
      {
        "user": "lhoestq",
        "body": "Yes for sure"
      }
    ]
  },
  {
    "issue_number": 6436,
    "title": "TypeError: <lambda>() takes 0 positional arguments but 1 was given",
    "author": "ahmadmustafaanis",
    "state": "closed",
    "created_at": "2023-11-19T13:10:20Z",
    "updated_at": "2025-05-05T18:21:21Z",
    "labels": [],
    "body": "### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-35-7b6becee3685>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 from datasets import Dataset\r\n\r\n9 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/__init__.py](https://localhost:8080/#) in <module>\r\n     20 __version__ = \"2.15.0\"\r\n     21 \r\n---> 22 from .arrow_dataset import Dataset\r\n     23 from .arrow_reader import ReadInstruction\r\n     24 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in <module>\r\n     61 import pyarrow.compute as pc\r\n     62 from huggingface_hub import CommitOperationAdd, CommitOperationDelete, DatasetCard, DatasetCardData, HfApi\r\n---> 63 from multiprocess import Pool\r\n     64 from requests import HTTPError\r\n     65 \r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/__init__.py](https://localhost:8080/#) in <module>\r\n     31 \r\n     32 import sys\r\n---> 33 from . import context\r\n     34 \r\n     35 #\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/context.py](https://localhost:8080/#) in <module>\r\n      4 \r\n      5 from . import process\r\n----> 6 from . import reduction\r\n      7 \r\n      8 __all__ = ()\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/reduction.py](https://localhost:8080/#) in <module>\r\n     14 import os\r\n     15 try:\r\n---> 16     import dill as pickle\r\n     17 except ImportError:\r\n     18     import pickle\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/__init__.py](https://localhost:8080/#) in <module>\r\n     24 \r\n     25 \r\n---> 26 from ._dill import (\r\n     27     dump, dumps, load, loads, copy,\r\n     28     Pickler, Unpickler, register, pickle, pickles, check,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/_dill.py](https://localhost:8080/#) in <module>\r\n    166 try:\r\n    167     from _pyio import open as _open\r\n--> 168     PyTextWrapperType = get_file_type('r', buffering=-1, open=_open)\r\n    169     PyBufferedRandomType = get_file_type('r+b', buffering=-1, open=_open)\r\n    170     PyBufferedReaderType = get_file_type('rb', buffering=-1, open=_open)\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/_dill.py](https://localhost:8080/#) in get_file_type(*args, **kwargs)\r\n    154 def get_file_type(*args, **kwargs):\r\n    155     open = kwargs.pop(\"open\", __builtin__.open)\r\n--> 156     f = open(os.devnull, *args, **kwargs)\r\n    157     t = type(f)\r\n    158     f.close()\r\n\r\n[/usr/lib/python3.10/_pyio.py](https://localhost:8080/#) in open(file, mode, buffering, encoding, errors, newline, closefd, opener)\r\n    280             return result\r\n    281         encoding = text_encoding(encoding)\r\n--> 282         text = TextIOWrapper(buffer, encoding, errors, newline, line_buffering)\r\n    283         result = text\r\n    284         text.mode = mode\r\n\r\n[/usr/lib/python3.10/_pyio.py](https://localhost:8080/#) in __init__(self, buffer, encoding, errors, newline, line_buffering, write_through)\r\n   2043                 encoding = \"utf-8\"\r\n   2044             else:\r\n-> 2045                 encoding = locale.getpreferredencoding(False)\r\n   2046 \r\n   2047         if not isinstance(encoding, str):\r\n\r\nTypeError: <lambda>() takes 0 positional arguments but 1 was given\r\n```\r\n\r\nor \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-36-652e886d387f>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 import datasets\r\n\r\n9 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/__init__.py](https://localhost:8080/#) in <module>\r\n     20 __version__ = \"2.15.0\"\r\n     21 \r\n---> 22 from .arrow_dataset import Dataset\r\n     23 from .arrow_reader import ReadInstruction\r\n     24 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in <module>\r\n     61 import pyarrow.compute as pc\r\n     62 from huggingface_hub import CommitOperationAdd, CommitOperationDelete, DatasetCard, DatasetCardData, HfApi\r\n---> 63 from multiprocess import Pool\r\n     64 from requests import HTTPError\r\n     65 \r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/__init__.py](https://localhost:8080/#) in <module>\r\n     31 \r\n     32 import sys\r\n---> 33 from . import context\r\n     34 \r\n     35 #\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/context.py](https://localhost:8080/#) in <module>\r\n      4 \r\n      5 from . import process\r\n----> 6 from . import reduction\r\n      7 \r\n      8 __all__ = ()\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/reduction.py](https://localhost:8080/#) in <module>\r\n     14 import os\r\n     15 try:\r\n---> 16     import dill as pickle\r\n     17 except ImportError:\r\n     18     import pickle\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/__init__.py](https://localhost:8080/#) in <module>\r\n     24 \r\n     25 \r\n---> 26 from ._dill import (\r\n     27     dump, dumps, load, loads, copy,\r\n     28     Pickler, Unpickler, register, pickle, pickles, check,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/_dill.py](https://localhost:8080/#) in <module>\r\n    166 try:\r\n    167     from _pyio import open as _open\r\n--> 168     PyTextWrapperType = get_file_type('r', buffering=-1, open=_open)\r\n    169     PyBufferedRandomType = get_file_type('r+b', buffering=-1, open=_open)\r\n    170     PyBufferedReaderType = get_file_type('rb', buffering=-1, open=_open)\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/_dill.py](https://localhost:8080/#) in get_file_type(*args, **kwargs)\r\n    154 def get_file_type(*args, **kwargs):\r\n    155     open = kwargs.pop(\"open\", __builtin__.open)\r\n--> 156     f = open(os.devnull, *args, **kwargs)\r\n    157     t = type(f)\r\n    158     f.close()\r\n\r\n[/usr/lib/python3.10/_pyio.py](https://localhost:8080/#) in open(file, mode, buffering, encoding, errors, newline, closefd, opener)\r\n    280             return result\r\n    281         encoding = text_encoding(encoding)\r\n--> 282         text = TextIOWrapper(buffer, encoding, errors, newline, line_buffering)\r\n    283         result = text\r\n    284         text.mode = mode\r\n\r\n[/usr/lib/python3.10/_pyio.py](https://localhost:8080/#) in __init__(self, buffer, encoding, errors, newline, line_buffering, write_through)\r\n   2043                 encoding = \"utf-8\"\r\n   2044             else:\r\n-> 2045                 encoding = locale.getpreferredencoding(False)\r\n   2046 \r\n   2047         if not isinstance(encoding, str):\r\n\r\nTypeError: <lambda>() takes 0 positional arguments but 1 was given\r\n```\n\n### Steps to reproduce the bug\n\n`import datasets` on colab\n\n### Expected behavior\n\nwork fine\n\n### Environment info\n\ncolab\r\n\r\n`!pip install datasets`",
    "comments": [
      {
        "user": "mariosasko",
        "body": "This looks like a problem with your environment rather than `datasets`."
      },
      {
        "user": "Shuntw6096",
        "body": "I meet the same problem,\r\nand originally use\r\n```python\r\nlocale.getpreferredencoding = lambda : \"UTF-8\"\r\n```\r\nand change to\r\n```\r\nlocale.getpreferredencoding = lambda x: \"UTF-8\"\r\n```\r\nand it works."
      },
      {
        "user": "Pu4",
        "body": "> I meet the same problem, and originally use\n> \n> locale.getpreferredencoding = lambda : \"UTF-8\"\n> \n> and change to\n> \n> ```\n> locale.getpreferredencoding = lambda x: \"UTF-8\"\n> ```\n> \n> and it works.\n\nThanks, works for me too."
      }
    ]
  },
  {
    "issue_number": 6532,
    "title": "[Feature request] Indexing datasets by a customly-defined id field to enable random access dataset items via the id",
    "author": "Yu-Shi",
    "state": "open",
    "created_at": "2023-12-25T11:37:10Z",
    "updated_at": "2025-05-05T13:25:24Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\r\n\r\nSome datasets may contain an id-like field, for example the `id` field in [wikimedia/wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia) and the `_id` field in [BeIR/dbpedia-entity](https://huggingface.co/datasets/BeIR/dbpedia-entity). HF datasets support efficient random access via row, but not via this kinds of id fields. I wonder if it is possible to add support for indexing by a custom \"id-like\" field to enable random access via such ids. The ids may be numbers or strings.\r\n\r\n### Motivation\r\n\r\nIn some cases, especially during inference/evaluation, I may want to find out the item that has a specified id, defined by the dataset itself.\r\n\r\nFor example, in a typical re-ranking setting in information retrieval, the user may want to re-rank the set of candidate documents of each query. The input is usually presented in a TREC-style run file, with the following format:\r\n\r\n```\r\n<qid> Q0 <docno> <rank> <score> <tag>\r\n```\r\n\r\nThe re-ranking program should be able to fetch the queries and documents according to the `<qid>` and `<docno>`, which are the original id defined in the query/document datasets. To accomplish this, I have to iterate over the whole HF dataset to get the mapping from real ids to row ids every time I start the program, which is time-consuming. Thus I want HF dataset to provide options for users to index by a custom id column, not by row.\r\n\r\n### Your contribution\r\n\r\nI'm not an expert in this project and I'm afraid that I'm not able to make contributions on the code.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "You can simply use a python dict as index:\r\n\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> ds = load_dataset(\"BeIR/dbpedia-entity\", \"corpus\", split=\"corpus\")\r\n>>> index = {key: idx for idx, key in enumerate(ds[\"_id\"])}\r\n>>> ds[index[\"<dbpedia:Pikachu>\"]]\r\n{'_id': '<dbpedia:Pikachu>',\r\n 'title': 'Pikachu',\r\n 'text': 'Pikachu (Japanese: ピカチュウ) are a fictional species of Pokémon.  Pokémon are fictional creatures that appear in an assortment of comic books, animated movies and television shows, video games, and trading card games licensed by The Pokémon Company, a Japanese corporation. The Pikachu design was conceived by Ken Sugimori.'}\r\n```"
      },
      {
        "user": "Yu-Shi",
        "body": "Thanks for your reply. Yes, I can do that, but it is time-consuming to do that every time I launch the program (some datasets are extremely big). HF Datasets has a nice feature to support instant data loading and efficient random access via row ids. I'm curious if this beneficial feature could be further extended to custom data columns.\r\n"
      },
      {
        "user": "davidmrau",
        "body": "+1 on the issue I think it would be extremely useful"
      }
    ]
  },
  {
    "issue_number": 4760,
    "title": "Issue with offline mode",
    "author": "SaulLu",
    "state": "closed",
    "created_at": "2022-07-28T12:45:14Z",
    "updated_at": "2025-05-04T16:44:59Z",
    "labels": [
      "bug"
    ],
    "body": "## Describe the bug\r\nI can't retrieve a cached dataset with offline mode enabled\r\n\r\n## Steps to reproduce the bug\r\n\r\nTo reproduce my issue, first, you'll need to run a script that will cache the dataset\r\n```python\r\nimport os\r\nos.environ[\"HF_DATASETS_OFFLINE\"] = \"0\"\r\n\r\nimport datasets\r\n\r\ndatasets.logging.set_verbosity_info()\r\nds_name = \"SaulLu/toy_struc_dataset\"\r\nds = datasets.load_dataset(ds_name)\r\nprint(ds)\r\n```\r\nthen, you can try to reload it in offline mode:\r\n```python\r\nimport os\r\nos.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\r\n\r\nimport datasets\r\n\r\ndatasets.logging.set_verbosity_info()\r\nds_name = \"SaulLu/toy_struc_dataset\"\r\nds = datasets.load_dataset(ds_name)\r\nprint(ds)\r\n```\r\n\r\n## Expected results\r\nI would have expected the 2nd snippet not to return any errors\r\n\r\n## Actual results\r\nThe 2nd snippet returns:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/lucile_huggingface_co/sandbox/evaluate/test_cache_datasets.py\", line 8, in <module>\r\n    ds = datasets.load_dataset(ds_name)\r\n  File \"/home/lucile_huggingface_co/anaconda3/envs/evaluate-dev/lib/python3.8/site-packages/datasets/load.py\", line 1723, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/lucile_huggingface_co/anaconda3/envs/evaluate-dev/lib/python3.8/site-packages/datasets/load.py\", line 1500, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/lucile_huggingface_co/anaconda3/envs/evaluate-dev/lib/python3.8/site-packages/datasets/load.py\", line 1241, in dataset_module_factory\r\n    raise ConnectionError(f\"Couln't reach the Hugging Face Hub for dataset '{path}': {e1}\") from None\r\nConnectionError: Couln't reach the Hugging Face Hub for dataset 'SaulLu/toy_struc_dataset': Offline mode is enabled.\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-4.19.0-21-cloud-amd64-x86_64-with-glibc2.17\r\n- Python version: 3.8.13\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3\r\n\r\nMaybe I'm misunderstanding something in the use of the offline mode (see [doc](https://huggingface.co/docs/datasets/v2.4.0/en/loading#offline)), is that the case?\r\n",
    "comments": [
      {
        "user": "albertvillanova",
        "body": "Hi @SaulLu, thanks for reporting.\r\n\r\nI think offline mode is not supported for datasets containing only data files (without any loading script). I'm having a look into this..."
      },
      {
        "user": "SaulLu",
        "body": "Thanks for your feedback! \r\n\r\nTo give you a little more info, if you don't set the offline mode flag, the script will load the cache. I first noticed this behavior with the `evaluate` library, and while trying to understand the downloading flow I realized that I had a similar error with datasets."
      },
      {
        "user": "albertvillanova",
        "body": "This is an issue we have to fix."
      }
    ]
  },
  {
    "issue_number": 7549,
    "title": "TypeError: Couldn't cast array of type string to null on webdataset format dataset",
    "author": "narugo1992",
    "state": "open",
    "created_at": "2025-05-02T15:18:07Z",
    "updated_at": "2025-05-02T15:37:05Z",
    "labels": [],
    "body": "### Describe the bug\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"animetimm/danbooru-wdtagger-v4-w640-ws-30k\")\n```\n\ngot\n\n```\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 626, in write_batch\n    arrays.append(pa.array(typed_sequence))\n  File \"pyarrow/array.pxi\", line 255, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\", line 117, in pyarrow.lib._handle_arrow_array_protocol\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 258, in __arrow_array__\n    out = cast_array_to_feature(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1798, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 2006, in cast_array_to_feature\n    arrays = [\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 2007, in <listcomp>\n    _c(array.field(name) if name in array_fields else null_array, subfeature)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1798, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 2066, in cast_array_to_feature\n    casted_array_values = _c(array.values, feature.feature)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1798, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 2103, in cast_array_to_feature\n    return array_cast(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1798, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1949, in array_cast\n    raise TypeError(f\"Couldn't cast array of type {_short_str(array.type)} to {_short_str(pa_type)}\")\nTypeError: Couldn't cast array of type string to null\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/load.py\", line 2084, in load_dataset\n    builder_instance.download_and_prepare(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 925, in download_and_prepare\n    self._download_and_prepare(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 1649, in _download_and_prepare\n    super()._download_and_prepare(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 1001, in _download_and_prepare\n    self._prepare_split(split_generator, **prepare_split_kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 1487, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 1644, in _prepare_split_single\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n```\n\n`datasets==3.5.1` whats wrong\n\nits inner json structure is like\n\n```yaml\n  features:\n    - name: \"image\"\n      dtype: \"image\"\n    - name: \"json.id\"\n      dtype: \"string\"\n    - name: \"json.width\"\n      dtype: \"int32\"\n    - name: \"json.height\"\n      dtype: \"int32\"\n    - name: \"json.rating\"\n      sequence:\n        dtype: \"string\"\n    - name: \"json.general_tags\"\n      sequence:\n        dtype: \"string\"\n    - name: \"json.character_tags\"\n      sequence:\n        dtype: \"string\"\n```\n\ni'm 100% sure all the jsons satisfies the abovementioned format.\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"animetimm/danbooru-wdtagger-v4-w640-ws-30k\")\n```\n\n### Expected behavior\n\nload the dataset successfully, with the abovementioned json format and webp images\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\n\n- `datasets` version: 3.5.1\n- Platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 20.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2025.3.0\n",
    "comments": [
      {
        "user": "narugo1992",
        "body": "seems to get fixed by explicitly adding `dataset_infos.json` like this\n\n```json\n{\n  \"default\": {\n    \"description\": \"Image dataset with tags and ratings\",\n    \"citation\": \"\",\n    \"homepage\": \"\",\n    \"license\": \"\",\n    \"features\": {\n      \"image\": {\n        \"dtype\": \"image\",\n        \"_type\": \"Image\"\n      },\n      \"json\": {\n        \"id\": {\n          \"dtype\": \"int32\",\n          \"_type\": \"Value\"\n        },\n        \"width\": {\n          \"dtype\": \"int32\",\n          \"_type\": \"Value\"\n        },\n        \"height\": {\n          \"dtype\": \"int32\",\n          \"_type\": \"Value\"\n        },\n        \"rating\": {\n          \"feature\": {\n            \"dtype\": \"string\",\n            \"_type\": \"Value\"\n          },\n          \"_type\": \"Sequence\"\n        },\n        \"general_tags\": {\n          \"feature\": {\n            \"dtype\": \"string\",\n            \"_type\": \"Value\"\n          },\n          \"_type\": \"Sequence\"\n        },\n        \"character_tags\": {\n          \"feature\": {\n            \"dtype\": \"string\",\n            \"_type\": \"Value\"\n          },\n          \"_type\": \"Sequence\"\n        }\n      }\n    },\n    \"builder_name\": \"webdataset\",\n    \"config_name\": \"default\",\n    \"version\": {\n      \"version_str\": \"1.0.0\",\n      \"description\": null,\n      \"major\": 1,\n      \"minor\": 0,\n      \"patch\": 0\n    }\n  }\n}\n\n```\n\nwill close this issue if no further issues found"
      }
    ]
  },
  {
    "issue_number": 7545,
    "title": "Networked Pull Through Cache",
    "author": "wrmedford",
    "state": "open",
    "created_at": "2025-04-30T15:16:33Z",
    "updated_at": "2025-04-30T15:16:33Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nIntroduce a HF_DATASET_CACHE_NETWORK_LOCATION configuration (e.g. an environment variable) together with a companion network cache service.\n\nEnable a three-tier cache lookup for datasets:\n\n1. Local on-disk cache\n2. Configurable network cache proxy\n3. Official Hugging Face Hub\n\n### Motivation\n\n- Distributed training & ephemeral jobs: In high-performance or containerized clusters, relying solely on a local disk cache either becomes a streaming bottleneck or incurs a heavy cold-start penalty as each job must re-download datasets.\n- Traffic & cost reduction: A pull-through network cache lets multiple consumers share a common cache layer, reducing duplicate downloads from the Hub and lowering egress costs.\n- Better streaming adoption: By offloading repeat dataset pulls to a locally managed cache proxy, streaming workloads can achieve higher throughput and more predictable latency.\n- Proven pattern: Similar proxy-cache solutions (e.g. Harbor’s Proxy Cache for Docker images) have demonstrated reliability and performance at scale: https://goharbor.io/docs/2.1.0/administration/configure-proxy-cache/\n\n### Your contribution\n\nI’m happy to draft the initial PR for adding HF_DATASET_CACHE_NETWORK_LOCATION support in datasets and sketch out a minimal cache-service prototype.\n\nI have limited bandwidth so I would be looking for collaborators if anyone else is interested. ",
    "comments": []
  },
  {
    "issue_number": 7543,
    "title": "The memory-disk mapping failure issue of the map function（resolved, but there are some suggestions.）",
    "author": "jxma20",
    "state": "closed",
    "created_at": "2025-04-29T03:04:59Z",
    "updated_at": "2025-04-30T02:22:17Z",
    "labels": [],
    "body": "### Describe the bug\n\n## bug\nWhen the map function processes a large dataset, it temporarily stores the data in a cache file on the disk. After the data is stored, the memory occupied by it is released. Therefore, when using the map function to process a large-scale dataset, only a dataset space of the size of `writer_batch_size` will be occupied in memory.\n\nHowever, I found that the map function does not actually reduce memory usage when I used it. At first, I thought there was a bug in the program, causing a memory leak—meaning the memory was not released after the data was stored in the cache. But later, I used a Linux command to check for recently modified files during program execution and found that no new files were created or modified. This indicates that the program did not store the dataset in the disk cache.\n## bug solved\nAfter modifying the parameters of the map function multiple times, I discovered the `cache_file_name` parameter. By changing it, the cache file can be stored in the specified directory. After making this change, I noticed that the cache file appeared. Initially, I found this quite incredible, but then I wondered if the cache file might have failed to be stored in a certain folder. This could be related to the fact that I don't have root privileges.\n\nSo, I delved into the source code of the map function to find out where the cache file would be stored by default. Eventually, I found the function `def _get_cache_file_path(self, fingerprint):`, which automatically generates the storage path for the cache file. The output was as follows: `/tmp/hf_datasets-j5qco9ug/cache-f2830487643b9cc2.arrow`. My hypothesis was confirmed: the lack of root privileges indeed prevented the cache file from being stored, which in turn prevented the release of memory. Therefore, changing the storage location to a folder where I have write access resolved the issue.\n\n### Steps to reproduce the bug\n\nmy code\n`train_data = train_data.map(process_fun, remove_columns=['image_name', 'question_type', 'concern', 'question', 'candidate_answers', 'answer'])`\n\n### Expected behavior\n\nAlthough my bug has been resolved, it still took me nearly a week to search for relevant information and debug the program. However, if a warning or error message about insufficient cache file write permissions could be provided during program execution, I might have been able to identify the cause more quickly. Therefore, I hope this aspect can be improved. I am documenting this bug here so that friends who encounter similar issues can solve their problems in a timely manner.\n\n### Environment info\n\npython: 3.10.15\ndatasets: 3.5.0",
    "comments": []
  },
  {
    "issue_number": 6764,
    "title": "load_dataset can't work with symbolic links",
    "author": "VladimirVincan",
    "state": "open",
    "created_at": "2024-03-29T17:49:28Z",
    "updated_at": "2025-04-29T15:06:28Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\r\n\r\nEnable the `load_dataset` function to load local datasets with symbolic links. \r\n\r\nE.g, this dataset can be loaded:\r\n├── example_dataset/\r\n│   ├── data/\r\n│   │   ├── train/\r\n│   │   │   ├── file0\r\n│   │   │   ├── file1\r\n│   │   ├── dev/\r\n│   │   │   ├── file2\r\n│   │   │   ├── file3\r\n│   ├── metadata.csv\r\n\r\nwhile this dataset can't:\r\n├── example_dataset_symlink/\r\n│   ├── data/\r\n│   │   ├── train/\r\n│   │   │   ├── sym0 -> file0\r\n│   │   │   ├── sym1 -> file1\r\n│   │   ├── dev/\r\n│   │   │   ├── sym2 -> file2\r\n│   │   │   ├── sym3 -> file3\r\n│   ├── metadata.csv\r\n\r\nI have created an example dataset in order to reproduce the problem:\r\n\r\n1. Unzip `example_dataset.zip`.\r\n2. Run `no_symlink.sh`. Training should start without issues. \r\n3. Run `symlink.sh`. You will see that all four examples will be in train split, instead of having two examples in train and two examples in dev. The script won't load the correct audio files.\r\n\r\n[example_dataset.zip](https://github.com/huggingface/datasets/files/14807053/example_dataset.zip)\r\n\r\n### Motivation\r\n\r\nI have a very large dataset locally. Instead of initiating training on the entire dataset, I need to start training on smaller subsets of the data. Due to the purpose of the experiments I am running, I will need to create many smaller datasets with overlapping data. Instead of copying the all the files for each subset, I would prefer copying symbolic links of the data. This way, the memory usage would not significantly increase beyond the initial dataset size.\r\n\r\nAdvantages of this approach:\r\n\r\n- It would leave a smaller memory footprint on the hard drive\r\n- Creating smaller datasets would be much faster\r\n\r\n### Your contribution\r\n\r\nI would gladly contribute, if this is something useful to the community. It seems like a simple change of code, something like `file_path = os.path.realpath(file_path)` should be added before loading the files. If anyone has insights on how to incorporate this functionality, I would greatly appreciate your knowledge and input.",
    "comments": [
      {
        "user": "TangGuohh",
        "body": "In fact,You can use a hard link instead of a symbolic link.Hard link works"
      }
    ]
  },
  {
    "issue_number": 7041,
    "title": "`sort` after `filter` unreasonably  slow",
    "author": "Tobin-rgb",
    "state": "closed",
    "created_at": "2024-07-12T03:29:27Z",
    "updated_at": "2025-04-29T09:49:25Z",
    "labels": [],
    "body": "### Describe the bug\n\nas the tittle says ...\n\n### Steps to reproduce the bug\n\n`sort` seems to be normal.\r\n\r\n```python\r\nfrom datasets import Dataset\r\nimport random\r\n\r\nnums = [{\"k\":random.choice(range(0,1000))} for _ in range(100000)]\r\n\r\nds = Dataset.from_list(nums)\r\n\r\nprint(\"start sort\")\r\n\r\nds = ds.sort(\"k\")\r\n\r\nprint(\"finish sort\")\r\n```\r\n\r\nbut `sort` after `filter` is extremely slow.\r\n\r\n```python\r\nfrom datasets import Dataset\r\nimport random\r\n\r\nnums = [{\"k\":random.choice(range(0,1000))} for _ in range(100000)]\r\n\r\nds = Dataset.from_list(nums)\r\n\r\nds = ds.filter(lambda x:x > 100, input_columns=\"k\")\r\n\r\nprint(\"start sort\")\r\n\r\nds = ds.sort(\"k\")\r\n\r\nprint(\"finish sort\")\r\n```\r\n\n\n### Expected behavior\n\nIs this a bug, or is it a misuse of the `sort` function?\n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-3.10.0-1127.19.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2023.10.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "`filter` add an indices mapping on top of the dataset, so `sort` has to gather all the rows that are kept to form a new Arrow table and sort the table. Gathering all the rows can take some time, but is a necessary step. You can try calling `ds = ds.flatten_indices()` before sorting to remove the indices mapping."
      },
      {
        "user": "Alzter",
        "body": "> `filter` add an indices mapping on top of the dataset, so `sort` has to gather all the rows that are kept to form a new Arrow table and sort the table. Gathering all the rows can take some time, but is a necessary step. You can try calling `ds = ds.flatten_indices()` before sorting to remove the indices mapping.\n\nThis worked, thank you so much."
      }
    ]
  },
  {
    "issue_number": 7509,
    "title": "Dataset uses excessive memory when loading files",
    "author": "avishaiElmakies",
    "state": "open",
    "created_at": "2025-04-13T21:09:49Z",
    "updated_at": "2025-04-28T15:18:55Z",
    "labels": [],
    "body": "### Describe the bug\n\nHi\nI am having an issue when loading a dataset.\nI have about 200 json files each about 1GB (total about 215GB). each row has a few features which are a list of ints.\nI am trying to load the dataset using `load_dataset`.\nThe dataset is about 1.5M samples\nI use `num_proc=32` and a node with 378GB of memory. \nAbout a third of the way there I get an OOM.\nI also saw an old bug with a similar issue, which says to set `writer_batch_size`. I tried to lower it to 10, but it still crashed.\nI also tried to lower the `num_proc` to 16 and even 8, but still the same issue.\n\n### Steps to reproduce the bug\n\n`dataset = load_dataset(\"json\", data_dir=data_config.train_path, num_proc=data_config.num_proc, writer_batch_size=50)[\"train\"]`\n\n\n### Expected behavior\n\nLoading a dataset with more than 100GB to spare should not cause an OOM error.\nmaybe i am missing something but I would love some help.\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Linux-6.6.20-aufs-1-x86_64-with-glibc2.36\n- Python version: 3.11.2\n- `huggingface_hub` version: 0.29.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0",
    "comments": [
      {
        "user": "avishaiElmakies",
        "body": "small update: I converted the jsons to parquet and it now works well with 32 proc and the same node. \nI still think this needs to be understood, since json is a very popular and easy-to-use format. "
      },
      {
        "user": "lhoestq",
        "body": "Hi ! The JSON loader loads full files in memory, unless they are JSON Lines. In this case it iterates on the JSON Lines in a memory efficient manner.\n\nI know there is an `ijson` package that works similarly but for general JSON files, maybe it can help and remove the need to load full JSON files in memory"
      },
      {
        "user": "avishaiElmakies",
        "body": "Hi, i understand that json files are probably loaded into memory to read them but aren't they released when we write all the file content into arrow or something? "
      }
    ]
  },
  {
    "issue_number": 7480,
    "title": "HF_DATASETS_CACHE ignored?",
    "author": "stephenroller",
    "state": "open",
    "created_at": "2025-03-26T17:19:34Z",
    "updated_at": "2025-04-28T10:16:16Z",
    "labels": [],
    "body": "### Describe the bug\n\nI'm struggling to get things to respect HF_DATASETS_CACHE.\n\nRationale: I'm on a system that uses NFS for homedir, so downloading to NFS is expensive, slow, and wastes valuable quota compared to local disk. Instead, it seems to rely mostly on HF_HUB_CACHE.\n\nCurrent version: 3.2.1dev. In the process of testing 3.4.0\n\n### Steps to reproduce the bug\n\n[Currently writing using datasets 3.2.1dev. Will follow up with 3.4.0 results]\n\ndump.py:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-100BT\", split=\"train\")\n```\n\nRepro steps\n```bash\n# ensure no cache\n$ mv ~/.cache/huggingface ~/.cache/huggingface.bak\n\n$ export HF_DATASETS_CACHE=/tmp/roller/datasets\n$ rm -rf ${HF_DATASETS_CACHE}\n$ env | grep HF | grep -v TOKEN\nHF_DATASETS_CACHE=/tmp/roller/datasets\n\n$ python dump.py\n# (omitted for brevity)\n\n# (while downloading) \n$ du -hcs ~/.cache/huggingface/hub\n18G     hub\n18G     total\n\n# (after downloading)\n$ du -hcs ~/.cache/huggingface/hub\n```\n\nIt's a shame because datasets supports s3 (which I could really use right now) but hub does not.\n\n### Expected behavior\n\n* ~/.cache/huggingface/hub stays empty\n* /tmp/roller/datasets becomes full of stuff\n\n### Environment info\n\n[Currently writing using datasets 3.2.1dev. Will follow up with 3.4.0 results]",
    "comments": [
      {
        "user": "stephenroller",
        "body": "FWIW, it does eventually write to /tmp/roller/datasets when generating the final version."
      },
      {
        "user": "Harry-Yang0518",
        "body": "Hey, I’d love to work on this issue but I am a beginner, can I work it with you?"
      },
      {
        "user": "Harry-Yang0518",
        "body": "Hi @lhoestq,\nI'd like to look into this issue but I'm still learning. Could you share any quick pointers on the HF_DATASETS_CACHE behavior here? Thanks!"
      }
    ]
  },
  {
    "issue_number": 743,
    "title": "load_dataset for CSV files not working",
    "author": "iliemihai",
    "state": "open",
    "created_at": "2020-10-19T14:53:51Z",
    "updated_at": "2025-04-24T06:35:25Z",
    "labels": [],
    "body": "Similar to #622, I've noticed there is a problem when trying to load a CSV file with datasets.\r\n\r\n`\r\nfrom datasets import load_dataset\r\n`\r\n`\r\ndataset = load_dataset(\"csv\", data_files=[\"./sample_data.csv\"], delimiter=\"\\t\", column_names=[\"title\", \"text\"], script_version=\"master\")\r\n`\r\n\r\nDisplayed error:\r\n`\r\n...\r\nArrowInvalid: CSV parse error: Expected 2 columns, got 1\r\n`\r\n\r\nI should mention that when I've tried to read data from `https://github.com/lhoestq/transformers/tree/custom-dataset-in-rag-retriever/examples/rag/test_data/my_knowledge_dataset.csv` it worked without a problem. I've read that there might be some problems with /r character, so I've removed them from the custom dataset, but the problem still remains.\r\n\r\nI've added a colab reproducing the bug, but unfortunately I cannot provide the dataset.\r\nhttps://colab.research.google.com/drive/1Qzu7sC-frZVeniiWOwzoCe_UHZsrlxu8?usp=sharing\r\n\r\nAre there any work around for it ?\r\nThank you",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Thank you !\r\nCould you provide a csv file that reproduces the error ?\r\nIt doesn't have to be one of your dataset. As long as it reproduces the error\r\nThat would help a lot !"
      },
      {
        "user": "iliemihai",
        "body": "I think another good example is the following:\r\n`\r\nfrom datasets import load_dataset\r\n`\r\n`\r\ndataset = load_dataset(\"csv\", data_files=[\"./sts-dev.csv\"], delimiter=\"\\t\", column_names=[\"one\", \"two\", \"three\", \"four\",  \"score\", \"sentence1\", \"sentence2\"], script_version=\"master\")`\r\n`\r\n\r\nDisplayed error `CSV parse error: Expected 7 columns, got 6` even tough I put 7 columns. First four columns from the csv don't have a name, so I've named them by default. The csv file is the .dev file from STSb benchmark dataset.\r\n\r\n"
      },
      {
        "user": "YipingNUS",
        "body": "Hi, seems I also can't read csv file. I was trying with a dummy csv with only three rows.\r\n\r\n```\r\ntext,label\r\nI hate google,negative\r\nI love Microsoft,positive\r\nI don't like you,negative\r\n```\r\nI was using the HuggingFace image in Paperspace Gradient (datasets==1.1.3). The following code doesn't work:\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('csv', script_version=\"master\", data_files=['test_data.csv'], delimiter=\",\")\r\n```\r\nIt outputs the following:\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset csv/default-3b6254ff4dd403e5 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-3b6254ff4dd403e5/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\r\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3b6254ff4dd403e5/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\r\n```\r\nBut `len(dataset)` gives `1` and I can't access rows with indexing `dataset[0]` (it gives `KeyError: 0`).\r\n\r\nHowever, loading from pandas dataframe is working.\r\n```\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\ndf = pd.read_csv('test_data.csv')\r\ndataset = Dataset.from_pandas(df)\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 7513,
    "title": "MemoryError while creating dataset from generator",
    "author": "simonreise",
    "state": "open",
    "created_at": "2025-04-15T01:02:02Z",
    "updated_at": "2025-04-23T19:37:08Z",
    "labels": [],
    "body": "### Describe the bug\n\n# TL:DR\n\n`Dataset.from_generator` function passes all of its arguments to `BuilderConfig.create_config_id`, including `generator` function itself. `BuilderConfig.create_config_id` function tries to hash all the args, which can take a large amount of time or even cause MemoryError if the dataset processed in a generator function is large enough.\n\nMaybe we should pop `generator` from `config_kwargs_to_add_to_suffix` before hashing to avoid it.\n\n# Full description\n\nI have a pretty large spatial imagery dataset that is generated from two xbatcher.BatchGenerators via custom `dataset_generator` function that looks like this if simplified:\n```\ndef dataset_generator():\n    for index in samples:\n        data_dict = {\n            \"key\": index,\n            \"x\": x_batches[index].data,\n            \"y\":  y_batches[index].data,\n        }\n        yield data_dict\n```\nThen I use `datasets.Dataset.from_generator` to generate the dataset itself.\n```\n# Create dataset\nds = datasets.Dataset.from_generator(\n    dataset_generator,\n    features=feat,\n    cache_dir=(output / \".cache\"),\n)\n```\n\nIt works nicely with pretty small data, but if the dataset is huge and barely fits in memory, it crashes with memory error:\n<details>\n<summary>Full stack trace</summary>\n\n```\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\remote_sensing_processor\\segmentation\\semantic\\tiles.py:248](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/remote_sensing_processor/segmentation/semantic/tiles.py#line=247), in generate_tiles(x, y, output, tile_size, shuffle, split, x_dtype, y_dtype, x_nodata, y_nodata)\n    245         yield data_dict\n    247 # Create dataset\n--> 248 ds = datasets.Dataset.from_generator(\n    249     dataset_generator,\n    250     features=feat,\n    251     cache_dir=(output / \".cache\"),\n    252 )\n    254 # Save dataset\n    255 ds.save_to_disk(output / name)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\arrow_dataset.py:1105](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/arrow_dataset.py#line=1104), in Dataset.from_generator(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, split, **kwargs)\n   1052 \"\"\"Create a Dataset from a generator.\n   1053 \n   1054 Args:\n   (...)   1101 ```\n   1102 \"\"\"\n   1103 from .io.generator import GeneratorDatasetInputStream\n-> 1105 return GeneratorDatasetInputStream(\n   1106     generator=generator,\n   1107     features=features,\n   1108     cache_dir=cache_dir,\n   1109     keep_in_memory=keep_in_memory,\n   1110     gen_kwargs=gen_kwargs,\n   1111     num_proc=num_proc,\n   1112     split=split,\n   1113     **kwargs,\n   1114 ).read()\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\io\\generator.py:29](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/io/generator.py#line=28), in GeneratorDatasetInputStream.__init__(self, generator, features, cache_dir, keep_in_memory, streaming, gen_kwargs, num_proc, split, **kwargs)\n      9 def __init__(\n     10     self,\n     11     generator: Callable,\n   (...)     19     **kwargs,\n     20 ):\n     21     super().__init__(\n     22         features=features,\n     23         cache_dir=cache_dir,\n   (...)     27         **kwargs,\n     28     )\n---> 29     self.builder = Generator(\n     30         cache_dir=cache_dir,\n     31         features=features,\n     32         generator=generator,\n     33         gen_kwargs=gen_kwargs,\n     34         split=split,\n     35         **kwargs,\n     36     )\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\builder.py:343](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/builder.py#line=342), in DatasetBuilder.__init__(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\n    341     config_kwargs[\"data_dir\"] = data_dir\n    342 self.config_kwargs = config_kwargs\n--> 343 self.config, self.config_id = self._create_builder_config(\n    344     config_name=config_name,\n    345     custom_features=features,\n    346     **config_kwargs,\n    347 )\n    349 # prepare info: DatasetInfo are a standardized dataclass across all datasets\n    350 # Prefill datasetinfo\n    351 if info is None:\n    352     # TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\builder.py:604](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/builder.py#line=603), in DatasetBuilder._create_builder_config(self, config_name, custom_features, **config_kwargs)\n    598 builder_config._resolve_data_files(\n    599     base_path=self.base_path,\n    600     download_config=DownloadConfig(token=self.token, storage_options=self.storage_options),\n    601 )\n    603 # compute the config id that is going to be used for caching\n--> 604 config_id = builder_config.create_config_id(\n    605     config_kwargs,\n    606     custom_features=custom_features,\n    607 )\n    608 is_custom = (config_id not in self.builder_configs) and config_id != \"default\"\n    609 if is_custom:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\builder.py:187](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/builder.py#line=186), in BuilderConfig.create_config_id(self, config_kwargs, custom_features)\n    185             suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\n    186     else:\n--> 187         suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\n    189 if custom_features is not None:\n    190     m = Hasher()\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\fingerprint.py:188](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/fingerprint.py#line=187), in Hasher.hash(cls, value)\n    186 @classmethod\n    187 def hash(cls, value: Any) -> str:\n--> 188     return cls.hash_bytes(dumps(value))\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:109](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=108), in dumps(obj)\n    107 \"\"\"Pickle an object to a string.\"\"\"\n    108 file = BytesIO()\n--> 109 dump(obj, file)\n    110 return file.getvalue()\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:103](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=102), in dump(obj, file)\n    101 def dump(obj, file):\n    102     \"\"\"Pickle an object to a file.\"\"\"\n--> 103     Pickler(file, recurse=True).dump(obj)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:420](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=419), in Pickler.dump(self, obj)\n    418 def dump(self, obj): #NOTE: if settings change, need to update attributes\n    419     logger.trace_setup(self)\n--> 420     StockPickler.dump(self, obj)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:484](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=483), in _Pickler.dump(self, obj)\n    482 if self.proto >= 4:\n    483     self.framer.start_framing()\n--> 484 self.save(obj)\n    485 self.write(STOP)\n    486 self.framer.end_framing()\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1985](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1984), in save_function(pickler, obj)\n   1982 if state_dict:\n   1983     state = state, state_dict\n-> 1985 _save_with_postproc(pickler, (_create_function, (\n   1986         obj.__code__, globs, obj.__name__, obj.__defaults__,\n   1987         closure\n   1988 ), state), obj=obj, postproc_list=postproc_list)\n   1990 # Lift closure cell update to earliest function (#458)\n   1991 if _postproc:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1117](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1116), in _save_with_postproc(pickler, reduction, is_pickler_dill, obj, postproc_list)\n   1115         continue\n   1116 else:\n-> 1117     pickler.save_reduce(*reduction)\n   1118 # pop None created by calling preprocessing step off stack\n   1119 pickler.write(POP)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:690](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=689), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    688 else:\n    689     save(func)\n--> 690     save(args)\n    691     write(REDUCE)\n    693 if obj is not None:\n    694     # If the object is already in the memo, this means it is\n    695     # recursive. In this case, throw away everything we put on the\n    696     # stack, and fetch the object back from the memo.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:905](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=904), in _Pickler.save_tuple(self, obj)\n    903 if n <= 3 and self.proto >= 2:\n    904     for element in obj:\n--> 905         save(element)\n    906     # Subtle.  Same as in the big comment below.\n    907     if id(obj) in memo:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\n    [... skipping similar frames: Pickler.save at line 70 (1 times), Pickler.save at line 414 (1 times)]\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:905](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=904), in _Pickler.save_tuple(self, obj)\n    903 if n <= 3 and self.proto >= 2:\n    904     for element in obj:\n--> 905         save(element)\n    906     # Subtle.  Same as in the big comment below.\n    907     if id(obj) in memo:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:905](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=904), in _Pickler.save_tuple(self, obj)\n    903 if n <= 3 and self.proto >= 2:\n    904     for element in obj:\n--> 905         save(element)\n    906     # Subtle.  Same as in the big comment below.\n    907     if id(obj) in memo:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:690](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=689), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    688 else:\n    689     save(func)\n--> 690     save(args)\n    691     write(REDUCE)\n    693 if obj is not None:\n    694     # If the object is already in the memo, this means it is\n    695     # recursive. In this case, throw away everything we put on the\n    696     # stack, and fetch the object back from the memo.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:920](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=919), in _Pickler.save_tuple(self, obj)\n    918 write(MARK)\n    919 for element in obj:\n--> 920     save(element)\n    922 if id(obj) in memo:\n    923     # Subtle.  d was not in memo when we entered save_tuple(), so\n    924     # the process of saving the tuple's elements must have saved\n   (...)    928     # could have been done in the \"for element\" loop instead, but\n    929     # recursive tuples are a rare thing.\n    930     get = self.get(memo[id(obj)][0])\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1019](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1018), in _Pickler._batch_setitems(self, items)\n   1017     k, v = tmp[0]\n   1018     save(k)\n-> 1019     save(v)\n   1020     write(SETITEM)\n   1021 # else tmp is empty, and we're done\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\n    [... skipping similar frames: Pickler.save at line 70 (1 times), Pickler.save at line 414 (1 times)]\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\n    [... skipping similar frames: Pickler.save at line 70 (1 times), Pickler.save at line 414 (1 times)]\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:920](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=919), in _Pickler.save_tuple(self, obj)\n    918 write(MARK)\n    919 for element in obj:\n--> 920     save(element)\n    922 if id(obj) in memo:\n    923     # Subtle.  d was not in memo when we entered save_tuple(), so\n    924     # the process of saving the tuple's elements must have saved\n   (...)    928     # could have been done in the \"for element\" loop instead, but\n    929     # recursive tuples are a rare thing.\n    930     get = self.get(memo[id(obj)][0])\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:809](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=808), in _Pickler.save_bytes(self, obj)\n    806         self.save_reduce(codecs.encode,\n    807                          (str(obj, 'latin1'), 'latin1'), obj=obj)\n    808     return\n--> 809 self._save_bytes_no_memo(obj)\n    810 self.memoize(obj)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:797](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=796), in _Pickler._save_bytes_no_memo(self, obj)\n    795     self._write_large_bytes(BINBYTES8 + pack(\"<Q\", n), obj)\n    796 elif n >= self.framer._FRAME_SIZE_TARGET:\n--> 797     self._write_large_bytes(BINBYTES + pack(\"<I\", n), obj)\n    798 else:\n    799     self.write(BINBYTES + pack(\"<I\", n) + obj)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:254](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=253), in _Framer.write_large_bytes(self, header, payload)\n    247 # Perform direct write of the header and payload of the large binary\n    248 # object. Be careful not to concatenate the header and the payload\n    249 # prior to calling 'write' as we do not want to allocate a large\n    250 # temporary bytes object.\n    251 # We intentionally do not insert a protocol 4 frame opcode to make\n    252 # it possible to optimize file.read calls in the loader.\n    253 write(header)\n--> 254 write(payload)\n\nMemoryError:\n\n```\n\n</details>\n\nMemory error is an expected type of error in such case, but when I started digging down, I found out that it occurs in a kinda unexpected place - in `create_config_id` function. It tries to hash `config_kwargs_to_add_to_suffix`, including generator function itself. \n\nI modified the `BuilderConfig.create_config_id` code like this to check which values are hashed and how much time it takes to hash them and ran it on a toy dataset:\n```\nprint(config_kwargs_to_add_to_suffix)\nstart_time = time.time()\n\nif all(isinstance(v, (str, bool, int, float)) for v in config_kwargs_to_add_to_suffix.values()):\n    suffix = \",\".join(\n        str(k) + \"=\" + urllib.parse.quote_plus(str(v)) for k, v in config_kwargs_to_add_to_suffix.items()\n    )\n    if len(suffix) > 32:  # hash if too long\n        suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\nelse:\n    suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\nend_time = time.time()\n\nprint(f\"Execution time: {end_time - start_time:.4f} seconds\")\nprint(suffix)\n```\n\nIn my case the content of `config_kwargs_to_add_to_suffix` was like this:\n```\n{'features': {'key': Value(dtype='int64', id=None), 'x': Array3D(shape=(44, 128, 128), dtype='float32', id=None), 'y_class': Array2D(shape=(128, 128), dtype='int32', id=None)}, 'gen_kwargs': None, 'generator': <function generate_tiles.<locals>.dataset_generator at 0x00000139D10D7920>, 'split': NamedSplit('train')}\n```\n\nAlso I noticed that hashing took a significant amount of time - 43.1482 seconds, while the overall function execution (with data loading, batching and saving dataset) took 2min 45s. The output of `create_config_id` is just a dataset id, so, it is inappropirately large amount of time.\n\nBut when I added `config_kwargs_to_add_to_suffix.pop(\"generator\", None)`, the hashing took only 0.0060 seconds.\n\nMaybe we shouldn't hash the generator function, as it can be really computationally and memory expensive.\n\n\n\n### Steps to reproduce the bug\n\nThis is a simplified example of a workflow I used to generate dataset. But I think that you can use almost any workflow to reproduce that bug.\n\n```\nimport pystac\nimport pystac_client\nimport planetary_computer\n\nimport numpy as np\nimport xarray as xr\nimport rioxarray as rxr\nimport dask\nimport xbatcher\n\nimport datasets\n\n\n# Loading a dataset, in our case - single Landsat image\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\nbrazil = [-60.2, -3.31]\ntime_of_interest = \"2021-06-01/2021-08-31\"\nsearch = catalog.search(collections=[\"landsat-c2-l2\"], intersects={\"type\": \"Point\", \"coordinates\": brazil}, datetime=time_of_interest)\nitems = search.item_collection()\nitem = min(items, key=lambda item: pystac.extensions.eo.EOExtension.ext(item).cloud_cover)\n\n# Getting x data\nbands = []\nfor band in [\"red\", \"green\", \"blue\", \"nir08\", \"coastal\", \"swir16\", \"swir22\", \"lwir11\"]:\n    with rxr.open_rasterio(item.assets[band].href, chunks=True, lock=True) as raster:\n        raster = raster.to_dataset('band')\n        #print(raster)\n        raster = raster.rename({1: band})\n        bands.append(raster)\nx = xr.merge(bands).squeeze().to_array(\"band\").persist()\n\n# Getting y data\nwith rxr.open_rasterio(item.assets['qa_pixel'].href, chunks=True, lock=True) as raster:\n    y = raster.squeeze().persist()\n\n# Setting up batches generators\nx_batches = xbatcher.BatchGenerator(ds=x, input_dims={\"x\": 256, \"y\": 256})\ny_batches = xbatcher.BatchGenerator(ds=y, input_dims={\"x\": 256, \"y\": 256})\n\n# Filtering samples that contain only nodata\nsamples = list(range(len(x_batches)))\nsamples_filtered = []\nfor i in samples:\n    if not np.array_equal(np.unique(x_batches[i]), np.array([0.])) and not np.array_equal(np.unique(y_batches[i]), np.array([0])):\n        samples_filtered.append(i)\nsamples = samples_filtered\nnp.random.shuffle(samples)\n\n# Setting up features\nfeat = {\n    \"key\": datasets.Value(dtype=\"int64\"), \n    \"x\": datasets.Array3D(dtype=\"float32\", shape=(4, 256, 256)),\n    \"y\": datasets.Array2D(dtype=\"int32\", shape=(256, 256))\n}\nfeat = datasets.Features(feat)\n\n# Setting up a generator\ndef dataset_generator():\n    for index in samples:\n        data_dict = {\n            \"key\": index,\n            \"x\": x_batches[index].data,\n            \"y\": y_batches[index].data,\n        }\n        yield data_dict\n\n# Create dataset\nds = datasets.Dataset.from_generator(\n    dataset_generator,\n    features=feat,\n    cache_dir=\"temp/cache\",\n)\n```\nPlease, try adding `config_kwargs_to_add_to_suffix.pop(\"generator\", None)` to `BuilderConfig.create_config_id` and then measuring how much time it takes to run \n``` \nif all(isinstance(v, (str, bool, int, float)) for v in config_kwargs_to_add_to_suffix.values()):\n    suffix = \",\".join(\n        str(k) + \"=\" + urllib.parse.quote_plus(str(v)) for k, v in config_kwargs_to_add_to_suffix.items()\n    )\n    if len(suffix) > 32:  # hash if too long\n        suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\nelse:\n    suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\n```\ncode block with and without `config_kwargs_to_add_to_suffix.pop(\"generator\", None)`\n\nIn my case the difference was 3.3828 seconds without popping generator function and 0.0010 seconds with popping.\n\n### Expected behavior\n\nMuch faster hashing and no MemoryErrors\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Windows-11-10.0.26100-SP0\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.30.1\n- PyArrow version: 17.0.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2024.12.0",
    "comments": [
      {
        "user": "simonreise",
        "body": "Upd: created a PR that can probably solve the problem: #7514"
      },
      {
        "user": "lhoestq",
        "body": "Hi ! We need to take the generator into account for the cache. The generator is hashed to make the dataset fingerprint used by the cache. This way you can reload the Dataset from the cache without regenerating in subsequent `from_generator` calls.\n\nMaybe instead of removing generator from the hasher input, we can let users pass their own Dataset fingerprint to `from_generator`, and if it's specified we don't need to hash anything"
      },
      {
        "user": "simonreise",
        "body": "Upd: I successfully generated a dataset from my large geospatial data with `generator` excluded from hashing and saved it to disk without running into memory errors. So, it looks like there are no other bottlenecks in dataset generation in my case\n\nMaybe letting users pass their own fingerprint to skip hashing can be a great solution to that issue!"
      }
    ]
  },
  {
    "issue_number": 7530,
    "title": "How to solve \"Spaces stuck in Building\" problems",
    "author": "ghost",
    "state": "closed",
    "created_at": "2025-04-21T03:08:38Z",
    "updated_at": "2025-04-22T07:49:52Z",
    "labels": [],
    "body": "### Describe the bug\n\nPublic spaces may stuck in Building after restarting, error log as follows:\n\nbuild error\nUnexpected job error\n\nERROR: failed to push spaces-registry.huggingface.tech/spaces/*:cpu-*-*: unexpected status from HEAD request to https://spaces-registry.huggingface.tech/v2/spaces/*/manifests/cpu-*-*: 401 Unauthorized\n\n### Steps to reproduce the bug\n\nRestart space / Factory rebuild cannot avoid it\n\n### Expected behavior\n\nFix this problem\n\n### Environment info\n\nno requirements.txt can still happen\npython gradio spaces",
    "comments": [
      {
        "user": "TheCuongt65",
        "body": "I'm facing the same issue—Space stuck in \"Building\" even after restart and Factory rebuild. Any fix?\n"
      },
      {
        "user": "ghost",
        "body": "> I'm facing the same issue—Space stuck in \"Building\" even after restart and Factory rebuild. Any fix?\n\nAlso see https://github.com/huggingface/huggingface_hub/issues/3019"
      },
      {
        "user": "Iaroslav856",
        "body": "I'm facing the same issue. The build fails with the same error, and restarting won't help. Is there a fix or ETA? "
      }
    ]
  },
  {
    "issue_number": 7529,
    "title": "audio folder builder cannot detect custom split name",
    "author": "phineas-pta",
    "state": "open",
    "created_at": "2025-04-20T16:53:21Z",
    "updated_at": "2025-04-20T16:53:21Z",
    "labels": [],
    "body": "### Describe the bug\n\nwhen using audio folder builder (`load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")`), it cannot detect custom split name other than train/validation/test\n\n### Steps to reproduce the bug\n\ni have the following folder structure\n```\nmy_dataset/\n├── train/\n│   ├── lorem.wav\n│   ├── …\n│   └── metadata.csv\n├── test/\n│   ├── ipsum.wav\n│   ├── …\n│   └── metadata.csv\n├── validation/\n│   ├── dolor.wav\n│   ├── …\n│   └── metadata.csv\n└── custom/\n    ├── sit.wav\n    ├── …\n    └── metadata.csv\n```\nusing `ds = load_dataset(\"audiofolder\", data_dir=\"/path/to/my_dataset\")`\n\n### Expected behavior\n\ni got `ds` with only 3 splits train/validation/test, whenever i rename train/validation/test folder it also disappear if i re-create `ds`\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Windows-11-10.0.26100-SP0\n- Python version: 3.12.8\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0",
    "comments": []
  },
  {
    "issue_number": 7520,
    "title": "Update items in the dataset without `map`",
    "author": "mashdragon",
    "state": "open",
    "created_at": "2025-04-15T19:39:01Z",
    "updated_at": "2025-04-19T18:47:46Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nI would like to be able to update items in my dataset without affecting all rows. At least if there was a range option, I would be able to process those items, save the dataset, and then continue.\n\nIf I am supposed to split the dataset first, that is not clear, since the docs suggest that any of those functions returns a new object, so I don't think I can do that.\n\n### Motivation\n\nI am applying an extremely time-consuming function to each item in my `Dataset`. Unfortunately, datasets only supports updating values via `map`, so if my computer dies in the middle of this long-running process, I lose all progress. This is far from ideal. I would like to use `datasets` throughout this processing, but this limitation is now forcing me to write my own dataset format just to do this intermediary operation.\n\nIt would be less intuitive but I suppose I could split and then concatenate the dataset before saving? But this feels very inefficient.\n\n### Your contribution\n\nI can test the feature.",
    "comments": [
      {
        "user": "Dref360",
        "body": "Hello!\n\nHave you looked at `Dataset.shard`? [Docs](https://huggingface.co/docs/datasets/en/process#shard)\n\nUsing this method you could break your dataset in N shards. Apply `map` on each shard and concatenate them back."
      }
    ]
  },
  {
    "issue_number": 7313,
    "title": "Cannot create a dataset with relative audio path",
    "author": "sedol1339",
    "state": "open",
    "created_at": "2024-12-09T07:34:20Z",
    "updated_at": "2025-04-19T07:13:08Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nHello! I want to create a dataset of parquet files, with audios stored as separate .mp3 files. However, it says \"No such file or directory\" (see the reproducing code).\r\n\r\n### Steps to reproduce the bug\r\n\r\nCreating a dataset\r\n```\r\nfrom pathlib import Path\r\nfrom datasets import Dataset, load_dataset, Audio\r\n\r\nPath('my_dataset/audio').mkdir(parents=True, exist_ok=True)\r\nPath('my_dataset/audio/file.mp3').touch(exist_ok=True)\r\nDataset.from_list(\r\n    [{'audio': {'path': 'audio/file.mp3'}}]\r\n).to_parquet('my_dataset/data.parquet')\r\n```\r\n\r\nResult:\r\n```\r\n# my_dataset\r\n# ├── audio\r\n# │   └── file.mp3\r\n# └── data.parquet\r\n```\r\n\r\nTrying to load the dataset\r\n```\r\ndataset = (\r\n    load_dataset('my_dataset', split='train')\r\n    .cast_column('audio', Audio(sampling_rate=16_000))\r\n)\r\ndataset[0]\r\n\r\n>>> FileNotFoundError: [Errno 2] No such file or directory: 'audio/file.mp3'\r\n```\r\n\r\n### Expected behavior\r\n\r\nI expect the dataset to load correctly.\r\n\r\nI've found 2 workarounds, but they are not very good:\r\n1. I can specify an absolute path to the audio, however, when I move the folder or upload to HF it will stop working.\r\n2. I can set `'path': 'file.mp3'`, and load with `load_dataset('my_dataset', data_dir='audio')` - it seems to work, but does this mean that anyone from Hugging Face who wants to use this dataset should also pass the `data_dir` argument, otherwise it won't work?\r\n\r\n### Environment info\r\n\r\ndatasets 3.1.0, Ubuntu 24.04.1",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hello ! when you `cast_column` you need the paths to be absolute paths or relative paths to your working directory, not the original dataset directory.\r\n\r\nThough I'd recommend structuring your dataset as an AudioFolder which automatically links a metadata.jsonl or csv to the audio files via relative paths **within** the dataset repository: https://huggingface.co/docs/datasets/v3.2.0/en/audio_load#audiofolder"
      },
      {
        "user": "sedol1339",
        "body": "@lhoestq thank you, but there are two problems with using AudioFolder:\r\n1. It is said that AudioFolder requires metadata.csv. However, my datset is too large and contains nested and np.ndarray fields, so I can't use csv.\r\n2. It is said that I need to load the dataset with `load_dataset(\"audiofolder\", ...)`. However, if possible, I want my dataset to be loaded as usual with `load_dataset(dataset_name)` after I upload if to HF."
      },
      {
        "user": "lhoestq",
        "body": "You can use metadata.jsonl if you have nested data :)\r\n\r\nAnd actually if you have a dataset structured as an AudioFolder then `load_dataset(dataset_name)` does work after uploading to HF"
      }
    ]
  },
  {
    "issue_number": 7440,
    "title": "IterableDataset raises FileNotFoundError instead of retrying",
    "author": "bauwenst",
    "state": "open",
    "created_at": "2025-03-07T19:14:18Z",
    "updated_at": "2025-04-17T23:40:35Z",
    "labels": [],
    "body": "### Describe the bug\n\nIn https://github.com/huggingface/datasets/issues/6843 it was noted that the streaming feature of `datasets` is highly susceptible to outages and doesn't back off for long (or even *at all*).\n\nI was training a model while streaming SlimPajama and training crashed with a `FileNotFoundError`. I can only assume that this was due to a momentary outage considering the file in question, `train/chunk9/example_train_3889.jsonl.zst`, [exists like all other files in SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B/blob/main/train/chunk9/example_train_3889.jsonl.zst).\n```python\n...\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 2226, in __iter__\n    for key, example in ex_iterable:\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1499, in __iter__\n    for x in self.ex_iterable:\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1067, in __iter__\n    yield from self._iter()\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1231, in _iter\n    for key, transformed_example in iter_outputs():\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1207, in iter_outputs\n    for i, key_example in inputs_iterator:\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1111, in iter_inputs\n    for key, example in iterator:\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 371, in __iter__\n    for key, pa_table in self.generate_tables_fn(**gen_kwags):\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/packaged_modules/json/json.py\", line 99, in _generate_tables\n    for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/utils/track.py\", line 50, in __iter__\n    for x in self.generator(*self.args):\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/utils/file_utils.py\", line 1378, in _iter_from_urlpaths\n    raise FileNotFoundError(urlpath)\nFileNotFoundError: zstd://example_train_3889.jsonl::hf://datasets/cerebras/SlimPajama-627B@2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/train/chunk9/example_train_3889.jsonl.zst\n```\n\nThat final `raise` is at the bottom of the following snippet:\nhttps://github.com/huggingface/datasets/blob/f693f4e93aabafa878470c80fd42ddb10ec550d6/src/datasets/utils/file_utils.py#L1354-L1379\n\nSo clearly, something choked up in `xisfile`.\n\n### Steps to reproduce the bug\n\nThis happens when streaming a dataset and iterating over it. In my case, that iteration is done in Trainer's `inner_training_loop`, but this is not relevant to the iterator.\n```python\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/accelerate/data_loader.py\", line 835, in __iter__\n    next_batch, next_batch_info = self._fetch_batches(main_iterator)\n```\n\n\n### Expected behavior\n\n\nThis bug and the linked issue have one thing in common: *when streaming fails to retrieve an example, the entire program gives up and crashes*. As users, we cannot even protect ourselves from this: when we are iterating over a dataset, we can't make `datasets` skip over a bad example or wait a little longer to retry the iteration, because when a Python generator/iterator raises an error, it loses all its context. \n\nIn other words: if you have something that looks like `for b in a: for c in b: for d in c:`, errors in the innermost loop can only be caught by a `try ... except` in `c.__iter__()`. There should be such exception handling in `datasets` and it should have a **configurable exponential back-off**: first wait and retry after 1 minute, then 2 minutes, then 4 minutes, then 8 minutes, ... and after a given amount of retries, **skip the bad example**, and **only after** skipping a given amount of examples, give up and crash. This was requested in https://github.com/huggingface/datasets/issues/6843 too, since currently there is only linear backoff *and* it is clearly not applied to `xisfile`.\n\n\n### Environment info\n\n- `datasets` version: 3.3.2 *(the latest version)*\n- Platform: Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28\n- Python version: 3.11.7\n- `huggingface_hub` version: 0.26.5\n- PyArrow version: 15.0.0\n- Pandas version: 2.2.0\n- `fsspec` version: 2024.10.0",
    "comments": [
      {
        "user": "bauwenst",
        "body": "I have since been training more models with identical architectures over the same dataset, and it is completely unstable. One has now failed at chunk9/1215, whilst others have gotten past that.\n```python\nFileNotFoundError: zstd://example_train_1215.jsonl::hf://datasets/cerebras/SlimPajama-627B@2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/train/chunk9/example_train_1215.jsonl.zst\n```\nBelow is the full training log, where you can clearly see the intermittent dataset issues. Note again that this model only got to epoch 0.11, whereas I have other models training on the exact same dataset right now that have gotten way beyond that. This is quickly turning into a highly expensive bug which I didn't have issues with in the past half year of using the same setup.\n<details>\n<summary>Training log of failed run</summary>\n\n```python\n  1%|          | 64/8192 [56:27<87:25:33, 38.72s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5ef28452-e903-4bd8-946d-f0c77f558a2a)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk5/example_holdout_4799.jsonl.zst\n  1%|          | 64/8192 [56:51<87:25:33, 38.72s/it]Retrying in 1s [Retry 1/5].\n  2%|▏         | 192/8192 [2:40:14<85:29:44, 38.47s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ba6e4c51-f4a4-407e-9934-3772550b7ce9)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk1/example_holdout_2770.jsonl.zst\n  2%|▏         | 192/8192 [2:40:53<85:29:44, 38.47s/it]Retrying in 1s [Retry 1/5].\n  2%|▏         | 192/8192 [2:40:53<85:29:44, 38.47s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: bdf2cfaa-7e0b-46a0-bec1-b1e573fa7998)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_4386.jsonl.zst\n  2%|▏         | 192/8192 [2:42:16<85:29:44, 38.47s/it]Retrying in 1s [Retry 1/5].\n  2%|▏         | 192/8192 [2:42:16<85:29:44, 38.47s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1dc5e455-8042-4c7b-9b97-5ded33dfea34)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk1/example_holdout_1763.jsonl.zst\n  2%|▏         | 192/8192 [2:42:30<85:29:44, 38.47s/it]Retrying in 1s [Retry 1/5].\n  2%|▏         | 192/8192 [2:42:30<85:29:44, 38.47s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9cf29917-8111-41fe-80aa-953df65c5803)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_5509.jsonl.zst\n  2%|▏         | 192/8192 [2:44:31<85:29:44, 38.47s/it]Retrying in 1s [Retry 1/5].\n  2%|▏         | 192/8192 [2:44:31<85:29:44, 38.47s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2515a0b0-3d81-409f-940c-e78ed5e2dbf8)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_3093.jsonl.zst\n  2%|▏         | 192/8192 [2:45:13<85:29:44, 38.47s/it]Retrying in 1s [Retry 1/5].\n  2%|▏         | 192/8192 [2:45:13<85:29:44, 38.47s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a4c1e0c7-1c7a-4377-bc7e-6f076473072b)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_3422.jsonl.zst\n  2%|▏         | 192/8192 [2:46:26<85:29:44, 38.47s/it]Retrying in 1s [Retry 1/5].\n  2%|▏         | 192/8192 [2:46:26<85:29:44, 38.47s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c7b0d366-db86-4d0c-a4e0-be251d26519e)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_2250.jsonl.zst\n  2%|▏         | 192/8192 [2:47:24<85:29:44, 38.47s/it]Retrying in 1s [Retry 1/5].\n  2%|▏         | 192/8192 [2:47:24<85:29:44, 38.47s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b0df5a1a-4836-46cf-8e45-58a7c1553309)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_6161.jsonl.zst\n  2%|▏         | 192/8192 [2:49:10<85:29:44, 38.47s/it]Retrying in 1s [Retry 1/5].\n  2%|▏         | 192/8192 [2:49:10<85:29:44, 38.47s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c1d97368-c0ae-45bb-ae10-5559b3ebc4e4)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_5782.jsonl.zst\n  2%|▏         | 192/8192 [2:49:30<85:29:44, 38.47s/it]Retrying in 1s [Retry 1/5].\n{'eval_loss': 10.482319831848145, 'eval_runtime': 902.7516, 'eval_samples_per_second': 18.149, 'eval_steps_per_second': 0.142, 'epoch': 0, 'num_input_tokens_seen': 0}\n{'loss': 10.4895, 'grad_norm': 2.9147818088531494, 'learning_rate': 3.90625e-06, 'epoch': 0.0, 'num_input_tokens_seen': 1048576}\n{'loss': 10.4832, 'grad_norm': 2.8206892013549805, 'learning_rate': 7.8125e-06, 'epoch': 0.0, 'num_input_tokens_seen': 2097152}\n{'loss': 10.4851, 'grad_norm': 2.910552978515625, 'learning_rate': 1.171875e-05, 'epoch': 0.0, 'num_input_tokens_seen': 3145728}\n{'loss': 10.486, 'grad_norm': 2.8042073249816895, 'learning_rate': 1.5625e-05, 'epoch': 0.0, 'num_input_tokens_seen': 4194304}\n{'loss': 10.4719, 'grad_norm': 2.83260440826416, 'learning_rate': 1.953125e-05, 'epoch': 0.0, 'num_input_tokens_seen': 5242880}\n{'loss': 10.4482, 'grad_norm': 2.916527032852173, 'learning_rate': 2.34375e-05, 'epoch': 0.0, 'num_input_tokens_seen': 6291456}\n{'loss': 10.4113, 'grad_norm': 2.911870241165161, 'learning_rate': 2.734375e-05, 'epoch': 0.0, 'num_input_tokens_seen': 7340032}\n{'loss': 10.3863, 'grad_norm': 2.8873367309570312, 'learning_rate': 3.125e-05, 'epoch': 0.0, 'num_input_tokens_seen': 8388608}\n{'loss': 10.3557, 'grad_norm': 2.7183432579040527, 'learning_rate': 3.5156250000000004e-05, 'epoch': 0.0, 'num_input_tokens_seen': 9437184}\n{'loss': 10.2795, 'grad_norm': 2.6743927001953125, 'learning_rate': 3.90625e-05, 'epoch': 0.0, 'num_input_tokens_seen': 10485760}\n{'loss': 10.2148, 'grad_norm': 2.3173940181732178, 'learning_rate': 4.296875e-05, 'epoch': 0.0, 'num_input_tokens_seen': 11534336}\n{'loss': 10.1482, 'grad_norm': 2.09787917137146, 'learning_rate': 4.6875e-05, 'epoch': 0.0, 'num_input_tokens_seen': 12582912}\n{'loss': 10.1024, 'grad_norm': 1.889390468597412, 'learning_rate': 5.0781250000000004e-05, 'epoch': 0.0, 'num_input_tokens_seen': 13631488}\n{'loss': 10.0418, 'grad_norm': 1.8319090604782104, 'learning_rate': 5.46875e-05, 'epoch': 0.0, 'num_input_tokens_seen': 14680064}\n{'loss': 10.0081, 'grad_norm': 1.7302652597427368, 'learning_rate': 5.859375e-05, 'epoch': 0.0, 'num_input_tokens_seen': 15728640}\n{'loss': 9.9525, 'grad_norm': 1.767600417137146, 'learning_rate': 6.25e-05, 'epoch': 0.0, 'num_input_tokens_seen': 16777216}\n{'loss': 9.9326, 'grad_norm': 2.1608240604400635, 'learning_rate': 6.640625e-05, 'epoch': 0.0, 'num_input_tokens_seen': 17825792}\n{'loss': 9.8478, 'grad_norm': 1.7399269342422485, 'learning_rate': 7.031250000000001e-05, 'epoch': 0.0, 'num_input_tokens_seen': 18874368}\n{'loss': 9.8215, 'grad_norm': 1.6564425230026245, 'learning_rate': 7.421875e-05, 'epoch': 0.0, 'num_input_tokens_seen': 19922944}\n{'loss': 9.7732, 'grad_norm': 1.6452653408050537, 'learning_rate': 7.8125e-05, 'epoch': 0.0, 'num_input_tokens_seen': 20971520}\n{'loss': 9.6896, 'grad_norm': 1.7053238153457642, 'learning_rate': 8.203125e-05, 'epoch': 0.0, 'num_input_tokens_seen': 22020096}\n{'loss': 9.6356, 'grad_norm': 1.7050201892852783, 'learning_rate': 8.59375e-05, 'epoch': 0.0, 'num_input_tokens_seen': 23068672}\n{'loss': 9.5781, 'grad_norm': 1.7155998945236206, 'learning_rate': 8.984375e-05, 'epoch': 0.0, 'num_input_tokens_seen': 24117248}\n{'loss': 9.5355, 'grad_norm': 1.697864294052124, 'learning_rate': 9.375e-05, 'epoch': 0.0, 'num_input_tokens_seen': 25165824}\n{'loss': 9.4718, 'grad_norm': 1.7598071098327637, 'learning_rate': 9.765625e-05, 'epoch': 0.0, 'num_input_tokens_seen': 26214400}\n{'loss': 9.3972, 'grad_norm': 1.7407673597335815, 'learning_rate': 0.00010156250000000001, 'epoch': 0.0, 'num_input_tokens_seen': 27262976}\n{'loss': 9.3303, 'grad_norm': 1.7710134983062744, 'learning_rate': 0.00010546875, 'epoch': 0.0, 'num_input_tokens_seen': 28311552}\n{'loss': 9.2973, 'grad_norm': 1.716180682182312, 'learning_rate': 0.000109375, 'epoch': 0.0, 'num_input_tokens_seen': 29360128}\n{'loss': 9.2049, 'grad_norm': 1.7579947710037231, 'learning_rate': 0.00011328125, 'epoch': 0.0, 'num_input_tokens_seen': 30408704}\n{'loss': 9.1656, 'grad_norm': 1.6988558769226074, 'learning_rate': 0.0001171875, 'epoch': 0.0, 'num_input_tokens_seen': 31457280}\n{'loss': 9.0966, 'grad_norm': 1.7036350965499878, 'learning_rate': 0.00012109375, 'epoch': 0.0, 'num_input_tokens_seen': 32505856}\n{'loss': 9.0107, 'grad_norm': 1.752451777458191, 'learning_rate': 0.000125, 'epoch': 0.0, 'num_input_tokens_seen': 33554432}\n{'loss': 8.9788, 'grad_norm': 1.6769776344299316, 'learning_rate': 0.00012890625, 'epoch': 0.0, 'num_input_tokens_seen': 34603008}\n{'loss': 8.9155, 'grad_norm': 1.6497987508773804, 'learning_rate': 0.0001328125, 'epoch': 0.0, 'num_input_tokens_seen': 35651584}\n{'loss': 8.8008, 'grad_norm': 1.722798466682434, 'learning_rate': 0.00013671875, 'epoch': 0.0, 'num_input_tokens_seen': 36700160}\n{'loss': 8.7727, 'grad_norm': 1.6046854257583618, 'learning_rate': 0.00014062500000000002, 'epoch': 0.0, 'num_input_tokens_seen': 37748736}\n{'loss': 8.682, 'grad_norm': 1.6132164001464844, 'learning_rate': 0.00014453125, 'epoch': 0.0, 'num_input_tokens_seen': 38797312}\n{'loss': 8.6516, 'grad_norm': 1.558968424797058, 'learning_rate': 0.0001484375, 'epoch': 0.0, 'num_input_tokens_seen': 39845888}\n{'loss': 8.5935, 'grad_norm': 1.6083673238754272, 'learning_rate': 0.00015234375, 'epoch': 0.0, 'num_input_tokens_seen': 40894464}\n{'loss': 8.4852, 'grad_norm': 1.5469273328781128, 'learning_rate': 0.00015625, 'epoch': 0.0, 'num_input_tokens_seen': 41943040}\n{'loss': 8.4342, 'grad_norm': 1.46219801902771, 'learning_rate': 0.00016015625, 'epoch': 0.01, 'num_input_tokens_seen': 42991616}\n{'loss': 8.3213, 'grad_norm': 1.473191261291504, 'learning_rate': 0.0001640625, 'epoch': 0.01, 'num_input_tokens_seen': 44040192}\n{'loss': 8.3193, 'grad_norm': 1.4024137258529663, 'learning_rate': 0.00016796875000000001, 'epoch': 0.01, 'num_input_tokens_seen': 45088768}\n{'loss': 8.1853, 'grad_norm': 1.3591463565826416, 'learning_rate': 0.000171875, 'epoch': 0.01, 'num_input_tokens_seen': 46137344}\n{'loss': 8.1109, 'grad_norm': 1.3547109365463257, 'learning_rate': 0.00017578125, 'epoch': 0.01, 'num_input_tokens_seen': 47185920}\n{'loss': 8.0741, 'grad_norm': 1.268977403640747, 'learning_rate': 0.0001796875, 'epoch': 0.01, 'num_input_tokens_seen': 48234496}\n{'loss': 8.0032, 'grad_norm': 1.222671389579773, 'learning_rate': 0.00018359375, 'epoch': 0.01, 'num_input_tokens_seen': 49283072}\n{'loss': 7.9346, 'grad_norm': 1.154278039932251, 'learning_rate': 0.0001875, 'epoch': 0.01, 'num_input_tokens_seen': 50331648}\n{'loss': 7.8823, 'grad_norm': 1.1396397352218628, 'learning_rate': 0.00019140625, 'epoch': 0.01, 'num_input_tokens_seen': 51380224}\n{'loss': 7.8444, 'grad_norm': 1.0608373880386353, 'learning_rate': 0.0001953125, 'epoch': 0.01, 'num_input_tokens_seen': 52428800}\n{'loss': 7.7794, 'grad_norm': 1.0165436267852783, 'learning_rate': 0.00019921875000000001, 'epoch': 0.01, 'num_input_tokens_seen': 53477376}\n{'loss': 7.7567, 'grad_norm': 0.8742461204528809, 'learning_rate': 0.00020312500000000002, 'epoch': 0.01, 'num_input_tokens_seen': 54525952}\n{'loss': 7.6489, 'grad_norm': 0.8699902296066284, 'learning_rate': 0.00020703125, 'epoch': 0.01, 'num_input_tokens_seen': 55574528}\n{'loss': 7.6062, 'grad_norm': 0.809831440448761, 'learning_rate': 0.0002109375, 'epoch': 0.01, 'num_input_tokens_seen': 56623104}\n{'loss': 7.5511, 'grad_norm': 0.7423847317695618, 'learning_rate': 0.00021484375, 'epoch': 0.01, 'num_input_tokens_seen': 57671680}\n{'loss': 7.4435, 'grad_norm': 0.7614696025848389, 'learning_rate': 0.00021875, 'epoch': 0.01, 'num_input_tokens_seen': 58720256}\n{'loss': 7.564, 'grad_norm': 0.5147746801376343, 'learning_rate': 0.00022265625, 'epoch': 0.01, 'num_input_tokens_seen': 59768832}\n{'loss': 7.5278, 'grad_norm': 0.4705545902252197, 'learning_rate': 0.0002265625, 'epoch': 0.01, 'num_input_tokens_seen': 60817408}\n{'loss': 7.5479, 'grad_norm': 0.3745419979095459, 'learning_rate': 0.00023046875000000001, 'epoch': 0.01, 'num_input_tokens_seen': 61865984}\n{'loss': 7.4759, 'grad_norm': 0.3893500566482544, 'learning_rate': 0.000234375, 'epoch': 0.01, 'num_input_tokens_seen': 62914560}\n{'loss': 7.5032, 'grad_norm': 0.31959569454193115, 'learning_rate': 0.00023828125, 'epoch': 0.01, 'num_input_tokens_seen': 63963136}\n{'loss': 7.421, 'grad_norm': 0.3203206956386566, 'learning_rate': 0.0002421875, 'epoch': 0.01, 'num_input_tokens_seen': 65011712}\n{'loss': 7.4998, 'grad_norm': 0.2730390429496765, 'learning_rate': 0.00024609375, 'epoch': 0.01, 'num_input_tokens_seen': 66060288}\n{'loss': 7.4157, 'grad_norm': 0.34872403740882874, 'learning_rate': 0.00025, 'epoch': 0.01, 'num_input_tokens_seen': 67108864}\n[2025-03-10 16:17:04 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5ef28452-e903-4bd8-946d-f0c77f558a2a)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk5/example_holdout_4799.jsonl.zst\n[2025-03-10 16:17:04 WARNING] Retrying in 1s [Retry 1/5].\n{'eval_loss': 7.471163749694824, 'eval_runtime': 651.4801, 'eval_samples_per_second': 25.149, 'eval_steps_per_second': 0.196, 'epoch': 0.01, 'num_input_tokens_seen': 67108864}\n{'loss': 7.5083, 'grad_norm': 0.339502215385437, 'learning_rate': 0.00025390625, 'epoch': 0.01, 'num_input_tokens_seen': 68157440}\n{'loss': 7.7083, 'grad_norm': 0.6426190137863159, 'learning_rate': 0.0002578125, 'epoch': 0.01, 'num_input_tokens_seen': 69206016}\n{'loss': 7.446, 'grad_norm': 0.9138129353523254, 'learning_rate': 0.00026171875, 'epoch': 0.01, 'num_input_tokens_seen': 70254592}\n{'loss': 7.3747, 'grad_norm': 1.2179911136627197, 'learning_rate': 0.000265625, 'epoch': 0.01, 'num_input_tokens_seen': 71303168}\n{'loss': 7.367, 'grad_norm': 0.7108445167541504, 'learning_rate': 0.00026953125, 'epoch': 0.01, 'num_input_tokens_seen': 72351744}\n{'loss': 7.4751, 'grad_norm': 0.7580183744430542, 'learning_rate': 0.0002734375, 'epoch': 0.01, 'num_input_tokens_seen': 73400320}\n{'loss': 7.3405, 'grad_norm': 0.7545790076255798, 'learning_rate': 0.00027734375000000003, 'epoch': 0.01, 'num_input_tokens_seen': 74448896}\n{'loss': 7.4194, 'grad_norm': 0.4764443039894104, 'learning_rate': 0.00028125000000000003, 'epoch': 0.01, 'num_input_tokens_seen': 75497472}\n{'loss': 7.2826, 'grad_norm': 0.5942808985710144, 'learning_rate': 0.00028515625, 'epoch': 0.01, 'num_input_tokens_seen': 76546048}\n{'loss': 7.3945, 'grad_norm': 0.5272891521453857, 'learning_rate': 0.0002890625, 'epoch': 0.01, 'num_input_tokens_seen': 77594624}\n{'loss': 7.3492, 'grad_norm': 0.465085506439209, 'learning_rate': 0.00029296875, 'epoch': 0.01, 'num_input_tokens_seen': 78643200}\n{'loss': 7.3658, 'grad_norm': 0.6932719349861145, 'learning_rate': 0.000296875, 'epoch': 0.01, 'num_input_tokens_seen': 79691776}\n{'loss': 7.3554, 'grad_norm': 0.49396172165870667, 'learning_rate': 0.00030078125, 'epoch': 0.01, 'num_input_tokens_seen': 80740352}\n{'loss': 7.2916, 'grad_norm': 0.3178255558013916, 'learning_rate': 0.0003046875, 'epoch': 0.01, 'num_input_tokens_seen': 81788928}\n{'loss': 7.2871, 'grad_norm': 0.5465154647827148, 'learning_rate': 0.00030859375, 'epoch': 0.01, 'num_input_tokens_seen': 82837504}\n{'loss': 7.262, 'grad_norm': 0.4718130826950073, 'learning_rate': 0.0003125, 'epoch': 0.01, 'num_input_tokens_seen': 83886080}\n{'loss': 7.2845, 'grad_norm': 0.5033366680145264, 'learning_rate': 0.00031640625, 'epoch': 0.01, 'num_input_tokens_seen': 84934656}\n{'loss': 7.2525, 'grad_norm': 0.5601146817207336, 'learning_rate': 0.0003203125, 'epoch': 0.01, 'num_input_tokens_seen': 85983232}\n{'loss': 7.1971, 'grad_norm': 0.5764456987380981, 'learning_rate': 0.00032421875, 'epoch': 0.01, 'num_input_tokens_seen': 87031808}\n{'loss': 7.1988, 'grad_norm': 0.6154745817184448, 'learning_rate': 0.000328125, 'epoch': 0.01, 'num_input_tokens_seen': 88080384}\n{'loss': 7.1987, 'grad_norm': 0.6701765656471252, 'learning_rate': 0.00033203125, 'epoch': 0.01, 'num_input_tokens_seen': 89128960}\n{'loss': 7.3324, 'grad_norm': 0.5648972988128662, 'learning_rate': 0.00033593750000000003, 'epoch': 0.01, 'num_input_tokens_seen': 90177536}\n{'loss': 7.2233, 'grad_norm': 0.5782461166381836, 'learning_rate': 0.00033984375000000003, 'epoch': 0.01, 'num_input_tokens_seen': 91226112}\n{'loss': 7.1995, 'grad_norm': 0.540762722492218, 'learning_rate': 0.00034375, 'epoch': 0.01, 'num_input_tokens_seen': 92274688}\n{'loss': 7.1214, 'grad_norm': 0.9524508118629456, 'learning_rate': 0.00034765625, 'epoch': 0.01, 'num_input_tokens_seen': 93323264}\n{'loss': 7.1603, 'grad_norm': 1.4820659160614014, 'learning_rate': 0.0003515625, 'epoch': 0.01, 'num_input_tokens_seen': 94371840}\n{'loss': 7.2364, 'grad_norm': 0.6124428510665894, 'learning_rate': 0.00035546875, 'epoch': 0.01, 'num_input_tokens_seen': 95420416}\n{'loss': 7.0258, 'grad_norm': 0.8897235989570618, 'learning_rate': 0.000359375, 'epoch': 0.01, 'num_input_tokens_seen': 96468992}\n{'loss': 7.1182, 'grad_norm': 0.9263321757316589, 'learning_rate': 0.00036328125, 'epoch': 0.01, 'num_input_tokens_seen': 97517568}\n{'loss': 7.109, 'grad_norm': 0.5800505876541138, 'learning_rate': 0.0003671875, 'epoch': 0.01, 'num_input_tokens_seen': 98566144}\n{'loss': 7.0449, 'grad_norm': 0.6776424050331116, 'learning_rate': 0.00037109375, 'epoch': 0.01, 'num_input_tokens_seen': 99614720}\n{'loss': 7.1272, 'grad_norm': 0.7616431713104248, 'learning_rate': 0.000375, 'epoch': 0.01, 'num_input_tokens_seen': 100663296}\n{'loss': 7.046, 'grad_norm': 0.5346249938011169, 'learning_rate': 0.00037890625, 'epoch': 0.01, 'num_input_tokens_seen': 101711872}\n{'loss': 7.0713, 'grad_norm': 0.6108944416046143, 'learning_rate': 0.0003828125, 'epoch': 0.01, 'num_input_tokens_seen': 102760448}\n{'loss': 7.1459, 'grad_norm': 0.4430749714374542, 'learning_rate': 0.00038671875, 'epoch': 0.01, 'num_input_tokens_seen': 103809024}\n{'loss': 7.0709, 'grad_norm': 0.6020255088806152, 'learning_rate': 0.000390625, 'epoch': 0.01, 'num_input_tokens_seen': 104857600}\n{'loss': 7.0144, 'grad_norm': 0.5525627732276917, 'learning_rate': 0.00039453125, 'epoch': 0.01, 'num_input_tokens_seen': 105906176}\n{'loss': 7.0926, 'grad_norm': 0.6909684538841248, 'learning_rate': 0.00039843750000000003, 'epoch': 0.01, 'num_input_tokens_seen': 106954752}\n{'loss': 7.0289, 'grad_norm': 0.5576740503311157, 'learning_rate': 0.00040234375000000003, 'epoch': 0.01, 'num_input_tokens_seen': 108003328}\n{'loss': 6.9173, 'grad_norm': 0.48874178528785706, 'learning_rate': 0.00040625000000000004, 'epoch': 0.01, 'num_input_tokens_seen': 109051904}\n{'loss': 6.9777, 'grad_norm': 0.3904782831668854, 'learning_rate': 0.00041015625, 'epoch': 0.01, 'num_input_tokens_seen': 110100480}\n{'loss': 6.9473, 'grad_norm': 0.3953755795955658, 'learning_rate': 0.0004140625, 'epoch': 0.01, 'num_input_tokens_seen': 111149056}\n{'loss': 6.9071, 'grad_norm': 0.43107134103775024, 'learning_rate': 0.00041796875, 'epoch': 0.01, 'num_input_tokens_seen': 112197632}\n{'loss': 6.9277, 'grad_norm': 0.33989447355270386, 'learning_rate': 0.000421875, 'epoch': 0.01, 'num_input_tokens_seen': 113246208}\n{'loss': 6.914, 'grad_norm': 0.3267095983028412, 'learning_rate': 0.00042578125, 'epoch': 0.01, 'num_input_tokens_seen': 114294784}\n{'loss': 6.6865, 'grad_norm': 0.4201946556568146, 'learning_rate': 0.0004296875, 'epoch': 0.01, 'num_input_tokens_seen': 115343360}\n{'loss': 6.8229, 'grad_norm': 0.345426082611084, 'learning_rate': 0.00043359375, 'epoch': 0.01, 'num_input_tokens_seen': 116391936}\n{'loss': 6.8599, 'grad_norm': 0.4104400873184204, 'learning_rate': 0.0004375, 'epoch': 0.01, 'num_input_tokens_seen': 117440512}\n{'loss': 6.7656, 'grad_norm': 0.6487549543380737, 'learning_rate': 0.00044140625, 'epoch': 0.01, 'num_input_tokens_seen': 118489088}\n{'loss': 6.8654, 'grad_norm': 1.5497283935546875, 'learning_rate': 0.0004453125, 'epoch': 0.01, 'num_input_tokens_seen': 119537664}\n{'loss': 6.8207, 'grad_norm': 1.9772824048995972, 'learning_rate': 0.00044921875, 'epoch': 0.01, 'num_input_tokens_seen': 120586240}\n{'loss': 6.7802, 'grad_norm': 0.9341455101966858, 'learning_rate': 0.000453125, 'epoch': 0.01, 'num_input_tokens_seen': 121634816}\n{'loss': 6.8017, 'grad_norm': 1.3528856039047241, 'learning_rate': 0.00045703125, 'epoch': 0.01, 'num_input_tokens_seen': 122683392}\n{'loss': 6.8344, 'grad_norm': 0.5852281451225281, 'learning_rate': 0.00046093750000000003, 'epoch': 0.01, 'num_input_tokens_seen': 123731968}\n{'loss': 6.8259, 'grad_norm': 0.9776580929756165, 'learning_rate': 0.00046484375000000003, 'epoch': 0.01, 'num_input_tokens_seen': 124780544}\n{'loss': 6.7581, 'grad_norm': 1.0398296117782593, 'learning_rate': 0.00046875, 'epoch': 0.01, 'num_input_tokens_seen': 125829120}\n{'loss': 6.7795, 'grad_norm': 1.1206268072128296, 'learning_rate': 0.00047265625, 'epoch': 0.01, 'num_input_tokens_seen': 126877696}\n{'loss': 6.5667, 'grad_norm': 0.6790318489074707, 'learning_rate': 0.0004765625, 'epoch': 0.01, 'num_input_tokens_seen': 127926272}\n{'loss': 6.7297, 'grad_norm': 1.2275055646896362, 'learning_rate': 0.00048046875, 'epoch': 0.02, 'num_input_tokens_seen': 128974848}\n{'loss': 6.7104, 'grad_norm': 1.1354466676712036, 'learning_rate': 0.000484375, 'epoch': 0.02, 'num_input_tokens_seen': 130023424}\n{'loss': 6.7025, 'grad_norm': 0.9035728573799133, 'learning_rate': 0.00048828125, 'epoch': 0.02, 'num_input_tokens_seen': 131072000}\n{'loss': 6.6391, 'grad_norm': 1.3942680358886719, 'learning_rate': 0.0004921875, 'epoch': 0.02, 'num_input_tokens_seen': 132120576}\n{'loss': 6.6011, 'grad_norm': 0.7435236573219299, 'learning_rate': 0.00049609375, 'epoch': 0.02, 'num_input_tokens_seen': 133169152}\n{'loss': 6.5135, 'grad_norm': 0.5970368385314941, 'learning_rate': 0.0005, 'epoch': 0.02, 'num_input_tokens_seen': 134217728}\n{'eval_loss': 6.573822021484375, 'eval_runtime': 629.9441, 'eval_samples_per_second': 26.009, 'eval_steps_per_second': 0.203, 'epoch': 0.02, 'num_input_tokens_seen': 134217728}\n{'loss': 6.5509, 'grad_norm': 0.7936264276504517, 'learning_rate': 0.00050390625, 'epoch': 0.02, 'num_input_tokens_seen': 135266304}\n{'loss': 6.6008, 'grad_norm': 0.6225885152816772, 'learning_rate': 0.0005078125, 'epoch': 0.02, 'num_input_tokens_seen': 136314880}\n{'loss': 6.4821, 'grad_norm': 0.5519376993179321, 'learning_rate': 0.00051171875, 'epoch': 0.02, 'num_input_tokens_seen': 137363456}\n{'loss': 6.3411, 'grad_norm': 0.5908603668212891, 'learning_rate': 0.000515625, 'epoch': 0.02, 'num_input_tokens_seen': 138412032}\n{'loss': 6.3464, 'grad_norm': 0.5101401209831238, 'learning_rate': 0.00051953125, 'epoch': 0.02, 'num_input_tokens_seen': 139460608}\n{'loss': 6.3638, 'grad_norm': 0.7352246046066284, 'learning_rate': 0.0005234375, 'epoch': 0.02, 'num_input_tokens_seen': 140509184}\n{'loss': 6.3429, 'grad_norm': 0.49651673436164856, 'learning_rate': 0.00052734375, 'epoch': 0.02, 'num_input_tokens_seen': 141557760}\n{'loss': 6.2987, 'grad_norm': 0.4835755527019501, 'learning_rate': 0.00053125, 'epoch': 0.02, 'num_input_tokens_seen': 142606336}\n{'loss': 6.2982, 'grad_norm': 0.5940163731575012, 'learning_rate': 0.00053515625, 'epoch': 0.02, 'num_input_tokens_seen': 143654912}\n{'loss': 6.267, 'grad_norm': 0.7658674120903015, 'learning_rate': 0.0005390625, 'epoch': 0.02, 'num_input_tokens_seen': 144703488}\n{'loss': 6.2102, 'grad_norm': 0.6704416275024414, 'learning_rate': 0.00054296875, 'epoch': 0.02, 'num_input_tokens_seen': 145752064}\n{'loss': 6.1956, 'grad_norm': 0.6615312099456787, 'learning_rate': 0.000546875, 'epoch': 0.02, 'num_input_tokens_seen': 146800640}\n{'loss': 6.286, 'grad_norm': 0.7957404255867004, 'learning_rate': 0.0005507812500000001, 'epoch': 0.02, 'num_input_tokens_seen': 147849216}\n{'loss': 6.2483, 'grad_norm': 0.6477276682853699, 'learning_rate': 0.0005546875000000001, 'epoch': 0.02, 'num_input_tokens_seen': 148897792}\n{'loss': 6.0944, 'grad_norm': 0.5753227472305298, 'learning_rate': 0.0005585937500000001, 'epoch': 0.02, 'num_input_tokens_seen': 149946368}\n{'loss': 6.0995, 'grad_norm': 0.5871054530143738, 'learning_rate': 0.0005625000000000001, 'epoch': 0.02, 'num_input_tokens_seen': 150994944}\n{'loss': 6.112, 'grad_norm': 0.7046136856079102, 'learning_rate': 0.00056640625, 'epoch': 0.02, 'num_input_tokens_seen': 152043520}\n{'loss': 6.102, 'grad_norm': 0.9357424378395081, 'learning_rate': 0.0005703125, 'epoch': 0.02, 'num_input_tokens_seen': 153092096}\n{'loss': 6.1407, 'grad_norm': 1.0577837228775024, 'learning_rate': 0.00057421875, 'epoch': 0.02, 'num_input_tokens_seen': 154140672}\n{'loss': 5.9836, 'grad_norm': 0.7795257568359375, 'learning_rate': 0.000578125, 'epoch': 0.02, 'num_input_tokens_seen': 155189248}\n{'loss': 6.1041, 'grad_norm': 0.8117634057998657, 'learning_rate': 0.00058203125, 'epoch': 0.02, 'num_input_tokens_seen': 156237824}\n{'loss': 5.9474, 'grad_norm': 0.8311094045639038, 'learning_rate': 0.0005859375, 'epoch': 0.02, 'num_input_tokens_seen': 157286400}\n{'loss': 5.9365, 'grad_norm': 0.8269851803779602, 'learning_rate': 0.00058984375, 'epoch': 0.02, 'num_input_tokens_seen': 158334976}\n{'loss': 5.9668, 'grad_norm': 0.701510488986969, 'learning_rate': 0.00059375, 'epoch': 0.02, 'num_input_tokens_seen': 159383552}\n{'loss': 5.9874, 'grad_norm': 0.49938252568244934, 'learning_rate': 0.00059765625, 'epoch': 0.02, 'num_input_tokens_seen': 160432128}\n{'loss': 5.8505, 'grad_norm': 0.6981683969497681, 'learning_rate': 0.0006015625, 'epoch': 0.02, 'num_input_tokens_seen': 161480704}\n{'loss': 6.0156, 'grad_norm': 0.5023297071456909, 'learning_rate': 0.00060546875, 'epoch': 0.02, 'num_input_tokens_seen': 162529280}\n{'loss': 5.8299, 'grad_norm': 0.6075630187988281, 'learning_rate': 0.000609375, 'epoch': 0.02, 'num_input_tokens_seen': 163577856}\n{'loss': 5.8203, 'grad_norm': 0.6051607728004456, 'learning_rate': 0.00061328125, 'epoch': 0.02, 'num_input_tokens_seen': 164626432}\n{'loss': 5.7705, 'grad_norm': 0.6384783983230591, 'learning_rate': 0.0006171875, 'epoch': 0.02, 'num_input_tokens_seen': 165675008}\n{'loss': 5.791, 'grad_norm': 0.5084705948829651, 'learning_rate': 0.00062109375, 'epoch': 0.02, 'num_input_tokens_seen': 166723584}\n{'loss': 5.6743, 'grad_norm': 0.4278322160243988, 'learning_rate': 0.000625, 'epoch': 0.02, 'num_input_tokens_seen': 167772160}\n{'loss': 5.7112, 'grad_norm': 0.5151192545890808, 'learning_rate': 0.00062890625, 'epoch': 0.02, 'num_input_tokens_seen': 168820736}\n{'loss': 5.5128, 'grad_norm': 0.6542677283287048, 'learning_rate': 0.0006328125, 'epoch': 0.02, 'num_input_tokens_seen': 169869312}\n{'loss': 5.6735, 'grad_norm': 0.6016008257865906, 'learning_rate': 0.00063671875, 'epoch': 0.02, 'num_input_tokens_seen': 170917888}\n{'loss': 5.6525, 'grad_norm': 0.48695647716522217, 'learning_rate': 0.000640625, 'epoch': 0.02, 'num_input_tokens_seen': 171966464}\n{'loss': 5.6051, 'grad_norm': 0.5894989371299744, 'learning_rate': 0.00064453125, 'epoch': 0.02, 'num_input_tokens_seen': 173015040}\n{'loss': 5.6377, 'grad_norm': 0.7626883387565613, 'learning_rate': 0.0006484375, 'epoch': 0.02, 'num_input_tokens_seen': 174063616}\n{'loss': 5.6038, 'grad_norm': 0.745198130607605, 'learning_rate': 0.00065234375, 'epoch': 0.02, 'num_input_tokens_seen': 175112192}\n{'loss': 5.5465, 'grad_norm': 0.7876908779144287, 'learning_rate': 0.00065625, 'epoch': 0.02, 'num_input_tokens_seen': 176160768}\n{'loss': 5.5903, 'grad_norm': 0.7416785359382629, 'learning_rate': 0.00066015625, 'epoch': 0.02, 'num_input_tokens_seen': 177209344}\n{'loss': 5.4993, 'grad_norm': 0.4493878185749054, 'learning_rate': 0.0006640625, 'epoch': 0.02, 'num_input_tokens_seen': 178257920}\n{'loss': 5.5612, 'grad_norm': 0.5095419883728027, 'learning_rate': 0.00066796875, 'epoch': 0.02, 'num_input_tokens_seen': 179306496}\n{'loss': 5.378, 'grad_norm': 0.6330733895301819, 'learning_rate': 0.0006718750000000001, 'epoch': 0.02, 'num_input_tokens_seen': 180355072}\n{'loss': 5.4875, 'grad_norm': 0.4710595905780792, 'learning_rate': 0.0006757812500000001, 'epoch': 0.02, 'num_input_tokens_seen': 181403648}\n{'loss': 5.4221, 'grad_norm': 0.5276287198066711, 'learning_rate': 0.0006796875000000001, 'epoch': 0.02, 'num_input_tokens_seen': 182452224}\n{'loss': 5.308, 'grad_norm': 0.6985499858856201, 'learning_rate': 0.0006835937500000001, 'epoch': 0.02, 'num_input_tokens_seen': 183500800}\n{'loss': 5.4455, 'grad_norm': 0.4874110519886017, 'learning_rate': 0.0006875, 'epoch': 0.02, 'num_input_tokens_seen': 184549376}\n{'loss': 5.476, 'grad_norm': 0.5807638764381409, 'learning_rate': 0.00069140625, 'epoch': 0.02, 'num_input_tokens_seen': 185597952}\n{'loss': 5.2876, 'grad_norm': 0.5431288480758667, 'learning_rate': 0.0006953125, 'epoch': 0.02, 'num_input_tokens_seen': 186646528}\n{'loss': 5.3881, 'grad_norm': 0.7681945562362671, 'learning_rate': 0.00069921875, 'epoch': 0.02, 'num_input_tokens_seen': 187695104}\n{'loss': 5.4006, 'grad_norm': 0.7372023463249207, 'learning_rate': 0.000703125, 'epoch': 0.02, 'num_input_tokens_seen': 188743680}\n{'loss': 5.3813, 'grad_norm': 0.7354347109794617, 'learning_rate': 0.00070703125, 'epoch': 0.02, 'num_input_tokens_seen': 189792256}\n{'loss': 5.3393, 'grad_norm': 0.5908933281898499, 'learning_rate': 0.0007109375, 'epoch': 0.02, 'num_input_tokens_seen': 190840832}\n{'loss': 5.3024, 'grad_norm': 0.5665153861045837, 'learning_rate': 0.00071484375, 'epoch': 0.02, 'num_input_tokens_seen': 191889408}\n{'loss': 5.2782, 'grad_norm': 0.5930947661399841, 'learning_rate': 0.00071875, 'epoch': 0.02, 'num_input_tokens_seen': 192937984}\n{'loss': 5.3199, 'grad_norm': 0.5926457643508911, 'learning_rate': 0.00072265625, 'epoch': 0.02, 'num_input_tokens_seen': 193986560}\n{'loss': 5.2949, 'grad_norm': 0.548610270023346, 'learning_rate': 0.0007265625, 'epoch': 0.02, 'num_input_tokens_seen': 195035136}\n{'loss': 5.3143, 'grad_norm': 0.6023995280265808, 'learning_rate': 0.00073046875, 'epoch': 0.02, 'num_input_tokens_seen': 196083712}\n{'loss': 5.2982, 'grad_norm': 1.0335254669189453, 'learning_rate': 0.000734375, 'epoch': 0.02, 'num_input_tokens_seen': 197132288}\n{'loss': 5.2933, 'grad_norm': 1.2596269845962524, 'learning_rate': 0.00073828125, 'epoch': 0.02, 'num_input_tokens_seen': 198180864}\n{'loss': 5.2524, 'grad_norm': 0.6956535577774048, 'learning_rate': 0.0007421875, 'epoch': 0.02, 'num_input_tokens_seen': 199229440}\n{'loss': 5.3543, 'grad_norm': 0.946761429309845, 'learning_rate': 0.00074609375, 'epoch': 0.02, 'num_input_tokens_seen': 200278016}\n{'loss': 5.1616, 'grad_norm': 0.9568974375724792, 'learning_rate': 0.00075, 'epoch': 0.02, 'num_input_tokens_seen': 201326592}\n[2025-03-10 18:01:06 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ba6e4c51-f4a4-407e-9934-3772550b7ce9)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk1/example_holdout_2770.jsonl.zst\n[2025-03-10 18:01:06 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-10 18:02:30 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: bdf2cfaa-7e0b-46a0-bec1-b1e573fa7998)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_4386.jsonl.zst\n[2025-03-10 18:02:30 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-10 18:02:44 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1dc5e455-8042-4c7b-9b97-5ded33dfea34)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk1/example_holdout_1763.jsonl.zst\n[2025-03-10 18:02:44 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-10 18:04:45 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9cf29917-8111-41fe-80aa-953df65c5803)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_5509.jsonl.zst\n[2025-03-10 18:04:45 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-10 18:05:26 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2515a0b0-3d81-409f-940c-e78ed5e2dbf8)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_3093.jsonl.zst\n[2025-03-10 18:05:26 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-10 18:06:39 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a4c1e0c7-1c7a-4377-bc7e-6f076473072b)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_3422.jsonl.zst\n[2025-03-10 18:06:39 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-10 18:07:37 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c7b0d366-db86-4d0c-a4e0-be251d26519e)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_2250.jsonl.zst\n[2025-03-10 18:07:37 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-10 18:09:23 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b0df5a1a-4836-46cf-8e45-58a7c1553309)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_6161.jsonl.zst\n[2025-03-10 18:09:23 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-10 18:09:44 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c1d97368-c0ae-45bb-ae10-5559b3ebc4e4)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_5782.jsonl.zst\n[2025-03-10 18:09:44 WARNING] Retrying in 1s [Retry 1/5].\n{'eval_loss': 5.276012420654297, 'eval_runtime': 754.8295, 'eval_samples_per_second': 21.706, 'eval_steps_per_second': 0.17, 'epoch': 0.02, 'num_input_tokens_seen': 201326592}\n{'loss': 5.2363, 'grad_norm': 0.8435476422309875, 'learning_rate': 0.00075390625, 'epoch': 0.02, 'num_input_tokens_seen': 202375168}\n{'loss': 5.1035, 'grad_norm': 1.1267820596694946, 'learning_rate': 0.0007578125, 'epoch': 0.02, 'num_input_tokens_seen': 203423744}\n{'loss': 5.3017, 'grad_norm': 0.8555666208267212, 'learning_rate': 0.00076171875, 'epoch': 0.02, 'num_input_tokens_seen': 204472320}\n{'loss': 5.1679, 'grad_norm': 0.7608171105384827, 'learning_rate': 0.000765625, 'epoch': 0.02, 'num_input_tokens_seen': 205520896}\n{'loss': 5.2326, 'grad_norm': 0.6787221431732178, 'learning_rate': 0.00076953125, 'epoch': 0.02, 'num_input_tokens_seen': 206569472}\n{'loss': 5.144, 'grad_norm': 0.6404955983161926, 'learning_rate': 0.0007734375, 'epoch': 0.02, 'num_input_tokens_seen': 207618048}\n{'loss': 5.1933, 'grad_norm': 0.6099393367767334, 'learning_rate': 0.00077734375, 'epoch': 0.02, 'num_input_tokens_seen': 208666624}\n{'loss': 5.0498, 'grad_norm': 0.5971768498420715, 'learning_rate': 0.00078125, 'epoch': 0.02, 'num_input_tokens_seen': 209715200}\n{'loss': 5.1443, 'grad_norm': 0.642633318901062, 'learning_rate': 0.00078515625, 'epoch': 0.02, 'num_input_tokens_seen': 210763776}\n{'loss': 5.2125, 'grad_norm': 0.706398606300354, 'learning_rate': 0.0007890625, 'epoch': 0.02, 'num_input_tokens_seen': 211812352}\n{'loss': 5.1882, 'grad_norm': 0.817449688911438, 'learning_rate': 0.00079296875, 'epoch': 0.02, 'num_input_tokens_seen': 212860928}\n{'loss': 5.0905, 'grad_norm': 0.9392185807228088, 'learning_rate': 0.0007968750000000001, 'epoch': 0.02, 'num_input_tokens_seen': 213909504}\n{'loss': 5.059, 'grad_norm': 0.5305852890014648, 'learning_rate': 0.0008007812500000001, 'epoch': 0.03, 'num_input_tokens_seen': 214958080}\n{'loss': 5.0838, 'grad_norm': 0.7662672996520996, 'learning_rate': 0.0008046875000000001, 'epoch': 0.03, 'num_input_tokens_seen': 216006656}\n{'loss': 5.0112, 'grad_norm': 0.5768160223960876, 'learning_rate': 0.0008085937500000001, 'epoch': 0.03, 'num_input_tokens_seen': 217055232}\n{'loss': 4.9684, 'grad_norm': 0.5972586870193481, 'learning_rate': 0.0008125000000000001, 'epoch': 0.03, 'num_input_tokens_seen': 218103808}\n{'loss': 5.0764, 'grad_norm': 0.559498131275177, 'learning_rate': 0.00081640625, 'epoch': 0.03, 'num_input_tokens_seen': 219152384}\n{'loss': 5.0117, 'grad_norm': 0.555585503578186, 'learning_rate': 0.0008203125, 'epoch': 0.03, 'num_input_tokens_seen': 220200960}\n{'loss': 5.1955, 'grad_norm': 0.6180793046951294, 'learning_rate': 0.00082421875, 'epoch': 0.03, 'num_input_tokens_seen': 221249536}\n{'loss': 5.1265, 'grad_norm': 0.5784006118774414, 'learning_rate': 0.000828125, 'epoch': 0.03, 'num_input_tokens_seen': 222298112}\n{'loss': 5.03, 'grad_norm': 0.5200456380844116, 'learning_rate': 0.00083203125, 'epoch': 0.03, 'num_input_tokens_seen': 223346688}\n{'loss': 5.051, 'grad_norm': 0.5112505555152893, 'learning_rate': 0.0008359375, 'epoch': 0.03, 'num_input_tokens_seen': 224395264}\n{'loss': 5.0994, 'grad_norm': 0.44979697465896606, 'learning_rate': 0.00083984375, 'epoch': 0.03, 'num_input_tokens_seen': 225443840}\n{'loss': 4.94, 'grad_norm': 0.46642380952835083, 'learning_rate': 0.00084375, 'epoch': 0.03, 'num_input_tokens_seen': 226492416}\n{'loss': 5.0562, 'grad_norm': 0.49667519330978394, 'learning_rate': 0.00084765625, 'epoch': 0.03, 'num_input_tokens_seen': 227540992}\n{'loss': 4.9217, 'grad_norm': 0.4302496314048767, 'learning_rate': 0.0008515625, 'epoch': 0.03, 'num_input_tokens_seen': 228589568}\n{'loss': 4.8588, 'grad_norm': 0.5326887369155884, 'learning_rate': 0.00085546875, 'epoch': 0.03, 'num_input_tokens_seen': 229638144}\n{'loss': 4.8501, 'grad_norm': 0.45604026317596436, 'learning_rate': 0.000859375, 'epoch': 0.03, 'num_input_tokens_seen': 230686720}\n{'loss': 4.8774, 'grad_norm': 0.4497997462749481, 'learning_rate': 0.00086328125, 'epoch': 0.03, 'num_input_tokens_seen': 231735296}\n{'loss': 5.0143, 'grad_norm': 0.526670515537262, 'learning_rate': 0.0008671875, 'epoch': 0.03, 'num_input_tokens_seen': 232783872}\n{'loss': 4.9512, 'grad_norm': 0.5823948979377747, 'learning_rate': 0.00087109375, 'epoch': 0.03, 'num_input_tokens_seen': 233832448}\n{'loss': 4.915, 'grad_norm': 0.6516634821891785, 'learning_rate': 0.000875, 'epoch': 0.03, 'num_input_tokens_seen': 234881024}\n{'loss': 4.9318, 'grad_norm': 0.7564677596092224, 'learning_rate': 0.00087890625, 'epoch': 0.03, 'num_input_tokens_seen': 235929600}\n{'loss': 4.9041, 'grad_norm': 0.7170491814613342, 'learning_rate': 0.0008828125, 'epoch': 0.03, 'num_input_tokens_seen': 236978176}\n{'loss': 4.9727, 'grad_norm': 0.7671059966087341, 'learning_rate': 0.00088671875, 'epoch': 0.03, 'num_input_tokens_seen': 238026752}\n{'loss': 4.7895, 'grad_norm': 0.8752806782722473, 'learning_rate': 0.000890625, 'epoch': 0.03, 'num_input_tokens_seen': 239075328}\n{'loss': 4.8845, 'grad_norm': 0.8313667178153992, 'learning_rate': 0.00089453125, 'epoch': 0.03, 'num_input_tokens_seen': 240123904}\n{'loss': 4.8325, 'grad_norm': 0.9223323464393616, 'learning_rate': 0.0008984375, 'epoch': 0.03, 'num_input_tokens_seen': 241172480}\n{'loss': 4.8991, 'grad_norm': 0.7362072467803955, 'learning_rate': 0.00090234375, 'epoch': 0.03, 'num_input_tokens_seen': 242221056}\n{'loss': 4.7443, 'grad_norm': 0.6667400598526001, 'learning_rate': 0.00090625, 'epoch': 0.03, 'num_input_tokens_seen': 243269632}\n{'loss': 4.8913, 'grad_norm': 0.5431771874427795, 'learning_rate': 0.00091015625, 'epoch': 0.03, 'num_input_tokens_seen': 244318208}\n{'loss': 4.8997, 'grad_norm': 0.5542160272598267, 'learning_rate': 0.0009140625, 'epoch': 0.03, 'num_input_tokens_seen': 245366784}\n{'loss': 4.8448, 'grad_norm': 0.6110911965370178, 'learning_rate': 0.0009179687500000001, 'epoch': 0.03, 'num_input_tokens_seen': 246415360}\n{'loss': 4.7975, 'grad_norm': 0.5550041794776917, 'learning_rate': 0.0009218750000000001, 'epoch': 0.03, 'num_input_tokens_seen': 247463936}\n{'loss': 4.87, 'grad_norm': 0.4778221547603607, 'learning_rate': 0.0009257812500000001, 'epoch': 0.03, 'num_input_tokens_seen': 248512512}\n{'loss': 4.7594, 'grad_norm': 0.35899603366851807, 'learning_rate': 0.0009296875000000001, 'epoch': 0.03, 'num_input_tokens_seen': 249561088}\n{'loss': 4.8338, 'grad_norm': 0.494094580411911, 'learning_rate': 0.0009335937500000001, 'epoch': 0.03, 'num_input_tokens_seen': 250609664}\n{'loss': 4.7424, 'grad_norm': 0.4671477675437927, 'learning_rate': 0.0009375, 'epoch': 0.03, 'num_input_tokens_seen': 251658240}\n{'loss': 4.7593, 'grad_norm': 0.4691649079322815, 'learning_rate': 0.00094140625, 'epoch': 0.03, 'num_input_tokens_seen': 252706816}\n{'loss': 4.7869, 'grad_norm': 0.6212939023971558, 'learning_rate': 0.0009453125, 'epoch': 0.03, 'num_input_tokens_seen': 253755392}\n{'loss': 4.7925, 'grad_norm': 0.621306300163269, 'learning_rate': 0.00094921875, 'epoch': 0.03, 'num_input_tokens_seen': 254803968}\n{'loss': 4.7714, 'grad_norm': 0.6991429328918457, 'learning_rate': 0.000953125, 'epoch': 0.03, 'num_input_tokens_seen': 255852544}\n{'loss': 5.2726, 'grad_norm': 1.016664743423462, 'learning_rate': 0.00095703125, 'epoch': 0.03, 'num_input_tokens_seen': 256901120}\n{'loss': 4.9125, 'grad_norm': 1.3091747760772705, 'learning_rate': 0.0009609375, 'epoch': 0.03, 'num_input_tokens_seen': 257949696}\n{'loss': 4.839, 'grad_norm': 1.2617076635360718, 'learning_rate': 0.00096484375, 'epoch': 0.03, 'num_input_tokens_seen': 258998272}\n{'loss': 4.8412, 'grad_norm': 0.9403041005134583, 'learning_rate': 0.00096875, 'epoch': 0.03, 'num_input_tokens_seen': 260046848}\n{'loss': 5.0193, 'grad_norm': 0.9802642464637756, 'learning_rate': 0.00097265625, 'epoch': 0.03, 'num_input_tokens_seen': 261095424}\n{'loss': 4.7372, 'grad_norm': 0.9636861085891724, 'learning_rate': 0.0009765625, 'epoch': 0.03, 'num_input_tokens_seen': 262144000}\n{'loss': 4.7878, 'grad_norm': 0.7803710699081421, 'learning_rate': 0.00098046875, 'epoch': 0.03, 'num_input_tokens_seen': 263192576}\n{'loss': 4.8126, 'grad_norm': 0.7087182402610779, 'learning_rate': 0.000984375, 'epoch': 0.03, 'num_input_tokens_seen': 264241152}\n{'loss': 4.7252, 'grad_norm': 0.7220279574394226, 'learning_rate': 0.00098828125, 'epoch': 0.03, 'num_input_tokens_seen': 265289728}\n{'loss': 4.7419, 'grad_norm': 0.6956494450569153, 'learning_rate': 0.0009921875, 'epoch': 0.03, 'num_input_tokens_seen': 266338304}\n{'loss': 4.8041, 'grad_norm': 0.8009976148605347, 'learning_rate': 0.00099609375, 'epoch': 0.03, 'num_input_tokens_seen': 267386880}\n{'loss': 4.7016, 'grad_norm': 0.6665300130844116, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 268435456}\n{'eval_loss': 4.753816604614258, 'eval_runtime': 661.8529, 'eval_samples_per_second': 24.755, 'eval_steps_per_second': 0.193, 'epoch': 0.03, 'num_input_tokens_seen': 268435456}\n{'loss': 4.6762, 'grad_norm': 0.5311985611915588, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 269484032}\n{'loss': 4.6296, 'grad_norm': 0.5160760879516602, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 270532608}\n{'loss': 4.7422, 'grad_norm': 0.5964047312736511, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 271581184}\n{'loss': 4.7396, 'grad_norm': 0.4793979227542877, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 272629760}\n{'loss': 4.733, 'grad_norm': 0.5280688405036926, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 273678336}\n{'loss': 4.9591, 'grad_norm': 0.8669152855873108, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 274726912}\n{'loss': 4.7953, 'grad_norm': 0.8417720198631287, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 275775488}\n{'loss': 4.7972, 'grad_norm': 0.9349585175514221, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 276824064}\n{'loss': 4.7233, 'grad_norm': 0.8441230654716492, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 277872640}\n{'loss': 4.8032, 'grad_norm': 0.7163352370262146, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 278921216}\n{'loss': 4.4369, 'grad_norm': 1.0364480018615723, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 279969792}\n{'loss': 4.557, 'grad_norm': 1.012042760848999, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 281018368}\n{'loss': 4.7696, 'grad_norm': 1.1818541288375854, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 282066944}\n{'loss': 4.7835, 'grad_norm': 0.8296499848365784, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 283115520}\n{'loss': 4.761, 'grad_norm': 0.6920194625854492, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 284164096}\n{'loss': 4.6239, 'grad_norm': 0.8495435118675232, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 285212672}\n{'loss': 4.6914, 'grad_norm': 0.6536931991577148, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 286261248}\n{'loss': 4.776, 'grad_norm': 0.7161967754364014, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 287309824}\n{'loss': 4.7096, 'grad_norm': 0.5441194176673889, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 288358400}\n{'loss': 4.7278, 'grad_norm': 0.5437328219413757, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 289406976}\n{'loss': 4.6126, 'grad_norm': 0.49404028058052063, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 290455552}\n{'loss': 4.6594, 'grad_norm': 0.4274217188358307, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 291504128}\n{'loss': 4.6365, 'grad_norm': 0.48871853947639465, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 292552704}\n{'loss': 4.5999, 'grad_norm': 0.5101707577705383, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 293601280}\n{'loss': 4.5869, 'grad_norm': 0.4579870104789734, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 294649856}\n{'loss': 4.5993, 'grad_norm': 0.44694098830223083, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 295698432}\n{'loss': 4.6369, 'grad_norm': 0.42955130338668823, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 296747008}\n{'loss': 4.5973, 'grad_norm': 0.532283365726471, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 297795584}\n{'loss': 4.3953, 'grad_norm': 0.5553389191627502, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 298844160}\n{'loss': 4.5501, 'grad_norm': 0.4733176529407501, 'learning_rate': 0.001, 'epoch': 0.03, 'num_input_tokens_seen': 299892736}\n{'loss': 4.4896, 'grad_norm': 0.5510519742965698, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 300941312}\n{'loss': 4.348, 'grad_norm': 0.5312983393669128, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 301989888}\n{'loss': 4.4, 'grad_norm': 0.4173823297023773, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 303038464}\n{'loss': 4.4971, 'grad_norm': 0.4799824357032776, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 304087040}\n{'loss': 4.5507, 'grad_norm': 0.4494017958641052, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 305135616}\n{'loss': 4.5655, 'grad_norm': 0.36501485109329224, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 306184192}\n{'loss': 4.5189, 'grad_norm': 0.4833853840827942, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 307232768}\n{'loss': 4.5387, 'grad_norm': 0.5214531421661377, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 308281344}\n{'loss': 4.5509, 'grad_norm': 0.5383253693580627, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 309329920}\n{'loss': 4.4112, 'grad_norm': 0.5364778637886047, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 310378496}\n{'loss': 4.568, 'grad_norm': 0.3624066114425659, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 311427072}\n{'loss': 4.5289, 'grad_norm': 0.5469081401824951, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 312475648}\n{'loss': 4.4953, 'grad_norm': 0.5212593674659729, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 313524224}\n{'loss': 4.4614, 'grad_norm': 0.36742305755615234, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 314572800}\n{'loss': 4.4757, 'grad_norm': 0.43591663241386414, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 315621376}\n{'loss': 4.5321, 'grad_norm': 0.483548104763031, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 316669952}\n{'loss': 4.449, 'grad_norm': 0.3971082866191864, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 317718528}\n{'loss': 4.4539, 'grad_norm': 0.3416251540184021, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 318767104}\n{'loss': 4.3456, 'grad_norm': 0.45731472969055176, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 319815680}\n{'loss': 4.4179, 'grad_norm': 0.4462226331233978, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 320864256}\n{'loss': 4.3691, 'grad_norm': 0.3393065631389618, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 321912832}\n{'loss': 4.4361, 'grad_norm': 0.39659640192985535, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 322961408}\n{'loss': 4.4166, 'grad_norm': 0.42212849855422974, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 324009984}\n{'loss': 4.3931, 'grad_norm': 0.3403238356113434, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 325058560}\n{'loss': 4.3003, 'grad_norm': 0.3405858278274536, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 326107136}\n{'loss': 4.4339, 'grad_norm': 0.42516669631004333, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 327155712}\n{'loss': 4.4258, 'grad_norm': 0.4387160539627075, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 328204288}\n{'loss': 4.3774, 'grad_norm': 0.3546140193939209, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 329252864}\n{'loss': 4.3261, 'grad_norm': 0.3842155933380127, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 330301440}\n{'loss': 4.2843, 'grad_norm': 0.32807183265686035, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 331350016}\n{'loss': 4.3627, 'grad_norm': 0.3635430932044983, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 332398592}\n{'loss': 4.3304, 'grad_norm': 0.32113364338874817, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 333447168}\n{'loss': 4.258, 'grad_norm': 0.3261938989162445, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 334495744}\n{'loss': 4.392, 'grad_norm': 0.35287028551101685, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 335544320}\n{'eval_loss': 4.340233325958252, 'eval_runtime': 641.4064, 'eval_samples_per_second': 25.544, 'eval_steps_per_second': 0.2, 'epoch': 0.04, 'num_input_tokens_seen': 335544320}\n{'loss': 4.4095, 'grad_norm': 0.30875736474990845, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 336592896}\n{'loss': 3.8896, 'grad_norm': 0.6334038972854614, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 337641472}\n{'loss': 4.449, 'grad_norm': 0.5519331693649292, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 338690048}\n{'loss': 4.4388, 'grad_norm': 0.4262654185295105, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 339738624}\n{'loss': 4.3918, 'grad_norm': 0.4348645508289337, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 340787200}\n{'loss': 4.3677, 'grad_norm': 0.3858915865421295, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 341835776}\n{'loss': 4.3343, 'grad_norm': 0.4542510509490967, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 342884352}\n{'loss': 4.3196, 'grad_norm': 0.4413583278656006, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 343932928}\n{'loss': 4.322, 'grad_norm': 0.5200892686843872, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 344981504}\n{'loss': 4.2409, 'grad_norm': 0.4969848692417145, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 346030080}\n{'loss': 4.2263, 'grad_norm': 0.43436068296432495, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 347078656}\n{'loss': 4.2271, 'grad_norm': 0.4760046899318695, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 348127232}\n{'loss': 4.3567, 'grad_norm': 0.43881112337112427, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 349175808}\n{'loss': 4.2606, 'grad_norm': 0.5361112952232361, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 350224384}\n{'loss': 4.3831, 'grad_norm': 0.5959597229957581, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 351272960}\n{'loss': 4.2899, 'grad_norm': 0.6709368824958801, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 352321536}\n{'loss': 4.2263, 'grad_norm': 0.6585149168968201, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 353370112}\n{'loss': 4.3428, 'grad_norm': 0.5447191596031189, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 354418688}\n{'loss': 4.3642, 'grad_norm': 0.576545238494873, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 355467264}\n{'loss': 4.025, 'grad_norm': 0.7567218542098999, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 356515840}\n{'loss': 4.2593, 'grad_norm': 0.6053742170333862, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 357564416}\n{'loss': 4.2864, 'grad_norm': 0.54949551820755, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 358612992}\n{'loss': 4.3183, 'grad_norm': 0.4792100489139557, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 359661568}\n{'loss': 4.2957, 'grad_norm': 0.4366244077682495, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 360710144}\n{'loss': 4.3502, 'grad_norm': 0.5610309839248657, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 361758720}\n{'loss': 4.2673, 'grad_norm': 0.42132946848869324, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 362807296}\n{'loss': 4.2565, 'grad_norm': 0.45927727222442627, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 363855872}\n{'loss': 4.3009, 'grad_norm': 0.40793168544769287, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 364904448}\n{'loss': 4.2584, 'grad_norm': 0.3818293511867523, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 365953024}\n{'loss': 4.3187, 'grad_norm': 0.4942944645881653, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 367001600}\n{'loss': 4.2056, 'grad_norm': 0.5316190719604492, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 368050176}\n{'loss': 4.2403, 'grad_norm': 0.4738222658634186, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 369098752}\n{'loss': 4.244, 'grad_norm': 0.41153445839881897, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 370147328}\n{'loss': 4.2876, 'grad_norm': 0.35864201188087463, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 371195904}\n{'loss': 4.2457, 'grad_norm': 0.4317127466201782, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 372244480}\n{'loss': 4.2138, 'grad_norm': 0.4922076165676117, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 373293056}\n{'loss': 4.1875, 'grad_norm': 0.5150508880615234, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 374341632}\n{'loss': 4.1485, 'grad_norm': 0.40701162815093994, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 375390208}\n{'loss': 4.1062, 'grad_norm': 0.40378910303115845, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 376438784}\n{'loss': 4.226, 'grad_norm': 0.4435281753540039, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 377487360}\n{'loss': 4.2034, 'grad_norm': 0.37908127903938293, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 378535936}\n{'loss': 4.1502, 'grad_norm': 0.408202588558197, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 379584512}\n{'loss': 4.1623, 'grad_norm': 0.4542413651943207, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 380633088}\n{'loss': 4.206, 'grad_norm': 0.5084658861160278, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 381681664}\n{'loss': 4.1867, 'grad_norm': 0.432908833026886, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 382730240}\n{'loss': 4.2377, 'grad_norm': 0.38273656368255615, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 383778816}\n{'loss': 4.1443, 'grad_norm': 0.39886555075645447, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 384827392}\n{'loss': 4.16, 'grad_norm': 0.4073260724544525, 'learning_rate': 0.001, 'epoch': 0.04, 'num_input_tokens_seen': 385875968}\n{'loss': 4.0871, 'grad_norm': 0.46062660217285156, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 386924544}\n{'loss': 4.1655, 'grad_norm': 0.3555128574371338, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 387973120}\n{'loss': 4.1993, 'grad_norm': 0.35318323969841003, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 389021696}\n{'loss': 4.0745, 'grad_norm': 0.3469637632369995, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 390070272}\n{'loss': 4.1844, 'grad_norm': 0.3650517761707306, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 391118848}\n{'loss': 4.1744, 'grad_norm': 0.4310692846775055, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 392167424}\n{'loss': 4.1896, 'grad_norm': 0.465585857629776, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 393216000}\n{'loss': 4.0568, 'grad_norm': 0.5539769530296326, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 394264576}\n{'loss': 4.2642, 'grad_norm': 0.5437971949577332, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 395313152}\n{'loss': 4.1705, 'grad_norm': 0.6534202694892883, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 396361728}\n{'loss': 3.9844, 'grad_norm': 0.7271204590797424, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 397410304}\n{'loss': 4.105, 'grad_norm': 0.7395262122154236, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 398458880}\n{'loss': 4.2332, 'grad_norm': 0.9734097719192505, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 399507456}\n{'loss': 4.1501, 'grad_norm': 1.1519765853881836, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 400556032}\n{'loss': 4.0756, 'grad_norm': 0.7837873697280884, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 401604608}\n{'loss': 4.013, 'grad_norm': 0.8097010850906372, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 402653184}\n{'eval_loss': 4.120734214782715, 'eval_runtime': 626.8806, 'eval_samples_per_second': 26.136, 'eval_steps_per_second': 0.204, 'epoch': 0.05, 'num_input_tokens_seen': 402653184}\n{'loss': 4.0955, 'grad_norm': 0.6811020970344543, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 403701760}\n{'loss': 4.0917, 'grad_norm': 0.5382081270217896, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 404750336}\n{'loss': 4.0414, 'grad_norm': 0.4250117242336273, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 405798912}\n{'loss': 4.1051, 'grad_norm': 0.4233124256134033, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 406847488}\n{'loss': 4.1475, 'grad_norm': 0.41960859298706055, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 407896064}\n{'loss': 4.0322, 'grad_norm': 0.4991297423839569, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 408944640}\n{'loss': 4.0664, 'grad_norm': 0.43890711665153503, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 409993216}\n{'loss': 4.1126, 'grad_norm': 0.38538315892219543, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 411041792}\n{'loss': 4.0591, 'grad_norm': 0.41170960664749146, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 412090368}\n{'loss': 4.1145, 'grad_norm': 0.42465972900390625, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 413138944}\n{'loss': 4.0393, 'grad_norm': 0.4215935468673706, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 414187520}\n{'loss': 3.9509, 'grad_norm': 0.5031537413597107, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 415236096}\n{'loss': 3.9314, 'grad_norm': 0.5212794542312622, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 416284672}\n{'loss': 4.062, 'grad_norm': 0.5779813528060913, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 417333248}\n{'loss': 4.0264, 'grad_norm': 0.5523960590362549, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 418381824}\n{'loss': 4.0366, 'grad_norm': 0.501869797706604, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 419430400}\n{'loss': 4.016, 'grad_norm': 0.390077143907547, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 420478976}\n{'loss': 3.9438, 'grad_norm': 0.39393457770347595, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 421527552}\n{'loss': 3.9882, 'grad_norm': 0.3395244777202606, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 422576128}\n{'loss': 3.95, 'grad_norm': 0.3985426425933838, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 423624704}\n{'loss': 3.9708, 'grad_norm': 0.4353885352611542, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 424673280}\n{'loss': 3.9959, 'grad_norm': 0.39546582102775574, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 425721856}\n{'loss': 3.9475, 'grad_norm': 0.3725046217441559, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 426770432}\n{'loss': 3.8599, 'grad_norm': 0.5391167998313904, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 427819008}\n{'loss': 3.9765, 'grad_norm': 0.5383077263832092, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 428867584}\n{'loss': 3.8999, 'grad_norm': 0.4455236494541168, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 429916160}\n{'loss': 4.0357, 'grad_norm': 0.4489726722240448, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 430964736}\n{'loss': 3.992, 'grad_norm': 0.45914894342422485, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 432013312}\n{'loss': 3.9556, 'grad_norm': 0.5718650817871094, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 433061888}\n{'loss': 3.9797, 'grad_norm': 0.5529163479804993, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 434110464}\n{'loss': 3.9479, 'grad_norm': 0.4689369201660156, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 435159040}\n{'loss': 3.9358, 'grad_norm': 0.448303759098053, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 436207616}\n{'loss': 3.9699, 'grad_norm': 0.4203392565250397, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 437256192}\n{'loss': 3.8173, 'grad_norm': 0.4046834707260132, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 438304768}\n{'loss': 3.8183, 'grad_norm': 0.3998134136199951, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 439353344}\n{'loss': 3.8477, 'grad_norm': 0.4120945632457733, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 440401920}\n{'loss': 3.8486, 'grad_norm': 0.39726078510284424, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 441450496}\n{'loss': 3.942, 'grad_norm': 0.399142861366272, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 442499072}\n{'loss': 3.9038, 'grad_norm': 0.41262856125831604, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 443547648}\n{'loss': 3.8447, 'grad_norm': 0.4645870327949524, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 444596224}\n{'loss': 3.9215, 'grad_norm': 0.49330976605415344, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 445644800}\n{'loss': 4.5329, 'grad_norm': 4.813076972961426, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 446693376}\n{'loss': 3.763, 'grad_norm': 1.0100675821304321, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 447741952}\n{'loss': 3.9888, 'grad_norm': 1.2422761917114258, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 448790528}\n{'loss': 3.9209, 'grad_norm': 1.1251254081726074, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 449839104}\n{'loss': 4.1438, 'grad_norm': 1.926529049873352, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 450887680}\n{'loss': 4.0952, 'grad_norm': 1.2948275804519653, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 451936256}\n{'loss': 3.9411, 'grad_norm': 1.1000643968582153, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 452984832}\n{'loss': 3.988, 'grad_norm': 1.3160468339920044, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 454033408}\n{'loss': 4.0241, 'grad_norm': 1.0201517343521118, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 455081984}\n{'loss': 3.9875, 'grad_norm': 0.9689710140228271, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 456130560}\n{'loss': 3.8684, 'grad_norm': 1.045577049255371, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 457179136}\n{'loss': 3.865, 'grad_norm': 0.931566059589386, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 458227712}\n{'loss': 3.728, 'grad_norm': 0.945274293422699, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 459276288}\n{'loss': 3.955, 'grad_norm': 0.7679930925369263, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 460324864}\n{'loss': 4.4113, 'grad_norm': 0.889451801776886, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 461373440}\n{'loss': 3.8928, 'grad_norm': 0.9069199562072754, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 462422016}\n{'loss': 3.9624, 'grad_norm': 0.8945743441581726, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 463470592}\n{'loss': 3.9698, 'grad_norm': 0.7373656630516052, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 464519168}\n{'loss': 3.921, 'grad_norm': 0.6688440442085266, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 465567744}\n{'loss': 3.8908, 'grad_norm': 0.5442579984664917, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 466616320}\n{'loss': 3.9138, 'grad_norm': 0.5583804845809937, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 467664896}\n{'loss': 3.8731, 'grad_norm': 0.504666268825531, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 468713472}\n{'loss': 3.7961, 'grad_norm': 0.4965992867946625, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 469762048}\n{'eval_loss': 3.7728981971740723, 'eval_runtime': 616.374, 'eval_samples_per_second': 26.581, 'eval_steps_per_second': 0.208, 'epoch': 0.05, 'num_input_tokens_seen': 469762048}\n{'loss': 3.8829, 'grad_norm': 0.44414225220680237, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 470810624}\n{'loss': 3.6939, 'grad_norm': 0.5276159644126892, 'learning_rate': 0.001, 'epoch': 0.05, 'num_input_tokens_seen': 471859200}\n{'loss': 3.8173, 'grad_norm': 0.4666613042354584, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 472907776}\n{'loss': 3.6884, 'grad_norm': 0.4581243097782135, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 473956352}\n{'loss': 3.789, 'grad_norm': 0.4697781205177307, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 475004928}\n{'loss': 3.8791, 'grad_norm': 0.5336131453514099, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 476053504}\n{'loss': 3.8077, 'grad_norm': 0.5709654092788696, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 477102080}\n{'loss': 3.8421, 'grad_norm': 0.5592761039733887, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 478150656}\n{'loss': 3.8135, 'grad_norm': 0.4490680694580078, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 479199232}\n{'loss': 3.7535, 'grad_norm': 0.3931736648082733, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 480247808}\n{'loss': 3.7885, 'grad_norm': 0.41578060388565063, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 481296384}\n{'loss': 3.6255, 'grad_norm': 0.429817795753479, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 482344960}\n{'loss': 3.7202, 'grad_norm': 0.49616578221321106, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 483393536}\n  9%|▊         | 704/8192 [9:33:48<79:08:04, 38.05s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0faae356-e828-4cff-9a49-42b397431927)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_185.jsonl.zst\n  9%|▊         | 704/8192 [9:38:28<79:08:04, 38.05s/it]Retrying in 1s [Retry 1/5].\n  9%|▊         | 704/8192 [9:38:28<79:08:04, 38.05s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9557423f-6937-4f70-b50f-05b0c01f5bf3)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_4035.jsonl.zst\n  9%|▊         | 704/8192 [9:44:58<79:08:04, 38.05s/it]Retrying in 1s [Retry 1/5].\n 10%|█         | 832/8192 [11:28:20<80:32:25, 39.39s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 939d1d36-c607-4d3c-a0a0-8e447579340b)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_165.jsonl.zst\n 10%|█         | 832/8192 [11:30:25<80:32:25, 39.39s/it]Retrying in 1s [Retry 1/5].\n 10%|█         | 832/8192 [11:30:25<80:32:25, 39.39s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0b99bfd1-07ae-46db-81fa-fc6ef0eabdbc)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_1529.jsonl.zst\n 10%|█         | 832/8192 [11:38:24<80:32:25, 39.39s/it]Retrying in 1s [Retry 1/5].\n 10%|█         | 832/8192 [11:38:24<80:32:25, 39.39s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c208d1bb-5d13-45d2-9a01-1d5a2defa598)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk5/example_holdout_4562.jsonl.zst\n 10%|█         | 832/8192 [11:39:58<80:32:25, 39.39s/it]Retrying in 1s [Retry 1/5].\n 10%|█         | 832/8192 [11:39:58<80:32:25, 39.39s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2bf98b5c-473b-4e00-aca2-b152efddb992)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_4414.jsonl.zst\n 10%|█         | 832/8192 [11:41:00<80:32:25, 39.39s/it]Retrying in 1s [Retry 1/5].\n 11%|█         | 896/8192 [12:24:54<77:09:28, 38.07s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3b8321b9-2d88-4bfa-9eca-b201c444cba3)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk5/example_holdout_405.jsonl.zst\n 11%|█         | 896/8192 [12:25:55<77:09:28, 38.07s/it]Retrying in 1s [Retry 1/5].\n 11%|█         | 896/8192 [12:25:55<77:09:28, 38.07s/it]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a98a238a-c0a4-4295-8502-316a89a7ae29)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk1/example_holdout_2524.jsonl.zst\n 11%|█         | 896/8192 [12:33:14<77:09:28, 38.07s/it]Retrying in 1s [Retry 1/5].\n 11%|█▏        | 922/8192 [12:52:49<76:09:46, 37.71s/it]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 36a7cc72-4605-416a-8742-59488d719150)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/train/chunk1/example_train_5267.jsonl.zst\n 11%|█▏        | 922/8192 [12:52:59<76:09:46, 37.71s/it]Retrying in 1s [Retry 1/5].\n 12%|█▏        | 943/8192 [13:06:07<76:15:57, 37.88s/it]\n{'loss': 3.7796, 'grad_norm': 0.4774172008037567, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 484442112}\n{'loss': 3.7779, 'grad_norm': 0.45830512046813965, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 485490688}\n{'loss': 3.6516, 'grad_norm': 0.4130597710609436, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 486539264}\n{'loss': 3.7018, 'grad_norm': 0.3804127275943756, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 487587840}\n{'loss': 3.6893, 'grad_norm': 0.36560356616973877, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 488636416}\n{'loss': 3.6362, 'grad_norm': 0.3827981948852539, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 489684992}\n{'loss': 3.5987, 'grad_norm': 0.37492236495018005, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 490733568}\n{'loss': 3.7165, 'grad_norm': 0.46995237469673157, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 491782144}\n{'loss': 3.6097, 'grad_norm': 0.4908960461616516, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 492830720}\n{'loss': 3.6035, 'grad_norm': 0.5318525433540344, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 493879296}\n{'loss': 3.6643, 'grad_norm': 0.4848596453666687, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 494927872}\n{'loss': 3.6586, 'grad_norm': 0.4421922266483307, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 495976448}\n{'loss': 3.5902, 'grad_norm': 0.4107126295566559, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 497025024}\n{'loss': 3.6937, 'grad_norm': 0.3975088894367218, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 498073600}\n{'loss': 3.6496, 'grad_norm': 0.4559416174888611, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 499122176}\n{'loss': 3.66, 'grad_norm': 0.41401296854019165, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 500170752}\n{'loss': 3.5551, 'grad_norm': 0.45235902070999146, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 501219328}\n{'loss': 3.4794, 'grad_norm': 0.427593857049942, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 502267904}\n{'loss': 3.5345, 'grad_norm': 0.4024144411087036, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 503316480}\n{'loss': 3.5784, 'grad_norm': 0.410284161567688, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 504365056}\n{'loss': 3.6177, 'grad_norm': 0.37683290243148804, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 505413632}\n{'loss': 3.5883, 'grad_norm': 0.417323499917984, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 506462208}\n{'loss': 3.5888, 'grad_norm': 0.4327872693538666, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 507510784}\n{'loss': 3.5891, 'grad_norm': 0.5366392731666565, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 508559360}\n{'loss': 3.3725, 'grad_norm': 0.45735156536102295, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 509607936}\n{'loss': 3.5674, 'grad_norm': 0.4255360960960388, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 510656512}\n{'loss': 3.3523, 'grad_norm': 0.6517689824104309, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 511705088}\n{'loss': 3.5901, 'grad_norm': 0.5713740587234497, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 512753664}\n{'loss': 3.542, 'grad_norm': 0.5570502281188965, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 513802240}\n{'loss': 3.4246, 'grad_norm': 0.6477808356285095, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 514850816}\n{'loss': 3.4954, 'grad_norm': 0.5195346474647522, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 515899392}\n{'loss': 3.6516, 'grad_norm': 0.5446246862411499, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 516947968}\n{'loss': 3.5955, 'grad_norm': 0.5475099086761475, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 517996544}\n{'loss': 3.5516, 'grad_norm': 0.4719395041465759, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 519045120}\n{'loss': 3.5439, 'grad_norm': 0.43647533655166626, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 520093696}\n{'loss': 3.579, 'grad_norm': 0.5048384070396423, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 521142272}\n{'loss': 3.4742, 'grad_norm': 0.4902295172214508, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 522190848}\n{'loss': 3.4363, 'grad_norm': 0.525496244430542, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 523239424}\n{'loss': 3.3658, 'grad_norm': 0.5224571824073792, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 524288000}\n{'loss': 3.4816, 'grad_norm': 0.45781856775283813, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 525336576}\n{'loss': 3.4612, 'grad_norm': 0.3764704763889313, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 526385152}\n{'loss': 3.5172, 'grad_norm': 0.3994409143924713, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 527433728}\n{'loss': 3.5462, 'grad_norm': 0.45144984126091003, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 528482304}\n{'loss': 3.5079, 'grad_norm': 0.4901409149169922, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 529530880}\n{'loss': 3.5187, 'grad_norm': 0.45689818263053894, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 530579456}\n{'loss': 3.4408, 'grad_norm': 0.4650699198246002, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 531628032}\n{'loss': 3.4019, 'grad_norm': 0.40419647097587585, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 532676608}\n{'loss': 3.5255, 'grad_norm': 0.3895981013774872, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 533725184}\n{'loss': 3.312, 'grad_norm': 0.46533191204071045, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 534773760}\n{'loss': 3.4233, 'grad_norm': 0.5021492838859558, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 535822336}\n{'loss': 3.4211, 'grad_norm': 0.6763796806335449, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 536870912}\n{'eval_loss': 3.38647198677063, 'eval_runtime': 681.5531, 'eval_samples_per_second': 24.039, 'eval_steps_per_second': 0.188, 'epoch': 0.06, 'num_input_tokens_seen': 536870912}\n{'loss': 3.2825, 'grad_norm': 0.75739586353302, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 537919488}\n{'loss': 3.4758, 'grad_norm': 0.49962809681892395, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 538968064}\n{'loss': 3.4105, 'grad_norm': 0.47640085220336914, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 540016640}\n{'loss': 3.4393, 'grad_norm': 0.4722411632537842, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 541065216}\n{'loss': 3.4254, 'grad_norm': 0.4715781807899475, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 542113792}\n{'loss': 3.3992, 'grad_norm': 0.474001407623291, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 543162368}\n{'loss': 3.4274, 'grad_norm': 0.48976385593414307, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 544210944}\n{'loss': 3.3255, 'grad_norm': 0.4819697141647339, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 545259520}\n{'loss': 3.3679, 'grad_norm': 0.37490880489349365, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 546308096}\n{'loss': 3.377, 'grad_norm': 0.4356544315814972, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 547356672}\n{'loss': 3.4294, 'grad_norm': 0.3786229193210602, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 548405248}\n{'loss': 3.2323, 'grad_norm': 0.4364008605480194, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 549453824}\n{'loss': 3.4615, 'grad_norm': 0.39242950081825256, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 550502400}\n{'loss': 3.3589, 'grad_norm': 0.4270903766155243, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 551550976}\n{'loss': 3.4366, 'grad_norm': 0.4204763174057007, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 552599552}\n{'loss': 3.3859, 'grad_norm': 0.554025411605835, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 553648128}\n{'loss': 3.2353, 'grad_norm': 0.5719075798988342, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 554696704}\n{'loss': 3.3798, 'grad_norm': 0.4803822338581085, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 555745280}\n{'loss': 3.1191, 'grad_norm': 0.5494056344032288, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 556793856}\n{'loss': 3.424, 'grad_norm': 0.4569101333618164, 'learning_rate': 0.001, 'epoch': 0.06, 'num_input_tokens_seen': 557842432}\n{'loss': 3.4299, 'grad_norm': 0.48103874921798706, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 558891008}\n{'loss': 3.3483, 'grad_norm': 0.44187718629837036, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 559939584}\n{'loss': 3.3196, 'grad_norm': 0.4359618127346039, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 560988160}\n{'loss': 3.4479, 'grad_norm': 0.37653473019599915, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 562036736}\n{'loss': 3.2509, 'grad_norm': 0.4397211968898773, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 563085312}\n{'loss': 3.4193, 'grad_norm': 0.5013746619224548, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 564133888}\n{'loss': 3.3391, 'grad_norm': 0.5044407844543457, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 565182464}\n{'loss': 3.3223, 'grad_norm': 0.45118412375450134, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 566231040}\n{'loss': 3.3041, 'grad_norm': 0.5617747902870178, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 567279616}\n{'loss': 3.3436, 'grad_norm': 0.5154598355293274, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 568328192}\n{'loss': 3.3739, 'grad_norm': 0.4647876024246216, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 569376768}\n{'loss': 3.3366, 'grad_norm': 0.3766598701477051, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 570425344}\n{'loss': 3.3098, 'grad_norm': 0.40857356786727905, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 571473920}\n{'loss': 3.0331, 'grad_norm': 0.4163903594017029, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 572522496}\n{'loss': 3.3184, 'grad_norm': 0.38519713282585144, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 573571072}\n{'loss': 3.3886, 'grad_norm': 0.38155344128608704, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 574619648}\n{'loss': 3.2855, 'grad_norm': 0.3684964179992676, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 575668224}\n{'loss': 3.0484, 'grad_norm': 0.3504279553890228, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 576716800}\n{'loss': 3.2702, 'grad_norm': 0.42653048038482666, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 577765376}\n{'loss': 3.312, 'grad_norm': 0.4263192415237427, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 578813952}\n{'loss': 3.3355, 'grad_norm': 0.4272316098213196, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 579862528}\n{'loss': 3.2806, 'grad_norm': 0.40996676683425903, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 580911104}\n{'loss': 3.2504, 'grad_norm': 0.403242826461792, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 581959680}\n{'loss': 3.2924, 'grad_norm': 0.46690869331359863, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 583008256}\n{'loss': 3.1466, 'grad_norm': 0.515250027179718, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 584056832}\n{'loss': 3.2898, 'grad_norm': 0.4872475266456604, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 585105408}\n{'loss': 3.3699, 'grad_norm': 0.43510228395462036, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 586153984}\n{'loss': 3.1568, 'grad_norm': 0.4732394814491272, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 587202560}\n{'loss': 3.2145, 'grad_norm': 0.49767330288887024, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 588251136}\n{'loss': 3.2966, 'grad_norm': 0.4968816936016083, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 589299712}\n{'loss': 3.2249, 'grad_norm': 0.4123048782348633, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 590348288}\n{'loss': 3.3819, 'grad_norm': 0.4349605143070221, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 591396864}\n{'loss': 3.3477, 'grad_norm': 0.47485488653182983, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 592445440}\n{'loss': 3.3202, 'grad_norm': 0.46784669160842896, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 593494016}\n{'loss': 3.2231, 'grad_norm': 0.42318931221961975, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 594542592}\n{'loss': 3.2901, 'grad_norm': 0.40393564105033875, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 595591168}\n{'loss': 3.2065, 'grad_norm': 0.4144214391708374, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 596639744}\n{'loss': 2.8698, 'grad_norm': 0.40921372175216675, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 597688320}\n{'loss': 3.2242, 'grad_norm': 0.35226207971572876, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 598736896}\n{'loss': 3.2125, 'grad_norm': 0.43364742398262024, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 599785472}\n{'loss': 3.2296, 'grad_norm': 0.4272080361843109, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 600834048}\n{'loss': 2.9346, 'grad_norm': 0.4155097007751465, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 601882624}\n{'loss': 3.2706, 'grad_norm': 0.4263918697834015, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 602931200}\n{'loss': 3.3124, 'grad_norm': 0.43336594104766846, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 603979776}\n{'eval_loss': 3.1686322689056396, 'eval_runtime': 664.0006, 'eval_samples_per_second': 24.675, 'eval_steps_per_second': 0.193, 'epoch': 0.07, 'num_input_tokens_seen': 603979776}\n{'loss': 3.349, 'grad_norm': 0.4504219889640808, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 605028352}\n{'loss': 3.3015, 'grad_norm': 0.5899333953857422, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 606076928}\n{'loss': 3.2036, 'grad_norm': 0.5814825892448425, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 607125504}\n{'loss': 3.2786, 'grad_norm': 0.3971703350543976, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 608174080}\n{'loss': 3.0979, 'grad_norm': 0.5669280290603638, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 609222656}\n{'loss': 3.0683, 'grad_norm': 0.4786263406276703, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 610271232}\n{'loss': 3.1731, 'grad_norm': 0.46415817737579346, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 611319808}\n{'loss': 3.2282, 'grad_norm': 0.4295870363712311, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 612368384}\n{'loss': 3.2196, 'grad_norm': 0.4184265732765198, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 613416960}\n{'loss': 3.2445, 'grad_norm': 0.4624122381210327, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 614465536}\n{'loss': 3.1135, 'grad_norm': 0.3681364059448242, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 615514112}\n{'loss': 3.1877, 'grad_norm': 0.3612712621688843, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 616562688}\n{'loss': 3.308, 'grad_norm': 0.34696292877197266, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 617611264}\n{'loss': 3.4995, 'grad_norm': 0.5025363564491272, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 618659840}\n{'loss': 3.1853, 'grad_norm': 0.6652331352233887, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 619708416}\n{'loss': 3.1844, 'grad_norm': 0.7156277894973755, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 620756992}\n{'loss': 3.2325, 'grad_norm': 0.5241081118583679, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 621805568}\n{'loss': 2.972, 'grad_norm': 0.5001779198646545, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 622854144}\n{'loss': 3.1742, 'grad_norm': 0.4062795341014862, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 623902720}\n{'loss': 3.2539, 'grad_norm': 0.4671201705932617, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 624951296}\n{'loss': 3.1948, 'grad_norm': 0.3894169330596924, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 625999872}\n{'loss': 3.2469, 'grad_norm': 0.4665684998035431, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 627048448}\n{'loss': 3.2742, 'grad_norm': 0.43211206793785095, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 628097024}\n{'loss': 3.1195, 'grad_norm': 0.4476025700569153, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 629145600}\n{'loss': 3.2127, 'grad_norm': 0.3596750795841217, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 630194176}\n{'loss': 3.1741, 'grad_norm': 0.40869519114494324, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 631242752}\n{'loss': 3.1708, 'grad_norm': 0.36658936738967896, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 632291328}\n{'loss': 3.0925, 'grad_norm': 0.35227081179618835, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 633339904}\n{'loss': 3.171, 'grad_norm': 0.3942136764526367, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 634388480}\n{'loss': 3.1729, 'grad_norm': 0.3163004219532013, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 635437056}\n{'loss': 3.1683, 'grad_norm': 0.35835322737693787, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 636485632}\n{'loss': 3.1118, 'grad_norm': 0.3395129144191742, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 637534208}\n{'loss': 3.2123, 'grad_norm': 0.38003110885620117, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 638582784}\n{'loss': 3.167, 'grad_norm': 0.4000258445739746, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 639631360}\n{'loss': 3.0668, 'grad_norm': 0.38393035531044006, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 640679936}\n{'loss': 2.9125, 'grad_norm': 0.38961607217788696, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 641728512}\n{'loss': 3.1024, 'grad_norm': 0.3406165540218353, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 642777088}\n{'loss': 3.1262, 'grad_norm': 0.4859096109867096, 'learning_rate': 0.001, 'epoch': 0.07, 'num_input_tokens_seen': 643825664}\n{'loss': 3.1155, 'grad_norm': 0.5454179048538208, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 644874240}\n{'loss': 3.1594, 'grad_norm': 0.46631914377212524, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 645922816}\n{'loss': 3.1164, 'grad_norm': 0.4049534797668457, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 646971392}\n{'loss': 2.9272, 'grad_norm': 0.32954707741737366, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 648019968}\n{'loss': 3.0888, 'grad_norm': 0.409853458404541, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 649068544}\n{'loss': 3.2185, 'grad_norm': 0.43080267310142517, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 650117120}\n{'loss': 3.1871, 'grad_norm': 0.4323279857635498, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 651165696}\n{'loss': 2.9759, 'grad_norm': 0.3696155846118927, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 652214272}\n{'loss': 3.1058, 'grad_norm': 0.3963398337364197, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 653262848}\n{'loss': 3.1214, 'grad_norm': 0.4020082652568817, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 654311424}\n{'loss': 3.0678, 'grad_norm': 0.4210987091064453, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 655360000}\n{'loss': 2.9177, 'grad_norm': 0.44535601139068604, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 656408576}\n{'loss': 3.1005, 'grad_norm': 0.363700807094574, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 657457152}\n{'loss': 3.0285, 'grad_norm': 0.393673837184906, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 658505728}\n{'loss': 3.031, 'grad_norm': 0.3472498059272766, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 659554304}\n{'loss': 3.1837, 'grad_norm': 0.45663976669311523, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 660602880}\n{'loss': 3.1636, 'grad_norm': 0.44765880703926086, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 661651456}\n{'loss': 3.0421, 'grad_norm': 0.5289708375930786, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 662700032}\n{'loss': 2.9394, 'grad_norm': 0.5272406339645386, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 663748608}\n{'loss': 3.2419, 'grad_norm': 0.5471237301826477, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 664797184}\n{'loss': 3.1506, 'grad_norm': 0.5762659311294556, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 665845760}\n{'loss': 3.1258, 'grad_norm': 0.5486758351325989, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 666894336}\n{'loss': 3.1686, 'grad_norm': 0.4877275228500366, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 667942912}\n{'loss': 3.1062, 'grad_norm': 0.35992035269737244, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 668991488}\n{'loss': 3.1655, 'grad_norm': 0.39184319972991943, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 670040064}\n{'loss': 3.1455, 'grad_norm': 0.46003854274749756, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 671088640}\n{'eval_loss': 3.036459445953369, 'eval_runtime': 676.6057, 'eval_samples_per_second': 24.215, 'eval_steps_per_second': 0.189, 'epoch': 0.08, 'num_input_tokens_seen': 671088640}\n{'loss': 3.1058, 'grad_norm': 0.45958808064460754, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 672137216}\n{'loss': 3.0861, 'grad_norm': 0.41562288999557495, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 673185792}\n{'loss': 3.1135, 'grad_norm': 0.38576263189315796, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 674234368}\n{'loss': 2.9998, 'grad_norm': 0.3936232924461365, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 675282944}\n{'loss': 3.1349, 'grad_norm': 0.3888678252696991, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 676331520}\n{'loss': 2.9192, 'grad_norm': 0.31759846210479736, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 677380096}\n{'loss': 3.1324, 'grad_norm': 0.3801535964012146, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 678428672}\n{'loss': 3.1064, 'grad_norm': 0.36299699544906616, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 679477248}\n{'loss': 3.2258, 'grad_norm': 0.36732324957847595, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 680525824}\n{'loss': 3.2162, 'grad_norm': 0.42108356952667236, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 681574400}\n{'loss': 3.2189, 'grad_norm': 0.4113474190235138, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 682622976}\n{'loss': 3.0585, 'grad_norm': 0.39936116337776184, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 683671552}\n{'loss': 3.0693, 'grad_norm': 0.35424771904945374, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 684720128}\n{'loss': 3.1134, 'grad_norm': 0.3333597183227539, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 685768704}\n{'loss': 3.0536, 'grad_norm': 0.37569180130958557, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 686817280}\n{'loss': 3.1396, 'grad_norm': 0.33836638927459717, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 687865856}\n{'loss': 3.1353, 'grad_norm': 0.31407052278518677, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 688914432}\n{'loss': 2.9977, 'grad_norm': 0.34316036105155945, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 689963008}\n{'loss': 3.1683, 'grad_norm': 0.3779186010360718, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 691011584}\n{'loss': 2.9567, 'grad_norm': 0.3414095342159271, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 692060160}\n{'loss': 3.0806, 'grad_norm': 0.31614938378334045, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 693108736}\n{'loss': 3.0975, 'grad_norm': 0.35552725195884705, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 694157312}\n{'loss': 3.0241, 'grad_norm': 0.38724133372306824, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 695205888}\n{'loss': 3.0701, 'grad_norm': 0.3581823408603668, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 696254464}\n{'loss': 3.0222, 'grad_norm': 0.3632317781448364, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 697303040}\n{'loss': 3.0188, 'grad_norm': 0.40560677647590637, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 698351616}\n{'loss': 3.106, 'grad_norm': 0.3953804075717926, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 699400192}\n{'loss': 3.1552, 'grad_norm': 0.40652376413345337, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 700448768}\n{'loss': 2.8893, 'grad_norm': 0.3625616133213043, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 701497344}\n{'loss': 2.9183, 'grad_norm': 0.3450768291950226, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 702545920}\n{'loss': 2.9828, 'grad_norm': 0.36742398142814636, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 703594496}\n{'loss': 3.0327, 'grad_norm': 0.3611394762992859, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 704643072}\n{'loss': 3.1466, 'grad_norm': 0.3593210279941559, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 705691648}\n{'loss': 3.0163, 'grad_norm': 0.3994838297367096, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 706740224}\n{'loss': 3.0563, 'grad_norm': 0.41202738881111145, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 707788800}\n{'loss': 3.0912, 'grad_norm': 0.3404449224472046, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 708837376}\n{'loss': 3.0108, 'grad_norm': 0.3745224177837372, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 709885952}\n{'loss': 3.0864, 'grad_norm': 0.4320204555988312, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 710934528}\n{'loss': 3.0387, 'grad_norm': 0.34649956226348877, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 711983104}\n{'loss': 3.013, 'grad_norm': 0.34744057059288025, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 713031680}\n{'loss': 3.0985, 'grad_norm': 0.3638330101966858, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 714080256}\n{'loss': 3.1498, 'grad_norm': 0.43823716044425964, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 715128832}\n{'loss': 3.0366, 'grad_norm': 0.6364668011665344, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 716177408}\n{'loss': 2.9614, 'grad_norm': 0.6294976472854614, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 717225984}\n{'loss': 3.0619, 'grad_norm': 0.5871465802192688, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 718274560}\n{'loss': 3.1489, 'grad_norm': 0.7779986262321472, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 719323136}\n{'loss': 3.1331, 'grad_norm': 1.102079153060913, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 720371712}\n{'loss': 3.1423, 'grad_norm': 0.6352481245994568, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 721420288}\n{'loss': 3.1509, 'grad_norm': 0.5698557496070862, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 722468864}\n{'loss': 2.6683, 'grad_norm': 0.501290500164032, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 723517440}\n{'loss': 3.0334, 'grad_norm': 0.4512772560119629, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 724566016}\n{'loss': 3.0485, 'grad_norm': 0.4409146308898926, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 725614592}\n{'loss': 3.0154, 'grad_norm': 0.3902524411678314, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 726663168}\n{'loss': 3.0742, 'grad_norm': 0.3692473769187927, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 727711744}\n{'loss': 2.8306, 'grad_norm': 0.385005384683609, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 728760320}\n{'loss': 2.9258, 'grad_norm': 0.37514418363571167, 'learning_rate': 0.001, 'epoch': 0.08, 'num_input_tokens_seen': 729808896}\n{'loss': 3.0061, 'grad_norm': 0.42038342356681824, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 730857472}\n{'loss': 3.0588, 'grad_norm': 0.40415653586387634, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 731906048}\n{'loss': 2.9542, 'grad_norm': 0.38514354825019836, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 732954624}\n{'loss': 2.9252, 'grad_norm': 0.3861909806728363, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 734003200}\n{'loss': 2.8432, 'grad_norm': 0.40519189834594727, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 735051776}\n{'loss': 2.9779, 'grad_norm': 0.37011685967445374, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 736100352}\n{'loss': 2.9908, 'grad_norm': 0.34850460290908813, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 737148928}\n{'loss': 2.9589, 'grad_norm': 0.371500700712204, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 738197504}\n[2025-03-11 00:58:41 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0faae356-e828-4cff-9a49-42b397431927)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_185.jsonl.zst\n[2025-03-11 00:58:41 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-11 01:05:12 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9557423f-6937-4f70-b50f-05b0c01f5bf3)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_4035.jsonl.zst\n[2025-03-11 01:05:12 WARNING] Retrying in 1s [Retry 1/5].\n{'eval_loss': 2.9496541023254395, 'eval_runtime': 714.5105, 'eval_samples_per_second': 22.93, 'eval_steps_per_second': 0.179, 'epoch': 0.09, 'num_input_tokens_seen': 738197504}\n{'loss': 2.9029, 'grad_norm': 0.3044391870498657, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 739246080}\n{'loss': 2.8536, 'grad_norm': 0.34875407814979553, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 740294656}\n{'loss': 2.8478, 'grad_norm': 0.4568244516849518, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 741343232}\n{'loss': 3.1164, 'grad_norm': 0.44005003571510315, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 742391808}\n{'loss': 2.8584, 'grad_norm': 0.39490336179733276, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 743440384}\n{'loss': 3.0681, 'grad_norm': 0.4427798092365265, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 744488960}\n{'loss': 3.0315, 'grad_norm': 0.4771106243133545, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 745537536}\n{'loss': 2.8794, 'grad_norm': 0.4624035656452179, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 746586112}\n{'loss': 2.9624, 'grad_norm': 0.4244724214076996, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 747634688}\n{'loss': 2.9925, 'grad_norm': 0.39176708459854126, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 748683264}\n{'loss': 2.9753, 'grad_norm': 0.43686383962631226, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 749731840}\n{'loss': 3.0718, 'grad_norm': 0.4536241590976715, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 750780416}\n{'loss': 3.0065, 'grad_norm': 0.3421417772769928, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 751828992}\n{'loss': 2.8965, 'grad_norm': 0.30937010049819946, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 752877568}\n{'loss': 3.0347, 'grad_norm': 0.33371758460998535, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 753926144}\n{'loss': 3.0133, 'grad_norm': 0.3285418450832367, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 754974720}\n{'loss': 3.1219, 'grad_norm': 0.33177846670150757, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 756023296}\n{'loss': 2.9354, 'grad_norm': 0.36487525701522827, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 757071872}\n{'loss': 3.133, 'grad_norm': 0.35576146841049194, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 758120448}\n{'loss': 2.9771, 'grad_norm': 0.4217855930328369, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 759169024}\n{'loss': 2.9906, 'grad_norm': 0.4007001519203186, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 760217600}\n{'loss': 3.0219, 'grad_norm': 0.36323100328445435, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 761266176}\n{'loss': 2.89, 'grad_norm': 0.323297381401062, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 762314752}\n{'loss': 2.8566, 'grad_norm': 0.3450233042240143, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 763363328}\n{'loss': 3.0536, 'grad_norm': 0.36228489875793457, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 764411904}\n{'loss': 2.9259, 'grad_norm': 0.3553276062011719, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 765460480}\n{'loss': 2.8431, 'grad_norm': 0.37074941396713257, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 766509056}\n{'loss': 3.0549, 'grad_norm': 0.4105451703071594, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 767557632}\n{'loss': 2.8431, 'grad_norm': 0.4433744549751282, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 768606208}\n{'loss': 2.9545, 'grad_norm': 0.4024113416671753, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 769654784}\n{'loss': 2.9237, 'grad_norm': 0.3534025549888611, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 770703360}\n{'loss': 2.9306, 'grad_norm': 0.3788505792617798, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 771751936}\n{'loss': 2.9218, 'grad_norm': 0.3302527666091919, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 772800512}\n{'loss': 3.0647, 'grad_norm': 0.36651748418807983, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 773849088}\n{'loss': 3.0289, 'grad_norm': 0.35838624835014343, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 774897664}\n{'loss': 2.9157, 'grad_norm': 0.34652525186538696, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 775946240}\n{'loss': 2.9358, 'grad_norm': 0.37369009852409363, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 776994816}\n{'loss': 3.0725, 'grad_norm': 0.37748783826828003, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 778043392}\n{'loss': 2.8444, 'grad_norm': 0.339287132024765, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 779091968}\n{'loss': 2.859, 'grad_norm': 0.3415367305278778, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 780140544}\n{'loss': 2.9334, 'grad_norm': 0.3661401569843292, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 781189120}\n{'loss': 3.0287, 'grad_norm': 0.3512025773525238, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 782237696}\n{'loss': 2.8093, 'grad_norm': 0.3412944972515106, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 783286272}\n{'loss': 2.9112, 'grad_norm': 0.35280412435531616, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 784334848}\n{'loss': 2.8939, 'grad_norm': 0.3652521073818207, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 785383424}\n{'loss': 2.961, 'grad_norm': 0.3336659371852875, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 786432000}\n{'loss': 2.9547, 'grad_norm': 0.3242711126804352, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 787480576}\n{'loss': 2.8035, 'grad_norm': 0.3276830017566681, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 788529152}\n{'loss': 2.9639, 'grad_norm': 0.32558611035346985, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 789577728}\n{'loss': 2.9981, 'grad_norm': 0.32141759991645813, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 790626304}\n{'loss': 2.8053, 'grad_norm': 0.33697575330734253, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 791674880}\n{'loss': 2.9265, 'grad_norm': 0.3305177092552185, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 792723456}\n{'loss': 2.9357, 'grad_norm': 0.3303467035293579, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 793772032}\n{'loss': 2.9209, 'grad_norm': 0.33826348185539246, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 794820608}\n{'loss': 3.0134, 'grad_norm': 0.3682444393634796, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 795869184}\n{'loss': 2.8786, 'grad_norm': 0.364545613527298, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 796917760}\n{'loss': 3.0202, 'grad_norm': 0.4031524360179901, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 797966336}\n{'loss': 2.4912, 'grad_norm': 0.40752920508384705, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 799014912}\n{'loss': 2.9311, 'grad_norm': 0.36912065744400024, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 800063488}\n{'loss': 2.8768, 'grad_norm': 0.3906254172325134, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 801112064}\n{'loss': 2.8677, 'grad_norm': 0.3680756092071533, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 802160640}\n{'loss': 2.967, 'grad_norm': 0.42479801177978516, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 803209216}\n{'loss': 3.0138, 'grad_norm': 0.4966808259487152, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 804257792}\n{'loss': 2.9186, 'grad_norm': 0.413562536239624, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 805306368}\n{'eval_loss': 2.8718671798706055, 'eval_runtime': 1149.5487, 'eval_samples_per_second': 14.253, 'eval_steps_per_second': 0.111, 'epoch': 0.09, 'num_input_tokens_seen': 805306368}\n{'loss': 2.8717, 'grad_norm': 0.3343268632888794, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 806354944}\n{'loss': 3.0123, 'grad_norm': 0.42326104640960693, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 807403520}\n{'loss': 2.9691, 'grad_norm': 0.35408785939216614, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 808452096}\n{'loss': 2.8862, 'grad_norm': 0.35168665647506714, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 809500672}\n{'loss': 2.9754, 'grad_norm': 0.3385300934314728, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 810549248}\n{'loss': 2.751, 'grad_norm': 0.36974239349365234, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 811597824}\n{'loss': 2.8481, 'grad_norm': 0.3535187244415283, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 812646400}\n{'loss': 2.9605, 'grad_norm': 0.39851564168930054, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 813694976}\n{'loss': 2.9251, 'grad_norm': 0.35983574390411377, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 814743552}\n{'loss': 2.8766, 'grad_norm': 0.34153202176094055, 'learning_rate': 0.001, 'epoch': 0.09, 'num_input_tokens_seen': 815792128}\n{'loss': 2.9205, 'grad_norm': 0.3700859546661377, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 816840704}\n{'loss': 2.7621, 'grad_norm': 0.3954067528247833, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 817889280}\n{'loss': 2.886, 'grad_norm': 0.4191531538963318, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 818937856}\n{'loss': 2.9203, 'grad_norm': 0.3315434157848358, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 819986432}\n{'loss': 2.9563, 'grad_norm': 0.3308311700820923, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 821035008}\n{'loss': 2.9391, 'grad_norm': 0.3073643445968628, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 822083584}\n{'loss': 2.7197, 'grad_norm': 0.3343094289302826, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 823132160}\n{'loss': 2.909, 'grad_norm': 0.31464704871177673, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 824180736}\n{'loss': 2.8581, 'grad_norm': 0.40213140845298767, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 825229312}\n{'loss': 2.9224, 'grad_norm': 0.36158621311187744, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 826277888}\n{'loss': 2.985, 'grad_norm': 0.3831183910369873, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 827326464}\n{'loss': 2.8964, 'grad_norm': 0.3219353258609772, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 828375040}\n{'loss': 3.0832, 'grad_norm': 0.31743234395980835, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 829423616}\n{'loss': 2.9602, 'grad_norm': 0.3629371225833893, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 830472192}\n{'loss': 2.8327, 'grad_norm': 0.3800980746746063, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 831520768}\n{'loss': 2.8298, 'grad_norm': 0.3349006772041321, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 832569344}\n{'loss': 2.9633, 'grad_norm': 0.3282972276210785, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 833617920}\n{'loss': 2.9234, 'grad_norm': 0.3283899128437042, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 834666496}\n{'loss': 2.9754, 'grad_norm': 0.33885031938552856, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 835715072}\n{'loss': 2.8825, 'grad_norm': 0.3113347589969635, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 836763648}\n{'loss': 2.9483, 'grad_norm': 0.3759271204471588, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 837812224}\n{'loss': 2.8577, 'grad_norm': 0.38608986139297485, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 838860800}\n{'loss': 2.6639, 'grad_norm': 0.3253604471683502, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 839909376}\n{'loss': 2.8295, 'grad_norm': 0.31234994530677795, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 840957952}\n{'loss': 2.9323, 'grad_norm': 0.37187162041664124, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 842006528}\n{'loss': 3.2357, 'grad_norm': 0.5417175889015198, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 843055104}\n{'loss': 2.8982, 'grad_norm': 0.6133915781974792, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 844103680}\n{'loss': 2.928, 'grad_norm': 0.7637872099876404, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 845152256}\n{'loss': 2.9283, 'grad_norm': 0.7322977781295776, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 846200832}\n{'loss': 2.8209, 'grad_norm': 0.5112255215644836, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 847249408}\n{'loss': 2.8696, 'grad_norm': 0.49990609288215637, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 848297984}\n{'loss': 2.9193, 'grad_norm': 0.4511178135871887, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 849346560}\n{'loss': 2.9658, 'grad_norm': 0.4653412997722626, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 850395136}\n{'loss': 2.889, 'grad_norm': 0.3913695812225342, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 851443712}\n{'loss': 2.9534, 'grad_norm': 0.39285045862197876, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 852492288}\n{'loss': 2.8341, 'grad_norm': 0.5052099227905273, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 853540864}\n{'loss': 3.0436, 'grad_norm': 0.5978823900222778, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 854589440}\n{'loss': 2.9484, 'grad_norm': 0.4584784507751465, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 855638016}\n{'loss': 2.8786, 'grad_norm': 0.40823692083358765, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 856686592}\n{'loss': 2.942, 'grad_norm': 0.4448293447494507, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 857735168}\n{'loss': 2.9347, 'grad_norm': 0.4112764596939087, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 858783744}\n{'loss': 2.8359, 'grad_norm': 0.3826068341732025, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 859832320}\n{'loss': 2.9277, 'grad_norm': 0.37165558338165283, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 860880896}\n{'loss': 2.6527, 'grad_norm': 0.4285834729671478, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 861929472}\n{'loss': 2.8451, 'grad_norm': 0.36497727036476135, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 862978048}\n{'loss': 2.9039, 'grad_norm': 0.35966625809669495, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 864026624}\n{'loss': 2.9268, 'grad_norm': 0.3529391586780548, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 865075200}\n{'loss': 2.9953, 'grad_norm': 0.3455546498298645, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 866123776}\n{'loss': 2.9307, 'grad_norm': 0.3788530230522156, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 867172352}\n{'loss': 2.9448, 'grad_norm': 0.35837656259536743, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 868220928}\n{'loss': 2.9937, 'grad_norm': 0.3842633366584778, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 869269504}\n{'loss': 2.8324, 'grad_norm': 0.32774215936660767, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 870318080}\n{'loss': 2.8613, 'grad_norm': 0.327158659696579, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 871366656}\n{'loss': 2.7653, 'grad_norm': 0.3515920639038086, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 872415232}\n[2025-03-11 02:50:38 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 939d1d36-c607-4d3c-a0a0-8e447579340b)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_165.jsonl.zst\n[2025-03-11 02:50:39 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-11 02:58:37 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0b99bfd1-07ae-46db-81fa-fc6ef0eabdbc)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_1529.jsonl.zst\n[2025-03-11 02:58:37 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-11 03:00:11 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c208d1bb-5d13-45d2-9a01-1d5a2defa598)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk5/example_holdout_4562.jsonl.zst\n[2025-03-11 03:00:11 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-11 03:01:14 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2bf98b5c-473b-4e00-aca2-b152efddb992)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk3/example_holdout_4414.jsonl.zst\n[2025-03-11 03:01:14 WARNING] Retrying in 1s [Retry 1/5].\n{'eval_loss': 2.816462278366089, 'eval_runtime': 954.8041, 'eval_samples_per_second': 17.16, 'eval_steps_per_second': 0.134, 'epoch': 0.1, 'num_input_tokens_seen': 872415232}\n{'loss': 2.867, 'grad_norm': 0.3173666000366211, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 873463808}\n{'loss': 2.8701, 'grad_norm': 0.3399354815483093, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 874512384}\n{'loss': 2.8575, 'grad_norm': 0.36704689264297485, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 875560960}\n{'loss': 2.9582, 'grad_norm': 0.33231136202812195, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 876609536}\n{'loss': 2.7719, 'grad_norm': 0.34316956996917725, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 877658112}\n{'loss': 2.8915, 'grad_norm': 0.3483976423740387, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 878706688}\n{'loss': 2.7566, 'grad_norm': 0.3104913532733917, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 879755264}\n{'loss': 3.0013, 'grad_norm': 0.38844239711761475, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 880803840}\n{'loss': 2.5568, 'grad_norm': 0.40875244140625, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 881852416}\n{'loss': 2.8336, 'grad_norm': 0.3538399934768677, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 882900992}\n{'loss': 2.9391, 'grad_norm': 0.3494492471218109, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 883949568}\n{'loss': 2.8535, 'grad_norm': 0.3472343981266022, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 884998144}\n{'loss': 2.9836, 'grad_norm': 0.34867390990257263, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 886046720}\n{'loss': 2.8416, 'grad_norm': 0.3527415096759796, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 887095296}\n{'loss': 2.8756, 'grad_norm': 0.3338777422904968, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 888143872}\n{'loss': 2.8428, 'grad_norm': 0.3345812261104584, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 889192448}\n{'loss': 2.8977, 'grad_norm': 0.31487980484962463, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 890241024}\n{'loss': 2.9543, 'grad_norm': 0.3655254542827606, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 891289600}\n{'loss': 2.9423, 'grad_norm': 0.33075806498527527, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 892338176}\n{'loss': 2.9001, 'grad_norm': 0.34644609689712524, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 893386752}\n{'loss': 2.9029, 'grad_norm': 0.39070528745651245, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 894435328}\n{'loss': 2.9101, 'grad_norm': 0.39556533098220825, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 895483904}\n{'loss': 2.8119, 'grad_norm': 0.39002978801727295, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 896532480}\n{'loss': 3.0102, 'grad_norm': 0.37797507643699646, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 897581056}\n{'loss': 2.666, 'grad_norm': 0.4306756258010864, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 898629632}\n{'loss': 2.9257, 'grad_norm': 0.4526049494743347, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 899678208}\n{'loss': 2.8196, 'grad_norm': 0.3978416621685028, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 900726784}\n{'loss': 2.9057, 'grad_norm': 0.3925896883010864, 'learning_rate': 0.001, 'epoch': 0.1, 'num_input_tokens_seen': 901775360}\n{'loss': 3.0017, 'grad_norm': 0.45828214287757874, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 902823936}\n{'loss': 2.89, 'grad_norm': 0.4745008647441864, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 903872512}\n{'loss': 2.7335, 'grad_norm': 0.4270082116127014, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 904921088}\n{'loss': 2.8234, 'grad_norm': 0.38832950592041016, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 905969664}\n{'loss': 2.8618, 'grad_norm': 0.3907729387283325, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 907018240}\n{'loss': 2.8703, 'grad_norm': 0.368655264377594, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 908066816}\n{'loss': 2.8321, 'grad_norm': 0.41538506746292114, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 909115392}\n{'loss': 2.886, 'grad_norm': 0.41877180337905884, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 910163968}\n{'loss': 2.6224, 'grad_norm': 0.33238673210144043, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 911212544}\n{'loss': 2.8617, 'grad_norm': 0.4095931351184845, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 912261120}\n{'loss': 2.8172, 'grad_norm': 0.41708603501319885, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 913309696}\n{'loss': 2.7658, 'grad_norm': 0.37449270486831665, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 914358272}\n{'loss': 2.9042, 'grad_norm': 0.3935737609863281, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 915406848}\n{'loss': 2.7612, 'grad_norm': 0.3586251735687256, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 916455424}\n{'loss': 2.8785, 'grad_norm': 0.3712047338485718, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 917504000}\n{'loss': 2.739, 'grad_norm': 0.37707045674324036, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 918552576}\n{'loss': 2.8372, 'grad_norm': 0.3432702422142029, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 919601152}\n{'loss': 2.5638, 'grad_norm': 0.3493041396141052, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 920649728}\n{'loss': 2.8759, 'grad_norm': 0.3401539623737335, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 921698304}\n{'loss': 3.0048, 'grad_norm': 0.4632040858268738, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 922746880}\n{'loss': 2.9394, 'grad_norm': 0.4968065023422241, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 923795456}\n{'loss': 2.8441, 'grad_norm': 0.5426673889160156, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 924844032}\n{'loss': 2.9975, 'grad_norm': 0.4630672037601471, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 925892608}\n{'loss': 2.9584, 'grad_norm': 0.38806748390197754, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 926941184}\n{'loss': 2.8904, 'grad_norm': 0.39797642827033997, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 927989760}\n{'loss': 2.5774, 'grad_norm': 0.4063512980937958, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 929038336}\n{'loss': 2.812, 'grad_norm': 0.3161136209964752, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 930086912}\n{'loss': 2.7483, 'grad_norm': 0.3628361225128174, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 931135488}\n{'loss': 2.7916, 'grad_norm': 0.37376269698143005, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 932184064}\n{'loss': 2.7985, 'grad_norm': 0.3399117887020111, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 933232640}\n{'loss': 2.7107, 'grad_norm': 0.3453179597854614, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 934281216}\n{'loss': 2.9254, 'grad_norm': 0.39461833238601685, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 935329792}\n{'loss': 2.8487, 'grad_norm': 0.3668413460254669, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 936378368}\n{'loss': 2.7928, 'grad_norm': 0.28304487466812134, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 937426944}\n{'loss': 2.8503, 'grad_norm': 0.35816267132759094, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 938475520}\n{'loss': 3.0328, 'grad_norm': 0.3540339469909668, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 939524096}\n[2025-03-11 03:46:08 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3b8321b9-2d88-4bfa-9eca-b201c444cba3)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk5/example_holdout_405.jsonl.zst\n[2025-03-11 03:46:08 WARNING] Retrying in 1s [Retry 1/5].\n[2025-03-11 03:53:27 WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a98a238a-c0a4-4295-8502-316a89a7ae29)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk1/example_holdout_2524.jsonl.zst\n[2025-03-11 03:53:27 WARNING] Retrying in 1s [Retry 1/5].\n{'eval_loss': 2.7651162147521973, 'eval_runtime': 687.962, 'eval_samples_per_second': 23.815, 'eval_steps_per_second': 0.186, 'epoch': 0.11, 'num_input_tokens_seen': 939524096}\n{'loss': 2.9368, 'grad_norm': 0.34962671995162964, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 940572672}\n{'loss': 2.3627, 'grad_norm': 0.37516310811042786, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 941621248}\n{'loss': 2.8854, 'grad_norm': 0.3487492501735687, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 942669824}\n{'loss': 2.7892, 'grad_norm': 0.37180987000465393, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 943718400}\n{'loss': 2.8067, 'grad_norm': 0.3387952744960785, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 944766976}\n{'loss': 2.841, 'grad_norm': 0.32076528668403625, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 945815552}\n{'loss': 2.7965, 'grad_norm': 0.3348572552204132, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 946864128}\n{'loss': 2.6788, 'grad_norm': 0.3531329929828644, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 947912704}\n{'loss': 2.7276, 'grad_norm': 0.300353467464447, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 948961280}\n{'loss': 2.8189, 'grad_norm': 0.3258875012397766, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 950009856}\n{'loss': 2.8388, 'grad_norm': 0.3434987962245941, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 951058432}\n{'loss': 2.856, 'grad_norm': 0.33045029640197754, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 952107008}\n{'loss': 2.658, 'grad_norm': 0.34896957874298096, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 953155584}\n{'loss': 2.8484, 'grad_norm': 0.3819083273410797, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 954204160}\n{'loss': 2.8402, 'grad_norm': 0.39541998505592346, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 955252736}\n{'loss': 2.8281, 'grad_norm': 0.3843367397785187, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 956301312}\n{'loss': 2.8339, 'grad_norm': 0.4067714214324951, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 957349888}\n{'loss': 2.8693, 'grad_norm': 0.3071018159389496, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 958398464}\n{'loss': 2.6747, 'grad_norm': 0.3676702380180359, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 959447040}\n{'loss': 2.6961, 'grad_norm': 0.357799232006073, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 960495616}\n{'loss': 2.7944, 'grad_norm': 0.318391352891922, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 961544192}\n{'loss': 2.8084, 'grad_norm': 0.32000190019607544, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 962592768}\n{'loss': 2.8024, 'grad_norm': 0.3250137269496918, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 963641344}\n{'loss': 2.7951, 'grad_norm': 0.33021438121795654, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 964689920}\n{'loss': 2.8069, 'grad_norm': 0.3257495164871216, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 965738496}\n{'loss': 2.8148, 'grad_norm': 0.3608018159866333, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 966787072}\n[2025-03-11 04:13:12 WARNING] '(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 36a7cc72-4605-416a-8742-59488d719150)')' thrown while requesting GET https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/train/chunk1/example_train_5267.jsonl.zst\n[2025-03-11 04:13:12 WARNING] Retrying in 1s [Retry 1/5].\n{'loss': 2.8089, 'grad_norm': 0.3657573163509369, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 967835648}\n{'loss': 2.8243, 'grad_norm': 0.3791966736316681, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 968884224}\n{'loss': 2.6837, 'grad_norm': 0.4036826193332672, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 969932800}\n{'loss': 2.6694, 'grad_norm': 0.34643635153770447, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 970981376}\n{'loss': 2.8455, 'grad_norm': 0.35321497917175293, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 972029952}\n{'loss': 2.5156, 'grad_norm': 0.3488744795322418, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 973078528}\n{'loss': 2.7185, 'grad_norm': 0.33396172523498535, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 974127104}\n{'loss': 2.856, 'grad_norm': 0.36425134539604187, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 975175680}\n{'loss': 2.7639, 'grad_norm': 0.34361588954925537, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 976224256}\n{'loss': 2.7777, 'grad_norm': 0.45501893758773804, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 977272832}\n{'loss': 2.8692, 'grad_norm': 0.4391760230064392, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 978321408}\n{'loss': 2.7885, 'grad_norm': 0.385729044675827, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 979369984}\n{'loss': 2.8622, 'grad_norm': 0.4122815728187561, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 980418560}\n{'loss': 2.674, 'grad_norm': 0.3223947584629059, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 981467136}\n{'loss': 2.7148, 'grad_norm': 0.39820024371147156, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 982515712}\n{'loss': 2.6975, 'grad_norm': 0.38311144709587097, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 983564288}\n{'loss': 2.8515, 'grad_norm': 0.4324709177017212, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 984612864}\n{'loss': 2.5684, 'grad_norm': 0.3579341471195221, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 985661440}\n{'loss': 2.9478, 'grad_norm': 0.4081536531448364, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 986710016}\n{'loss': 2.7375, 'grad_norm': 0.4332145154476166, 'learning_rate': 0.001, 'epoch': 0.11, 'num_input_tokens_seen': 987758592}\n{'loss': 2.7773, 'grad_norm': 0.43510711193084717, 'learning_rate': 0.001, 'epoch': 0.12, 'num_input_tokens_seen': 988807168}\n...\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/utils/file_utils.py\", line 1378, in _iter_from_urlpaths\n    raise FileNotFoundError(urlpath)\nFileNotFoundError: zstd://example_train_1215.jsonl::hf://datasets/cerebras/SlimPajama-627B@2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/train/chunk9/example_train_1215.jsonl.zst\n```\n\n</details>"
      },
      {
        "user": "bauwenst",
        "body": "Two more today:\n```python\nFileNotFoundError: zstd://example_holdout_5012.jsonl::hf://datasets/cerebras/SlimPajama-627B@2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk4/example_holdout_5012.jsonl.zst\n```\nand\n```python\nFileNotFoundError: zstd://example_holdout_3073.jsonl::hf://datasets/cerebras/SlimPajama-627B@2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/validation/chunk2/example_holdout_3073.jsonl.zst\n```\nboth of which exist on the hub ([here](https://huggingface.co/datasets/cerebras/SlimPajama-627B/blob/main/validation/chunk4/example_holdout_5012.jsonl.zst) and [here](https://huggingface.co/datasets/cerebras/SlimPajama-627B/blob/main/validation/chunk2/example_holdout_3073.jsonl.zst))."
      },
      {
        "user": "Rabona17",
        "body": "I also observe the same thing when using streaming with DCLM dataset with 64 GPUs. I have tried ```export HF_DATASETS_STREAMING_PARALLELISM=1``` but doesn't help."
      }
    ]
  },
  {
    "issue_number": 6348,
    "title": "Parquet stream-conversion fails to embed images/audio files from gated repos",
    "author": "severo",
    "state": "open",
    "created_at": "2023-10-25T12:12:44Z",
    "updated_at": "2025-04-17T12:21:43Z",
    "labels": [
      "bug"
    ],
    "body": "it seems to be an issue with datasets not passing the token to embed_table_storage when generating a dataset\r\n\r\nSee https://github.com/huggingface/datasets-server/issues/2010",
    "comments": [
      {
        "user": "pr0mila",
        "body": "I have created a project to stream audio in the datasets viewer on Hugging Face using Parquet.\n\nhttps://github.com/pr0mila/ParquetToHuggingFace"
      }
    ]
  },
  {
    "issue_number": 6602,
    "title": "Index error when data is large",
    "author": "ChenchaoZhao",
    "state": "open",
    "created_at": "2024-01-18T23:00:47Z",
    "updated_at": "2025-04-16T04:13:01Z",
    "labels": [],
    "body": "### Describe the bug\n\nAt `save_to_disk` step, the `max_shard_size` by default is `500MB`. However, one row of the dataset might be larger than `500MB` then the saving will throw an index error. Without looking at the source code, the bug is due to wrong calculation of number of shards which i think is \r\n`total_size / min(max_shard_size, row_size)` which should be `total_size / max(max_shard_size, row_size)`\r\n\r\nThe fix is setting a larger `max_shard_size`\n\n### Steps to reproduce the bug\n\n1. create a dataset with large dense tensors per row\r\n2. set a small `max_shard_size` say 1MB\r\n3. `save_to_disk`\n\n### Expected behavior\n\n```\r\nraise IndexError(f\"Index {index} out of range for dataset of size {size}.\")\r\n\r\nIndexError: Index 10 out of range for dataset of size 10.\r\n```\n\n### Environment info\n\n- `datasets` version: 2.16.0\r\n- Platform: Linux-5.10.201-168.748.amzn2int.x86_64-x86_64-with-glibc2.26\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.12.2",
    "comments": [
      {
        "user": "BaoLocPham",
        "body": "I'm facing this problem while doing my translation of [mteb/stackexchange-clustering](https://huggingface.co/datasets/mteb/stackexchange-clustering). each row has lots of samples (up to 100k samples), because in this dataset, each row represent multiple clusters.\nmy hack is to setting `max_shard_size` to 20Gb or even larger\n```py\nfinal_dataset.push_to_hub(\n        output_dataset, \n        private=True,\n        max_shard_size=\"20GB\"  # This will ensure appropriate sharding based on data size\n    )\n```\nIt will work, but depends on your data size. "
      }
    ]
  },
  {
    "issue_number": 7508,
    "title": "Iterating over Image feature columns is extremely slow",
    "author": "sohamparikh",
    "state": "open",
    "created_at": "2025-04-10T19:00:54Z",
    "updated_at": "2025-04-15T17:57:08Z",
    "labels": [],
    "body": "We are trying to load datasets where the image column stores `PIL.PngImagePlugin.PngImageFile` images. However, iterating over these datasets is extremely slow. \nWhat I have found:\n1. It is the presence of the image column that causes the slowdown. Removing the column from the dataset results in blazingly fast (as expected) times\n2. It is ~2x faster to iterate when the column contains a single image as opposed to a list of images i.e., the feature is a Sequence of Image objects. We often need multiple images per sample, so we need to work with a list of images\n3. It is ~17x faster to store paths to PNG files and load them using `PIL.Image.open`, as opposed to iterating over a `Dataset` with an Image column, and ~30x faster compared to `Sequence` of `Image`s. See a simple script below with an openly available dataset.\n\n\nIt would be great to understand the standard practices for storing and loading multimodal datasets (image + text). \n\nhttps://huggingface.co/docs/datasets/en/image_load seems a bit underdeveloped? (e.g., `dataset.decode` only works with `IterableDataset`, but it's not clear from the doc)\n\nThanks!\n\n```python\nfrom datasets import load_dataset, load_from_disk\nfrom PIL import Image\nfrom pathlib import Path\n\nds = load_dataset(\"getomni-ai/ocr-benchmark\")\n\nfor idx, sample in enumerate(ds[\"test\"]):\n    image = sample[\"image\"]\n    image.save(f\"/tmp/ds_files/images/image_{idx}.png\")\n\nds.save_to_disk(\"/tmp/ds_columns\")\n\n# Remove the 'image' column\nds[\"test\"] = ds[\"test\"].remove_columns([\"image\"])\n\n# Create image paths for each sample\nimage_paths = [f\"images/image_{idx}.png\" for idx in range(len(ds[\"test\"]))]\n\n# Add the 'image_path' column to the dataset\nds[\"test\"] = ds[\"test\"].add_column(\"image_path\", image_paths)\n\n# Save the updated dataset\nds.save_to_disk(\"/tmp/ds_files\")\nfiles_path = Path(\"/tmp/ds_files\")\ncolumn_path = Path(\"/tmp/ds_columns\")\n\n# load and benchmark\nds_file = load_from_disk(files_path)\nds_column = load_from_disk(column_path)\nimport time\n\n\nimages_files = []\nstart = time.time()\nfor idx in range(len(ds_file[\"test\"])):\n    image_path = files_path / ds_file[\"test\"][idx][\"image_path\"]\n    image = Image.open(image_path)\n    images_files.append(image)\nend = time.time()\nprint(f\"Time taken to load images from files: {end - start} seconds\")\n\n# Time taken to load images from files: 1.2364635467529297 seconds\n\n\nimages_column = []\nstart = time.time()\nfor idx in range(len(ds_column[\"test\"])):\n    images_column.append(ds_column[\"test\"][idx][\"image\"])\nend = time.time()\nprint(f\"Time taken to load images from columns: {end - start} seconds\")\n\n# Time taken to load images from columns: 20.49347186088562 seconds\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Could it be because the `Image()` type in dataset does `image  = Image.open(image_path)` and also `image.load()` which actually loads the image data in memory ? This is needed to avoid too many open files issues, see https://github.com/huggingface/datasets/issues/3985"
      },
      {
        "user": "sohamparikh",
        "body": "Yes, that seems to be it. For my purposes, I've cast the column to `Image(decode=False)`, and only load the images when necessary, which is much much faster"
      }
    ]
  },
  {
    "issue_number": 7502,
    "title": "`load_dataset` of size 40GB creates a cache of >720GB",
    "author": "pietrolesci",
    "state": "closed",
    "created_at": "2025-04-07T16:52:34Z",
    "updated_at": "2025-04-15T15:22:12Z",
    "labels": [],
    "body": "Hi there,\n\nI am trying to load a dataset from the Hugging Face Hub and split it into train and validation splits. Somehow, when I try to do it with `load_dataset`, it exhausts my disk quota. So, I tried manually downloading the parquet files from the hub and loading them as follows:\n\n```python\n ds = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"parquet\", \n                data_dir=f\"{local_dir}/{tok}\", \n                cache_dir=cache_dir, \n                num_proc=min(12, os.cpu_count()),   # type: ignore\n                split=ReadInstruction(\"train\", from_=0, to=NUM_TRAIN, unit=\"abs\"),  # type: ignore\n            ),\n            \"validation\": load_dataset(\n                \"parquet\", \n                data_dir=f\"{local_dir}/{tok}\", \n                cache_dir=cache_dir, \n                num_proc=min(12, os.cpu_count()),   # type: ignore\n                split=ReadInstruction(\"train\", from_=NUM_TRAIN, unit=\"abs\"),  # type: ignore\n            )\n        }\n    )\n\n```\n\nwhich still strangely creates 720GB of cache. In addition, if I remove the raw parquet file folder (`f\"{local_dir}/{tok}\"` in this example), I am not able to load anything. So, I am left wondering what this cache is doing. Am I missing something? Is there a solution to this problem?\n\nThanks a lot in advance for your help!\n\nA related issue: https://github.com/huggingface/transformers/issues/10204#issue-809007443.\n\n---\n\nPython: 3.11.11\ndatasets: 3.5.0\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Parquet is a compressed format. When you load a dataset, it uncompresses the Parquet data into Arrow data on your disk. That's why you can indeed end up with 720GB of uncompressed data on disk. The uncompression is needed to enable performant dataset objects (especially for random access).\n\nTo save some storage you can instead load the dataset with `streaming=True`. This way you get an `IterableDataset` that reads the Parquet data iteratively without ever writing to disk.\n\nPS: `ReadInstruction` might not be implemented for `streaming=True`, if it's the case you can use `ds.take()` and `ds.skip()` instead"
      },
      {
        "user": "pietrolesci",
        "body": "Hi @lhoestq, thanks a lot for your answer. This makes perfect sense. I will try using the streaming mode. Closing the issue."
      }
    ]
  },
  {
    "issue_number": 7494,
    "title": "Broken links in pdf loading documentation",
    "author": "VyoJ",
    "state": "closed",
    "created_at": "2025-04-02T06:45:22Z",
    "updated_at": "2025-04-15T13:36:25Z",
    "labels": [],
    "body": "### Describe the bug\n\nHi, just a couple of small issues I ran into while reading the docs for [loading pdf data](https://huggingface.co/docs/datasets/main/en/document_load):\n\n1. The link for the [`Create a pdf dataset`](https://huggingface.co/docs/datasets/main/en/document_load#pdffolder) points to https://huggingface.co/docs/datasets/main/en/pdf_dataset instead of https://huggingface.co/docs/datasets/main/en/document_dataset and hence gives a 404 error.\n\n2. At the top of the page, it's mentioned that to work with pdf datasets we need to have the `pdfplumber` package installed but the link to its installation guide points to `pytorch/vision` [installation instructions](https://github.com/pytorch/vision#installation) instead of `pdfplumber`'s [guide](https://github.com/jsvine/pdfplumber#installation)\n\nI love the work on enabling pdf dataset support and these small tweaks would help everyone navigate the docs better. Thanks!\n\n### Steps to reproduce the bug\n\nThe issue is on the [Load Document Data](https://huggingface.co/docs/datasets/main/en/document_load) page of the datasets docs.\n\n### Expected behavior\n\n1. For solving the first issue, I went through the [source .mdx code](https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx?plain=1#L188) of the datasets docs and found that the link is pointing to `./pdf_dataset` instead of `./document_dataset`\n\n2. For the second issue, I went through the [source .mdx code](https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx?plain=1#L13) of the datasets docs and found that the link is `pytorch/vision` [installation instructions](https://github.com/pytorch/vision#installation) instead of `pdfplumber`'s [guide](https://github.com/jsvine/pdfplumber#installation)\n\nJust replacing these two links should fix the bugs\n\n### Environment info\n\ndatasets v3.5.0 (main at the time of writing)",
    "comments": [
      {
        "user": "lhoestq",
        "body": "thanks for reporting ! I fixed the links, the docs will be updated in the next release"
      }
    ]
  },
  {
    "issue_number": 7518,
    "title": "num_proc parallelization works only for first ~10s.",
    "author": "pshishodiaa",
    "state": "open",
    "created_at": "2025-04-15T11:44:03Z",
    "updated_at": "2025-04-15T13:12:13Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen I try to load an already downloaded dataset with num_proc=64, the speed is very high for the first 10-20 seconds acheiving 30-40K samples / s, and 100% utilization for all cores but it soon drops to <= 1000 with almost 0% utilization for most cores. \n\n### Steps to reproduce the bug\n\n```\n// download dataset with cli\n!huggingface-cli download --repo-type dataset timm/imagenet-1k-wds --max-workers 32\n\nfrom datasets import load_dataset\nds = load_dataset(\"timm/imagenet-1k-wds\", num_proc=64)\n```\n\n\n### Expected behavior\n\n100% core utilization throughout. \n\n### Environment info\n\nAzure A100-80GB, 16 cores VM\n\n![Image](https://github.com/user-attachments/assets/69d00fe3-d720-4474-9439-21e046d85034)",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi, can you check if the processes are still alive ? It's a bit weird because `datasets` does check if processes crash and return an error in that case"
      },
      {
        "user": "pshishodiaa",
        "body": "Thank you for reverting quickly. I digged a bit, and realized my disk's IOPS is also limited - which is causing this. will check further and report if it's an issue of hf datasets' side or mine. "
      }
    ]
  },
  {
    "issue_number": 7496,
    "title": "Json builder: Allow features to override problematic Arrow types",
    "author": "edmcman",
    "state": "open",
    "created_at": "2025-04-02T19:27:16Z",
    "updated_at": "2025-04-15T13:06:09Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nIn the JSON builder, use explicitly requested feature types before or while converting to Arrow.\n\n### Motivation\n\nWorking with JSON datasets is really hard because of Arrow.  At the very least, it seems like it should be possible to work-around these problems by explicitly setting problematic columns's types.  But it seems like this is not possible because the features are only used *after* converting to arrow.\n\nHere's a simple example where the Arrow error could potentially be avoided by converting the column to a string: https://colab.research.google.com/drive/16QHRdbUwKSrpwVfGwu8V8AHr8v2dv0dt?usp=sharing\n\n### Your contribution\n\nMaybe with some guidance.  I'm not very familiar with arrow or pandas.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! It would be cool indeed, currently the JSON data are generally loaded here: \n\nhttps://github.com/huggingface/datasets/blob/90e5bf8a8599b625d6103ee5ac83b98269991141/src/datasets/packaged_modules/json/json.py#L137-L140\n\nMaybe we can pass a Arrow `schema` to avoid errors ?"
      }
    ]
  },
  {
    "issue_number": 7506,
    "title": "HfHubHTTPError: 429 Client Error: Too Many Requests for URL when trying to access Fineweb-10BT on 4A100 GPUs using SLURM",
    "author": "calvintanama",
    "state": "open",
    "created_at": "2025-04-09T06:32:04Z",
    "updated_at": "2025-04-15T13:04:31Z",
    "labels": [],
    "body": "### Describe the bug\n\nI am trying to run some finetunings on 4 A100 GPUs using SLURM using axolotl training framework which in turn uses Huggingface's Trainer and Accelerate on [Fineweb-10BT](https://huggingface.co/datasets/HuggingFaceFW/fineweb), but I end up running into 429 Client Error: Too Many Requests for URL error when I call next(dataloader_iter). Funny is, that I can run some test fine tuning (for just 200 training steps) in 1 A100 GPU using SLURM. Is there any rate limiter set for querying dataset? I could run the fine tuning with the same settings (4 A100 GPUs in SLURM) last month.\n\n### Steps to reproduce the bug\n\nYou would need a server installed with SLURM\n\n1. Create conda environment\n1.1 conda create -n example_env -c conda-forge gxx=11 python=3.10\n1.2 conda activate example_env\n1.3 pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n1.4 conda install nvidia/label/cuda-12.4.0::cuda-toolkit\n1.5 Download flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n1.6 pip3 install packaging\n1.7 pip3 install ninja\n1.8 pip3 install mlflow\n1.9 Clone https://github.com/calvintanama/axolotl.git\n1.10 `cd` to `axolotl`\n1.11 pip3 install -e '.[deepspeed]'\n\n2. Run the training\n2.1. Create a folder called `config_run` in axolotl directory\n2.2. Copy `config/phi3_pruned_extra_pretrain_22_29_bottleneck_residual_8_a100_4.yaml` to `config_run`\n2.3. Change yaml file in the `config_run` accordingly\n2.4. Change directory and conda environment name in `jobs/train_phi3_22_29_bottleneck_residual_8_a100_4_temp.sh`\n2.5. `jobs/train_phi3_22_29_bottleneck_residual_8_a100_4_temp.sh`\n\n### Expected behavior\n\nThis should not cause any error, but gotten\n\n```\nFile \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/accelerate/data_loader.py\", line 552, in __iter__\n[rank3]:     current_batch = next(dataloader_iter)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n[rank3]:     data = self._next_data()\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n[rank3]:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n[rank3]:     data.append(next(self.dataset_iter))\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/accelerate/data_loader.py\", line 338, in __iter__\n[rank3]:     for element in self.dataset:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 2266, in __iter__\n[rank3]:     for key, example in ex_iterable:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1866, in __iter__\n[rank3]:     for key, example in self.ex_iterable:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1084, in __iter__\n[rank3]:     yield from self._iter()\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1263, in _iter\n[rank3]:     for key, transformed_example in outputs:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1258, in <genexpr>\n[rank3]:     outputs = (\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1244, in iter_outputs\n[rank3]:     for i, key_example in inputs_iterator:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1106, in iter_batched_inputs\n[rank3]:     for key, example in iterator:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1866, in __iter__\n[rank3]:     for key, example in self.ex_iterable:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1535, in __iter__\n[rank3]:     for x in self.ex_iterable:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 374, in __iter__\n[rank3]:     for key, pa_table in self.generate_tables_fn(**gen_kwags):\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 90, in _generate_tables\n[rank3]:     if parquet_fragment.row_groups:\n[rank3]:   File \"pyarrow/_dataset_parquet.pyx\", line 386, in pyarrow._dataset_parquet.ParquetFileFragment.row_groups.__get__\n[rank3]:   File \"pyarrow/_dataset_parquet.pyx\", line 393, in pyarrow._dataset_parquet.ParquetFileFragment.metadata.__get__\n[rank3]:   File \"pyarrow/_dataset_parquet.pyx\", line 382, in pyarrow._dataset_parquet.ParquetFileFragment.ensure_complete_metadata\n[rank3]:   File \"pyarrow/error.pxi\", line 89, in pyarrow.lib.check_status\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 827, in read_with_retries\n[rank3]:     out = read(*args, **kwargs)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 1013, in read\n[rank3]:     return super().read(length)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/fsspec/spec.py\", line 1941, in read\n[rank3]:     out = self.cache._fetch(self.loc, self.loc + length)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/fsspec/caching.py\", line 234, in _fetch\n[rank3]:     self.cache = self.fetcher(start, end)  # new block replaces old\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 976, in _fetch_range\n[rank3]:     hf_raise_for_status(r)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 482, in hf_raise_for_status\n[rank3]:     raise _format(HfHubHTTPError, str(e), response) from e\n[rank3]: huggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/datasets/HuggingFaceFW/fineweb/resolve/0f039043b23fe1d4eed300b504aa4b4a68f1c7ba/sample/10BT/006_00000.parquet\n```\n\n### Environment info\n\n- datasets 3.5.0\n- torch 2.5.1\n- transformers 4.46.2",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! make sure to be logged in with your HF account (e.g. using `huggingface-cli login` or passing `token=` to `load_dataset()`), otherwise you'll get rate limited at one point"
      }
    ]
  },
  {
    "issue_number": 7500,
    "title": "Make `with_format` correctly indicate that a `Dataset` is compatible with PyTorch's `Dataset` class",
    "author": "benglewis",
    "state": "open",
    "created_at": "2025-04-06T09:56:09Z",
    "updated_at": "2025-04-15T12:57:39Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nCurrently `datasets` does not correctly indicate to the Python type-checker (e.g. `pyright` / `Pylance`) that the output of `with_format` is compatible with PyTorch's `Dataloader` since it does not indicate that the HuggingFace `Dataset` is compatible with the PyTorch `Dataset` class. It would be great if we could get the typing to work nicely.\n\n### Motivation\n\nTo avoid casting types in our Python code.\n\n### Your contribution\n\nI would be happy to contribute a PR if this is something that may be accepted and could work with the current approach.\nThis doesn't have to be for just PyTorch, but I imagine that the same thing would be useful for `tensorflow` and such, but we only have a need for PyTorch at this stage.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Does the torch `DataLoader` really require the dataset to be a subclass of `torch.utils.data.Dataset` ? Or is there a simpler type we could use ?\n\nPS: also note that a dataset without `with_format()` can also be used in a torch `DataLoader` . Calling `with_format(\"torch\")` simply makes the output of the dataset torch Tensors in an efficient way."
      }
    ]
  },
  {
    "issue_number": 7507,
    "title": "Front-end statistical data quantity deviation",
    "author": "rangehow",
    "state": "open",
    "created_at": "2025-04-10T02:51:38Z",
    "updated_at": "2025-04-15T12:54:51Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhile browsing the dataset at https://huggingface.co/datasets/NeuML/wikipedia-20250123, I noticed that a dataset with nearly 7M entries was estimated to be only 4M in size—almost half the actual amount. According to the post-download loading and the dataset_info (https://huggingface.co/datasets/NeuML/wikipedia-20250123/blob/main/train/dataset_info.json), the true data volume is indeed close to 7M. This significant discrepancy could mislead users when sorting datasets by row count. Why not directly retrieve this information from dataset_info?\n\nNot sure if this is the right place to report this bug, but leaving it here for the team's awareness.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! the format of this dataset is not supported by the Dataset Viewer. It looks like this dataset was saved using `save_to_disk()` which is meant for local storage / easy reload without compression, not for sharing online."
      }
    ]
  },
  {
    "issue_number": 7503,
    "title": "Inconsistency between load_dataset and load_from_disk functionality",
    "author": "zzzzzec",
    "state": "open",
    "created_at": "2025-04-08T03:46:22Z",
    "updated_at": "2025-04-15T12:39:53Z",
    "labels": [],
    "body": "## Issue Description\n\nI've encountered confusion when using `load_dataset` and `load_from_disk` in the datasets library. Specifically, when working offline with the gsm8k dataset, I can load it using a local path:\n\n```python\nimport datasets\nds = datasets.load_dataset('/root/xxx/datasets/gsm8k', 'main')\n```\noutput:\n```text\nDatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 7473\n    })\n    test: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 1319\n    })\n})\n```\n\nThis works as expected. However, after processing the dataset (converting answer format from #### to \\boxed{})\n```python\nimport datasets\nds = datasets.load_dataset('/root/xxx/datasets/gsm8k', 'main')\nds_train = ds['train']\nds_test = ds['test']\nimport re\ndef convert(sample):\n    solution = sample['answer']\n    solution = re.sub(r'####\\s*(\\S+)', r'\\\\boxed{\\1}', solution)\n    sample = {\n        'problem': sample['question'],\n        'solution': solution\n    }\n    return sample\n\nds_train = ds_train.map(convert, remove_columns=['question', 'answer'])\nds_test = ds_test.map(convert,remove_columns=['question', 'answer'])\n```\n\n I saved it using save_to_disk:\n```python\nfrom datasets.dataset_dict import DatasetDict\ndata_dict = DatasetDict({\n    'train': ds_train,\n    'test': ds_test\n})\ndata_dict.save_to_disk('/root/xxx/datasets/gsm8k-new')\n```\nBut now I can only load it using load_from_disk:\n\n```python\nnew_ds = load_from_disk('/root/xxx/datasets/gsm8k-new')\n```\noutput:\n```text\nDatasetDict({\n    train: Dataset({\n        features: ['problem', 'solution'],\n        num_rows: 7473\n    })\n    test: Dataset({\n        features: ['problem', 'solution'],\n        num_rows: 1319\n    })\n})\n```\n\nAttempting to use load_dataset produces unexpected results:\n```python\nnew_ds = load_dataset('/root/xxx/datasets/gsm8k-new')\n```\noutput:\n```text\nDatasetDict({\n    train: Dataset({\n        features: ['_data_files', '_fingerprint', '_format_columns', '_format_kwargs', '_format_type', '_output_all_columns', '_split'],\n        num_rows: 1\n    })\n    test: Dataset({\n        features: ['_data_files', '_fingerprint', '_format_columns', '_format_kwargs', '_format_type', '_output_all_columns', '_split'],\n        num_rows: 1\n    })\n})\n```\nQuestions\n1. Why is it designed such that after using `save_to_disk`, the dataset cannot be loaded with `load_dataset`? For small projects with limited code, it might be relatively easy to change all instances of `load_dataset` to `load_from_disk`. However, for complex frameworks like TRL or lighteval, diving into the framework code to change `load_dataset` to `load_from_disk` is extremely tedious and error-prone.\nAdditionally, `load_from_disk` cannot load datasets directly downloaded from the hub, which means that if you need to modify a dataset, you have to choose between using `load_from_disk` or `load_dataset`. This creates an unnecessary dichotomy in the API and complicates workflow when working with modified datasets.\n2. What's the recommended approach for this use case? Should I manually process my gsm8k-new dataset to make it compatible with load_dataset? Is there a standard way to convert between these formats?\n\nthanks~",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! you can find more info here: https://github.com/huggingface/datasets/issues/5044#issuecomment-1263714347\n\n> What's the recommended approach for this use case? Should I manually process my gsm8k-new dataset to make it compatible with load_dataset? Is there a standard way to convert between these formats?\n\nYou can use push_to_hub() or to_parquet() for example"
      }
    ]
  },
  {
    "issue_number": 7504,
    "title": "BuilderConfig ParquetConfig(...) doesn't have a 'use_auth_token' key.",
    "author": "tteguayco",
    "state": "open",
    "created_at": "2025-04-08T10:55:03Z",
    "updated_at": "2025-04-15T12:36:28Z",
    "labels": [],
    "body": "### Describe the bug\n\nTrying to run the following fine-tuning script (based on this page [here](https://github.com/huggingface/instruction-tuned-sd)):\n\n```\n! accelerate launch /content/instruction-tuned-sd/finetune_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=${MODEL_ID} \\\n    --dataset_name=${DATASET_NAME} \\\n    --use_ema \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=512 --random_flip \\\n    --train_batch_size=2 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=500 \\\n    --checkpointing_steps=25 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=20 \\\n    --conditioning_dropout_prob=0.1 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --output_dir=${OUTPUT_DIR} \\\n    --original_image_column=before \\\n    --edit_prompt=prompt \\\n    --edited_image=after\n```\n\nbut I keep getting the following error:\n\n```\nTraceback (most recent call last):\n  File \"/content/instruction-tuned-sd/finetune_instruct_pix2pix.py\", line 1137, in <module>\n    main()\n  File \"/content/instruction-tuned-sd/finetune_instruct_pix2pix.py\", line 652, in main\n    dataset = load_dataset(\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 2129, in load_dataset\n    builder_instance = load_dataset_builder(\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 1886, in load_dataset_builder\n    builder_instance: DatasetBuilder = builder_cls(\n                                       ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 342, in __init__\n    self.config, self.config_id = self._create_builder_config(\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 590, in _create_builder_config\n    raise ValueError(f\"BuilderConfig {builder_config} doesn't have a '{key}' key.\")\nValueError: BuilderConfig ParquetConfig(name='default', version=0.0.0, data_dir=None, data_files={'train': ['data/train-*']}, description=None, batch_size=None, columns=None, features=None, filters=None) doesn't have a 'use_auth_token' key.\nTraceback (most recent call last):\n  File \"/usr/local/bin/accelerate\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n```\n\nAny ideas? `datasets` version should be `3.2.0`.\n\n### Steps to reproduce the bug\n\nJust running the script above.\n\n### Expected behavior\n\nNo errors\n\n### Environment info\n\nPython 3.11.11\n\ndatasets==3.2.0",
    "comments": [
      {
        "user": "Alex9154",
        "body": "I encountered the same error, have you resolved it?"
      },
      {
        "user": "lhoestq",
        "body": "Hi ! `use_auth_token` has been deprecated and removed some time ago. You should use `token` instead in `load_dataset()`"
      }
    ]
  },
  {
    "issue_number": 7497,
    "title": "How to convert videos to images?",
    "author": "Loki-Lu",
    "state": "open",
    "created_at": "2025-04-03T07:08:39Z",
    "updated_at": "2025-04-15T12:35:15Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nDoes someone know how to return the images from videos?\n\n### Motivation\n\nI am trying to use openpi(https://github.com/Physical-Intelligence/openpi) to finetune my Lerobot dataset(V2.0 and V2.1). I find that although the codedaset is v2.0, they are different. It seems like Lerobot V2.0 has two version, one is data include images infos and another one is separate to data and videos.\n\nDoes someone know how to return the images from videos?\n\n\n\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! there is some documentation here on how to read video frames: https://huggingface.co/docs/datasets/video_load"
      }
    ]
  },
  {
    "issue_number": 7493,
    "title": "push_to_hub does not upload videos",
    "author": "DominikVincent",
    "state": "open",
    "created_at": "2025-04-01T17:00:20Z",
    "updated_at": "2025-04-15T12:34:23Z",
    "labels": [],
    "body": "### Describe the bug\n\nHello,\n\nI would like to upload a video dataset (some .mp4 files and some segments within them), i.e. rows correspond to subsequences from videos. Videos might be referenced by several rows.\n\nI created a dataset locally and it references the videos and the video readers can read them correctly. I use push_to_hub() to upload the dataset to the hub.\n\nExpectation: A user uses `load_dataset` and can load the videos.\n\nHowever, the videos seem to be just referenced via paths on the computer and not uploaded to the hub. Therefore a target user cannot load the videos in the dataset.\n\n### Steps to reproduce the bug\n\n1. create a video dataset with paths e.g. { [\"videos\"]: [path1, path2, ...]}\n2. dataset.push_to_hub\n3. on a different computer (or same pc if relative paths are used in a different folder):\n```\ndataset = load_dataset(\"siplab/egosim\", split=\"train\")\nvideo = dataset[0][\"video_head\"]\n```\n\n3. will fail\n\n### Expected behavior\n\nExpectation: A user uses `load_dataset` and can load the videos.\n\n### Environment info\n\ndatasets                       3.1.0\nPython 3.8.18",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! the `Video` type is still experimental, and in particular `push_to_hub` doesn't upload videos at the moment (only the paths).\n\nThere is an open question to either upload the videos inside the Parquet files, or rather have them as separate files (which is great to enable remote seeking/streaming)"
      }
    ]
  },
  {
    "issue_number": 7516,
    "title": "unsloth/DeepSeek-R1-Distill-Qwen-32B server error",
    "author": "Editor-1",
    "state": "closed",
    "created_at": "2025-04-15T09:26:53Z",
    "updated_at": "2025-04-15T09:57:26Z",
    "labels": [],
    "body": "### Describe the bug\n\nhfhubhttperror: 500 server error: internal server error for url: https://huggingface.co/api/models/unsloth/deepseek-r1-distill-qwen-32b-bnb-4bit/commits/main (request id: root=1-67fe23fa-3a2150eb444c2a823c388579;de3aed68-c397-4da5-94d4-6565efd3b919) internal error - we're working hard to fix this as soon as possible!\n\n### Steps to reproduce the bug\n\nunsloth/DeepSeek-R1-Distill-Qwen-32B server error\n\n### Expected behavior\n\nNetwork repair\n\n### Environment info\n\nThe web side is also unavailable",
    "comments": []
  },
  {
    "issue_number": 7512,
    "title": ".map() fails if function uses pyvista",
    "author": "el-hult",
    "state": "open",
    "created_at": "2025-04-14T19:43:02Z",
    "updated_at": "2025-04-14T20:01:53Z",
    "labels": [],
    "body": "### Describe the bug\n\nUsing PyVista inside a .map() produces a crash with `objc[78796]: +[NSResponder initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.`\n\n### Steps to reproduce the bug\n\nRun \n\n```python\nimport numpy as np\nimport pyvista as pv\nimport datasets\n\ndata = [{\"coords\": np.random.rand(5, 3)} for _ in range(3)]\n\n\ndef render_point(example):\n  plotter = pv.Plotter(off_screen=True)\n  cloud = pv.PolyData(example[\"coords\"])\n  plotter.add_mesh(cloud)\n  img = plotter.screenshot(return_img=True)\n  return {\"image\": img}\n\n\n# breaks if num_proc>1\nds = datasets.Dataset.from_list(data).map(render_point, num_proc=2)\n\n```\n\n### Expected behavior\n\nIt should work. Just like when I use a process pool to make it work.\n\n```python\nimport numpy as np\nimport pyvista as pv\nimport multiprocessing\n\ndata = [{\"coords\": np.random.rand(5, 3)} for _ in range(3)]\n\n\ndef render_point(example):\n  plotter = pv.Plotter(off_screen=True)\n  cloud = pv.PolyData(example[\"coords\"])\n  plotter.add_mesh(cloud)\n  img = plotter.screenshot(return_img=True)\n  return {\"image\": img}\n\n\nif __name__ == \"__main__\":\n  with multiprocessing.Pool(processes=2) as pool:\n    results = pool.map(render_point, data)\n  print(results[0][\"image\"].shape)\n```\n\n### Environment info\n\n\n- `datasets` version: 3.3.2\n- Platform: macOS-15.3.2-arm64-arm-64bit\n- Python version: 3.11.10\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.10.0\n\nAnd then I suppose pyvista info is good to have.\n\n```python\nimport pyvista as pv\nprint(pv.Report())\n```\ngives \n--------------------------------------------------------------------------------\n  Date: Mon Apr 14 21:42:08 2025 CEST\n\n                OS : Darwin (macOS 15.3.2)\n            CPU(s) : 10\n           Machine : arm64\n      Architecture : 64bit\n               RAM : 32.0 GiB\n       Environment : IPython\n       File system : apfs\n        GPU Vendor : Apple\n      GPU Renderer : Apple M1 Max\n       GPU Version : 4.1 Metal - 89.3\n  MathText Support : True\n\n  Python 3.11.10 (main, Oct  7 2024, 23:25:27) [Clang 18.1.8 ]\n\n           pyvista : 0.44.2\n               vtk : 9.4.0\n             numpy : 2.2.2\n        matplotlib : 3.10.0\n            scooby : 0.10.0\n             pooch : 1.8.2\n            pillow : 11.1.0\n           imageio : 2.36.1\n             PyQt5 : 5.15.11\n           IPython : 8.30.0\n             scipy : 1.14.1\n              tqdm : 4.67.1\n        jupyterlab : 4.3.5\n      nest_asyncio : 1.6.0\n--------------------------------------------------------------------------------",
    "comments": [
      {
        "user": "el-hult",
        "body": "I found a similar (?) issue in https://github.com/huggingface/datasets/issues/6435, where someone had issues with forks and CUDA. According to https://huggingface.co/docs/datasets/main/en/process#multiprocessing we should do \n\n```\nfrom multiprocess import set_start_method\nset_start_method(\"spawn\")\n```\n\nto avoid the fork. The updated code\n\n```python\nimport numpy as np\nimport pyvista as pv\nimport datasets\nimport multiprocess\n\ndata = [{\"coords\": np.random.rand(5, 3)} for _ in range(3)]\n\ndef render_point(example):\n  plotter = pv.Plotter(off_screen=True)\n  cloud = pv.PolyData(example[\"coords\"])\n  plotter.add_mesh(cloud)\n  img = plotter.screenshot(return_img=True)\n  return {\"image\": img}\n\n\n# breaks if num_proc>1\nmultiprocess.set_start_method(\"spawn\")\nds = datasets.Dataset.from_list(data).map(render_point, num_proc=2)\n```\n\ninstead fails with `TypeError: fork_exec() takes exactly 23 arguments (21 given)` which also seems like a bug to me."
      }
    ]
  },
  {
    "issue_number": 7334,
    "title": "TypeError: Value.__init__() missing 1 required positional argument: 'dtype'",
    "author": "ghost",
    "state": "open",
    "created_at": "2024-12-15T04:08:46Z",
    "updated_at": "2025-04-14T10:25:12Z",
    "labels": [],
    "body": "### Describe the bug\n\nds = load_dataset(\r\n    \"./xxx.py\",\r\n    name=\"default\",\r\n    split=\"train\",\r\n)\r\n\r\nThe datasets does not support debugging locally anymore...\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\r\n    \"./repo.py\",\r\n    name=\"default\",\r\n    split=\"train\",\r\n)\r\n\r\nfor item in ds:\r\n    print(item)\r\n```\r\n\r\nIt works fine for \"username/repo\", but it does not work for \"./repo.py\" when debugging locally...\r\nRunning above code template will report TypeError: Value.__init__() missing 1 required positional argument: 'dtype'\n\n### Expected behavior\n\nfix this bug\n\n### Environment info\n\npython 3.10 datasets==2.21",
    "comments": [
      {
        "user": "jaffe-fly",
        "body": "same error \n```\ndata = load_dataset('/opt/deepseek_R1_finetune/hf_datasets/openai/gsm8k', 'main')[split] \n```"
      },
      {
        "user": "jaffe-fly",
        "body": "> same error\n> \n> ```\n> data = load_dataset('/opt/deepseek_R1_finetune/hf_datasets/openai/gsm8k', 'main')[split] \n> ```\n\nhttps://github.com/huggingface/open-r1/issues/204  this help me"
      }
    ]
  },
  {
    "issue_number": 4864,
    "title": "Allow pathlib PoxisPath in Dataset.read_json",
    "author": "changjonathanc",
    "state": "open",
    "created_at": "2022-08-19T12:59:17Z",
    "updated_at": "2025-04-11T17:22:48Z",
    "labels": [
      "enhancement"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\n```\r\nfrom pathlib import Path\r\nfrom datasets import Dataset\r\nds = Dataset.read_json(Path('data.json'))\r\n```\r\ncauses an error\r\n```\r\nAttributeError: 'PosixPath' object has no attribute 'decode'\r\n```\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt should be able to accept PosixPath and read the json from inside.",
    "comments": [
      {
        "user": "vvvm23",
        "body": "This same error will occur using `ds = datasets.load_dataset('json', data_files=['test.jsonl'])`"
      },
      {
        "user": "vvvm23",
        "body": "@cccntu I want to make a quick fix for this, but I am struggling to find where the json dataset builder is. Do you know?"
      },
      {
        "user": "changjonathanc",
        "body": "@vvvm23 I think you mean think:\r\n```python\r\nds = datasets.load_dataset('json', data_files=[Path('test.jsonl')])\r\n```\r\nAnd the place you want to modify is here:\r\n```\r\nutils/file_utils.py:64, in is_remote_url(url_or_filename)\r\n     63 def is_remote_url(url_or_filename: str) -> bool:\r\n---> 64     parsed = urlparse(url_or_filename)\r\n     65     return parsed.scheme in (\"http\", \"https\", \"s3\", \"gs\", \"hdfs\", \"ftp\")\r\n```\r\n\r\nProbably just need to check first if `url_or_filename` is [PathLike](https://docs.python.org/3/library/os.html#os.PathLike) and return False early.\r\n\r\nBtw, I tried installing from main, and ran my code above and got a different error. Probably because the API have changed.\r\n`AttributeError: module 'datasets' has no attribute 'read_json'`\r\n"
      }
    ]
  },
  {
    "issue_number": 4661,
    "title": "Concurrency bug when using same cache among several jobs",
    "author": "ioana-blue",
    "state": "open",
    "created_at": "2022-07-08T01:58:11Z",
    "updated_at": "2025-04-10T13:21:23Z",
    "labels": [
      "bug"
    ],
    "body": "## Describe the bug\r\nI used to see this bug with an older version of the datasets. It seems to persist. \r\n\r\nThis is my concrete scenario: I launch several evaluation jobs on a cluster in which I share the file system and I share the cache directory used by huggingface libraries. The evaluation jobs read the same *.csv files. If my jobs get all scheduled pretty much at the same time, there are all kinds of weird concurrency errors. Sometime it crashes silently. This time I got lucky that it crashed with a stack trace that I can share and maybe you get to the bottom of this. If you don't have a similar setup available, it may be hard to reproduce as you really need two jobs accessing the same file at the same time to see this type of bug.  \r\n\r\n## Steps to reproduce the bug\r\nI'm running a modified version of `run_glue.py` script adapted to my use case. I've seen the same problem when running some glue datasets as well (so it's not specific to loading the datasets from csv files).  \r\n\r\n## Expected results\r\nNo crash, concurrent access to the (intermediate) files just fine.\r\n\r\n## Actual results\r\nCrashes due to races/concurrency bugs. \r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.1.0\r\n\r\nStack trace that I just got with the crash (I've obfuscated some names, it should still be quite informative):\r\n\r\n```\r\nRunning tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\r\nTraceback (most recent call last):\r\n  File \"../../src/models//run_*******.py\", line 600, in <module>\r\n    main()\r\n  File \"../../src/models//run_*******.py\", line 444, in main\r\n    raw_datasets = raw_datasets.map(\r\n  File \"/*******//envs/tr-crt/lib/python3.8/site-packages/datasets/dataset_dict.py\", line 770, in map\r\n    {\r\n  File \"/*******//envs/tr-crt/lib/python3.8/site-packages/datasets/dataset_dict.py\", line 771, in <dictcomp>\r\n    k: dataset.map(\r\n  File \"/*******//envs/tr-crt/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2376, in map\r\n    return self._map_single(\r\n  File \"/*******/envs/tr-crt/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 551, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/*******//envs/tr-crt/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 518, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/*******/envs/tr-crt/lib/python3.8/site-packages/datasets/fingerprint.py\", line 458, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/*******//envs/tr-crt/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2776, in _map_single\r\n    buf_writer, writer, tmp_file = init_buffer_and_writer()\r\n  File \"/*******//envs/tr-crt/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2696, in init_buffer_and_writer\r\n    tmp_file = tempfile.NamedTemporaryFile(\"wb\", dir=os.path.dirname(cache_file_name), delete=False)\r\n  File \"/*******//envs/tr-crt/lib/python3.8/tempfile.py\", line 541, in NamedTemporaryFile\r\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\r\n  File \"/*******//envs/tr-crt/lib/python3.8/tempfile.py\", line 250, in _mkstemp_inner\r\n    fd = _os.open(file, flags, 0o600)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/*******/cache-transformers//transformers/csv/default-ef9cd184210742a7/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/tmps8l6j5yc'\r\n```\r\n\r\nAs I ran 100s of experiments last year for an empirical paper, I ran into this type of bugs several times. I found several bandaid/work-arounds, e.g., run one job first that caches the dataset => eliminate concurrency; OR use unique caches => eliminate concurrency (but increase storage space), etc. and it all works fine. \r\n\r\nI'd like to help you fixing this bug as it's really annoying to always apply the work arounds. Let me know what other info from my side could help you figure out the issue.\r\n\r\nThanks for your help!\r\n",
    "comments": [
      {
        "user": "ioana-blue",
        "body": "I can confirm that if I run one job first that processes the dataset, then I can run any jobs in parallel with no problem (no write-concurrency anymore...). "
      },
      {
        "user": "mariosasko",
        "body": "Hi! That's weird. It seems like the error points to the `mkstemp` function, but the official docs state the following:\r\n```\r\nThere are no race conditions in the file’s creation, assuming that the platform properly implements the [os.O_EXCL](https://docs.python.org/3/library/os.html#os.O_EXCL) flag for [os.open()](https://docs.python.org/3/library/os.html#os.open)\r\n```\r\nSo this could mean your platform doesn't support that flag.\r\n\r\n~~Can you please check if wrapping the temp file creation (the line `tmp_file = tempfile.NamedTemporaryFile(\"wb\", dir=os.path.dirname(cache_file_name), delete=False)` in `_map_single`) with the `multiprocess.Lock` fixes the issue?~~\r\nPerhaps wrapping the temp file creation in `_map_single`  with `filelock` could work:\r\n```python\r\nwith FileLock(lock_path):\r\n    tmp_file = tempfile.NamedTemporaryFile(\"wb\", dir=os.path.dirname(cache_file_name), delete=False)\r\n```\r\nCan you please check if that helps?"
      },
      {
        "user": "ygorg",
        "body": "**Edit**: while writing my comment I took the time the read previous comments. By wrapping `dl_manager.download_and_extract` with a **FileLock** it works like a charm ! Thx @mariosasko \n\nOS : MacOS 14.7.4 (intel)\nPython : 3.12\ndatasets : 3.5.0\n\nAdding to this, I had a similar problem when 2 process concurrently load different subsets of the same dataset that needs to be extracted. The use case is similar as OP : running a benchmark.\n\nThe dataloader needs to download and extract a zip file, with ~20 ~100Mo files.\n\nWhen 2 processes executes `load_dataset(\"TESTLOAD.py\", name=\"a\", trust_remote_code=True)` at the same time it is fine (there must some lock on `(\"TESTLOAD.py\", \"a\")`).\nBut when running `load_dataset(\"TESTLOAD.py\", name=\"a\", trust_remote_code=True)` and `load_dataset(\"TESTLOAD.py\", name=\"b\", trust_remote_code=True)` (cf. `test.py`.\n\nHere is what I managed to understand as a table using the scripts below. Step 3 is attested by the `os.listdir` in `TESTLOAD.py`.\n\n| steps | process a | process b |\n|---|---|---|\n| 1 | download | wait |\n| 2 | end of download | wait |\n| 3 | extracting | FAIL to open a not yet extracted file |\n| 4 | end of extraction | KO |\n| 5 | OK | KO |\n\n<details>\n\n<summary>TESTLOAD.py (dataloader)</summary>\n\n```python\nimport os\nimport datasets\n# from filelock import FileLock\n_URL = \"/Users/ygallina/Documents/dr-benchmark/GSC-v1.1_big.zip\"\n\nclass TESTLOAD(datasets.GeneratorBasedBuilder):\n\n\tBUILDER_CONFIGS = [\n\t\tdatasets.BuilderConfig(name='a'),\n\t\tdatasets.BuilderConfig(name='b')\n\t]\n\n\tdef _info(self):\n\t\tfeatures = datasets.Features({\n\t\t\t\"id\": datasets.Value(\"string\"),\n\t\t})\n\t\treturn datasets.DatasetInfo(features=features)\n\n\tdef _split_generators(self, dl_manager):\n\t\tc2p = {'a': \"Medline_GSC_en_fr_man.xml\", 'b': \"Medline_GSC_en_es_man.xml\"}\n\t\t# with FileLock(\"path/to/tmp.lock\"):\n\t\tdata_dir = dl_manager.download_and_extract(_URL)\n\n\t\tprint(os.listdir(data_dir))\n\n\t\tdata_dir = data_dir + \"/\" + c2p[self.config.name]\n\t\treturn [datasets.SplitGenerator(\n\t\t\tname=datasets.Split.TRAIN,\n\t\t\tgen_kwargs={\"data_dir\": data_dir}\n\t\t)]\n\n\tdef _generate_examples(self, data_dir):\n\t\tf = open(data_dir)\n\t\tf.close()\n\t\tyield 0, {'id': data_dir}\n```\n\n</details>\n\n\n<details>\n\n<summary>test.py (main file)</summary>\n\nCommands to execute the test\n```bash\nrm -rf \"test\"\nHF_HOME=\"test\" python test.py\n```\n\n```python\nimport os\nfrom datasets import load_dataset\n\n# Forking to make sure access will be concurrent\n# Waiting to fork after imports (because it takes a while)\nchild = os.fork()\n\nif child:\n    print('child')\n    ds = load_dataset('TESTLOAD.py', 'a', trust_remote_code=True)\n    print(f\"child, {ds['train'][0]}\")\nelse:\n    print('parent')\n    ds = load_dataset('TESTLOAD.py', 'b', trust_remote_code=True)\n    print(f\"parent, {ds['train'][0]}\")\n```\n\n</details>"
      }
    ]
  },
  {
    "issue_number": 7472,
    "title": "Label casting during `map` process is canceled after the `map` process",
    "author": "yoshitomo-matsubara",
    "state": "closed",
    "created_at": "2025-03-21T07:56:22Z",
    "updated_at": "2025-04-10T05:11:15Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen preprocessing a multi-label dataset, I introduced a step to convert int labels to float labels as [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) expects float labels and forward function of models in transformers package internally use `BCEWithLogitsLoss`\n\nHowever, the casting was canceled after `.map` process and the label values still use int values, which leads to an error\n```\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1711, in forward\n    loss = loss_fct(logits, labels)\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 819, in forward\n    return F.binary_cross_entropy_with_logits(\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/torch/nn/functional.py\", line 3628, in binary_cross_entropy_with_logits\n    return torch.binary_cross_entropy_with_logits(\nRuntimeError: result type Float can't be cast to the desired output type Long\n```\n\nThis seems like happening only when the original labels are int values (see examples below)\n\n### Steps to reproduce the bug\n\nIf the original dataset uses a list of int labels, it will cancel the int->float casting\n\n```python\nfrom datasets import Dataset\n\ndata = {\n    'text': ['text1', 'text2', 'text3', 'text4'],\n    'labels': [[0, 1, 2], [3], [3, 4], [3]]\n}\n\ndataset = Dataset.from_dict(data)\n\nlabel_set = set([label for labels in data['labels'] for label in labels])\nlabel2idx = {label: idx for idx, label in enumerate(sorted(label_set))}\n\n\ndef multi_labels_to_ids(labels):\n    ids = [0.0] * len(label2idx)\n    for label in labels:\n        ids[label2idx[label]] = 1.0\n    return ids\n\ndef preprocess(examples):\n    result = {'sentence': [[0, 3, 4] for _ in range(len(examples['labels']))]}\n    print('\"labels\" are int', examples['labels'])\n    result['labels'] = [multi_labels_to_ids(l) for l in examples['labels']]\n    print('\"labels\" were converted to multi-label format with float values', result['labels'])\n    return result\n\n\npreprocessed_dataset = dataset.map(preprocess, batched=True, remove_columns=['labels', 'text'])\nprint(preprocessed_dataset[0]['labels'])\n# Output: \"[1, 1, 1, 0, 0]\"\n# Expected: \"[1.0, 1.0, 1.0, 0.0, 0.0]\"\n```\n\nIf the original dataset uses non-int labels, it works as expected.\n\n```python\nfrom datasets import Dataset\n\ndata = {\n    'text': ['text1', 'text2', 'text3', 'text4'],\n    'labels': [['label1', 'label2', 'label3'], ['label4'], ['label4', 'label5'], ['label4']]\n}\n\ndataset = Dataset.from_dict(data)\n\nlabel_set = set([label for labels in data['labels'] for label in labels])\nlabel2idx = {label: idx for idx, label in enumerate(sorted(label_set))}\n\n\ndef multi_labels_to_ids(labels):\n    ids = [0.0] * len(label2idx)\n    for label in labels:\n        ids[label2idx[label]] = 1.0\n    return ids\n\ndef preprocess(examples):\n    result = {'sentence': [[0, 3, 4] for _ in range(len(examples['labels']))]}\n    print('\"labels\" are int', examples['labels'])\n    result['labels'] = [multi_labels_to_ids(l) for l in examples['labels']]\n    print('\"labels\" were converted to multi-label format with float values', result['labels'])\n    return result\n\n\npreprocessed_dataset = dataset.map(preprocess, batched=True, remove_columns=['labels', 'text'])\nprint(preprocessed_dataset[0]['labels'])\n# Output: \"[1.0, 1.0, 1.0, 0.0, 0.0]\"\n# Expected: \"[1.0, 1.0, 1.0, 0.0, 0.0]\"\n```\n\nNote that the only difference between these two examples is\n>    'labels': [[0, 1, 2], [3], [3, 4], [3]]\n\nv.s\n\n>    'labels': [['label1', 'label2', 'label3'], ['label4'], ['label4', 'label5'], ['label4']]\n\n\n\n### Expected behavior\n\nEven if the original dataset uses a list of int labels, the int->float casting during `.map` process should not be canceled as shown in the above example\n\n### Environment info\n\nOS Ubuntu 22.04 LTS\nPython 3.10.11\ndatasets v3.4.1",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! By default `map()` tries to keep the types of each column of the dataset, so here it reuses the int type since all your float values can be converted to integers. But I agree it would be nice to store float values as float values and don't try to reuse the same type in this case.\n\nIn the meantime, you can either store the float values in a new column, or pass the output `features=` manually to `map()`"
      },
      {
        "user": "yoshitomo-matsubara",
        "body": "Hi @lhoestq \n\nThank you for the answer & suggestion!\n\nCan we add some flag to `map()` function like `reuses_original_type=True` and skip reusing the original type when it's False?\n\nLet me know if it sounds like a reasonable solution. I am happy to submit a PR for this."
      },
      {
        "user": "lhoestq",
        "body": "In general we try to avoid adding new parameters when it's already possible to achieve the same results with existing parameters (here `features=`). But since it's not always convenient to know in advance the `features=` I'm open to contributions to adding this parameter yes"
      }
    ]
  },
  {
    "issue_number": 7164,
    "title": "fsspec.exceptions.FSTimeoutError when downloading dataset",
    "author": "timonmerk",
    "state": "open",
    "created_at": "2024-09-24T08:45:05Z",
    "updated_at": "2025-04-09T22:25:56Z",
    "labels": [],
    "body": "### Describe the bug\n\nI am trying to download the `librispeech_asr` `clean` dataset, which results in a `FSTimeoutError` exception after downloading around 61% of the data.\n\n### Steps to reproduce the bug\n\n```\r\nimport datasets\r\ndatasets.load_dataset(\"librispeech_asr\", \"clean\")\r\n```\r\n\r\nThe output is as follows:\r\n\r\n> Downloading data:  61%|██████████████▋         | 3.92G/6.39G [05:00<03:06, 13.2MB/s]Traceback (most recent call last):\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/fsspec/asyn.py\", line 56, in _runner\r\n>     result[0] = await coro\r\n>                 ^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/fsspec/implementations/http.py\", line 262, in _get_file\r\n>     chunk = await r.content.read(chunk_size)\r\n>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/aiohttp/streams.py\", line 393, in read\r\n>     await self._wait(\"read\")\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/aiohttp/streams.py\", line 311, in _wait\r\n>     with self._timer:\r\n>          ^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/aiohttp/helpers.py\", line 713, in __exit__\r\n>     raise asyncio.TimeoutError from None\r\n> TimeoutError\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/load_dataset.py\", line 3, in <module>\r\n>     datasets.load_dataset(\"librispeech_asr\", \"clean\")\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/load.py\", line 2096, in load_dataset\r\n>     builder_instance.download_and_prepare(\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/builder.py\", line 924, in download_and_prepare\r\n>     self._download_and_prepare(\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/builder.py\", line 1647, in _download_and_prepare\r\n>     super()._download_and_prepare(\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/builder.py\", line 977, in _download_and_prepare\r\n>     split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n>                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/.cache/huggingface/modules/datasets_modules/datasets/librispeech_asr/2712a8f82f0d20807a56faadcd08734f9bdd24c850bb118ba21ff33ebff0432f/librispeech_asr.py\", line 115, in _split_generators\r\n>     archive_path = dl_manager.download(_DL_URLS[self.config.name])\r\n>                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py\", line 159, in download\r\n>     downloaded_path_or_paths = map_nested(\r\n>                                ^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py\", line 512, in map_nested\r\n>     _single_map_nested((function, obj, batched, batch_size, types, None, True, None))\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py\", line 380, in _single_map_nested\r\n>     return [mapped_item for batch in iter_batched(data_struct, batch_size) for mapped_item in function(batch)]\r\n>                                                                                               ^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py\", line 216, in _download_batched\r\n>     self._download_single(url_or_filename, download_config=download_config)\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py\", line 225, in _download_single\r\n>     out = cached_path(url_or_filename, download_config=download_config)\r\n>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py\", line 205, in cached_path\r\n>     output_path = get_from_cache(\r\n>                   ^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py\", line 415, in get_from_cache\r\n>     fsspec_get(url, temp_file, storage_options=storage_options, desc=download_desc, disable_tqdm=disable_tqdm)\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py\", line 334, in fsspec_get\r\n>     fs.get_file(path, temp_file.name, callback=callback)\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/fsspec/asyn.py\", line 118, in wrapper\r\n>     return sync(self.loop, func, *args, **kwargs)\r\n>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/fsspec/asyn.py\", line 101, in sync\r\n>     raise FSTimeoutError from return_result\r\n> fsspec.exceptions.FSTimeoutError\r\n> Downloading data:  61%|██████████████▋         | 3.92G/6.39G [05:00<03:09, 13.0MB/s]\n\n### Expected behavior\n\nComplete the download\n\n### Environment info\n\nPython version 3.12.6\r\n\r\nDependencies:\r\n> dependencies = [\r\n>     \"accelerate>=0.34.2\",\r\n>     \"datasets[audio]>=3.0.0\",\r\n>     \"ipython>=8.18.1\",\r\n>     \"librosa>=0.10.2.post1\",\r\n>     \"torch>=2.4.1\",\r\n>     \"torchaudio>=2.4.1\",\r\n>     \"transformers>=4.44.2\",\r\n> ]\r\n\r\nMacOS 14.6.1 (23G93)",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! If you check the dataset loading script [here](https://huggingface.co/datasets/openslr/librispeech_asr/blob/main/librispeech_asr.py) you'll see that it downloads the data from OpenSLR, and apparently their storage has timeout issues. It would be great to ultimately host the dataset on Hugging Face instead.\r\n\r\nIn the meantime I can only recommend to try again later :/"
      },
      {
        "user": "timonmerk",
        "body": "Ok, still many thanks!"
      },
      {
        "user": "Epiphero",
        "body": "I'm also getting this same error but for `CSTR-Edinburgh/vctk`, so I don't think it's the remote host that's timing out, since I also time out at exactly 5 minutes.  It seems there is a universal fsspec timeout that's getting hit starting in v3."
      }
    ]
  },
  {
    "issue_number": 5044,
    "title": "integrate `load_from_disk` into `load_dataset` ",
    "author": "stas00",
    "state": "open",
    "created_at": "2022-09-29T17:37:12Z",
    "updated_at": "2025-04-09T16:18:58Z",
    "labels": [
      "enhancement"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nIs it possible to make `load_dataset` more universal similar to `from_pretrained` in `transformers` so that it can handle the hub, and the local path datasets of all supported types?\r\n\r\nCurrently one has to choose a different loader depending on how the dataset has been created.\r\n\r\ne.g. this won't work:\r\n\r\n```\r\n$ git clone https://huggingface.co/datasets/severo/test-parquet\r\n$ python -c 'from datasets import load_dataset; ds=load_dataset(\"test-parquet\"); \\\r\nds.save_to_disk(\"my_dataset\"); load_dataset(\"my_dataset\")'\r\n\r\n[...]\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/stas/anaconda3/envs/py38-pt112/lib/python3.8/site-packages/datasets/load.py\", line 1746, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/stas/anaconda3/envs/py38-pt112/lib/python3.8/site-packages/datasets/builder.py\", line 704, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/stas/anaconda3/envs/py38-pt112/lib/python3.8/site-packages/datasets/builder.py\", line 793, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/stas/anaconda3/envs/py38-pt112/lib/python3.8/site-packages/datasets/builder.py\", line 1277, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/home/stas/anaconda3/envs/py38-pt112/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 524, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"/home/stas/anaconda3/envs/py38-pt112/lib/python3.8/site-packages/datasets/table.py\", line 2005, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"/home/stas/anaconda3/envs/py38-pt112/lib/python3.8/site-packages/datasets/table.py\", line 1968, in cast_table_to_schema\r\n    raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nbecause column names don't match\")\r\nValueError: Couldn't cast\r\n_data_files: list<item: struct<filename: string>>\r\n  child 0, item: struct<filename: string>\r\n      child 0, filename: string\r\n```\r\n\r\nboth times the dataset is being loaded from disk. Why does it fail the second time?\r\n\r\nWhy can't `save_to_disk` generate a dataset that can be immediately loaded by `load_dataset`?\r\n\r\ne.g. the simplest hack would be to have `save_to_disk` add some flag to the saved dataset, that tells `load_dataset` to internally call `load_from_disk`. like having `save_to_disk` create a `load_me_with_load_from_disk.txt` file ;) and `load_dataset` will support that feature from saved datasets from new `datasets` versions. The old ones will still need to use `load_from_disk` explicitly. Unless the flag is not needed and one can immediately tell by looking at the saved dataset that it was saved via `save_to_disk` and thus use `load_from_disk` internally.\r\n\r\nThe use-case is defining a simple API where the user only ever needs to pass a `dataset_name_or_path` and it will always just work. Currently one needs to manually add additional switches telling the system whether to use one loading method or the other which works but it's not smooth.\r\n\r\nThank you!",
    "comments": [
      {
        "user": "lhoestq",
        "body": "I agree the situation is not ideal and it would be awesome to use `load_dataset` to reload a dataset saved locally !\r\n\r\nFor context:\r\n\r\n- `load_dataset` works in three steps: download the dataset, then prepare it as an arrow dataset, and finally return a memory mapped arrow dataset. In particular it creates a cache directory to store the arrow data and the subsequent cache files for `map`.\r\n\r\n- `load_from_disk` directly returns a memory mapped dataset from the arrow file (similar to `Dataset.from_file`). It doesn't create a cache diretory, instead all the subsequent `map` calls write in the same directory as the original data. \r\n\r\nIf we want to keep the download_and_prepare step for consistency, it would unnecessarily copy the arrow data into the datasets cache. On the other hand if we don't do this step, the cache directory doesn't exist which is inconsistent.\r\n\r\nI'm curious, what would you expect to happen in this situation ?"
      },
      {
        "user": "stas00",
        "body": "Thank you for the detailed breakdown, @lhoestq \r\n\r\n> I'm curious, what would you expect to happen in this situation ?\r\n\r\n1. the simplest solution is to add a flag to the dataset saved by `save_to_disk` and have `load_dataset` check that flag - if it's set simply switch control to `load_from_disk` behind the scenes. So `load_dataset` detects it's a local filesystem, looks inside to see whether it's something it can cache or whether it should use it directly as is and continues accordingly with one of the 2 dataset-type specific APIs.\r\n\r\n2. the more evolved solution is to look at a dataset produced by `save_to_disk` as a remote resource like hub. So the first time `load_dataset` sees it, it'll take a fingerprint and create a normal cached dataset. On subsequent uses it'll again discover it as a remote resource, validate that it has it cached via the fingerprint and serve as a normal dataset. \r\n\r\nAs you said the cons of approach 2 is that if the dataset is huge it'll make 2 copies on the same machine. So it's possible that both approaches can be integrated. Say if `save_to_disc(do_not_cache=True)` is passed it'll use solution 1, otherwise solution 2. or could even symlink the huge arrow files to the cache instead? or perhaps it's more intuitive to use `load_dataset(do_not_cache=True)` instead. So that one can choose whether to make a cached copy or not for the locally saved dataset. i.e. a simple at use point user control.\r\n\r\nSurely there are other ways to handle it, this is just one possibility.\r\n"
      },
      {
        "user": "lhoestq",
        "body": "I think the simplest is to always memory map the local file without copy, but still have a cached directory in the cache at `~/.cache/huggingface` instead of saving `map` results next to the original data.\r\n\r\nIn practice we can even use symlinks if it makes the implementation simpler"
      }
    ]
  },
  {
    "issue_number": 7505,
    "title": "HfHubHTTPError: 403 Forbidden: None. Cannot access content at: https://hf.co/api/s3proxy",
    "author": "hissain",
    "state": "open",
    "created_at": "2025-04-08T14:08:40Z",
    "updated_at": "2025-04-08T14:08:40Z",
    "labels": [],
    "body": "I have already logged in Huggingface using CLI with my valid token. Now trying to download the datasets using following code:\n\n\n    from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n    from datasets import load_dataset, DatasetDict, Audio\n    \n    def load_and_preprocess_dataset():\n        dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"bn\")\n        dataset = dataset.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n        dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n        dataset = DatasetDict({\n            \"train\": dataset[\"train\"],\n            \"test\": dataset[\"test\"]\n        })\n        return dataset\n    \n    load_and_preprocess_dataset()\n\n\nI am getting following error:\n\n    Downloading data: 100%\n     25/25 [00:01<00:00, 25.31files/s]\n    ---------------------------------------------------------------------------\n    HTTPError                                 Traceback (most recent call last)\n    File ~/github/bangla-asr/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409, in hf_raise_for_status(response, endpoint_name)\n        408 try:\n    --> 409     response.raise_for_status()\n        410 except HTTPError as e:\n    \n    File ~/github/bangla-asr/.venv/lib/python3.11/site-packages/requests/models.py:1024, in Response.raise_for_status(self)\n       1023 if http_error_msg:\n    -> 1024     raise HTTPError(http_error_msg, response=self)\n    \n    HTTPError: 403 Client Error: BlockSIEL for url: https://hf.co/api/s3proxy?GET=https%3A%2F%2Fhf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com%2Frepos%2Fa3%2F86%2Fa386bf65687d8a6928c1ea57c383aa3faade32f5171150e25af3fc1cfc273db8%2F67f1ac9cabd539bfbff3acbc549b60647833a250dc638866f22bf1b64e68806d%3FX-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Content-Sha256%3DUNSIGNED-PAYLOAD%26X-Amz-Credential%3DAKIA2JU7TKAQLC2QXPN7%252F20250408%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Date%3D20250408T134345Z%26X-Amz-Expires%3D3600%26X-Amz-Signature%3D621e731d4fd6d08afbf568379797746ab8e2b853b6728ff5e1122fef6e56880b%26X-Amz-SignedHeaders%3Dhost%26response-content-disposition%3Dinline%253B%2520filename%252A%253DUTF-8%2527%2527bn_validated_1.tar%253B%2520filename%253D%2522bn_validated_1.tar%2522%253B%26response-content-type%3Dapplication%252Fx-tar%26x-id%3DGetObject&HEAD=https%3A%2F%2Fhf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com%2Frepos%2Fa3%2F86%2Fa386bf65687d8a6928c1ea57c383aa3faade32f5171150e25af3fc1cfc273db8%2F67f1ac9cabd539bfbff3acbc549b60647833a250dc638866f22bf1b64e68806d%3FX-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Content-Sha256%3DUNSIGNED-PAYLOAD%26X-Amz-Credential%3DAKIA2JU7TKAQLC2QXPN7%252F20250408%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Date%3D20250408T134345Z%26X-Amz-Expires%3D3600%26X-Amz-Signature%3D15254fb79d30b0dc36b94a28138e675e0e00bb475b8a3ae774418500b095a661%26X-Amz-SignedHeaders%3Dhost&sign=eyJhbGciOiJIUzI1NiJ9.eyJyZWRpcmVjdF9kb21haW4iOiJoZi1odWItbGZzLXVzLWVhc3QtMS5zMy51cy1lYXN0LTEuYW1hem9uYXdzLmNvbSIsImlhdCI6MTc0NDExOTgyNSwiZXhwIjoxNzQ0MjA2MjI1LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.5sJzudFDU3SmOdOLlwmQCOfQFf2r7y9590HoX8WBkRk\n    \n    The above exception was the direct cause of the following exception:\n    \n    HfHubHTTPError                            Traceback (most recent call last)\n    Cell In[16], line 15\n          9     dataset = DatasetDict({\n         10         \"train\": dataset[\"train\"],\n         11         \"test\": dataset[\"test\"]\n         12     })\n         13     return dataset\n    ---> 15 load_and_preprocess_dataset()\n         17 # def setup_model():\n         18 #     processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n    ...\n        475     range_header = response.request.headers.get(\"Range\")\n    \n    HfHubHTTPError: 403 Forbidden: None.\n    Cannot access content at: https://hf.co/api/s3proxy?GET=https%3A%2F%2Fhf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com%2Frepos%2Fa3%2F86%2Fa386bf6568724a6928c1ea57c383aa3faade32f5171150e25af3fc1cfc273db8%2F67f1ac9cabd539bfbff3acbc549b60647833a250dc638786f22bf1b64e68806d%3FX-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Content-Sha256%3DUNSIGNED-PAYLOAD%26X-Amz-Credential%3DAKIA2JU7TKAQLC2QXPN7%252F20250408%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Date%3D20250408T134345Z%26X-Amz-Expires%3D3600%26X-Amz-Signature%3D621e731d4fd6d08afbf568379797746ab394b853b6728ff5e1122fef6e56880b%26X-Amz-SignedHeaders%3Dhost%26response-content-disposition%3Dinline%253B%2520filename%252A%253DUTF-8%2527%2527bn_validated_1.tar%253B%2520filename%253D%2522bn_validated_1.tar%2522%253B%26response-content-type%3Dapplication%252Fx-tar%26x-id%3DGetObject&HEAD=https%3A%2F%2Fhf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com%2Frepos%2Fa3%2F86%2Fa386bf65687ab76928c1ea57c383aa3faade32f5171150e25af3fc1cfc273db8%2F67f1ac9cabd539bfbff3acbc549b60647833a250d2338866f222f1b64e68806d%3FX-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Content-Sha256%3DUNSIGNED-PAYLOAD%26X-Amz-Credential%3DAKIA2JU7TKAQLC2QXPN7%252F20250408%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Date%3D20250408T134345Z%26X-Amz-Expires%3D3600%26X-Amz-Signature%3D15254fb79d30b0dc36b94a28138e675e0e00bb475b8a3ae774418500b095a661%26X-Amz-SignedHeaders%3Dhost&sign=eyJhbGciOiJIUzI1NiJ9.eyJyZWRpcmVjds9kb21haW4iOiJoZi1odWItbGZzLXVzLWVhc3QtMS5zMy51cy1lYXN0LTEuYW1hem9uYXdzLmNvbSIsImlhdCI6MTc0NDExOT2yNSwiZXhwIjoxNzQ0MjA2MjI1LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.5sJzudFDU3SmOdOLlwmQdOfQFf2r7y9590HoX8WBkRk.\n    Make sure your token has the correct permissions.\n\n\n**What's wrong with the code?** Please note that the error is happening only when I am running from my office network due to probably proxy. Which URL, I need to take a proxy exception?",
    "comments": []
  },
  {
    "issue_number": 5950,
    "title": "Support for data with instance-wise dictionary as features",
    "author": "richardwth",
    "state": "open",
    "created_at": "2023-06-13T15:49:00Z",
    "updated_at": "2025-04-07T13:20:37Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nI notice that when loading data instances with feature type of python dictionary, the dictionary keys would be broadcast so that every instance has the same set of keys. Please see an example in the Motivation section.\r\n\r\nIt is possible to avoid this behavior, i.e., load dictionary features as it is and do not broadcast the keys among instances? Please note that these dictionaries would have to be processed dynamically at each training iteration into strings (and tokenized).\n\n### Motivation\n\nI am trying to load a dataset from a json file. Each instance of the dataset has a feature that is a dictionary but its keys depend on the instance. Every two instances may have different keys. For example, imagine a dataset that contains a set of math expressions from a bunch of mutually redundant expressions:\r\n```\r\n{\r\n    \"index\": 0,\r\n    \"feature\": {\r\n        \"2 * x + y >= 3\": [\"2 * x + y >= 3\", \"4 * x + 2 * y >= 6\"],\r\n        ...\r\n    }\r\n},\r\n...\r\n{\r\n    \"index\": 9999,\r\n    \"feature\": {\r\n        \"x >= 6\": [\"x >= 6\", \"x >= 0\", \"x >= -1\"],\r\n        ...\r\n    }\r\n},\r\n...\r\n```\r\nWhen directly loading the dataset using `data = load_dataset(\"json\", data_files=file_paths, split='train')`, each instance would have all the keys from other instances and None as values. That is, instance of index 0 becomes:\r\n```\r\n{\r\n    \"index\": 0,\r\n    \"feature\": {\r\n        \"2 * x + y >= 3\": [\"2 * x + y >= 3\", \"4 * x + 2 * y >= 6\"],\r\n        ...\r\n        \"x >= 6\": None,  # keys from other instances\r\n        ...\r\n    }\r\n},\r\n```\r\nThis is not desirable. Moreover, issue would be raised if I attempt to combine two such datasets using `data = concatenate_datasets(multi_datasets)`, perhaps because their dictionary features contain different keys.\r\n\r\nA solution I can think of is to store the dictionary features as a long string, and evaluate it later. Please kindly suggest any other solution using existing methods of datasets.\n\n### Your contribution\n\nN/A",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! We use the Arrow columnar format under the hood, which doesn't support such dictionaries: each field must have a fixed type and exist in each sample.\r\n\r\nInstead you can restructure your data like\r\n```\r\n{\r\n    \"index\": 0,\r\n    \"keys\": [\"2 * x + y >= 3\"],\r\n    \"values\": [[\"2 * x + y >= 3\", \"4 * x + 2 * y >= 6\"]],\r\n    }\r\n},\r\n...\r\n{\r\n    \"index\": 9999,\r\n    \"keys\": [\"x >= 6\"],\r\n    \"values\": [[\"x >= 6\", \"x >= 0\", \"x >= -1\"]],\r\n},\r\n...\r\n```"
      },
      {
        "user": "edmcman",
        "body": "Maybe there could be some type of automated conversion from dicts to tuples.  I am also trying to wrangle a json-based dataset into `datasets` and it's awful because of this issue."
      },
      {
        "user": "lhoestq",
        "body": "Alternatively we can maybe support the [Json extension type](https://arrow.apache.org/docs/python/generated/pyarrow.JsonType.html#pyarrow.JsonType) in `pyarrow` ?\n\nbtw `datasets` is open to contributions on this subject if you'd like to take a look"
      }
    ]
  },
  {
    "issue_number": 3822,
    "title": "Add Biwi Kinect Head Pose Database",
    "author": "osanseviero",
    "state": "closed",
    "created_at": "2022-03-04T08:48:39Z",
    "updated_at": "2025-04-07T13:04:25Z",
    "labels": [
      "dataset request",
      "vision"
    ],
    "body": "## Adding a Dataset\r\n- **Name:** Biwi Kinect Head Pose Database\r\n- **Description:** Over 15K images of 20 people recorded with a Kinect while turning their heads around freely. For each frame, depth and rgb images are provided, together with ground in the form of the 3D location of the head and its rotation angles.\r\n- **Data:** [*link to the Github repository or current dataset location*](https://icu.ee.ethz.ch/research/datsets.html)\r\n- **Motivation:** Useful pose estimation dataset\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "comments": [
      {
        "user": "dnaveenr",
        "body": "Official dataset location : https://icu.ee.ethz.ch/research/datsets.html\r\nIn the \"Biwi Kinect Head Pose Database\" section, I do not find any information regarding \"Downloading the dataset.\" . Do we mail the authors regarding this ?\r\n\r\nI found the dataset on Kaggle : [Link](https://www.kaggle.com/kmader/biwi-kinect-head-pose-database) , but since 🤗 does not host any of the datasets, this would require the user to provide their Kaggle username and API key to download. \r\n\r\nAny inputs on how we could proceed ? Thank you.\r\n[ Need your inputs here, @lhoestq or @mariosasko ]"
      },
      {
        "user": "mariosasko",
        "body": "Hi @dnaveenr! Thanks for tackling this issue. This link should work: https://data.vision.ee.ethz.ch/cvl/gfanelli/kinect_head_pose_db.tgz"
      },
      {
        "user": "dnaveenr",
        "body": "#self-assign"
      }
    ]
  },
  {
    "issue_number": 7501,
    "title": "Nested Feature raises ArrowNotImplementedError: Unsupported cast using function cast_struct",
    "author": "yaner-here",
    "state": "closed",
    "created_at": "2025-04-07T12:35:39Z",
    "updated_at": "2025-04-07T12:43:04Z",
    "labels": [],
    "body": "### Describe the bug\n\n`datasets.Features` seems to be unable to handle json file that contains fields of `list[dict]`.\n\n### Steps to reproduce the bug\n\n```json\n// test.json\n{\"a\": 1, \"b\": [{\"c\": 2, \"d\": 3}, {\"c\": 4, \"d\": 5}]}\n{\"a\": 5, \"b\": [{\"c\": 7, \"d\": 8}, {\"c\": 9, \"d\": 10}]}\n```\n\n```python\nimport json\nfrom datasets import Dataset, Features, Value, Sequence, load_dataset\n\nannotation_feature = Features({\n    \"a\": Value(\"int32\"),\n    \"b\": Sequence({\n        \"c\": Value(\"int32\"),\n        \"d\": Value(\"int32\"),\n    }),\n})\nannotation_dataset = load_dataset(\n    \"json\", \n    data_files=\"test.json\",\n    features=annotation_feature\n)\n```\n\n```\nArrowNotImplementedError: Unsupported cast from list<item: struct<c: int32, d: int32>> to struct using function cast_struct\n\nThe above exception was the direct cause of the following exception:\n\nDatasetGenerationError                    Traceback (most recent call last)\nCell In[46], line 11\n      2 from datasets import Dataset, Features, Value, Sequence, load_dataset\n      4 annotation_feature = Features({\n      5     \"a\": Value(\"int32\"),\n      6     \"b\": Sequence({\n   (...)      9     }),\n     10 })\n---> 11 annotation_dataset = load_dataset(\n     12     \"json\", \n     13     data_files=\"test.json\",\n     14     features=annotation_feature\n     15 )\n```\n\n### Expected behavior\n\nA `datasets.Datasets` instance should be initialized.\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Linux-6.11.0-21-generic-x86_64-with-glibc2.39\n- Python version: 3.11.11\n- `huggingface_hub` version: 0.30.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0",
    "comments": [
      {
        "user": "yaner-here",
        "body": "Solved by the default `load_dataset(features)` parameters. Do not use `Sequence` for the `list` in `list[any]` json schema, just simply use `[]`. For example, `\"b\": Sequence({...})` fails but `\"b\": [{...}]` works fine."
      }
    ]
  },
  {
    "issue_number": 353,
    "title": "[Dataset requests] New datasets for Text Classification",
    "author": "thomwolf",
    "state": "open",
    "created_at": "2020-07-08T12:17:58Z",
    "updated_at": "2025-04-05T09:28:15Z",
    "labels": [
      "help wanted",
      "dataset request"
    ],
    "body": "We are missing a few datasets for Text Classification which is an important field.\r\n\r\nNamely, it would be really nice to add:\r\n- [x] TREC-6 dataset (see here for instance: https://pytorchnlp.readthedocs.io/en/latest/source/torchnlp.datasets.html#torchnlp.datasets.trec_dataset)  **[done]**\r\n  - #386\r\n- [x] Yelp-5\r\n  - #1315\r\n- [x] Movie review (Movie Review (MR) dataset [156]) **[done (same as rotten_tomatoes)]**\r\n- [x] SST (Stanford Sentiment Treebank) **[include in glue]**\r\n  - #1934\r\n- [ ] Multi-Perspective Question Answering (MPQA) dataset **[require authentication (indeed manual download)]**\r\n- [x] Amazon. This is a popular corpus of product reviews collected from the Amazon website [159]. It contains labels for both binary classification and multi-class (5-class) classification\r\n  - #791\r\n  - #1389\r\n- [x] 20 Newsgroups. The 20 Newsgroups dataset  **[done]**\r\n  - #410 \r\n- [x] Sogou News dataset **[done]**\r\n  - #450\r\n- [x] Reuters news. The Reuters-21578 dataset [165] **[done]**\r\n  - #471 \r\n- [x] DBpedia. The DBpedia dataset [170]\r\n  - #1116\r\n- [ ] Ohsumed. The Ohsumed collection [171] is a subset of the MEDLINE database\r\n- [ ] EUR-Lex. The EUR-Lex dataset\r\n- [x] WOS. The Web Of Science (WOS) dataset **[done]**\r\n  - #424\r\n- [ ] PubMed. PubMed [173]\r\n- [x] TREC-QA: TREC-6 + TREC-50\r\n  - See above: TREC-6 dataset\r\n- [x] Quora. The Quora dataset [180]\r\n  - #366\r\n\r\nAll these datasets are cited in https://arxiv.org/abs/2004.03705",
    "comments": [
      {
        "user": "thomwolf",
        "body": "Pinging @mariamabarham as well"
      },
      {
        "user": "jxmorris12",
        "body": "- `nlp` has MR! It's called `rotten_tomatoes`\r\n- SST is part of GLUE, or is that just SST-2?\r\n- `nlp` also has `ag_news`, a popular news classification dataset\r\n\r\nI'd also like to see:\r\n- the Yahoo Answers topic classification dataset\r\n- the Kaggle Fake News classification dataset"
      },
      {
        "user": "mariamabarham",
        "body": "Thanks @jxmorris12 for pointing this out. \r\n\r\nIn glue we only have SST-2 maybe we can add separately SST-1.\r\n"
      }
    ]
  },
  {
    "issue_number": 7498,
    "title": "Extreme memory bandwidth.",
    "author": "J0SZ",
    "state": "open",
    "created_at": "2025-04-03T11:09:08Z",
    "updated_at": "2025-04-03T11:11:22Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen I use hf datasets on 4 GPU with 40 workers I get some extreme memory bandwidth of constant ~3GB/s. \n\nHowever, if I wrap the dataset in `IterableDataset`, this issue is gone and the data also loads way faster (4x faster training on 1 worker).\n\nIt seems like the workers don't share memory and basically duplicate the data 4x40.\n\n\n\n### Steps to reproduce the bug\n\nTrainer arguments:\n```\n    dataloader_pin_memory=True,\n    dataloader_num_workers=40,\n    dataloader_prefetch_factor=2,\n    dataloader_persistent_workers=True,\n```\nCall trainer:\n```\ntrainer = Trainer(\n    model=model,\n    args=train_args,\n    train_dataset=load_from_disk('..').with_fromat('torch'),\n)\n```\nThe dataset has 600GB and consists of 1225 files.\n\n### Expected behavior\n\nThe optimal bandwidth should be 100MB/s to keep up with GPU.\n\n### Environment info\n\nLinux\nPython 3.11\ndatasets==3.2.0\n\n",
    "comments": []
  },
  {
    "issue_number": 7477,
    "title": "What is the canonical way to compress a Dataset?",
    "author": "eric-czech",
    "state": "open",
    "created_at": "2025-03-25T16:47:51Z",
    "updated_at": "2025-04-03T09:13:11Z",
    "labels": [],
    "body": "Given that Arrow is the preferred backend for a Dataset, what is a user supposed to do if they want concurrent reads, concurrent writes AND on-disk compression for a larger dataset?\n\nParquet would be the obvious answer except that there is no native support for writing sharded, parquet datasets concurrently [[1](https://github.com/huggingface/datasets/issues/7047)].\n\nAm I missing something?  \n\nAnd if so, why is this not the standard/default way that `Dataset`'s work as they do in Xarray, Ray Data, Composer, etc.?",
    "comments": [
      {
        "user": "eric-czech",
        "body": "I saw this post by @lhoestq: https://discuss.huggingface.co/t/increased-arrow-table-size-by-factor-of-2/26561/4 suggesting that there is at least some internal code for writing sharded parquet datasets non-concurrently.  This appears to be that code: https://github.com/huggingface/datasets/blob/94ccd1b4fada8a92cea96dc8df4e915041d695b6/src/datasets/arrow_dataset.py#L5380-L5397\n\nIs there any fundamental reason (e.g. race conditions) that this kind of operation couldn't exist as a utility or method on a `Dataset` with a `num_proc` argument?  I am not seeing any other issues explicitly for that ask. \n"
      },
      {
        "user": "lhoestq",
        "body": "We simply haven't implemented a method to save as sharded parquet locally yet ^^'\n\nRight now the only sharded parquet export method is `push_to_hub()` which writes to HF. But we can have a local one as well. \n\nIn the meantime the easiest way to export as sharded parquet locally is to `.shard()` and `.to_parquet()` (see code from my comment [here](https://github.com/huggingface/datasets/issues/7047#issuecomment-2233163406))"
      },
      {
        "user": "eric-czech",
        "body": "> In the meantime the easiest way to export as sharded parquet locally is to .shard() and .to_parquet()\n\nMakes sense, BUT how can it be done concurrently?  I could of course use multiprocessing myself or a dozen other libraries for parallelizing single-node/local operations like that.\n\nWhat I'm asking though is, what is the way to do this that is most canonical for `datasets` specifically?  I.e. what is least likely to causing pickling or other issues because it is used frequently internally by `datasets` and already likely tests for a lot of library-native edge-cases?"
      }
    ]
  },
  {
    "issue_number": 6848,
    "title": "Cant Downlaod Common Voice 17.0 hy-AM ",
    "author": "mheryerznkanyan",
    "state": "open",
    "created_at": "2024-04-29T10:06:02Z",
    "updated_at": "2025-04-01T20:48:09Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nI want to download Common Voice 17.0 hy-AM but it returns an error. \r\n```\r\n\r\nThe version_base parameter is not specified.\r\nPlease specify a compatability version level, or None.\r\nWill assume defaults for version 1.1\r\n  @hydra.main(config_name='hfds_config', config_path=None)\r\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\r\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\r\n  ret = run_job(\r\n/usr/local/lib/python3.10/dist-packages/datasets/load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_17_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_17_0\r\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\r\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n  warnings.warn(\r\nReading metadata...: 6180it [00:00, 133224.37it/s]les/s]\r\nGenerating train split: 0 examples [00:00, ? examples/s]\r\nHuggingFace datasets failed due to some reason (stack trace below).\r\nFor certain datasets (eg: MCV), it may be necessary to login to the huggingface-cli (via `huggingface-cli login`).\r\nOnce logged in, you need to set `use_auth_token=True` when calling this script.\r\n\r\nTraceback error for reference :\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1743, in _prepare_split_single\r\n    example = self.info.features.encode_example(record) if self.info.features is not None else record\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\", line 1878, in encode_example\r\n    return encode_nested_example(self, example)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\", line 1243, in encode_nested_example\r\n    {\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\", line 1243, in <dictcomp>\r\n    {\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 326, in zip_dict\r\n    yield key, tuple(d[key] for d in dicts)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 326, in <genexpr>\r\n    yield key, tuple(d[key] for d in dicts)\r\nKeyError: 'sentence_id'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/nemo/scripts/speech_recognition/convert_hf_dataset_to_nemo.py\", line 358, in main\r\n    dataset = load_dataset(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2549, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1005, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1767, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1100, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1605, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1762, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\ncv_17 = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"hy-AM\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt works fine with common_voice_16_1\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.18.0\r\n- Platform: Linux-5.15.0-1042-nvidia-x86_64-with-glibc2.35\r\n- Python version: 3.11.6\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.2.0",
    "comments": [
      {
        "user": "SalomonKisters",
        "body": "Same issue here."
      },
      {
        "user": "Harry-Yang0518",
        "body": "#self-assign"
      },
      {
        "user": "Harry-Yang0518",
        "body": "Hi @mheryerznkanyan ,\nI tested it on a Linux-5.14.0-284.86.1.el9_2.x86_64-x86_64-with-glibc2.34 machine using the same package versions you mentioned, and it works fine now.\nDoes it work on your machine as well?"
      }
    ]
  },
  {
    "issue_number": 6899,
    "title": "List of dictionary features get standardized",
    "author": "sohamparikh",
    "state": "open",
    "created_at": "2024-05-15T14:11:35Z",
    "updated_at": "2025-04-01T20:48:03Z",
    "labels": [],
    "body": "### Describe the bug\n\nHi, i’m trying to create a HF dataset from a list using Dataset.from_list.\r\n\r\nEach sample in the list is a dict with the same keys (which will be my features). The values for each feature are a list of dictionaries, and each such dictionary has a different set of keys. However, the datasets library standardizes all dictionaries under a feature and adds all possible keys (with None value) from all the dictionaries under that feature.\r\n\r\nHow can I keep the same set of keys as in the original list for each dictionary under a feature?\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import Dataset\r\n\r\n# Define a function to generate a sample with \"tools\" feature\r\ndef generate_sample():\r\n    # Generate random sample data\r\n    sample_data = {\r\n        \"text\": \"Sample text\",\r\n        \"feature_1\": []\r\n    }\r\n    \r\n    # Add feature_1 with random keys for this sample\r\n    feature_1 = [{\"key1\": \"value1\"}, {\"key2\": \"value2\"}]  # Example feature_1 with random keys\r\n    sample_data[\"feature_1\"].extend(feature_1)\r\n    \r\n    return sample_data\r\n\r\n# Generate multiple samples\r\nnum_samples = 10\r\nsamples = [generate_sample() for _ in range(num_samples)]\r\n\r\n# Create a Hugging Face Dataset\r\ndataset = Dataset.from_list(samples)\r\ndataset[0]\r\n```\r\n\r\n```{'text': 'Sample text', 'feature_1': [{'key1': 'value1', 'key2': None}, {'key1': None, 'key2': 'value2'}]}```\n\n### Expected behavior\n\n```{'text': 'Sample text', 'feature_1': [{'key1': 'value1'}, {'key2': 'value2'}]}```\n\n### Environment info\n\n- `datasets` version: 2.19.1\r\n- Platform: Linux-5.15.0-1040-nvidia-x86_64-with-glibc2.35\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.23.0\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0",
    "comments": [
      {
        "user": "edmcman",
        "body": "I think this may be a limitation of the arrow format"
      },
      {
        "user": "edmcman",
        "body": "Dupe of #5950\n"
      }
    ]
  },
  {
    "issue_number": 7481,
    "title": "deal with python `10_000` legal number in slice syntax",
    "author": "sfc-gh-sbekman",
    "state": "closed",
    "created_at": "2025-03-26T20:10:54Z",
    "updated_at": "2025-03-28T16:20:44Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\n```\nIn [6]: ds = datasets.load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:1000]\")\n\nIn [7]: ds = datasets.load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:1_000]\")\n[dozens of frames skipped]\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_reader.py:444, in _str_to_read_instruction(spec)\n    442 res = _SUB_SPEC_RE.match(spec)\n    443 if not res:\n--> 444     raise ValueError(f\"Unrecognized instruction format: {spec}\")\nValueError: Unrecognized instruction format: train_sft[:1_000]\n```\n\nIt took me a while to understand what the problem was. But apparently `pyarrow` doesn't allow python numbers that may include `_` as in `1_000`. The `_` aids readability since `10_000_000` vs `10000000` is obviously easier to grasp of what the actual number is.\n\nFeature request:\n\nideally `datasets` being a python module will do the right thing and convert python numbers into whatever pyarrow supports - in this case stripping `_`s.\n\nSecond best it'd err and tell the user that using numbers with `_` in split slices is not acceptible, so that the user won't have to deal with a huge pyarrow assert they know nothing about.\n\nThank you!",
    "comments": [
      {
        "user": "lhoestq",
        "body": "should be an easy fix, I opened a PR"
      }
    ]
  },
  {
    "issue_number": 7442,
    "title": "Flexible Loader",
    "author": "dipta007",
    "state": "open",
    "created_at": "2025-03-09T16:55:03Z",
    "updated_at": "2025-03-27T23:58:17Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nCan we have a utility function that will use `load_from_disk` when given the local path and `load_dataset` if given an HF dataset?\n\nIt can be something as simple as this one:\n\n```\ndef load_hf_dataset(path_or_name):\n    if os.path.exists(path_or_name):\n        return load_from_disk(path_or_name)\n    else:\n        return load_dataset(path_or_name)\n```\n\n### Motivation\n\nThis can be done inside the user codebase, too, but in my experience, it becomes repetitive code.\n\n### Your contribution\n\nI can open a pull request.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Ideally `save_to_disk` should save in a format compatible with load_dataset, wdyt ?"
      },
      {
        "user": "dipta007",
        "body": "> Ideally `save_to_disk` should save in a format compatible with load_dataset, wdyt ?\n\nThat would be perfect if not at least a flexible loader."
      },
      {
        "user": "dipta007",
        "body": "@lhoestq For now, you can use this small utility library: [nanoml](https://pypi.org/project/nanoml/)\n```python\nfrom nanoml.data import load_dataset_flexible\n```\n\nI actively develop and maintain this utility library. Open to contributors. Please open issues, PR, or feature requests."
      }
    ]
  },
  {
    "issue_number": 7486,
    "title": "`shared_datadir` fixture is missing",
    "author": "lahwaacz",
    "state": "closed",
    "created_at": "2025-03-27T18:17:12Z",
    "updated_at": "2025-03-27T19:49:11Z",
    "labels": [],
    "body": "### Describe the bug\n\nRunning the tests for the latest release fails due to missing `shared_datadir` fixture.\n\n### Steps to reproduce the bug\n\nRunning `pytest` while building a package for Arch Linux leads to these errors:\n\n```\n==================================== ERRORS ====================================\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>1] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>2] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>3] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>4] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>5] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>6] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_______________ ERROR at setup of test_dataset_with_pdf_feature ________________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 34\n  @require_pdfplumber\n  def test_dataset_with_pdf_feature(shared_datadir):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:34\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>0] _________\n[gw46] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n```\n\n### Expected behavior\n\nAll fixtures used in tests should be available.\n\n### Environment info\n\nArch Linux build system, building the [python-datasets](https://gitlab.archlinux.org/archlinux/packaging/packages/python-datasets) package.\n\nThere are actually [many deselected tests](https://gitlab.archlinux.org/archlinux/packaging/packages/python-datasets/-/blob/6f97957f0c326cc7b3da6b7f12326305bcaef374/PKGBUILD#L66-148) which were failing on previous releases, but these errors popped up in 3.5.0.",
    "comments": [
      {
        "user": "lahwaacz",
        "body": "OK I was missing the `pytest-datadir` package. Sorry for the noise!"
      }
    ]
  },
  {
    "issue_number": 7373,
    "title": "Excessive RAM Usage After Dataset Concatenation concatenate_datasets",
    "author": "sam-hey",
    "state": "open",
    "created_at": "2025-01-16T16:33:10Z",
    "updated_at": "2025-03-27T17:40:59Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen loading a dataset from disk, concatenating it, and starting the training process, the RAM usage progressively increases until the kernel terminates the process due to excessive memory consumption.\n\nhttps://github.com/huggingface/datasets/issues/2276\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import  DatasetDict, concatenate_datasets\n\ndataset = DatasetDict.load_from_disk(\"data\")\n\n...\n...\n\ncombined_dataset = concatenate_datasets(\n        [dataset[split] for split in dataset]\n    )\n\n#start SentenceTransformer training\n```\n\n### Expected behavior\n\nI would not expect RAM utilization to increase after concatenation. Removing the concatenation step resolves the issue\n\n### Environment info\n\nsentence-transformers==3.1.1\ndatasets==3.2.0\n\npython3.10",
    "comments": [
      {
        "user": "sam-hey",
        "body": "![Image](https://github.com/user-attachments/assets/b6f8bcbd-44af-413e-bc06-65380eb0f746)\n\n![Image](https://github.com/user-attachments/assets/a241fcd8-4b62-495c-926c-685f82015dfb)\n\nAdding a img from memray\nhttps://gist.github.com/sam-hey/00c958f13fb0f7b54d17197fe353002f"
      },
      {
        "user": "nepfaff",
        "body": "I'm having the same issue where concatenation seems to use a huge amount of RAM.\n\n```python\n# Load all chunks and concatenate them into a final dataset.\n        chunk_datasets = [\n            Dataset.load_from_disk(file, keep_in_memory=False)\n            for file in tqdm(chunk_files, desc=\"Loading chunk datasets\")\n        ]\n        logging.info(\"Concatenating chunk datasets...\")\n        final_dataset = concatenate_datasets(chunk_datasets)\n```\n\nThis is a real issue for me as the final dataset is a few terabytes in size. I'm using datasets version `3.1.0`. Also tested with version `3.4.1`"
      },
      {
        "user": "sam-hey",
        "body": "I did have a short look, the error seems to be from `memory_map` and the stream not being closed. \n\nhttps://github.com/huggingface/datasets/blob/5f8d2ad9a1b0bccfd962d998987228addfd5be9f/src/datasets/table.py#L48-L50\n\n\nDid not have the time to test jet: https://github.com/sam-hey/datasets/tree/fix/concatenate_datasets\n\nI will probably have a better look in a couple of days. \n\n"
      }
    ]
  },
  {
    "issue_number": 7471,
    "title": "Adding argument to `_get_data_files_patterns`",
    "author": "SangbumChoi",
    "state": "closed",
    "created_at": "2025-03-21T07:17:53Z",
    "updated_at": "2025-03-27T12:30:52Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nHow about adding if the user already know about the pattern?\n\nhttps://github.com/huggingface/datasets/blob/a256b85cbc67aa3f0e75d32d6586afc507cf535b/src/datasets/data_files.py#L252\n\n### Motivation\n\nWhile using this load_dataset people might use 10M of images for the local files.\nHowever, due to searching all the appropriate file pattern in fsspec, purely searching this pattern takes more than 10 hours (real use-case).\n\n### Your contribution\n\nYeah I can make this happen if this seems valid. @lhoestq WDYT?\nsuch like\n```\ndef _get_data_files_patterns(pattern_resolver: Callable[[str], list[str]], patterns: PATTERNS) -> dict[str, list[str]]:\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! The pattern can be specified in advance in YAML in the README.md of the dataset :)\n\nFor example\n\n```\n---\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: \"train/*\"\n  - split: test\n    path: \"test/*\"\n---\n```\n\nSee the docs at https://huggingface.co/docs/hub/en/datasets-manual-configuration"
      },
      {
        "user": "SangbumChoi",
        "body": "@lhoestq How can we choose in this case ? https://huggingface.co/datasets/datasets-examples/doc-image-5\n"
      },
      {
        "user": "lhoestq",
        "body": "choose what ? sorry I didn't get it ^^'"
      }
    ]
  },
  {
    "issue_number": 7425,
    "title": "load_dataset(\"livecodebench/code_generation_lite\", version_tag=\"release_v2\")  TypeError: 'NoneType' object is not callable",
    "author": "dshwei",
    "state": "open",
    "created_at": "2025-02-27T07:36:02Z",
    "updated_at": "2025-03-27T05:05:33Z",
    "labels": [],
    "body": "### Describe the bug\n\nfrom datasets import load_dataset\nlcb_codegen = load_dataset(\"livecodebench/code_generation_lite\", version_tag=\"release_v2\")\n\nor\nconfigs = get_dataset_config_names(\"livecodebench/code_generation_lite\", trust_remote_code=True)\n\nboth error:\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/workspace/miniconda/envs/grpo/lib/python3.10/site-packages/datasets/load.py\", line 2131, in load_dataset\nbuilder_instance = load_dataset_builder(\nFile \"/workspace/miniconda/envs/grpo/lib/python3.10/site-packages/datasets/load.py\", line 1888, in load_dataset_builder\nbuilder_instance: DatasetBuilder = builder_cls(\nTypeError: 'NoneType' object is not callable\n\n### Steps to reproduce the bug\n\nfrom datasets import get_dataset_config_names\n\nconfigs = get_dataset_config_names(\"livecodebench/code_generation_lite\", trust_remote_code=True)\nOR \nlcb_codegen = load_dataset(\"livecodebench/code_generation_lite\", version_tag=\"release_v2\")\n\n### Expected behavior\n\nload  datasets  livecodebench/code_generation_lite\n\n\n### Environment info\n\n import  datasets\nversion '3.3.2'",
    "comments": [
      {
        "user": "zwxandy",
        "body": "> datasets\n\nHi, have you solved this bug? Today I also met the same problem about `livecodebench/code_generation_lite` when evaluating the `Open-R1` repo. I am looking forward to your reply!\n\n![Image](https://github.com/user-attachments/assets/02e92fbf-da33-41b3-b8d4-f79b293a54f1)"
      },
      {
        "user": "Serzhanov",
        "body": "Hey guys,\nI tried to reproduce the issue and it works fine. I used google colab as enviroment.\n\n![Image](https://github.com/user-attachments/assets/024dd8e1-bd10-470b-9a6d-60759ffdb984)"
      },
      {
        "user": "zwxandy",
        "body": "> Hey guys, I tried to reproduce the issue and it works fine. I used google colab as enviroment.\n> \n> ![Image](https://github.com/user-attachments/assets/024dd8e1-bd10-470b-9a6d-60759ffdb984)\n\nThanks for your kind reply! I wonder which Python version do you use? My Python version is 3.11.11 and datasets version is 3.3.2 but I still met this bug.\n\n<img width=\"1121\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7c2c5007-ee55-4030-94b9-01fcdea0bf4a\" />"
      }
    ]
  },
  {
    "issue_number": 7479,
    "title": "Features.from_arrow_schema is destructive",
    "author": "BramVanroy",
    "state": "open",
    "created_at": "2025-03-26T16:46:43Z",
    "updated_at": "2025-03-26T16:46:58Z",
    "labels": [],
    "body": "### Describe the bug\n\nI came across this, perhaps niche, bug where `Features` does not/cannot account for pyarrow's `nullable=False` option in Fields. Interestingly, I found that in regular \"flat\" fields this does not necessarily lead to conflicts, but when a non-nullable field is in a struct, an incompatibility arises.\n\nIt's not easy to explain in words, so the minimal example below should help I hope.\n\nNote that I suggest a solution in the comments in the code, simply allowing `Dataset.to_parquet` to allow for a `schema` argument which, when provided, will override the default ds.features.arrow_schema.\n\n### Steps to reproduce the bug\n\n```python\nimport os\nfrom datasets import Dataset, Features\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n# HF datasets is destructive when you call Features.from_arrow_schema(schema) on a schema \n# because it will not account for nullable and non-nullable fields in structs (it will always allow nullable)\n# Reloading the same dataset with the original schema will raise an error because the schema is not the same anymore\nnon_nullable_schema = pa.schema(\n    [\n        pa.field(\"text\", pa.string(), nullable=False),\n        pa.field(\"meta\",\n            pa.struct(\n                [\n                    pa.field(\"date\", pa.list_(pa.string()), nullable=False),\n                ],\n            ),\n        ),\n\n    ]\n)\nprint(\"ORIGINAL SCHEMA\")\nprint(non_nullable_schema)\nprint()\n\nfeats = Features.from_arrow_schema(non_nullable_schema)\n\nprint(\"FEATUR-IZED SCHEMA (nullable-restrictions are gone)\")\nprint(feats.arrow_schema)\nprint()\n\nds = Dataset.from_dict(\n    {\n        \"text\": [\"a\", \"b\", \"c\"],\n        \"meta\": [{\"date\": [\"2021-01-01\"]}, {\"date\": [\"2021-01-02\"]}, {\"date\": [\"2021-01-03\"]}],\n    },\n    features=feats,\n)\n\nfname = \"tmp.parquet\"\n\n# This is not possible: TypeError: pyarrow.parquet.core.ParquetWriter() got multiple values for keyword argument 'schema'\n# Though I believe this would be the easiest fix: allow schema to be passed to to_parquet and overwrite the schema in the dataset\n# ds.to_parquet(fname, schema=non_nullable_schema)\n\nds.to_parquet(fname)\n\ntry:\n    _ = pq.read_table(fname, schema=non_nullable_schema)\nfinally:\n    os.unlink(fname)\n```\n\n\n### Expected behavior\n\n- Non-destructive behavior when converting an arrow schema to Features; or\n- the ability to override the default arrow schema with a custom one\n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-5.14.0-427.20.1.el9_4.x86_64-x86_64-with-glibc2.34\n- Python version: 3.11.10\n- `huggingface_hub` version: 0.27.1\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0",
    "comments": []
  },
  {
    "issue_number": 842,
    "title": "How to enable `.map()` pre-processing pipelines to support multi-node parallelism?",
    "author": "shangw-nvidia",
    "state": "open",
    "created_at": "2020-11-12T02:04:38Z",
    "updated_at": "2025-03-26T09:10:22Z",
    "labels": [],
    "body": "Hi,\r\n\r\nCurrently, multiprocessing can be enabled for the `.map()` stages on a single node. However, in the case of multi-node training, (since more than one node would be available) I'm wondering if it's possible to extend the parallel processing among nodes, instead of only 1 node running the `.map()` while the other node is waiting for it to finish?\r\n\r\nThanks!",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Right now multiprocessing only runs on single node.\r\n\r\nHowever it's probably possible to extend it to support multi nodes. Indeed we're using the `multiprocess` library from the `pathos` project to do multiprocessing in `datasets`, and `pathos` is made to support parallelism on several nodes. More info about pathos [on the pathos repo](https://github.com/uqfoundation/pathos).\r\n\r\nIf you're familiar with pathos or if you want to give it a try, it could be a nice addition to the library :)"
      },
      {
        "user": "VictorSanh",
        "body": "Curious to hear if anything on that side changed or if you suggestions to do it changed @lhoestq :)\r\n\r\nFor our use-case, we are entering the regime where trading a few more instances to save a few days would be nice :)"
      },
      {
        "user": "lhoestq",
        "body": "Currently for multi-node setups we're mostly going towards a nice integration with Dask. But I wouldn't exclude exploring `pathos` more at one point"
      }
    ]
  },
  {
    "issue_number": 7467,
    "title": "load_dataset with streaming hangs on parquet datasets",
    "author": "The0nix",
    "state": "open",
    "created_at": "2025-03-18T23:33:54Z",
    "updated_at": "2025-03-25T10:28:04Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen I try to load a dataset with parquet files (e.g. \"bigcode/the-stack\") the dataset loads, but python interpreter can't exit and hangs\n\n### Steps to reproduce the bug\n\n```python3\nimport datasets\n\nprint('Start')\ndataset = datasets.load_dataset(\"bigcode/the-stack\", data_dir=\"data/yaml\", streaming=True, split=\"train\")\nit = iter(dataset)\nnext(it)\nprint('Finish')\n```\n\nThe program prints finish but doesn't exit and hangs indefinitely.\nI tried this on two different machines and several datasets.\n\n### Expected behavior\n\nThe program exits successfully\n\n### Environment info\n\ndatasets==3.4.1\nPython 3.12.9.\nMacOS and Ubuntu Linux",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! The issue comes from `pyarrow`, I reported it here: https://github.com/apache/arrow/issues/45214 (feel free to comment / thumb up).\n\nAlternatively we can try to find something else than `ParquetFileFragment.to_batches()` to iterate on Parquet data and keep the option the pass `filters=`..."
      }
    ]
  },
  {
    "issue_number": 7468,
    "title": "function `load_dataset`  can't solve folder path with regex characters like \"[]\"",
    "author": "Hpeox",
    "state": "open",
    "created_at": "2025-03-20T05:21:59Z",
    "updated_at": "2025-03-25T10:18:12Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen using the `load_dataset` function with a folder path containing regex special characters (such as \"[]\"), the issue occurs due to how the path is handled in the `resolve_pattern` function. This function passes the unprocessed path directly to `AbstractFileSystem.glob`, which supports regular expressions. As a result, the globbing mechanism interprets these characters as regex patterns, leading to a traversal of the entire disk partition instead of confining the search to the intended directory.\n\n### Steps to reproduce the bug\n\njust create a folder like `E:\\[D_DATA]\\koch_test`, then `load_dataset(\"parquet\", data_dir=\"E:\\[D_DATA]\\\\test\", split=\"train\")`\nit will keep searching the whole disk.\n\nI add two `print` in `glob` and `resolve_pattern` to see the path\n\n### Expected behavior\n\nit should load the dataset as in normal folders\n\n### Environment info\n\n- `datasets` version: 3.3.2\n- Platform: Windows-10-10.0.22631-SP0\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.29.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Have you tried escaping the glob special characters `[` and `]` ?\n\nbtw note that`AbstractFileSystem.glob` doesn't support regex, instead it supports glob patterns as in the python library [glob](https://docs.python.org/3/library/glob.html)\n"
      }
    ]
  },
  {
    "issue_number": 6896,
    "title": "Regression bug: `NonMatchingSplitsSizesError` for (possibly) overwritten dataset",
    "author": "finiteautomata",
    "state": "open",
    "created_at": "2024-05-13T15:41:57Z",
    "updated_at": "2025-03-25T01:21:06Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nWhile trying to load the dataset `https://huggingface.co/datasets/pysentimiento/spanish-tweets-small`, I get this error:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nNonMatchingSplitsSizesError               Traceback (most recent call last)\r\n[<ipython-input-1-d6a3c721d3b8>](https://localhost:8080/#) in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 ds = load_dataset(\"pysentimiento/spanish-tweets-small\")\r\n\r\n3 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2150 \r\n   2151     # Download and prepare data\r\n-> 2152     builder_instance.download_and_prepare(\r\n   2153         download_config=download_config,\r\n   2154         download_mode=download_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    946                         if num_proc is not None:\r\n    947                             prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 948                         self._download_and_prepare(\r\n    949                             dl_manager=dl_manager,\r\n    950                             verification_mode=verification_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1059 \r\n   1060         if verification_mode == VerificationMode.BASIC_CHECKS or verification_mode == VerificationMode.ALL_CHECKS:\r\n-> 1061             verify_splits(self.info.splits, split_dict)\r\n   1062 \r\n   1063         # Update the info object with the splits.\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/utils/info_utils.py](https://localhost:8080/#) in verify_splits(expected_splits, recorded_splits)\r\n     98     ]\r\n     99     if len(bad_splits) > 0:\r\n--> 100         raise NonMatchingSplitsSizesError(str(bad_splits))\r\n    101     logger.info(\"All the splits matched successfully.\")\r\n    102 \r\n\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=82649695458, num_examples=597433111, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=3358310095, num_examples=24898932, shard_lengths=[3626991, 3716991, 4036990, 3506990, 3676990, 3716990, 2616990], dataset_name='spanish-tweets-small')}]\r\n```\r\n\r\nI think I had this dataset updated, might be related to #6271 \r\n\r\nIt is working fine as late in `2.10.0` , but not in `2.13.0` onwards.\r\n\r\n### Steps to reproduce the bug\r\n\r\n\r\n```python\r\n\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"pysentimiento/spanish-tweets-small\")\r\n```\r\n\r\nYou can run it in [this notebook](https://colab.research.google.com/drive/1FdhqLiVimHIlkn7B54DbhizeQ4U3vGVl#scrollTo=YgA50cBSibUg)\r\n\r\n### Expected behavior\r\n\r\nLoad the dataset without any error\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.20.3\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.0.3",
    "comments": [
      {
        "user": "francescotaioli",
        "body": "Same issue here\n"
      }
    ]
  },
  {
    "issue_number": 7261,
    "title": "Cannot load the cache when mapping the dataset",
    "author": "zhangn77",
    "state": "open",
    "created_at": "2024-10-29T08:29:40Z",
    "updated_at": "2025-03-24T13:27:55Z",
    "labels": [],
    "body": "### Describe the bug\n\nI'm training the flux controlnet. The train_dataset.map() takes long time to finish. However, when I killed one training process and want to restart a new training with the same dataset. I can't reuse the mapped result even I defined the cache dir for the dataset.\r\n\r\nwith accelerator.main_process_first():\r\n        from datasets.fingerprint import Hasher\r\n\r\n        # fingerprint used by the cache for the other processes to load the result\r\n        # details: https://github.com/huggingface/diffusers/pull/4038#discussion_r1266078401\r\n        new_fingerprint = Hasher.hash(args)\r\n        train_dataset = train_dataset.map(\r\n            compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint, batch_size=10,\r\n        )\n\n### Steps to reproduce the bug\n\ntrain flux controlnet and start again\r\n\r\n\n\n### Expected behavior\n\nwill not map again\n\n### Environment info\n\nlatest diffusers\r\n\r\n",
    "comments": [
      {
        "user": "shawn-AI-Tech",
        "body": "@zhangn77 Hi ，have you solved this problem? I encountered the same issue during training. Could we discuss it?"
      },
      {
        "user": "MtYCNN",
        "body": "I also encountered the same problem, why is that？"
      }
    ]
  },
  {
    "issue_number": 7406,
    "title": "Adding Core Maintainer List to CONTRIBUTING.md",
    "author": "jp1924",
    "state": "closed",
    "created_at": "2025-02-17T00:32:40Z",
    "updated_at": "2025-03-24T10:57:54Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nI propose adding a core maintainer list to the `CONTRIBUTING.md` file.\n\n### Motivation\n\nThe Transformers and Liger-Kernel projects maintain lists of core maintainers for each module. \nHowever, the Datasets project doesn't have such a list. \n\n### Your contribution\n\nI have nothing to add here.",
    "comments": [
      {
        "user": "jp1924",
        "body": "@lhoestq"
      },
      {
        "user": "lhoestq",
        "body": "there is no per-module maintainer and the list is me alone nowadays ^^'"
      },
      {
        "user": "jp1924",
        "body": "@lhoestq \nOh... I feel for you. \nWhat are your criteria for choosing a core maintainer? \nIt seems like it's too much work for you to manage all this code by yourself.\n\nAlso, if you don't mind, can you check this PR for me?\n#7368 I'd like this to be added as soon as possible because I need it."
      }
    ]
  },
  {
    "issue_number": 7356,
    "title": "How about adding a feature to pass the key when performing map on DatasetDict?",
    "author": "jp1924",
    "state": "closed",
    "created_at": "2025-01-06T08:13:52Z",
    "updated_at": "2025-03-24T10:57:47Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nAdd a feature to pass the key of the DatasetDict when performing map\n\n### Motivation\n\nI often preprocess using map on DatasetDict. \r\nSometimes, I need to preprocess train and valid data differently depending on the task. \r\nSo, I thought it would be nice to pass the key (like train, valid) when performing map on DatasetDict. \r\n\r\nWhat do you think?\n\n### Your contribution\n\nI can submit a pull request to add the feature to pass the key of the DatasetDict when performing map.",
    "comments": [
      {
        "user": "jp1924",
        "body": "@lhoestq \r\nIf it's okay with you, can I work on this?"
      },
      {
        "user": "lhoestq",
        "body": "Hi ! Can you give an example of what it would look like to use this new feature ?\r\n\r\nNote that currently you can already do\r\n\r\n```python\r\nds[\"train\"] = ds[\"train\"].map(process_train)\r\nds[\"test\"] = ds[\"test\"].map(process_test)\r\n```"
      },
      {
        "user": "jp1924",
        "body": "@lhoestq \nThanks for the response! \nLet me clarify what I'm looking for with an example:\n\nCurrently, we need to write separate processing functions or call .map() separately:\n```python\n# Current approach\ndef process_train(example):\n    # Training-specific processing\n    return example\n\ndef process_valid(example):\n    # Validation-specific processing\n    return example\n\nds[\"train\"] = ds[\"train\"].map(process_train)\nds[\"valid\"] = ds[\"valid\"].map(process_valid)\n```\n\nWhat I'm proposing is to have a single processing function that knows which split it's processing:\n\n```python\n# Proposed feature\ndef process(example, split_key):\n    if split_key == \"train\":\n        # Training-specific processing\n    elif split_key == \"valid\":\n        # Validation-specific processing\n    return example\n\n# Using with_key=True to pass the split information\nds = ds.map(process, with_key=True)\n```\n\nThis becomes particularly useful when:\n1. The processing logic is heavily shared between splits but needs minor adjustments\n2. You want to maintain the processing logic in one place for better maintainability\n3. The processing function is complex and you want to avoid duplicating code\n\nSo I wanted to request this feature to achieve this kind of functionality. \nI've created a draft PR implementing this: https://github.com/huggingface/datasets/pull/7240/files\n"
      }
    ]
  },
  {
    "issue_number": 7448,
    "title": "`datasets.disable_caching` doesn't work",
    "author": "UCC-team",
    "state": "open",
    "created_at": "2025-03-13T06:40:12Z",
    "updated_at": "2025-03-22T04:37:07Z",
    "labels": [],
    "body": "When I use `Dataset.from_generator(my_gen)` to load my dataset, it simply skips my changes to the generator function.\nI tried `datasets.disable_caching`, but it doesn't work!",
    "comments": [
      {
        "user": "UCC-team",
        "body": "cc"
      },
      {
        "user": "hrbigelow",
        "body": "Yes I have the same issue.  It's a confusingly named function.  See [here](https://github.com/huggingface/datasets/blob/main/src/datasets/fingerprint.py#L115-L130)\n\n```\n...\nIf disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\n    More precisely, if the caching is disabled:\n    - cache files are always recreated\n    - cache files are written to a temporary directory that is deleted when session closes\n    - cache files are named using a random hash instead of the dataset fingerprint\n```\n\nAlso, unfortunately the member variable `ds.cache_files` is not populated either.\n\nI'll let you know if I find a solution."
      }
    ]
  },
  {
    "issue_number": 7473,
    "title": "Webdataset data format problem",
    "author": "edmcman",
    "state": "closed",
    "created_at": "2025-03-21T17:23:52Z",
    "updated_at": "2025-03-21T19:19:58Z",
    "labels": [],
    "body": "### Describe the bug\n\nPlease see https://huggingface.co/datasets/ejschwartz/idioms/discussions/1\n\nError code: FileFormatMismatchBetweenSplitsError\n\nAll three splits, train, test, and validation, use webdataset. But only the train split has more than one file. How can I force the other two splits to also be interpreted as being the webdataset format?  (I don't think there is currently a way, but happy to be told that I am wrong.)\n\n### Steps to reproduce the bug\n\n```\nimport datasets\ndatasets.load_dataset(\"ejschwartz/idioms\")\n\n### Expected behavior\n\nThe dataset loads.  Alternatively, there is a YAML syntax for manually specifying the format.\n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\n- Python version: 3.10.12\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0",
    "comments": [
      {
        "user": "edmcman",
        "body": "I was able to work around it"
      }
    ]
  },
  {
    "issue_number": 5806,
    "title": "Return the name of the currently loaded file in the load_dataset function.",
    "author": "s-JoL",
    "state": "open",
    "created_at": "2023-04-28T13:50:15Z",
    "updated_at": "2025-03-21T12:07:15Z",
    "labels": [
      "enhancement",
      "good first issue"
    ],
    "body": "### Feature request\n\nAdd an optional parameter return_file_name in the load_dataset function. When it is set to True, the function will include the name of the file corresponding to the current line as a feature in the returned output.\n\n### Motivation\n\nWhen training large language models, machine problems may interrupt the training process. In such cases, it is common to load a previously saved checkpoint to resume training. I would like to be able to obtain the names of the previously trained data shards, so that I can skip these parts of the data during continued training to avoid overfitting and redundant training time.\n\n### Your contribution\n\nI currently use a dataset in jsonl format, so I am primarily interested in the json format. I suggest adding the file name to the returned table here https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/json/json.py#L92.",
    "comments": [
      {
        "user": "mariosasko",
        "body": "Implementing this makes sense (e.g., `tensorflow_datasets`' imagefolder returns image filenames). Also, in Datasets 3.0, we plan only to store the bytes of an image/audio, not its path, so this feature would be useful when the path info is still needed."
      },
      {
        "user": "tsabbir96",
        "body": "Hey @mariosasko, Can I work on this issue, this one seems interesting to implement. I have contributed to jupyterlab recently, and would love to contribute here as well. "
      },
      {
        "user": "albertvillanova",
        "body": "@tsabbir96 if you are planning to start working on this, you can take on this issue by writing a comment with only the keyword: #self-assign"
      }
    ]
  },
  {
    "issue_number": 7469,
    "title": "Custom split name with the web interface",
    "author": "vince62s",
    "state": "closed",
    "created_at": "2025-03-20T20:45:59Z",
    "updated_at": "2025-03-21T07:20:37Z",
    "labels": [],
    "body": "### Describe the bug\n\nAccording the doc here: https://huggingface.co/docs/hub/datasets-file-names-and-splits#custom-split-name\nit should infer the split name from the subdir of data or the beg of the name of the files in data.\nWhen doing this manually through web upload it does not work. it uses \"train\" as a unique split.\nexample: https://huggingface.co/datasets/eole-nlp/estimator_chatml\n\n### Steps to reproduce the bug\n\nfollow the link above\n\n### Expected behavior\n\nthere should be two splits \"mlqe\" and \"1720_da\"\n\n### Environment info\n\nwebsite",
    "comments": []
  },
  {
    "issue_number": 3753,
    "title": "Expanding streaming capabilities",
    "author": "lvwerra",
    "state": "open",
    "created_at": "2022-02-18T10:45:41Z",
    "updated_at": "2025-03-19T14:50:14Z",
    "labels": [
      "enhancement"
    ],
    "body": "Some ideas for a few features that could be useful when working with large datasets in streaming mode. \r\n\r\n## `filter` for `IterableDataset`\r\nAdding filtering to streaming datasets would be useful in several scenarios:\r\n- filter a dataset with many languages for a subset of languages\r\n- filter a dataset for specific licenses\r\n- other custom logic to get a subset\r\nThe only way to achieve this at the moment is I think through writing a custom loading script and implementing filters there.\r\n\r\n## `IterableDataset` to `Dataset` conversion\r\nIn combination with the above filter a functionality to \"play\" the whole stream would be useful. The motivation is that often one might filter the dataset to get a manageable size for experimentation. In that case streaming mode is no longer necessary as the filtered dataset is small enough and it would be useful to be able to play through the whole stream to create a normal `Dataset` with all its benefits.\r\n\r\n```python\r\nds = load_dataset(\"some_large_dataset\", streaming=True) \r\nds_filter = ds.filter(lambda x: x[\"lang\"]=\"fr\")\r\nds_filter = ds_filter.stream() # here the `IterableDataset` is converted to a `Dataset`\r\n```\r\nNaturally, this could be expanded with `stream(n=1000)` which creates a `Dataset` with the first `n` elements similar to `take`.\r\n\r\n## Stream to the Hub\r\nWhile streaming allows to use a dataset as is without saving the whole dataset on the local machine it is currently not possible to process a dataset and add it to the hub. The only way to do this is by downloading the full dataset and saving the processed dataset again before pushing them to the hub. The API could looks something like:\r\n\r\n```python\r\nds = load_dataset(\"some_large_dataset\", streaming=True)\r\nds_filter = ds.filter(some_filter_func)\r\nds_processed = ds_filter.map(some_processing_func)\r\nds_processed.push_to_hub(\"new_better_dataset\", batch_size=100_000)\r\n```\r\n\r\nUnder the hood this could be done by processing and aggregating `batch_size` elements and then pushing that batch as a single file to the hub. With this functionality one could process and create TB scale datasets while only requiring size of `batch_size` local disk space.\r\n\r\ncc @lhoestq @albertvillanova ",
    "comments": [
      {
        "user": "mariosasko",
        "body": "Related to: https://github.com/huggingface/datasets/issues/3444"
      },
      {
        "user": "lhoestq",
        "body": "Cool ! `filter` will be very useful. There can be a filter that you can apply on a streaming dataset:\r\n```python\r\nload_dataset(..., streaming=True).filter(lambda x: x[\"lang\"] == \"sw\")\r\n```\r\n\r\nOtherwise if you want to apply a filter on the source files that are going to be used for streaming, the logic has to be impIemented directly in the dataset script, or if there's no dataset script this can be done with pattern matching\r\n```python\r\nload_dataset(..., lang=\"sw\")                    # if the dataset script supports this parameter\r\nload_dataset(..., data_files=\"data/lang=sw/*\")  # if there's no dataset script, but only data files\r\n```\r\n\r\n--------------\r\n\r\nHere are also some additional ideas of API to convert from iterable to map-style dataset:\r\n```python\r\non_disk_dataset = streaming_dataset.to_disk()\r\non_disk_dataset = streaming_dataset.to_disk(path=\"path/to/my/dataset/dir\")\r\n\r\nin_memory_dataset = streaming_dataset.take(100).to_memory()  # to experiment without having to write files\r\n```\r\n--------------\r\n\r\nFinally regarding `push_to_hub`, we can replace `batch_size` by `shard_size` (same API as for on-disk datasets). The default is 500MB per file\r\n\r\nLet me know what you think !"
      },
      {
        "user": "athewsey",
        "body": "Regarding conversion, I'd also ask for some kind of equivalent to `save_to_disk` for an `IterableDataset`.\r\n\r\nSimilarly to the streaming to hub idea, my use case would be to define a sequence of dataset transforms via `.map()`, using an `IterableDataset` as the input (so processing could start without doing whole download up-front), but streaming the resultant processed dataset just to disk."
      }
    ]
  },
  {
    "issue_number": 5720,
    "title": "Streaming IterableDatasets do not work with torch DataLoaders",
    "author": "jlehrer1",
    "state": "open",
    "created_at": "2023-04-08T18:45:48Z",
    "updated_at": "2025-03-19T14:06:47Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen using streaming datasets set up with train/val split using `.skip()` and `.take()`, the following error occurs when iterating over a torch dataloader:\r\n```\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 363, in __iter__\r\n    self._iterator = self._get_iterator()\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 314, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 927, in __init__\r\n    w.start()\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/multiprocessing/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/Users/julian/miniconda3/envs/sims/lib/python3.9/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_generate_examples_from_tables_wrapper.<locals>.wrapper'\r\n```\r\n\r\nTo reproduce, run the code \r\n```\r\nfrom datasets import load_dataset\r\ndata = load_dataset(args.dataset_name, split=\"train\", streaming=True)\r\ntrain_len = 5000\r\nval_len = 100\r\n\r\ntrain, val = data.take(train_len), data.skip(train_len).take(val_len)\r\ntraindata = IterableClipDataset(data, context_length=args.max_len, tokenizer=tokenizer, image_key=\"url\", text_key=\"text\")\r\n\r\ntraindata = DataLoader(traindata, batch_size=args.batch_size, num_workers=args.num_workers, persistent_workers=True)\r\n```\r\n\r\nWhere the class IterableClipDataset is a simple wrapper to cast the dataset to a torch iterabledataset, defined via \r\n```\r\nfrom torch.utils.data import Dataset, IterableDataset\r\nfrom torchvision.transforms import Compose, Resize, ToTensor\r\nfrom transformers import AutoTokenizer\r\nimport requests\r\nfrom PIL import Image\r\n\r\nclass IterableClipDataset(IterableDataset):\r\n    def __init__(self, dataset, context_length: int, image_transform=None, tokenizer=None, image_key=\"image\", text_key=\"text\"):\r\n        self.dataset = dataset\r\n        self.context_length = context_length\r\n        self.image_transform = Compose([Resize((224, 224)), ToTensor()]) if image_transform is None else image_transform\r\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") if tokenizer is None else tokenizer\r\n        self.image_key = image_key\r\n        self.text_key = text_key\r\n\r\n    def read_image(self, url: str):\r\n        try: # Try to read the image\r\n            image = Image.open(requests.get(url, stream=True).raw)\r\n        except:\r\n            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\r\n        return image\r\n    \r\n    def process_sample(self, image, text):\r\n        if isinstance(image, str):\r\n            image = self.read_image(image)\r\n        if self.image_transform is not None:\r\n            image = self.image_transform(image)\r\n        text = self.tokenizer.encode(\r\n            text, add_special_tokens=True, max_length=self.context_length, truncation=True, padding=\"max_length\"\r\n        )\r\n        text = torch.tensor(text, dtype=torch.long)\r\n        return image, text\r\n\r\n    def __iter__(self):\r\n        for sample in self.dataset:\r\n            image, text = sample[self.image_key], sample[self.text_key]\r\n            yield self.process_sample(image, text)\r\n```\r\n\r\n\n\n### Steps to reproduce the bug\n\nSteps to reproduce\r\n1. Install `datasets`, `torch`, and `PIL` (if you want to reproduce exactly)\r\n2. Run the code above\n\n### Expected behavior\n\nBatched data is produced from the dataloader \n\n### Environment info\n\n```\r\ndatasets == 2.9.0\r\npython == 3.9.12\r\ntorch == 1.11.0\r\n```",
    "comments": [
      {
        "user": "jlehrer1",
        "body": "Edit: This behavior is true even without `.take/.set`"
      },
      {
        "user": "ivanprado",
        "body": "I'm experiencing the same problem that @jlehrer1. I was able to reproduce it with a very small example:\r\n\r\n```py\r\nfrom datasets import Dataset, load_dataset, load_dataset_builder\r\nfrom torch.utils.data import DataLoader\r\n\r\n\r\ndef my_gen():\r\n    for i in range(1, 4):\r\n        yield {\"a\": i}\r\n\r\n# Saving the dataset as a parquet file\r\ndataset = Dataset.from_generator(my_gen)\r\ntrain_path = \"/tmp/test.parquet\"\r\ndataset.to_parquet(train_path)\r\n\r\n# Creating a local dataset from the parquet file\r\ndata_files = {\"train\": [str(train_path)]}\r\nbuilder = load_dataset_builder(\"parquet\", data_files=data_files)\r\nbuilder.download_and_prepare(\"/tmp/test_ds\", file_format=\"parquet\")\r\n\r\n# Loading the dataset from the local directory as streaming\r\ndataset = load_dataset(\"parquet\", data_dir=\"/tmp/test_ds\", split=\"train\", streaming=True)\r\ndataset.with_format(\"torch\")\r\n\r\ndl = DataLoader(dataset, batch_size=2, num_workers=1)\r\nfor row in dl:\r\n    print(row)\r\n```\r\n\r\nMy env info:\r\n```\r\ndatasets                      2.11.0\r\ntorch                         2.0.0\r\ntorchvision                   0.15.1\r\nPython 3.9.16\r\n```\r\n\r\nNote that the example above doesn't fail if the number of workers used is `0`"
      },
      {
        "user": "mariosasko",
        "body": "I cannot reproduce this error, not even with your MRE @ivanprado (your env appears to be the same as Colab's, and your code runs there without issues). "
      }
    ]
  },
  {
    "issue_number": 7196,
    "title": "concatenate_datasets does not preserve shuffling state",
    "author": "alex-hh",
    "state": "open",
    "created_at": "2024-10-03T14:30:38Z",
    "updated_at": "2025-03-18T10:56:47Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nAfter concatenate datasets on an iterable dataset, the shuffling state is destroyed, similar to #7156 \r\n\r\nThis means concatenation cant be used for resolving uneven numbers of samples across devices when using iterable datasets in a distributed setting as discussed in #6623 \r\n\r\nI also noticed that the number of shards is the same after concatenation, which I found surprising, but I don't understand the internals well enough to know whether this is actually surprising or not\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport datasets\r\nimport torch.utils.data\r\n\r\n\r\ndef gen(shards):\r\n    yield {\"shards\": shards}\r\n\r\n\r\ndef main():\r\n    dataset1 = datasets.IterableDataset.from_generator(\r\n        gen, gen_kwargs={\"shards\": list(range(25))}  # TODO: how to understand this?\r\n    )\r\n    dataset2 = datasets.IterableDataset.from_generator(\r\n        gen, gen_kwargs={\"shards\": list(range(25, 50))}  # TODO: how to understand this?\r\n    )\r\n    dataset1 = dataset1.shuffle(buffer_size=1)\r\n    dataset2 = dataset2.shuffle(buffer_size=1)\r\n    print(dataset1.n_shards)\r\n    print(dataset2.n_shards)\r\n\r\n    dataset = datasets.concatenate_datasets(\r\n        [dataset1, dataset2]\r\n    )\r\n    print(dataset.n_shards)\r\n    # dataset = dataset1\r\n\r\n    dataloader = torch.utils.data.DataLoader(\r\n        dataset,\r\n        batch_size=8,\r\n        num_workers=0,\r\n    )\r\n\r\n    for i, batch in enumerate(dataloader):\r\n        print(batch)\r\n    print(\"\\nNew epoch\")\r\n\r\n    dataset = dataset.set_epoch(1)\r\n\r\n    for i, batch in enumerate(dataloader):\r\n        print(batch)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Expected behavior\r\n\r\nShuffling state should be preserved\r\n\r\n### Environment info\r\n\r\nLatest datasets",
    "comments": [
      {
        "user": "lhoestq",
        "body": "It also does preserve `split_by_node`, so in the meantime you should call `shuffle` or `split_by_node` AFTER `interleave_datasets` or `concatenate_datasets`"
      }
    ]
  },
  {
    "issue_number": 7156,
    "title": "interleave_datasets resets shuffle state",
    "author": "jonathanasdf",
    "state": "open",
    "created_at": "2024-09-20T17:57:54Z",
    "updated_at": "2025-03-18T10:56:25Z",
    "labels": [],
    "body": "### Describe the bug\n\n```\r\nimport datasets\r\nimport torch.utils.data\r\n\r\n\r\ndef gen(shards):\r\n    yield {\"shards\": shards}\r\n\r\n\r\ndef main():\r\n    dataset = datasets.IterableDataset.from_generator(\r\n        gen,\r\n        gen_kwargs={'shards': list(range(25))}\r\n    )\r\n    dataset = dataset.shuffle(buffer_size=1)\r\n    dataset = datasets.interleave_datasets(\r\n        [dataset, dataset], probabilities=[1, 0], stopping_strategy=\"all_exhausted\"\r\n    )\r\n\r\n    dataloader = torch.utils.data.DataLoader(\r\n        dataset,\r\n        batch_size=8,\r\n        num_workers=8,\r\n    )\r\n\r\n    for i, batch in enumerate(dataloader):\r\n        print(batch)\r\n        if i >= 10:\r\n            break\r\n\r\n\r\nif __name__ == \"__main__\":\r\n     main()\r\n```\n\n### Steps to reproduce the bug\n\nRun the script, it will output\r\n\r\n```\r\n{'shards': [tensor([ 0,  8, 16, 24,  0,  8, 16, 24])]}\r\n{'shards': [tensor([ 1,  9, 17,  1,  9, 17,  1,  9])]}\r\n{'shards': [tensor([ 2, 10, 18,  2, 10, 18,  2, 10])]}\r\n{'shards': [tensor([ 3, 11, 19,  3, 11, 19,  3, 11])]}\r\n{'shards': [tensor([ 4, 12, 20,  4, 12, 20,  4, 12])]}\r\n{'shards': [tensor([ 5, 13, 21,  5, 13, 21,  5, 13])]}\r\n{'shards': [tensor([ 6, 14, 22,  6, 14, 22,  6, 14])]}\r\n{'shards': [tensor([ 7, 15, 23,  7, 15, 23,  7, 15])]}\r\n{'shards': [tensor([ 0,  8, 16, 24,  0,  8, 16, 24])]}\r\n{'shards': [tensor([17,  1,  9, 17,  1,  9, 17,  1])]}\r\n{'shards': [tensor([18,  2, 10, 18,  2, 10, 18,  2])]}\r\n```\n\n### Expected behavior\n\nThe shards should be shuffled.\n\n### Environment info\n\n- `datasets` version: 3.0.0\r\n- Platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.25.0\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.6.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "It also does preserve `split_by_node`, so in the meantime you should call `shuffle` or `split_by_node` AFTER `interleave_datasets` or `concatenate_datasets`"
      }
    ]
  },
  {
    "issue_number": 7192,
    "title": "Add repeat() for iterable datasets",
    "author": "alex-hh",
    "state": "closed",
    "created_at": "2024-10-02T17:48:13Z",
    "updated_at": "2025-03-18T10:48:33Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\r\n\r\nIt would be useful to be able to straightforwardly repeat iterable datasets indefinitely, to provide complete control over starting and ending of iteration to the user.\r\n\r\nAn IterableDataset.repeat(n) function could do this automatically\r\n\r\n### Motivation\r\n\r\nThis feature was discussed in this issue https://github.com/huggingface/datasets/issues/7147, and would resolve the need to use the hack of interleave datasets with probability 0 as a simple way to achieve this functionality.\r\n\r\nAn additional benefit might be the simplification of the use of iterable datasets in a distributed setting:\r\nIf the user can assume that datasets will repeat indefinitely, then issues around different numbers of samples appearing on different devices (e.g. https://github.com/huggingface/datasets/issues/6437, https://github.com/huggingface/datasets/issues/6594, https://github.com/huggingface/datasets/issues/6623, https://github.com/huggingface/datasets/issues/6719) can potentially be straightforwardly resolved by simply doing:\r\n\r\nids.repeat(None).take(n_samples_per_epoch)\r\n\r\n\r\n### Your contribution\r\n\r\nI'm not familiar enough with the codebase to assess how straightforward this would be to implement.\r\n\r\nIf it might be very straightforward, I could possibly have a go.",
    "comments": [
      {
        "user": "alex-hh",
        "body": "perhaps concatenate_datasets can already be used to achieve almost the same effect? "
      },
      {
        "user": "lhoestq",
        "body": "`concatenate_datasets` does the job when there is a finite number of repetitions, but in case of `.repeat()` forever we need a new logic in `iterable_dataset.py`"
      },
      {
        "user": "lhoestq",
        "body": "done in https://github.com/huggingface/datasets/pull/7198"
      }
    ]
  },
  {
    "issue_number": 7461,
    "title": "List of images behave differently on IterableDataset and Dataset",
    "author": "FredrikNoren",
    "state": "closed",
    "created_at": "2025-03-17T15:59:23Z",
    "updated_at": "2025-03-18T08:57:17Z",
    "labels": [],
    "body": "### Describe the bug\n\nThis code:\n\n```python\ndef train_iterable_gen():\n        images = np.array(load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\").resize((128, 128)))\n        yield {\n            \"images\": np.expand_dims(images, axis=0),\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [{\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\" }]\n                },\n                {\n                    \"role\": \"assistant\",\n                    \"content\": [{\"type\": \"text\", \"text\": \"duck\" }]\n                }\n            ]\n        }\n    train_ds = Dataset.from_generator(train_iterable_gen,\n                                      features=Features({\n                                           'images': [datasets.Image(mode=None, decode=True, id=None)],\n                                           'messages': [{'content': [{'text': datasets.Value(dtype='string', id=None), 'type': datasets.Value(dtype='string', id=None) }], 'role': datasets.Value(dtype='string', id=None)}]\n                                           } )\n                                           )\n```\n\nworks as I'd expect; if I iterate the dataset then the `images` column returns a `List[PIL.Image.Image]`, i.e. `'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=128x128 at 0x77EFB7EF4680>]`.\n\nBut if I change `Dataset` to `IterableDataset`, the `images` column changes into `'images': [{'path': None, 'bytes': ..]`\n\n### Steps to reproduce the bug\n\nThe code above +\n\n```python\ndef load_image(url):\n        response = requests.get(url)\n        image = Image.open(io.BytesIO(response.content))\n        return image\n```\n\nI'm feeding it to SFTTrainer\n\n### Expected behavior\n\nDataset and IterableDataset would behave the same\n\n### Environment info\n\n```yaml\nrequires-python = \">=3.12\"\ndependencies = [\n    \"av>=14.1.0\",\n    \"boto3>=1.36.7\",\n    \"datasets>=3.3.2\",\n    \"docker>=7.1.0\",\n    \"google-cloud-storage>=2.19.0\",\n    \"grpcio>=1.70.0\",\n    \"grpcio-tools>=1.70.0\",\n    \"moviepy>=2.1.2\",\n    \"open-clip-torch>=2.31.0\",\n    \"opencv-python>=4.11.0.86; sys_platform == 'darwin'\",\n    \"opencv-python-headless>=4.11.0.86; sys_platform == 'linux'\",\n    \"pandas>=2.2.3\",\n    \"pillow>=10.4.0\",\n    \"plotly>=6.0.0\",\n    \"py-spy>=0.4.0\",\n    \"pydantic>=2.10.6\",\n    \"pydantic-settings>=2.7.1\",\n    \"pymysql>=1.1.1\",\n    \"ray[data,default,serve,train,tune]>=2.43.0\",\n    \"torch>=2.6.0\",\n    \"torchmetrics>=1.6.1\",\n    \"torchvision>=0.21.0\",\n    \"transformers[torch]@git+https://github.com/huggingface/transformers\",\n    \"wandb>=0.19.4\",\n    # https://github.com/Dao-AILab/flash-attention/issues/833\n    \"flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl; sys_platform == 'linux'\",\n    \"trl@https://github.com/huggingface/trl.git\",\n    \"peft>=0.14.0\",\n]\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Can you try with `datasets` ^3.4 released recently ? on my side it works with IterableDataset on the recent version :)\n\n```python\nIn [20]: def train_iterable_gen():\n    ...:         images = np.array(load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\").resize((128, 128)))\n    ...:         yield {\n    ...:             \"images\": np.expand_dims(images, axis=0),\n    ...:             \"messages\": [\n    ...:                 {\n    ...:                     \"role\": \"user\",\n    ...:                     \"content\": [{\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\" }]\n    ...:                 },\n    ...:                 {\n    ...:                     \"role\": \"assistant\",\n    ...:                     \"content\": [{\"type\": \"text\", \"text\": \"duck\" }]\n    ...:                 }\n    ...:             ]\n    ...:         }\n    ...: \n    ...: train_ds = IterableDataset.from_generator(train_iterable_gen,\n    ...:                                     features=Features({\n    ...:                                         'images': [datasets.Image(mode=None, decode=True, id=None)],\n    ...:                                         'messages': [{'content': [{'text': datasets.Value(dtype='string', id=None), 'type': datasets.Value(dtype='string', id=None) }],\n    ...: 'role': datasets.Value(dtype='string', id=None)}]\n    ...:                                         } )\n    ...:                                         )\n\n\nIn [21]: \n\nIn [21]: next(iter(train_ds))\n/Users/quentinlhoest/hf/datasets/src/datasets/features/image.py:338: UserWarning: Downcasting array dtype int64 to uint8 to be compatible with 'Pillow'\n  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\nOut[21]: \n{'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=128x128>],\n 'messages': [{'content': [{'text': None, 'type': 'image'}], 'role': 'user'},\n  {'content': [{'type': 'text', 'text': 'duck'}], 'role': 'assistant'}]}\n```"
      },
      {
        "user": "FredrikNoren",
        "body": "Hm I tried it here and it works as expected, even on datasets 3.3.2. I guess maybe something in the SFTTrainer is doing additional processing on the dataset, I'll have a look there.\n\nThanks @lhoestq!"
      }
    ]
  },
  {
    "issue_number": 7455,
    "title": "Problems with local dataset after upgrade from 3.3.2 to 3.4.0",
    "author": "andjoer",
    "state": "open",
    "created_at": "2025-03-15T09:22:50Z",
    "updated_at": "2025-03-17T16:20:43Z",
    "labels": [],
    "body": "### Describe the bug\n\nI was not able to open a local saved dataset anymore that was created using an older datasets version after the upgrade yesterday from datasets 3.3.2 to 3.4.0\n\nThe traceback is \n\n```\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/arrow/arrow.py\", line 67, in _generate_tables\n    batches = pa.ipc.open_stream(f)\n  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 190, in open_stream\n    return RecordBatchStreamReader(source, options=options,\n  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 52, in __init__\n    self._open(source, options=options, memory_pool=memory_pool)\n  File \"pyarrow/ipc.pxi\", line 1006, in pyarrow.lib._RecordBatchStreamReader._open\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Expected to read 538970747 metadata bytes, but only read 2126\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1855, in _prepare_split_single\n    for _, table in generator:\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/arrow/arrow.py\", line 69, in _generate_tables\n    reader = pa.ipc.open_file(f)\n  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 234, in open_file\n    return RecordBatchFileReader(\n  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 110, in __init__\n    self._open(source, footer_offset=footer_offset,\n  File \"pyarrow/ipc.pxi\", line 1090, in pyarrow.lib._RecordBatchFileReader._open\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Not an Arrow file\n```\n\n### Steps to reproduce the bug\n\nLoad a dataset from a local folder with \n\n```\ndataset = load_dataset(\n                args.train_data_dir,\n                cache_dir=args.cache_dir,\n            )\n```\nas it is done for example in the training script for SD3 controlnet.\n\nThis is the minimal script to test it: \n\n```\nfrom datasets import load_dataset\n\ndef main():\n    dataset = load_dataset(\n        \"local_dataset\",  \n    )\n    print(dataset)\n    print(\"Sample data:\", dataset[\"train\"][0])\n\nif __name__ == \"__main__\":\n    main()\n````\n\n### Expected behavior\n\nWork in 3.4.0 like in 3.3.2\n\n### Environment info\n\n- `datasets` version: 3.4.0\n- Platform: Linux-5.15.0-75-generic-x86_64-with-glibc2.35\n- Python version: 3.10.12\n- `huggingface_hub` version: 0.29.3\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! I just released 3.4.1 with a fix, let me know if it's working now !"
      }
    ]
  },
  {
    "issue_number": 7458,
    "title": "Loading the `laion/filtered-wit` dataset in streaming mode fails on v3.4.0",
    "author": "nikita-savelyevv",
    "state": "closed",
    "created_at": "2025-03-17T14:54:02Z",
    "updated_at": "2025-03-17T16:02:04Z",
    "labels": [],
    "body": "### Describe the bug\n\nLoading https://huggingface.co/datasets/laion/filtered-wit in streaming mode fails after update to `datasets==3.4.0`. The dataset loads fine on v3.3.2.\n\n### Steps to reproduce the bug\n\nSteps to reproduce:\n```\npip install datastes==3.4.0\npython -c \"from datasets import load_dataset; load_dataset('laion/filtered-wit', split='train', streaming=True)\"\n```\n\nResults in:\n```\n$ python -c \"from datasets import load_dataset; load_dataset('laion/filtered-wit', split='train', streaming=True)\"\nRepo card metadata block was not found. Setting CardData to empty.\nResolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 560/560 [00:00<00:00, 2280.24it/s]\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/load.py\", line 2080, in load_dataset\n    return builder_instance.as_streaming_dataset(split=split)\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/builder.py\", line 1265, in as_streaming_dataset\n    splits_generators = {sg.name: sg for sg in self._split_generators(dl_manager)}\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 49, in _split_generators\n    data_files = dl_manager.download_and_extract(self.config.data_files)\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 169, in download_and_extract\n    return self.extract(self.download(url_or_urls))\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 121, in extract\n    urlpaths = map_nested(self._extract, url_or_urls, map_tuple=True)\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 496, in map_nested\n    mapped = [\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 497, in <listcomp>\n    map_nested(\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 513, in map_nested\n    mapped = [\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 514, in <listcomp>\n    _single_map_nested((function, obj, batched, batch_size, types, None, True, None))\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 375, in _single_map_nested\n    return function(data_struct)\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 131, in _extract\n    raise NotImplementedError(\nNotImplementedError: Extraction protocol for TAR archives like 'hf://datasets/laion/filtered-wit@c38ca7464e9934d9a49f88b3f60f5ad63b245465/data/00000.tar' is not implemented in streaming mode. Please use `dl_manager.iter_archive` instead.\n\nExample usage:\n\n        url = dl_manager.download(url)\n        tar_archive_iterator = dl_manager.iter_archive(url)\n\n        for filename, file in tar_archive_iterator:\n                ...\n```\n\n### Expected behavior\n\nDataset loads successfully.\n\n### Environment info\n\nUbuntu 20.04.6. Python 3.9. Datasets 3.4.0.\n\npip freeze:\n```\naiohappyeyeballs==2.6.1\naiohttp==3.11.14\naiosignal==1.3.2\nasync-timeout==5.0.1\nattrs==25.3.0\ncertifi==2025.1.31\ncharset-normalizer==3.4.1\ndatasets==3.4.0\ndill==0.3.8\nfilelock==3.18.0\nfrozenlist==1.5.0\nfsspec==2024.12.0\nhuggingface-hub==0.29.3\nidna==3.10\nmultidict==6.1.0\nmultiprocess==0.70.16\nnumpy==2.0.2\npackaging==24.2\npandas==2.2.3\npropcache==0.3.0\npyarrow==19.0.1\npython-dateutil==2.9.0.post0\npytz==2025.1\nPyYAML==6.0.2\nrequests==2.32.3\nsix==1.17.0\ntqdm==4.67.1\ntyping_extensions==4.12.2\ntzdata==2025.1\nurllib3==2.3.0\nxxhash==3.5.0\nyarl==1.18.3\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "thanks for reporting, I released 3.4.1 with a fix"
      }
    ]
  },
  {
    "issue_number": 7456,
    "title": ".add_faiss_index and .add_elasticsearch_index returns ImportError at Google Colab",
    "author": "MapleBloom",
    "state": "open",
    "created_at": "2025-03-16T00:51:49Z",
    "updated_at": "2025-03-17T15:57:19Z",
    "labels": [],
    "body": "### Describe the bug\n\nAt Google Colab\n```!pip install faiss-cpu``` works\n```import faiss``` no error\nbut\n```embeddings_dataset.add_faiss_index(column='embeddings')```\nreturns\n```\n[/usr/local/lib/python3.11/dist-packages/datasets/search.py](https://localhost:8080/#) in init(self, device, string_factory, metric_type, custom_index)\n247 self.faiss_index = custom_index\n248 if not _has_faiss:\n--> 249 raise ImportError(\n250 \"You must install Faiss to use FaissIndex. To do so you can run conda install -c pytorch faiss-cpu or conda install -c pytorch faiss-gpu. \"\n251 \"A community supported package is also available on pypi: pip install faiss-cpu or pip install faiss-gpu. \"\n```\n\nbecause\n```_has_faiss = importlib.util.find_spec(\"faiss\") is not None```    at the beginning of ```datasets/search.py``` returns ```False```\nwhen\nthe same code at colab notebook returns\n```ModuleSpec(name='faiss', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7b7851449f50>, origin='/usr/local/lib/python3.11/dist-packages/faiss/init.py', submodule_search_locations=['/usr/local/lib/python3.11/dist-packages/faiss'])```\n\nBut\n```\nimport datasets\ndatasets.search._has_faiss\n```\nat ```colab notebook``` also returns ```False```\n\nThe same story with   ```_has_elasticsearch```\n\n### Steps to reproduce the bug\n\n1. Follow https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt at Google Colab\n2. till ```embeddings_dataset.add_faiss_index(column='embeddings')```\n3. ```embeddings_dataset.add_elasticsearch_index(column='embeddings')```\n4. https://colab.research.google.com/drive/1h2cjuiClblqzbNQgrcoLYOC8zBqTLLcv#scrollTo=3ddzRp72auOF\n\n### Expected behavior\n\nI've only started Tutorial and don't know exactly. But something tells me that ```embeddings_dataset.add_faiss_index(column='embeddings')```\nshould work without ```Import Error```\n\n### Environment info\n\nGoogle Colab notebook with default config",
    "comments": [
      {
        "user": "Akshay-Sisodia",
        "body": "I can fix this.\nIt's mainly because faiss-gpu requires python<=3.10 but the default python version in colab is 3.11. We just have to downgrade the CPython version down to 3.10 and it should work fine.\n"
      },
      {
        "user": "MapleBloom",
        "body": "I think I just had no chance to meet with faiss-cpu.\nIt could be import problem? \n_has_faiss gets its value at the beginning of datasets/search.\nI tried to call object before import faiss, so _has_faiss took False. And never updated later. "
      },
      {
        "user": "Akshay-Sisodia",
        "body": "Yes you can't meet the requirements because faiss-cpu runs only on\r\npython3.10 and lower but the default version for colab is python3.11 which\r\nresults in pip not being able to find wheels for faiss-cpu with python3.11.\r\n\r\nOn Mon, 17 Mar, 2025, 3:56 pm MapleBloom, ***@***.***> wrote:\r\n\r\n> I think I just had no chance to meet with faiss-cpu.\r\n> It could be import problem?\r\n> _has_faiss gets its value at the beginning of datasets/search.\r\n> I tried to call object before import faiss, so _has_faiss took False. And\r\n> never updated later.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/huggingface/datasets/issues/7456#issuecomment-2728975672>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AVUSZMBVD7LEDDUGALOTVN32U2PMBAVCNFSM6AAAAABZDBA426VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDOMRYHE3TKNRXGI>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n> [image: MapleBloom]*MapleBloom* left a comment (huggingface/datasets#7456)\r\n> <https://github.com/huggingface/datasets/issues/7456#issuecomment-2728975672>\r\n>\r\n> I think I just had no chance to meet with faiss-cpu.\r\n> It could be import problem?\r\n> _has_faiss gets its value at the beginning of datasets/search.\r\n> I tried to call object before import faiss, so _has_faiss took False. And\r\n> never updated later.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/huggingface/datasets/issues/7456#issuecomment-2728975672>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AVUSZMBVD7LEDDUGALOTVN32U2PMBAVCNFSM6AAAAABZDBA426VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDOMRYHE3TKNRXGI>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      }
    ]
  },
  {
    "issue_number": 7449,
    "title": "Cannot load data with different schemas from different parquet files",
    "author": "li-plus",
    "state": "closed",
    "created_at": "2025-03-13T08:14:49Z",
    "updated_at": "2025-03-17T07:27:48Z",
    "labels": [],
    "body": "### Describe the bug\n\nCannot load samples with optional fields from different files. The schema cannot be correctly derived.\n\n### Steps to reproduce the bug\n\nWhen I place two samples with an optional field `some_extra_field` within a single parquet file, it can be loaded via `load_dataset`.\n\n```python\nimport pandas as pd\nfrom datasets import load_dataset\n\ndata = [\n    {'conversations': {'role': 'user', 'content': 'hello'}},\n    {'conversations': {'role': 'user', 'content': 'hi', 'some_extra_field': 'some_value'}}\n]\ndf = pd.DataFrame(data)\ndf.to_parquet('data.parquet')\n\ndataset = load_dataset('parquet', data_files='data.parquet', split='train')\nprint(dataset.features)\n```\n\nThe schema can be derived. `some_extra_field` is set to None for the first row where it is absent.\n```\n{'conversations': {'content': Value(dtype='string', id=None), 'role': Value(dtype='string', id=None), 'some_extra_field': Value(dtype='string', id=None)}}\n```\n\nHowever, when I separate the samples into different files, it cannot be loaded.\n```python\nimport pandas as pd\nfrom datasets import load_dataset\n\ndata1 = [{'conversations': {'role': 'user', 'content': 'hello'}}]\npd.DataFrame(data1).to_parquet('data1.parquet')\ndata2 = [{'conversations': {'role': 'user', 'content': 'hi', 'some_extra_field': 'some_value'}}]\npd.DataFrame(data2).to_parquet('data2.parquet')\n\ndataset = load_dataset('parquet', data_files=['data1.parquet', 'data2.parquet'], split='train')\nprint(dataset.features)\n```\n\nTraceback:\n```\nTraceback (most recent call last):\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/builder.py\", line 1854, in _prepare_split_single\n    for _, table in generator:\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 106, in _generate_tables\n    yield f\"{file_idx}_{batch_idx}\", self._cast_table(pa_table)\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 73, in _cast_table\n    pa_table = table_cast(pa_table, self.info.features.arrow_schema)\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 2292, in table_cast\n    return cast_table_to_schema(table, schema)\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 2245, in cast_table_to_schema\n    arrays = [\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 2246, in <listcomp>\n    cast_array_to_feature(\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 1795, in wrapper\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 1795, in <listcomp>\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 2108, in cast_array_to_feature\n    raise TypeError(f\"Couldn't cast array of type\\n{_short_str(array.type)}\\nto\\n{_short_str(feature)}\")\nTypeError: Couldn't cast array of type\nstruct<content: string, role: string, some_extra_field: string>\nto\n{'content': Value(dtype='string', id=None), 'role': Value(dtype='string', id=None)}\n```\n\n\n### Expected behavior\n\nCorrectly load data with optional fields from different parquet files.\n\n### Environment info\n\n- `datasets` version: 3.3.2\n- Platform: Linux-5.10.135.bsk.4-amd64-x86_64-with-glibc2.31\n- Python version: 3.9.2\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 17.0.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2024.3.1",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! `load_dataset` expects all the data_files to have the same schema.\n\nMaybe you can try enforcing certain `features` using:\n\n```python\nfeatures = Features({\"conversations\": {'content': Value('string'), 'role': Value('string',)}})\nds = load_dataset(..., features=features)\n```"
      },
      {
        "user": "li-plus",
        "body": "Thanks! It works if I explicitly specify all nested fields of the data."
      }
    ]
  },
  {
    "issue_number": 6985,
    "title": "AttributeError: module 'pyarrow.lib' has no attribute 'ListViewType'",
    "author": "firmai",
    "state": "closed",
    "created_at": "2024-06-19T13:22:28Z",
    "updated_at": "2025-03-14T18:47:53Z",
    "labels": [],
    "body": "### Describe the bug\n\nI have been struggling with this for two days, any help would be appreciated. Python 3.10\r\n\r\n```\r\nfrom setfit import SetFitModel\r\nfrom huggingface_hub import login\r\n\r\naccess_token_read = \"cccxxxccc\"\r\n\r\n# Authenticate with the Hugging Face Hub\r\nlogin(token=access_token_read)\r\n\r\n# Load the models from the Hugging Face Hub\r\ntrainer_relv = SetFitModel.from_pretrained(\"snowdere/trainer_relevance\")\r\ntrainer_trust = SetFitModel.from_pretrained(\"snowdere/trainer_trust\")\r\ntrainer_sent = SetFitModel.from_pretrained(\"snowdere/trainer_sent\")\r\ntrainer_topic = SetFitModel.from_pretrained(\"snowdere/trainer_topic\")\r\n\r\n```\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[6], line 1\r\n----> 1 from setfit import SetFitModel\r\n      2 from huggingface_hub import login\r\n      4 access_token_read = \"ccsddsds\"\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/setfit/__init__.py:7\r\n      4 import os\r\n      5 import warnings\r\n----> 7 from .data import get_templated_dataset, sample_dataset\r\n      8 from .model_card import SetFitModelCardData\r\n      9 from .modeling import SetFitHead, SetFitModel\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/setfit/data.py:5\r\n      3 import pandas as pd\r\n      4 import torch\r\n----> 5 from datasets import Dataset, DatasetDict, load_dataset\r\n      6 from torch.utils.data import Dataset as TorchDataset\r\n      8 from . import logging\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/__init__.py:18\r\n      1 # ruff: noqa\r\n      2 # Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\r\n      3 #\r\n   (...)\r\n     13 # See the License for the specific language governing permissions and\r\n     14 # limitations under the License.\r\n     16 __version__ = \"2.19.0\"\r\n---> 18 from .arrow_dataset import Dataset\r\n     19 from .arrow_reader import ReadInstruction\r\n     20 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:76\r\n     73 from tqdm.contrib.concurrent import thread_map\r\n     75 from . import config\r\n---> 76 from .arrow_reader import ArrowReader\r\n     77 from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n     78 from .data_files import sanitize_patterns\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:29\r\n     26 from typing import TYPE_CHECKING, List, Optional, Union\r\n     28 import pyarrow as pa\r\n---> 29 import pyarrow.parquet as pq\r\n     30 from tqdm.contrib.concurrent import thread_map\r\n     32 from .download.download_config import DownloadConfig\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/parquet/__init__.py:20\r\n      1 # Licensed to the Apache Software Foundation (ASF) under one\r\n      2 # or more contributor license agreements.  See the NOTICE file\r\n      3 # distributed with this work for additional information\r\n   (...)\r\n     17 \r\n     18 # flake8: noqa\r\n---> 20 from .core import *\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/parquet/core.py:33\r\n     30 import pyarrow as pa\r\n     32 try:\r\n---> 33     import pyarrow._parquet as _parquet\r\n     34 except ImportError as exc:\r\n     35     raise ImportError(\r\n     36         \"The pyarrow installation is not built with support \"\r\n     37         f\"for the Parquet file format ({str(exc)})\"\r\n     38     ) from None\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/_parquet.pyx:1, in init pyarrow._parquet()\r\n\r\nAttributeError: module 'pyarrow.lib' has no attribute 'ListViewType'\r\n```\r\n\r\nsetfit: 1.0.3\r\ntransformers: 4.41.2\r\nlingua-language-detector: 2.0.2\r\npolars: 0.20.31\r\nlightning: None\r\ngoogle-cloud-bigquery: 3.24.0\r\nshapely: 2.0.4\r\npyarrow: 16.0.0\n\n### Steps to reproduce the bug\n\nI have tried all version combinations for Dataset and Pyarrow, the all have the same error since a few days ago. This is accross multiple scripts I have. \n\n### Expected behavior\n\nJust ron normally. \n\n### Environment info\n\n3.10",
    "comments": [
      {
        "user": "albertvillanova",
        "body": "Please note that the error is raised just at import:\r\n```python\r\nimport pyarrow.parquet as pq\r\n```\r\n\r\nTherefore it must be caused by some problem with your pyarrow installation. I would recommend you uninstall and install pyarrow again.\r\n\r\nI also see that it seems you use conda to install pyarrow. Please note that pyarrow offers 3 different packages in conda-forge: https://arrow.apache.org/docs/python/install.html#using-conda\r\n```\r\nconda install -c conda-forge pyarrow\r\n```\r\n> While the pyarrow [conda-forge](https://conda-forge.org/) package is the right choice for most users, both a minimal and maximal variant of the package exist, either of which may be better for your use case. See [Differences between conda-forge packages](https://arrow.apache.org/docs/python/install.html#python-conda-differences).\r\n\r\nPlease, make sure you install the right one: I guess it is either `pyarrow` (or `pyarrow-all`)."
      },
      {
        "user": "NicoNicoNico123",
        "body": "I have same issue, please downgrade pyarrow==15.0.2, it seem datasets library need to be fix"
      },
      {
        "user": "albertvillanova",
        "body": "It is not a problem with the `datasets` library: we support latest version of `pyarrow` and our Continuous Integration tests are using pyarrow 16.1.0 without any problem.\r\n\r\nThe error reported here is raised when importing pyarrow.parquet:\r\n```\r\n---> 29 import pyarrow.parquet as pq\r\n```\r\n```\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/parquet/__init__.py:20\r\n      1 # Licensed to the Apache Software Foundation (ASF) under one\r\n      2 # or more contributor license agreements.  See the NOTICE file\r\n      3 # distributed with this work for additional information\r\n   (...)\r\n     17 \r\n     18 # flake8: noqa\r\n---> 20 from .core import *\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/parquet/core.py:33\r\n     30 import pyarrow as pa\r\n     32 try:\r\n---> 33     import pyarrow._parquet as _parquet\r\n     34 except ImportError as exc:\r\n     35     raise ImportError(\r\n     36         \"The pyarrow installation is not built with support \"\r\n     37         f\"for the Parquet file format ({str(exc)})\"\r\n     38     ) from None\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/_parquet.pyx:1, in init pyarrow._parquet()\r\n\r\nAttributeError: module 'pyarrow.lib' has no attribute 'ListViewType'\r\n```\r\n\r\nThis can only be explained if pyarrow was not properly installed. \r\n\r\nIf the user just installed `pyarrow-core` from conda-forge, then its parquet subpackage is not installed and cannot be imported. You can check pyarrow docs:\r\n- Differences between conda-forge packages: https://arrow.apache.org/docs/python/install.html#python-conda-differences\r\n> The `pyarrow-core` package includes the following functionality:\r\n> ...\r\n> The `pyarrow` package adds the following:\r\n> ...\r\n> Parquet (i.e., `pyarrow.parquet`)"
      }
    ]
  },
  {
    "issue_number": 7447,
    "title": "Epochs shortened after resuming mid-epoch with Iterable dataset+StatefulDataloader(persistent_workers=True)",
    "author": "dhruvdcoder",
    "state": "closed",
    "created_at": "2025-03-12T21:41:05Z",
    "updated_at": "2025-03-14T17:26:59Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen `torchdata.stateful_dataloader.StatefulDataloader(persistent_workers=True)` the epochs after resuming only iterate through the examples that were left in the epoch when the training was interrupted. For example, in the script below training is interrupted on step 124 (epoch 1) when 3 batches are left. Then after resuming, the rest of epochs (2 and 3) only iterate through these 3 batches.\n\n### Steps to reproduce the bug\n\nRun the following script with and with PERSISTENT_WORKERS=true.\n\n```python\n# !/usr/bin/env python3\n# torch==2.5.1\n# datasets==3.3.2\n# torchdata>=0.9.0\nimport datasets\nimport pprint\nfrom torchdata.stateful_dataloader import StatefulDataLoader\n\nimport os\n\nPERSISTENT_WORKERS = (\n    os.environ.get(\"PERSISTENT_WORKERS\", \"False\").lower() == \"true\"\n)\n\n# PERSISTENT_WORKERS = True  # Incorrect resume\n\n\n# ds = datasets.load_from_disk(\"dataset\").to_iterable_dataset(num_shards=4)\ndef generator():\n    for i in range(128):\n        yield {\"x\": i}\n\n\nds = datasets.Dataset.from_generator(\n    generator, features=datasets.Features({\"x\": datasets.Value(\"int32\")})\n).to_iterable_dataset(num_shards=4)\n\ndl = StatefulDataLoader(\n    ds, batch_size=2, num_workers=2, persistent_workers=PERSISTENT_WORKERS\n)\nglobal_step = 0\nepoch = 0\nds_state_dict = None\nstate_dict = None\nresumed = False\nwhile True:\n    if epoch >= 3:\n        break\n    if state_dict is not None:\n        dl.load_state_dict(state_dict)\n        state_dict = None\n        ds_state_dict = None\n        resumed = True\n        print(\"resumed\")\n    for i, batch in enumerate(dl):\n        print(f\"epoch: {epoch}, global_step: {global_step}, batch: {batch}\")\n        global_step += 1  # consume datapoint\n        # simulate error\n        if global_step == 124 and not resumed:\n            ds_state_dict = ds.state_dict()\n            state_dict = dl.state_dict()\n            print(\"checkpoint\")\n            print(\"ds_state_dict\")\n            pprint.pprint(ds_state_dict)\n            print(\"dl_state_dict\")\n            pprint.pprint(state_dict)\n            break\n\n    if state_dict is None:\n        ds.set_epoch(epoch)\n        epoch += 1\n```\n\nThe script checkpoints when there are three batches left in the second epoch. After resuming, only the last three batches are repeated in the rest of the epochs.\n\nIf it helps, following are the two state_dicts for the dataloader save at the same step with the two settings. The left one is for `PERSISTENT_WORKERS=False`\n\n![Image](https://github.com/user-attachments/assets/c97d6502-d7bd-4ef4-ae2d-66fe1a9732b1)\n\n### Expected behavior\n\nAll the elements in the dataset should be iterated through in the epochs following the one where we resumed. The expected behavior can be seen by setting `PERSISTENT_WORKERS=False`. \n\n### Environment info\n\ntorch==2.5.1\ndatasets==3.3.2\ntorchdata>=0.9.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Thanks for reporting ! Maybe we should store the epoch in the state_dict, and then when the dataset is iterated on again after setting a new epoch it should restart from scratch instead of resuming ? wdyt ?"
      },
      {
        "user": "dhruvdcoder",
        "body": "But why does this only happen when `persistent_workers=True`? I would expect it to work correctly even without storing the epoch number in the state_dict of the iterable dataset. "
      },
      {
        "user": "lhoestq",
        "body": "I think persistent_workers=False simply ignores the dataset state_dict when it starts a new epoch, that's why the issue doesn't appear in that case"
      }
    ]
  },
  {
    "issue_number": 6290,
    "title": "Incremental dataset (e.g. `.push_to_hub(..., append=True)`)",
    "author": "Wauplin",
    "state": "open",
    "created_at": "2023-10-10T15:18:03Z",
    "updated_at": "2025-03-12T13:41:26Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nHave the possibility to do `ds.push_to_hub(..., append=True)`.\n\n### Motivation\n\nRequested in this [comment](https://huggingface.co/datasets/laion/dalle-3-dataset/discussions/3#65252597c4edc168202a5eaa) and \r\nthis [comment](https://huggingface.co/datasets/laion/dalle-3-dataset/discussions/4#6524f675c9607bdffb208d8f). Discussed internally on [slack](https://huggingface.slack.com/archives/C02EMARJ65P/p1696950642610639?thread_ts=1690554266.830949&cid=C02EMARJ65P).\r\n\n\n### Your contribution\n\nWhat I suggest to do for parquet datasets is to use `CommitOperationCopy` + `CommitOperationDelete` from `huggingface_hub`:\r\n1. list files\r\n2. copy files from parquet-0001-of-0004 to parquet-0001-of-0005\r\n3. delete files like parquet-0001-of-0004\r\n4. generate + add last parquet file parquet-0005-of-0005\r\n\r\n=> make a single commit with all commit operations at once\r\n\r\n\r\nI think it should be quite straightforward to implement. Happy to review a PR (maybe conflicting with the ongoing \"1 commit push_to_hub\" PR https://github.com/huggingface/datasets/pull/6269)",
    "comments": [
      {
        "user": "ZachNagengast",
        "body": "Yea I think waiting for #6269 would be best, or branching from it. For reference, this [PR](https://github.com/LAION-AI/Discord-Scrapers/pull/2) is progressing pretty well which will do similar using the hf hub for our LAION dataset bot https://github.com/LAION-AI/Discord-Scrapers/pull/2. "
      },
      {
        "user": "nqyy",
        "body": "Is there any update on this?"
      },
      {
        "user": "Elfsong",
        "body": "Is there any update on this?"
      }
    ]
  },
  {
    "issue_number": 7446,
    "title": "pyarrow.lib.ArrowTypeError: Expected dict key of type str or bytes, got 'int'",
    "author": "rangehow",
    "state": "open",
    "created_at": "2025-03-12T07:48:37Z",
    "updated_at": "2025-03-12T07:48:37Z",
    "labels": [],
    "body": "### Describe the bug\n\nA dict with its keys are all str but get following error\n```python\ntest_data=[{'input_ids':[1,2,3],'labels':[[Counter({2:1})]]}]\ndataset = datasets.Dataset.from_list(test_data)\n```\n```bash\npyarrow.lib.ArrowTypeError: Expected dict key of type str or bytes, got 'int'\n```\n\n### Steps to reproduce the bug\n\n.\n\n### Expected behavior\n\n.\n\n### Environment info\n\ndatasets 3.3.2",
    "comments": []
  },
  {
    "issue_number": 7443,
    "title": "index error when num_shards > len(dataset)",
    "author": "eminorhan",
    "state": "open",
    "created_at": "2025-03-10T22:40:59Z",
    "updated_at": "2025-03-10T23:43:08Z",
    "labels": [],
    "body": "In `ds.push_to_hub()` and `ds.save_to_disk()`, `num_shards` must be smaller than or equal to the number of rows in the dataset, but currently this is not checked anywhere inside these functions. Attempting to invoke these functions with `num_shards > len(dataset)` should raise an informative `ValueError`.\n\nI frequently work with datasets with a small number of rows where each row is pretty large, so I often encounter this issue, where the function runs until the shard index in `ds.shard(num_shards, indx)` goes out of bounds. Ideally, a `ValueError` should be raised before reaching this point (i.e. as soon as `ds.push_to_hub()` or `ds.save_to_disk()` is invoked with `num_shards > len(dataset)`).\n\nIt seems that adding something like:\n```python\nif len(self) < num_shards:\n   raise ValueError(f\"num_shards ({num_shards}) must be smaller than or equal to the number of rows in the dataset ({len(self)}). Please either reduce num_shards or increase max_shard_size to make sure num_shards <= len(dataset).\")\n```\nto the beginning of the definition of the `ds.shard()` function [here](https://github.com/huggingface/datasets/blob/f693f4e93aabafa878470c80fd42ddb10ec550d6/src/datasets/arrow_dataset.py#L4728) would deal with this issue for both `ds.push_to_hub()` and `ds.save_to_disk()`, but I'm not exactly sure if this is the best place to raise the `ValueError` (it seems that a more correct way to do it would be to write separate checks for `ds.push_to_hub()` and `ds.save_to_disk()`). I'd be happy to submit a PR if you think something along these lines would be acceptable.",
    "comments": [
      {
        "user": "eminorhan",
        "body": "Actually, looking at the code a bit more carefully, maybe an even better solution is to explicitly set `num_shards=len(self)` somewhere inside both `push_to_hub()` and `save_to_disk()` when these functions are invoked with `num_shards > len(dataset)`."
      }
    ]
  },
  {
    "issue_number": 7127,
    "title": "Caching shuffles by np.random.Generator results in unintiutive behavior",
    "author": "el-hult",
    "state": "open",
    "created_at": "2024-08-26T10:29:48Z",
    "updated_at": "2025-03-10T17:12:57Z",
    "labels": [],
    "body": "### Describe the bug\n\nCreate a dataset. Save it to disk. Load from disk. Shuffle, usning a `np.random.Generator`. Iterate. Shuffle again. Iterate. The iterates are different since the supplied np.random.Generator has progressed between the shuffles.\r\n\r\nLoad dataset from disk again. Shuffle and Iterate. See same result as before. Shuffle and iterate, and this time it does not have the same shuffling as ion previous run.\r\n\r\nThe motivation is I have a deep learning loop with \r\n\r\n```\r\nfor epoch in range(10):\r\n    for batch in dataset.shuffle(generator=generator).iter(batch_size=32):\r\n        .... # do stuff\r\n```\r\nwhere I want a new shuffling at every epoch. Instead I get the same shuffling.\n\n### Steps to reproduce the bug\n\nRun the code below two times. \r\n\r\n```python\r\nimport datasets\r\nimport numpy as np\r\n\r\ngenerator = np.random.default_rng(0)\r\nds = datasets.Dataset.from_dict(mapping={\"X\":range(1000)})\r\nds.save_to_disk(\"tmp\")\r\nprint(\"First loop: \", end=\"\")\r\nfor _ in range(10):\r\n    print(next(ds.shuffle(generator=generator).iter(batch_size=1))['X'], end=\", \")\r\nprint(\"\")\r\n\r\nprint(\"Second loop: \", end=\"\")\r\nds = datasets.Dataset.load_from_disk(\"tmp\")\r\nfor _ in range(10):\r\n    print(next(ds.shuffle(generator=generator).iter(batch_size=1))['X'], end=\", \")\r\nprint(\"\")\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\n$ python main.py \r\nSaving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 495019.95 examples/s]\r\nFirst loop: 459, 739, 72, 943, 241, 181, 845, 830, 896, 334, \r\nSecond loop: 741, 847, 944, 795, 483, 842, 717, 865, 231, 840,\r\n$ python main.py \r\nSaving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 22243.40 examples/s]\r\nFirst loop: 459, 739, 72, 943, 241, 181, 845, 830, 896, 334, \r\nSecond loop: 741, 741, 741, 741, 741, 741, 741, 741, 741, 741, \r\n```\r\n\r\nThe second loop, on the second run, only spits out \"741, 741, 741....\" which is *not* the desired output\n\n### Expected behavior\n\nI want the dataset to shuffle at every epoch since I provide it with a generator for shuffling. \n\n### Environment info\n\nDatasets version 2.21.0\r\nUbuntu linux.",
    "comments": [
      {
        "user": "el-hult",
        "body": "I first thought this was a mistake of mine, and also posted on stack overflow. https://stackoverflow.com/questions/78913797/iterating-a-huggingface-dataset-from-disk-using-generator-seems-broken-how-to-d \r\n\r\nIt seems to me the issue is the caching step in \r\n\r\nhttps://github.com/huggingface/datasets/blob/be5cff059a2a5b89d7a97bc04739c4919ab8089f/src/datasets/arrow_dataset.py#L4306-L4316\r\n\r\nbecause the shuffle happens after checking the cache, the rng state won't advance if the cache is used. This is VERY confusing. Also not documented.\r\n\r\nMy proposal is that you remove the API for using a Generator, and only keep the seed-based API since that is functional and cache-compatible."
      }
    ]
  },
  {
    "issue_number": 7441,
    "title": "`drop_last_batch` does not drop the last batch using IterableDataset + interleave_datasets + multi_worker",
    "author": "memray",
    "state": "open",
    "created_at": "2025-03-08T10:28:44Z",
    "updated_at": "2025-03-09T21:27:33Z",
    "labels": [],
    "body": "### Describe the bug\n\nSee the script below\n`drop_last_batch=True` is defined using map() for each dataset. \nThe last batch for each dataset is expected to be dropped, id 21-25.\nThe code behaves as expected when num_workers=0 or 1.\nWhen using num_workers>1, 'a-11', 'b-11', 'a-12', 'b-12' are gone and instead 21 and 22 are sampled.\n\n### Steps to reproduce the bug\n\n```\nfrom datasets import Dataset\nfrom datasets import interleave_datasets\nfrom torch.utils.data import DataLoader\n\ndef convert_to_str(batch, dataset_name):\n    batch['a'] = [f\"{dataset_name}-{e}\" for e in batch['a']]\n    return batch\n\ndef gen1():\n    for ii in range(1, 25):\n        yield {\"a\": ii}\n\ndef gen2():\n    for ii in range(1, 25):\n        yield {\"a\": ii}\n\n# https://github.com/huggingface/datasets/issues/6565\nif __name__ == '__main__':\n    dataset1 = Dataset.from_generator(gen1).to_iterable_dataset(num_shards=2)\n    dataset2 = Dataset.from_generator(gen2).to_iterable_dataset(num_shards=2)\n    dataset1 = dataset1.map(lambda x: convert_to_str(x, dataset_name=\"a\"), batched=True, batch_size=10, drop_last_batch=True)\n    dataset2 = dataset2.map(lambda x: convert_to_str(x, dataset_name=\"b\"), batched=True, batch_size=10, drop_last_batch=True)\n\n    interleaved = interleave_datasets([dataset1, dataset2], stopping_strategy=\"all_exhausted\")\n\n    print(f\"num_workers=0\")\n    loader = DataLoader(interleaved, batch_size=5, num_workers=0)\n    i = 0\n    for b in loader:\n        print(i, b['a'])\n        i += 1\n\n    print('=-' * 20)\n    print(f\"num_workers=1\")\n    loader = DataLoader(interleaved, batch_size=5, num_workers=1)\n    i = 0\n    for b in loader:\n        print(i, b['a'])\n        i += 1\n\n    print('=-' * 20)\n    print(f\"num_workers=2\")\n    loader = DataLoader(interleaved, batch_size=5, num_workers=2)\n    i = 0\n    for b in loader:\n        print(i, b['a'])\n        i += 1\n\n    print('=-' * 20)\n    print(f\"num_workers=3\")\n    loader = DataLoader(interleaved, batch_size=5, num_workers=3)\n    i = 0\n    for b in loader:\n        print(i, b['a'])\n        i += 1\n```\n\n\noutput is:\n```\nnum_workers=0\n0 ['a-1', 'b-1', 'a-2', 'b-2', 'a-3']\n1 ['b-3', 'a-4', 'b-4', 'a-5', 'b-5']\n2 ['a-6', 'b-6', 'a-7', 'b-7', 'a-8']\n3 ['b-8', 'a-9', 'b-9', 'a-10', 'b-10']\n4 ['a-11', 'b-11', 'a-12', 'b-12', 'a-13']\n5 ['b-13', 'a-14', 'b-14', 'a-15', 'b-15']\n6 ['a-16', 'b-16', 'a-17', 'b-17', 'a-18']\n7 ['b-18', 'a-19', 'b-19', 'a-20', 'b-20']\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nnum_workers=1\n0 ['a-1', 'b-1', 'a-2', 'b-2', 'a-3']\n1 ['b-3', 'a-4', 'b-4', 'a-5', 'b-5']\n2 ['a-6', 'b-6', 'a-7', 'b-7', 'a-8']\n3 ['b-8', 'a-9', 'b-9', 'a-10', 'b-10']\n4 ['a-11', 'b-11', 'a-12', 'b-12', 'a-13']\n5 ['b-13', 'a-14', 'b-14', 'a-15', 'b-15']\n6 ['a-16', 'b-16', 'a-17', 'b-17', 'a-18']\n7 ['b-18', 'a-19', 'b-19', 'a-20', 'b-20']\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nnum_workers=2\n0 ['a-1', 'b-1', 'a-2', 'b-2', 'a-3']\n1 ['a-13', 'b-13', 'a-14', 'b-14', 'a-15']\n2 ['b-3', 'a-4', 'b-4', 'a-5', 'b-5']\n3 ['b-15', 'a-16', 'b-16', 'a-17', 'b-17']\n4 ['a-6', 'b-6', 'a-7', 'b-7', 'a-8']\n5 ['a-18', 'b-18', 'a-19', 'b-19', 'a-20']\n6 ['b-8', 'a-9', 'b-9', 'a-10', 'b-10']\n7 ['b-20', 'a-21', 'b-21', 'a-22', 'b-22']\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nnum_workers=3\nToo many dataloader workers: 3 (max is dataset.num_shards=2). Stopping 1 dataloader workers.\n0 ['a-1', 'b-1', 'a-2', 'b-2', 'a-3']\n1 ['a-13', 'b-13', 'a-14', 'b-14', 'a-15']\n2 ['b-3', 'a-4', 'b-4', 'a-5', 'b-5']\n3 ['b-15', 'a-16', 'b-16', 'a-17', 'b-17']\n4 ['a-6', 'b-6', 'a-7', 'b-7', 'a-8']\n5 ['a-18', 'b-18', 'a-19', 'b-19', 'a-20']\n6 ['b-8', 'a-9', 'b-9', 'a-10', 'b-10']\n7 ['b-20', 'a-21', 'b-21', 'a-22', 'b-22']\n\n\n```\n\n### Expected behavior\n\n`'a-21', 'b-21', 'a-22', 'b-22'` should be dropped\n\n### Environment info\n\n- `datasets` version: 3.3.2\n- Platform: Linux-5.15.0-1056-aws-x86_64-with-glibc2.31\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.28.0\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.6.1\n",
    "comments": [
      {
        "user": "ItsVicky25",
        "body": "Hi @memray, I’d like to help fix the issue with `drop_last_batch` not working when `num_workers > 1`. I’ll investigate and propose a solution. Thanks!\n"
      },
      {
        "user": "memray",
        "body": "Thank you very much for offering to help! I also noticed a problem related to a previous issue and left a comment [here](https://github.com/huggingface/datasets/issues/6565#issuecomment-2708169303) (the code checks the validity before certain columns removed). Can you take a look as well?"
      }
    ]
  },
  {
    "issue_number": 6359,
    "title": "Stuck in \"Resolving data files...\"",
    "author": "Luciennnnnnn",
    "state": "open",
    "created_at": "2023-10-27T12:01:51Z",
    "updated_at": "2025-03-09T02:18:19Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nI have an image dataset with 300k images, the size of image is 768 * 768.\r\n\r\nWhen I run `dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/img_dir\", split='train')` in second time, it takes 50 minutes to finish \"Resolving data files\" part, what's going on in this part?\r\n\r\nFrom my understand, after Arrow files been created in the first run, the second run should not take time longer than one or two minutes.\r\n\r\n### Steps to reproduce the bug\r\n\r\n# Run following code two times\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/img_dir\", split='train')\r\n\r\n### Expected behavior\r\n\r\nFast dataset building\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.0-60-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3",
    "comments": [
      {
        "user": "mariosasko",
        "body": "Most likely, the data file inference logic is the problem here.\r\n\r\nYou can run the following code to verify this:\r\n```python\r\nimport time\r\nfrom datasets.data_files import get_data_patterns\r\nstart_time = time.time()\r\nget_data_patterns(\"/path/to/img_dir\")\r\nend_time = time.time()\r\nprint(f\"Elapsed time: {end_time - start_time:.2f}s\")\r\n```\r\n \r\nWe plan to optimize this for the next version (or version after that). In the meantime, specifying the split patterns manually should give better performance:\r\n```python\r\nds = load_dataset(\"imagefolder\", data_files={\"train\": \"path/to/img_dir/train/**\", ...}, split=\"train\")\r\n```"
      },
      {
        "user": "Luciennnnnnn",
        "body": "Hi, @mariosasko, you are right; data file inference logic is extremely slow.\r\n\r\nI have done a similar test, that is I modify the source code of datasets/load.py to measure the cost of two suspicious operations:\r\n```python\r\ndef get_module(self) -> DatasetModule:\r\n        base_path = Path(self.data_dir or \"\").expanduser().resolve().as_posix()\r\n        start = time.time()\r\n        patterns = sanitize_patterns(self.data_files) if self.data_files is not None else get_data_patterns(base_path)\r\n        print(f\"patterns: {time.time() - start}\")\r\n        start = time.time()\r\n        data_files = DataFilesDict.from_patterns(\r\n            patterns,\r\n            download_config=self.download_config,\r\n            base_path=base_path,\r\n        )\r\n        print(f\"data_files: {time.time() - start}\")\r\n```\r\nIt gaves:\r\npatterns: 3062.2050700187683\r\ndata_files: 413.9576675891876\r\n\r\nThus, these two operations contribute to almost all of load time. What's going on in them?"
      },
      {
        "user": "Luciennnnnnn",
        "body": "Furthermore, what's my current workaround about this problem? Should I save it by `save_to_disk()` and load dataset through `load_from_disk`?"
      }
    ]
  },
  {
    "issue_number": 7357,
    "title": "Python process aborded with GIL issue when using image dataset",
    "author": "AlexKoff88",
    "state": "open",
    "created_at": "2025-01-06T11:29:30Z",
    "updated_at": "2025-03-08T15:59:36Z",
    "labels": [],
    "body": "### Describe the bug\n\nThe issue is visible only with the latest `datasets==3.2.0`.\r\n\r\nWhen using image dataset the Python process gets aborted right before the exit with the following error:\r\n```\r\nFatal Python error: PyGILState_Release: thread state 0x7fa1f409ade0 must be current when releasing\r\nPython runtime state: finalizing (tstate=0x0000000000ad2958)\r\n\r\nThread 0x00007fa33d157740 (most recent call first):\r\n  <no Python frame>\r\n\r\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._boun\r\nded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.ts\r\nlibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.t\r\nslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._l\r\nibs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pan\r\ndas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join,\r\n pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, charset_normalizer.md, requests.pa\r\nckages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, markupsafe._speedups, PIL._imaging, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards\r\n, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, sentencepiece._sentencepiece, sklearn.__check_build._check_build, psutil._psut\r\nil_linux, psutil._psutil_posix, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.l\r\ninalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_up\r\ndate, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack,\r\n scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flo\r\nw, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial\r\n._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._group_columns, s\r\ncipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, sc\r\nipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.l\r\ninalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integr\r\nate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._r\r\ngi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._ansari_swilk_statis\r\ntics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, sklearn.utils._isf\r\ninite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.p\r\nreprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._bas\r\ne, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distanc\r\nes_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, s\r\nklearn.metrics._pairwise_fast, PIL._imagingft, google._upb._message, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv,\r\nh5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, _cffi_backend, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs\r\n, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, propcache._helpers_c, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, xxhash\r\n._xxhash, pyarrow._json, pyarrow._acero, pyarrow._csv, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, regex._regex, scipy\r\n.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, PIL._imagingmath, PIL._webp (total: 236)\r\nAborted (core dumped)\r\n```an \n\n### Steps to reproduce the bug\n\nInstall `datasets==3.2.0`\r\n\r\nRun the following script:\r\n```python\r\nimport datasets\r\n\r\n\r\nDATASET_NAME = \"phiyodr/InpaintCOCO\"\r\nNUM_SAMPLES = 10\r\n\r\n\r\ndef preprocess_fn(example):\r\n    return {\r\n        \"prompts\": example[\"inpaint_caption\"],\r\n        \"images\": example[\"coco_image\"],\r\n        \"masks\": example[\"mask\"],\r\n    }\r\n\r\n\r\ndefault_dataset = datasets.load_dataset(\r\n    DATASET_NAME, split=\"test\", streaming=True\r\n).filter(lambda example: example[\"inpaint_caption\"] != \"\").take(NUM_SAMPLES)\r\ntest_data = default_dataset.map(\r\n    lambda x: preprocess_fn(x), remove_columns=default_dataset.column_names\r\n)\r\n\r\nfor data in test_data:\r\n    print(data[\"prompts\"])\r\n\r\n``\n\n### Expected behavior\n\nThe script should not hang or crash.\n\n### Environment info\n\n- `datasets` version: 3.2.0\r\n- Platform: Linux-5.15.0-50-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.0\r\n- `huggingface_hub` version: 0.25.1\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.2.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "The issue seems to come from `pyarrow`, I opened an issue on their side at https://github.com/apache/arrow/issues/45214"
      }
    ]
  },
  {
    "issue_number": 6565,
    "title": " `drop_last_batch=True` for IterableDataset map function is ignored with multiprocessing DataLoader ",
    "author": "naba89",
    "state": "closed",
    "created_at": "2024-01-07T02:46:50Z",
    "updated_at": "2025-03-08T09:46:05Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nScenario:\r\n- Interleaving two iterable datasets of unequal lengths (`all_exhausted`), followed by a batch mapping with batch size 2 to effectively merge the two datasets and get a sample from each dataset in a single batch, with `drop_last_batch=True` to skip the last batch in case it doesn't have two samples.\r\n\r\nWhat works:\r\n- Using DataLoader with `num_workers=0`\r\n\r\nWhat does not work:\r\n- Using DataLoader with `num_workers=1`, errors in the last batch.\r\n\r\nBasically, `drop_last_batch=True` is ignored when using multiple dataloading workers.\r\n\r\nPlease take a look at the minimal repro script below.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import Dataset, interleave_datasets\r\nfrom torch.utils.data import DataLoader\r\n\r\n\r\ndef merge_samples(batch):\r\n    assert len(batch['a']) == 2, \"Batch size must be 2\"\r\n    batch['c'] = [batch['a'][0]]\r\n    batch['d'] = [batch['a'][1]]\r\n    return batch\r\n\r\n\r\ndef gen1():\r\n    for ii in range(1, 8385):\r\n        yield {\"a\": ii}\r\n\r\n\r\ndef gen2():\r\n    for ii in range(1, 5302):\r\n        yield {\"a\": ii}\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    dataset1 = Dataset.from_generator(gen1).to_iterable_dataset(num_shards=1024)\r\n    dataset2 = Dataset.from_generator(gen2).to_iterable_dataset(num_shards=1024)\r\n\r\n    interleaved = interleave_datasets([dataset1, dataset2], stopping_strategy=\"all_exhausted\")\r\n    mapped = interleaved.map(merge_samples, batched=True, batch_size=2, remove_columns=interleaved.column_names,\r\n                             drop_last_batch=True)\r\n\r\n    # Works\r\n    loader = DataLoader(mapped, batch_size=32, num_workers=0)\r\n    i = 0\r\n    for b in loader:\r\n        print(i, b['c'].shape, b['d'].shape)\r\n        i += 1\r\n\r\n    print(\"DataLoader with num_workers=0 works\")\r\n\r\n    # Doesn't work\r\n    loader = DataLoader(mapped, batch_size=32, num_workers=1)\r\n    i = 0\r\n    for b in loader:\r\n        print(i, b['c'].shape, b['d'].shape)\r\n        i += 1\r\n\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\n `drop_last_batch=True` should have same behaviour for `num_workers=0` and `num_workers>=1`\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.16.1\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.6.0\r\n\r\nI have also tested on Linux and got the same behavior.",
    "comments": [
      {
        "user": "naba89",
        "body": "My current workaround this issue is to return `None` in the second element and then filter out samples which have `None` in  them.\r\n\r\n```python\r\ndef merge_samples(batch):\r\n    if len(batch['a']) == 1:\r\n        batch['c'] = [batch['a'][0]]\r\n        batch['d'] = [None]\r\n    else:\r\n        batch['c'] = [batch['a'][0]]\r\n        batch['d'] = [batch['a'][1]]\r\n    return batch\r\n    \r\ndef filter_fn(x):\r\n    return x['d'] is not None\r\n\r\n# other code...\r\nmapped = mapped.filter(filter_fn)\r\n```"
      },
      {
        "user": "memray",
        "body": "Hi @lhoestq , I found that this script no longer works due to this [line](https://github.com/huggingface/datasets/blob/f693f4e93aabafa878470c80fd42ddb10ec550d6/src/datasets/iterable_dataset.py#L1141), where it checks the validity before the column `a` is removed. Is this expected?"
      }
    ]
  },
  {
    "issue_number": 5207,
    "title": "Connection error of the HuggingFace's dataset Hub due to SSLError with proxy",
    "author": "leemgs",
    "state": "open",
    "created_at": "2022-11-07T06:56:23Z",
    "updated_at": "2025-03-08T09:04:10Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nIt's weird. I could not normally  connect  the dataset Hub  of  HuggingFace due to a SSLError in my office. \r\nEven when I try to connect using my company's proxy address (e.g., http_proxy and https_proxy), \r\nI'm getting the  SSLError issue. What should I do to download the datanet stored in HuggingFace normally?\r\nI welcome any comments. I think those comments will be helpful to me.\r\n\r\n* Dataset address - https://huggingface.co/datasets/moyix/debian_csrc/viewer/moyix--debian_csrc\r\n* Log message\r\n```\r\n      ............ OMISSION ..............\r\nTraceback (most recent call last):\r\n  File \"/data/home/geunsik-lim/qtlab/./transformers/examples/pytorch/language-modeling/run_clm.py\", line 587, in <module>\r\n    main()\r\n  File \"/data/home/geunsik-lim/qtlab/./transformers/examples/pytorch/language-modeling/run_clm.py\", line 278, in main\r\n    raw_datasets = load_dataset(\r\n  File \"/home/geunsik-lim/anaconda3/envs/deepspeed/lib/python3.10/site-packages/datasets/load.py\", line 1719, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/geunsik-lim/anaconda3/envs/deepspeed/lib/python3.10/site-packages/datasets/load.py\", line 1497, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/geunsik-lim/anaconda3/envs/deepspeed/lib/python3.10/site-packages/datasets/load.py\", line 1222, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/home/geunsik-lim/anaconda3/envs/deepspeed/lib/python3.10/site-packages/datasets/load.py\", line 1179, in dataset_module_factory\r\n    raise ConnectionError(f\"Couldn't reach '{path}' on the Hub ({type(e).__name__})\")\r\nConnectionError: Couldn't reach 'moyix/debian_csrc' on the Hub (SSLError)\r\n[2022-11-07 15:23:38,476] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 6760\r\n[2022-11-07 15:23:38,476] [ERROR] [launch.py:324:sigkill_handler] ['/home/geunsik-lim/anaconda3/envs/deepspeed/bin/python', '-u', './transformers/examples/pytorch/language-modeling/run_clm.py', '--local_rank=0', '--model_name_or_path=Salesforce/codegen-350M-multi', '--per_device_train_batch_size=1', '--learning_rate', '2e-5', '--num_train_epochs', '1', '--output_dir=./codegen-350M-finetuned', '--overwrite_output_dir', '--dataset_name', 'moyix/debian_csrc', '--cache_dir', '/data/home/geunsik-lim/.cache', '--tokenizer_name', 'Salesforce/codegen-350M-multi', '--block_size', '2048', '--gradient_accumulation_steps', '32', '--do_train', '--fp16', '--deepspeed', 'ds_config_zero2.json'] exits with return code = 1\r\n\r\nreal    0m7.742s\r\nuser    0m4.930s\r\n\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nSteps to reproduce this behavior. \r\n\r\n\r\n\r\n```\r\n(deepspeed) geunsik-lim@ai02:~/qtlab$ ./test_debian_csrc_dataset.py\r\nTraceback (most recent call last):\r\n  File \"/data/home/geunsik-lim/qtlab/./test_debian_csrc_dataset.py\", line 6, in <module>\r\n    dataset = load_dataset(\"moyix/debian_csrc\")\r\n  File \"/home/geunsik-lim/anaconda3/envs/deepspeed/lib/python3.10/site-packages/datasets/load.py\", line 1719, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/geunsik-lim/anaconda3/envs/deepspeed/lib/python3.10/site-packages/datasets/load.py\", line 1497, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/geunsik-lim/anaconda3/envs/deepspeed/lib/python3.10/site-packages/datasets/load.py\", line 1222, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/home/geunsik-lim/anaconda3/envs/deepspeed/lib/python3.10/site-packages/datasets/load.py\", line 1179, in dataset_module_factory\r\n    raise ConnectionError(f\"Couldn't reach '{path}' on the Hub ({type(e).__name__})\")\r\nConnectionError: Couldn't reach 'moyix/debian_csrc' on the Hub (SSLError)\r\n(deepspeed) geunsik-lim@ai02:~/qtlab$\r\n(deepspeed) geunsik-lim@ai02:~/qtlab$\r\n(deepspeed) geunsik-lim@ai02:~/qtlab$\r\n(deepspeed) geunsik-lim@ai02:~/qtlab$ cat ./test_debian_csrc_dataset.py\r\n#!/usr/bin/env python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"moyix/debian_csrc\")\r\n\r\n```\r\n\r\n1.  Adde proxy address of a company in /etc/profile\r\n2.  Download dataset with load_dataset() function of  datasets package that is provided by HuggingFace.\r\n3.  In this case, the address would be \"moyix--debian_csrc\".\r\n4.  I get the \"`ConnectionError: Couldn't reach 'moyix/debian_csrc' on the Hub (SSLError`)\" error message.\r\n\r\n### Expected behavior\r\n\r\n\r\n* error message:\r\nConnectionError: Couldn't reach 'moyix/debian_csrc' on the Hub (SSLError)\r\n\r\n### Environment info\r\n\r\n\r\n* software version information:\r\n\r\n```\r\n(deepspeed) geunsik-lim@ai02:~$\r\n(deepspeed) geunsik-lim@ai02:~$ conda list -f pytorch\r\n# packages in environment at /home/geunsik-lim/anaconda3/envs/deepspeed:\r\n#\r\n# Name                    Version                   Build  Channel\r\npytorch                   1.13.0          py3.10_cuda11.7_cudnn8.5.0_0    pytorch\r\n(deepspeed) geunsik-lim@ai02:~$ conda list -f python\r\n# packages in environment at /home/geunsik-lim/anaconda3/envs/deepspeed:\r\n#\r\n# Name                    Version                   Build  Channel\r\npython                    3.10.6               haa1d7c7_1\r\n(deepspeed) geunsik-lim@ai02:~$ conda list -f datasets\r\n# packages in environment at /home/geunsik-lim/anaconda3/envs/deepspeed:\r\n#\r\n# Name                    Version                   Build  Channel\r\ndatasets                  2.6.1                      py_0    huggingface\r\n(deepspeed) geunsik-lim@ai02:~$ uname -a\r\nLinux ai02 5.4.0-131-generic #147-Ubuntu SMP Fri Oct 14 17:07:22 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n(deepspeed) geunsik-lim@ai02:~$ cat /etc/lsb-release\r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=20.04\r\nDISTRIB_CODENAME=focal\r\nDISTRIB_DESCRIPTION=\"Ubuntu 20.04.5 LTS\"\r\n\r\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! It looks like an issue with your python environment, can you make sure you're able to run GET requests to https://huggingface.co using `requests` in python ?"
      },
      {
        "user": "leemgs",
        "body": "Thanks for your reply. Does this mean that I have to use the `do_dataset `function and the `requests `function to download the dataset from the company's proxy environment?\r\n\r\n\r\n* Reference: \r\n```bash\r\n### How to load this dataset directly with the [datasets](https://github.com/huggingface/datasets) library\r\n\r\n\r\n* https://huggingface.co/datasets/moyix/debian_csrc\r\n\r\n* from datasets import load_dataset\r\ndataset = load_dataset(\"moyix/debian_csrc\")\r\n\r\n\r\n\r\n### Or just clone the dataset repo\r\n\r\n\r\ngit lfs install\r\ngit clone https://huggingface.co/datasets/moyix/debian_csrc\r\n# if you want to clone without large files – just their pointers\r\n# prepend your git clone with the following env var:\r\nGIT_LFS_SKIP_SMUDGE=1\r\n```"
      },
      {
        "user": "lhoestq",
        "body": "You can use `requests` to see if downloading a file from the Hugging Face Hub works. If so, then `datasets` should work as well. If not, then you have to find another way using an internet connection that works"
      }
    ]
  },
  {
    "issue_number": 7387,
    "title": "Dynamic adjusting dataloader sampling weight",
    "author": "whc688",
    "state": "open",
    "created_at": "2025-02-10T03:18:47Z",
    "updated_at": "2025-03-07T14:06:54Z",
    "labels": [],
    "body": "Hi,\nThanks for your wonderful work! I'm wondering is there a way to dynamically adjust the sampling weight of each data in the dataset during training? Looking forward to your reply, thanks again.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "You mean based on a condition that has to be checked on-the-fly during training ? Otherwise if you know in advance after how many samples you need to change the sampling you can simply concatenate the two mixes"
      },
      {
        "user": "whc688",
        "body": "Yes, like during training, if one data sample's prediction is consistently wrong, its sampling weight gets higher and higher, and if one data sample's prediction is already correct, then we rarely sample it"
      },
      {
        "user": "lhoestq",
        "body": "it's not possible to use `interleave_datasets()` and modify the probabilities while iterating on the dataset at the moment, so you'd have to implement your own torch `Sampler` or your own`IterableDataset` to implement this logic"
      }
    ]
  },
  {
    "issue_number": 7149,
    "title": "Datasets Unknown Keyword Argument Error - task_templates",
    "author": "varungupta31",
    "state": "closed",
    "created_at": "2024-09-13T10:30:57Z",
    "updated_at": "2025-03-06T07:11:55Z",
    "labels": [],
    "body": "### Describe the bug\n\nIssue\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nexamples = load_dataset('facebook/winoground', use_auth_token=<YOUR USER ACCESS TOKEN>)\r\n```\r\n\r\nGives error\r\n\r\n```\r\nTypeError: DatasetInfo.__init__() got an unexpected keyword argument 'task_templates'\r\n```\r\n\r\nA simple downgrade to lower `datasets v 2.21.0` solves it.\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. `pip install datsets`\r\n2.\r\n```python\r\nfrom datasets import load_dataset\r\nexamples = load_dataset('facebook/winoground', use_auth_token=<YOUR USER ACCESS TOKEN>)\r\n```\r\n\r\n\n\n### Expected behavior\n\nShould load the dataset correctly.\n\n### Environment info\n\n- Datasets version  `3.0.0`\r\n- `transformers` version: 4.45.0.dev0\r\n- Platform: Linux-6.8.0-40-generic-x86_64-with-glibc2.35\r\n- Python version: 3.12.4\r\n- Huggingface_hub version: 0.24.6\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 0.35.0.dev0\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.4.1+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: Yes\r\n",
    "comments": [
      {
        "user": "albertvillanova",
        "body": "Thanks, for reporting.\r\n\r\nWe have been fixing most Hub datasets to remove the deprecated (and now non-supported) task templates, but we missed the \"facebook/winoground\".\r\n\r\nIt is fixed now: https://huggingface.co/datasets/facebook/winoground/discussions/8\r\n\r\n"
      },
      {
        "user": "laleye",
        "body": "Hello @albertvillanova \r\n\r\nI got the same error while loading this dataset: https://huggingface.co/datasets/alaleye/aloresb...\r\n\r\nHow can I fix it ? \r\nThanks"
      },
      {
        "user": "i4seeu",
        "body": "I am getting the same error on the below code, any fix to this ?\n\n```\nfrom datasets import load_dataset\n\nminds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\nminds\n```"
      }
    ]
  },
  {
    "issue_number": 7430,
    "title": "Error in code \"Time to slice and dice\" from course \"NLP Course\"",
    "author": "Yurkmez",
    "state": "closed",
    "created_at": "2025-02-28T11:36:10Z",
    "updated_at": "2025-03-05T11:32:47Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen we execute code \n```\nfrequencies = (\n    train_df[\"condition\"]\n    .value_counts()\n    .to_frame()\n    .reset_index()\n    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n)\nfrequencies.head()\n```\nanswer should be like this\n\ncondition      | frequency\nbirth control | 27655\ndepression    | 8023\nacne              | 5209\nanxiety          | 4991\npain               | 4744\n\nbut he is different\n\nfrequency      | count\nbirth control  | 27655\ndepression    | 8023\nacne              | 5209\nanxiety          | 4991\npain               | 4744\n\nthis is not correct, correct code\n```\nfrequencies = (\n    train_df[\"condition\"]\n    .value_counts()\n    .to_frame()\n    .reset_index()\n    .rename(columns={\"index\": \"condition\", \"count\": \"frequency\"})\n)\n````\n\n### Steps to reproduce the bug\n\n```\nfrequencies = (\ntrain_df[\"condition\"]\n.value_counts()\n.to_frame()\n.reset_index()\n.rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n)\nfrequencies.head()\n```\n\n### Expected behavior\n\n\ncondition      | frequency\nbirth control | 27655\ndepression    | 8023\nacne              | 5209\nanxiety          | 4991\npain               | 4744\n\n### Environment info\n\nGoogle Colab",
    "comments": [
      {
        "user": "lhoestq",
        "body": "You should open an issue in the NLP course website / github page. I'm closing this issue if you don't mind"
      },
      {
        "user": "Yurkmez",
        "body": "ok, i don't mind, i'll mark the error there"
      }
    ]
  },
  {
    "issue_number": 7431,
    "title": "Issues with large Datasets",
    "author": "nikitabelooussovbtis",
    "state": "open",
    "created_at": "2025-02-28T14:05:22Z",
    "updated_at": "2025-03-04T15:02:26Z",
    "labels": [],
    "body": "### Describe the bug\n\nIf the coco annotation file is too large the dataset will not be able to load it, not entirely sure were the issue is but I am guessing it is due to the code trying to load it all as one line into a dataframe. This was for object detections.\n\nMy current work around is the following code but would be nice to be able to do it without worrying about it also probably there is a better way of doing it:\n`\n            dataset_dict = json.load(open(\"./local_data/annotations/train.json\"))\n            df = pd.DataFrame(columns=['images', 'annotations', 'categories'])\n            df = df._append({'images': dataset_dict['images'], 'annotations': dataset_dict['annotations'], 'categories': dataset_dict['categories']}, ignore_index=True)\n            train=Dataset.from_pandas(df)\n\n            dataset_dict = json.load(open(\"./local_data/annotations/validation.json\"))\n            df = pd.DataFrame(columns=['images', 'annotations', 'categories'])\n            df = df._append({'images': dataset_dict['images'], 'annotations': dataset_dict['annotations'],\n                             'categories': dataset_dict['categories']}, ignore_index=True)\n            val = Dataset.from_pandas(df)\n            dataset_dict = json.load(open(\"./local_data/annotations/test.json\"))\n            df = pd.DataFrame(columns=['images', 'annotations', 'categories'])\n            df = df._append({'images': dataset_dict['images'], 'annotations': dataset_dict['annotations'],\n                             'categories': dataset_dict['categories']}, ignore_index=True)\n            test = Dataset.from_pandas(df)\n            dataset = DatasetDict({'train': train, 'validation': val, 'test': test})\n`\n\n### Steps to reproduce the bug\n\n1) step up directory in and have the json files in coco format\n\n-local_data\n|-images\n|---1.jpg\n|---2.jpg\n|---....\n|---n.jpg\n|-annotations\n|---test.json\n|---train.json\n|---validation.json\n\n2) try to load local_data into a dataset if the file is larger than about 300kb it will cause an error.\n\n### Expected behavior\n\nThat it loads the jsons preferably in the same format as it has done with a smaller size.\n\n### Environment info\n\n- `datasets` version: 3.3.3.dev0\n- Platform: Linux-6.11.0-17-generic-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- `huggingface_hub` version: 0.29.0\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "what's the error message ?"
      },
      {
        "user": "nikitabelooussovbtis",
        "body": "This was the final error message that it was giving pyarrow.lib.ArrowInvalid: JSON parse error: Column() changed from object to string in row 0"
      },
      {
        "user": "nikitabelooussovbtis",
        "body": "Here is the list of errors:\n\nTraceback (most recent call last):\n  File \".venv/lib/python3.12/site-packages/datasets/packaged_modules/json/json.py\", line 160, in _generate_tables\n    df = pandas_read_json(f)\n         ^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/datasets/packaged_modules/json/json.py\", line 38, in pandas_read_json\n    return pd.read_json(path_or_buf, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/pandas/io/json/_json.py\", line 815, in read_json\n    return json_reader.read()\n           ^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/pandas/io/json/_json.py\", line 1025, in read\n    obj = self._get_object_parser(self.data)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/pandas/io/json/_json.py\", line 1051, in _get_object_parser\n    obj = FrameParser(json, **kwargs).parse()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/pandas/io/json/_json.py\", line 1187, in parse\n    self._parse()\n  File \".venv/lib/python3.12/site-packages/pandas/io/json/_json.py\", line 1402, in _parse\n    self.obj = DataFrame(\n               ^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/pandas/core/frame.py\", line 778, in __init__\n    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/pandas/core/internals/construction.py\", line 503, in dict_to_mgr\n    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/pandas/core/internals/construction.py\", line 114, in arrays_to_mgr\n    index = _extract_index(arrays)\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/pandas/core/internals/construction.py\", line 677, in _extract_index\n    raise ValueError(\"All arrays must be of the same length\")\nValueError: All arrays must be of the same length\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \".venv/lib/python3.12/site-packages/datasets/builder.py\", line 1854, in _prepare_split_single\n    for _, table in generator:\n  File \".venv/lib/python3.12/site-packages/datasets/packaged_modules/json/json.py\", line 163, in _generate_tables\n    raise e\n  File \".venv/lib/python3.12/site-packages/datasets/packaged_modules/json/json.py\", line 137, in _generate_tables\n    pa_table = paj.read_json(\n               ^^^^^^^^^^^^^^\n  File \"pyarrow/_json.pyx\", line 308, in pyarrow._json.read_json\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: JSON parse error: Column() changed from object to number in row 0\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"run_object_detection.py\", line 582, in <module>\n    main()\n  File \"run_object_detection.py\", line 407, in main\n    dataset = load_dataset(\n              ^^^^^^^^^^^^^\n  File \"venv/lib/python3.12/site-packages/datasets/load.py\", line 2151, in load_dataset\n    builder_instance.download_and_prepare(\n  File \".venv/lib/python3.12/site-packages/datasets/builder.py\", line 924, in download_and_prepare\n    self._download_and_prepare(\n  File \".venv/lib/python3.12/site-packages/datasets/builder.py\", line 1000, in _download_and_prepare\n    self._prepare_split(split_generator, **prepare_split_kwargs)\n  File \".venv/lib/python3.12/site-packages/datasets/builder.py\", line 1741, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n  File \".venv/lib/python3.12/site-packages/datasets/builder.py\", line 1897, in _prepare_split_single\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset"
      }
    ]
  },
  {
    "issue_number": 3044,
    "title": "Inconsistent caching behaviour when using `Dataset.map()` with a `new_fingerprint` and `num_proc>1`",
    "author": "vlievin",
    "state": "open",
    "created_at": "2021-10-08T09:07:10Z",
    "updated_at": "2025-03-04T07:16:00Z",
    "labels": [
      "bug"
    ],
    "body": "## Describe the bug\r\nCaching does not work when using `Dataset.map()` with:\r\n1. a function that cannot be deterministically fingerprinted \r\n2. `num_proc>1`\r\n3.  using a custom fingerprint set with the argument `new_fingerprint`. \r\n\r\nThis means that the dataset will be mapped with the function for each and every call, which does not happen if `num_proc==1`. In that case (`num_proc==1`) subsequent calls will load the transformed dataset from the cache, which is the  expected behaviour. The example can easily be translated into a unit test.\r\n\r\nI have a fix and will submit a pull request asap. \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport hashlib\r\nimport json\r\nimport os\r\nfrom typing import Dict, Any\r\n\r\nimport numpy as np\r\nfrom datasets import load_dataset, Dataset\r\n\r\nBatch = Dict[str, Any]\r\nfilename = 'example.json'\r\n\r\n\r\nclass Transformation():\r\n    \"\"\"A transformation with a random state that cannot be fingerprinted\"\"\"\r\n\r\n    def __init__(self):\r\n        self.state = np.random.random()\r\n\r\n    def __call__(self, batch: Batch) -> Batch:\r\n        batch['x'] = [np.random.random() for _ in batch['x']]\r\n        return batch\r\n\r\n\r\ndef generate_dataset():\r\n    \"\"\"generate a simple dataset\"\"\"\r\n    rgn = np.random.RandomState(24)\r\n    data = {\r\n        'data': [{'x': float(y), 'y': -float(y)} for y in\r\n                 rgn.random(size=(1000,))]}\r\n    if not os.path.exists(filename):\r\n        with open(filename, 'w') as f:\r\n            f.write(json.dumps(data))\r\n\r\n    return filename\r\n\r\n\r\ndef process_dataset_with_cache(num_proc=1, remove_cache=False,\r\n                               cache_expected_to_exist=False):\r\n\r\n    # load the generated dataset\r\n    dset: Dataset = next(\r\n        iter(load_dataset('json', data_files=filename, field='data').values()))\r\n    new_fingerprint = hashlib.md5(\"static-id\".encode(\"utf8\")).hexdigest()\r\n\r\n    # get the expected cached path\r\n    cache_path = dset._get_cache_file_path(new_fingerprint)\r\n    if remove_cache and os.path.exists(cache_path):\r\n        os.remove(cache_path)\r\n\r\n     # check that the cache exists, and print a statement\r\n    # if was actually expected to exist\r\n    cache_exist = os.path.exists(cache_path)\r\n    print(f\"> cache file exists={cache_exist}\")\r\n    if cache_expected_to_exist and not cache_exist:\r\n        print(\"=== Cache does not exist! ====\")\r\n\r\n    # apply the transformation with the new fingerprint\r\n    dset = dset.map(\r\n        Transformation(),\r\n        batched=True,\r\n        num_proc=num_proc,\r\n        new_fingerprint=new_fingerprint,\r\n        desc=\"mapping dataset with transformation\")\r\n\r\n\r\ngenerate_dataset()\r\n\r\nfor num_proc in [1, 2]:\r\n    print(f\"# num_proc={num_proc}, first pass\")\r\n    # first pass to generate the cache (always create a new cache here)\r\n    process_dataset_with_cache(remove_cache=True,\r\n                               num_proc=num_proc,\r\n                               cache_expected_to_exist=False)\r\n    print(f\"# num_proc={num_proc}, second pass\")\r\n    # second pass, expects the cache to exist\r\n    process_dataset_with_cache(remove_cache=False,\r\n                               num_proc=num_proc,\r\n                               cache_expected_to_exist=True)\r\n\r\nos.remove(filename)\r\n\r\n```\r\n\r\n## Expected results\r\nIn the above python example, with `num_proc=2`, the **cache file should exist in the second call** of `process_dataset_with_cache`  (\"=== Cache does not exist! ====\" should not be printed). \r\nWhen the cache is successfully created, `map()` is called only one time.\r\n\r\n## Actual results\r\nIn the above python example, with `num_proc=2`, the **cache does not exist in the second call** of `process_dataset_with_cache` (this results in printing \"=== Cache does not exist! ====\"). \r\nBecause the cache doesn't exist, the `map()` method is executed a second time and the dataset is not loaded from the cache.\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.8\r\n- PyArrow version: 5.0.0\r\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Following the discussion in #3045 if would be nice to have a way to let users have a nice experience with caching even if the function is not hashable.\r\n\r\nCurrently a workaround is to make the function picklable. This can be done by implementing a callable class instead, that can be pickled using by implementing a custom `__getstate__` method for example.\r\n\r\nHowever it sounds pretty complicated for a simple thing. Maybe one idea would be to have something similar to streamlit: they allow users to register the hashing of their own objects.\r\n\r\nSee the documentation about their `hash_funcs` here: https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter\r\n\r\nHere is the example they give:\r\n\r\n```python\r\nclass FileReference:\r\n    def __init__(self, filename):\r\n        self.filename = filename\r\n\r\ndef hash_file_reference(file_reference):\r\n    filename = file_reference.filename\r\n    return (filename, os.path.getmtime(filename))\r\n\r\n@st.cache(hash_funcs={FileReference: hash_file_reference})\r\ndef func(file_reference):\r\n    ...\r\n```"
      },
      {
        "user": "vlievin",
        "body": "My solution was to generate a custom hash, and use the hash as a `new_fingerprint` argument to the `map()` method to enable caching. This works, but is quite hacky.\r\n\r\n@lhoestq, this approach is very neat, this would make the whole caching mechanic more explicit. I don't have so much time to look into this right now, but I might give it a try in the future. "
      },
      {
        "user": "BramVanroy",
        "body": "Almost a year later and I'm in a similar boat. Using custom fingerprints and when using multiprocessing the cached datasets are saved with a template at the end of the filename (something like \"000001_of_000008\" for every process of num_proc). So if in the next time you run the script you set num_proc to a different number, the cache cannot be used.\r\n\r\nIs there any way to get around this? I am processing a huge dataset so I do the processing on one machine and then transfer the processed data to another in its cache dir but currently that's not possible due to num_proc mismatch. "
      }
    ]
  },
  {
    "issue_number": 7427,
    "title": "Error splitting the input into NAL units.",
    "author": "MengHao666",
    "state": "open",
    "created_at": "2025-02-28T02:30:15Z",
    "updated_at": "2025-03-04T01:40:28Z",
    "labels": [],
    "body": "### Describe the bug\n\n\nI am trying to finetune qwen2.5-vl on 16 * 80G GPUS, and I use `LLaMA-Factory` and set `preprocessing_num_workers=16`. However, I met the following error and the program seem to got crush. It seems that the error come from `datasets` library\n\nThe error logging is like following:\n```text\nConverting format of dataset (num_proc=16): 100%|█████████▉| 19265/19267 [11:44<00:00,  5.88 examples/s]\nConverting format of dataset (num_proc=16): 100%|█████████▉| 19266/19267 [11:44<00:00,  5.02 examples/s]\nConverting format of dataset (num_proc=16): 100%|██████████| 19267/19267 [11:44<00:00,  5.44 examples/s]\nConverting format of dataset (num_proc=16): 100%|██████████| 19267/19267 [11:44<00:00, 27.34 examples/s]\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [00:00<?, ? examples/s]\nInvalid NAL unit size (45405 > 35540).\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (7131 > 3225).\nmissing picture in access unit with size 54860\nInvalid NAL unit size (48042 > 33645).\nmissing picture in access unit with size 3229\nmissing picture in access unit with size 33649\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (48042 > 33645).\nError splitting the input into NAL units.\nmissing picture in access unit with size 35544\nInvalid NAL unit size (45405 > 35540).\nError splitting the input into NAL units.\nError splitting the input into NAL units.\nInvalid NAL unit size (8187 > 7069).\nmissing picture in access unit with size 7073\nInvalid NAL unit size (8187 > 7069).\nError splitting the input into NAL units.\nInvalid NAL unit size (7131 > 3225).\nError splitting the input into NAL units.\nInvalid NAL unit size (14013 > 5998).\nmissing picture in access unit with size 6002\nInvalid NAL unit size (14013 > 5998).\nError splitting the input into NAL units.\nInvalid NAL unit size (17173 > 7231).\nmissing picture in access unit with size 7235\nInvalid NAL unit size (17173 > 7231).\nError splitting the input into NAL units.\nInvalid NAL unit size (16964 > 6055).\nmissing picture in access unit with size 6059\nInvalid NAL unit size (16964 > 6055).\nException in thread Thread-9 (accepter)Error splitting the input into NAL units.\n:\nTraceback (most recent call last):\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [13:22<?, ? examples/s]    self.run()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 953, in run\n    Invalid NAL unit size (7032 > 2927).\nmissing picture in access unit with size 2931\nself._target(*self._args, **self._kwargs)\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/multiprocess/managers.py\", line 194, in accepter\nInvalid NAL unit size (7032 > 2927).\nError splitting the input into NAL units.\n    t.start()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 935, in start\n    Invalid NAL unit size (28973 > 6121).\nmissing picture in access unit with size 6125\n_start_new_thread(self._bootstrap, ())Invalid NAL unit size (28973 > 6121).\n\nRuntimeError: can't start new threadError splitting the input into NAL units.\n\nInvalid NAL unit size (4411 > 296).\nmissing picture in access unit with size 300\nInvalid NAL unit size (4411 > 296).\nError splitting the input into NAL units.\nInvalid NAL unit size (14414 > 1471).\nmissing picture in access unit with size 1475\nInvalid NAL unit size (14414 > 1471).\nError splitting the input into NAL units.\nInvalid NAL unit size (5283 > 1792).\nmissing picture in access unit with size 1796\nInvalid NAL unit size (5283 > 1792).\nError splitting the input into NAL units.\nInvalid NAL unit size (79147 > 10042).\nmissing picture in access unit with size 10046\nInvalid NAL unit size (79147 > 10042).\nError splitting the input into NAL units.\nInvalid NAL unit size (45405 > 35540).\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (7131 > 3225).\nmissing picture in access unit with size 54860\nInvalid NAL unit size (48042 > 33645).\nmissing picture in access unit with size 3229\nmissing picture in access unit with size 33649\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (48042 > 33645).\nError splitting the input into NAL units.\nmissing picture in access unit with size 35544\nInvalid NAL unit size (45405 > 35540).\nError splitting the input into NAL units.\nError splitting the input into NAL units.\nInvalid NAL unit size (8187 > 7069).\nmissing picture in access unit with size 7073\nInvalid NAL unit size (8187 > 7069).\nError splitting the input into NAL units.\nInvalid NAL unit size (7131 > 3225).\nError splitting the input into NAL units.\nInvalid NAL unit size (14013 > 5998).\nmissing picture in access unit with size 6002\nInvalid NAL unit size (14013 > 5998).\nError splitting the input into NAL units.\nInvalid NAL unit size (17173 > 7231).\nmissing picture in access unit with size 7235\nInvalid NAL unit size (17173 > 7231).\nError splitting the input into NAL units.\nInvalid NAL unit size (16964 > 6055).\nmissing picture in access unit with size 6059\nInvalid NAL unit size (16964 > 6055).\nException in thread Thread-9 (accepter)Error splitting the input into NAL units.\n:\nTraceback (most recent call last):\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [13:22<?, ? examples/s]    self.run()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 953, in run\n    Invalid NAL unit size (7032 > 2927).\nmissing picture in access unit with size 2931\nself._target(*self._args, **self._kwargs)\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/multiprocess/managers.py\", line 194, in accepter\nInvalid NAL unit size (7032 > 2927).\nError splitting the input into NAL units.\n    t.start()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 935, in start\n    Invalid NAL unit size (28973 > 6121).\nmissing picture in access unit with size 6125\n_start_new_thread(self._bootstrap, ())Invalid NAL unit size (28973 > 6121).\n\nRuntimeError: can't start new threadError splitting the input into NAL units.\n\nInvalid NAL unit size (4411 > 296).\nmissing picture in access unit with size 300\nInvalid NAL unit size (4411 > 296).\nError splitting the input into NAL units.\nInvalid NAL unit size (14414 > 1471).\nmissing picture in access unit with size 1475\nInvalid NAL unit size (14414 > 1471).\nError splitting the input into NAL units.\nInvalid NAL unit size (5283 > 1792).\nmissing picture in access unit with size 1796\nInvalid NAL unit size (5283 > 1792).\nError splitting the input into NAL units.\nInvalid NAL unit size (79147 > 10042).\nmissing picture in access unit with size 10046\nInvalid NAL unit size (79147 > 10042).\nError splitting the input into NAL units.\nInvalid NAL unit size (45405 > 35540).\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (7131 > 3225).\nmissing picture in access unit with size 54860\nInvalid NAL unit size (48042 > 33645).\nmissing picture in access unit with size 3229\nmissing picture in access unit with size 33649\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (48042 > 33645).\nError splitting the input into NAL units.\nmissing picture in access unit with size 35544\nInvalid NAL unit size (45405 > 35540).\nError splitting the input into NAL units.\nError splitting the input into NAL units.\nInvalid NAL unit size (8187 > 7069).\nmissing picture in access unit with size 7073\nInvalid NAL unit size (8187 > 7069).\nError splitting the input into NAL units.\nInvalid NAL unit size (7131 > 3225).\nError splitting the input into NAL units.\nInvalid NAL unit size (14013 > 5998).\nmissing picture in access unit with size 6002\nInvalid NAL unit size (14013 > 5998).\nError splitting the input into NAL units.\nInvalid NAL unit size (17173 > 7231).\nmissing picture in access unit with size 7235\nInvalid NAL unit size (17173 > 7231).\nError splitting the input into NAL units.\nInvalid NAL unit size (16964 > 6055).\nmissing picture in access unit with size 6059\nInvalid NAL unit size (16964 > 6055).\nException in thread Thread-9 (accepter)Error splitting the input into NAL units.\n:\nTraceback (most recent call last):\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [13:22<?, ? examples/s]    self.run()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 953, in run\n    Invalid NAL unit size (7032 > 2927).\nmissing picture in access unit with size 2931\nself._target(*self._args, **self._kwargs)\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/multiprocess/managers.py\", line 194, in accepter\nInvalid NAL unit size (7032 > 2927).\nError splitting the input into NAL units.\n    t.start()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 935, in start\n    Invalid NAL unit size (28973 > 6121).\nmissing picture in access unit with size 6125\n_start_new_thread(self._bootstrap, ())Invalid NAL unit size (28973 > 6121).\n\nRuntimeError: can't start new threadError splitting the input into NAL units.\n\nInvalid NAL unit size (4411 > 296).\nmissing picture in access unit with size 300\nInvalid NAL unit size (4411 > 296).\nError splitting the input into NAL units.\nInvalid NAL unit size (14414 > 1471).\nmissing picture in access unit with size 1475\nInvalid NAL unit size (14414 > 1471).\nError splitting the input into NAL units.\nInvalid NAL unit size (5283 > 1792).\nmissing picture in access unit with size 1796\nInvalid NAL unit size (5283 > 1792).\nError splitting the input into NAL units.\nInvalid NAL unit size (79147 > 10042).\nmissing picture in access unit with size 10046\nInvalid NAL unit size (79147 > 10042).\nError splitting the input into NAL units.\nInvalid NAL unit size (45405 > 35540).\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (7131 > 3225).\nmissing picture in access unit with size 54860\nInvalid NAL unit size (48042 > 33645).\nmissing picture in access unit with size 3229\nmissing picture in access unit with size 33649\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (48042 > 33645).\nError splitting the input into NAL units.\nmissing picture in access unit with size 35544\nInvalid NAL unit size (45405 > 35540).\nError splitting the input into NAL units.\nError splitting the input into NAL units.\nInvalid NAL unit size (8187 > 7069).\nmissing picture in access unit with size 7073\nInvalid NAL unit size (8187 > 7069).\nError splitting the input into NAL units.\nInvalid NAL unit size (7131 > 3225).\nError splitting the input into NAL units.\nInvalid NAL unit size (14013 > 5998).\nmissing picture in access unit with size 6002\nInvalid NAL unit size (14013 > 5998).\nError splitting the input into NAL units.\nInvalid NAL unit size (17173 > 7231).\nmissing picture in access unit with size 7235\nInvalid NAL unit size (17173 > 7231).\nError splitting the input into NAL units.\nInvalid NAL unit size (16964 > 6055).\nmissing picture in access unit with size 6059\nInvalid NAL unit size (16964 > 6055).\nException in thread Thread-9 (accepter)Error splitting the input into NAL units.\n:\nTraceback (most recent call last):\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [13:22<?, ? examples/s]    self.run()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 953, in run\n    Invalid NAL unit size (7032 > 2927).\nmissing picture in access unit with size 2931\nself._target(*self._args, **self._kwargs)\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/multiprocess/managers.py\", line 194, in accepter\nInvalid NAL unit size (7032 > 2927).\nError splitting the input into NAL units.\n    t.start()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 935, in start\n    Invalid NAL unit size (28973 > 6121).\nmissing picture in access unit with size 6125\n_start_new_thread(self._bootstrap, ())Invalid NAL unit size (28973 > 6121).\n\nRuntimeError: can't start new threadError splitting the input into NAL units.\n\nInvalid NAL unit size (4411 > 296).\nmissing picture in access unit with size 300\nInvalid NAL unit size (4411 > 296).\nError splitting the input into NAL units.\nInvalid NAL unit size (14414 > 1471).\nmissing picture in access unit with size 1475\nInvalid NAL unit size (14414 > 1471).\nError splitting the input into NAL units.\nInvalid NAL unit size (5283 > 1792).\nmissing picture in access unit with size 1796\nInvalid NAL unit size (5283 > 1792).\nError splitting the input into NAL units.\nInvalid NAL unit size (79147 > 10042).\nmissing picture in access unit with size 10046\nInvalid NAL unit size (79147 > 10042).\nError splitting the input into NAL units.\n\n```\n\n\n### Others\n\n_No response_\n\n### Steps to reproduce the bug\n\nNone\n\n### Expected behavior\n\nexcpect to run successfully\n\n### Environment info\n\n```\ntransformers==4.49.0\ndatasets==3.2.0\naccelerate==1.2.1\npeft==0.12.0\ntrl==0.9.6\ntokenizers==0.21.0\ngradio>=4.38.0,<=5.18.0\npandas>=2.0.0\nscipy\neinops\nsentencepiece\ntiktoken\nprotobuf\nuvicorn\npydantic\nfastapi\nsse-starlette\nmatplotlib>=3.7.0\nfire\npackaging\npyyaml\nnumpy<2.0.0\nav\nlibrosa\ntyro<0.9.0\nopenlm-hub\nqwen-vl-utils\n\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "First time I see this error :/ maybe it's an issue with your version of `multiprocess` and `dill` ? Make sure they are compatible with `datasets`"
      },
      {
        "user": "MengHao666",
        "body": "> First time I see this error :/ maybe it's an issue with your version of `multiprocess` and `dill` ? Make sure they are compatible with `datasets`\n\nany recommendation for  `multiprocess` and `dill`"
      }
    ]
  },
  {
    "issue_number": 7423,
    "title": "Row indexing a dataset with numpy integers",
    "author": "DavidRConnell",
    "state": "open",
    "created_at": "2025-02-25T18:44:45Z",
    "updated_at": "2025-03-03T17:55:24Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nAllow indexing datasets with a scalar numpy integer type.\n\n### Motivation\n\nIndexing a dataset with a scalar numpy.int* object raises a TypeError. This is due to the test in `datasets/formatting/formatting.py:key_to_query_type`\n\n``` python\ndef key_to_query_type(key: Union[int, slice, range, str, Iterable]) -> str:\n    if isinstance(key, int):\n        return \"row\"\n    elif isinstance(key, str):\n        return \"column\"\n    elif isinstance(key, (slice, range, Iterable)):\n        return \"batch\"\n    _raise_bad_key_type(key)\n```\n\nIn the row case, it checks if key is an int, which returns false when key is integer like but not a builtin python integer type. This is counterintuitive because a numpy array of np.int64s can be used for the batch case.\n\nFor example:\n\n``` python\nimport numpy as np\n\nimport datasets\n\ndataset = datasets.Dataset.from_dict({\"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]})\n\n# Regular indexing\ndataset[0]\ndataset[:2]\n\n# Indexing with numpy data types (expect same results)\nidx = np.asarray([0, 1])\ndataset[idx]  # Succeeds when using an array of np.int64 values\ndataset[idx[0]]  # Fails with TypeError when using scalar np.int64\n```\n\nFor the user, this can be solved by wrapping `idx[0]` in `int` but the test could also be changed in `key_to_query_type` to accept a less strict definition of int.\n\n``` diff\n+import numbers\n+\ndef key_to_query_type(key: Union[int, slice, range, str, Iterable]) -> str:\n+   if isinstance(key, numbers.Integral):\n-   if isinstance(key, int):\n        return \"row\"\n    elif isinstance(key, str):\n        return \"column\"\n    elif isinstance(key, (slice, range, Iterable)):\n        return \"batch\"\n    _raise_bad_key_type(key)\n```\n\nLooking at how others do it, pandas has an `is_integer` definition that it checks which uses `is_integer_object` defined in `pandas/_libs/utils.pxd`:\n\n``` cython\ncdef inline bint is_integer_object(object obj) noexcept:\n    \"\"\"\n    Cython equivalent of\n\n    `isinstance(val, (int, np.integer)) and not isinstance(val, (bool, np.timedelta64))`\n\n    Parameters\n    ----------\n    val : object\n\n    Returns\n    -------\n    is_integer : bool\n\n    Notes\n    -----\n    This counts np.timedelta64 objects as integers.\n    \"\"\"\n    return (not PyBool_Check(obj) and isinstance(obj, (int, cnp.integer))\n            and not is_timedelta64_object(obj))\n```\n\nThis would be less flexible as it explicitly checks for numpy integer, but worth noting that they had the need to ensure the key is not a bool.\n\n### Your contribution\n\nI can submit a pull request with the above changes after checking that indexing succeeds with the numpy integer type. Or if there is a different integer check that would be preferred I could add that.\n\nIf there is a reason not to want this behavior that is fine too.\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Would be cool to be consistent when it comes to indexing with numpy objects, if we do accept numpy arrays we should indeed accept numpy integers. Your idea sounds reasonable, I'd also be in favor of adding a simple test as well"
      }
    ]
  },
  {
    "issue_number": 7421,
    "title": "DVC integration broken",
    "author": "maxstrobel",
    "state": "open",
    "created_at": "2025-02-25T13:14:31Z",
    "updated_at": "2025-03-03T17:42:02Z",
    "labels": [],
    "body": "### Describe the bug\n\nThe DVC integration seems to be broken. \nFollowed this guide: https://dvc.org/doc/user-guide/integrations/huggingface\n\n### Steps to reproduce the bug\n\n#### Script to reproduce\n~~~python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\n    \"csv\",\n    data_files=\"dvc://workshop/satellite-data/jan_train.csv\",\n    storage_options={\"url\": \"https://github.com/iterative/dataset-registry.git\"},\n)\n\nprint(dataset)\n~~~\n\n#### Error log\n~~~\nTraceback (most recent call last):\n  File \"C:\\tmp\\test\\load.py\", line 3, in <module>\n    dataset = load_dataset(\n              ^^^^^^^^^^^^^\n  File \"C:\\tmp\\test\\.venv\\Lib\\site-packages\\datasets\\load.py\", line 2151, in load_dataset\n    builder_instance.download_and_prepare(\n  File \"C:\\tmp\\test\\.venv\\Lib\\site-packages\\datasets\\builder.py\", line 808, in download_and_prepare\n    fs, output_dir = url_to_fs(output_dir, **(storage_options or {}))\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: url_to_fs() got multiple values for argument 'url'\n~~~\n\n\n\n### Expected behavior\n\nIntegration would work and the indicated file is downloaded and opened.\n\n### Environment info\n\n#### Python version\n~~~\npython --version\nPython 3.11.10\n~~~\n\n#### Venv (pip install datasets dvc):\n~~~\nPackage                Version\n---------------------- -----------\naiohappyeyeballs       2.4.6\naiohttp                3.11.13\naiohttp-retry          2.9.1\naiosignal              1.3.2\namqp                   5.3.1\nannotated-types        0.7.0\nantlr4-python3-runtime 4.9.3\nappdirs                1.4.4\nasyncssh               2.20.0\natpublic               5.1\nattrs                  25.1.0\nbilliard               4.2.1\ncelery                 5.4.0\ncertifi                2025.1.31\ncffi                   1.17.1\ncharset-normalizer     3.4.1\nclick                  8.1.8\nclick-didyoumean       0.3.1\nclick-plugins          1.1.1\nclick-repl             0.3.0\ncolorama               0.4.6\nconfigobj              5.0.9\ncryptography           44.0.1\ndatasets               3.3.2\ndictdiffer             0.9.0\ndill                   0.3.8\ndiskcache              5.6.3\ndistro                 1.9.0\ndpath                  2.2.0\ndulwich                0.22.7\ndvc                    3.59.1\ndvc-data               3.16.9\ndvc-http               2.32.0\ndvc-objects            5.1.0\ndvc-render             1.0.2\ndvc-studio-client      0.21.0\ndvc-task               0.40.2\nentrypoints            0.4\nfilelock               3.17.0\nflatten-dict           0.4.2\nflufl-lock             8.1.0\nfrozenlist             1.5.0\nfsspec                 2024.12.0\nfuncy                  2.0\ngitdb                  4.0.12\ngitpython              3.1.44\ngrandalf               0.8\ngto                    1.7.2\nhuggingface-hub        0.29.1\nhydra-core             1.3.2\nidna                   3.10\niterative-telemetry    0.0.10\nkombu                  5.4.2\nmarkdown-it-py         3.0.0\nmdurl                  0.1.2\nmultidict              6.1.0\nmultiprocess           0.70.16\nnetworkx               3.4.2\nnumpy                  2.2.3\nomegaconf              2.3.0\norjson                 3.10.15\npackaging              24.2\npandas                 2.2.3\npathspec               0.12.1\nplatformdirs           4.3.6\nprompt-toolkit         3.0.50\npropcache              0.3.0\npsutil                 7.0.0\npyarrow                19.0.1\npycparser              2.22\npydantic               2.10.6\npydantic-core          2.27.2\npydot                  3.0.4\npygit2                 1.17.0\npygments               2.19.1\npygtrie                2.5.0\npyparsing              3.2.1\npython-dateutil        2.9.0.post0\npytz                   2025.1\npywin32                308\npyyaml                 6.0.2\nrequests               2.32.3\nrich                   13.9.4\nruamel-yaml            0.18.10\nruamel-yaml-clib       0.2.12\nscmrepo                3.3.10\nsemver                 3.0.4\nsetuptools             75.8.0\nshellingham            1.5.4\nshortuuid              1.0.13\nshtab                  1.7.1\nsix                    1.17.0\nsmmap                  5.0.2\nsqltrie                0.11.2\ntabulate               0.9.0\ntomlkit                0.13.2\ntqdm                   4.67.1\ntyper                  0.15.1\ntyping-extensions      4.12.2\ntzdata                 2025.1\nurllib3                2.3.0\nvine                   5.1.0\nvoluptuous             0.15.2\nwcwidth                0.2.13\nxxhash                 3.5.0\nyarl                   1.18.3\nzc-lockfile            3.0.post1\n~~~",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Unfortunately `url` is a reserved argument in `fsspec.url_to_fs`, so ideally file system implementations like DVC should use another argument name to avoid this kind of errors"
      }
    ]
  },
  {
    "issue_number": 7217,
    "title": "ds.map(f, num_proc=10) is slower than df.apply",
    "author": "lanlanlanlanlanlan365",
    "state": "open",
    "created_at": "2024-10-11T11:04:05Z",
    "updated_at": "2025-02-28T21:21:01Z",
    "labels": [],
    "body": "### Describe the bug\n\npandas columns: song_id, song_name\r\nds = Dataset.from_pandas(df)\r\n\r\ndef has_cover(song_name):\r\n    if song_name is None or pd.isna(song_name):\r\n        return False\r\n    return 'cover' in song_name.lower()\r\n\r\ndf['has_cover'] = df.song_name.progress_apply(has_cover)\r\nds = ds.map(lambda x: {'has_cover': has_cover(x['song_name'])}, num_proc=10)\r\n\r\ntime cost: \r\n1. df.apply: 100%|██████████| 12500592/12500592 [00:13<00:00, 959825.47it/s]\r\n2. ds.map: Map (num_proc=10):  31%\r\n 3899028/12500592 [00:28<00:38, 222532.89 examples/s]\n\n### Steps to reproduce the bug\n\npandas columns: song_id, song_name\r\nds = Dataset.from_pandas(df)\r\n\r\ndef has_cover(song_name):\r\n    if song_name is None or pd.isna(song_name):\r\n        return False\r\n    return 'cover' in song_name.lower()\r\n\r\ndf['has_cover'] = df.song_name.progress_apply(has_cover)\r\nds = ds.map(lambda x: {'has_cover': has_cover(x['song_name'])}, num_proc=10)\n\n### Expected behavior\n\nds.map is ~num_proc faster than df.apply\n\n### Environment info\n\npandas: 2.2.2\r\ndatasets: 2.19.1",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! `map()` reads all the columns and writes the resulting dataset with all the columns as well, while df.column_name.apply only reads and writes one column and does it in memory. So this is speed difference is actually expected.\r\n\r\nMoreover using multiprocessing on a dataset that lives in memory (from_pandas uses the same in-memory data as the pandas DataFrame while load_dataset or from_generator load from disk) requires to copy the data to each subprocess which can also be slow. Data loaded from disk don't need to be copied though since they work as a form of shared memory thanks to memory mapping.\r\n\r\nHowever you can make you map() call much faster by making it read and write only the column you want:\r\n\r\n```python\r\nhas_cover_ds = ds.map(lambda song_name: {'has_cover': has_cover(song_name)}, input_columns=[\"song_name\"], remove_columns=ds.column_names)  # outputs a dataset with 1 column\r\nds = ds.concatenate_datasets([ds, has_cover_ds], axis=1)\r\n```\r\n\r\nand if your dataset is loaded from disk you can pass num_proc=10 and get a nice speed up as well (no need to copy the data to subprocesses)"
      },
      {
        "user": "bryant1410",
        "body": "Isn't there a way to do memory mapping with the in-memory dataset without saving it to disk?"
      },
      {
        "user": "bryant1410",
        "body": "Maybe saving it to a memory-mapped filesystem? It'd be like a trick to make datasets save to \"disk\" but actually it's memory. But it feels like there should be a better \"automatic\" way provided by `datasets`."
      }
    ]
  },
  {
    "issue_number": 5811,
    "title": "load_dataset: TypeError: 'NoneType' object is not callable, on local dataset filename changes",
    "author": "durapensa",
    "state": "open",
    "created_at": "2023-04-30T13:27:17Z",
    "updated_at": "2025-02-27T07:32:30Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nI've adapted Databrick's [train_dolly.py](/databrickslabs/dolly/blob/master/train_dolly.py) to train using a local dataset, which has been working. Upon changing the filenames of the `.json` & `.py` files in my local dataset directory, `dataset = load_dataset(path_or_dataset)[\"train\"]` throws the error:\r\n\r\n```python\r\n2023-04-30 09:10:52 INFO [training.trainer] Loading dataset from dushowxa-characters\r\nTraceback (most recent call last):\r\n  File \"/data/dushowxa-dolly/train_dushowxa.py\", line 26, in <module>\r\n    load_training_dataset()\r\n  File \"/data/dushowxa-dolly/training/trainer.py\", line 89, in load_training_dataset\r\n    dataset = load_dataset(path_or_dataset)[\"train\"]\r\n  File \"/data/dushowxa-dolly/.venv/lib/python3.10/site-packages/datasets/load.py\", line 1773, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/data/dushowxa-dolly/.venv/lib/python3.10/site-packages/datasets/load.py\", line 1528, in load_dataset_builder\r\n    builder_instance: DatasetBuilder = builder_cls(\r\nTypeError: 'NoneType' object is not callable\r\n```\r\nThe local dataset filenames were of the form `dushowxa-characters/expanse-dushowxa-characters.json` and are now of the form `dushowxa-characters/dushowxa-characters.json` (the word `expanse-` was removed from the filenames). Is this perhaps a dataset caching issue?\r\n\r\nI have attempted to manually clear caches, but to no effect:\r\n\r\n```sh\r\nrm -rfv ~/.cache/huggingface/datasets/*\r\nrm -rfv ~/.cache/huggingface/modules/* \r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun `python3 train_dushowxa.py` (adapted from Databrick's [train_dolly.py](/databrickslabs/dolly/blob/master/train_dolly.py)).\r\n\r\n### Expected behavior\r\n\r\nTraining succeeds as before local dataset filenames were changed.\r\n\r\n### Environment info\r\n\r\nUbuntu 22.04, Python 3.10.6, venv\r\n```python\r\naccelerate>=0.16.0,<1\r\nclick>=8.0.4,<9\r\ndatasets>=2.10.0,<3\r\ndeepspeed>=0.9.0,<1\r\ntransformers[torch]>=4.28.1,<5\r\nlangchain>=0.0.139\r\n```",
    "comments": [
      {
        "user": "mariosasko",
        "body": "This error means a `DatasetBuilder` subclass that generates the dataset could not be found inside the script, so make sure `dushowxa-characters/dushowxa-characters.py `is a valid dataset script (assuming `path_or_dataset` is `dushowxa-characters`)\r\n\r\nAlso, we should improve the error to make it more obvious what the problem is."
      },
      {
        "user": "dshwei",
        "body": "from datasets import load_dataset\nlcb_codegen = load_dataset(\"livecodebench/code_generation_lite\", version_tag=\"release_v2\")\n\nor  \nconfigs = get_dataset_config_names(\"livecodebench/code_generation_lite\", trust_remote_code=True)\n\n**both  error:**\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/workspace/miniconda/envs/grpo/lib/python3.10/site-packages/datasets/load.py\", line 2131, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"/workspace/miniconda/envs/grpo/lib/python3.10/site-packages/datasets/load.py\", line 1888, in load_dataset_builder\n    builder_instance: DatasetBuilder = builder_cls(\nTypeError: 'NoneType' object is not callable"
      }
    ]
  },
  {
    "issue_number": 7420,
    "title": "better correspondence between cached and saved datasets created using from_generator",
    "author": "vttrifonov",
    "state": "open",
    "created_at": "2025-02-24T22:14:37Z",
    "updated_at": "2025-02-26T03:10:22Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nAt the moment `.from_generator` can only create a dataset that lives in the cache. The cached dataset cannot be loaded with `load_from_disk` because the cache folder is missing `state.json`. So the only way to convert this cached dataset to a regular is to use `save_to_disk` which needs to create a copy of the cached dataset. For large datasets this can end up wasting a lot of space. In my case the saving operation failed so I am stuck with a large cached dataset and no clear way to convert to a `Dataset` that I can use. The requested feature is to provide a way to be able to load a cached dataset using `.load_from_disk`. Alternatively `.from_generator` can create the dataset at a specified location so that it can be loaded from there with `.load_from_disk`.\n\n### Motivation\n\nI have the following workflow which has exposed some awkwardness about the Datasets saving/caching.\n\n1. I created a cached dataset using `.from_generator` which was cached in a folder. This dataset is rather large (~600GB) with many shards.\n2. I tried to save this dataset using `.save_to_disk` to another location so that I can use later as a `Dataset`. This essentially creates another copy (for a total of 1.2TB!) of what is already in the cache... In my case the saving operation keeps dying for some reason and I am stuck with a cached dataset and no copy.\n3. Now I am trying to \"save\" the existing cached dataset but it is not clear how to access the cached files after `.from_generator` has finished e.g. from a different process. I should not be even looking at the cache but I really do not want to waste another 2hr to generate the set so that if fails agains (I already did this couple of times). \n- I tried `.load_from_disk` but it does not work with cached files and complains that this is not a `Dataset` (!).\n- I looked at `.from_file` which takes one file but the cached file has many (shards) so I am not sure how to make this work. \n- I tried `.load_dataset` but this seems to either try to \"download\" a copy (of a file which is already in the local file system!) which I will then need to save or I need to use `streaming=False` to create an `IterableDataset `which then I need to convert (using the cache) to `Dataset` so that I can save it. With both options I  will end up with 3 copies of the same dataset for a total of ~2TB! I am hoping here is another way to do this...\n\nMaybe I am missing something here: I looked at docs and forums but no luck. I have a bunch of arrow files cached by `Dataset.from_generator` and no clean way to make them into a `Dataset` that I can use. \n\nThis all could be so much easer if `load_from_disk` can recognize the cached files and produce a `Dataset`: after the cache is created I would not have to \"save\" it again and I can just load it when I need.  At the moment `load_from_disk` needs `state.json` which is lacking in the cache folder. So perhaps `.from_generator` could be made to \"finalize\" (e.g. create `state.json`) the dataset once it is done so that it can be loaded easily. Or provide `.from_generator` with a `save_to_dir` parameter in addition to `cache_dir` which can be used for the whole process including creating the `state.json` at the end. \n\nAs a proof of concept I just created `state.json` by hand and `load_from_disk` worked using the cache! So it seems to be the missing piece here.\n\n### Your contribution\n\nTime permitting I can look into `.from_generator` to see if adding  `state.json` is feasible.",
    "comments": []
  },
  {
    "issue_number": 7197,
    "title": "ConnectionError: Couldn't reach 'allenai/c4' on the Hub (ConnectionError)数据集下不下来，怎么回事",
    "author": "Mrgengli",
    "state": "open",
    "created_at": "2024-10-04T09:33:25Z",
    "updated_at": "2025-02-26T02:26:16Z",
    "labels": [],
    "body": "### Describe the bug\n\nfrom datasets import load_dataset\r\nprint(\"11\")\r\ntraindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\r\nprint(\"22\")\r\nvaldata = load_dataset('ptb_text_only',\r\n                           'penn_treebank',\r\n                           split='validation')\n\n### Steps to reproduce the bug\n\n1\n\n### Expected behavior\n\n1\n\n### Environment info\n\n1",
    "comments": [
      {
        "user": "skpig",
        "body": "Also cant download \"allenai/c4\", but with different error reported:\r\n```\r\nTraceback (most recent call last):                                                                                                                                                                                                                                                                                                                                                                          \r\n  File \"/***/lib/python3.10/site-packages/datasets/load.py\", line 2074, in load_dataset                                                                                                                                              \r\n    builder_instance = load_dataset_builder(                                                                                                                                                                                                                  \r\n  File \"/***/lib/python3.10/site-packages/datasets/load.py\", line 1795, in load_dataset_builder                                                                                                                                      \r\n    dataset_module = dataset_module_factory(                                                                                                                                                                                                                  \r\n  File \"/***/lib/python3.10/site-packages/datasets/load.py\", line 1659, in dataset_module_factory                                                                                                                                    \r\n    raise e1 from None                                                                                                                                                                                                                                        \r\n  File \"/***/lib/python3.10/site-packages/datasets/load.py\", line 1647, in dataset_module_factory                                                                                                                                    \r\n    ).get_module()                                                                                                                                                                                                                                            \r\n  File \"/***/lib/python3.10/site-packages/datasets/load.py\", line 1069, in get_module                                                                                                                                                \r\n    module_name, default_builder_kwargs = infer_module_for_data_files(                                                                                                                                                                                        \r\n  File \"/***/lib/python3.10/site-packages/datasets/load.py\", line 594, in infer_module_for_data_files                                                                                                                                \r\n    raise DataFilesNotFoundError(\"No (supported) data files found\" + (f\" in {path}\" if path else \"\"))                                                                                                                                                         \r\ndatasets.exceptions.DataFilesNotFoundError: No (supported) data files found in allenai/c4  \r\n```\r\n\r\n## Code to reproduce\r\n```\r\ndataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True,trust_remote_code=True,\r\n                        cache_dir=\"dataset/en\",\r\n                       download_mode=\"force_redownload\")\r\n```\r\n\r\n## Environment\r\ndatasets                 3.0.1 \r\nhuggingface_hub  0.25.1"
      },
      {
        "user": "liouxiao",
        "body": "应该是网络问题，无法访问外网？"
      }
    ]
  },
  {
    "issue_number": 7418,
    "title": "pyarrow.lib.arrowinvalid: cannot mix list and non-list, non-null values with map function",
    "author": "alexxchen",
    "state": "open",
    "created_at": "2025-02-21T10:58:06Z",
    "updated_at": "2025-02-25T15:26:46Z",
    "labels": [],
    "body": "### Describe the bug\n\nEncounter pyarrow.lib.arrowinvalid error with map function in some example when loading the dataset\n\n### Steps to reproduce the bug\n\n```\nfrom datasets import load_dataset\nfrom PIL import Image, PngImagePlugin\n\ndataset = load_dataset(\"leonardPKU/GEOQA_R1V_Train_8K\")\nsystem_prompt=\"You are a helpful AI Assistant\"\ndef make_conversation(example):\n    prompt = []\n\n    prompt.append({\"role\": \"system\", \"content\": system_prompt})\n    prompt.append(\n            {\n                \"role\": \"user\", \n                \"content\": [\n                        {\"type\": \"image\"},\n                        {\"type\": \"text\", \"text\": example[\"problem\"]},\n                        ]\n            }\n        )\n    return {\"prompt\": prompt}\n\ndef check_data_types(example):\n    for key, value in example.items():\n        if key == 'image':\n            if not isinstance(value, PngImagePlugin.PngImageFile):\n                print(value)\n        if key == \"problem\" or key == \"solution\":\n            if not isinstance(value, str):\n                print(value)\n\n    return example\n\ndataset = dataset.map(check_data_types)\ndataset = dataset.map(make_conversation)\n```\n\n### Expected behavior\n\nSuccessfully process the dataset with map\n\n### Environment info\n\ndatasets==3.3.1",
    "comments": [
      {
        "user": "alexxchen",
        "body": "@lhoestq "
      },
      {
        "user": "lhoestq",
        "body": "Can you try passing text: None for the image object ? Pyarrow expects all the objects to have the exact same type, in particular the dicttionaries in \"content\" should all have the keys \"type\" and \"text\""
      },
      {
        "user": "alexxchen",
        "body": "The following modification on system prompt works, but it is different from the usual way to use it.\n```\ndef make_conversation(example):\n    prompt = []\n\n    prompt.append({\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]})\n    prompt.append(\n            {\n                \"role\": \"user\", \n                \"content\": [\n                        {\"type\": \"image\"},\n                        {\"type\": \"text\", \"text\": example[\"problem\"]},\n                        ]\n            }\n        )\n    return {\"prompt\": prompt}\n```"
      }
    ]
  },
  {
    "issue_number": 5243,
    "title": "Download only split data",
    "author": "capsabogdan",
    "state": "open",
    "created_at": "2022-11-15T10:15:54Z",
    "updated_at": "2025-02-25T14:47:03Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nIs it possible to download only the data that I am requesting and not the entire dataset? I run out of disk spaceas it seems to download the entire dataset, instead of only the part needed.\r\n\r\ncommon_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", \r\n                                    cache_dir=\"cache/path...\",\r\n                                    use_auth_token=True,\r\n                                    download_config=DownloadConfig(delete_extracted='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')\r\n                                    )\r\n\n\n### Motivation\n\nefficiency improvement\n\n### Your contribution\n\nn/a",
    "comments": [
      {
        "user": "polinaeterna",
        "body": "Hi @capsabogdan! Unfortunately, it's hard to implement because quite often datasets data is being hosted in a single archive for all splits :( So we have to download the whole archive to split it into splits. This is the case for CommonVoice too. \r\n\r\nHowever, for cases when data is distributed in separate archives ащк different splits I suppose it can (and will) be implemented someday. \r\n\r\n\r\nBtw for quick check of the dataset you can use [streaming](https://huggingface.co/docs/datasets/stream):\r\n```python\r\ncv = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True)\r\ncv = iter(cv)\r\nprint(next(cv))\r\n\r\n>> {'client_id': 'a07b17f8234ded5e847443ea6f423cef745cbbc7537fb637d58326000aa751e829a21c4fd0a35fc17fb833aa7e95ebafce5efd19beeb8d843887b85e4eb35f5b',\r\n>>  'path': None,\r\n>>  'audio': {'path': 'cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100363.mp3',\r\n>>  'array': array([ 0.0000000e+00,  1.1748125e-14,  1.5450088e-14, ...,\r\n>>          1.3011958e-06, -6.3548953e-08, -9.9098514e-08], dtype=float32),\r\n>> ...}\r\n\r\n```"
      },
      {
        "user": "capsabogdan",
        "body": "thank you for the answer but am not sure if this will not be helpful, as we\nneed maybe just 10% of the datasets for some experiment\n\ncan we get just a portion of the dataset with stream?\n\n\nis there really no solution? :(\n\nAm Di., 15. Nov. 2022 um 16:55 Uhr schrieb Polina Kazakova <\n***@***.***>:\n\n> Hi @capsabogdan <https://github.com/capsabogdan>! Unfortunately, it's\n> hard to implement because quite often datasets data is being hosted in a\n> single archive for all splits :( So we have to download the whole archive\n> to split it into splits. This is the case for CommonVoice too.\n>\n> However, for cases when data is distributed in separate archives in\n> different splits I suppose it can be implemented someday.\n>\n> Btw for quick check of the dataset you can use streaming\n> <https://huggingface.co/docs/datasets/stream>:\n>\n> cv = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True)cv = iter(cv)print(next(cv))\n> >> {'client_id': 'a07b17f8234ded5e847443ea6f423cef745cbbc7537fb637d58326000aa751e829a21c4fd0a35fc17fb833aa7e95ebafce5efd19beeb8d843887b85e4eb35f5b',>>  'path': None,>>  'audio': {'path': 'cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100363.mp3',>>  'array': array([ 0.0000000e+00,  1.1748125e-14,  1.5450088e-14, ...,>>          1.3011958e-06, -6.3548953e-08, -9.9098514e-08], dtype=float32),>> ...}\n>\n> —\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/5243#issuecomment-1315512887>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALSIFOC3JYRCTH54OBRUJULWIOW6PANCNFSM6AAAAAASAYO2LY>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n"
      },
      {
        "user": "capsabogdan",
        "body": "maybe it would be nice if you guys ould do some sort of shard before\nloading the dataset, so users can download just chunks of data :)\n\nI think this would be very helpful\n\nAm Di., 15. Nov. 2022 um 19:24 Uhr schrieb Bogdan Capsa <\n***@***.***>:\n\n> thank you for the answer but am not sure if this will not be helpful, as\n> we need maybe just 10% of the datasets for some experiment\n>\n> can we get just a portion of the dataset with stream?\n>\n>\n> is there really no solution? :(\n>\n> Am Di., 15. Nov. 2022 um 16:55 Uhr schrieb Polina Kazakova <\n> ***@***.***>:\n>\n>> Hi @capsabogdan <https://github.com/capsabogdan>! Unfortunately, it's\n>> hard to implement because quite often datasets data is being hosted in a\n>> single archive for all splits :( So we have to download the whole archive\n>> to split it into splits. This is the case for CommonVoice too.\n>>\n>> However, for cases when data is distributed in separate archives in\n>> different splits I suppose it can be implemented someday.\n>>\n>> Btw for quick check of the dataset you can use streaming\n>> <https://huggingface.co/docs/datasets/stream>:\n>>\n>> cv = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True)cv = iter(cv)print(next(cv))\n>> >> {'client_id': 'a07b17f8234ded5e847443ea6f423cef745cbbc7537fb637d58326000aa751e829a21c4fd0a35fc17fb833aa7e95ebafce5efd19beeb8d843887b85e4eb35f5b',>>  'path': None,>>  'audio': {'path': 'cv-corpus-11.0-2022-09-21/en/clips/common_voice_en_100363.mp3',>>  'array': array([ 0.0000000e+00,  1.1748125e-14,  1.5450088e-14, ...,>>          1.3011958e-06, -6.3548953e-08, -9.9098514e-08], dtype=float32),>> ...}\n>>\n>> —\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/huggingface/datasets/issues/5243#issuecomment-1315512887>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ALSIFOC3JYRCTH54OBRUJULWIOW6PANCNFSM6AAAAAASAYO2LY>\n>> .\n>> You are receiving this because you were mentioned.Message ID:\n>> ***@***.***>\n>>\n>\n"
      }
    ]
  },
  {
    "issue_number": 7419,
    "title": "Import order crashes script execution",
    "author": "DamienMatias",
    "state": "open",
    "created_at": "2025-02-24T17:03:43Z",
    "updated_at": "2025-02-24T17:03:43Z",
    "labels": [],
    "body": "### Describe the bug\n\nHello,\n\nI'm trying to convert an HF dataset into a TFRecord so I'm importing `tensorflow` and `datasets` to do so.\nDepending in what order I'm importing those librairies, my code hangs forever and is unkillable (CTRL+C doesn't work, I need to kill my shell entirely).\n\nThank you for your help\n🙏 \n\n### Steps to reproduce the bug\n\nIf you run the following script, this will hang forever : \n```python\nimport tensorflow as tf\nimport datasets\n\ndataset = datasets.load_dataset(\"imagenet-1k\", split=\"validation\", streaming=True)\nprint(next(iter(dataset)))\n```\n\nhowever running the following will work fine (I just changed the order of the imports) :\n```python\nimport datasets\nimport tensorflow as tf\n\ndataset = datasets.load_dataset(\"imagenet-1k\", split=\"validation\", streaming=True)\nprint(next(iter(dataset)))\n```\n\n### Expected behavior\n\nI'm expecting the script to reach the end and my case print the content of the first item in the dataset \n```\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=408x500 at 0x70C646A03110>, 'label': 91}\n```\n\n### Environment info\n\n```\n$ datasets-cli env\n- `datasets` version: 3.3.2\n- Platform: Linux-6.8.0-1017-aws-x86_64-with-glibc2.35\n- Python version: 3.11.7\n- `huggingface_hub` version: 0.29.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n```\n\nI'm also using `tensorflow==2.18.0`.",
    "comments": []
  },
  {
    "issue_number": 7360,
    "title": "error when loading dataset in Hugging Face: NoneType error is not callable",
    "author": "nanu23333",
    "state": "open",
    "created_at": "2025-01-07T02:11:36Z",
    "updated_at": "2025-02-24T13:32:52Z",
    "labels": [],
    "body": "### Describe the bug\n\nI met an error when running a notebook provide by Hugging Face,  and met the error.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[2], line 5\r\n      3 # Load the enhancers dataset from the InstaDeep Hugging Face ressources\r\n      4 dataset_name = \"enhancers_types\"\r\n----> 5 train_dataset_enhancers = load_dataset(\r\n      6         \"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\",\r\n      7         dataset_name,\r\n      8         split=\"train\",\r\n      9         streaming= False,\r\n     10     )\r\n     11 test_dataset_enhancers = load_dataset(\r\n     12         \"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\",\r\n     13         dataset_name,\r\n     14         split=\"test\",\r\n     15         streaming= False,\r\n     16     )\r\n\r\nFile /public/home/hhl/miniconda3/envs/transformer/lib/python3.9/site-packages/datasets/load.py:2129, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2124 verification_mode = VerificationMode(\r\n   2125     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\r\n   2126 )\r\n   2128 # Create a dataset builder\r\n-> 2129 builder_instance = load_dataset_builder(\r\n   2130     path=path,\r\n   2131     name=name,\r\n   2132     data_dir=data_dir,\r\n   2133     data_files=data_files,\r\n   2134     cache_dir=cache_dir,\r\n   2135     features=features,\r\n   2136     download_config=download_config,\r\n   2137     download_mode=download_mode,\r\n   2138     revision=revision,\r\n   2139     token=token,\r\n   2140     storage_options=storage_options,\r\n   2141     trust_remote_code=trust_remote_code,\r\n   2142     _require_default_config_name=name is None,\r\n   2143     **config_kwargs,\r\n   2144 )\r\n   2146 # Return iterable dataset in case of streaming\r\n   2147 if streaming:\r\n\r\nFile /public/home/hhl/miniconda3/envs/transformer/lib/python3.9/site-packages/datasets/load.py:1886, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\r\n   1884 builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\r\n   1885 # Instantiate the dataset builder\r\n-> 1886 builder_instance: DatasetBuilder = builder_cls(\r\n   1887     cache_dir=cache_dir,\r\n   1888     dataset_name=dataset_name,\r\n   1889     config_name=config_name,\r\n   1890     data_dir=data_dir,\r\n   1891     data_files=data_files,\r\n   1892     hash=dataset_module.hash,\r\n   1893     info=info,\r\n   1894     features=features,\r\n   1895     token=token,\r\n   1896     storage_options=storage_options,\r\n   1897     **builder_kwargs,\r\n   1898     **config_kwargs,\r\n   1899 )\r\n   1900 builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\r\n   1902 return builder_instance\r\n\r\nTypeError: 'NoneType' object is not callable\r\n```\r\n\r\nI have checked my internet, it worked well. And the dataset name was just copied from the Hugging Face. \r\nTotally no idea what is wrong! \n\n### Steps to reproduce the bug\n\nTo reproduce the bug you may run\r\n\r\n```\r\nfrom datasets import load_dataset, Dataset\r\n\r\n# Load the enhancers dataset from the InstaDeep Hugging Face ressources\r\ndataset_name = \"enhancers_types\"\r\ntrain_dataset_enhancers = load_dataset(\r\n        \"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\",\r\n        dataset_name,\r\n        split=\"train\",\r\n        streaming= False,\r\n    )\r\ntest_dataset_enhancers = load_dataset(\r\n        \"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\",\r\n        dataset_name,\r\n        split=\"test\",\r\n        streaming= False,\r\n    )\r\n```\n\n### Expected behavior\n\n1. what may be the reasons of the error\r\n2. how can I fine which reason lead to the error\r\n3. how can I save the problem\n\n### Environment info\n\n```\r\n- `datasets` version: 3.2.0\r\n- Platform: Linux-5.15.0-117-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.21\r\n- `huggingface_hub` version: 0.27.0\r\n- PyArrow version: 18.1.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.9.0\r\n```",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! I couldn't reproduce on my side, can you try deleting your cache at `~/.cache/huggingface/modules/datasets_modules/datasets/InstaDeepAI--nucleotide_transformer_downstream_tasks_revised` and try again ? For some reason `datasets` wasn't able to find the DatasetBuilder class in the python script of this dataset"
      },
      {
        "user": "Ray-Eldath",
        "body": "I've met the same problem when importing [LongBench-v1](https://github.com/THUDM/LongBench/blob/main/LongBench/README.md). the debugger reports `dataset_module.builder_configs_parameters.builder_configs` as `None` so that no `builder_cls` gets created:\n\n<img width=\"711\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b62bdea7-442b-47dc-b892-87f4d235e324\" />\n\ndoes this mean that I need to downgrade `datasets`?"
      },
      {
        "user": "Ray-Eldath",
        "body": "I tried downgrading `datasets` to v2.20.0 and it works fine now...\n\nI think there might be some compatibility issues during code updates between `v2.20.0` and `v3.0.0` 🤔 \n\nalso I suggest @nanu23333 to see if downgrading works."
      }
    ]
  },
  {
    "issue_number": 7415,
    "title": "Shard Dataset at specific indices",
    "author": "nikonikolov",
    "state": "open",
    "created_at": "2025-02-20T10:43:10Z",
    "updated_at": "2025-02-24T11:06:45Z",
    "labels": [],
    "body": "I have a dataset of sequences, where each example in the sequence is a separate row in the dataset (similar to LeRobotDataset). When running `Dataset.save_to_disk` how can I provide indices where it's possible to shard the dataset such that no episode spans more than 1 shard. Consequently, when I run `Dataset.load_from_disk`, how can I load just a subset of the shards to save memory and time on different ranks?\n\nI guess an alternative to this would be, given a loaded `Dataset`, how can I run `Dataset.shard` such that sharding doesn't split any episode across shards?",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! if it's an option I'd suggest to have one sequence per row instead.\n\nOtherwise you'd have to make your own save/load mechanism"
      },
      {
        "user": "nikonikolov",
        "body": "Saving one sequence per row is very difficult and heavy and makes all the optimizations pointless. How would a custom save/load mechanism look like?"
      },
      {
        "user": "lhoestq",
        "body": "You can use `pyarrow` for example to save/load individual arrow or parquet files and control what they contain"
      }
    ]
  },
  {
    "issue_number": 7222,
    "title": "TypeError: Couldn't cast array of type string to null in long json",
    "author": "nokados",
    "state": "open",
    "created_at": "2024-10-12T08:14:59Z",
    "updated_at": "2025-02-23T13:01:47Z",
    "labels": [],
    "body": "### Describe the bug\n\nIn general, changing the type from string to null is allowed within a dataset — there are even examples of this in the documentation.\r\n\r\nHowever, if the dataset is large and unevenly distributed, this allowance stops working. The schema gets locked in after reading a chunk. \r\n\r\nConsequently, if all values in the first chunk of a field are, for example, null, the field will be locked as type null, and if a string appears in that field in the second chunk, it will trigger this error:\r\n\r\n<details> \r\n  <summary>Traceback </summary>\r\n\r\n```\r\n   TypeError                                 Traceback (most recent call last)\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1868                     try:\r\n-> 1869                         writer.write_table(table)\r\n   1870                     except CastError as cast_error:\r\n\r\n14 frames\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py](https://localhost:8080/#) in write_table(self, pa_table, writer_batch_size)\r\n    579         pa_table = pa_table.combine_chunks()\r\n--> 580         pa_table = table_cast(pa_table, self._schema)\r\n    581         if self.embed_local_files:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in table_cast(table, schema)\r\n   2291     if table.schema != schema:\r\n-> 2292         return cast_table_to_schema(table, schema)\r\n   2293     elif table.schema.metadata != schema.metadata:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in cast_table_to_schema(table, schema)\r\n   2244         )\r\n-> 2245     arrays = [\r\n   2246         cast_array_to_feature(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in <listcomp>(.0)\r\n   2245     arrays = [\r\n-> 2246         cast_array_to_feature(\r\n   2247             table[name] if name in table_column_names else pa.array([None] * len(table), type=schema.field(name).type),\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in wrapper(array, *args, **kwargs)\r\n   1794         if isinstance(array, pa.ChunkedArray):\r\n-> 1795             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1796         else:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in <listcomp>(.0)\r\n   1794         if isinstance(array, pa.ChunkedArray):\r\n-> 1795             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1796         else:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in cast_array_to_feature(array, feature, allow_primitive_to_str, allow_decimal_to_str)\r\n   2101     elif not isinstance(feature, (Sequence, dict, list, tuple)):\r\n-> 2102         return array_cast(\r\n   2103             array,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in wrapper(array, *args, **kwargs)\r\n   1796         else:\r\n-> 1797             return func(array, *args, **kwargs)\r\n   1798 \r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in array_cast(array, pa_type, allow_primitive_to_str, allow_decimal_to_str)\r\n   1947         if pa.types.is_null(pa_type) and not pa.types.is_null(array.type):\r\n-> 1948             raise TypeError(f\"Couldn't cast array of type {_short_str(array.type)} to {_short_str(pa_type)}\")\r\n   1949         return array.cast(pa_type)\r\n\r\nTypeError: Couldn't cast array of type string to null\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\n\r\n[<ipython-input-353-e02f83980611>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 dd = load_dataset(\"json\", data_files=[\"TEST.json\"])\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2094 \r\n   2095     # Download and prepare data\r\n-> 2096     builder_instance.download_and_prepare(\r\n   2097         download_config=download_config,\r\n   2098         download_mode=download_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    922                     if num_proc is not None:\r\n    923                         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 924                     self._download_and_prepare(\r\n    925                         dl_manager=dl_manager,\r\n    926                         verification_mode=verification_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n    997             try:\r\n    998                 # Prepare split will record examples associated to the split\r\n--> 999                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n   1000             except OSError as e:\r\n   1001                 raise OSError(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\r\n   1738             job_id = 0\r\n   1739             with pbar:\r\n-> 1740                 for job_id, done, content in self._prepare_split_single(\r\n   1741                     gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1742                 ):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1894             if isinstance(e, DatasetGenerationError):\r\n   1895                 raise\r\n-> 1896             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1897 \r\n   1898         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n</details>\n\n### Steps to reproduce the bug\n\n```python\r\nimport json\r\nfrom datasets import load_dataset\r\n\r\nwith open(\"TEST.json\", \"w\") as f:\r\n    row = {\"ballast\": \"qwerty\" * 1000, \"b\": None}\r\n    row_str = json.dumps(row) + \"\\n\"\r\n    line_size = len(row_str)\r\n    chunk_size = 10 << 20\r\n    lines_in_chunk = chunk_size // line_size + 1\r\n    print(f\"Writing {lines_in_chunk} lines\")\r\n    for i in range(lines_in_chunk):\r\n        f.write(row_str)\r\n    null_row = {\"ballast\": \"Gotcha\", \"b\": \"Not Null\"}\r\n    f.write(json.dumps(null_row) + \"\\n\")\r\n\r\nload_dataset(\"json\", data_files=[\"TEST.json\"])\r\n```\n\n### Expected behavior\n\nConcatenation of the chunks without errors\n\n### Environment info\n\n- `datasets` version: 3.0.1\r\n- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.24.7\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.6.1",
    "comments": [
      {
        "user": "KurtMica",
        "body": "I am encountering this same issue. It seems that the library manages to recognise an optional column (but not **exclusively** null) if there is at least one non-null instance within the same file. For example, given a `test_0.jsonl` file:\r\n```json\r\n{\"a\": \"a1\", \"b\": \"b1\", \"c\": null, \"d\": null}\r\n{\"a\": \"a2\", \"b\": null, \"c\": \"c2\", \"d\": null}\r\n```\r\nthe data is correctly loaded, recognising that columns `b` & `c` are optional, while `d` is null.\r\n```python\r\n{'a': ['a1', 'a2'], 'b': ['b1', None], 'c': [None, 'c2'], 'd': [None, None]}\r\n```\r\n\r\nBut if the `config` has another file, say `test_1.jsonl` where `d` now has some non-null values:\r\n```json\r\n{\"a\": null, \"b\": \"b3\", \"c\": \"c3\", \"d\": \"d3\"}\r\n{\"a\": \"a4\", \"b\": \"b4\", \"c\": null, \"d\": null}\r\n```\r\nthen, an error is raised:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1869                     try:\r\n-> 1870                         writer.write_table(table)\r\n   1871                     except CastError as cast_error:\r\n\r\n14 frames\r\n\r\nTypeError: Couldn't cast array of type string to null\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1895             if isinstance(e, DatasetGenerationError):\r\n   1896                 raise\r\n-> 1897             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1898 \r\n   1899         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n---\r\n\r\nI have created a [sample repository](https://huggingface.co/datasets/KurtMica/optional_columns_mutiple_files) if that helps. Interestingly, the dataset viewer correctly shows the data across files, although it still indicates the above error."
      },
      {
        "user": "KurtMica",
        "body": " Managed to find a workaround, by [specifying the features explicitly](https://huggingface.co/docs/datasets/main/en/loading#specify-features), which is also possible to do directly using the [YAML file configuration](https://discuss.huggingface.co/t/appropriate-yaml-for-dataset-info-list-float/74418)."
      },
      {
        "user": "renweizhukov",
        "body": "I hit the same issue for `datasets 3.2.0`. Given the two jsonl files with the same content but different ordering, `load_dataset` worked for one but did not work for the other.\n\n```\nfrom datasets import load_dataset\n\nissues_dataset = load_dataset(\n    \"json\", data_files=\"NeMo-issues-fixed.jsonl\", split=\"train\"\n)\nissues_dataset\n```\n\nFor [NeMo-issues.jsonl](https://github.com/renweizhukov/jupyter-lab-notebook/blob/main/hugging-face-nlp-course/NeMo-issues.jsonl), I got an exception:\n\n```\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py:1870](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py#line=1869), in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\n   1869 try:\n-> 1870     writer.write_table(table)\n   1871 except CastError as cast_error:\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/arrow_writer.py:622](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/arrow_writer.py#line=621), in ArrowWriter.write_table(self, pa_table, writer_batch_size)\n    621 pa_table = pa_table.combine_chunks()\n--> 622 pa_table = table_cast(pa_table, self._schema)\n    623 if self.embed_local_files:\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py:2292](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py#line=2291), in table_cast(table, schema)\n   2291 if table.schema != schema:\n-> 2292     return cast_table_to_schema(table, schema)\n   2293 elif table.schema.metadata != schema.metadata:\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py:2246](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py#line=2245), in cast_table_to_schema(table, schema)\n   2240     raise CastError(\n   2241         f\"Couldn't cast\\n{_short_str(table.schema)}\\nto\\n{_short_str(features)}\\nbecause column names don't match\",\n   2242         table_column_names=table.column_names,\n   2243         requested_column_names=list(features),\n   2244     )\n   2245 arrays = [\n-> 2246     cast_array_to_feature(\n   2247         table[name] if name in table_column_names else pa.array([None] * len(table), type=schema.field(name).type),\n   2248         feature,\n   2249     )\n   2250     for name, feature in features.items()\n   2251 ]\n   2252 return pa.Table.from_arrays(arrays, schema=schema)\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py:1795](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py#line=1794), in _wrap_for_chunked_arrays.<locals>.wrapper(array, *args, **kwargs)\n   1794 if isinstance(array, pa.ChunkedArray):\n-> 1795     return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\n   1796 else:\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py:2102](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py#line=2101), in cast_array_to_feature(array, feature, allow_primitive_to_str, allow_decimal_to_str)\n   2101 elif not isinstance(feature, (Sequence, dict, list, tuple)):\n-> 2102     return array_cast(\n   2103         array,\n   2104         feature(),\n   2105         allow_primitive_to_str=allow_primitive_to_str,\n   2106         allow_decimal_to_str=allow_decimal_to_str,\n   2107     )\n   2108 raise TypeError(f\"Couldn't cast array of type\\n{_short_str(array.type)}\\nto\\n{_short_str(feature)}\")\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py:1797](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py#line=1796), in _wrap_for_chunked_arrays.<locals>.wrapper(array, *args, **kwargs)\n   1796 else:\n-> 1797     return func(array, *args, **kwargs)\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py:1948](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/table.py#line=1947), in array_cast(array, pa_type, allow_primitive_to_str, allow_decimal_to_str)\n   1947 if pa.types.is_null(pa_type) and not pa.types.is_null(array.type):\n-> 1948     raise TypeError(f\"Couldn't cast array of type {_short_str(array.type)} to {_short_str(pa_type)}\")\n   1949 return array.cast(pa_type)\n\nTypeError: Couldn't cast array of type string to null\n\nThe above exception was the direct cause of the following exception:\n\nDatasetGenerationError                    Traceback (most recent call last)\nCell In[73], line 3\n      1 from datasets import load_dataset\n----> 3 issues_dataset = load_dataset(\n      4     \"json\", data_files=\"NeMo-issues.jsonl\", split=\"train\"\n      5 )\n      6 issues_dataset\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/load.py:2151](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/load.py#line=2150), in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\n   2148     return builder_instance.as_streaming_dataset(split=split)\n   2150 # Download and prepare data\n-> 2151 builder_instance.download_and_prepare(\n   2152     download_config=download_config,\n   2153     download_mode=download_mode,\n   2154     verification_mode=verification_mode,\n   2155     num_proc=num_proc,\n   2156     storage_options=storage_options,\n   2157 )\n   2159 # Build dataset for splits\n   2160 keep_in_memory = (\n   2161     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\n   2162 )\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py:924](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py#line=923), in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\n    922 if num_proc is not None:\n    923     prepare_split_kwargs[\"num_proc\"] = num_proc\n--> 924 self._download_and_prepare(\n    925     dl_manager=dl_manager,\n    926     verification_mode=verification_mode,\n    927     **prepare_split_kwargs,\n    928     **download_and_prepare_kwargs,\n    929 )\n    930 # Sync info\n    931 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py:1000](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py#line=999), in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\n    996 split_dict.add(split_generator.split_info)\n    998 try:\n    999     # Prepare split will record examples associated to the split\n-> 1000     self._prepare_split(split_generator, **prepare_split_kwargs)\n   1001 except OSError as e:\n   1002     raise OSError(\n   1003         \"Cannot find data file. \"\n   1004         + (self.manual_download_instructions or \"\")\n   1005         + \"\\nOriginal erro[r:\\n](file:///R:/n)\"\n   1006         + str(e)\n   1007     ) from None\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py:1741](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py#line=1740), in ArrowBasedBuilder._prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\n   1739 job_id = 0\n   1740 with pbar:\n-> 1741     for job_id, done, content in self._prepare_split_single(\n   1742         gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\n   1743     ):\n   1744         if done:\n   1745             result = content\n\nFile [~/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py:1897](http://localhost:8888/home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages/datasets/builder.py#line=1896), in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\n   1895     if isinstance(e, DatasetGenerationError):\n   1896         raise\n-> 1897     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\n   1899 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\n\nDatasetGenerationError: An error occurred while generating the dataset\n```\n\nFor [NeMo-issues-fixed.json](https://github.com/renweizhukov/jupyter-lab-notebook/blob/main/hugging-face-nlp-course/NeMo-issues-fixed.jsonl) which consists of the last 1000 lines and then the first 9000 lines of NeMo-issues.jsonl, I could load the data:\n\n```\nDataset({\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'sub_issues_summary', 'active_lock_reason', 'draft', 'pull_request', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason'],\n    num_rows: 10000\n})\n```"
      }
    ]
  },
  {
    "issue_number": 6393,
    "title": "Filter occasionally hangs",
    "author": "dakinggg",
    "state": "closed",
    "created_at": "2023-11-09T06:18:30Z",
    "updated_at": "2025-02-22T00:49:19Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nA call to `.filter` occasionally hangs (after the filter is complete, according to tqdm)\r\n\r\nThere is a trace produced\r\n```\r\nException ignored in: <function Dataset.__del__ at 0x7efb48130c10>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/datasets/arrow_dataset.py\", line 1366, in __del__\r\n    if hasattr(self, \"_indices\"):\r\n  File \"/usr/lib/python3/dist-packages/composer/core/engine.py\", line 123, in sigterm_handler\r\n    sys.exit(128 + signal)\r\nSystemExit: 143\r\n```\r\n\r\nbut I'm not sure if the trace is actually from `datasets`, or from surrounding code that is trying to clean up after datasets gets stuck.\r\n\r\nUnfortunately I can't reproduce this issue anywhere close to reliably. It happens infrequently when using `num_procs > 1`. Anecdotally I started seeing it when using larger datasets (~10M samples).\r\n\r\n### Steps to reproduce the bug\r\n\r\nN/A see description\r\n\r\n### Expected behavior\r\n\r\nmap/filter calls always complete sucessfully\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.4.0-137-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.2",
    "comments": [
      {
        "user": "dakinggg",
        "body": "It looks like I may not be the first to encounter this: https://github.com/huggingface/datasets/issues/3172"
      },
      {
        "user": "dakinggg",
        "body": "Adding some more information, it seems to occur more frequently with large (millions of samples) datasets."
      },
      {
        "user": "dakinggg",
        "body": "More information. My code is structured as (1) load (2) map (3) filter (4) filter. It was always the second filter that failed. Combining the two filters into one seems to reliably work."
      }
    ]
  },
  {
    "issue_number": 6734,
    "title": "Tokenization slows towards end of dataset",
    "author": "ethansmith2000",
    "state": "open",
    "created_at": "2024-03-15T03:27:36Z",
    "updated_at": "2025-02-20T17:40:54Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nMapped tokenization slows down substantially towards end of dataset.\r\n\r\ntrain set started off very slow, caught up to 20k then tapered off til the end.\r\n\r\nwhat's particularly strange is that the tokenization crashed a few times before due to errors with invalid tokens somewhere or corrupted downloads, and the speed ups/downs consistently happened the same times\r\n\r\n```bash\r\nRunning tokenizer on dataset (num_proc=48):   0%|          | 847000/881416735 [12:18<252:45:45, 967.72 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):   0%|          | 848000/881416735 [12:19<224:16:10, 1090.66 examples/s]\r\n\r\nRunning tokenizer on dataset (num_proc=48):  10%|▉         | 84964000/881416735 [3:48:00<11:21:34, 19476.01 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  10%|▉         | 84967000/881416735 [3:48:00<12:04:01, 18333.79 examples/s]\r\n\r\nRunning tokenizer on dataset (num_proc=48):  61%|██████    | 538631977/881416735 [13:46:40<27:50:04, 3420.84 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  61%|██████    | 538632977/881416735 [13:46:40<23:48:20, 3999.77 examples/s]\r\n\r\nRunning tokenizer on dataset (num_proc=48): 100%|█████████▉| 881365886/881416735 [38:30:19<04:34, 185.10 examples/s]\r\nRunning tokenizer on dataset (num_proc=48): 100%|█████████▉| 881366886/881416735 [38:30:25<04:36, 180.57 examples/s]\r\n```\r\n\r\nand validation set as well\r\n\r\n```bash\r\nRunning tokenizer on dataset (num_proc=48):  90%|████████▉ | 41544000/46390354 [28:44<02:37, 30798.76 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  90%|████████▉ | 41550000/46390354 [28:44<02:08, 37698.08 examples/s]\r\n\r\nRunning tokenizer on dataset (num_proc=48):  96%|█████████▋| 44747422/46390354 [2:15:48<12:22:44, 36.87 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  96%|█████████▋| 44747422/46390354 [2:16:00<12:22:44, 36.87 examples/s]\r\n\r\n```\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n    using the following kwargs\r\n    \r\n```python\r\nwith accelerator.main_process_first():\r\n        lm_datasets = tokenized_datasets.map(\r\n            group_texts,\r\n            batched=True,\r\n            num_proc=48\r\n            load_from_cache_file=True,\r\n            desc=f\"Grouping texts in chunks of {block_size}\",\r\n        )\r\n```\r\n\r\nrunning through slurm script \r\n```bash\r\n#SBATCH --partition=gpu-nvidia-a100\r\n#SBATCH --nodes=1\r\n#SBATCH --ntasks=1\r\n#SBATCH --gpus-per-task=8\r\n#SBATCH --cpus-per-task=96\r\n```\r\n\r\nusing this dataset https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\r\n\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nConstant speed throughout\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.15.0-1049-aws-x86_64-with-glibc2.10\r\n- Python version: 3.8.18\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.10.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! First note that if the dataset is not heterogeneous / shuffled, there might be places in the data with shorter texts that are faster to tokenize.\r\n\r\nMoreover, the way `num_proc` works is by slicing the dataset and passing each slice to a process to run the `map()` function. So at the very end of `map()`, some processes might have finished transforming their slice of data while others are still running, causing the throughput to become lower."
      },
      {
        "user": "ethansmith2000",
        "body": "I did see some comments about how num_proc=None could help and outputting numpy arrays can also help in the docs, but this seems quite odd now dropping down to 1it/s\r\n\r\n```bash\r\nRunning tokenizer on dataset (num_proc=48):  99%|█████████▉| 46048888/46390354 [12:33:30<4:20:32, 21.84 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  99%|█████████▉| 46049888/46390354 [12:36:11<8:37:59, 10.95 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  99%|█████████▉| 46050888/46390354 [12:46:35<24:56:56,  3.78 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  99%|█████████▉| 46051888/46390354 [12:56:43<35:08:10,  2.68 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  99%|█████████▉| 46052888/46390354 [13:06:58<42:05:41,  2.23 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  99%|█████████▉| 46053888/46390354 [13:16:01<44:40:18,  2.09 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  99%|█████████▉| 46054888/46390354 [13:25:11<46:35:28,  2.00 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  99%|█████████▉| 46055888/46390354 [13:34:23<47:55:34,  1.94 examples/s]\r\n```\r\n\r\n"
      },
      {
        "user": "lsh0520",
        "body": "@ethansmith2000 Hi, did you solve this problem? I'm strugging with the same problem now."
      }
    ]
  },
  {
    "issue_number": 7413,
    "title": "Documentation on multiple media files of the same type with WebDataset",
    "author": "DCNemesis",
    "state": "open",
    "created_at": "2025-02-18T16:13:20Z",
    "updated_at": "2025-02-20T14:17:54Z",
    "labels": [],
    "body": "The [current documentation](https://huggingface.co/docs/datasets/en/video_dataset) on a creating a video dataset includes only examples with one media file and one json. It would be useful to have examples where multiple files of the same type are included. For example, in a sign language dataset, you may have a base video and a video annotation of the extracted pose. According to the WebDataset documentation, this should be able to be done with period separated filenames. For example:\n\n```e39871fd9fd74f55.base.mp4\ne39871fd9fd74f55.pose.mp4\ne39871fd9fd74f55.json\nf18b91585c4d3f3e.base.mp4\nf18b91585c4d3f3e.pose.mp4\nf18b91585c4d3f3e.json\n... \n```\n\nIf you can confirm that this method of including multiple media files works with huggingface datasets and include an example in the documentation, I'd appreciate it.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Yes this is correct and it works with huggingface datasets as well ! Feel free to include an example here: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx"
      }
    ]
  },
  {
    "issue_number": 7399,
    "title": "Synchronize parameters for various datasets",
    "author": "grofte",
    "state": "open",
    "created_at": "2025-02-14T09:15:11Z",
    "updated_at": "2025-02-19T11:50:29Z",
    "labels": [],
    "body": "### Describe the bug\n\n[IterableDatasetDict](https://huggingface.co/docs/datasets/v3.2.0/en/package_reference/main_classes#datasets.IterableDatasetDict.map) map function is missing the `desc` parameter. You can see the equivalent map function for [Dataset here](https://huggingface.co/docs/datasets/v3.2.0/en/package_reference/main_classes#datasets.Dataset.map).\n\nThere might be other parameters missing - I haven't checked.\n\n### Steps to reproduce the bug\n\nfrom datasets import Dataset, IterableDataset, IterableDatasetDict\n\nds = IterableDatasetDict({\"train\": Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=3), \n                          \"validate\": Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=3)})\n\nfor d in ds[\"train\"]:\n    print(d)\n\nds = ds.map(lambda x: {k: v+1 for k, v in x.items()}, desc=\"increment\")\n\nfor d in ds[\"train\"]:\n    print(d)\n\n### Expected behavior\n\nThe description parameter should be available for all datasets (or none). \n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 17.0.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2024.9.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! the `desc` parameter is only available for Dataset / DatasetDict for the progress bar of `map()``\n\nSince IterableDataset only runs the map functions when you iterate over the dataset, there is no progress bar and `desc` is useless. We could still add the argument for parity but it wouldn't be used for anything"
      },
      {
        "user": "grofte",
        "body": "I think you should add it. It doesn't hurt. The reason I ran into it was because I re-wrote a pipeline to use either a stream or a fully loaded dataset. Of course I can simply remove it but it is nice to have on the memory loaded dataset. "
      }
    ]
  },
  {
    "issue_number": 2869,
    "title": "TypeError: 'NoneType' object is not callable",
    "author": "Chenfei-Kang",
    "state": "closed",
    "created_at": "2021-09-03T11:27:39Z",
    "updated_at": "2025-02-19T09:57:34Z",
    "labels": [
      "bug"
    ],
    "body": "## Describe the bug\r\n\r\nTypeError: 'NoneType' object is not callable\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\ndataset = datasets.load_dataset(\"glue\", 'cola')\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform:\r\n- Python version: 3.7\r\n- PyArrow version:\r\n",
    "comments": [
      {
        "user": "albertvillanova",
        "body": "Hi, @Chenfei-Kang.\r\n\r\nI'm sorry, but I'm not able to reproduce your bug:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"glue\", 'cola')\r\nds\r\n```\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 8551\r\n    })\r\n    validation: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 1043\r\n    })\r\n    test: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 1063\r\n    })\r\n})\r\n```\r\n\r\nCould you please give more details and environment info (platform, PyArrow version)?"
      },
      {
        "user": "Chenfei-Kang",
        "body": "> Hi, @Chenfei-Kang.\r\n> \r\n> I'm sorry, but I'm not able to reproduce your bug:\r\n> \r\n> ```python\r\n> from datasets import load_dataset\r\n> \r\n> ds = load_dataset(\"glue\", 'cola')\r\n> ds\r\n> ```\r\n> \r\n> ```\r\n> DatasetDict({\r\n>     train: Dataset({\r\n>         features: ['sentence', 'label', 'idx'],\r\n>         num_rows: 8551\r\n>     })\r\n>     validation: Dataset({\r\n>         features: ['sentence', 'label', 'idx'],\r\n>         num_rows: 1043\r\n>     })\r\n>     test: Dataset({\r\n>         features: ['sentence', 'label', 'idx'],\r\n>         num_rows: 1063\r\n>     })\r\n> })\r\n> ```\r\n> \r\n> Could you please give more details and environment info (platform, PyArrow version)?\r\n\r\nSorry to reply you so late.\r\nplatform: pycharm 2021 + anaconda with python 3.7\r\nPyArrow version: 5.0.0\r\nhuggingface-hub: 0.0.16\r\ndatasets: 1.9.0\r\n"
      },
      {
        "user": "albertvillanova",
        "body": "- For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\r\n- In relation with the error, you just gave us the error type and message (`TypeError: 'NoneType' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?"
      }
    ]
  },
  {
    "issue_number": 7412,
    "title": "Index Error Invalid Ket is out of bounds for size 0 for code-search-net/code_search_net dataset",
    "author": "harshakhmk",
    "state": "open",
    "created_at": "2025-02-18T05:58:33Z",
    "updated_at": "2025-02-18T06:42:07Z",
    "labels": [],
    "body": "### Describe the bug\n\nI am trying to do model pruning on sentence-transformers/all-mini-L6-v2 for the code-search-net/code_search_net dataset using INCTrainer class \nHowever I am getting below error\n\n```\nraise IndexError(f\"Invalid Key: {key is our of bounds for size {size}\")\nIndexError: Invalid key: 1840208 is out of bounds for size 0\n```\n\n### Steps to reproduce the bug\n\nModel pruning on the above dataset using the below guide \n\nhttps://huggingface.co/docs/optimum/en/intel/neural_compressor/optimization#pruning\n### Expected behavior\n\nThe modsl should be successfully pruned \n\n### Environment info\n\nTorch version: 2.4.1\nPython version: 3.8.10",
    "comments": []
  },
  {
    "issue_number": 7404,
    "title": "Performance regression in `dataset.filter`",
    "author": "ttim",
    "state": "closed",
    "created_at": "2025-02-16T22:19:14Z",
    "updated_at": "2025-02-17T17:46:06Z",
    "labels": [],
    "body": "### Describe the bug\n\nWe're filtering dataset of ~1M (small-ish) records. At some point in the code we do `dataset.filter`, before (including 3.2.0) it was taking couple of seconds, and now it takes 4 hours.\n\nWe use 16 threads/workers, and stack trace at them look as follows:\n```\nTraceback (most recent call last):\n  File \"/python/lib/python3.12/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/python/lib/python3.12/site-packages/multiprocess/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/python/lib/python3.12/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/python/lib/python3.12/site-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/python/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3511, in _map_single\n    for i, batch in iter_outputs(shard_iterable):\n  File \"/python/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3461, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/python/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3390, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/python/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 6416, in get_indices_from_mask_function\n    indices_array = indices_mapping.column(0).take(indices_array)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/table.pxi\", line 1079, in pyarrow.lib.ChunkedArray.take\n  File \"/python/lib/python3.12/site-packages/pyarrow/compute.py\", line 458, in take\n    def take(data, indices, *, boundscheck=True, memory_pool=None):\n```\n\n### Steps to reproduce the bug\n\n1. Save dataset of 1M records in arrow\n2. Filter it with 16 threads\n3. Watch it take too long\n\n### Expected behavior\n\nFiltering done fast\n\n### Environment info\n\ndatasets 3.3.0, python 3.12",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Thanks for reporting, I'll fix the regression today"
      },
      {
        "user": "lhoestq",
        "body": "I just released `datasets` 3.3.1 with a fix, let me know if it's good now :)"
      },
      {
        "user": "ttim",
        "body": "@lhoestq it fixed the issue.\n\nThis was (very) fast, thank you very much!"
      }
    ]
  },
  {
    "issue_number": 7405,
    "title": "Lazy loading of environment variables",
    "author": "nikvaessen",
    "state": "open",
    "created_at": "2025-02-16T22:31:41Z",
    "updated_at": "2025-02-17T15:17:18Z",
    "labels": [],
    "body": "### Describe the bug\n\nLoading a `.env` file after an `import datasets` call does not correctly use the environment variables. \n\nThis is due the fact that environment variables are read at import time:\nhttps://github.com/huggingface/datasets/blob/de062f0552a810c52077543c1169c38c1f0c53fc/src/datasets/config.py#L155C1-L155C80\n\n### Steps to reproduce the bug\n\n```bash\n# make tmp dir\nmkdir -p /tmp/debug-env\n# make .env file\necho HF_HOME=/tmp/debug-env/data > /tmp/debug-env/.env\n# first load dotenv, downloads to /tmp/debug-env/data\nuv run --with datasets,python-dotenv python3 -c \\\n'import dotenv; dotenv.load_dotenv(\"/tmp/debug-env/.env\"); import datasets; datasets.load_dataset(\"Anthropic/hh-rlhf\")' \n# first import datasets, downloads to `~/.cache/huggingface`\nuv run --with datasets,python-dotenv python3 -c \\\n'import datasets; import dotenv; dotenv.load_dotenv(\"/tmp/debug-env/.env\"); datasets.load_dataset(\"Anthropic/hh-rlhf\")' \n\n```\n\n### Expected behavior\n\nI expect that setting environment variables with something like this:\n\n```python3\nif __name__ == \"__main__\":\n    load_dotenv()\n    main()\n```\nworks correctly. \n\n### Environment info\n\n    \"datasets>=3.3.0\",\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Many python packages out there, including `huggingface_hub`, do load the environment variables on import.\nYou should `load_dotenv()` before importing the libraries.\n\nFor example you can move all you imports inside your `main()` function"
      }
    ]
  },
  {
    "issue_number": 4114,
    "title": "Allow downloading just some columns of a dataset",
    "author": "osanseviero",
    "state": "open",
    "created_at": "2022-04-06T16:38:46Z",
    "updated_at": "2025-02-17T15:10:56Z",
    "labels": [
      "enhancement"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\nSome people are interested in doing label analysis of a CV dataset without downloading all the images. Downloading the whole dataset does not always makes sense for this kind of use case\r\n\r\n**Describe the solution you'd like**\r\nBe able to just download some columns of a dataset, such as doing\r\n```python\r\nload_dataset(\"huggan/wikiart\",columns=[\"artist\", \"genre\"])\r\n```\r\n\r\nAlthough this might make things a bit complicated in terms of local caching of datasets.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "In the general case you can’t always reduce the quantity of data to download, since you can’t parse CSV or JSON data without downloading the whole files right ? ^^ However we could explore this case-by-case I guess"
      },
      {
        "user": "osanseviero",
        "body": "Actually for csv pandas has `usecols` which allows loading a subset of columns in a more efficient way afaik, but yes, you're right this might be more complex than I thought."
      },
      {
        "user": "lukasugar",
        "body": "Bumping the visibility of this :) Is there a recommended way of doing this?"
      }
    ]
  },
  {
    "issue_number": 5990,
    "title": "Pushing a large dataset on the hub consistently hangs",
    "author": "AntreasAntoniou",
    "state": "open",
    "created_at": "2023-06-10T14:46:47Z",
    "updated_at": "2025-02-15T09:29:10Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nOnce I have locally built a large dataset that I want to push to hub, I use the recommended approach of .push_to_hub to get the dataset on the hub, and after pushing a few shards, it consistently hangs. This has happened over 40 times over the past week, and despite my best efforts to try and catch this happening and kill a process and restart, it seems to be extremely time wasting -- so I came to you to report this and to seek help. \r\n\r\nI already tried installing hf_transfer, but it doesn't support Byte file uploads so I uninstalled it.\n\n### Reproduction\n\n```python\r\nimport multiprocessing as mp\r\nimport pathlib\r\nfrom math import ceil\r\n\r\nimport datasets\r\nimport numpy as np\r\nfrom tqdm.auto import tqdm\r\n\r\nfrom tali.data.data import select_subtitles_between_timestamps\r\nfrom tali.utils import load_json\r\n\r\ntali_dataset_dir = \"/data/\"\r\n\r\nif __name__ == \"__main__\":\r\n    full_dataset = datasets.load_dataset(\r\n        \"Antreas/TALI\", num_proc=mp.cpu_count(), cache_dir=tali_dataset_dir\r\n    )\r\n\r\n    def data_generator(set_name, percentage: float = 1.0):\r\n        dataset = full_dataset[set_name]\r\n\r\n        for item in tqdm(dataset):\r\n            video_list = item[\"youtube_content_video\"]\r\n            video_list = np.random.choice(\r\n                video_list, int(ceil(len(video_list) * percentage))\r\n            )\r\n            if len(video_list) == 0:\r\n                continue\r\n            captions = item[\"youtube_subtitle_text\"]\r\n            captions = select_subtitles_between_timestamps(\r\n                subtitle_dict=load_json(\r\n                    captions.replace(\r\n                        \"/data/\",\r\n                        tali_dataset_dir,\r\n                    )\r\n                ),\r\n                starting_timestamp=0,\r\n                ending_timestamp=100000000,\r\n            )\r\n\r\n            for video_path in video_list:\r\n                temp_path = video_path.replace(\"/data/\", tali_dataset_dir)\r\n                video_path_actual: pathlib.Path = pathlib.Path(temp_path)\r\n\r\n                if video_path_actual.exists():\r\n                    item[\"youtube_content_video\"] = open(video_path_actual, \"rb\").read()\r\n                    item[\"youtube_subtitle_text\"] = captions\r\n                    yield item\r\n\r\n    train_generator = lambda: data_generator(\"train\", percentage=0.1)\r\n    val_generator = lambda: data_generator(\"val\")\r\n    test_generator = lambda: data_generator(\"test\")\r\n\r\n    train_data = datasets.Dataset.from_generator(\r\n        train_generator,\r\n        num_proc=mp.cpu_count(),\r\n        writer_batch_size=5000,\r\n        cache_dir=tali_dataset_dir,\r\n    )\r\n\r\n    val_data = datasets.Dataset.from_generator(\r\n        val_generator,\r\n        writer_batch_size=5000,\r\n        num_proc=mp.cpu_count(),\r\n        cache_dir=tali_dataset_dir,\r\n    )\r\n\r\n    test_data = datasets.Dataset.from_generator(\r\n        test_generator,\r\n        writer_batch_size=5000,\r\n        num_proc=mp.cpu_count(),\r\n        cache_dir=tali_dataset_dir,\r\n    )\r\n\r\n    dataset = datasets.DatasetDict(\r\n        {\r\n            \"train\": train_data,\r\n            \"val\": val_data,\r\n            \"test\": test_data,\r\n        }\r\n    )\r\n    succesful_competion = False\r\n    while not succesful_competion:\r\n        try:\r\n            dataset.push_to_hub(repo_id=\"Antreas/TALI-small\", max_shard_size=\"5GB\")\r\n            succesful_competion = True\r\n        except Exception as e:\r\n            print(e)\r\n```\n\n### Logs\n\n```shell\nPushing dataset shards to the dataset hub:  33%|██████████████████████████████████████▎                                                                            | 7/21 [24:33<49:06, 210.45s/it]\r\nError while uploading 'data/val-00007-of-00021-6b216a984af1a4c8.parquet' to the Hub.                                                                                                               \r\nPushing split train to the Hub.                                                                                                                                                                    \r\nResuming upload of the dataset shards.                                                                                                                                                             \r\nPushing dataset shards to the dataset hub: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [42:10<00:00, 55.01s/it]\r\nPushing split val to the Hub.                                                                                                                                                                      \r\nResuming upload of the dataset shards.                                                                                                                                                             \r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.55ba/s]\r\nUpload 1 LFS files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:23<00:00, 23.51s/it]\r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.39ba/s]\r\nUpload 1 LFS files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:30<00:00, 30.19s/it]\r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.28ba/s]\r\nUpload 1 LFS files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:24<00:00, 24.08s/it]\r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42ba/s]\r\nUpload 1 LFS files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:23<00:00, 23.97s/it]\r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49ba/s]\r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.54ba/s^\r\nUpload 1 LFS files:   0%|                                                                                                                                                    | 0/1 [04:42<?, ?it/s]\r\nPushing dataset shards to the dataset hub:  52%|████████████████████████████████████████████████████████████▏                                                      | 11/21 [17:23<15:48, 94.82s/it]\r\n\r\nThat's where it got stuck\n```\n\n\n### System info\n\n```shell\n- huggingface_hub version: 0.15.1\r\n- Platform: Linux-5.4.0-147-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.11\r\n- Running in iPython ?: No\r\n- Running in notebook ?: No\r\n- Running in Google Colab ?: No\r\n- Token path ?: /root/.cache/huggingface/token\r\n- Has saved token ?: True\r\n- Who am I ?: Antreas\r\n- Configured git credential helpers: store\r\n- FastAI: N/A\r\n- Tensorflow: N/A\r\n- Torch: 2.1.0.dev20230606+cu121\r\n- Jinja2: 3.1.2\r\n- Graphviz: N/A\r\n- Pydot: N/A\r\n- Pillow: 9.5.0\r\n- hf_transfer: N/A\r\n- gradio: N/A\r\n- numpy: 1.24.3\r\n- ENDPOINT: https://huggingface.co\r\n- HUGGINGFACE_HUB_CACHE: /root/.cache/huggingface/hub\r\n- HUGGINGFACE_ASSETS_CACHE: /root/.cache/huggingface/assets\r\n- HF_TOKEN_PATH: /root/.cache/huggingface/token\r\n- HF_HUB_OFFLINE: False\r\n- HF_HUB_DISABLE_TELEMETRY: False\r\n- HF_HUB_DISABLE_PROGRESS_BARS: None\r\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\r\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\r\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\r\n- HF_HUB_ENABLE_HF_TRANSFER: False\n```\n",
    "comments": [
      {
        "user": "Wauplin",
        "body": "Hi @AntreasAntoniou , sorry to know you are facing this issue. To help debugging it, could you tell me:\r\n- What is the total dataset size?\r\n- Is it always failing on the same shard or is the hanging problem happening randomly?\r\n- Were you able to save the dataset as parquet locally? This would help us determine if the problem comes from the upload or the file generation.\r\n\r\nI'm cc-ing @lhoestq who might have some insights from a `datasets` perspective."
      },
      {
        "user": "lhoestq",
        "body": "One trick that can also help is to check the traceback when you kill your python process: it will show where in the code it was hanging"
      },
      {
        "user": "AntreasAntoniou",
        "body": "Right. So I did the trick @lhoestq suggested. Here is where things seem to hang\r\n\r\n```\r\nError while uploading 'data/train-00120-of-00195-466c2dbab2eb9989.parquet' to the Hub.                                                                                                     \r\nPushing split train to the Hub.                                                                                                                                                            \r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.15s/ba]\r\nUpload 1 LFS files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:52<00:00, 52.12s/it]\r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.08s/ba]\r\nUpload 1 LFS files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:45<00:00, 45.54s/it]\r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.08s/ba]\r\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.03s/ba^Upload 1 LFS files:   0%|                                                                                                                                         | 0/1 [\r\n21:27:35<?, ?it/s]                                                                                                                                                                         \r\nPushing dataset shards to the dataset hub:  63%|█████████████████████████████████████████████████████████████▎                                    | 122/195 [23:37:11<14:07:59, 696.98s/it]\r\n^CError in sys.excepthook:                                                                                                                                                                 \r\nTraceback (most recent call last):                                                                                                                                                         \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/console.py\", line 1699, in print                                                                                            \r\n    extend(render(renderable, render_options))                                                                                                                                             \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/console.py\", line 1335, in render                                                                                           \r\n    yield from self.render(render_output, _options)                                                                                                                                        \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/console.py\", line 1331, in render                                                                                           \r\n    for render_output in iter_render:                                                                                                                                                      \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/constrain.py\", line 29, in __rich_console__                                                                                 \r\n    yield from console.render(self.renderable, child_options)                                                                                                                              \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/console.py\", line 1331, in render                                                                                           \r\n    for render_output in iter_render:                                                                                                                                                      \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/panel.py\", line 220, in __rich_console__                                                                                    \r\n    lines = console.render_lines(renderable, child_options, style=style)                                                                                                                   \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/console.py\", line 1371, in render_lines                                                                                     \r\n    lines = list(                                                                                                                                                                          \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/segment.py\", line 292, in split_and_crop_lines                                                                              \r\n    for segment in segments:                                                                                                                                                               \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/console.py\", line 1331, in render                                                                                           \r\n    for render_output in iter_render:                                                                                                                                                      \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/padding.py\", line 97, in __rich_console__                                                                                   \r\n    lines = console.render_lines(                                                                                                                                                          \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/console.py\", line 1371, in render_lines                                                                                     \r\n    lines = list(                                                                                                                                                                          \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/segment.py\", line 292, in split_and_crop_lines                                                                              \r\n    for segment in segments:                                                                                                                                                               \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/console.py\", line 1335, in render                                                                                           \r\n    yield from self.render(render_output, _options)                                                                                                                                        \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/console.py\", line 1331, in render                                                                                           \r\n    for render_output in iter_render:                                                                                                                                                      \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/syntax.py\", line 611, in __rich_console__                                                                                   \r\n    segments = Segments(self._get_syntax(console, options))                                                                                                                                \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/segment.py\", line 668, in __init__                                                                                          \r\n    self.segments = list(segments)                                                                                                                                                         \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/syntax.py\", line 674, in _get_syntax                                                                                        \r\n    lines: Union[List[Text], Lines] = text.split(\"\\n\", allow_blank=ends_on_nl)                                                                                                             \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/text.py\", line 1042, in split                                                                                               \r\n    lines = Lines(                                                                                                                                                                         \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/containers.py\", line 70, in __init__                                                                                        \r\n    self._lines: List[\"Text\"] = list(lines)                                                                                                                                                \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/text.py\", line 1043, in <genexpr>                                                                                           \r\n    line for line in self.divide(flatten_spans()) if line.plain != separator                                                                                                               \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/rich/text.py\", line 385, in plain                                                    \r\n    if len(self._text) != 1:                                                                                                                                                               \r\nKeyboardInterrupt                                                                                                                                                                                                                                                                              \r\n                                                                                                                                                                                                                                            \r\nOriginal exception was:                                                                                                                                                                                                                                                                        \r\nTraceback (most recent call last):                                                                                                                                                                                                          \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map                                                                                                                                                                               \r\n    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))                                                                                                                                                          \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/tqdm/std.py\", line 1178, in __iter__                                                                                                                                                                                                 \r\n    for obj in iterable:                                                                                                                                                                   \r\n  File \"/opt/conda/envs/main/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator                                                                                                                                                                                         \r\n    yield _result_or_cancel(fs.pop())                                                                                                                                                      \r\n  File \"/opt/conda/envs/main/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel                                                                                                                                                                                       \r\n    return fut.result(timeout)                                                                                                                                                             \r\n  File \"/opt/conda/envs/main/lib/python3.10/concurrent/futures/_base.py\", line 453, in result                                                                                                                                                                                                  \r\n    self._condition.wait(timeout)                                                                                                                                                                                                           \r\n  File \"/opt/conda/envs/main/lib/python3.10/threading.py\", line 320, in wait                                                                                                                                                                                                                   \r\n    waiter.acquire()                                                                                                                                                                                                                        \r\nKeyboardInterrupt                                                                                                                                                                                                                                                                              \r\n                                                                                                                      \r\nDuring handling of the above exception, another exception occurred:                                                                                                                                                                                                                            \r\n                                                                                                                                                                                                                                            \r\nTraceback (most recent call last):                                                                                                                                                                                                                                                             \r\n  File \"/TALI/tali/scripts/validate_dataset.py\", line 127, in <module>                                                                                                            \r\n    train_dataset.push_to_hub(repo_id=\"Antreas/TALI-base\", max_shard_size=\"5GB\")                                                                                                                                                                                                               \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/datasets/dataset_dict.py\", line 1583, in push_to_hub                                                                                                                                                                                                                                                      \r\n    repo_id, split, uploaded_size, dataset_nbytes, _, _ = self[split]._push_parquet_shards_to_hub(                                                                                                                                                                                             \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 5275, in _push_parquet_shards_to_hub                                                                                                                                                                                                                                     \r\n    _retry(                                                                                                                                                                                                                                                                                    \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 282, in _retry                                                                                                                                                                                                                                                        \r\n    return func(*func_args, **func_kwargs)                                                                                                                                                                                                                                                     \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn                                                                                                                                                                                                                                             \r\n    return fn(*args, **kwargs)                                                                                                                 \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 826, in _inner                                                                                                                                                                                                                                                           \r\n    return fn(self, *args, **kwargs)                                                                                                                                                                                                                                                           \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 3205, in upload_file                                                                                                                                                                                                                                                     \r\n    commit_info = self.create_commit(                                  \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn                                                                                                                                                                                                                                             \r\n    return fn(*args, **kwargs)                                                           \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 826, in _inner                                                                                                                                                                                                                                                           \r\n    return fn(self, *args, **kwargs)                                   \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2680, in create_commit                                                                                                                                                                                                                                                   \r\n    upload_lfs_files(                                                                    \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn                                                                                                                                                                                                                                             \r\n    return fn(*args, **kwargs)                                                           \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/huggingface_hub/_commit_api.py\", line 353, in upload_lfs_files                                                                                                                                                                                                                                            \r\n    thread_map(                                                                          \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/tqdm/contrib/concurrent.py\", line 69, in thread_map                                                                                                                                                                                                                                                       \r\n    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)                                                                                                       \r\n  File \"/opt/conda/envs/main/lib/python3.10/site-packages/tqdm/contrib/concurrent.py\", line 49, in _executor_map                                                                                                                                                                                                                                                    \r\n    with PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,                                                                                                   \r\n  File \"/opt/conda/envs/main/lib/python3.10/concurrent/futures/_base.py\", line 649, in __exit__                                                                                                                                                                                                                                                                     \r\n    self.shutdown(wait=True)                                                             \r\n  File \"/opt/conda/envs/main/lib/python3.10/concurrent/futures/thread.py\", line 235, in shutdown                                                                                                                                                                                                                                                                    \r\n    t.join()                                                                             \r\n  File \"/opt/conda/envs/main/lib/python3.10/threading.py\", line 1096, in join                                                                                                     \r\n    self._wait_for_tstate_lock()                                                         \r\n  File \"/opt/conda/envs/main/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock                                                                                                                                                                                                                                                                      \r\n    if lock.acquire(block, timeout):                                                     \r\nKeyboardInterrupt                                                                        \r\n```"
      }
    ]
  },
  {
    "issue_number": 7400,
    "title": "504 Gateway Timeout when uploading large dataset to Hugging Face Hub",
    "author": "hotchpotch",
    "state": "open",
    "created_at": "2025-02-14T02:18:35Z",
    "updated_at": "2025-02-14T23:48:36Z",
    "labels": [],
    "body": "### Description\nI encountered consistent 504 Gateway Timeout errors while attempting to upload a large dataset (approximately 500GB) to the Hugging Face Hub. The upload fails during the process with a Gateway Timeout error.\n\nI will continue trying to upload. While it might succeed in future attempts, I wanted to report this issue in the meantime.\n\n### Reproduction\n- I attempted the upload 3 times\n- Each attempt resulted in the same 504 error during the upload process (not at the start, but in the middle of the upload)\n- Using `dataset.push_to_hub()` method\n\n### Environment Information\n\n\n```\n- huggingface_hub version: 0.28.0\n- Platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.39\n- Python version: 3.11.10\n- Running in iPython ?: No\n- Running in notebook ?: No\n- Running in Google Colab ?: No\n- Running in Google Colab Enterprise ?: No\n- Token path ?: /home/hotchpotch/.cache/huggingface/token\n- Has saved token ?: True\n- Who am I ?: hotchpotch\n- Configured git credential helpers: store\n- FastAI: N/A\n- Tensorflow: N/A\n- Torch: 2.5.1\n- Jinja2: 3.1.5\n- Graphviz: N/A\n- keras: N/A\n- Pydot: N/A\n- Pillow: 10.4.0\n- hf_transfer: N/A\n- gradio: N/A\n- tensorboard: N/A\n- numpy: 1.26.4\n- pydantic: 2.10.6\n- aiohttp: 3.11.11\n- ENDPOINT: https://huggingface.co\n- HF_HUB_CACHE: /home/hotchpotch/.cache/huggingface/hub\n- HF_ASSETS_CACHE: /home/hotchpotch/.cache/huggingface/assets\n- HF_TOKEN_PATH: /home/hotchpotch/.cache/huggingface/token\n- HF_STORED_TOKENS_PATH: /home/hotchpotch/.cache/huggingface/stored_tokens\n- HF_HUB_OFFLINE: False\n- HF_HUB_DISABLE_TELEMETRY: False\n- HF_HUB_DISABLE_PROGRESS_BARS: None\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\n- HF_HUB_ENABLE_HF_TRANSFER: False\n- HF_HUB_ETAG_TIMEOUT: 10\n- HF_HUB_DOWNLOAD_TIMEOUT: 10\n\n```\n\n\n\n### Full Error Traceback\n\n```python\nTraceback (most recent call last):\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 504 Server Error: Gateway Time-out for url: https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese.git/info/lfs/objects/batch\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/create_edu_japanese_ds/upload_edu_japanese_ds.py\", line 12, in <module>\n    ds.push_to_hub(\"hotchpotch/fineweb-2-edu-japanese\", private=True)\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/datasets/dataset_dict.py\", line 1665, in push_to_hub\n    split_additions, uploaded_size, dataset_nbytes = self[split]._push_parquet_shards_to_hub(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 5301, in _push_parquet_shards_to_hub\n    api.preupload_lfs_files(\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py\", line 4215, in preupload_lfs_files\n    _upload_lfs_files(\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/_commit_api.py\", line 395, in _upload_lfs_files\n    batch_actions_chunk, batch_errors_chunk = post_lfs_batch_info(\n                                              ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/lfs.py\", line 168, in post_lfs_batch_info\n    hf_raise_for_status(resp)\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 477, in hf_raise_for_status\n    raise _format(HfHubHTTPError, str(e), response) from e\nhuggingface_hub.errors.HfHubHTTPError: 504 Server Error: Gateway Time-out for url: https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese.git/info/lfs/objects/batch\n```\n",
    "comments": [
      {
        "user": "Wauplin",
        "body": "I transferred to the `datasets` repository. Is there any retry mechanism in `datasets` @lhoestq ?\n\nAnother solution @hotchpotch if you want to get your dataset pushed to the Hub in a robust way is to save it to a local folder first and then use `huggingface-cli upload-large-folder` (see https://huggingface.co/docs/huggingface_hub/guides/upload#upload-a-large-folder). It has better retry mechanism in case of failure."
      },
      {
        "user": "lhoestq",
        "body": "There is no retry mechanism for `api.preupload_lfs_files` in `push_to_hub()` but we can definitely add one here\n\nhttps://github.com/huggingface/datasets/blob/de062f0552a810c52077543c1169c38c1f0c53fc/src/datasets/arrow_dataset.py#L5372"
      },
      {
        "user": "hotchpotch",
        "body": "@Wauplin \n\nThank you! I believe that to use load_dataset() to read data from Hugging Face, we need to first save the markdown metadata and parquet files in our local filesystem, then upload them using upload-large-folder. If you know how to do this, could you please let me know?\n\n"
      }
    ]
  },
  {
    "issue_number": 5338,
    "title": "`map()` stops every 1000 steps",
    "author": "bayartsogt-ya",
    "state": "closed",
    "created_at": "2022-12-07T19:09:40Z",
    "updated_at": "2025-02-14T18:10:07Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nI am passing the following `prepare_dataset` function to `Dataset.map` (code is inspired from [here](https://github.com/huggingface/community-events/blob/main/whisper-fine-tuning-event/run_speech_recognition_seq2seq_streaming.py#L454))\r\n```python3\r\ndef prepare_dataset(batch):\r\n    # load and resample audio data from 48 to 16kHz\r\n    audio = batch[\"audio\"]\r\n\r\n    # compute log-Mel input features from input audio array \r\n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\r\n\r\n    # encode target text to label ids \r\n    batch[\"labels\"] = tokenizer(batch[text_column]).input_ids\r\n    return batch\r\n...\r\ntrain_ds = train_ds.map(prepare_dataset)\r\n```\r\n\r\nHere is the exact code I am running https://github.com/bayartsogt-ya/whisper-multiple-hf-datasets/blob/main/train.py#L70-L71\r\n\r\nIt starts using all the cores (I am not sure why because I did not pass `num_proc`)\r\nthen progress bar stops at every 1k steps. (starts using a single core)\r\nthen come back to using all the cores again.\r\n\r\nlink to [screen record](https://youtu.be/jPQpQQGp6Gc)\r\n\r\nCan someone explain this process and maybe provide a way to improve this pipeline? cc: @lhoestq \r\n\r\n### Steps to reproduce the bug\r\n\r\n1. load the dataset\r\n2. create a Whisper processor\r\n3. create a `prepare_dataset` function\r\n4. pass the function to `dataset.map(prepare_dataset)`\r\n\r\n### Expected behavior\r\n\r\n- Use a single core per a function\r\n- not to stop at some point?\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.7.1.dev0\r\n- Platform: Linux-5.4.0-109-generic-x86_64-with-glibc2.27\r\n- Python version: 3.8.10\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.2",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi !\r\n\r\n> It starts using all the cores (I am not sure why because I did not pass num_proc)\r\n\r\nThe tokenizer uses Rust code that is multithreaded. And maybe the `feature_extractor` might run some things in parallel as well - but I'm not super familiar with its internals.\r\n\r\n> then progress bar stops at every 1k steps. (starts using a single core)\r\n\r\nEvery 1000 examples we flush the processed examples to disk. It is this way because Arrow is a columnar format: you must write data chunk by chunk. The processing in on hold while writing right now - maybe this can be improved in the future."
      },
      {
        "user": "bayartsogt-ya",
        "body": "Hi @lhoestq \r\nThanks for the explanation! it was so helpful! Let me check why `feature_extractor` is running on multiple cpus."
      },
      {
        "user": "julien-blanchon",
        "body": "Hey @lhoestq, any news about this flush operation ? This really really slow down most of my .map and .filter operation. I'm sometime taking 10x more time waiting for the flush than actually processing the 1k items. My current workarround is to set `writer_batch_size=len(dataset)`."
      }
    ]
  },
  {
    "issue_number": 7394,
    "title": "Using load_dataset with data_files and split arguments yields an error",
    "author": "devon-research",
    "state": "open",
    "created_at": "2025-02-12T04:50:11Z",
    "updated_at": "2025-02-12T04:50:11Z",
    "labels": [],
    "body": "### Describe the bug\n\nIt seems the list of valid splits recorded by the package becomes incorrectly overwritten when using the `data_files` argument.\n\nIf I run\n```python\nfrom datasets import load_dataset\nload_dataset(\"allenai/super\", split=\"all_examples\", data_files=\"tasks/expert.jsonl\")\n```\nthen I get the error\n```\nValueError: Unknown split \"all_examples\". Should be one of ['train'].\n```\n\nHowever, if I run\n```python\nfrom datasets import load_dataset\nload_dataset(\"allenai/super\", split=\"train\", name=\"Expert\")\n```\nthen I get\n```\nValueError: Unknown split \"train\". Should be one of ['all_examples'].\n```\n\n### Steps to reproduce the bug\n\nRun\n```python\nfrom datasets import load_dataset\nload_dataset(\"allenai/super\", split=\"all_examples\", data_files=\"tasks/expert.jsonl\")\n```\n\n### Expected behavior\n\nNo error.\n\n### Environment info\n\nPython = 3.12\ndatasets = 3.2.0",
    "comments": []
  },
  {
    "issue_number": 2689,
    "title": "cannot save the dataset to disk after rename_column",
    "author": "PaulLerner",
    "state": "closed",
    "created_at": "2021-07-21T08:13:40Z",
    "updated_at": "2025-02-11T23:23:17Z",
    "labels": [
      "bug"
    ],
    "body": "## Describe the bug\r\nIf you use `rename_column` and do no other modification, you will be unable to save the dataset using `save_to_disk`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nIn [1]: from datasets import Dataset, load_from_disk\r\nIn [5]: dataset=Dataset.from_dict({'foo': [0]})\r\nIn [7]: dataset.save_to_disk('foo')\r\nIn [8]: dataset=load_from_disk('foo')\r\nIn [10]: dataset=dataset.rename_column('foo', 'bar')\r\nIn [11]: dataset.save_to_disk('foo')\r\n---------------------------------------------------------------------------\r\nPermissionError                           Traceback (most recent call last)\r\n<ipython-input-11-a3bc0d4fc339> in <module>\r\n----> 1 dataset.save_to_disk('foo')\r\n\r\n/mnt/beegfs/projects/meerqat/anaconda3/envs/meerqat/lib/python3.7/site-packages/datasets/arrow_dataset.py in save_to_disk(self, dataset_path\r\n, fs)\r\n    597             if Path(dataset_path, config.DATASET_ARROW_FILENAME) in cache_files_paths:\r\n    598                 raise PermissionError(\r\n--> 599                     f\"Tried to overwrite {Path(dataset_path, config.DATASET_ARROW_FILENAME)} but a dataset can't overwrite itself.\"\r\n    600                 )\r\n    601             if Path(dataset_path, config.DATASET_INDICES_FILENAME) in cache_files_paths:\r\n\r\nPermissionError: Tried to overwrite foo/dataset.arrow but a dataset can't overwrite itself.\r\n```\r\n\r\nN. B. I created the dataset from dict to enable easy reproduction but the same happens if you load an existing dataset (e.g. starting from `In [8]`)\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-centos-7.9.2009-Core\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n\r\n",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! That's because you are trying to overwrite a file that is already open and being used.\r\nIndeed `foo/dataset.arrow` is open and used by your `dataset` object.\r\n\r\nWhen you do `rename_column`, the resulting dataset reads the data from the same arrow file.\r\nIn other cases like when using `map` on the other hand, the resulting dataset reads the data from another arrow file that is the result of the map transform.\r\n\r\nTherefore overwriting a dataset after `rename_column` is not possible, but it is possible after `map`, since `rename_column` doesn't switch to using another arrow file (the actual data stay the same)."
      },
      {
        "user": "PaulLerner",
        "body": "Ok, thanks for clearing it up :)"
      },
      {
        "user": "sd3ntato",
        "body": "so what would be the right way to read a dataset, then change something and then overwrite it with the new version?"
      }
    ]
  },
  {
    "issue_number": 7389,
    "title": "Getting statistics about filtered examples",
    "author": "jonathanasdf",
    "state": "closed",
    "created_at": "2025-02-10T20:48:29Z",
    "updated_at": "2025-02-11T20:44:15Z",
    "labels": [],
    "body": "@lhoestq wondering if the team has thought about this and if there are any recommendations?\n\nCurrently when processing datasets some examples are bound to get filtered out, whether it's due to bad format, or length is too long, or any other custom filters that might be getting applied. Let's just focus on the filter by length for now, since that would be something that gets applied dynamically for each training run. Say we want to show a graph in W&B with the running total of the number of filtered examples so far.\n\nWhat would be a good way to go about hooking this up? Because the map/filter operations happen before the DataLoader batches are created, at training time if we're just grabbing batches from the DataLoader then we won't know how many things have been filtered already. But there's not really a good way to include a 'num_filtered' key into the dataset itself either because dataset map/filter process examples independently and don't have a way to track a running sum.\n\nThe only approach I can kind of think of is having a 'is_filtered' key in the dataset, and then creating a custom batcher/collator that reads that and tracks the metric?",
    "comments": [
      {
        "user": "lhoestq",
        "body": "You can actually track a running sum in map() or filter() :)\n\n```python\nnum_filtered = 0\n\ndef f(x):\n    global num_filtered\n    condition = len(x[\"text\"]) < 1000\n    if not condition:\n        num_filtered += 1\n    return condition\n\nds = ds.filter(f)\nprint(num_filtered)\n```\n\nand if you want to use multiprocessing, make sure to use a variable that is shared across processes\n\n\n```python\nfrom multiprocess import Manager\n\nmanager = Manager()\nnum_filtered = manager.Value('i', 0)\n\ndef f(x):\n    global num_filtered\n    condition = len(x[\"text\"]) < 1000\n    if not condition:\n        num_filtered.value += 1\n    return condition\n\nds = ds.filter(f, num_proc=4)\nprint(num_filtered.value)\n```\n\nPS: `datasets` uses `multiprocess` instead of the `multiprocessing` package to support lambda functions in map() and filter()"
      },
      {
        "user": "jonathanasdf",
        "body": "Oh that's great to know!\n\nI guess this value would not be exactly synced with the batch in cases of pre-fetch and shuffle buffers and so on, but that's probably fine. Thanks!"
      }
    ]
  },
  {
    "issue_number": 7392,
    "title": "push_to_hub payload too large error when using large ClassLabel feature",
    "author": "DavidRConnell",
    "state": "open",
    "created_at": "2025-02-11T17:51:34Z",
    "updated_at": "2025-02-11T18:01:31Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen using `datasets.DatasetDict.push_to_hub` an `HfHubHTTPError: 413 Client Error: Payload Too Large for url` is raised if the dataset contains a large `ClassLabel` feature. Even if the total size of the dataset is small.\n\n### Steps to reproduce the bug\n\n``` python\nimport random\nimport sys\n\nimport datasets\n\nrandom.seed(42)\n\n\ndef random_str(sz):\n    return \"\".join(chr(random.randint(ord(\"a\"), ord(\"z\"))) for _ in range(sz))\n\n\ndata = datasets.DatasetDict(\n    {\n        str(i): datasets.Dataset.from_dict(\n            {\n                \"label\": [list(range(3)) for _ in range(10)],\n                \"abstract\": [random_str(10_000) for _ in range(10)],\n            },\n        )\n        for i in range(3)\n    }\n)\nfeatures = data[\"1\"].features.copy()\nfeatures[\"label\"] = datasets.Sequence(\n    datasets.ClassLabel(names=[str(i) for i in range(50_000)])\n)\ndata = data.map(lambda examples: {}, features=features)\n\nfeat_size = sys.getsizeof(data[\"1\"].features[\"label\"].feature.names)\nprint(f\"Size of ClassLabel names: {feat_size}\")\n# Size of ClassLabel names: 444376\n\n\ndata.push_to_hub(\"dconnell/pubtator3_test\")\n```\n\nNote that this succeeds if `ClassLabel` has fewer names or if `ClassLabel` is replaced with `Value(\"int64\")`\n\n\n### Expected behavior\n\nShould push the dataset to hub.\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\n\n- `datasets` version: 3.2.0\n- Platform: Linux-5.15.0-126-generic-x86_64-with-glibc2.35\n- Python version: 3.12.8\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0\n",
    "comments": [
      {
        "user": "DavidRConnell",
        "body": "See also <https://discuss.huggingface.co/t/datasetdict-push-to-hub-failing-with-payload-to-large/140083/8>\n"
      }
    ]
  },
  {
    "issue_number": 7388,
    "title": "OSError: [Errno 22] Invalid argument forbidden character",
    "author": "langflogit",
    "state": "closed",
    "created_at": "2025-02-10T17:46:31Z",
    "updated_at": "2025-02-11T13:42:32Z",
    "labels": [],
    "body": "### Describe the bug\n\nI'm on Windows and i'm trying to load a datasets but i'm having title error because files in the repository are named with charactere like < >which can't be in a name file. Could it be possible to load this datasets but removing those charactere ?\n\n### Steps to reproduce the bug\n\nload_dataset(\"CATMuS/medieval\") on Windows\n\n### Expected behavior\n\nMaking the function to erase the forbidden character to allow loading the datasets who have those characters. \n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Windows-10-10.0.19045-SP0\n- Python version: 3.12.2\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0",
    "comments": [
      {
        "user": "lhoestq",
        "body": "You can probably copy the dataset in your HF account and rename the files (without having to download them to your disk). Or alternatively feel free to open a Pull Request to this dataset with the renamed file"
      },
      {
        "user": "langflogit",
        "body": "Thank you, that will help me work around this problem"
      }
    ]
  },
  {
    "issue_number": 7391,
    "title": "AttributeError: module 'pyarrow.lib' has no attribute 'ListViewType'",
    "author": "LinXin04",
    "state": "open",
    "created_at": "2025-02-11T12:02:26Z",
    "updated_at": "2025-02-11T12:02:26Z",
    "labels": [],
    "body": "pyarrow 尝试了若干个版本都不可以",
    "comments": []
  },
  {
    "issue_number": 7390,
    "title": "Re-add py.typed",
    "author": "NeilGirdhar",
    "state": "open",
    "created_at": "2025-02-10T22:12:52Z",
    "updated_at": "2025-02-10T22:12:52Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nThe motivation for removing py.typed no longer seems to apply.  Would a solution like [this one](https://github.com/huggingface/huggingface_hub/pull/2752) work here?\n\n### Motivation\n\nMyPy support is broken.  As more type checkers come out, such as RedKnot, these may also be broken.  It would be good to be PEP 561 compliant as long as it's not too onerous.\n\n### Your contribution\n\nI can re-add py.typed, but I don't know how to make sur all of the `__all__` files are provided (although you may not need to with modern PyRight).",
    "comments": []
  },
  {
    "issue_number": 7327,
    "title": ".map() is not caching and ram goes OOM",
    "author": "simeneide",
    "state": "open",
    "created_at": "2024-12-13T14:22:56Z",
    "updated_at": "2025-02-10T10:42:38Z",
    "labels": [],
    "body": "### Describe the bug\n\nIm trying to run a fairly simple map that is converting a dataset into numpy arrays. however, it just piles up on memory and doesnt write to disk. Ive tried multiple cache techniques such as specifying the cache dir, setting max mem, +++ but none seem to work. What am I missing here?\n\n### Steps to reproduce the bug\n\n```\r\nfrom pydub import AudioSegment\r\nimport io\r\nimport base64\r\nimport numpy as np\r\nimport os\r\nCACHE_PATH = \"/mnt/extdisk/cache\" # \"/root/.cache/huggingface/\"# \r\nos.environ[\"HF_HOME\"] = CACHE_PATH\r\nimport datasets\r\nimport logging\r\nlogger = logging.getLogger()\r\nlogger.setLevel(logging.INFO)\r\n# Create a handler for Jupyter notebook\r\nhandler = logging.StreamHandler()\r\nformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\nhandler.setFormatter(formatter)\r\nlogger.addHandler(handler)\r\n\r\n#datasets.config.IN_MEMORY_MAX_SIZE= 1000#*(2**30) #50 gb\r\nprint(datasets.config.HF_CACHE_HOME)\r\nprint(datasets.config.HF_DATASETS_CACHE)\r\n# Decode the base64 string into bytes\r\ndef convert_mp3_to_audio_segment(example):\r\n    \"\"\"\r\n    example = ds['train'][0]\r\n    \"\"\"\r\n    try:\r\n        audio_data_bytes = base64.b64decode(example['audio'])\r\n        # Use pydub to load the MP3 audio from the decoded bytes\r\n        audio_segment = AudioSegment.from_file(io.BytesIO(audio_data_bytes), format=\"mp3\")\r\n        # Resample to 24_000\r\n        audio_segment = audio_segment.set_frame_rate(24_000)\r\n        audio = {'sampling_rate' : audio_segment.frame_rate,\r\n        'array' : np.array(audio_segment.get_array_of_samples(), dtype=\"float\")}\r\n        del audio_segment\r\n        duration = len(audio['array']) / audio['sampling_rate']\r\n    except Exception as e:\r\n        logger.warning(f\"Failed to convert audio for {example['id']}. Error: {e}\")\r\n        audio = {'sampling_rate' : 0,\r\n        'array' : np.array([]), duration : 0}\r\n    return {'audio' : audio, 'duration' : duration}\r\n\r\nds = datasets.load_dataset(\"NbAiLab/nb_distil_speech_noconcat_stortinget\", cache_dir=CACHE_PATH, keep_in_memory=False)\r\n\r\n#%%\r\nnum_proc=32\r\nds_processed = (\r\n    ds\r\n    #.select(range(10))\r\n    .map(convert_mp3_to_audio_segment, num_proc=num_proc, desc=\"Converting mp3 to audio segment\") #, cache_file_name=f\"{CACHE_PATH}/stortinget_audio\" # , cache_file_name=\"test\"\r\n)\r\n```\n\n### Expected behavior\n\nthe map should write to disk\n\n### Environment info\n\n- `datasets` version: 3.2.0\r\n- Platform: Linux-6.8.0-45-generic-x86_64-with-glibc2.39\r\n- Python version: 3.12.7\r\n- `huggingface_hub` version: 0.26.3\r\n- PyArrow version: 18.1.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.9.0",
    "comments": [
      {
        "user": "98MM",
        "body": "I have the same issue - any update on this?"
      }
    ]
  },
  {
    "issue_number": 5717,
    "title": "Errror when saving to disk a dataset of images",
    "author": "jplu",
    "state": "open",
    "created_at": "2023-04-07T11:59:17Z",
    "updated_at": "2025-02-09T09:35:25Z",
    "labels": [],
    "body": "### Describe the bug\n\nHello!\r\n\r\nI have an issue when I try to save on disk my dataset of images. The error I get is:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jplu/miniconda3/envs/image-xp/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 1442, in save_to_disk\r\n    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):\r\n  File \"/home/jplu/miniconda3/envs/image-xp/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 1473, in _save_to_disk_single\r\n    writer.write_table(pa_table)\r\n  File \"/home/jplu/miniconda3/envs/image-xp/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 570, in write_table\r\n    pa_table = embed_table_storage(pa_table)\r\n  File \"/home/jplu/miniconda3/envs/image-xp/lib/python3.10/site-packages/datasets/table.py\", line 2268, in embed_table_storage\r\n    arrays = [\r\n  File \"/home/jplu/miniconda3/envs/image-xp/lib/python3.10/site-packages/datasets/table.py\", line 2269, in <listcomp>\r\n    embed_array_storage(table[name], feature) if require_storage_embed(feature) else table[name]\r\n  File \"/home/jplu/miniconda3/envs/image-xp/lib/python3.10/site-packages/datasets/table.py\", line 1817, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/jplu/miniconda3/envs/image-xp/lib/python3.10/site-packages/datasets/table.py\", line 1817, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/jplu/miniconda3/envs/image-xp/lib/python3.10/site-packages/datasets/table.py\", line 2142, in embed_array_storage\r\n    return feature.embed_storage(array)\r\n  File \"/home/jplu/miniconda3/envs/image-xp/lib/python3.10/site-packages/datasets/features/image.py\", line 269, in embed_storage\r\n    storage = pa.StructArray.from_arrays([bytes_array, path_array], [\"bytes\", \"path\"], mask=bytes_array.is_null())\r\n  File \"pyarrow/array.pxi\", line 2766, in pyarrow.lib.StructArray.from_arrays\r\n  File \"pyarrow/array.pxi\", line 2961, in pyarrow.lib.c_mask_inverted_from_obj\r\nTypeError: Mask must be a pyarrow.Array of type boolean\r\n```\r\n\r\nMy dataset is around 50K images, is this error might be due to a bad image?\r\n\r\nThanks for the help.\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/dataset\")\r\ndataset[\"train\"].save_to_disk(\"./myds\", num_shards=40)\r\n```\n\n### Expected behavior\n\nHaving my dataset properly saved to disk.\n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.13.3\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0",
    "comments": [
      {
        "user": "jplu",
        "body": "Looks like as long as the number of shards makes a batch lower than 1000 images it works. In my training set I have 40K images. If I use `num_shards=40` (batch of 1000 images) I get the error, but if I update it to `num_shards=50` (batch of 800 images) it works.\r\n\r\nI will be happy to share my dataset privately if it can help to better debug."
      },
      {
        "user": "mariosasko",
        "body": "Hi! I didn't manage to reproduce this behavior, so sharing the dataset with us would help a lot. \r\n\r\n> My dataset is around 50K images, is this error might be due to a bad image?\r\n\r\nThis shouldn't be the case as we save raw data to disk without decoding it."
      },
      {
        "user": "jplu",
        "body": "OK, thanks! The dataset is currently hosted on a gcs bucket. How would you like to proceed for sharing the link? "
      }
    ]
  },
  {
    "issue_number": 7386,
    "title": "Add bookfolder Dataset Builder for Digital Book Formats",
    "author": "shikanime",
    "state": "closed",
    "created_at": "2025-02-08T14:27:55Z",
    "updated_at": "2025-02-08T14:30:10Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nThis feature proposes adding a new dataset builder called bookfolder to the datasets library. This builder would allow users to easily load datasets consisting of various digital book formats, including: AZW, AZW3, CB7, CBR, CBT, CBZ, EPUB, MOBI, and PDF.\n\n### Motivation\n\nCurrently, loading datasets of these digital book files requires manual effort. This would also lower the barrier to entry for working with these formats, enabling more diverse and interesting datasets to be used within the Hugging Face ecosystem.\n\n### Your contribution\n\nThis feature is rather simple as it will be based on the folder-based builder, similar to imagefolder. I'm willing to contribute to this feature by submitting a PR",
    "comments": [
      {
        "user": "shikanime",
        "body": "On second thought, probably not a good idea."
      }
    ]
  },
  {
    "issue_number": 5284,
    "title": "Features of IterableDataset set to None by remove column",
    "author": "sanchit-gandhi",
    "state": "closed",
    "created_at": "2022-11-23T10:54:59Z",
    "updated_at": "2025-02-07T11:36:41Z",
    "labels": [
      "bug",
      "streaming"
    ],
    "body": "### Describe the bug\r\n\r\nThe `remove_column` method of the IterableDataset sets the dataset features to None.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import Audio, load_dataset\r\n\r\n# load LS in streaming mode\r\ndataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\r\n\r\n# check original features\r\nprint(\"Original features: \", dataset.features.keys())\r\n\r\n# define features to remove: we KEEP audio and text\r\nCOLUMNS_TO_REMOVE = ['chapter_id', 'speaker_id', 'file', 'id']\r\n\r\ndataset = dataset.remove_columns(COLUMNS_TO_REMOVE)\r\n\r\n# check processed features, uh-oh!\r\nprint(\"Processed features: \", dataset.features)\r\n\r\n# streaming the first audio sample still works\r\nprint(\"First sample:\", next(iter(ds)))\r\n```\r\n\r\n**Print Output:**\r\n\r\n```\r\nOriginal features:  dict_keys(['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'])\r\nProcessed features:  None\r\nFirst sample: {'audio': {'path': '2277-149896-0000.flac', 'array': array([ 0.00186157,  0.0005188 ,  0.00024414, ..., -0.00097656,\r\n       -0.00109863, -0.00146484]), 'sampling_rate': 16000}, 'text': \"HE WAS IN A FEVERED STATE OF MIND OWING TO THE BLIGHT HIS WIFE'S ACTION THREATENED TO CAST UPON HIS ENTIRE FUTURE\"}\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe features should be those **not** removed by the `remove_column` method, i.e. audio and text.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.7.1\r\n- Platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.15\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.3.5\r\n\r\n(Running on Google Colab for a blog post: https://colab.research.google.com/drive/1ySCQREPZEl4msLfxb79pYYOWjUZhkr9y#scrollTo=8pRDGiVmH2ml)\r\n\r\ncc @polinaeterna @lhoestq ",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Related to https://github.com/huggingface/datasets/issues/5245"
      },
      {
        "user": "alvarobartt",
        "body": "#self-assign"
      },
      {
        "user": "sanchit-gandhi",
        "body": "Thanks @lhoestq and @alvarobartt!\r\n\r\nThis would be extremely helpful to have working for the Whisper fine-tuning event - we're **only** training using streaming mode, so it'll be quite important to have this feature working to make training as easy as possible!\r\n\r\n_c.f._ https://twitter.com/sanchitgandhi99/status/1592188332171493377"
      }
    ]
  },
  {
    "issue_number": 6936,
    "title": "save_to_disk() freezes when saving on s3 bucket with multiprocessing",
    "author": "ycattan",
    "state": "open",
    "created_at": "2024-05-30T16:48:39Z",
    "updated_at": "2025-02-06T22:12:52Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nI'm trying to save a `Dataset` using the `save_to_disk()` function with:\r\n- `num_proc > 1`\r\n- `dataset_path` being a s3 bucket path e.g. \"s3://{bucket_name}/{dataset_folder}/\"\r\n\r\nThe hf progress bar shows up but the saving does not seem to start. \r\nWhen using one processor only (`num_proc=1`), everything works fine.\r\nWhen saving the dataset on local disk (as opposed to s3 bucket) with `num_proc > 1`, everything works fine.\r\n\r\nThank you for your help! :)\r\n\r\n### Steps to reproduce the bug\r\n\r\nI tried without any storage options:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\nsandbox_ds = load_dataset(\"openai_humaneval\")\r\nsandbox_ds[\"test\"].save_to_disk(\r\n    \"s3://bucket-name/test_multiprocessing_saving/\",\r\n    num_proc=4,\r\n)\r\n```\r\n\r\n\r\nand with the specific s3fs storage options: \r\n```\r\nfrom datasets import load_dataset\r\nfrom s3fs import S3FileSystem\r\n\r\ndef get_s3fs():\r\n    return S3FileSystem()\r\n\r\nsandbox_ds = load_dataset(\"openai_humaneval\")\r\nsandbox_ds[\"test\"].save_to_disk(\r\n    \"s3://bucket-name/test_multiprocessing_saving/\",\r\n    num_proc=4,\r\n    storage_options=get_s3fs().storage_options, # also tried: storage_options=S3FileSystem().storage_options\r\n)\r\n```\r\n\r\nI'm guessing I might use `storage_options` parameter wrongly, but I didn't find anything online that made it work.\r\n\r\n**NB**: Behavior is the same when trying to save the whole `DatasetDict`.\r\n\r\n### Expected behavior\r\n\r\nProgress bar fills in and saving is carried out.\r\n\r\n### Environment info\r\n\r\n`datasets==2.18.0`",
    "comments": [
      {
        "user": "sfc-gh-ywei",
        "body": "I got the same issue. Any updates so far for this issue?"
      },
      {
        "user": "solanovisitor",
        "body": "Same here. Any updates?"
      },
      {
        "user": "grapefroot",
        "body": "+1, experiencing this as well"
      }
    ]
  },
  {
    "issue_number": 6905,
    "title": "Extraction protocol for arrow files is not defined",
    "author": "radulescupetru",
    "state": "closed",
    "created_at": "2024-05-17T16:01:41Z",
    "updated_at": "2025-02-06T19:50:22Z",
    "labels": [],
    "body": "### Describe the bug\n\nPassing files with `.arrow` extension into data_files argument, at least when `streaming=True` is very slow.\n\n### Steps to reproduce the bug\n\n Basically it goes through the `_get_extraction_protocol` method located [here](https://github.com/huggingface/datasets/blob/main/src/datasets/utils/file_utils.py#L820)\r\nThe method then looks at some base known extensions where `arrow` is not defined so it proceeds to determine the compression with the magic number method which is slow when dealing with a lot of files which are stored in s3 and by looking at this predefined list, I don't see `arrow` in there either so in the end it return None:\r\n\r\n```\r\nMAGIC_NUMBER_TO_COMPRESSION_PROTOCOL = {\r\n    bytes.fromhex(\"504B0304\"): \"zip\",\r\n    bytes.fromhex(\"504B0506\"): \"zip\",  # empty archive\r\n    bytes.fromhex(\"504B0708\"): \"zip\",  # spanned archive\r\n    bytes.fromhex(\"425A68\"): \"bz2\",\r\n    bytes.fromhex(\"1F8B\"): \"gzip\",\r\n    bytes.fromhex(\"FD377A585A00\"): \"xz\",\r\n    bytes.fromhex(\"04224D18\"): \"lz4\",\r\n    bytes.fromhex(\"28B52FFD\"): \"zstd\",\r\n}\r\n```\n\n### Expected behavior\n\nMy expectation is that `arrow` would be in the known lists so it would return None without going through the magic number method.\n\n### Environment info\n\ndatasets 2.19.0",
    "comments": [
      {
        "user": "radulescupetru",
        "body": "Fixed in https://github.com/huggingface/datasets/pull/7083"
      }
    ]
  },
  {
    "issue_number": 2022,
    "title": "ValueError when rename_column on splitted dataset",
    "author": "simonschoe",
    "state": "closed",
    "created_at": "2021-03-10T09:40:38Z",
    "updated_at": "2025-02-05T13:36:07Z",
    "labels": [],
    "body": "Hi there,\r\nI am loading `.tsv` file via `load_dataset` and subsequently split the rows into training and test set via the `ReadInstruction` API like so:\r\n\r\n```python\r\nsplit = {\r\n    'train': ReadInstruction('train', to=90, unit='%'),\r\n    'test': ReadInstruction('train', from_=-10, unit='%')\r\n}\r\n\r\ndataset = load_dataset(\r\n    path='csv',               # use 'text' loading script to load from local txt-files\r\n    delimiter='\\t',           # xxx\r\n    data_files=text_files,    # list of paths to local text files\r\n    split=split,              # xxx\r\n)\r\n\r\ndataset\r\n```\r\n\r\nPart of output:\r\n```python\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['sentence', 'sentiment'],\r\n        num_rows: 900\r\n    })\r\n    test: Dataset({\r\n        features: ['sentence', 'sentiment'],\r\n        num_rows: 100\r\n    })\r\n})\r\n```\r\nAfterwards I'd like to rename the 'sentence' column to 'text' in order to be compatible with my modelin pipeline. If I run the following code I experience a `ValueError` however:\r\n```python\r\ndataset['train'].rename_column('sentence', 'text')\r\n```\r\n```python\r\n/usr/local/lib/python3.7/dist-packages/datasets/splits.py in __init__(self, name)\r\n    353         for split_name in split_names_from_instruction:\r\n    354             if not re.match(_split_re, split_name):\r\n--> 355                 raise ValueError(f\"Split name should match '{_split_re}'' but got '{split_name}'.\")\r\n    356 \r\n    357     def __str__(self):\r\n\r\nValueError: Split name should match '^\\w+(\\.\\w+)*$'' but got 'ReadInstruction('.\r\n```\r\nIn particular, these behavior does not arise if I use the deprecated `rename_column_` method. Any idea what causes the error? Would assume something in the way I defined the split.\r\n\r\nThanks in advance! :)",
    "comments": [
      {
        "user": "mariosasko",
        "body": "Hi,\r\n\r\nThis is a bug so thanks for reporting it. `Dataset.__setstate__`  is the problem, which is called when `Dataset.rename_column` tries to copy the dataset with `copy.deepcopy(self)`. This only happens if the `split` arg in `load_dataset` was defined as `ReadInstruction`.\r\n\r\nTo overcome this issue, use the named splits API (for now):\r\n```python\r\ntrain_ds, test_ds = load_dataset(\r\n    path='csv',               \r\n    delimiter='\\t',          \r\n    data_files=text_files,    \r\n    split=['train[:90%]', 'train[-10%:]'],\r\n)\r\n\r\ntrain_ds = train_ds.rename_column('sentence', 'text')\r\n```"
      },
      {
        "user": "lhoestq",
        "body": "This has been fixed in #2043 , thanks @mariosasko \r\nThe fix is available on master and we'll do a new release soon :)\r\n\r\nfeel free to re-open if you still have issues"
      }
    ]
  },
  {
    "issue_number": 7175,
    "title": "[FSTimeoutError] load_dataset",
    "author": "cosmo3769",
    "state": "closed",
    "created_at": "2024-09-26T15:42:29Z",
    "updated_at": "2025-02-01T09:09:35Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen using `load_dataset`to load [HuggingFaceM4/VQAv2](https://huggingface.co/datasets/HuggingFaceM4/VQAv2), I am getting `FSTimeoutError`. \r\n\r\n### Error\r\n\r\n```\r\nTimeoutError: \r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nFSTimeoutError                            Traceback (most recent call last)\r\n[/usr/local/lib/python3.10/dist-packages/fsspec/asyn.py](https://klh9mr78js-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20240924-060116_RC00_678132060#) in sync(loop, func, timeout, *args, **kwargs)\r\n     99     if isinstance(return_result, asyncio.TimeoutError):\r\n    100         # suppress asyncio.TimeoutError, raise FSTimeoutError\r\n--> 101         raise FSTimeoutError from return_result\r\n    102     elif isinstance(return_result, BaseException):\r\n    103         raise return_result\r\n\r\nFSTimeoutError:\r\n```\r\n\r\nIt usually fails around 5-6 GB.\r\n\r\n<img width=\"847\" alt=\"Screenshot 2024-09-26 at 9 10 19 PM\" src=\"https://github.com/user-attachments/assets/ff91995a-fb55-4de6-8214-94025d6c8470\">\r\n\n\n### Steps to reproduce the bug\n\nTo reproduce it, run this in colab notebook:\r\n\r\n```\r\n!pip install -q -U datasets\r\n\r\nfrom datasets import load_dataset\r\nds = load_dataset('HuggingFaceM4/VQAv2', split=\"train[:10%]\")\r\n```\r\n\r\n\n\n### Expected behavior\n\nIt should download properly.\n\n### Environment info\n\nUsing Colab Notebook.",
    "comments": [
      {
        "user": "cosmo3769",
        "body": "Is this `FSTimeoutError` due to download network issue from remote resource (from where it is being accessed)?"
      },
      {
        "user": "crlotwhite",
        "body": "It seems to happen for all datasets, not just a specific one, and especially for versions after 3.0. (3.0.0, 3.0.1 have this problem)\r\n\r\nI had the same error on a different dataset, but after downgrading to datasets==2.21.0, the problem was solved."
      },
      {
        "user": "lhoestq",
        "body": "Same as https://github.com/huggingface/datasets/issues/7164\r\n\r\nThis dataset is made of a python script that downloads data from elsewhere than HF, so availability depends on the original host. Ultimately it would be nice to host the files of this dataset on HF\r\n\r\nin `datasets` <3.0 there were lots of mechanisms that got removed after the decision to make datasets with python loading scripts legacy for security and maintenance reasons (we only do very basic support now)"
      }
    ]
  },
  {
    "issue_number": 5881,
    "title": "Split dataset by node: index error when sharding iterable dataset",
    "author": "sanchit-gandhi",
    "state": "open",
    "created_at": "2023-05-22T10:36:13Z",
    "updated_at": "2025-01-31T16:36:30Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nContext: we're splitting an iterable dataset by node and then passing it to a torch data loader with multiple workers\r\n\r\nWhen we iterate over it for 5 steps, we don't get an error\r\n\r\nWhen we instead iterate over it for 8 steps, we get an `IndexError` when fetching the data if we have too many workers\r\n\r\n### Steps to reproduce the bug\r\n\r\nHere, we have 2 JAX processes (`jax.process_count() = 2`) which we split the dataset over. The dataset loading script can be found here: https://huggingface.co/datasets/distil-whisper/librispeech_asr/blob/c6a1e805cbfeed5057400ac5937327d7e30281b8/librispeech_asr.py#L310\r\n\r\n<details>\r\n\r\n<summary> Code to reproduce </summary>\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nimport jax\r\nfrom datasets.distributed import split_dataset_by_node\r\nfrom torch.utils.data import DataLoader\r\nfrom tqdm import tqdm\r\n\r\n# load an example dataset (https://huggingface.co/datasets/distil-whisper/librispeech_asr)\r\ndataset = load_dataset(\"distil-whisper/librispeech_asr\", \"all\", split=\"train.clean.100\", streaming=True)\r\n# just keep the text column -> no need to define a collator\r\ndataset_text = dataset.remove_columns(set(dataset.features.keys()) - {\"text\"})\r\n\r\n# define some constants\r\nbatch_size = 256\r\nnum_examples = 5  # works for 5 examples, doesn't for 8\r\nnum_workers = dataset_text.n_shards\r\n\r\n# try with multiple workers\r\ndataloader = DataLoader(dataset_text, batch_size=batch_size, num_workers=num_workers, drop_last=True)\r\n\r\nfor i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Multiple workers\"):\r\n    if i == num_examples:\r\n        break\r\n\r\n# try splitting by node (we can't do this with `dataset_text` since `split_dataset_by_node` expects the Audio column for an ASR dataset)\r\ndataset = split_dataset_by_node(dataset, rank=jax.process_index(), world_size=jax.process_count())\r\n# remove the text column again\r\ndataset_text = dataset.remove_columns(set(dataset.features.keys()) - {\"text\"})\r\ndataloader = DataLoader(dataset_text, batch_size=16, num_workers=num_workers // 2, drop_last=True)\r\n\r\nfor i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Split by node\"):\r\n    if i == num_examples:\r\n        break\r\n\r\n# too many workers\r\ndataloader = DataLoader(dataset_text, batch_size=256, num_workers=num_workers, drop_last=True)\r\nfor i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Too many workers\"):\r\n    if i == num_examples:\r\n        break\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n\r\n<summary> With 5 examples: </summary>\r\n\r\n```\r\nMultiple workers: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.33s/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.                                                       \r\nSplit by node: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.76s/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.                                                       \r\nToo many dataloader workers: 14 (max is dataset.n_shards=7). Stopping 7 dataloader workers.                             \r\nTo parallelize data loading, we give each process some shards (or data sources) to process. Therefore it's unnecessary t\r\no have a number of workers greater than dataset.n_shards=7. To enable more parallelism, please split the dataset in more\r\n files than 7.                                                                                                          \r\nToo many workers: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.03s/it]\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n\r\n<summary> With 7 examples: </summary>\r\n\r\n```\r\nMultiple workers: 100%|███████████████████████████████████████████████████████████████████| 8/8 [00:13<00:00,  1.71s/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.\r\nSplit by node: 100%|██████████████████████████████████████████████████████████████████████| 8/8 [00:11<00:00,  1.38s/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.\r\nToo many dataloader workers: 14 (max is dataset.n_shards=7). Stopping 7 dataloader workers.\r\nTo parallelize data loading, we give each process some shards (or data sources) to process. Therefore it's unnecessary to have a number of workers greater than dataset.n_shards=7. To enable more parallelism, please split the dataset in more files than 7.\r\nToo many workers:  88%|██████████████████████████████████████████████████████████▋        | 7/8 [00:13<00:01,  1.89s/it]\r\nTraceback (most recent call last):\r\n  File \"distil-whisper/test_librispeech.py\", line 36, in <module>\r\n    for i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Too many workers\"):\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/tqdm/std.py\", line 1178, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\r\n    data = self._next_data()\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1325, in _next_data\r\n    return self._process_data(data)\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\r\n    data.reraise()\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/_utils.py\", line 644, in reraise\r\n    raise exception\r\nIndexError: Caught IndexError in DataLoader worker process 7.\r\nOriginal Traceback (most recent call last):\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\r\n    data.append(next(self.dataset_iter))\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 986, in __iter__\r\n    yield from self._iter_pytorch(ex_iterable)\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 920, in _iter_pytorch\r\n    for key, example in ex_iterable.shard_data_sources(worker_info.id, worker_info.num_workers):\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 540, in shard_data_sources\r\n    self.ex_iterable.shard_data_sources(worker_id, num_workers),\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 796, in shard_data_sources\r\n    self.ex_iterable.shard_data_sources(worker_id, num_workers),\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 126, in shard_data_sources\r\n    requested_gen_kwargs = _merge_gen_kwargs([gen_kwargs_list[i] for i in shard_indices])\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/utils/sharding.py\", line 76, in _merge_gen_kwargs\r\n    for key in gen_kwargs_list[0]\r\nIndexError: list index out of range\r\n```\r\n\r\n</details>\r\n\r\n### Expected behavior\r\n\r\nShould pass for both 5 and 7 examples\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.12.1.dev0\r\n- Platform: Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1",
    "comments": [
      {
        "user": "sanchit-gandhi",
        "body": "cc @lhoestq in case you have any ideas here! Might need a multi-host set-up to debug (can give you access to a JAX one if you need)"
      },
      {
        "user": "Munikumar09",
        "body": "I am also facing the same problem. Could you let me know if you found a solution for this?"
      },
      {
        "user": "lhoestq",
        "body": "I couldn't reproduce with the latest version of `datasets` 2.16.1, can you update `datasets` and try again ?"
      }
    ]
  },
  {
    "issue_number": 6598,
    "title": "Unexpected keyword argument 'hf' when downloading CSV dataset from S3",
    "author": "dguenms",
    "state": "closed",
    "created_at": "2024-01-16T15:16:01Z",
    "updated_at": "2025-01-31T15:35:33Z",
    "labels": [],
    "body": "### Describe the bug\n\nI receive this error message when using `load_dataset` with \"csv\" path and `dataset_files=s3://...`:\r\n```\r\nTypeError: Session.__init__() got an unexpected keyword argument 'hf'\r\n```\r\n\r\nI found a similar issue here: https://stackoverflow.com/questions/77596258/aws-issue-load-dataset-from-s3-fails-with-unexpected-keyword-argument-error-in\r\n\r\nFull stacktrace:\r\n```\r\n.../site-packages/datasets/load.py:2549: in load_dataset\r\n    builder_instance.download_and_prepare(\r\n.../site-packages/datasets/builder.py:1005: in download_and_prepare\r\n    self._download_and_prepare(\r\n.../site-packages/datasets/builder.py:1078: in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n.../site-packages/datasets/packaged_modules/csv/csv.py:147: in _split_generators\r\n    data_files = dl_manager.download_and_extract(self.config.data_files)\r\n.../site-packages/datasets/download/download_manager.py:562: in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n.../site-packages/datasets/download/download_manager.py:426: in download\r\n    downloaded_path_or_paths = map_nested(\r\n.../site-packages/datasets/utils/py_utils.py:466: in map_nested\r\n    mapped = [\r\n.../site-packages/datasets/utils/py_utils.py:467: in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n.../site-packages/datasets/utils/py_utils.py:387: in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n.../site-packages/datasets/utils/py_utils.py:387: in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n.../site-packages/datasets/utils/py_utils.py:370: in _single_map_nested\r\n    return function(data_struct)\r\n.../site-packages/datasets/download/download_manager.py:451: in _download\r\n    out = cached_path(url_or_filename, download_config=download_config)\r\n.../site-packages/datasets/utils/file_utils.py:188: in cached_path\r\n    output_path = get_from_cache(\r\n...1/site-packages/datasets/utils/file_utils.py:511: in get_from_cache\r\n    response = fsspec_head(url, storage_options=storage_options)\r\n.../site-packages/datasets/utils/file_utils.py:316: in fsspec_head\r\n    fs, _, paths = fsspec.get_fs_token_paths(url, storage_options=storage_options)\r\n.../site-packages/fsspec/core.py:622: in get_fs_token_paths\r\n    fs = filesystem(protocol, **inkwargs)\r\n.../site-packages/fsspec/registry.py:290: in filesystem\r\n    return cls(**storage_options)\r\n.../site-packages/fsspec/spec.py:79: in __call__\r\n    obj = super().__call__(*args, **kwargs)\r\n.../site-packages/s3fs/core.py:187: in __init__\r\n    self.s3 = self.connect()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <s3fs.core.S3FileSystem object at 0x1500a1310>, refresh = True\r\n\r\n    def connect(self, refresh=True):\r\n        \"\"\"\r\n        Establish S3 connection object.\r\n    \r\n        Parameters\r\n        ----------\r\n        refresh : bool\r\n            Whether to create new session/client, even if a previous one with\r\n            the same parameters already exists. If False (default), an\r\n            existing one will be used if possible\r\n        \"\"\"\r\n        if refresh is False:\r\n            # back compat: we store whole FS instance now\r\n            return self.s3\r\n        anon, key, secret, kwargs, ckwargs, token, ssl = (\r\n            self.anon, self.key, self.secret, self.kwargs,\r\n            self.client_kwargs, self.token, self.use_ssl)\r\n    \r\n        if not self.passed_in_session:\r\n>           self.session = botocore.session.Session(**self.kwargs)\r\nE           TypeError: Session.__init__() got an unexpected keyword argument 'hf'\r\n```\n\n### Steps to reproduce the bug\n\n1. Assuming a valid CSV file located at `s3://bucket/data.csv`\r\n2. Run the below code:\r\n```\r\nstorage_options = {\r\n    \"key\": \"...\",\r\n    \"secret\": \"...\",\r\n    \"client_kwargs\": {\r\n        \"endpoint_url\": \"...\",\r\n    }\r\n}\r\nload_dataset(\"csv\", data_files=\"s3://bucket/data.csv\", storage_options=storage_options)\r\n```\r\n\r\nEncountered in version `2.16.1` but also reproduced in `2.16.0` and `2.15.0`.\r\n\r\nNote: I encountered this in a unit test using a `moto` mock for S3, however since the error occurs before the session is instantiated, it should not be the issue.\n\n### Expected behavior\n\nNo exception is raised, the boto3 session is created successfully, and the CSV file is downloaded successfully and returned as a dataset.\r\n\r\n===\r\n\r\nAfter some research I found that `DownloadConfig` has a `__post_init__` method that always forces this value to be set in its `storage_options`, even though in case of an S3 location the storage options get passed on to the S3 Session which does not expect this parameter. I assume this parameter is needed when reading from the huggingface hub and should not be set in this context.\r\n\r\nUnfortunately there is nothing the user can do to work around it. Even if you manually do something like:\r\n```\r\ndownload_config = DownloadConfig()\r\ndel download_config.storage_options[\"hf\"]\r\nload_dataset(\"csv\", data_files=\"s3://bucket/data.csv\", download_config=download_config)\r\n```\r\nthe library will still reinsert this parameter when `download_config = self.download_config.copy()` in line 418 of `download_manager.py` (`DownloadManager.download`).\r\n\r\nTherefore `load_dataset` currently cannot be used to read a dataset in CSV format from an S3 location.\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: macOS-14.2.1-arm64-arm-64bit\r\n- Python version: 3.11.7\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0\r\n",
    "comments": [
      {
        "user": "deepbot86",
        "body": "I am facing similar issue while reading a csv file from s3. Wondering if somebody has found a workaround. "
      },
      {
        "user": "ChenchaoZhao",
        "body": "same thing happened to other formats like parquet"
      },
      {
        "user": "SinaTavakoli",
        "body": "I am facing similar issue while reading a parquet file from s3.\r\ni try with every version between 2.14 to 2.16.1 but it dosen't  work "
      }
    ]
  },
  {
    "issue_number": 7323,
    "title": "Unexpected cache behaviour using load_dataset",
    "author": "Moritz-Wirth",
    "state": "closed",
    "created_at": "2024-12-12T14:03:00Z",
    "updated_at": "2025-01-31T11:34:24Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nFollowing the (Cache management)[https://huggingface.co/docs/datasets/en/cache] docu and previous behaviour from datasets version 2.18.0, one is able to change the cache directory. Previously, all downloaded/extracted/etc files were found in this folder. As i have recently update to the latest version this is not the case anymore. Downloaded files are stored in `~/.cache/huggingface/hub`.\r\nProviding the `cache_dir` argument in `load_dataset` the cache directory is created and there are some files but the bulk is still in `~/.cache/huggingface/hub`.\r\n\r\nI believe this could be solved by adding the cache_dir argument [here](https://github.com/huggingface/datasets/blob/fdda5585ab18ea1292547f36c969d12c408ab842/src/datasets/utils/file_utils.py#L188)\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nFor example using https://huggingface.co/datasets/ashraq/esc50:\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"ashraq/esc50\", \"default\", cache_dir=\"~/custom/cache/path/esc50\")\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect the bulk of files related to the dataset to be stored somewhere in `~/custom/cache/path/esc50`, but it seems they are in `~/.cache/huggingface/hub/datasets--ashraq--esc50`. \r\n\r\n### Environment info\r\n\r\n- `datasets` version: 3.2.0\r\n- Platform: Linux-5.14.0-503.15.1.el9_5.x86_64-x86_64-with-glibc2.34\r\n- Python version: 3.10.14\r\n- `huggingface_hub` version: 0.26.5\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.6.1",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Since `datasets` 3.x,  the `datasets` specific files are in `cache_dir=` and the HF files are cached using `huggingface_hub` and you can set its cache directory using the `HF_HOME` environment variable.\r\n\r\nThey are independent, for example you can delete the Hub cache (containing downloaded files) but still reload your cached datasets from the `datasets` cache (containing prepared datasets in Arrow format)"
      }
    ]
  },
  {
    "issue_number": 3444,
    "title": "Align the Dataset and IterableDataset processing API",
    "author": "lhoestq",
    "state": "open",
    "created_at": "2021-12-16T11:26:11Z",
    "updated_at": "2025-01-31T11:07:07Z",
    "labels": [
      "enhancement",
      "generic discussion"
    ],
    "body": "## Intro\n\nitems marked like <s>this</s> are done already :)\n\nCurrently the two classes have two distinct API for processing:\n\n### The `.map()` method\n\nBoth have those parameters in common: function, batched, batch_size\n\n- IterableDataset is missing those parameters:\n<s>with_indices</s>, with_rank, <s>input_columns</s>, <s>drop_last_batch</s>, <s>remove_columns</s>, features, disable_nullable, fn_kwargs, num_proc\n\n- Dataset also has additional parameters that are exclusive, due to caching:\nkeep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, suffix_template, new_fingerprint\n\n- <s>There is also an important difference in terms of behavior:\n**Dataset.map adds new columns** (with dict.update)\nBUT\n**IterableDataset discards previous columns** (it overwrites the dict)\nIMO the two methods should have the same behavior. This would be an important breaking change though.</s>\n\n- Dataset.map is eager while IterableDataset.map is lazy\n\n### The `.shuffle()` method\n\n- <s>Both have an optional seed parameter, but IterableDataset requires a mandatory parameter buffer_size to control the size of the local buffer used for approximate shuffling.</s>\n\n- <s>IterableDataset is missing the parameter generator</s>\n\n- Also Dataset has exclusive parameters due to caching: keep_in_memory, load_from_cache_file, indices_cache_file_name, writer_batch_size, new_fingerprint\n\n### The `.with_format()` method\n\n- <s>IterableDataset only supports \"torch\" (it misses tf, jax, pandas, arrow)</s> and is missing the parameters: columns, output_all_columns and format_kwargs\n- other methods like `set_format`, `reset_format` or `formatted_as` are also missing\n\n### Other methods\n\n- Both have the same `remove_columns` method\n- IterableDataset is missing: <s>cast</s>, <s>cast_column</s>, <s>filter</s>, <s>rename_column</s>, <s>rename_columns</s>, class_encode_column, flatten, train_test_split, <s>shard</s>\n- Some other methods are missing but we can discuss them: set_transform, formatted_as, with_transform\n- And others don't really make sense for an iterable dataset: select, sort, <s>add_column</s>, add_item\n- Dataset is missing skip and take, that IterableDataset implements.\n\n## Questions\n\nI think it would be nice to be able to switch between streaming and regular dataset easily, without changing the processing code significantly.\n\n1. What should be aligned and what shouldn't between those two APIs ?\n\nIMO the minimum is to align the main processing methods.\n\nIt would mean aligning breaking the current `Iterable.map` to have the same behavior as `Dataset.map` (add columns with dict.update), and add multiprocessing as well as the missing parameters. DONE ✅\n\nIt would also mean implementing the missing methods: cast, cast_column, filter, rename_column, rename_columns, class_encode_column, flatten, prepare_for_task, train_test_split, shard. WIP 🟠\n\n2. What are the breaking changes for IterableDataset ?\n\nThe main breaking change would be the change of behavior of `IterableDataset.map`, because currently it discards all the previous columns instead of keeping them. DONE ✅\n\n3. Shall we also do some changes for regular datasets ?\n\nI agree the simplest would be to have the exact same methods for both Dataset and IterableDataset. However this is probably not a good idea because it would prevent users from using the best benefits of them. That's why we can keep some aspects of regular datasets as they are:\n- keep the eager Dataset.map with caching\n- keep the with_transform method for lazy processing\n- keep Dataset.select (it could also be added to IterableDataset even though it's not recommended)\n\nWe could have a completely aligned `map` method if both methods were lazy by default, but this is a very big breaking change so I'm not sure we can consider doing that.\n\nFor information, TFDS does lazy map by default, and has an additional `.cache()` method.\n\n## Opinions ?\n\nI'd love to gather some opinions about this here. If the two APIs are more aligned it would be awesome for the examples in `transformers`, and it would create a satisfactory experience for users that want to switch from one mode to the other.\n\ncc @mariosasko @albertvillanova @thomwolf @patrickvonplaten @sgugger ",
    "comments": [
      {
        "user": "thomwolf",
        "body": "Yes I agree, these should be as aligned as possible. Maybe we can also check the feedback in the survey at http://hf.co/oss-survey and see if people mentioned related things on the API (in particular if we go the breaking change way, it would be good to be sure we are taking the right direction for the community)."
      },
      {
        "user": "mariosasko",
        "body": "I like this proposal.\r\n\r\n> There is also an important difference in terms of behavior:\r\nDataset.map adds new columns (with dict.update)\r\nBUT\r\nIterableDataset discards previous columns (it overwrites the dict)\r\nIMO the two methods should have the same behavior. This would be an important breaking change though.\r\n\r\n> The main breaking change would be the change of behavior of IterableDataset.map, because currently it discards all the previous columns instead of keeping them.\r\n\r\nYes, this behavior of `IterableDataset.map` was surprising to me the first time I used it because I was expecting the same behavior as `Dataset.map`, so I'm OK with the breaking change here.\r\n\r\n> IterableDataset only supports \"torch\" (it misses tf, jax, pandas, arrow) and is missing the parameters: columns, output_all_columns and format_kwargs\r\n\r\n\\+  it's also missing the actual formatting code (we return unformatted tensors)\r\n> We could have a completely aligned map method if both methods were lazy by default, but this is a very big breaking change so I'm not sure we can consider doing that.\r\n\r\n> For information, TFDS does lazy map by default, and has an additional .cache() method.\r\n\r\nIf I understand this part correctly, the idea would be for `Dataset.map` to behave similarly to `Dataset.with_transform` (lazy processing) and to have an option to cache processed data (with `.cache()`). This idea is really nice because it can also be applied to `IterableDataset` to fix https://github.com/huggingface/datasets/issues/3142 (again we get the aligned APIs). However, this change would break a lot of things, so I'm still not sure if this is a step in the right direction (maybe it's OK for Datasets 2.0?) \r\n> If the two APIs are more aligned it would be awesome for the examples in transformers, and it would create a satisfactory experience for users that want to switch from one mode to the other.\r\n\r\nYes, it would be amazing to have an option to easily switch between these two modes.\r\n\r\nI agree with the rest.\r\n"
      },
      {
        "user": "lhoestq",
        "body": "> If I understand this part correctly, the idea would be for Dataset.map to behave similarly to Dataset.with_transform (lazy processing) and to have an option to cache processed data (with .cache()). This idea is really nice because it can also be applied to IterableDataset to fix #3142 (again we get the aligned APIs). However, this change would break a lot of things, so I'm still not sure if this is a step in the right direction (maybe it's OK for Datasets 2.0?)\r\n\r\nYea this is too big of a change in my opinion. Anyway it's fine as it is right now with streaming=lazy and regular=eager."
      }
    ]
  },
  {
    "issue_number": 7377,
    "title": "Support for sparse arrays with the Arrow Sparse Tensor format?",
    "author": "JulesGM",
    "state": "open",
    "created_at": "2025-01-21T20:14:35Z",
    "updated_at": "2025-01-30T14:06:45Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nAI in biology is becoming a big thing. One thing that would be a huge benefit to the field that Huggingface Datasets doesn't currently have is native support for **sparse arrays**. \n\n\nArrow has support for sparse tensors. \nhttps://arrow.apache.org/docs/format/Other.html#sparse-tensor\n\n\nIt would be a big deal if Hugging Face Datasets supported sparse tensors as a feature type, natively. \n\n\n### Motivation\n\nThis is important for example in the field of transcriptomics (modeling and understanding gene expression), because a large fraction of the genes are not expressed (zero). More generally, in science, sparse arrays are very common, so adding support for them would be very benefitial, it would make just using Hugging Face Dataset objects a lot more straightforward and clean.\n\n\n### Your contribution\n\nWe can discuss this further once the team comments of what they think about the feature, and if there were previous attempts at making it work, and understanding their evaluation of how hard it would be. My intuition is that it should be fairly straightforward, as the Arrow backend already supports it.",
    "comments": [
      {
        "user": "lhoestq",
        "body": "Hi ! Unfortunately the Sparse Tensor structure in Arrow is not part of the Arrow format (yes it's confusing...), so it's not possible to use it in `datasets`. It's a separate structure that doesn't correspond to any type or extension type in Arrow.\n\nThe Arrow community recently added an extension type for fixed shape tensors at https://arrow.apache.org/docs/format/CanonicalExtensions.html#fixed-shape-tensor, it should be possible to contribute an extension type for sparse tensors as well."
      }
    ]
  }
]