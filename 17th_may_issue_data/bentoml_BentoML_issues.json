[
  {
    "issue_number": 5380,
    "title": "bug: Mismatch between log and behavior when both requirements_txt and packages are specified",
    "author": "isuyyy",
    "state": "open",
    "created_at": "2025-06-04T07:09:16Z",
    "updated_at": "2025-06-04T07:09:37Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nIt is said that the packages field will be ignored when requirements_txt and packages are both given([here](https://github.com/bentoml/BentoML/blob/main/src/bentoml/_internal/bento/build_config.py#L499-L510)), but it actually does not.\n\n### To reproduce\n\nUse the following files:\n\n`pyproject.toml`\n\n```toml\n[project]\nname = \"serving\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"==3.9.6\"\ndependencies = [\n    \"loguru\"\n]\n\n[tool.bentoml.build]\nservice = \"serving.service:QuickStart\"\ninclude = [\"*.py\"]\nexclude = [\"tests/\"]\n\n[tool.bentoml.build.python]\nrequirements_txt = \"requirements.txt\"\n```\n\n`requirements.txt`\n\n```\nclick\n```\n\nThen run:\n\n```\nbentoml build \n```\n\n### Expected behavior\n\nOnly the packages in requirements.txt (i.e. click) should be installed, and loguru from pyproject.toml should be ignored.\n\nActual behavior:\nBoth click and loguru are included in the Bento (verified via `bentoml get ${bento_name}`\n\n### Environment\n\nbentoml: 1.4.15",
    "comments": []
  },
  {
    "issue_number": 5376,
    "title": "feature: Support building Bento using uv.lock instead of requirements.txt",
    "author": "isuyyy",
    "state": "closed",
    "created_at": "2025-05-30T14:19:48Z",
    "updated_at": "2025-06-04T03:24:27Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nWhen developing a service using [uv](https://github.com/astral-sh/uv), it's common to initialize the project with `uv init` and manage dependencies via `uv add`, which generates a `uv.lock` file to capture an exact snapshot of the environment.\n\nIt would be great if BentoML could support `uv.lock` as a first-class source for building environments, as an alternative to `requirements.txt` or inferring from `pyproject.toml`. If `uv.lock` is present and `lock_packages: false` is set in `bentofile.yaml`, BentoML could leverage the pre-resolved lock file to ensure reproducibility and avoid re-resolving dependencies unnecessarily.\n\n### Motivation\n\nCurrently, BentoML copies the `[project].dependencies` from `pyproject.toml` into `bento_home/bentos/envs/python/requirements.txt` and resolves them again, independently of the existing `uv.lock`. This can lead to discrepancies between the development environment and the Bento environment, reducing reproducibility.\n\n### Other\n\n_No response_\n",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Hi there, thanks for bringing this up.\n\nBy default, we use uv to compile `requirements.txt.lock` when using with build. iiuc `uv.lock` is platform independent. However, we are also using uv internally to install packages (you can check out the generated containerfile in your bento), which is super fast.\n\nIt would be great if you can provide us with some numbers to see if the improvement is worth the increased complexity.\n\nalso cc @frostming for visibility."
      },
      {
        "user": "frostming",
        "body": "Created a PR to export the dependencies to requirements.txt from image API"
      }
    ]
  },
  {
    "issue_number": 5372,
    "title": "feature: opentelemetry logs exporter for all logs, not only monitor",
    "author": "MattiasDC",
    "state": "open",
    "created_at": "2025-05-27T15:56:48Z",
    "updated_at": "2025-06-03T16:39:11Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nI would like it if it was possible to redirect all logs easily to an opentelemetry collector. Currently I only see monitoring related logs in my collector. I'm using these opentelemetry environment variables:\n- OTEL_LOGS_EXPORTER\n- OTEL_PYTHON_LOG_LEVEL\n- OTEL_EXPORTER_OTLP_ENDPOINT\n- OTEL_EXPORTER_OTLP_PROTOCOL\n- OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED\n- OTEL_PYTHON_LOG_CORRELATION\n\nI run my bentoml command prepended with `opentelemetry-instrument`, but it seems that bentoml starts subprocesses which are not automatically instrumented.\n\n### Motivation\n\nThis feature would make it easy to set up opentelemetry log collector for all logs outputted by bentoml directly with otlp\n\n### Other\n\nThe reason for this seems to come from this code: https://github.com/open-telemetry/opentelemetry-python-contrib/blob/e2ba6d43c0d9906e664da09abe8f5404e9026ec0/opentelemetry-instrumentation/src/opentelemetry/instrumentation/auto_instrumentation/__init__.py#L123",
    "comments": [
      {
        "user": "aarnphm",
        "body": "See https://docs.bentoml.com/en/latest/build-with-bentoml/observability/tracing.html#id3"
      },
      {
        "user": "MattiasDC",
        "body": "But isn't that only for the traces themselves? E.g. in [this example](https://opentelemetry.io/docs/zero-code/python/logs-example/) there is both a traces exporter and a logs exporter\n\n```\nexport OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true\nopentelemetry-instrument \\\n  --traces_exporter console,otlp \\\n  --metrics_exporter console,otlp \\\n  --logs_exporter console,otlp \\\n  --service_name python-logs-example \\\n  python $(pwd)/example.py\n```\n"
      },
      {
        "user": "MattiasDC",
        "body": "For example, I ran into an issue with a deployed model and found a trace with a 500 (internal server error) response. I then wanted to see the callstack of the internal server error. For this I had to check the container logs, as the logs themselves are not available in ELK, as they are not being send to the collector. Only the logs of the `bentoml_monitor_data` logger are being sent to opentelemetry.\nhttps://github.com/bentoml/BentoML/blob/1b09445b97b310f736cff7e2fd059f8873bd3d40/src/bentoml/_internal/monitoring/otlp.py#L202-L220\n\nThere is no way to configure this for the root logger and since BentoML uses `Watcher`, autoinstrumentation is to my understanding not possible"
      }
    ]
  },
  {
    "issue_number": 5375,
    "title": "bug: When installed with uv, there is assertion error: version 1 does not have a validation schema",
    "author": "kftam1994",
    "state": "closed",
    "created_at": "2025-05-30T11:47:55Z",
    "updated_at": "2025-06-03T01:35:21Z",
    "labels": [
      "feedback-wanted"
    ],
    "body": "### Describe the bug\n\nIt raises error when the following code is run:\nimport bentoml\n\nclient = bentoml.SyncHTTPClient('http://localhost:3000')\n\nThe error traceback indicates it is from this line:\nhttps://github.com/bentoml/BentoML/blob/1b09445b97b310f736cff7e2fd059f8873bd3d40/src/bentoml/_internal/configuration/helpers.py#L111\n\n### To reproduce\n\nimport bentoml\n\nclient = bentoml.SyncHTTPClient('http://localhost:3000')\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: 1.4.15\npython: 3.12\nplatform: Windows ",
    "comments": [
      {
        "user": "frostming",
        "body": "It looks impossible from reading the code.\n\nCan you run this?\n\n```python\nimport bentoml._internal.configuration.v1 as v1\nv1.SCHEMA\n```"
      },
      {
        "user": "kftam1994",
        "body": "I found the reason that there is a schema.py in my code repository and conflict with schema package\nthank you "
      }
    ]
  },
  {
    "issue_number": 4884,
    "title": "bug: Getting KeyError when Name or version is missing from YAML Config",
    "author": "muneebable",
    "state": "open",
    "created_at": "2024-07-26T17:17:59Z",
    "updated_at": "2025-05-30T09:09:18Z",
    "labels": [
      "feedback-wanted"
    ],
    "body": "### Describe the bug\r\n\r\nI was doing a bentoml build and I am getting the error\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"~/usr/vendors/pyenv/versions/housing_valuation/bin/bentoml\", line 8, in <module>\r\n    sys.exit(cli())\r\n             ^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml_cli/utils.py\", line 362, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml_cli/utils.py\", line 333, in wrapper\r\n    return_value = func(*args, **kwargs)\r\n                   ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/click/decorators.py\", line 33, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml_cli/utils.py\", line 290, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/click/decorators.py\", line 33, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml_cli/bentos.py\", line 371, in build\r\n    ).save(_bento_store)\r\n      ^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml/_internal/bento/bento.py\", line 390, in save\r\n    with bento_store.register(self.tag) as bento_path:\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/lib/python3.11/contextlib.py\", line 144, in __exit__\r\n    next(self.gen)\r\n  File ~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml/_internal/store.py\", line 177, in register\r\n    or self.get(_tag).creation_time >= self.get(_tag.name).creation_time\r\n       ^^^^^^^^^^^^^^\r\n  File \"/~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml/_internal/store.py\", line 141, in get\r\n    return self._get_item(_tag)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File ~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml/_internal/store.py\", line 102, in _get_item\r\n    return self._item_type.from_fs(self._fs.opendir(tag.path()))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml/_internal/bento/bento.py\", line 307, in from_fs\r\n    info = BentoInfo.from_yaml_file(bento_yaml)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~/usr/vendors/pyenv/versions/3.11.4/envs/housing_valuation/lib/python3.11/site-packages/bentoml/_internal/bento/bento.py\", line 539, in from_yaml_file\r\n    yaml_content[\"tag\"] = Tag(yaml_content[\"name\"], yaml_content[\"version\"])\r\n                              ~~~~~~~~~~~~^^^^^^^^\r\nKeyError: 'name'\r\n```\r\n\r\nThis error should be catched instead of failing the build. My yaml content look like this\r\n\r\n```yaml\r\n{\r\n  \"service\": \"service:svc\",\r\n  \"bentoml_version\": \"1.1.1\",\r\n  \"creation_time\": \"2024-07-26T17:03:11.523664+00:00\",\r\n  \"labels\": {\r\n    \"owner\": \"root\",\r\n    \"stage\": \"dev\"\r\n  },\r\n  \"models\": [\r\n    {\r\n      \"tag\": \"project_neighbor_scaler:usnn32sloc3v4bgt\",\r\n      \"module\": \"bentoml.sklearn\",\r\n      \"creation_time\": \"2024-07-26T17:01:04.181244+00:00\"\r\n    },\r\n    {\r\n      \"tag\": \"project_target_scaler:usmvjqcloc4u6bgt\",\r\n      \"module\": \"bentoml.sklearn\",\r\n      \"creation_time\": \"2024-07-26T17:01:04.171380+00:00\"\r\n    },\r\n    {\r\n      \"tag\": \"project_model:usip2gklocshgbgt\",\r\n      \"module\": \"bentoml.pytorch\",\r\n      \"creation_time\": \"2024-07-26T17:01:04.122590+00:00\"\r\n    }\r\n  ],\r\n  \"runners\": [\r\n    {\r\n      \"name\": \"project_model\",\r\n      \"runnable_type\": \"NewClass\",\r\n      \"embedded\": False,\r\n      \"models\": [\r\n        \"project_model:usip2gklocshgbgt\"\r\n      ],\r\n      \"resource_config\": None\r\n    },\r\n    {\r\n      \"name\": \"project_target_scaler\",\r\n      \"runnable_type\": \"SklearnRunnable\",\r\n      \"embedded\": False,\r\n      \"models\": [\r\n        \"project_target_scaler:usmvjqcloc4u6bgt\"\r\n      ],\r\n      \"resource_config\": None\r\n    },\r\n    {\r\n      \"name\": \"project_neighbor_scaler\",\r\n      \"runnable_type\": \"SklearnRunnable\",\r\n      \"embedded\": False,\r\n      \"models\": [\r\n        \"project_neighbor_scaler:usnn32sloc3v4bgt\"\r\n      ],\r\n      \"resource_config\": None\r\n    }\r\n  ],\r\n  \"apis\": [\r\n    {\r\n      \"name\": \"inference\",\r\n      \"input_type\": \"JSON\",\r\n      \"output_type\": \"JSON\"\r\n    }\r\n  ],\r\n  \"docker\": {\r\n    \"distro\": \"debian\",\r\n    \"python_version\": \"3.11\",\r\n    \"cuda_version\": None,\r\n    \"env\": None,\r\n    \"system_packages\": [\r\n      \"libblas-dev\",\r\n      \"liblapack-dev\",\r\n      \"gfortran\"\r\n    ],\r\n    \"setup_script\": None,\r\n    \"base_image\": None,\r\n    \"dockerfile_template\": None\r\n  },\r\n  \"python\": {\r\n    \"requirements_txt\": \"requirements/requirements.txt\",\r\n    \"packages\": None,\r\n    \"lock_packages\": None,\r\n    \"index_url\": None,\r\n    \"no_index\": None,\r\n    \"trusted_host\": None,\r\n    \"find_links\": None,\r\n    \"extra_index_url\": None,\r\n    \"pip_args\": None,\r\n    \"wheels\": None\r\n  },\r\n  \"conda\": {\r\n    \"environment_yml\": None,\r\n    \"channels\": None,\r\n    \"dependencies\": None,\r\n    \"pip\": None\r\n  }\r\n}\r\n```\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nbentoml: 1.1.1\r\npython 3.11.4\r\n",
    "comments": [
      {
        "user": "frostming",
        "body": "Can you try it on the latest version?"
      },
      {
        "user": "maurin-at-homiwoo",
        "body": "I'm a colleague of Muneeb who reported the bug and we got it again in our CI system. I investigate it a little bit and got more information on the issue. \n\nThe issue comes when we updated the bentoml version on a system that already generated model in the old version. This old version don't expect a `name` field in the bentofile.yaml. \n\nIn the folder `$HOME/bentoml/bentos/my-model-name` we had a few folders with model generated with the old versions. Their `bento.yaml` file don't have a `name` field. \n\nWhen we upgraded the version of bentoml, we added the `name` in our bentofile.yaml. The error arise when we try to build the model. It seems that when bento tries to read the config of the last version, it fails because the `name` field is not present. \n\nThe workaround is either to rename the model or to empty the old bentos for this model. "
      }
    ]
  },
  {
    "issue_number": 5374,
    "title": "feature: How to change APT source when building BentoML container image? Slow build due to using deb.debian.org",
    "author": "darlingbud",
    "state": "open",
    "created_at": "2025-05-30T07:36:33Z",
    "updated_at": "2025-05-30T07:36:33Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nWhen I use the bentoml containerize command to build a Docker image, the build process is very slow due to network issues. The log shows that it's spending a lot of time on:\n```\n=> [base-container  6/12] RUN apt-get update && apt-get install -q -y --no-install-recommends --allow-  237.7s\n```\nThis seems to be caused by the default use of `http://deb.debian.org/debian` as the APT package source, which is very slow in China.\n\nI have successfully solved the pip source issue before by configuring a domestic mirror (e.g., Tsinghua or Aliyun), but now I'm facing similar performance issues with the Debian APT sources during the BentoML container build.\n\nI tried to customize the Dockerfile template using:\n```\ndocker:\n  dockerfile_template: \"./Dockerfile_template.j2\"\n```\nAnd added the following lines in the Dockerfile template to replace the APT sources with Aliyun's mirror:\n```\n# Replace APT sources with Aliyun mirror\nRUN sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list && \\\n    apt-get update && \\\n    apt-get upgrade -y\n```\nHowever, this causes an error during the build step under the stage:\n```\nFROM none AS base-container\n```\nIs there any way to configure the APT source used during the BentoML container build process?\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 5373,
    "title": "feature:  `index_url` from bentofile.yaml not reflected in generated Dockerfile",
    "author": "darlingbud",
    "state": "closed",
    "created_at": "2025-05-29T08:46:12Z",
    "updated_at": "2025-05-30T02:47:29Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\n\nI followed the documentation to configure the `bentofile.yaml` and added the `index_url` parameter under the `python` section. However, this configuration does not seem to be reflected in the generated Dockerfile.\n\nThe `--index-url` argument is missing during package installation in the Docker build stage. As a result, dependencies are installed from the default PyPI source instead of the specified mirror.\n\nthis is bentofile.yaml\n```\nservice: \"service:svc\"\ndescription :\n  This is an inline description for the Service. BentoML is awesome!\n\ninclude:\n  - \"*.py\"\n\npython:\n  requirements_txt: './requirements.txt'\n  index_url: \"https://pypi.tuna.tsinghua.edu.cn/simple\"\n  # lock_packages: false\n  \ndocker:\n  distro: debian\n  python_version: \"3.12.3\"\n  \n```\nand the dockerfile part like this\n```\n# install python packages\nRUN uv --directory ./env/python/ pip install -r requirements.txt\n```\n\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "It should be in the final `requirements.txt`, please check it."
      },
      {
        "user": "darlingbud",
        "body": "> It should be in the final `requirements.txt`, please check it.\n\nThank you !, It's in the final requirements.txt.\nbut I also have  problems  , I have a question regarding the Dockerfile used in BentoML's containerization process.\n\nIn the Dockerfile, there is a step like this:\n`RUN command -v uv >/dev/null || pip install uv.`\n\nDue to some network restrictions, I need the pip install command to use a custom PyPI mirror like the --index-url option. So I tried modifying the Dockerfile located at `/bentoml/bentos/.../Dockerfile`, and added an environment variable like this:\n`ENV PIP_INDEX_URL=https://mirrors.aliyun.com/pypi/simple` .\n\nHowever, when I tested the bentoml containerize command, it didn't seem to take effect. From what I observed, it looks like BentoML generates a temporary Dockerfile in the` /tmp `directory during the build process, which may not be the same as the Dockerfile under `/bentoml/bentos`.\n\nSo my questions are:\n\nIs it possible to customize or override the Dockerfile used during the bentoml containerize step?\nHow can I ensure that all pip install commands (including those executed by tools like uv) respect a specified --index-url?\nCan setting ENV PIP_INDEX_URL=... in the Dockerfile affect the entire build context?\nAny help or guidance would be greatly appreciated!"
      },
      {
        "user": "frostming",
        "body": "As the env vars are set at an early stage: https://github.com/bentoml/BentoML/blob/6eea834188f4452a127332097339df84603b546a/src/bentoml/_internal/container/frontend/dockerfile/templates/base_v2.j2#L52-L56\n\nYou can set env `PIP_INDEX_URL` to do that:\n\n```yaml\nenvs:\n  - name: PIP_INDEX_URL\n    value: https://mirrors.aliyun.com/pypi/simple\n```"
      }
    ]
  },
  {
    "issue_number": 5371,
    "title": "setup_script docker option doesn't work in base_v2.j2 as they are copied too late in the Dockerfile",
    "author": "MattiasDC",
    "state": "closed",
    "created_at": "2025-05-27T13:22:56Z",
    "updated_at": "2025-05-28T06:54:45Z",
    "labels": [],
    "body": "This issue is the same as was solved for [base.j2](https://github.com/bentoml/BentoML/blob/6eea834188f4452a127332097339df84603b546a/src/bentoml/_internal/container/frontend/dockerfile/templates/base.j2#L78) template in #3713. The setup scripts reside in ./env/docker but this content is only copied to the image after the _post_commands_ run\nhttps://github.com/bentoml/BentoML/blob/6eea834188f4452a127332097339df84603b546a/src/bentoml/_internal/container/frontend/dockerfile/templates/base_v2.j2#L79-L84\n\nIn my case I get the following error when building the docker image:\n[ 8/11] RUN chmod +x ./env/docker/script__e1b7bffceef9a7dfaaa2f6f68507a616 && ./env/docker/script__e1b7bffceef9a7dfaaa2f6f68507a616:\n0.261 chmod: cannot access './env/docker/script__e1b7bffceef9a7dfaaa2f6f68507a616': No such file or directory",
    "comments": [
      {
        "user": "frostming",
        "body": "> The setup scripts reside in ./env/docker but this content is only copied to the image after the _post_commands_ run\n\nIncorrect, https://github.com/bentoml/BentoML/blob/6eea834188f4452a127332097339df84603b546a/src/bentoml/_internal/container/frontend/dockerfile/templates/base_v2.j2#L63"
      },
      {
        "user": "MattiasDC",
        "body": "Thanks for pointing that out to me, seems that it was fixed with a recent version"
      }
    ]
  },
  {
    "issue_number": 5365,
    "title": "bug: Bentoml Pytorch model serve bug",
    "author": "Shayantan1012",
    "state": "open",
    "created_at": "2025-05-18T06:47:48Z",
    "updated_at": "2025-05-19T00:36:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n```\nC:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\io.py:7: BentoMLDeprecationWarning: bentoml.io is deprecated since BentoML v1.4 and will be removed in a future version. Please upgrade to new style IO types instead.   \n  warn_deprecated(\nC:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\Xray\\ml\\model\\model_service.py:11: BentoMLDeprecationWarning: bentoml.pytorch is deprecated since v1.4 and will be removed in a future version.\n  bento_model = bentoml.pytorch.get(BENTOML_MODEL_NAME)\nC:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\models\\model.py:354: BentoMLDeprecationWarning: Runner is deprecated since BentoML v1.4 and will be removed in a future version. Please upgrade to new style services.\n  return Runner(\nC:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\Xray\\ml\\model\\model_service.py:15: BentoMLDeprecationWarning: bentoml.Service is deprecated since BentoML v1.4 and will be removed in a future version. Please upgrade to @bentoml.service().\n  svc = bentoml.Service(name=BENTOML_SERVICE_NAME, runners=[runner])\nLoaded model: Model(tag=\"xray_model:evviuwztuohmoaav\")\nCustom objects: {'xray_train_transforms': Compose(\n    Resize(size=224, interpolation=bilinear, max_size=None, antialias=True)\n    CenterCrop(size=(224, 224))\n    ColorJitter(brightness=(0.9, 1.1), contrast=(0.9, 1.1), saturation=(0.9, 1.1), hue=(-0.1, 0.1))\n    RandomHorizontalFlip(p=0.5)\n    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)}\nTransform key found: True\n2025-05-18T11:25:11+0530 [ERROR] [runner:xray_model:1] An exception occurred while instantiating runner 'xray_model', see details below:\n2025-05-18T11:25:11+0530 [ERROR] [runner:xray_model:1] Traceback (most recent call last):\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 313, in init_local\n    self._set_handle(LocalRunnerRef)\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 155, in _set_handle\n    runner_handle = handle_class(self, *args, **kwargs)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\runner\\runner_handle\\local.py\", line 27, in __init_\n    self._runnable = runner.runnable_class(**runner.runnable_init_params)  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 52, in __init_\n    functools.partial(cls._init_, *args, **kwargs)(\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 76, in __init_\n    self.model: ModelType = loader(bento_model, device_id=self.device_id)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\frameworks\\pytorch.py\", line 81, in load_model\n    model: \"torch.nn.Module\" = torch.load(\n                               ^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\torch\\serialization.py\", line 1524, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint.\n        (1) In PyTorch 2.6, we changed the default value of the weights_only argument in torch.load from False to True. Re-running torch.load with weights_only set to False will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n        (2) Alternatively, to load with weights_only=True please check the recommended steps in the following error message.  \n        WeightsUnpickler error: Unsupported global: GLOBAL Xray.ml.model.arch.Net was not an allowed global by default. Please use torch.serialization.add_safe_globals([Xray.ml.model.arch.Net]) or the torch.serialization.safe_globals([Xray.ml.model.arch.Net]) context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n\n2025-05-18T11:25:11+0530 [ERROR] [runner:xray_model:1] Traceback (most recent call last):\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\starlette\\routing.py\", line 692, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n  File \"C:\\Program Files\\Python312\\Lib\\contextlib.py\", line 210, in _aenter_\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\server\\base_app.py\", line 74, in lifespan\n    ret = on_startup(app)\n          ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\utils\\init_.py\", line 460, in wrapper\n    return func()\n           ^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 323, in init_local\n    raise e\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 313, in init_local\n    self._set_handle(LocalRunnerRef)\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 155, in _set_handle\n    runner_handle = handle_class(self, *args, **kwargs)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\runner\\runner_handle\\local.py\", line 27, in __init_\n    self._runnable = runner.runnable_class(**runner.runnable_init_params)  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 52, in __init_\n    functools.partial(cls._init_, *args, **kwargs)(\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 76, in __init_\n    self.model: ModelType = loader(bento_model, device_id=self.device_id)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\frameworks\\pytorch.py\", line 81, in load_model\n    model: \"torch.nn.Module\" = torch.load(\n                               ^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\torch\\serialization.py\", line 1524, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint.\n        (1) In PyTorch 2.6, we changed the default value of the weights_only argument in torch.load from False to True. Re-running torch.load with weights_only set to False will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n        (2) Alternatively, to load with weights_only=True please check the recommended steps in the following error message.  \n        WeightsUnpickler error: Unsupported global: GLOBAL Xray.ml.model.arch.Net was not an allowed global by default. Please use torch.serialization.add_safe_globals([Xray.ml.model.arch.Net]) or the torch.serialization.safe_globals([Xray.ml.model.arch.Net]) context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n\n2025-05-18T11:25:11+0530 [ERROR] [runner:xray_model:1] Traceback (most recent call last):\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\starlette\\routing.py\", line 692, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n  File \"C:\\Program Files\\Python312\\Lib\\contextlib.py\", line 210, in _aenter_\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\server\\base_app.py\", line 74, in lifespan\n    ret = on_startup(app)\n          ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\utils\\init_.py\", line 460, in wrapper\n    return func()\n           ^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 323, in init_local\n    raise e\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 313, in init_local\n    self._set_handle(LocalRunnerRef)\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 155, in _set_handle\n    runner_handle = handle_class(self, *args, **kwargs)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\runner\\runner_handle\\local.py\", line 27, in __init_\n    self._runnable = runner.runnable_class(**runner.runnable_init_params)  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 52, in __init_\n    functools.partial(cls._init_, *args, **kwargs)(\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 76, in __init_\n    self.model: ModelType = loader(bento_model, device_id=self.device_id)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\frameworks\\pytorch.py\", line 81, in load_model\n    model: \"torch.nn.Module\" = torch.load(\n                               ^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\torch\\serialization.py\", line 1524, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint.\n        (1) In PyTorch 2.6, we changed the default value of the weights_only argument in torch.load from False to True. Re-running torch.load with weights_only set to False will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n        (2) Alternatively, to load with weights_only=True please check the recommended steps in the following error message.  \n        WeightsUnpickler error: Unsupported global: GLOBAL Xray.ml.model.arch.Net was not an allowed global by default. Please use torch.serialization.add_safe_globals([Xray.ml.model.arch.Net]) or the torch.serialization.safe_globals([Xray.ml.model.arch.Net]) context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n\n2025-05-18T11:25:11+0530 [ERROR] [runner:xray_model:1] Application startup failed. Exiting.\n2025-05-18T11:25:11+0530 [ERROR] [runner:xray_model:1] Application startup failed. Exiting.\n-----------------------------------------------------------------------\nC:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\io.py:7: BentoMLDeprecationWarning: bentoml.io is deprecated since BentoML v1.4 and will be removed in a future version. Please upgrade to new style IO types instead.   \n  warn_deprecated(\nC:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\Xray\\ml\\model\\model_service.py:11: BentoMLDeprecationWarning: bentoml.pytorch is deprecated since v1.4 and will be removed in a future version.\n  bento_model = bentoml.pytorch.get(BENTOML_MODEL_NAME)\nC:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\models\\model.py:354: BentoMLDeprecationWarning: Runner is deprecated since BentoML v1.4 and will be removed in a future version. Please upgrade to new style services.\n  return Runner(\nC:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\Xray\\ml\\model\\model_service.py:15: BentoMLDeprecationWarning: bentoml.Service is deprecated since BentoML v1.4 and will be removed in a future version. Please upgrade to @bentoml.service().\n  svc = bentoml.Service(name=BENTOML_SERVICE_NAME, runners=[runner])\nLoaded model: Model(tag=\"xray_model:evviuwztuohmoaav\")\nCustom objects: {'xray_train_transforms': Compose(\n    Resize(size=224, interpolation=bilinear, max_size=None, antialias=True)\n    CenterCrop(size=(224, 224))\n    ColorJitter(brightness=(0.9, 1.1), contrast=(0.9, 1.1), saturation=(0.9, 1.1), hue=(-0.1, 0.1))\n    RandomHorizontalFlip(p=0.5)\n    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)}\nTransform key found: True\n2025-05-18T11:25:21+0530 [ERROR] [runner:xray_model:1] An exception occurred while instantiating runner 'xray_model', see details below:\n2025-05-18T11:25:21+0530 [ERROR] [runner:xray_model:1] Traceback (most recent call last):\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 313, in init_local\n    self._set_handle(LocalRunnerRef)\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 155, in _set_handle\n    runner_handle = handle_class(self, *args, **kwargs)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\runner\\runner_handle\\local.py\", line 27, in __init_\n    self._runnable = runner.runnable_class(**runner.runnable_init_params)  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 52, in __init_\n    functools.partial(cls._init_, *args, **kwargs)(\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 76, in __init_\n    self.model: ModelType = loader(bento_model, device_id=self.device_id)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\frameworks\\pytorch.py\", line 81, in load_model\n    model: \"torch.nn.Module\" = torch.load(\n                               ^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\torch\\serialization.py\", line 1524, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint.\n        (1) In PyTorch 2.6, we changed the default value of the weights_only argument in torch.load from False to True. Re-running torch.load with weights_only set to False will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n        (2) Alternatively, to load with weights_only=True please check the recommended steps in the following error message.  \n        WeightsUnpickler error: Unsupported global: GLOBAL Xray.ml.model.arch.Net was not an allowed global by default. Please use torch.serialization.add_safe_globals([Xray.ml.model.arch.Net]) or the torch.serialization.safe_globals([Xray.ml.model.arch.Net]) context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n\n2025-05-18T11:25:21+0530 [ERROR] [runner:xray_model:1] Traceback (most recent call last):\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\starlette\\routing.py\", line 692, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n  File \"C:\\Program Files\\Python312\\Lib\\contextlib.py\", line 210, in _aenter_\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\server\\base_app.py\", line 74, in lifespan\n    ret = on_startup(app)\n          ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\utils\\init_.py\", line 460, in wrapper\n    return func()\n           ^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 323, in init_local\n    raise e\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 313, in init_local\n    self._set_handle(LocalRunnerRef)\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 155, in _set_handle\n    runner_handle = handle_class(self, *args, **kwargs)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\runner\\runner_handle\\local.py\", line 27, in __init_\n    self._runnable = runner.runnable_class(**runner.runnable_init_params)  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 52, in __init_\n    functools.partial(cls._init_, *args, **kwargs)(\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 76, in __init_\n    self.model: ModelType = loader(bento_model, device_id=self.device_id)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\frameworks\\pytorch.py\", line 81, in load_model\n    model: \"torch.nn.Module\" = torch.load(\n                               ^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\torch\\serialization.py\", line 1524, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint.\n        (1) In PyTorch 2.6, we changed the default value of the weights_only argument in torch.load from False to True. Re-running torch.load with weights_only set to False will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n        (2) Alternatively, to load with weights_only=True please check the recommended steps in the following error message.  \n        WeightsUnpickler error: Unsupported global: GLOBAL Xray.ml.model.arch.Net was not an allowed global by default. Please use torch.serialization.add_safe_globals([Xray.ml.model.arch.Net]) or the torch.serialization.safe_globals([Xray.ml.model.arch.Net]) context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n\n2025-05-18T11:25:21+0530 [ERROR] [runner:xray_model:1] Traceback (most recent call last):\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\starlette\\routing.py\", line 692, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n  File \"C:\\Program Files\\Python312\\Lib\\contextlib.py\", line 210, in _aenter_\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\server\\base_app.py\", line 74, in lifespan\n    ret = on_startup(app)\n          ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\utils\\init_.py\", line 460, in wrapper\n    return func()\n           ^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 323, in init_local\n    raise e\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 313, in init_local\n    self._set_handle(LocalRunnerRef)\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\runner\\runner.py\", line 155, in _set_handle\n    runner_handle = handle_class(self, *args, **kwargs)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\runner\\runner_handle\\local.py\", line 27, in __init_\n    self._runnable = runner.runnable_class(**runner.runnable_init_params)  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 52, in __init_\n    functools.partial(cls._init_, *args, **kwargs)(\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\internal\\frameworks\\common\\pytorch.py\", line 76, in __init_\n    self.model: ModelType = loader(bento_model, device_id=self.device_id)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\bentoml\\_internal\\frameworks\\pytorch.py\", line 81, in load_model\n    model: \"torch.nn.Module\" = torch.load(\n                               ^^^^^^^^^^^\n  File \"C:\\Users\\SHAYANTAN BISWAS\\Desktop\\DeepLearningProject\\venv\\Lib\\site-packages\\torch\\serialization.py\", line 1524, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint.\n        (1) In PyTorch 2.6, we changed the default value of the weights_only argument in torch.load from False to True. Re-running torch.load with weights_only set to False will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n        (2) Alternatively, to load with weights_only=True please check the recommended steps in the following error message.  \n        WeightsUnpickler error: Unsupported global: GLOBAL Xray.ml.model.arch.Net was not an allowed global by default. Please use torch.serialization.add_safe_globals([Xray.ml.model.arch.Net]) or the torch.serialization.safe_globals([Xray.ml.model.arch.Net]) context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n```\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n```yaml\nservice: \"Xray.ml.model.model_service:svc\"\nlabels:\n    owner: Shayantan Biswas\n    stage: dev\ninclude:\n    - \"Xray/ml/model/model_service.py\"\n    - \"Xray/constant/TrainingPipeline/__init__.py\"\npython:\n    packages:\n        - torch\n        - torchvision\n        - Pillow\n        - numpy\n    extra_index_url:\n        - \"https://download.pytorch.org/whl/cpu\"\n```",
    "comments": [
      {
        "user": "frostming",
        "body": "Please upgrade to the usage of `@bentoml.service` and load the Pytorch model yourself."
      }
    ]
  },
  {
    "issue_number": 4500,
    "title": "bug: KeyError: 'name'",
    "author": "sunnysavita10",
    "state": "closed",
    "created_at": "2024-02-12T10:16:16Z",
    "updated_at": "2025-05-17T09:53:08Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\n![image](https://github.com/bentoml/BentoML/assets/56354186/4fd514f3-e944-423c-95f2-cb6f567fe9fd)\r\n\r\ni was getting the keyerror while i was running the code\r\nbentoml version.\r\nbentoml==1.0.25\r\nbentoml==1.0.10\r\n\r\nwith the latest version also\r\npip install bentoml==1.2.2\r\n\r\nhere i have tested with the python==3.8, 3.9,3,10\r\n\r\n\n\n### To reproduce\n\nnone\n\n### Expected behavior\n\nnone\n\n### Environment\n\nnone",
    "comments": [
      {
        "user": "frostming",
        "body": "What code you were running and what command you were invoking?"
      },
      {
        "user": "MikeOMa",
        "body": "Hey,\n\nI had a similar issue.\n\nI think there was a bentoml version that wrote an invalid bento.yaml file to `/Users/name/bentoml/bentos/project_name` (1.0.2 is my guess but not sure).\n\n\n\nThen even if I updated my python bentoml version it would try to parse the broken file created by the old bentoml version.\n\nTo fix: just delete the bentos folder above if theres nothing important in it.\n\nTo reproduce: Go to that folder, break a bento.yaml file in the latest bento by deleting name and version.\nThen try to run bentoml build in the main project.\n\n"
      },
      {
        "user": "henrykohl",
        "body": "Try to install the following packages\nbentoml==1.3.0\nscikit-learn==1.2.2\ncattrs==23.1.1\npydantic==2.5.1\n\nI have tested with the python==3.8. Everything is good."
      }
    ]
  },
  {
    "issue_number": 5343,
    "title": "bug: `client_cls is httpx.Client` pattern does not allow customizing `client_cls`",
    "author": "Kakadus",
    "state": "closed",
    "created_at": "2025-05-06T18:09:12Z",
    "updated_at": "2025-05-07T17:07:08Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWe wanted to pass custom limits to the httpx client, so that the number of connections and max keepalive connections are changed. As bentoml does not support this, we subclasses `httpx.AsyncClient` and set the limits there. We apply the limits by overriding client_cls, but this does not work.\n\n Bentoml uses `client_cls is httpx.Client` and `client_cls is httpx.AsyncClient` several times, but `MyHttpxClient is not httpx.AsyncClient` although it inherits from it. If bentoml would use `issubclass`, then everything would be fine.\n\nPlease not that this is not directly a feature request for custom limits directly (that would be also great!), but rather a request for extensibility of the bentoml client.\n\n### To reproduce\n\n```py\nclass MyHttpxClient(httpx.AsyncClient): ...\n\n\nclass MyClient(bentoml.AsyncHTTPClient):\n    client_cls = MyHttpxClient\n\n\nasync def test_it() -> None:\n    async with MyClient(...) as client:\n        await client.predict(...)\n```\n\nThis fails with `RuntimeError: Attempted to send an sync request with an AsyncClient instance.` because of \nhttps://github.com/bentoml/BentoML/blob/a567a3a42867206b7defdd5db0541a4fd5b23a14/src/_bentoml_impl/client/http.py#L226-L233: \n(recall that `self.client_cls is not httpx.AsyncClient`)\n\n### Expected behavior\n\nBentoml uses the overridden client class and it works.\n\n### Environment\n\nbentoml: a567a3a42867206b7defdd5db0541a4fd5b23a14\npython: 3.12\nOS: Arch Linux",
    "comments": [
      {
        "user": "aarnphm",
        "body": "This is not necessarily a bug, but we might want to check for subclass here @frostming "
      },
      {
        "user": "frostming",
        "body": "I think the correct way to customize is to override `_make_client` method."
      },
      {
        "user": "Kakadus",
        "body": "Thanks a lot! The response & fix time is really incredible! 🚀 "
      }
    ]
  },
  {
    "issue_number": 5333,
    "title": "bug: Cannot set index-url while containerizing",
    "author": "kobiche",
    "state": "closed",
    "created_at": "2025-04-25T11:09:27Z",
    "updated_at": "2025-04-27T03:12:04Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nHey!\n\nI have the following problem when I run the `bentoml containerize` command:\n```\n => ERROR [base-container 11/13] RUN --mount=type=cache,target=/root/.cache/ uv --directory ./env/python/ pip install -r requirements.txt                            0.5s\n------\n > [base-container 11/13] RUN --mount=type=cache,target=/root/.cache/ uv --directory ./env/python/ pip install -r requirements.txt:\n0.320 error: Multiple `--index-url` values provided at requirements.txt:6:89\n------\nDockerfile:47\n--------------------\n  45 |     COPY --chown=bentoml:bentoml ./env/python ./env/python/\n  46 |     # install python packages\n  47 | >>> RUN --mount=type=cache,target=/root/.cache/ uv --directory ./env/python/ pip install -r requirements.txt\n  48 |\n  49 |\n--------------------\nERROR: failed to solve: process \"/bin/sh -c uv --directory ./env/python/ pip install -r requirements.txt\" did not complete successfully: exit code: 2\nERROR:\nEncountered exception while trying to building image: Command '['/usr/bin/docker', 'build', '--tag', 'smarter_embeddings:0.1.0', '--file', '/tmp/tmpjmx1ifq7fsTempFS/env/docker/Dockerfile', '/tmp/tmpjmx1ifq7fsTempFS/']' returned non-zero exit status 1.\nTraceback (most recent call last):\n  File \"/srv/workspaces/ricardoch/AI/smarter/.venv/lib/python3.11/site-packages/bentoml/_internal/container/__init__.py\", line 251, in build\n    return builder.build(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/workspaces/ricardoch/AI/smarter/.venv/lib/python3.11/site-packages/bentoml/_internal/container/base.py\", line 190, in build\n    raise BentoMLException(str(e)) from None\nbentoml.exceptions.BentoMLException: Command '['/usr/bin/docker', 'build', '--tag', 'smarter_embeddings:0.1.0', '--file', '/tmp/tmpjmx1ifq7fsTempFS/env/docker/Dockerfile', '/tmp/tmpjmx1ifq7fsTempFS/']' returned non-zero exit status 1.\n```\n\nThis is due to the fact that I set my own index-url in the TOML file.\n\n### To reproduce\n\nDirectory structure:\n└── embedding/\n    ├── build_bentoml.sh\n    ├── embedding_service.py\n    ├── pyproject.toml\n    └── requirements.txt\n\n\nFile: build_bentoml.sh\n```\n##### IMPORTANT ######\nexport INDEX_PWD=\"...\"\nexport INDEX_USR=\"...\"\nexport VERSION=\"0.1.0\"\n\nIMAGE_NAME=embeddings\nDEPLOYMENT_NAME=\"${IMAGE_NAME}:${VERSION}\"\n\nbentoml delete ${DEPLOYMENT_NAME} -y -q\nbentoml build --name ${IMAGE_NAME} --version ${VERSION}\necho \"Current list of bentoml\"\nbentoml list\nbentoml containerize ${DEPLOYMENT_NAME}\n```\n\n\nFile: embedding_service.py\n```\nimport logging.config\nimport typing\n\nimport bentoml\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom torch import Tensor\n\n\nmodel_name = \"BAAI/bge-m3\"\nworkers = 1\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nSAMPLE_SENTENCES = [\n    \"The sun dips below the horizon, painting the sky orange.\",\n    \"A gentle breeze whispers through the autumn leaves.\",\n    \"The moon casts a silver glow on the tranquil lake.\",\n    \"A solitary lighthouse stands guard on the rocky shore.\",\n    \"The city awakens as morning light filters through the streets.\",\n    \"Stars twinkle in the velvety blanket of the night sky.\",\n    \"The aroma of fresh coffee fills the cozy kitchen.\",\n    \"A curious kitten pounces on a fluttering butterfly.\"\n]\n\ndictConfig = {\n    'version': 1,\n    'formatters': {\n        'extended': {'format': '%(asctime)s %(levelname)-1.1s %(name)s, %(funcName)s(%(lineno)d): %(message)s'}},\n    'handlers': {\n        'console': {\n            'formatter': 'extended',\n            'class': 'logging.StreamHandler'}},\n    'loggers': {\n        'uvicorn': {'level': 'INFO'},\n        'uvicorn.error': {'level': 'INFO'},\n        'uvicorn.access': {'level': 'WARNING'}},\n    'root': {'level': 'INFO',\n             'handlers': ['console']}}\nlogging.config.dictConfig(dictConfig)\nlogger = logging.getLogger(__name__)\nlogger.info(f\"Serving with device: {device}\")\n\n\n@bentoml.service(\n    workers=workers,\n    traffic={\"timeout\": 60},\n    metrics={\"enabled\": True}\n)\nclass SentenceTransformers:\n    def __init__(self) -> None:\n        # Load model and tokenizer\n        self.device = device\n        self.model = SentenceTransformer(model_name).to(self.device)\n        logger.info(f\"Model loaded device: {self.device}\")\n\n    @bentoml.api(batchable=True)\n    def encode(\n        self,\n        sentences: typing.List[str] = SAMPLE_SENTENCES,\n    ) -> Tensor:\n        logger.debug(f\"encoding sentences: {len(sentences)}\")\n        # Tokenize sentences\n        sentence_embeddings = self.model.encode(sentences, show_progress_bar=False)\n        return sentence_embeddings\n```\n\n\nFile: pyproject.toml\n```\n[tool.bentoml.build]\nservice = \"embeddings:SentenceTransformers\"\n\n[tool.bentoml.build.python]\nrequirements_txt = \"requirements.txt\"\nlock_packages = false  # true also does not work\nindex_url = \"https://${INDEX_USR}:${INDEX_PWD}@private-index/repository/pypi-group/simple/\"\nextra_index_url = [\n    \"https://download.pytorch.org/whl/cu118\",\n    \"https://download.pytorch.org/whl/cu121\",\n    \"https://download.pytorch.org/whl/cu124\",\n    \"https://download.pytorch.org/whl/cpu\",\n]\n```\n\nFile: requirements.txt\n```\nsentence-transformers>=4.1.0,<5.0.0\ntorch>=2.6.0,<3.0.0\ntransformers>=4.51.3,<5.0.0\n```\n\n### Expected behavior\n\nBeing able to re-define the index-url.\n\n### Environment\n\nbentoml: 1.4.10\npython: 3.11.11\nplatform: Ubuntu 20.04.6",
    "comments": []
  },
  {
    "issue_number": 4134,
    "title": "feature: Native SpaCy runner for v1.0.0 architecture",
    "author": "connorbrinton",
    "state": "open",
    "created_at": "2023-08-18T18:10:56Z",
    "updated_at": "2025-04-26T14:53:03Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nBentoML previously supported SpaCy as one of its built-in runner types. However, with the introduction of the v1.0.0 release, native support for SpaCy was removed.\r\n\r\nIt would be great to have native support for a built-in SpaCy runner 🙂\n\n### Motivation\n\nWe've successfully deployed BentoML for a PyTorch-based image classification application, but also heavily use SpaCy in production. We'd like to move these SpaCy-based services over to BentoML, and having native support for SpaCy in BentoML would make this easier.\r\n\r\nAs a workaround, we plan to implement a custom runner for SpaCy for our use case. We'll try to reuse code from [the previous implementation](https://github.com/larme/BentoML/blob/40da498fd82f0e682499251c809e1928529e7f71/bentoml/_internal/frameworks/spacy.py#L306) as much as possible.\n\n### Other\n\nSince we'll already be writing a (custom) SpaCy runner, we may be able to contribute our implementation to BentoML if that would be of interest",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Happy to review if you submit a PR"
      },
      {
        "user": "SergheiDinu",
        "body": "Did u manage to have one?"
      },
      {
        "user": "ved1beta",
        "body": "working on this :D"
      }
    ]
  },
  {
    "issue_number": 3836,
    "title": "BentoML Integration with the Flux Operator",
    "author": "vsoch",
    "state": "closed",
    "created_at": "2023-05-06T19:28:37Z",
    "updated_at": "2025-04-26T03:58:43Z",
    "labels": [],
    "body": "hi! I'm a developer for the [Flux Operator](https://github.com/flux-framework/flux-operator), which (from a high level) let's you deploy an HPC (supercomputer) cluster on Kubernetes. It comes down to a set of networked pods (via an indexed job) and we've been able to run distributed setups on it from Tensorflow, Pytorch, etc.\r\n\r\nI just stumbled on BentoML, and aside from loving the idea and docs (I love the dark background with pink!) I'm interested in understanding how I might deploy Bento in our operator. What I can easily do now is launch (and expose) a service from a main pod that we call the broker (that has created a network for the other pods) and I could easily run an equivalent bento start command in the same way. What I'm interested in is how to setup or configure bento so that it can register / be aware of other machines (akin to some of the distributed ML libraries to provide hostnames) _or_ I could write a plugin that would work directly with the flux scheduler to allocate / get results back for tasks from the main server.\r\n\r\nApologies for my ignorance about how everything works - I just stumbled on it this morning and am really excited to try it out! If you have suggestions for the above, what I might look at first, it would be greatly appreciated!",
    "comments": [
      {
        "user": "vsoch",
        "body": "What exactly about this issue is \"not planned\" - a person actually responding to me? In a timeframe of years later? 🙃 "
      }
    ]
  },
  {
    "issue_number": 4111,
    "title": "feature: Use `json` mode to dump pydantic v2 models by default",
    "author": "andletenkov",
    "state": "closed",
    "created_at": "2023-08-10T11:49:27Z",
    "updated_at": "2025-04-26T00:03:37Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nHi! I think it will be convenient to dump pydantic v2 models via `model_dump(mode=\"json\")` because in many cases it will allow to omit an implementation of custom json encoders and let `pydantic-core` do the job.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 4089,
    "title": "bug: error in deploying in production mode (while it is running okay with --deployment)",
    "author": "flavourabbit",
    "state": "closed",
    "created_at": "2023-08-01T10:03:58Z",
    "updated_at": "2025-04-26T00:03:37Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\n\r\nI implemented a Transformers based project using Huggingface module.\r\nAlthough it runs okay with --deployment `bentoml serve service:svc --development --reload`,\r\nBentoML spits out error when I try to inference, specifically `bentoml serve service:svc --development --reload`\r\n\r\n### To reproduce\r\n\r\nAs this is a confidential project, I can't share the code snippet.\r\n\r\nHowever, the error log says\r\n```\r\nbentoml/src/bentoml/_internal/runner/runner_handle/remote.py\", line 244, in async_run_method\r\n    raise RemoteException(\r\nbentoml.exceptions.RemoteException: An unexpected exception occurred in remote runner owlvit_runner: [404] Not Found\r\n```\r\n\r\nI altered the `bentoml/_internal/runner/runner_handle/remote.py` to print out the detail.\r\n```\r\n        try:\r\n            print(resp)\r\n            content_type = resp.headers[\"Content-Type\"]\r\n            print('content_type', content_type)\r\n            assert content_type.lower().startswith(\"application/vnd.bentoml.\")    # <- this is making the exception\r\n```\r\n\r\nThen, it shows as following,\r\n```\r\n<ClientResponse(http://127.0.0.1:8000/predict) [404 Not Found]>\r\n<CIMultiDictProxy('Date': 'Tue, 01 Aug 2023 09:55:27 GMT', 'Server': 'uvicorn', 'Content-Length': '9', 'Content-Type': 'text/plain; charset=utf-8')>\r\ncontent_type text/plain; charset=utf-8\r\n```\r\n\r\n1) I wonder why ClientResponse is looking into http://127.0.0.1:8000/predict\r\n    although I sent post request to  'http://a.b.c.d:3000/predict_image' \r\n\r\n2) It is odd that the API is working in development mode\r\n\r\nI've searched the Internet for debugging this issue but no luck.\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nRun smoothly as same way in development mode.\r\n\r\n### Environment\r\n\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.1.0.post7+g1ed3e6ff.d20230801\r\n`python`: 3.10.6\r\n`platform`: Linux-5.19.0-46-generic-x86_64-with-glibc2.35\r\n`uid_gid`: 1000:1000\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiofiles==23.1.0\r\naiohttp==3.8.4\r\naiosignal==1.3.1\r\naltair==5.0.1\r\nanyio==3.7.0\r\nappdirs==1.4.4\r\napturl==0.5.2\r\nasgiref==3.5.0\r\nasync-timeout==4.0.2\r\nattrs==23.1.0\r\nbcrypt==3.2.0\r\n-e git+https://github.com/bentoml/bentoml.git@1ed3e6ffd9fcc2daa0097782996f472bf5acb9c3#egg=bentoml\r\nblinker==1.4\r\nBrlapi==0.8.3\r\nbuild==0.10.0\r\ncattrs==23.1.2\r\ncertifi==2020.6.20\r\nchardet==4.0.0\r\ncharset-normalizer==3.1.0\r\ncircus==0.18.0\r\nclick==8.0.3\r\nclick-option-group==0.5.6\r\ncloudpickle==2.2.1\r\ncmake==3.26.4\r\ncolorama==0.4.4\r\ncommand-not-found==0.3\r\nconfigobj==5.0.6\r\ncontextlib2==21.6.0\r\ncontourpy==1.1.0\r\ncryptography==3.4.8\r\ncupshelpers==1.0\r\ncycler==0.11.0\r\ndbus-python==1.2.18\r\ndeepmerge==1.1.0\r\ndefer==1.0.6\r\nDeprecated==1.2.14\r\ndill==0.3.6\r\ndistlib==0.3.7\r\ndistro==1.7.0\r\ndistro-info===1.1build1\r\ndnspython==2.1.0\r\nduplicity==0.8.21\r\nexceptiongroup==1.1.1\r\nfastapi==0.98.0\r\nfasteners==0.14.1\r\nffmpy==0.3.0\r\nfilelock==3.12.2\r\nfonttools==4.40.0\r\nfrozenlist==1.3.3\r\nfs==2.4.16\r\nfsspec==2023.6.0\r\nftfy==6.1.1\r\nfuture==0.18.2\r\ngpg===1.16.0-unknown\r\ngradio==3.35.2\r\ngradio_client==0.2.7\r\nh11==0.14.0\r\nhttpcore==0.17.2\r\nhttplib2==0.20.2\r\nhttpx==0.24.1\r\nhuggingface-hub==0.15.1\r\nidna==3.3\r\nimportlib-metadata==6.0.1\r\niotop==0.6\r\njeepney==0.7.1\r\nJinja2==3.1.2\r\njsonschema==4.17.3\r\nkeyring==23.5.0\r\nkiwisolver==1.4.4\r\nlanguage-selector==0.1\r\nlaunchpadlib==1.10.16\r\nlazr.restfulclient==0.14.4\r\nlazr.uri==1.0.6\r\nlinkify-it-py==2.0.2\r\nlit==16.0.6\r\nlockfile==0.12.2\r\nloguru==0.7.0\r\nlouis==3.20.0\r\nmacaroonbakery==1.3.1\r\nMako==1.1.3\r\nMarkdown==3.3.6\r\nmarkdown-it-py==2.2.0\r\nMarkupSafe==2.0.1\r\nmatplotlib==3.7.1\r\nmdit-py-plugins==0.3.3\r\nmdurl==0.1.2\r\nmonotonic==1.6\r\nmore-itertools==8.10.0\r\nmpmath==1.3.0\r\nmultidict==6.0.4\r\nnetifaces==0.11.0\r\nnetworkx==3.1\r\nnumpy==1.25.0\r\nnvidia-cublas-cu11==11.10.3.66\r\nnvidia-cuda-cupti-cu11==11.7.101\r\nnvidia-cuda-nvrtc-cu11==11.7.99\r\nnvidia-cuda-runtime-cu11==11.7.99\r\nnvidia-cudnn-cu11==8.5.0.96\r\nnvidia-cufft-cu11==10.9.0.58\r\nnvidia-curand-cu11==10.2.10.91\r\nnvidia-cusolver-cu11==11.4.0.1\r\nnvidia-cusparse-cu11==11.7.4.91\r\nnvidia-nccl-cu11==2.14.3\r\nnvidia-nvtx-cu11==11.7.91\r\noauthlib==3.2.0\r\nolefile==0.46\r\nopencv-python==4.7.0.72\r\nopentelemetry-api==1.18.0\r\nopentelemetry-instrumentation==0.39b0\r\nopentelemetry-instrumentation-aiohttp-client==0.39b0\r\nopentelemetry-instrumentation-asgi==0.39b0\r\nopentelemetry-sdk==1.18.0\r\nopentelemetry-semantic-conventions==0.39b0\r\nopentelemetry-util-http==0.39b0\r\norjson==3.9.1\r\npackaging==23.1\r\npandas==2.0.3\r\nparamiko==2.9.3\r\npascal-voc-writer==0.1.4\r\npathspec==0.11.2\r\npexpect==4.8.0\r\nPillow==9.0.1\r\npip-requirements-parser==32.0.1\r\npip-tools==7.1.0\r\nplatformdirs==3.10.0\r\nprometheus-client==0.17.1\r\nprotobuf==3.12.4\r\npsutil==5.9.0\r\nptyprocess==0.7.0\r\npycairo==1.20.1\r\npycups==2.0.1\r\npydantic==1.10.9\r\npydub==0.25.1\r\nPygments==2.15.1\r\nPyGObject==3.42.1\r\nPyJWT==2.3.0\r\npymacaroons==0.13.0\r\nPyNaCl==1.5.0\r\npynvml==11.5.0\r\npyparsing==2.4.7\r\npyproject_hooks==1.0.0\r\npyRFC3339==1.1\r\npyrsistent==0.19.3\r\npython-apt==2.4.0+ubuntu1\r\npython-dateutil==2.8.2\r\npython-debian===0.1.43ubuntu1\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.6\r\npytz==2022.1\r\npyxdg==0.27\r\nPyYAML==5.4.1\r\npyzmq==25.1.0\r\nregex==2023.6.3\r\nreportlab==3.6.8\r\nrequests==2.25.1\r\nrequests-toolbelt==0.9.1\r\nrich==13.5.0\r\nsafetensors==0.3.1\r\nschema==0.7.5\r\nscipy==1.11.1\r\nscreen-resolution-extra==0.0.0\r\nseaborn==0.12.2\r\nSecretStorage==3.3.1\r\nsemantic-version==2.10.0\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nssh-import-id==5.11\r\nstarlette==0.27.0\r\nsympy==1.12\r\nsystemd-python==234\r\nterminator==2.1.1\r\ntokenizers==0.13.3\r\ntomli==2.0.1\r\ntoolz==0.12.0\r\ntorch==2.0.1\r\ntorchvision==0.15.2\r\ntornado==6.3.2\r\ntqdm==4.65.0\r\ntransformers @ file://some/path\r\ntriton==2.0.0\r\ntyping_extensions==4.7.0\r\ntzdata==2023.3\r\nubuntu-advantage-tools==8001\r\nubuntu-drivers-common==0.0.0\r\nuc-micro-py==1.0.2\r\nufw==0.36.1\r\nultralytics==8.0.120\r\nunattended-upgrades==0.1\r\nurllib3==1.26.5\r\nusb-creator==0.3.7\r\nuvicorn==0.22.0\r\nvirtualenv==20.24.2\r\nwadllib==1.3.6\r\nwatchfiles==0.19.0\r\nwcwidth==0.2.6\r\nwebsockets==11.0.3\r\nwrapt==1.15.0\r\nwsproto==1.0.0\r\nxdg==5\r\nxkit==0.0.0\r\nyarl==1.9.2\r\nzipp==1.0.0\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "bojiang",
        "body": "@flavourabbit \r\nThe 8000 is the port of runner, and `predict` means you are calling runner by `runner.predict.run`"
      },
      {
        "user": "bojiang",
        "body": "The thing is, it seems that the runner doesn't have `predict` method on it. How did you create the runner?"
      }
    ]
  },
  {
    "issue_number": 4039,
    "title": "bug: PyTorch MLflow example's container returns 404 exception for API calls",
    "author": "majeranr",
    "state": "closed",
    "created_at": "2023-07-12T11:37:14Z",
    "updated_at": "2025-04-26T00:03:37Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nI was following the [official example ](https://github.com/bentoml/BentoML/tree/main/examples/mlflow/pytorch) but I encountered an issue with containerization while using default conda due to some dependencies error. Then I ran onto [this issue](https://github.com/bentoml/BentoML/issues/3146) and change the bentofile.yaml to:\r\n```\r\nservice: \"service:svc\"\r\ninclude:\r\n  - \"service.py\"\r\npython:\r\n  packages:\r\n    - torch\r\n    - torchvision\r\n    - mlflow\r\n    - protobuf\r\n    - bentoml\r\n```\r\n\r\nWhile `bentoml serve service.py:svc` works absolutely fine, same as the `bentoml containerize`, the container returns an error:\r\n\r\n```\r\n2023-07-12T11:15:06+0000 [INFO] [runner:mlflow_pytorch_mnist:1] _ (scheme=http,method=POST,path=http://127.0.0.1:8000/predict,type=application/octet-stream,length=9408) (status=404,type=text/plain; charset=utf-8,length=9) 2.084ms (trace=476453e61a33a7d0e6009adb8e691436,span=09664493bbd60a56,sampled=0,service.name=mlflow_pytorch_mnist)\r\n2023-07-12T11:15:06+0000 [ERROR] [api_server:15] Exception on /predict [POST] (trace=476453e61a33a7d0e6009adb8e691436,span=5cd753eedaa0957d,sampled=0,service.name=mlflow_pytorch_mnist_demo)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/bentoml/_internal/server/http_app.py\", line 341, in api_func\r\n    output = await api.func(*args)\r\n  File \"/home/bentoml/bento/src/service.py\", line 16, in predict\r\n    return await mnist_runner.predict.async_run(input_arr)\r\n  File \"/usr/local/lib/python3.7/site-packages/bentoml/_internal/runner/runner.py\", line 55, in async_run\r\n    return await self.runner._runner_handle.async_run_method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/bentoml/_internal/runner/runner_handle/remote.py\", line 244, in async_run_method\r\n    ) from None\r\nbentoml.exceptions.RemoteException: An unexpected exception occurred in remote runner mlflow_pytorch_mnist: [404] Not Found\r\n2023-07-12T11:15:06+0000 [INFO] [api_server:15] 172.17.0.1:35336 (scheme=http,method=POST,path=/predict,type=application/json,length=24007) (status=500,type=application/json,length=2) 66.715ms (trace=476453e61a33a7d0e6009adb8e691436,span=5cd753eedaa0957d,sampled=0,service.name=mlflow_pytorch_mnist_demo)\r\n```\r\n\r\nThe only one thing I changed in `mnist.py` is an addition of MLflow tracking server uri & experiment with lines:\r\n\r\n```\r\nos.environ['MLFLOW_TRACKING_TOKEN']=\"<token>\"\r\nos.environ['MLFLOW_TRACKING_SERVER_CERT_PATH']=\"<path to cert>\"\r\n\r\nmlflow.set_tracking_uri(\"<mlflow instance's url>\")\r\nmlflow.set_experiment(\"<experiment's name>\")\r\n```\r\n\r\nHowever changing the model in `service.py` from bentoml.mlflow to bentoml.pytorch (and adjusting model's name) also produces the same error.\r\n\r\nI also tried changing `service.py` from:\r\n\r\n```\r\nimport bentoml\r\n\r\nmnist_runner = bentoml.mlflow.get(\"mlflow_pytorch_mnist:latest\").to_runner()\r\n\r\nsvc = bentoml.Service(\"mlflow_pytorch_mnist_demo\", runners=[mnist_runner])\r\n\r\ninput_spec = bentoml.io.NumpyNdarray(\r\n    dtype=\"float32\",\r\n    shape=[-1, 1, 28, 28],\r\n    enforce_dtype=True,\r\n)\r\n\r\n\r\n@svc.api(input=input_spec, output=bentoml.io.NumpyNdarray())\r\nasync def predict(input_arr):\r\n    return await mnist_runner.predict.async_run(input_arr)\r\n```\r\n to:\r\n\r\n```\r\nimport bentoml\r\n\r\nmnist_runner = bentoml.mlflow.get(\"mlflow_pytorch_mnist:latest\").to_runner()\r\n\r\nsvc = bentoml.Service(\"mlflow_pytorch_mnist_demo\", runners=[mnist_runner])\r\n\r\ninput_spec = bentoml.io.NumpyNdarray(\r\n    dtype=\"float32\",\r\n    shape=[-1, 1, 28, 28],\r\n    enforce_dtype=True,\r\n)\r\n\r\n\r\n@svc.api(input=input_spec, output=bentoml.io.NumpyNdarray())\r\ndef predict(input_arr):\r\n    return mnist_runner.predict.run(input_arr)\r\n```\r\n But it also produced the same error.\r\n\r\nAccording to line:\r\n\r\n```\r\n2023-07-14T07:14:11+0000 [DEBUG] [api_server:10] Default runner method set to 'predict', it can be accessed both via 'runner.run' and 'runner.predict.async_run'.\r\n```\r\n\r\nI also tried changing `service.py` to:\r\n\r\n```\r\nimport bentoml\r\n\r\nmnist_runner = bentoml.mlflow.get(\"mlflow_pytorch_mnist:latest\").to_runner()\r\n\r\nsvc = bentoml.Service(\"mlflow_pytorch_mnist_demo\", runners=[mnist_runner])\r\n\r\ninput_spec = bentoml.io.NumpyNdarray(\r\n    dtype=\"float32\",\r\n    shape=[-1, 1, 28, 28],\r\n    enforce_dtype=True,\r\n)\r\n\r\n\r\n@svc.api(input=input_spec, output=bentoml.io.NumpyNdarray())\r\nasync def predict(input_arr):\r\n    return await mnist_runner.run(input_arr)\r\n```\r\n\r\nBut ended up with error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/bentoml/_internal/server/http_app.py\", line 341, in api_func\r\n    output = await api.func(*args)\r\n  File \"/home/bentoml/bento/src/service.py\", line 20, in predict\r\n    return await mnist_runner.run(input_arr)\r\n  File \"/usr/local/lib/python3.7/site-packages/bentoml/_internal/runner/runner.py\", line 52, in run\r\n    return self.runner._runner_handle.run_method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/bentoml/_internal/runner/runner_handle/remote.py\", line 290, in run_method\r\n    *args,\r\n  File \"/usr/local/lib/python3.7/site-packages/anyio/from_thread.py\", line 45, in run\r\n    raise RuntimeError(\"This function can only be run from an AnyIO worker thread\")\r\nRuntimeError: This function can only be run from an AnyIO worker thread\r\n2023-07-14T07:14:59+0000 [INFO] [api_server:9] 172.17.0.1:33646 (scheme=http,method=POST,path=/predict,type=application/json,length=24007) (status=500,type=application/json,length=110) 4.461ms (trace=bf16c819f82aadfe0a0292c52d7064ac,span=701c33aad979d04c,sampled=0,service.name=mlflow_pytorch_mnist_demo)\r\n```\r\n\r\nanyio 3.7.1 & aiohttp 3.8.4 are installed. \r\n\r\nI've been doing multiple tests with different versions of dependencies (including locking same versions as mentioned in conda.yaml), different model etc and result is still the same.\r\nI even logged onto a bash session to the container to see if all required files are there, but those were present.\r\nHowever running `bentoml models list` or `bentoml list` directly on the container does not return any results, is it an expected behaviour?\r\n\r\nExample from [BentoML Tutorial](https://docs.bentoml.org/en/latest/tutorial.html) works fine.\r\n\r\nUnfortunately after deeper research and support from another person I still have no idea what was not found\r\n\r\n### To reproduce\r\n\r\nSteps to reproduce:\r\n\r\n1. Follow [the example](https://github.com/bentoml/BentoML/tree/main/examples/mlflow/pytorch) until the step `bentoml containerize`\r\n2. According to [the issue](https://github.com/bentoml/BentoML/issues/3146): change the `bentofile.yaml` to:\r\n\r\n```\r\nservice: \"service:svc\"\r\ninclude:\r\n  - \"service.py\"\r\npython:\r\n  packages:\r\n    - torch\r\n    - torchvision\r\n    - mlflow\r\n    - protobuf\r\n    - bentoml\r\n```\r\n\r\n3. Continue with the example from GitHub\r\n4. Serve the model with `docker run -it --rm -p 3000:3000 mlflow_pytorch_mnist_demo:latest serve`\r\n5. Send the request with:\r\n\r\n```\r\ncurl -X POST -H \"Content-Type:application/json\"   -d @test_input.json   http://localhost:3000/predict\r\n```\r\n6. Result:\r\n\r\n```\r\n2023-07-12T11:22:41+0000 [INFO] [runner:mlflow_pytorch_mnist:1] _ (scheme=http,method=POST,path=http://127.0.0.1:8000/predict,type=application/octet-stream,length=9408) (status=404,type=text/plain; charset=utf-8,length=9) 1.489ms (trace=d6013656ce992cceb04176ef0dcc29b9,span=a86dcb8b3fd26f1b,sampled=0,service.name=mlflow_pytorch_mnist)\r\n2023-07-12T11:22:41+0000 [ERROR] [api_server:14] Exception on /predict [POST] (trace=d6013656ce992cceb04176ef0dcc29b9,span=c773d1402b639afe,sampled=0,service.name=mlflow_pytorch_mnist_demo)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/bentoml/_internal/server/http_app.py\", line 341, in api_func\r\n    output = await api.func(*args)\r\n  File \"/home/bentoml/bento/src/service.py\", line 20, in predict\r\n    return await mnist_runner.predict.async_run(input_arr)\r\n  File \"/usr/local/lib/python3.7/site-packages/bentoml/_internal/runner/runner.py\", line 55, in async_run\r\n    return await self.runner._runner_handle.async_run_method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/bentoml/_internal/runner/runner_handle/remote.py\", line 244, in async_run_method\r\n    ) from None\r\nbentoml.exceptions.RemoteException: An unexpected exception occurred in remote runner mlflow_pytorch_mnist: [404] Not Found\r\n2023-07-12T11:22:41+0000 [INFO] [api_server:14] 172.17.0.1:35380 (scheme=http,method=POST,path=/predict,type=application/json,length=24007) (status=500,type=application/json,length=2) 59.387ms (trace=d6013656ce992cceb04176ef0dcc29b9,span=c773d1402b639afe,sampled=0,service.name=mlflow_pytorch_mnist_demo)\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nModel should behave same as with `bentoml serve`, so return the 200 and prediction results:\r\n\r\n```\r\n2023-07-12T13:27:16+0200 [INFO] [runner:mlflow_pytorch_mnist:1] _ (scheme=http,method=POST,path=/predict,type=application/octet-stream,length=9408) (status=200,type=application/vnd.bentoml.NdarrayContainer,length=120) 5.956ms (trace=dd39b7dc12026999433920a234459293,span=e25a26f49ceca2f5,sampled=0,service.name=mlflow_pytorch_mnist)\r\n2023-07-12T13:27:16+0200 [INFO] [api_server:14] 127.0.0.1:36430 (scheme=http,method=POST,path=/predict,type=application/json,length=24007) (status=200,type=application/json,length=634) 56.324ms (trace=dd39b7dc12026999433920a234459293,span=5a9bab64b0880d24,sampled=0,service.name=mlflow_pytorch_mnist_demo)\r\n```\r\n\r\n&&\r\n\r\n```\r\n[[-3.6152830123901367, -5.332465171813965, -3.0992157459259033, -0.8537688255310059, -3.2960684299468994, -3.9919497966766357, -7.9404096603393555, -0.033282458782196045, -3.728358268737793, -0.5474755167961121], [-0.08536237478256226, -2.9697699546813965, -0.0837031900882721, -1.4068245887756348, -4.206423282623291, -1.7627538442611694, -0.270295113325119, -7.176737308502197, -0.5251529216766357, -4.596581935882568], [-2.902038097381592, -0.05778511241078377, -3.3463895320892334, -1.1108677387237549, -0.0533248595893383, -0.21076475083827972, -1.4418506622314453, -3.4429407119750977, -0.9558041095733643, -0.8879327178001404]]\r\n```\r\n\r\n### Environment\r\n\r\nbentoml: 1.0.23, tried also with 1.0.1 & 1.0.8\r\npython: 3.7.16, tried also with 3.8\r\nplatform: Linux\r\nmlflow: 1.30.1, tried also with 2.4, 2.4.2\r\ntorch: 1.8.1, tried also with 2.0.1\r\ntorchvision: 0.9.1, tried also with 0.15.2",
    "comments": []
  },
  {
    "issue_number": 4037,
    "title": "feat: List of Image IO Descriptor",
    "author": "ssheng",
    "state": "closed",
    "created_at": "2023-07-12T09:59:53Z",
    "updated_at": "2025-04-26T00:03:36Z",
    "labels": [],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 4016,
    "title": "feature: GGML support",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2023-07-03T21:30:34Z",
    "updated_at": "2025-04-26T00:03:36Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nSerialization for GGML:\r\n\r\n```python\r\nbentoml.ggml.save_model\r\n\r\nbentoml.ggml.load_model\r\n```\r\n\r\nIt is worth noting that `bentoml.ggml` also provides an entrypoint for converting the model weights from PyTorch, Tensorflow or HF directly to GGML:\r\n\r\n```python\r\nbentoml.ggml.convert_weights_to_ggml(\"/path/to/weight\", format: t.Literal['pt', 'tf', 'hf'] = ...)\r\n```\r\n\r\nGGML runner will be available with CoreML, CPU, and CUDA support:\r\n\r\n```python\r\n\r\nbentoml.ggml.get().to_runner() -> GGMLRunner\r\n```\r\n\r\nThe development for this feature will live under `bentoml/OpenLLM`, and I will port back to BentoML once the API is more mature.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3975,
    "title": "bug: numpy scalars (empty .shape) fail when passing to runner",
    "author": "mkmenta",
    "state": "closed",
    "created_at": "2023-06-21T15:06:53Z",
    "updated_at": "2025-04-26T00:03:36Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nHi!\r\nwhen I am trying to run something like\r\n```python3\r\nawait runner.async_run(np.array(10))\r\n```\r\nan `IndexError: tuple index out of range` is raised [here](https://github.com/bentoml/BentoML/blob/89e5fdaf20862a6361e1d68d75f0976a374bc97f/src/bentoml/_internal/runner/container.py#L304) because of the access to `batch.shape[batch_dim]`. Given that:\r\n```python3\r\n>>> import numpy as np\r\n>>> np.array(10).shape\r\n()\r\n```\r\n\r\n**Possible solution:**\r\nI tried substituting:\r\n```python3 \r\nreturn cls.create_payload(\r\n            pickle.dumps(batch),\r\n            batch.shape[batch_dim],\r\n            {\"format\": \"default\"},\r\n        )\r\n```\r\nby\r\n```python3\r\nreturn cls.create_payload(\r\n            pickle.dumps(batch),\r\n            -1,\r\n            {\"format\": \"default\"},\r\n        )\r\n```\r\nand the issue was fixed.\n\n### To reproduce\n\n1. Load any ONNX model in a runner (or any model that uses numpy as an input, I guess)\r\n2. Try executing something like:\r\n```python3\r\nawait runner.async_run(np.array(10))\r\n```\r\n\n\n### Expected behavior\n\nNo `IndexError: tuple index out of range` raised.\n\n### Environment\n\nbentoml: v1.0.22",
    "comments": [
      {
        "user": "bojiang",
        "body": "@mkmenta Do you use `batchable=True` during model saving? Would you like provide the model saving script and the service.py here? You can remove the sensitive information."
      },
      {
        "user": "mkmenta",
        "body": "@bojiang no I set it to false. Actually the model saving script was just the model saving with `batchable=False`:\r\n```python3\r\nsaved_model = bentoml.onnx.save_model(\"model\",\r\n                                      onnx.load(os.path.join(cur_path, \"model.onnx\")),\r\n                                      signatures={\"run\": {\"batchable\": False}}\r\n                                      )\r\n```"
      },
      {
        "user": "aarnphm",
        "body": "You will probably need to save the model with batch_size set to -1 when doing `onnx.export`\r\n\r\nToy example of `torch.onnx.export` \r\n\r\n```python\r\n    torch.onnx.export(\r\n        model,\r\n        im,\r\n        f,\r\n        verbose=False,\r\n        opset_version=opset,\r\n        input_names=['images'],\r\n        output_names=[\"outputs0\", \"outputs1\"],\r\n        dynamic_axes={\"outputs0\": {0: \"batch\", ...}})\r\n```"
      }
    ]
  },
  {
    "issue_number": 3756,
    "title": "bug: bentoml build issue when specifying dtype in PandasDataFrame",
    "author": "eric-shiu",
    "state": "closed",
    "created_at": "2023-04-15T00:25:53Z",
    "updated_at": "2025-04-26T00:03:35Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nI have a bento with an endpoint that takes a PandasDataFrame. I'm using .from_sample to provide example data, and using the `dtype` and enforce_dtype` parameters to enforce certain columns to string. \r\n\r\nUpon executing `bentoml build`, it throws the error:\r\n```\r\n  File \"/Users/Eric.Shiu/opt/miniconda3/envs/bento/lib/python3.8/site-packages/bentoml/_internal/io_descriptors/pandas.py\", line 72, in _openapi_types\r\n    if item.startswith(\"int\"):\r\nTypeError: startswith() takes at least 1 argument (0 given)\r\n```\r\n\r\nI also tried multiple ways to specify and enforce the datatypes, but all of which causes issues. I believe there's an issue with `_openapi_types` method that is incompatible with the input.\r\n\r\n### To reproduce\r\n\r\nFirst I specify the API input and output using `from_sample`\r\n\r\n```\r\ndtype_spec = {c: str for c in STRING_COLUMNS}\r\n\r\n@svc.api(\r\n    input=PandasDataFrame.from_sample(df_sample_input, orient=\"index\", enforce_dtype=True, dtype=dtype_spec),\r\n    output=PandasDataFrame.from_sample(df_sample_output, orient=\"index\", enforce_dtype=True, dtype=dtype_spec),\r\n)\r\ndef predict(df: pd.DataFrame) -> pd.DataFrame:\r\n  ...\r\n```\r\n\r\nUpon executing `bentoml build`, it throws the error:\r\n```\r\n  File \"/Users/Eric.Shiu/opt/miniconda3/envs/bento/lib/python3.8/site-packages/bentoml/_internal/io_descriptors/pandas.py\", line 72, in _openapi_types\r\n    if item.startswith(\"int\"):\r\nTypeError: startswith() takes at least 1 argument (0 given)\r\n```\r\n\r\nIt appears that `_openapi_types()` takes a string as input\r\n```\r\ndef _openapi_types(item: str) -> str:\r\n```\r\nbut it is not type checked when being called in `_dataframe_openapi_schema`\r\n```\r\ndef _dataframe_openapi_schema(\r\n    dtype: bool | ext.PdDTypeArg | None,\r\n    orient: ext.DataFrameOrient = None,\r\n) -> Schema:  # pragma: no cover\r\n   if isinstance(dtype, dict):\r\n   ...\r\n                        k: Schema(type=_openapi_types(v)) for k, v in dtype.items()\r\n```\r\n\r\nAlternatively, if I try \r\n```\r\ndtype_spec = {c: pd.StringDtype() for c in STRING_COLUMNS}\r\n```\r\nit throws the error:\r\n```\r\nFile \"/Users/Eric.Shiu/opt/miniconda3/envs/bento/lib/python3.8/site-packages/bentoml/_internal/io_descriptors/pandas.py\", line 72, in _openapi_types\r\n    if item.startswith(\"int\"):\r\nAttributeError: 'StringDtype' object has no attribute 'startswith'\r\n```\r\n\r\n\r\nFinally, if I use\r\n```\r\ndtype_spec = {c: \"str\" for c in STRING_COLUMNS}\r\n```\r\nBento does complete the build, but for each string column it will display the warning\r\n```\r\n<class 'str'> is not yet supported.\r\n```\r\nwhich comes from PandasDataFrame._convert_dtype().\r\nBut in this case, the swagger API definition will fail to load.\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\n`bentoml build` executes successfully and Swagger API docs loads properly, while being able to enforcing dtype\r\n\r\n\r\n### Environment\r\n\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.0.16\r\n`python`: 3.8.16\r\n`platform`: macOS-10.16-x86_64-i386-64bit\r\n`uid_gid`: 504:20\r\n`conda`: 4.14.0\r\n`in_conda_env`: True",
    "comments": []
  },
  {
    "issue_number": 3740,
    "title": "bug: `PERMISSION_DENIED` when trying to send OpenTelemetry trace data from a bento service to NewRelic",
    "author": "phitoduck",
    "state": "closed",
    "created_at": "2023-04-03T21:42:13Z",
    "updated_at": "2025-04-26T00:03:35Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nHi there, I'm trying to send trace data generated by my BentoML application to NewRelic.\r\n\r\nActually, I'm trying to get metrics *and* traces into NewRelic, but I think I may have solved metrics, and so I think this issue should be restricted to traces.\r\n\r\nBasically: when I try to send my traces to NewRelic from my BentoML service, I get a `PERMISSION_DENIED` error from within my BentoML app.\r\n\r\nSome resources:\r\n\r\n- There's a [thread in slack](https://bentoml.slack.com/archives/CKRANBHPH/p1679082751461319?thread_ts=1679008498.103059&cid=CKRANBHPH) documenting my approach. @ssheng is on the thread 😁. This issue is essentially a condensed version of the thread.\r\n- I made a minimal example and instructions for how to run it on GitHub [in this repository](https://github.com/phitoduck/bentoml-opentelemetry-newrelic).\r\n\r\nI opened up a support case with NewRelic, including my minimal example. They ran it locally and were able to reproduce my error when using their own token. 👍\r\n\r\nThey sent me this:\r\n\r\n> Hello,\r\n>\r\n> Thanks for your patience.\r\n>\r\n> I went over this with our engineers within the OTEL space and unfortunately they are not familiar enough with the BentoML product to advise on how to resolve this. \r\n>\r\n> The issue doesn't appear to be related to the license key being invalid, since it was failing with the permission denied message for me as well. The team suggested filing an issue with BentoML as the next step, and letting them know that there is an issue with sending data when authentication is required. \r\n>\r\n> I'm sorry I don't have better news.\r\n\r\nCopy/pasting from the thread:\r\n\r\nHere's my `docker-compose.yaml` file. Note that it relies on a `.env` file with the contents `NEW_RELIC_LICENSE_KEY=<my key>`\r\n\r\n```yaml\r\nversion: \"3.8\"\r\n\r\nservices:\r\n  bento-service:\r\n    image: dummy-service:latest\r\n    ports:\r\n      - \"3000:3000\"\r\n    volumes:\r\n      - .:/home/bentoml/bento/src/\r\n      - ./bentoml_configuration.yaml:/home/bentoml/bentoml_configuration.yaml\r\n    command: serve --port 3000 --debug # have tried with --production as well\r\n    environment:\r\n      OTEL_EXPORTER_OTLP_ENDPOINT: https://otlp.nr-data.net/\r\n      # I've tried enabling/disabling this\r\n      OTEL_EXPORTER_OTLP_HEADERS: api-key=${NEW_RELIC_LICENSE_KEY},service.name=opentelemetry-bento-svc\r\n      BENTO_CONFIG: /home/bentoml/bentoml_configuration.yaml\r\n    env_file:\r\n      - .env\r\n```\r\n\r\nAnd here's my `bentoml_configuration.yaml`\r\n\r\n```yaml\r\nversion: 1\r\napi_server:\r\n  tracing:\r\n    enabled: true\r\n    exporter_type: otlp\r\n    sample_rate: 1.0\r\n    otlp:\r\n      protocol: grpc\r\n      endpoint: https://otlp.nr-data.net/\r\n      grpc:\r\n        insecure: false\r\n        headers:\r\n          - [\"grpc-encoding\", \"gzip\"]\r\n          - [\"api-key\", \"${NEW_RELIC_LICENSE_KEY}\"]\r\n          - [\"service.name\", \"opentelemetry-bento-svc\"]\r\n```\r\n\r\nBut when I run `docker-compose up`, `bentoml` gives me this warning\r\n\r\nWhen I try to push traces to NewRelic, I get\r\n\r\n```log\r\nFailed to export traces, error code: StatusCode.PERMISSION_DENIED\r\n```\r\n\r\nAnd when I remove the API key altogether, I get\r\n\r\n```log\r\nFailed to export traces, error code: StatusCode.UNAUTHENTICATED\r\n```\r\n\r\n### To reproduce\r\n\r\nCovered in the description 👍\r\n\r\n### Expected behavior\r\n\r\nWe should not see a permission error, but instead should have the traces successfully appear in the NewRelic console. \r\n\r\nThe intended result of this would be twofold:\r\n\r\n1. Be able to trace requests that span multiple REST APIs where all REST APIs are instrumented with OpenTelemetry and some of those REST APIs are BentoML services\r\n2. Have a trace ID placed in the return header of all requests to BentoML services so we can look up the logs that were generated while that request was being handled\r\n\r\n### Environment\r\n\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=/home/bentoml/bentoml_configuration.yaml\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.0.16\r\n`python`: 3.10.10\r\n`platform`: Linux-5.15.49-linuxkit-x86_64-with-glibc2.31\r\n`uid_gid`: 1034:1034\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiohttp==3.8.4\r\naiosignal==1.3.1\r\nanyio==3.6.2\r\nappdirs==1.4.4\r\nasgiref==3.6.0\r\nasync-timeout==4.0.2\r\nattrs==22.2.0\r\nbackoff==2.2.1\r\nbentoml==1.0.16\r\nbuild==0.10.0\r\ncattrs==22.2.0\r\ncertifi==2022.12.7\r\ncharset-normalizer==3.1.0\r\ncircus==0.18.0\r\nclick==8.1.3\r\nclick-option-group==0.5.5\r\ncloudpickle==2.2.1\r\ncontextlib2==21.6.0\r\ndeepmerge==1.1.0\r\nDeprecated==1.2.13\r\nexceptiongroup==1.1.1\r\nfrozenlist==1.3.3\r\nfs==2.4.16\r\ngoogleapis-common-protos==1.58.0\r\ngrpcio==1.51.3\r\nh11==0.14.0\r\nidna==3.4\r\nJinja2==3.1.2\r\nmarkdown-it-py==2.2.0\r\nMarkupSafe==2.1.2\r\nmdurl==0.1.2\r\nmultidict==6.0.4\r\nnumpy==1.24.2\r\nopentelemetry-api==1.14.0\r\nopentelemetry-exporter-otlp==1.14.0\r\nopentelemetry-exporter-otlp-proto-grpc==1.14.0\r\nopentelemetry-exporter-otlp-proto-http==1.14.0\r\nopentelemetry-instrumentation==0.35b0\r\nopentelemetry-instrumentation-aiohttp-client==0.35b0\r\nopentelemetry-instrumentation-asgi==0.35b0\r\nopentelemetry-proto==1.14.0\r\nopentelemetry-sdk==1.14.0\r\nopentelemetry-semantic-conventions==0.35b0\r\nopentelemetry-util-http==0.35b0\r\npackaging==23.0\r\npathspec==0.11.1\r\npip-requirements-parser==32.0.1\r\npip-tools==6.12.3\r\nprometheus-client==0.16.0\r\nprotobuf==3.20.3\r\npsutil==5.9.4\r\npydantic==1.10.6\r\nPygments==2.14.0\r\npynvml==11.5.0\r\npyparsing==3.0.9\r\npyproject_hooks==1.0.0\r\npython-dateutil==2.8.2\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.6\r\nPyYAML==6.0\r\npyzmq==25.0.2\r\nrequests==2.28.2\r\nrich==13.3.2\r\nschema==0.7.5\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nstarlette==0.25.0\r\ntomli==2.0.1\r\ntornado==6.2\r\ntyping_extensions==4.5.0\r\nurllib3==1.26.15\r\nuvicorn==0.21.1\r\nwatchfiles==0.18.1\r\nwrapt==1.15.0\r\nyarl==1.8.2\r\n```\r\n\r\n</details>",
    "comments": []
  },
  {
    "issue_number": 3695,
    "title": "bug: bentoml serve hangs infinitely when importing local modules",
    "author": "SoonbeomChoi",
    "state": "closed",
    "created_at": "2023-03-22T18:03:36Z",
    "updated_at": "2025-04-26T00:03:34Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nI recently upgrade bentoml to the latest version from 1.0.10 and 'bentoml serve' suddenly doesn't work for this version.\r\nI found that importing local python modules with __init__.py re-runs 'bentoml serve' infinitly (it is not reload it re-runs whole python script from start to the end). python script itself works without error but 'bentoml serve' hangs when I import such local modules.\r\n\r\n\"Init: Language setting - universal: true' in the image is just single line print() in my service.py but it keeps printing and bentoml doesn't startup.\r\n![Screen Shot 2023-03-23 at 2 33 00 AM](https://user-images.githubusercontent.com/15067112/226997047-08701268-d294-493f-a623-bcc28791fa6b.jpg)\r\n\r\nFor example, if I have 'util' folder as below and 'import util' or 'from util import a' makes this problem. When I refactor the identical code without __init__.py it solves the problem but I don't think it is the ideal solution.\r\n`util\r\n  - __init__.py\r\n  - a.py\r\n  - b.py`\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nbentoml: 1.0.16\r\npython: 3.9.13",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Hi there, can you send the output of `bentoml serve --production --debug`?"
      },
      {
        "user": "sauyon",
        "body": "I've attempted to reproduce this locally but it seems to work fine for me, could you share an example bento that doesn't work, and share the full output of `bentoml env`?"
      },
      {
        "user": "SoonbeomChoi",
        "body": "Thanks for your fast response.\r\nI'm sorry I tested my code more deeply and find out that importing module itself wasn't the problem. importing wandb causes the infinite loop and my modules somehow imports wandb or part of it.\r\n\r\nNow I can solve the problem just by removing import wandb but may I ask you if you have any guess why importing wandb causes infinite loop.\r\n\r\nHere are some envs might related with this issue.\r\npython: 3.9.13\r\nbentoml: 1.0.16\r\nwandb: 0.13.11\r\nuvicorn: 0.21.1\r\nstarlette: 0.25.0"
      }
    ]
  },
  {
    "issue_number": 3580,
    "title": "rfc: build improvement",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2023-02-17T21:08:26Z",
    "updated_at": "2025-04-26T00:03:34Z",
    "labels": [],
    "body": "## Problem statement\r\n\r\nFrom community reports and internal discussion, `bentoml build` currently has the following caveats:\r\n\r\n- imports users' `service.py` as a module, which means all of the code in `service.py` will be invoked during the build, including every dependency imported in `service.py`.\r\nThis is not ideal as it requires users to have all the dependencies installed to build, which might not always be available in the CI/CD environment.\r\n\r\n    Current workaround we have seen from the community is that users will have to setup their environment beforehand, to run it on CI. This means users will end up with installing dependencies twice, one for build and one during containerise.\r\n\r\n- `bentoml build` creates a bento that includes a Dockerfile, which will be used by `containerize` to package a BentoContainer. Often times for CI, the desired behaviour is that build should be able to resolve\r\nto the container directly.\r\n\r\n## Proposed solutions\r\n\r\n1. importing `service.py` during build\r\n\r\na. Using `--env`\r\n\r\n- #3396 introduces `--env` argument to serve, which allow serving within conda environment. We should also be able to extend this to container, virtualenv, mamba, and so on.\r\n`--env` can also be used during `build`, which will build the given bento with the specified environment:\r\n\r\n```bash\r\nbentoml build --env container\r\n```\r\n\r\nThe behaviour is as follows:\r\n- creates a container that contains all of the necessary dependencies defined under `bentofile.yaml` (PyPI, Conda, system packages, setup script)\r\n- Attach the container to build the directory and build the Bento inside the container environment\r\n    - For BuildKit supported container daemon, we can use ``--output=type=local,dest=/path/to/bentoml_home/bentos/bento_name/version`` to copy the built Bento to local machine,\r\n    - For daemons that doesn't support BuildKit, use `cp` instead.\r\n- The container that is used to build the Bento can also be used for `containerize` and `serve` directly.\r\n\r\n> NOTE: #2495 mentions about this capability. All of the APIs are there to be used, so community contributions are welcome.\r\n\r\nb. Not using `--env`\r\n\r\nIf users wish not to use `--env`, then to solve this issue, we will need to extract the Service object from `service.py` without actually importing it to the file.\r\n- Propose that we write a custom Python parser :smile:\r\n- Or running build in a subprocess\r\n\r\n2. `bentoml build` directly to a container\r\n\r\nFor CI, what we can support is that `build` can also do `containerize` directly via flag `--format=container`:\r\n\r\n```bash\r\nbentoml build --format=container\r\n```\r\nby default, build will still create a Bento. (``--format=bento``)\r\n\r\nWould love to hear more feedback and comments on this.\r\n\r\n## Additional context\r\n\r\n#3577 suggests that we should refactor the containerization steps so that it will cache the environment setup, and move the model copying to later steps. This fails into `--env container` proposal, where it set up all of the dependencies inside the container once.",
    "comments": [
      {
        "user": "Quasarman",
        "body": "Awesome looking forward to not having to install packages twice in a CI/CD env!"
      },
      {
        "user": "charu-vl",
        "body": "@aarnphm thanks for writing this up\r\n\r\nI think option 1a is ideal, if bentoml can take a dockerfile as input or use the dockerfile specified in the bento config to do it. It would be tedious to need a running container to start the build process. Another reason why option 1a seems reasonable is because it seems to fit into the workflow of creating custom deployment containers (like the sagemaker workflow) better\r\n\r\nDo you know roughly how much time it would take to implement something like this?"
      },
      {
        "user": "aarnphm",
        "body": "> @aarnphm thanks for writing this up\r\n> \r\n> I think option 1a is ideal, if bentoml can take a dockerfile as input or use the dockerfile specified in the bento config to do it. It would be tedious to need a running container to start the build process. Another reason why option 1a seems reasonable is because it seems to fit into the workflow of creating custom deployment containers (like the sagemaker workflow) better\r\n> \r\n> Do you know roughly how much time it would take to implement something like this?\r\n\r\nSorry but I don't understand this. 1a requires a container runtime in order to build it. I don't think providing additional dockerfile would be necessary.\r\n\r\nNot sure if I understand what you mean by \"sagemaker workflow\"? I believe bentoctl would help with this (which is not relevant to this issue)."
      }
    ]
  },
  {
    "issue_number": 5320,
    "title": "bug: Error when containerize with alpine as backend instead debian",
    "author": "kascesar",
    "state": "open",
    "created_at": "2025-04-15T20:59:20Z",
    "updated_at": "2025-04-16T00:31:44Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nHaving docker error when containerize with alpine linux as backend.\n\nThe error are:\n\n```\nbentoml containerize weando:qj2oimq2hoq4lmg4\n\n\nINFO: Building OCI-compliant image for test:qj2oimq2hoq4lmg4 with docker\n\n[+] Building 11.6s (10/16)                                                                                                                                 docker:default\n => [internal] load build definition from Dockerfile                                                                                                                 0.0s\n => => transferring dockerfile: 1.79kB                                                                                                                               0.0s\n => [internal] load metadata for docker.io/library/python:3.10-alpine                                                                                                1.3s\n => [internal] load .dockerignore                                                                                                                                    0.0s\n => => transferring context: 2B                                                                                                                                      0.0s\n => [base-container  1/12] FROM docker.io/library/python:3.10-alpine@sha256:0733909561f552d8557618ee738b2a5cbf3fddfddf92c0fb261b293b90a51f12                         0.0s\n => [internal] load build context                                                                                                                                    0.0s\n => => transferring context: 6.30MB                                                                                                                                  0.0s\n => CACHED [base-container  2/12] RUN if command -v groupadd &>/dev/null; then     groupadd -g 1034 -o bentoml && useradd -m -u 1034 -g 1034 -o -r bentoml;     els  0.0s\n => CACHED [base-container  3/12] RUN mkdir /home/bentoml/bento && chown bentoml:bentoml /home/bentoml/bento -R                                                      0.0s\n => CACHED [base-container  4/12] WORKDIR /home/bentoml/bento                                                                                                        0.0s\n => CACHED [base-container  5/12] COPY --chown=bentoml:bentoml ./env/docker ./env/docker/                                                                            0.0s\n => ERROR [base-container  6/12] RUN apk add --update bash gcc libc-dev shadow musl-dev build-base linux-headers g++ git                                            10.2s\n------                                                                                                                                                                    \n > [base-container  6/12] RUN apk add --update bash gcc libc-dev shadow musl-dev build-base linux-headers g++ git:                                                        \n0.125 fetch https://dl-cdn.alpinelinux.org/alpine/v3.21/main/x86_64/APKINDEX.tar.gz                                                                                       \n5.150 fetch https://dl-cdn.alpinelinux.org/alpine/v3.21/community/x86_64/APKINDEX.tar.gz                                                                                  \n5.150 WARNING: updating and opening https://dl-cdn.alpinelinux.org/alpine/v3.21/main: temporary error (try again later)\n10.18 WARNING: updating and opening https://dl-cdn.alpinelinux.org/alpine/v3.21/community: temporary error (try again later)\n10.18 ERROR: unable to select packages:\n10.18   bash (no such package):\n10.18     required by: world[bash]\n10.18   build-base (no such package):\n10.18     required by: world[build-base]\n10.18   g++ (no such package):\n10.18     required by: world[g++]\n10.18   gcc (no such package):\n10.18     required by: world[gcc]\n10.18   git (no such package):\n10.18     required by: world[git]\n10.18   libc-dev (no such package):\n10.18     required by: world[libc-dev]\n10.18   linux-headers (no such package):\n10.18     required by: world[linux-headers]\n10.18   musl-dev (no such package):\n10.18     required by: world[musl-dev]\n10.18   shadow (no such package):\n10.18     required by: world[shadow]\n------\nDockerfile:37\n--------------------\n  35 |     \n  36 |     COPY --chown=bentoml:bentoml ./env/docker ./env/docker/\n  37 | >>> RUN apk add --update bash gcc libc-dev shadow musl-dev build-base linux-headers g++ git\n  38 |     RUN command -v uv >/dev/null || pip install uv\n  39 |     RUN UV_PYTHON_INSTALL_DIR=/app/python/ uv venv --python 3.10 /app/.venv && \\\n--------------------\nERROR: failed to solve: process \"/bin/sh -c apk add --update bash gcc libc-dev shadow musl-dev build-base linux-headers g++ git\" did not complete successfully: exit code: 9\nERROR: \nEncountered exception while trying to building image: Command '['/usr/bin/docker', 'build', '--tag', 'weando:qj2oimq2hoq4lmg4', '--file', '/tmp/tmpvmt8yhsifsTempFS/env/docker/Dockerfile', '/tmp/tmpvmt8yhsifsTempFS/']' returned non-zero exit status 1.\nTraceback (most recent call last):\n  File \"/home/cesar/develop/anomaly/test_bento/.venv/lib/python3.10/site-packages/bentoml/_internal/container/__init__.py\", line 251, in build\n    return builder.build(**kwargs)\n  File \"/home/cesar/develop/anomaly/test_bento/.venv/lib/python3.10/site-packages/bentoml/_internal/container/base.py\", line 190, in build\n    raise BentoMLException(str(e)) from None\nbentoml.exceptions.BentoMLException: Command '['/usr/bin/docker', 'build', '--tag', 'weando:qj2oimq2hoq4lmg4', '--file', '/tmp/tmpvmt8yhsifsTempFS/env/docker/Dockerfile', '/tmp/tmpvmt8yhsifsTempFS/']' returned non-zero exit status 1.\n\n```\n\n### To reproduce\n\nSteps to reporiduce:\n\n* service.py\n\n```python\nfrom typing import Any\n\nimport bentoml\n\nimage = (\n    bentoml.images.PythonImage(python_version=\"3.10\", distro=\"alpine\")\n)\n\n@bentoml.service(image=image)\nclass Test:\n    @bentoml.api\n    def predict(self, *args: Any, **kwrgs: Any) -> str:\n        return \"Test\"\n\n```\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: 1.4.8\nos: pop_os 22.04\nenv: pipenv python 3.10",
    "comments": [
      {
        "user": "frostming",
        "body": "> 5.150 WARNING: updating and opening https://dl-cdn.alpinelinux.org/alpine/v3.21/main: temporary error (try again later)\n> 10.18 WARNING: updating and opening https://dl-cdn.alpinelinux.org/alpine/v3.21/community: temporary error (try again later)\n> 10.18 ERROR: unable to select packages:\n> 10.18   bash (no such package):\n\nThis doesn't seem to be BentoML's fault, check your network condition."
      }
    ]
  },
  {
    "issue_number": 5319,
    "title": "bug: no Models with name `rf_model` exists in BentoML store",
    "author": "Haochen92",
    "state": "closed",
    "created_at": "2025-04-14T11:03:53Z",
    "updated_at": "2025-04-15T02:56:05Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nJust started experimenting with bentoML and created a simple random forest model with scikit learn.\n\nModel was saved with this code:\n```python\nsaved_model = bentoml.sklearn.save_model(\n    name=\"rf_model\",\n    model=model, \n    signatures={'predict':{'batchable':True}}\n)\n```\n\nService.py file:\n\n```python\nimport bentoml\nimport numpy as np\nfrom typing import Dict, Any\n\n# configurations\nmy_image = bentoml.images.Image(python_version='3.10', distro='alpine') \\\n    .requirements_file('requirements.txt')\n    \nmodel = bentoml.sklearn.load_model('rf_model:latest')\n\n@bentoml.service(name='match_prediction', image=my_image)\nclass MatchPredictionService:\n    def __init__(self):\n        self.model = model\n        \n    @bentoml.api\n    def predict(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n        \n        features = np.array(input_data[\"features\"])\n        \n        prediction = self.model.predict(features)\n        \n        return {\"prediction\": prediction.tolist()}\n```\n\nWhen running bentoml serve locally: no issues:\n```bash\n2025-04-14T10:57:46+0000 [INFO] [cli] Loading service from default location 'service.py'\n2025-04-14T10:57:47+0000 [INFO] [cli] Loading service from default location 'service.py'\n2025-04-14T10:57:49+0000 [INFO] [cli] Starting production HTTP BentoServer from \".\" listening on http://localhost:3000 (Press CTRL+C to quit)\n2025-04-14T10:57:49+0000 [INFO] [:1] Loading service from default location 'service.py'\n2025-04-14T10:57:51+0000 [INFO] [:1] Service match_prediction initialized\n```\n\nhowever, after using bentoml build followed by containerize, \n\n```bash\nSuccessfully built Bento container for \"match_prediction:xh4dudqzdwqcyrlk\" with tag(s) \n\"match_prediction:xh4dudqzdwqcyrlk\"\n\n\nRunning: \n    docker run --rm -p 3000:3000 match_prediction:xh4dudqzdwqcyrlk\nGives the error:\nError: [serve] `serve` failed: no Models with name 'rf_model' exist in BentoML store <osfs '/home/bentoml/models'>\nexit status 1\n```\n\n\n\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nscikit-learn\nnumpy\npandas",
    "comments": [
      {
        "user": "Haochen92",
        "body": "I manged to solve it by importing BentoModel, instantiating it with BentoModel outside of __init__ and load the model with sklearn. Not sure if this is the intended design, as I would have assumed bentoml.sklearn.load_model() should have triggered the model to be saved to the container by default. However, when browsing the issues, i see that this is supposed to be a legacy error so i'm not sure if im doing something wrong. \n\nThis worked for me.\n```python\nimport bentoml \nfrom bentoml.models import BentoModel\nimport numpy as np\nfrom typing import Dict, Any\n\n# configurations\nmy_image = bentoml.images.Image(python_version='3.10', distro='alpine') \\\n    .requirements_file('requirements.txt')\n\n@bentoml.service(name='match_prediction', image=my_image)\nclass MatchPredictionService:\n    \n    rf_model = BentoModel('rf_model:latest')\n    \n    def __init__(self):\n        self.model = bentoml.sklearn.load_model(self.rf_model)\n```"
      },
      {
        "user": "frostming",
        "body": "Yes, this is the recommended way to declare models -- you must specify them in the class attributes to make bentoml collect them when containerizing. Closing it now."
      }
    ]
  },
  {
    "issue_number": 2395,
    "title": "bentoml.exceptions.NotFound: no Models with name Error",
    "author": "sarmientoj24",
    "state": "closed",
    "created_at": "2022-04-06T10:00:36Z",
    "updated_at": "2025-04-14T10:55:25Z",
    "labels": [
      "bug"
    ],
    "body": "I followed the instructions for building the bento using `bentoml build` with the ff `bentofile.yaml`\r\n\r\n```\r\n# bentofile.yaml\r\nservice: \"my__service.py:service\"  \r\ndescription: \"file: ./README.md\"\r\nlabels:\r\n    owner: team\r\n    stage: demo\r\ninclude:\r\n - \"*.py\"  # A pattern for matching which files to include in the bento\r\npython:\r\n  packages:\r\n   - onnx\r\n   - onnxruntime\r\n   - numpy\r\n   - opencv-python-headless\r\n   - Pillow\r\n   - pyarrow\r\n ```\r\nAt this point, I tried serving the bento and it works\r\n```\r\n$ bentoml serve my_service:latest --production\r\n```\r\n\r\nThen containerized the Bento using the ff command:\r\n```\r\n$ bentoml containerize my_service:latest\r\n```\r\nwhich was pretty successful.\r\n\r\nWhen running the docker, this error shows up\r\n```\r\n04/06/22 09:53:34 INFO     [cli] Service loaded from Bento directory: bentoml.Se\r\n                           rvice(tag=\"bipa_detection_onnx:sarlh2vvrwnkfump\",    \r\n                           path=\"/home/bentoml/bento/\")                         \r\n04/06/22 09:53:34 INFO     [cli] Starting production BentoServer from           \r\n                           \"bento_identifier\" running on http://0.0.0.0:5000    \r\n                           (Press CTRL+C to quit)                               \r\n04/06/22 09:53:35 INFO     [bipa_detection_onnx] Service loaded from Bento      \r\n                           directory: bentoml.Service(tag=\"bipa_detection_onnx:s\r\n                           arlh2vvrwnkfump\", path=\"/home/bentoml/bento/\")       \r\n04/06/22 09:53:35 INFO     [api_server] Service loaded from Bento directory: ben\r\n                           toml.Service(tag=\"bipa_detection_onnx:sarlh2vvrwnkfum\r\n                           p\", path=\"/home/bentoml/bento/\")                     \r\n04/06/22 09:53:35 INFO     [bipa_detection_onnx] Started server process [26]    \r\n04/06/22 09:53:35 INFO     [bipa_detection_onnx] Waiting for application        \r\n                           startup.                                             \r\n04/06/22 09:53:35 ERROR    [bipa_detection_onnx] Traceback (most recent call    \r\n                           last):                                               \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/fs/osfs.py\", \r\n                           line 655, in open                                    \r\n                               **options                                        \r\n                           FileNotFoundError: [Errno 2] No such file or         \r\n                           directory:                                           \r\n                           b'/home/bentoml/models/bipa_detection_onnx/latest'   \r\n                                                                                \r\n                           During handling of the above exception, another      \r\n                           exception occurred:                                  \r\n                                                                                \r\n                           Traceback (most recent call last):                   \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/store.py\", line 103, in get              \r\n                               _tag.version =                                   \r\n                           self._fs.readtext(_tag.latest_path())                \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/fs/base.py\", \r\n                           line 692, in readtext                                \r\n                               path, mode=\"rt\", encoding=encoding,              \r\n                           errors=errors, newline=newline                       \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/fs/osfs.py\", \r\n                           line 655, in open                                    \r\n                               **options                                        \r\n                             File \"/opt/conda/lib/python3.7/site-packages/fs/err\r\n                           or_tools.py\", line 89, in __exit__                   \r\n                               reraise(fserror, fserror(self._path,             \r\n                           exc=exc_value), traceback)                           \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/six.py\", line\r\n                           718, in reraise                                      \r\n                               raise value.with_traceback(tb)                   \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/fs/osfs.py\", \r\n                           line 655, in open                                    \r\n                               **options                                        \r\n                           fs.errors.ResourceNotFound: resource                 \r\n                           'bipa_detection_onnx/latest' not found               \r\n                                                                                \r\n                           During handling of the above exception, another      \r\n                           exception occurred:                                  \r\n                                                                                \r\n                           Traceback (most recent call last):                   \r\n                             File \"/opt/conda/lib/python3.7/site-packages/starle\r\n                           tte/routing.py\", line 624, in lifespan               \r\n                               async with self.lifespan_context(app):           \r\n                             File \"/opt/conda/lib/python3.7/site-packages/starle\r\n                           tte/routing.py\", line 521, in __aenter__             \r\n                               await self._router.startup()                     \r\n                             File \"/opt/conda/lib/python3.7/site-packages/starle\r\n                           tte/routing.py\", line 603, in startup                \r\n                               handler()                                        \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/runner/local.py\", line 16, in setup      \r\n                               self._runner._setup()  # type:                   \r\n                           ignore[reportPrivateUsage]                           \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/frameworks/onnx.py\", line 334, in _setup \r\n                               session_options=session_options,                 \r\n                             File \"/opt/conda/lib/python3.7/site-packages/simple\r\n                           _di/__init__.py\", line 139, in _                     \r\n                               return func(*_inject_args(bind.args),            \r\n                           **_inject_kwargs(bind.kwargs))                       \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/frameworks/onnx.py\", line 126, in load   \r\n                               model = model_store.get(tag)                     \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/store.py\", line 106, in get              \r\n                               f\"no {self._item_type.__name__}s with name       \r\n                           '{_tag.name}' exist in BentoML store {self._fs}\"     \r\n                           bentoml.exceptions.NotFound: no Models with name     \r\n                           'bipa_detection_onnx' exist in BentoML store <osfs   \r\n                           '/home/bentoml/models'>                              \r\n                                                                                \r\n04/06/22 09:53:35 INFO     [api_server] Started server process [27]             \r\n04/06/22 09:53:35 ERROR    [bipa_detection_onnx] Application startup failed.    \r\n                           Exiting.                                             \r\n04/06/22 09:53:35 INFO     [api_server] Waiting for application startup.        \r\n04/06/22 09:53:35 INFO     [api_server] Application startup complete.           \r\n04/06/22 09:53:36 INFO     [bipa_detection_onnx] Service loaded from Bento      \r\n                           directory: bentoml.Service(tag=\"bipa_detection_onnx:s\r\n                           arlh2vvrwnkfump\", path=\"/home/bentoml/bento/\")       \r\n04/06/22 09:53:36 INFO     [bipa_detection_onnx] Started server process [62]    \r\n04/06/22 09:53:36 INFO     [bipa_detection_onnx] Waiting for application        \r\n                           startup.                                             \r\n04/06/22 09:53:36 ERROR    [bipa_detection_onnx] Traceback (most recent call    \r\n                           last):                                               \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/fs/osfs.py\", \r\n                           line 655, in open                                    \r\n                               **options                                        \r\n                           FileNotFoundError: [Errno 2] No such file or         \r\n                           directory:                                           \r\n                           b'/home/bentoml/models/bipa_detection_onnx/latest'   \r\n                                                                                \r\n                           During handling of the above exception, another      \r\n                           exception occurred:                                  \r\n                                                                                \r\n                           Traceback (most recent call last):                   \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/store.py\", line 103, in get              \r\n                               _tag.version =                                   \r\n                           self._fs.readtext(_tag.latest_path())                \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/fs/base.py\", \r\n                           line 692, in readtext                                \r\n                               path, mode=\"rt\", encoding=encoding,              \r\n                           errors=errors, newline=newline                       \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/fs/osfs.py\", \r\n                           line 655, in open                                    \r\n                               **options                                        \r\n                             File \"/opt/conda/lib/python3.7/site-packages/fs/err\r\n                           or_tools.py\", line 89, in __exit__                   \r\n                               reraise(fserror, fserror(self._path,             \r\n                           exc=exc_value), traceback)                           \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/six.py\", line\r\n                           718, in reraise                                      \r\n                               raise value.with_traceback(tb)                   \r\n                             File                                               \r\n                           \"/opt/conda/lib/python3.7/site-packages/fs/osfs.py\", \r\n                           line 655, in open                                    \r\n                               **options                                        \r\n                           fs.errors.ResourceNotFound: resource                 \r\n                           'bipa_detection_onnx/latest' not found               \r\n                                                                                \r\n                           During handling of the above exception, another      \r\n                           exception occurred:                                  \r\n                                                                                \r\n                           Traceback (most recent call last):                   \r\n                             File \"/opt/conda/lib/python3.7/site-packages/starle\r\n                           tte/routing.py\", line 624, in lifespan               \r\n                               async with self.lifespan_context(app):           \r\n                             File \"/opt/conda/lib/python3.7/site-packages/starle\r\n                           tte/routing.py\", line 521, in __aenter__             \r\n                               await self._router.startup()                     \r\n                             File \"/opt/conda/lib/python3.7/site-packages/starle\r\n                           tte/routing.py\", line 603, in startup                \r\n                               handler()                                        \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/runner/local.py\", line 16, in setup      \r\n                               self._runner._setup()  # type:                   \r\n                           ignore[reportPrivateUsage]                           \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/frameworks/onnx.py\", line 334, in _setup \r\n                               session_options=session_options,                 \r\n                             File \"/opt/conda/lib/python3.7/site-packages/simple\r\n                           _di/__init__.py\", line 139, in _                     \r\n                               return func(*_inject_args(bind.args),            \r\n                           **_inject_kwargs(bind.kwargs))                       \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/frameworks/onnx.py\", line 126, in load   \r\n                               model = model_store.get(tag)                     \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/store.py\", line 106, in get              \r\n                               f\"no {self._item_type.__name__}s with name       \r\n                           '{_tag.name}' exist in BentoML store {self._fs}\"     \r\n                           bentoml.exceptions.NotFound: no Models with name     \r\n                           'bipa_detection_onnx' exist in BentoML store <osfs   \r\n                           '/home/bentoml/models'> \r\n```\r\n\r\n1. What would be the possible problem on this?\r\n2. Do I need to save the model before building the Bento? \r\n3. Doesn't containerizing it also packages the model itself?\r\n\r\n",
    "comments": [
      {
        "user": "timliubentoml",
        "body": "Hi @sarmientoj24! Hope I can help.\r\n\r\nWhat would be the possible problem on this?\r\nI'm not totally sure why \"bentoml serve\" would work, but containerizing it then running it would throw this error. If you reference a model name which you haven't saved, then you it will definitely not work and throw this error though\r\n\r\nDo I need to save the model before building the Bento?\r\nYes, definitely, you need to save with the same model name that you later load it in your service.\r\n\r\nDoesn't containerizing it also packages the model itself?\r\n- Not quite sure what you mean by this. You must first save your model under a particular name. Then you define that name in your service. When building the bento (step before containerization), bentoml looks at the models that you want to build with the bento and includes those models\r\n\r\nAre you sure you saved it with consistent naming? If you go look at the bento in ~/bentoml/bentos, is the model present in the bento itself?"
      },
      {
        "user": "sarmientoj24",
        "body": "> If you reference a model name which you haven't saved, then you it will definitely not work and throw this error though  \r\n\r\nSo is there a need to save the model locally first? I am under the impression that a new model is saved automatically when you create the bento using `bentoml build`? Hence, I do not need to create it prior to building the bento.\r\n\r\nMy tree directory is this\r\n```\r\n├── bentos\r\n│   └── bipa_detection_onnx\r\n│       ├── latest\r\n│       └── sarlh2vvrwnkfump    <--- Here is the model automatically created by bentoml build\r\n│           ├── apis\r\n│           │   └── openapi.yaml\r\n│           ├── bento.yaml\r\n│           ├── env\r\n│           │   ├── conda\r\n│           │   ├── docker\r\n│           │   │   ├── Dockerfile\r\n│           │   │   ├── entrypoint.sh\r\n│           │   │   └── init.sh\r\n│           │   └── python\r\n│           │       ├── requirements.lock.txt\r\n│           │       ├── requirements.txt\r\n│           │       └── version.txt\r\n│           ├── models\r\n│           │   └── bipa_detection_onnx\r\n│           │       ├── latest\r\n│           │       └── toyaauft52menp6x\r\n│           │           ├── model.yaml\r\n│           │           └── saved_model.onnx\r\n│           ├── README.md\r\n│           └── src\r\n```"
      },
      {
        "user": "sarmientoj24",
        "body": "Recreated a new one where I used my saved model.\r\n\r\nInside service script\r\n```\r\nrunner = bentoml.onnx.load_runner(\r\n    \"bipa_detection_onnx:toyaauft52menp6x\", providers=[\"CPUExecutionProvider\"]\r\n)\r\nservice = bentoml.Service(\"bipa_detection_onnx\", runners=[runner])\r\n```\r\n\r\n### Building (using same YAML as before)\r\n```\r\n(yolov5) user@user:~/workspace/user/user$ bentoml build\r\nWednesday, 06 April, 2022 11:08:01 PM  INFO     [cli] Building BentoML service \"bipa_detection_onnx:licgpmvvxonkfump\" from build context                    \r\n                                                \"/home/user/workspace/user/user\"                                                                           \r\nWednesday, 06 April, 2022 11:08:01 PM  INFO     [cli] Packing model \"bipa_detection_onnx:toyaauft52menp6x\" from                                             \r\n                                                \"/home/user/bentoml/models/bipa_detection_onnx/toyaauft52menp6x\"                                           \r\nWednesday, 06 April, 2022 11:08:01 PM  INFO     [cli] Locking PyPI package versions..                                                                       \r\nWednesday, 06 April, 2022 11:08:04 PM  INFO     [cli]                                                                                                       \r\n                                                ██████╗░███████╗███╗░░██╗████████╗░█████╗░███╗░░░███╗██╗░░░░░                                               \r\n                                                ██╔══██╗██╔════╝████╗░██║╚══██╔══╝██╔══██╗████╗░████║██║░░░░░                                               \r\n                                                ██████╦╝█████╗░░██╔██╗██║░░░██║░░░██║░░██║██╔████╔██║██║░░░░░                                               \r\n                                                ██╔══██╗██╔══╝░░██║╚████║░░░██║░░░██║░░██║██║╚██╔╝██║██║░░░░░                                               \r\n                                                ██████╦╝███████╗██║░╚███║░░░██║░░░╚█████╔╝██║░╚═╝░██║███████╗                                               \r\n                                                ╚═════╝░╚══════╝╚═╝░░╚══╝░░░╚═╝░░░░╚════╝░╚═╝░░░░░╚═╝╚══════╝                                               \r\n                                                                                                                                                            \r\nWednesday, 06 April, 2022 11:08:04 PM  INFO     [cli] Successfully built Bento(tag=\"bipa_detection_onnx:licgpmvvxonkfump\") at                               \r\n                                                \"/home/user/bentoml/bentos/bipa_detection_onnx/licgpmvvxonkfump/\"   \r\n```\r\n\r\n```\r\n$ bentoml list\r\n Tag                                   Service                                Path                                           Size       Creation Time       \r\n bipa_detection_onnx:licgpmvvxonkfump  cavity_pa_detection_bipa_onnx:service  /home/user/bentoml/bentos/bipa_detection_on…  79.75 MiB  2022-04-06 15:08:04\r\n```\r\n\r\n### Testing serving the Bento (works)\r\n```\r\n(yolov5) user@user:~/workspace/app/app$ bentoml serve bipa_detection_onnx:licgpmvvxonkfump --production\r\nWednesday, 06 April, 2022 11:31:00 PM  INFO     [cli] Service loaded from Bento store: bentoml.Service(tag=\"bipa_detection_onnx:licgpmvvxonkfump\",          \r\n                                                path=\"/home/user/bentoml/bentos/bipa_detection_onnx/licgpmvvxonkfump\")                                     \r\nWednesday, 06 April, 2022 11:31:00 PM  INFO     [cli] Starting production BentoServer from \"bento_identifier\" running on http://0.0.0.0:5000 (Press CTRL+C  \r\n                                                to quit)                                                                                                    \r\nWednesday, 06 April, 2022 11:31:01 PM  INFO     [bipa_detection_onnx] Service loaded from Bento store:                                                      \r\n                                                bentoml.Service(tag=\"bipa_detection_onnx:licgpmvvxonkfump\",                                                 \r\n                                                path=\"/home/user/bentoml/bentos/bipa_detection_onnx/licgpmvvxonkfump\")                                     \r\nWednesday, 06 April, 2022 11:31:01 PM  INFO     [api_server] Service loaded from Bento store: bentoml.Service(tag=\"bipa_detection_onnx:licgpmvvxonkfump\",   \r\n                                                path=\"/home/user/bentoml/bentos/bipa_detection_onnx/licgpmvvxonkfump\"\r\n```\r\n### Containerize (successful)\r\n```\r\n(yolov5) user@user:~/workspace/app/app$ bentoml containerize bipa_detection_onnx:latest\r\nWednesday, 06 April, 2022 11:13:15 PM  INFO     [cli] Building docker image for Bento(tag=\"bipa_detection_onnx:licgpmvvxonkfump\")...                        \r\nWednesday, 06 April, 2022 11:13:39 PM  INFO     [cli] Successfully built docker image \"bipa_detection_onnx:licgpmvvxonkfump\r\n```\r\n### Serve (Error)\r\n```\r\n(yolov5) user@user:~/workspace/app/app$ docker run bipa_detection_onnx:licgpmvvxonkfump\r\n04/06/22 15:28:17 INFO     [cli] Service loaded from Bento directory: bentoml.Se\r\n                           rvice(tag=\"bipa_detection_onnx:licgpmvvxonkfump\",    \r\n                           path=\"/home/bentoml/bento/\")                         \r\n04/06/22 15:28:17 INFO     [cli] Starting production BentoServer from           \r\n                           \"bento_identifier\" running on http://0.0.0.0:5000    \r\n                           (Press CTRL+C to quit)                               \r\n04/06/22 15:28:18 INFO     [bipa_detection_onnx] Service loaded from Bento      \r\n                           directory: bentoml.Service(tag=\"bipa_detection_onnx:l\r\n                           icgpmvvxonkfump\", path=\"/home/bentoml/bento/\")       \r\n04/06/22 15:28:18 INFO     [bipa_detection_onnx] Started server process [26]    \r\n04/06/22 15:28:18 INFO     [bipa_detection_onnx] Waiting for application        \r\n                           startup.                                             \r\n04/06/22 15:28:18 ERROR    [bipa_detection_onnx] Traceback (most recent call    \r\n                           last):                                               \r\n                             File \"/opt/conda/lib/python3.7/site-packages/starle\r\n                           tte/routing.py\", line 624, in lifespan               \r\n                               async with self.lifespan_context(app):           \r\n                             File \"/opt/conda/lib/python3.7/site-packages/starle\r\n                           tte/routing.py\", line 521, in __aenter__             \r\n                               await self._router.startup()                     \r\n                             File \"/opt/conda/lib/python3.7/site-packages/starle\r\n                           tte/routing.py\", line 603, in startup                \r\n                               handler()                                        \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/runner/local.py\", line 16, in setup      \r\n                               self._runner._setup()  # type:                   \r\n                           ignore[reportPrivateUsage]                           \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/frameworks/onnx.py\", line 334, in _setup \r\n                               session_options=session_options,                 \r\n                             File \"/opt/conda/lib/python3.7/site-packages/simple\r\n                           _di/__init__.py\", line 139, in _                     \r\n                               return func(*_inject_args(bind.args),            \r\n                           **_inject_kwargs(bind.kwargs))                       \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/frameworks/onnx.py\", line 126, in load   \r\n                               model = model_store.get(tag)                     \r\n                             File \"/opt/conda/lib/python3.7/site-packages/bentom\r\n                           l/_internal/store.py\", line 117, in get              \r\n                               f\"{self._item_type.__name__} '{tag}' is not found\r\n                           in BentoML store {self._fs}\"                         \r\n                           bentoml.exceptions.NotFound: Model                   \r\n                           'bipa_detection_onnx:toyaauft52menp6x' is not found  \r\n                           in BentoML store <osfs '/home/bentoml/models'>\r\n```\r\n\r\n### Extra (bentoml tree)\r\n```\r\n── bentos\r\n│   └── bipa_detection_onnx\r\n│       ├── latest\r\n│       └── licgpmvvxonkfump\r\n│           ├── apis\r\n│           │   └── openapi.yaml\r\n│           ├── bento.yaml\r\n│           ├── env\r\n│           │   ├── conda\r\n│           │   ├── docker\r\n│           │   │   ├── Dockerfile\r\n│           │   │   ├── entrypoint.sh\r\n│           │   │   └── init.sh\r\n│           │   └── python\r\n│           │       ├── requirements.lock.txt\r\n│           │       ├── requirements.txt\r\n│           │       └── version.txt\r\n│           ├── models\r\n│           │   └── bipa_detection_onnx\r\n│           │       ├── latest\r\n│           │       └── toyaauft52menp6x\r\n│           │           ├── model.yaml\r\n│           │           └── saved_model.onnx\r\n│           ├── README.md\r\n│           └── src\r\n├── models\r\n│   ├── bipa_detection_onnx\r\n│   │   ├── latest\r\n│   │   └── toyaauft52menp6x\r\n│   │       ├── model.yaml\r\n│   │       └── saved_model.onnx\r\n```"
      }
    ]
  },
  {
    "issue_number": 5310,
    "title": "bug: bentoml not found when using `command: [\"bentoml\"]` due to missing venv path in $PATH",
    "author": "restato",
    "state": "closed",
    "created_at": "2025-04-08T15:24:46Z",
    "updated_at": "2025-04-11T20:16:05Z",
    "labels": [
      "bug"
    ],
    "body": "\n\n### Describe the bug\n\nWhen deploying a Bento container to Kubernetes with a `command: [\"bentoml\", \"serve\"]`, the container fails to start with the following error:\n\n```\nOCI runtime create failed: exec: “bentoml”: executable file not found in $PATH\n```\n\nThis happens because in the current container setup, `bentoml` is installed inside the virtual environment (`/app/.venv/bin/bentoml`), but that path is not included in the default `$PATH` during container startup via `exec`.\n\nWhile the `bentoml` binary is discoverable inside a shell (`kubectl exec` shows it in `$PATH`), it is **not available during `exec`-style entrypoint execution**, which Kubernetes uses for `command`.\n\n### Expected behavior\n\nThe container should be able to run `bentoml` directly via:\n\n```yaml\ncommand: [\"bentoml\", \"serve\"]\n```\n\nWithout requiring users to hardcode absolute paths like /app/.venv/bin/bentoml, or manually inject the virtualenv path into PATH.\n\n### Proposed solution\n\nIn the generated Dockerfile template\n\n```dockerfile\nENV PATH=\"/app/.venv/bin:$PATH\"\n```\n\nThis ensures the virtual environment’s bin directory is available in $PATH during container startup, including in exec-style runtimes (like K8s).\n\nAdditional context\n\t•\tThis issue didn’t occur in previous version (1.2.20) where bentoml was installed into /usr/local/bin.\n\t•\tIt’s especially impactful for Helm chart users and Kubernetes environments where command: is overridden.\n\nLet me know if you’d like me to submit a PR for this. Happy to contribute!\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nUsing Python 3.13.2 environment at: serving/.venv\n#### Environment variable\n\n```bash\nBENTOML_DEBUG=''\nBENTOML_QUIET=''\nBENTOML_BUNDLE_LOCAL_BUILD=''\nBENTOML_DO_NOT_TRACK=''\nBENTOML_CONFIG=''\nBENTOML_CONFIG_OPTIONS=''\nBENTOML_PORT=''\nBENTOML_HOST=''\nBENTOML_API_WORKERS=''\n```\n\n#### System information\n\n`bentoml`: 1.4.8\n`python`: 3.13.2\n`platform`: macOS-15.3.2-arm64-arm-64bit-Mach-O\n`uid_gid`: 501:20\n<details><summary><code>pip_packages</code></summary>\n\n<br>\n\n```\na2wsgi==1.10.8\naiohappyeyeballs==2.6.1\naiohttp==3.11.16\naiosignal==1.3.2\naiosqlite==0.21.0\nannotated-types==0.7.0\nanyio==4.9.0\nappdirs==1.4.4\nasgiref==3.8.1\nattrs==25.3.0\nbentoml==1.4.8\ncattrs==23.1.2\ncertifi==2025.1.31\nclick==8.1.8\nclick-option-group==0.5.7\ncloudpickle==3.1.1\ndeprecated==1.2.18\nfrozenlist==1.5.0\nfs==2.4.16\nh11==0.14.0\nhttpcore==1.0.7\nhttpx==0.28.1\nhttpx-ws==0.7.2\nidna==3.10\nimportlib-metadata==8.6.1\niniconfig==2.1.0\njinja2==3.1.6\nkantoku==0.18.3\nmarkdown-it-py==3.0.0\nmarkupsafe==3.0.2\nmdurl==0.1.2\nmultidict==6.3.2\nnumpy==2.2.4\nnvidia-ml-py==12.570.86\nopentelemetry-api==1.31.1\nopentelemetry-instrumentation==0.52b1\nopentelemetry-instrumentation-aiohttp-client==0.52b1\nopentelemetry-instrumentation-asgi==0.52b1\nopentelemetry-sdk==1.31.1\nopentelemetry-semantic-conventions==0.52b1\nopentelemetry-util-http==0.52b1\npackaging==24.2\npathspec==0.12.1\npillow==11.1.0\npip-requirements-parser==32.0.1\npluggy==1.5.0\nprometheus-client==0.21.1\nprompt-toolkit==3.0.50\npropcache==0.3.1\npsutil==7.0.0\npydantic==2.11.2\npydantic-core==2.33.1\npygments==2.19.1\npyparsing==3.2.3\npytest==8.3.5\npython-dateutil==2.9.0.post0\npython-dotenv==1.1.0\npython-json-logger==3.3.0\npython-multipart==0.0.20\npyyaml==6.0.2\npyzmq==26.4.0\nquestionary==2.1.0\nrich==14.0.0\nruff==0.11.4\nschema==0.7.7\nsetuptools==78.1.0\nsimple-di==0.1.5\nsix==1.17.0\nsniffio==1.3.1\nstarlette==0.46.1\ntomli-w==1.2.0\ntornado==6.4.2\ntyping-extensions==4.13.1\ntyping-inspection==0.4.0\nuvicorn==0.34.0\nwatchfiles==1.0.5\nwcwidth==0.2.13\nwrapt==1.17.2\nwsproto==1.2.0\nyarl==1.19.0\nzipp==3.21.0\n```\n\n</details>",
    "comments": [
      {
        "user": "frostming",
        "body": "SGTM, can you submit a PR?"
      }
    ]
  },
  {
    "issue_number": 4590,
    "title": "feature:  Allow to use HTTP other Method (GET, PUT, DELETE) to impl kserve predict protocol v2",
    "author": "KimSoungRyoul",
    "state": "closed",
    "created_at": "2024-03-18T04:45:11Z",
    "updated_at": "2025-03-30T22:05:12Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nhttps://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md\r\n\r\n~~~python\r\n\r\nsvc = bentoml.Service(...)\r\n\r\n@svc.api(\r\n    route=\"predict/${PATH_VARIABLE}\",\r\n    method=\"GET\", # <----\r\n    \r\n)\r\nasync def predict(path_variable: str):\r\n     ....\r\n\r\n~~~\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "KimSoungRyoul",
        "body": "hi @frostming \r\n\r\nI think that support other Method (as least `GET`) not only `POST` in BentoML is useful to extend BentoML\r\n\r\nCan you include this issue in bentoml team's maelstrom or \r\nif you're positive about this minor feature, I can make the PR\r\n\r\nthanks "
      },
      {
        "user": "alvaro-stylesage",
        "body": "Hello, are there any updates on this issue? Is it currently possible to define an endpoint with `@bentoml.api` that will be consumed via GET method? Thanks a lot for the update."
      },
      {
        "user": "frostming",
        "body": "> Hello, are there any updates on this issue? Is it currently possible to define an endpoint with `@bentoml.api` that will be consumed via GET method? Thanks a lot for the update.\n\nNo, but you can mount a FastAPI app providing GET endpoints\n\nhttps://docs.bentoml.com/en/latest/build-with-bentoml/asgi.html"
      }
    ]
  },
  {
    "issue_number": 5290,
    "title": "bug: AsyncHttpClient sends two sync requests on construction",
    "author": "Kakadus",
    "state": "closed",
    "created_at": "2025-03-24T23:49:30Z",
    "updated_at": "2025-03-28T00:32:28Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nSee https://github.com/bentoml/BentoML/blob/main/src/_bentoml_impl/client/http.py#L183-L186 and https://github.com/bentoml/BentoML/blob/main/src/_bentoml_impl/client/http.py#L332-L334. This completely hangs the event queue. As we used to create new clients repeatedly, this adds up.\n\nI suggest wrapping `AsyncHttpClient` in a `await asyncio.get_running_loop().run_in_executor(None, AsyncHttpClient)` for now.\n\n### To reproduce\n\nAdd breakpoints to the quoted code and run:\n\n`AsyncHttpClient()`\n\n### Expected behavior\n\nI expect that all http requests are done asynchronously with the AsyncHttpClient. \n\n### Environment\n\nbentoml: v1.4.5\npython: 3.12\nenvironment: Arch Linux",
    "comments": []
  },
  {
    "issue_number": 5294,
    "title": "bug: Alpine groupadd error when building docker image -same error with podman as backend-",
    "author": "kascesar",
    "state": "closed",
    "created_at": "2025-03-25T13:51:05Z",
    "updated_at": "2025-03-27T14:24:34Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nHello, want to tell that having an error when build Bento docker with \"Alpine\" back-end instead as \"Debian\".\n```bash\n[+] Building 1.9s (6/17)                                                                                                         docker:default\n => [internal] load build definition from Dockerfile                                                                                       0.0s\n => => transferring dockerfile: 1.63kB                                                                                                     0.0s\n => [internal] load metadata for docker.io/library/python:3.10-alpine                                                                      1.7s\n => [internal] load .dockerignore                                                                                                          0.0s\n => => transferring context: 2B                                                                                                            0.0s\n => [internal] load build context                                                                                                          0.0s\n => => transferring context: 17.92kB                                                                                                       0.0s\n => CACHED [base-container  1/13] FROM docker.io/library/python:3.10-alpine@sha256:4c4097e46608e9b9025a486e3b72d628ac8947a3caa20f142c2e8c  0.0s\n => ERROR [base-container  2/13] RUN groupadd -g 1034 -o bentoml && useradd -m -u 1034 -g 1034 -o -r bentoml                               0.2s\n------                                                                                                                                          \n > [base-container  2/13] RUN groupadd -g 1034 -o bentoml && useradd -m -u 1034 -g 1034 -o -r bentoml:\n0.137 /bin/sh: groupadd: not found\n------\nDockerfile:19\n--------------------\n  17 |     ARG BENTO_USER_UID=1034\n  18 |     ARG BENTO_USER_GID=1034\n  19 | >>> RUN groupadd -g $BENTO_USER_GID -o $BENTO_USER && useradd -m -u $BENTO_USER_UID -g $BENTO_USER_GID -o -r $BENTO_USER\n  20 |     \n  21 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c groupadd -g $BENTO_USER_GID -o $BENTO_USER && useradd -m -u $BENTO_USER_UID -g $BENTO_USER_GID -o -r $BENTO_USER\" did not complete successfully: exit code: 127\nERROR: \nEncountered exception while trying to building image: Command '['/usr/bin/docker', 'build', '--tag', 'weando:tt6awdqjp6felmg4', '--file', '/tmp/tmpqo0iyltxfsTempFS/env/docker/Dockerfile', '/tmp/tmpqo0iyltxfsTempFS/']' returned non-zero exit status 1.\nTraceback (most recent call last):\n  File \"/home/cesar/develop/anomaly/anomaly-model/.venv/lib/python3.10/site-packages/bentoml/_internal/container/__init__.py\", line 251, in build\n    return builder.build(**kwargs)\n  File \"/home/cesar/develop/anomaly/anomaly-model/.venv/lib/python3.10/site-packages/bentoml/_internal/container/base.py\", line 190, in build\n    raise BentoMLException(str(e)) from None\nbentoml.exceptions.BentoMLException: Command '['/usr/bin/docker', 'build', '--tag', 'weando:tt6awdqjp6felmg4', '--file', '/tmp/tmpqo0iyltxfsTempFS/env/docker/Dockerfile', '/tmp/tmpqo0iyltxfsTempFS/']' returned non-zero exit status 1.\n```\n\n\n\n### To reproduce\n\nThe code:\n\n```python\nfrom typing import Any\n\nimport bentoml\n\nimage = (\n    bentoml.images.PythonImage(python_version=\"3.10\", distro=\"alpine\")\n)\n\n\n@bentoml.service(image=image)\nclass Weando:\n    @bentoml.api\n    def predict(self, *args: Any, **kwrgs: Any) -> str:\n        return \"Wea\"\n\n```\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: 1.4.5\npython: 3.10\ndistro: pop_os 22.04\npython env: pipenv",
    "comments": []
  },
  {
    "issue_number": 4171,
    "title": "bug: Protocol not available error",
    "author": "neodawn",
    "state": "closed",
    "created_at": "2023-06-27T11:33:20Z",
    "updated_at": "2025-03-21T08:17:29Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nIn Windows Subsystem for Linux (WSL), I got an error when I ran the following command: `openllm start opt`\r\n\r\n```\r\n(.venv) ak@ak:/mnt/c/Tools/openllm$ openllm start opt\r\n2023-06-27T16:53:32+0530 [WARNING] [cli] 'CUDA_VISIBLE_DEVICES' has no effect when only CPU is available.\r\n2023-06-27T16:53:33+0530 [INFO] [cli] Prometheus metrics for HTTP BentoServer from \"_service.py:svc\" can be accessed at http://localhost:3000/metrics.\r\n2023-06-27T16:53:34+0530 [ERROR] [cli] Exception in callback <bound method Arbiter.manage_watchers of <circus.arbiter.Arbiter object at 0x7fd0dc3b8040>>\r\nTraceback (most recent call last):\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/tornado/ioloop.py\", line 919, in _run\r\n    val = self.callback()\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/circus/util.py\", line 1038, in wrapper\r\n    raise ConflictError(\"arbiter is already running %s command\"\r\ncircus.exc.ConflictError: arbiter is already running arbiter_start_watchers command\r\n2023-06-27T16:53:35+0530 [ERROR] [cli] Exception in callback <bound method Arbiter.manage_watchers of <circus.arbiter.Arbiter object at 0x7fd0dc3b8040>>\r\nTraceback (most recent call last):\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/tornado/ioloop.py\", line 919, in _run\r\n    val = self.callback()\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/circus/util.py\", line 1038, in wrapper\r\n    raise ConflictError(\"arbiter is already running %s command\"\r\ncircus.exc.ConflictError: arbiter is already running arbiter_start_watchers command\r\n2023-06-27T16:53:35+0530 [INFO] [cli] Starting production HTTP BentoServer from \"_service.py:svc\" listening on http://0.0.0.0:3000 (Press CTRL+C to quit)\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/bentoml_cli/worker/http_api_server.py\", line 189, in <module>\r\n    main()  # pylint: disable=no-value-for-parameter\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/bentoml_cli/worker/http_api_server.py\", line 183, in main\r\n    sock = socket.socket(fileno=fd)\r\n  File \"/usr/lib/python3.10/socket.py\", line 232, in __init__\r\n    _socket.socket.__init__(self, family, type, proto, fileno)\r\nOSError: [Errno 92] Protocol not available\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/bentoml_cli/worker/http_api_server.py\", line 189, in <module>\r\n    main()  # pylint: disable=no-value-for-parameter\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/bentoml_cli/worker/http_api_server.py\", line 183, in main\r\n    sock = socket.socket(fileno=fd)\r\n  File \"/usr/lib/python3.10/socket.py\", line 232, in __init__\r\n    _socket.socket.__init__(self, family, type, proto, fileno)\r\nOSError: [Errno 92] Protocol not available\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/bentoml_cli/worker/runner.py\", line 151, in <module>\r\n    main()  # pylint: disable=no-value-for-parameter\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/mnt/c/Tools/openllm/.venv/lib/python3.10/site-packages/bentoml_cli/worker/runner.py\", line 145, in main\r\n    sock = socket.socket(fileno=fd)\r\n  File \"/usr/lib/python3.10/socket.py\", line 232, in __init__\r\n    _socket.socket.__init__(self, family, type, proto, fileno)\r\n```\r\n\r\n### To reproduce\r\n\r\n1. `pip install openllm`\r\n2. `openllm start opt`\r\n\r\n### Logs\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.0.22\r\n`python`: 3.10.6\r\n`platform`: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.35\r\n`uid_gid`: 1000:1000\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\nabsl-py==1.4.0\r\naccelerate==0.20.3\r\naiohttp==3.8.4\r\naiosignal==1.3.1\r\nanyio==3.7.0\r\nappdirs==1.4.4\r\nasgiref==3.7.2\r\nastunparse==1.6.3\r\nasync-timeout==4.0.2\r\nattrs==23.1.0\r\nbentoml==1.0.22\r\nbuild==0.10.0\r\ncached-property==1.5.2\r\ncachetools==5.3.1\r\ncattrs==23.1.2\r\ncertifi==2023.5.7\r\ncharset-normalizer==3.1.0\r\nchex==0.1.7\r\ncircus==0.18.0\r\nclick==8.1.3\r\nclick-option-group==0.5.6\r\ncloudpickle==2.2.1\r\ncmake==3.26.4\r\ncoloredlogs==15.0.1\r\ncontextlib2==21.6.0\r\ndatasets==2.13.1\r\ndeepmerge==1.1.0\r\nDeprecated==1.2.14\r\ndill==0.3.6\r\ndm-tree==0.1.8\r\netils==1.3.0\r\nexceptiongroup==1.1.1\r\nfilelock==3.12.2\r\nfiletype==1.2.0\r\nflatbuffers==23.5.26\r\nflax==0.6.11\r\nfrozenlist==1.3.3\r\nfs==2.4.16\r\nfsspec==2023.6.0\r\ngast==0.4.0\r\ngoogle-auth==2.21.0\r\ngoogle-auth-oauthlib==1.0.0\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.56.0\r\ngrpcio-health-checking==1.48.2\r\nh11==0.14.0\r\nh5py==3.9.0\r\nhttpcore==0.17.2\r\nhttpx==0.24.1\r\nhuggingface-hub==0.15.1\r\nhumanfriendly==10.0\r\nidna==3.4\r\nimportlib-metadata==6.0.1\r\nimportlib-resources==5.12.0\r\ninflection==0.5.1\r\njax==0.4.13\r\njaxlib==0.4.13\r\nJinja2==3.1.2\r\nkeras==2.12.0\r\nlibclang==16.0.0\r\nlit==16.0.6\r\nMarkdown==3.4.3\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.3\r\nmdurl==0.1.2\r\nml-dtypes==0.2.0\r\nmpmath==1.3.0\r\nmsgpack==1.0.5\r\nmultidict==6.0.4\r\nmultiprocess==0.70.14\r\nnest-asyncio==1.5.6\r\nnetworkx==3.1\r\nnumpy==1.23.5\r\nnvidia-cublas-cu11==11.10.3.66\r\nnvidia-cuda-cupti-cu11==11.7.101\r\nnvidia-cuda-nvrtc-cu11==11.7.99\r\nnvidia-cuda-runtime-cu11==11.7.99\r\nnvidia-cudnn-cu11==8.5.0.96\r\nnvidia-cufft-cu11==10.9.0.58\r\nnvidia-curand-cu11==10.2.10.91\r\nnvidia-cusolver-cu11==11.4.0.1\r\nnvidia-cusparse-cu11==11.7.4.91\r\nnvidia-nccl-cu11==2.14.3\r\nnvidia-nvtx-cu11==11.7.91\r\noauthlib==3.2.2\r\nopenllm==0.1.16\r\nopentelemetry-api==1.17.0\r\nopentelemetry-instrumentation==0.38b0\r\nopentelemetry-instrumentation-aiohttp-client==0.38b0\r\nopentelemetry-instrumentation-asgi==0.38b0\r\nopentelemetry-instrumentation-grpc==0.38b0\r\nopentelemetry-sdk==1.17.0\r\nopentelemetry-semantic-conventions==0.38b0\r\nopentelemetry-util-http==0.38b0\r\nopt-einsum==3.3.0\r\noptax==0.1.5\r\noptimum==1.8.8\r\norbax-checkpoint==0.2.6\r\norjson==3.9.1\r\npackaging==23.1\r\npandas==2.0.2\r\npathspec==0.11.1\r\nPillow==9.5.0\r\npip-requirements-parser==32.0.1\r\npip-tools==6.13.0\r\nprometheus-client==0.17.0\r\nprotobuf==3.20.3\r\npsutil==5.9.5\r\npyarrow==12.0.1\r\npyasn1==0.5.0\r\npyasn1-modules==0.3.0\r\npydantic==1.10.9\r\nPygments==2.15.1\r\npynvml==11.5.0\r\npyparsing==3.1.0\r\npyproject_hooks==1.0.0\r\npython-dateutil==2.8.2\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.6\r\npytz==2023.3\r\nPyYAML==6.0\r\npyzmq==25.1.0\r\nregex==2023.6.3\r\nrequests==2.31.0\r\nrequests-oauthlib==1.3.1\r\nrich==13.4.2\r\nrsa==4.9\r\nsafetensors==0.3.1\r\nschema==0.7.5\r\nscipy==1.11.0\r\nsentencepiece==0.1.99\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nstarlette==0.28.0\r\nsympy==1.12\r\ntabulate==0.9.0\r\ntensorboard==2.12.3\r\ntensorboard-data-server==0.7.1\r\ntensorflow==2.12.0\r\ntensorflow-estimator==2.12.0\r\ntensorflow-io-gcs-filesystem==0.32.0\r\ntensorstore==0.1.39\r\ntermcolor==2.3.0\r\ntokenizers==0.13.3\r\ntomli==2.0.1\r\ntoolz==0.12.0\r\ntorch==2.0.1\r\ntorchvision==0.15.2\r\ntornado==6.3.2\r\ntqdm==4.65.0\r\ntransformers==4.30.2\r\ntriton==2.0.0\r\ntyping_extensions==4.6.3\r\ntzdata==2023.3\r\nurllib3==1.26.16\r\nuvicorn==0.22.0\r\nwatchfiles==0.19.0\r\nwcwidth==0.2.6\r\nWerkzeug==2.3.6\r\nwrapt==1.14.1\r\nxxhash==3.2.0\r\nyarl==1.9.2\r\nzipp==3.15.0\r\n```\r\n\r\n</details>\r\n\r\n- `transformers` version: 4.30.2\r\n- Platform: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.15.1\r\n- Safetensors version: 0.3.1\r\n- PyTorch version (GPU?): 2.0.1+cu117 (False)\r\n- Tensorflow version (GPU?): 2.12.0 (False)\r\n- Flax version (CPU?/GPU?/TPU?): 0.6.11 (cpu)\r\n- Jax version: 0.4.13\r\n- JaxLib version: 0.4.13\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n### System information (Optional)\r\n\r\n_No response_",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Going to transfer this upstream to BentoML since it is a BentoML issue."
      },
      {
        "user": "azachar",
        "body": "Has there been any progress here? I'm experiencing the same issue on a brand new installation as of now. I'm only running BentoML with the Iris example on WSL1."
      }
    ]
  },
  {
    "issue_number": 4170,
    "title": "feat: Response streaming over gRPC",
    "author": "Bec-k",
    "state": "closed",
    "created_at": "2023-06-23T10:22:57Z",
    "updated_at": "2025-03-21T08:17:18Z",
    "labels": [],
    "body": "### Feature request\n\nWould be nice to have a streaming feature for generation API, so that response would stream token per token and won't wait until full response is generated. gRPC have built-in support for streaming responses, proto code generation also does that. Only work is required in your server, to pipe tokens into the stream.\n\n### Motivation\n\nThis feature would allow to stream response while it is generating, instead of waiting until it is fully generated.\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "aarnphm",
        "body": "This would requires BentoML gRPC feature to support streaming, which it is not currently"
      },
      {
        "user": "aarnphm",
        "body": "Streaming is now supported via SSE. gRPC streaming will requires streaming support for gRPC on BentoML. I'm going to transfer this  to BentoML for now since SSE should be sufficient enough for most use case."
      },
      {
        "user": "Bec-k",
        "body": "Any documentation is available for that?"
      }
    ]
  },
  {
    "issue_number": 4009,
    "title": "feature: Support for poetry for installing dependencies in bento",
    "author": "arkodoescode",
    "state": "closed",
    "created_at": "2023-07-02T14:34:35Z",
    "updated_at": "2025-03-21T08:17:18Z",
    "labels": [],
    "body": "### Feature request\n\nI would like it if bentofile python field could support a `pyproject.toml` value in requirements instead of a `requirements.txt` file with support for poetry.\n\n### Motivation\n\nPoetry is used in production quite a lot and this seems like a natural choice.\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "bojiang",
        "body": "Agree. It makes sense to support PEP 621.\r\nAnd the good news is pip-tools (we are using it to lock the dependencies) already supports it. Thus it should be a natural change.\r\nhttps://github.com/jazzband/pip-tools/issues/1510\r\n"
      },
      {
        "user": "bojiang",
        "body": "Would you like to help adding this feature?"
      },
      {
        "user": "aarnphm",
        "body": "I think adding poetry is nice, but it is just adding a churn for us to maintain.\r\n\r\nEsp in the past, we haven't had good experiences with poetry. Not sure if this has changed with the recent release. Maybe we can add support for pdm since we have @frostming 😃"
      }
    ]
  },
  {
    "issue_number": 4093,
    "title": "bug:  'XGBClassifier' object has no attribute '__call__'",
    "author": "trongnghia05",
    "state": "closed",
    "created_at": "2023-08-03T04:47:12Z",
    "updated_at": "2025-03-21T08:16:55Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nOh, I have a model trained using the XGBoost library following these steps:\r\n\r\nStep 1: Save the model after training using joblib.dump(best_model, abspath(config.model.path)).\r\n\r\nStep 2: After training, I used BentoML to save the model using bentoml.picklable_model.save_model.\r\n\r\nStep 3: When I try to load the model to perform an API call using bentoml.picklable_model.get, I encounter the error 'XGBClassifier' object has no attribute '__call__'.\r\n\r\nI'm not sure why, even though in Step 2, after using save_model, I saw that a __call__ method was present in the model signature, specifically displayed as follows: \"Using the default model signature for pickable model ({'call': ModelSignature(batchable=False, batch_dim=(0, 0), input_spec=None, output_spec=None)}) for model 'xgboost'.\" This means that my model already has the __call__ method, but when trying to infer, the error still says \"no attribute 'call'\".\r\n\r\n\r\n\r\ntrain code:\r\n```python\r\nimport warnings\r\n\r\nwarnings.filterwarnings(action=\"ignore\")\r\n\r\nfrom functools import partial\r\nfrom typing import Callable\r\nfrom bentoml.types import ModelSignature\r\nimport hydra\r\nimport joblib\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom hydra.utils import to_absolute_path as abspath\r\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\r\nfrom omegaconf import DictConfig\r\nfrom sklearn.metrics import accuracy_score\r\nfrom xgboost import XGBClassifier\r\nimport bentoml\r\n\r\ndef load_data(path: DictConfig):\r\n    X_train = pd.read_csv(abspath(path.X_train.path))\r\n    X_test = pd.read_csv(abspath(path.X_test.path))\r\n    y_train = pd.read_csv(abspath(path.y_train.path))\r\n    y_test = pd.read_csv(abspath(path.y_test.path))\r\n    return X_train, X_test, y_train, y_test\r\n\r\n\r\ndef get_objective(\r\n    X_train: pd.DataFrame,\r\n    y_train: pd.DataFrame,\r\n    X_test: pd.DataFrame,\r\n    y_test: pd.DataFrame,\r\n    config: DictConfig,\r\n    space: dict,\r\n):\r\n\r\n    model = XGBClassifier(\r\n        use_label_encoder=config.model.use_label_encoder,\r\n        objective=config.model.objective,\r\n        n_estimators=space[\"n_estimators\"],\r\n        max_depth=int(space[\"max_depth\"]),\r\n        gamma=space[\"gamma\"],\r\n        reg_alpha=int(space[\"reg_alpha\"]),\r\n        min_child_weight=int(space[\"min_child_weight\"]),\r\n        colsample_bytree=int(space[\"colsample_bytree\"]),\r\n    )\r\n\r\n    evaluation = [(X_train, y_train), (X_test, y_test)]\r\n\r\n    model.fit(\r\n        X_train,\r\n        y_train,\r\n        eval_set=evaluation,\r\n        eval_metric=config.model.eval_metric,\r\n        early_stopping_rounds=config.model.early_stopping_rounds,\r\n    )\r\n    prediction = model.predict(X_test.values)\r\n    accuracy = accuracy_score(y_test, prediction)\r\n    print(\"SCORE:\", accuracy)\r\n    return {\"loss\": -accuracy, \"status\": STATUS_OK, \"model\": model}\r\n\r\n\r\ndef optimize(objective: Callable, space: dict):\r\n    trials = Trials()\r\n    best_hyperparams = fmin(\r\n        fn=objective,\r\n        space=space,\r\n        algo=tpe.suggest,\r\n        max_evals=100,\r\n        trials=trials,\r\n    )\r\n    print(\"The best hyperparameters are : \", \"\\n\")\r\n    print(best_hyperparams)\r\n    best_model = trials.results[\r\n        np.argmin([r[\"loss\"] for r in trials.results])\r\n    ][\"model\"]\r\n    return best_model\r\n\r\n\r\n@hydra.main(config_path=\"../../config\", config_name=\"main\")\r\ndef train(config: DictConfig):\r\n    \"\"\"Function to train the model\"\"\"\r\n\r\n    X_train, X_test, y_train, y_test = load_data(config.processed)\r\n\r\n    # Define space\r\n    space = {\r\n        \"max_depth\": hp.quniform(\"max_depth\", **config.model.max_depth),\r\n        \"gamma\": hp.uniform(\"gamma\", **config.model.gamma),\r\n        \"reg_alpha\": hp.quniform(\"reg_alpha\", **config.model.reg_alpha),\r\n        \"reg_lambda\": hp.uniform(\"reg_lambda\", **config.model.reg_lambda),\r\n        \"colsample_bytree\": hp.uniform(\r\n            \"colsample_bytree\", **config.model.colsample_bytree\r\n        ),\r\n        \"min_child_weight\": hp.quniform(\r\n            \"min_child_weight\", **config.model.min_child_weight\r\n        ),\r\n        \"n_estimators\": config.model.n_estimators,\r\n        \"seed\": config.model.seed,\r\n    }\r\n    objective = partial(\r\n        get_objective, X_train, y_train, X_test, y_test, config\r\n    )\r\n\r\n    # Find best model\r\n    best_model = optimize(objective, space)\r\n    bentoml.picklable_model.save_model(config.model.name, best_model,\r\n                                       signatures={\"__call__\": ModelSignature(batchable=False)})\r\n    # Save model\r\n    joblib.dump(best_model, abspath(config.model.path))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train()\r\n``` \r\n\r\nsave model:\r\n```python\r\nimport bentoml\r\nimport hydra\r\nimport joblib\r\nfrom hydra.utils import to_absolute_path as abspath\r\nfrom omegaconf import DictConfig\r\nfrom bentoml.types import ModelSignature\r\n\r\ndef load_model(model_path: str):\r\n    return joblib.load(model_path)\r\n\r\n\r\n@hydra.main(config_path=\"../../config\", config_name=\"main\")\r\ndef save_to_bentoml(config: DictConfig):\r\n    model = load_model(abspath(config.model.path))\r\n    bentoml.picklable_model.save_model(config.model.name, model)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    save_to_bentoml()\r\n``` \r\nservice:\r\n```python\r\nimport bentoml\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom bentoml.io import JSON, NumpyNdarray\r\nfrom hydra import compose, initialize\r\nfrom patsy import dmatrix\r\nfrom pydantic import BaseModel\r\n\r\nwith initialize( config_path=\"../../config\"):\r\n    config = compose(config_name=\"main\")\r\n    FEATURES = config.process.features\r\n    MODEL_NAME = config.model.name\r\n\r\n\r\nclass Employee(BaseModel):\r\n    City: str = \"Pune\"\r\n    PaymentTier: int = 1\r\n    Age: int = 25\r\n    Gender: str = \"Female\"\r\n    EverBenched: str = \"No\"\r\n    ExperienceInCurrentDomain: int = 1\r\n\r\n\r\ndef add_dummy_data(df: pd.DataFrame):\r\n    \"\"\"Add dummy rows so that patsy can create features similar to the train dataset\"\"\"\r\n    rows = {\r\n        \"City\": [\"Bangalore\", \"New Delhi\", \"Pune\"],\r\n        \"Gender\": [\"Male\", \"Female\", \"Female\"],\r\n        \"EverBenched\": [\"Yes\", \"Yes\", \"No\"],\r\n        \"PaymentTier\": [0, 0, 0],\r\n        \"Age\": [0, 0, 0],\r\n        \"ExperienceInCurrentDomain\": [0, 0, 0],\r\n    }\r\n    dummy_df = pd.DataFrame(rows)\r\n    return pd.concat([df, dummy_df])\r\n\r\n\r\ndef rename_columns(X: pd.DataFrame):\r\n    X.columns = X.columns.str.replace(\"[\", \"_\", regex=True).str.replace(\r\n        \"]\", \"\", regex=True\r\n    )\r\n    return X\r\n\r\n\r\ndef transform_data(df: pd.DataFrame):\r\n    \"\"\"Transform the data\"\"\"\r\n    dummy_df = add_dummy_data(df)\r\n    feature_str = \" + \".join(FEATURES)\r\n    dummy_X = dmatrix(f\"{feature_str} - 1\", dummy_df, return_type=\"dataframe\")\r\n    dummy_X = rename_columns(dummy_X)\r\n    return dummy_X.iloc[0, :].values.reshape(1, -1)\r\n\r\n\r\nmodel = bentoml.picklable_model.get(\r\n    f\"{MODEL_NAME}:latest\"\r\n).to_runner()\r\n# Create service with the model\r\nservice = bentoml.Service(\"predict_employee\", runners=[model])\r\n\r\n\r\n@service.api(input=JSON(pydantic_model=Employee), output=NumpyNdarray())\r\ndef predict(employee: Employee) -> np.ndarray:\r\n    \"\"\"Transform the data then make predictions\"\"\"\r\n    df = pd.DataFrame(employee.dict(), index=[0])\r\n    df = transform_data(df)\r\n    result = model.run(df)[0]\r\n    return np.array(result)\r\n\r\n``` \r\n\n\n### To reproduce\n\nOh, I have a model trained using the XGBoost library following these steps:\r\n\r\nStep 1: Save the model after training using joblib.dump(best_model, abspath(config.model.path)).\r\n\r\nStep 2: After training, I used BentoML to save the model using bentoml.picklable_model.save_model.\r\n\r\nStep 3: When I try to load the model to perform an API call using bentoml.picklable_model.get, I encounter the error 'XGBClassifier' object has no attribute '__call__'.\r\n\r\nI'm not sure why, even though in Step 2, after using save_model, I saw that a __call__ method was present in the model signature, specifically displayed as follows: \"Using the default model signature for pickable model ({'call': ModelSignature(batchable=False, batch_dim=(0, 0), input_spec=None, output_spec=None)}) for model 'xgboost'.\" This means that my model already has the __call__ method, but when trying to infer, the error still says \"no attribute 'call'\".\r\n\n\n### Expected behavior\n\nThe expected behavior when calling the /predict API is to receive the correct results, not an error like \"'XGBClassifier' object has no attribute 'call'.\"\n\n### Environment\n\nThese are the library versions I used:\r\n\r\nbentoml==1.1.0\r\ndagshub==0.1.8\r\ndeepchecks==0.6.1\r\nhydra-core==1.2.0\r\nhyperopt==0.2.7\r\njoblib==1.1.1\r\nmlflow==1.25.1\r\nnumpy==1.22.4\r\npandas==1.4.2\r\npandera==0.13.4\r\npatsy==0.5.2\r\npydantic==1.9.1\r\npytest_steps==1.8.0\r\nrequests==2.28.0\r\nscikit_learn==1.2.1\r\nstreamlit==1.10.0\r\nxgboost==1.7.6\r\ndvc==2.8.1\r\nfsspec==2022.7.1",
    "comments": [
      {
        "user": "trongnghia05",
        "body": "I realize that when serving, I call model.run(df), and it seems like this function does something to reload the model using cloudpickle, but after loading, it does not convert it into a runner, causing the saved model to not have the registered __call__ method when using save_model. How can I handle this situation?"
      },
      {
        "user": "trongnghia05",
        "body": "The following simple code snippet also does not work because when run model.run, the model does not have the registered __call__ method.\r\n```python\r\nimport bentoml\r\n\r\nmodel = bentoml.picklable_model.get(\"xgboost:latest\").to_runner()\r\nmodel.init_local()\r\n# print(getattr(model, \"__call__\"))\r\nmodel.run([[5.9, 3., 5.1, 1.8]])\r\n``` "
      },
      {
        "user": "frostming",
        "body": "So the result of this line:\r\n\r\n```python\r\n# Find best model\r\nbest_model = optimize(objective, space)\r\n```\r\n\r\nCan `best_model` be called directly with `best_model(...)` ? If not, how is it supposed to be used in prediction?"
      }
    ]
  },
  {
    "issue_number": 4036,
    "title": "bug: get Bento CR bentoes.resources.yatai.ai not found",
    "author": "devJackie",
    "state": "closed",
    "created_at": "2023-07-12T08:11:37Z",
    "updated_at": "2025-03-21T08:16:55Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nAfter deploying from yatai UI, the following error occurs in yatai-deployment log.\r\n\r\nIn lower versions, other errors related to CRDS occur.\r\nHowever, in version 1.1.13, the error that occurred in 1.1.9~1.1.12 no longer occurs, and the following error occurs.\r\n\r\n\r\nyatai-deployment error log :\r\n<img width=\"1579\" alt=\"image\" src=\"https://github.com/bentoml/BentoML/assets/10051032/c4652533-f924-43e1-afa1-152bd4f489ea\">\r\n\r\n1.689146204559899e+09\tERROR\tconversion-webhook\t**failed to convert\t{\"request\": \"65e85fef-9cf3-49b5-a6ea-73856cf0e910\", \"error\": \"get Bento CR iris-classifier--0-0-2-iris-classifier: bentoes.resources.yatai.ai \\\"iris-classifier--0-0-2-iris-classifier\\\" not found\"**, \"errorVerbose\": \"bentoes.resources.yatai.ai \\\"iris-classifier--0-0-2-iris-classifier\\\" not found\\nget Bento CR iris-classifier--0-0-2-iris-classifier\\ngithub.com/bentoml/yatai-deployment/apis/serving/v1alpha3.getBentoTagFromBentoName\\n\\t/workspace/apis/serving/v1alpha3/bentodeployment_conversion.go:284\\ngithub.com/bentoml/yatai-deployment/apis/serving/v1alpha3.(*BentoDeployment).ConvertFrom\\n\\t/workspace/apis/serving/v1alpha3/bentodeployment_conversion.go:390\\nsigs.k8s.io/controller-runtime/pkg/webhook/conversion.(*Webhook).convertObject\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.13.0/pkg/webhook/conversion/conversion.go:142\\nsigs.k8s.io/controller-runtime/pkg/webhook/conversion.(*Webhook).handleConvertRequest\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.13.0/pkg/webhook/conversion/conversion.go:107\\nsigs.k8s.io/controller-runtime/pkg/webhook/conversion.(*Webhook).ServeHTTP\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.13.0/pkg/webhook/conversion/conversion.go:74\\ngithub.com/prometheus/client_golang/prometheus/promhttp.InstrumentHandlerInFlight.func1\\n\\t/go/pkg/mod/github.com/prometheus/client_golang@v1.12.2/prometheus/promhttp/instrument_server.go:40\\nnet/http.HandlerFunc.ServeHTTP\\n\\t/usr/local/go/src/net/http/server.go:2109\\ngithub.com/prometheus/client_golang/prometheus/promhttp.InstrumentHandlerCounter.func1\\n\\t/go/pkg/mod/github.com/prometheus/client_golang@v1.12.2/prometheus/promhttp/instrument_server.go:117\\nnet/http.HandlerFunc.ServeHTTP\\n\\t/usr/local/go/src/net/http/server.go:2109\\ngithub.com/prometheus/client_golang/prometheus/promhttp.InstrumentHandlerDuration.func2\\n\\t/go/pkg/mod/github.com/prometheus/client_golang@v1.12.2/prometheus/promhttp/instrument_server.go:84\\nnet/http.HandlerFunc.ServeHTTP\\n\\t/usr/local/go/src/net/http/server.go:2109\\nnet/http.(*ServeMux).ServeHTTP\\n\\t/usr/local/go/src/net/http/server.go:2487\\nnet/http.serverHandler.ServeHTTP\\n\\t/usr/local/go/src/net/http/server.go:2947\\nnet/http.(*conn).serve\\n\\t/usr/local/go/src/net/http/server.go:1991\\nruntime.goexit\\n\\t/usr/local/go/src/runtime/asm_amd64.s:1594\"}\r\nsigs.k8s.io/controller-runtime/pkg/webhook/conversion.(*Webhook).ServeHTTP\r\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.13.0/pkg/webhook/conversion/conversion.go:76\r\ngithub.com/prometheus/client_golang/prometheus/promhttp.InstrumentHandlerInFlight.func1\r\n\t/go/pkg/mod/github.com/prometheus/client_golang@v1.12.2/prometheus/promhttp/instrument_server.go:40\r\nnet/http.HandlerFunc.ServeHTTP\r\n\t/usr/local/go/src/net/http/server.go:2109\r\ngithub.com/prometheus/client_golang/prometheus/promhttp.InstrumentHandlerCounter.func1\r\n\t/go/pkg/mod/github.com/prometheus/client_golang@v1.12.2/prometheus/promhttp/instrument_server.go:117\r\nnet/http.HandlerFunc.ServeHTTP\r\n\t/usr/local/go/src/net/http/server.go:2109\r\ngithub.com/prometheus/client_golang/prometheus/promhttp.InstrumentHandlerDuration.func2\r\n\t/go/pkg/mod/github.com/prometheus/client_golang@v1.12.2/prometheus/promhttp/instrument_server.go:84\r\nnet/http.HandlerFunc.ServeHTTP\r\n\t/usr/local/go/src/net/http/server.go:2109\r\nnet/http.(*ServeMux).ServeHTTP\r\n\t/usr/local/go/src/net/http/server.go:2487\r\nnet/http.serverHandler.ServeHTTP\r\n\t/usr/local/go/src/net/http/server.go:2947\r\nnet/http.(*conn).serve\r\n\t/usr/local/go/src/net/http/server.go:1991\r\n\t\r\n\t\r\nyatai-image-builder log:\r\n<img width=\"2095\" alt=\"image\" src=\"https://github.com/bentoml/BentoML/assets/10051032/399a237d-1365-467b-9aa3-0cd9f4fb0375\">\r\n\r\n\r\n### To reproduce\r\n\r\nFor installation, refer to the yatai installation guide.\r\n-> https://docs.yatai.io/en/latest/installation/index.html\r\n\r\nI read the bentoml quickstart guide and deployed it to the kubernetes environment.\r\n-> https://github.com/bentoml/BentoML/tree/main/examples/quickstart\r\n\r\n1. bentoml login\r\n2. bentoml build\r\n3. bentoml push\r\n\r\nAfter push, we proceeded to deploy through yatai UI.\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n- kubernetes\r\n-> 1.26.4\r\n\r\n- yatai\r\n-> yatai-1.1.9\r\n-> yatai-image-builder-1.1.7\r\n-> yatai-deployment-1.1.13",
    "comments": []
  },
  {
    "issue_number": 4031,
    "title": "feature: Allow passing docker run arguments in host_bento",
    "author": "mkmenta",
    "state": "closed",
    "created_at": "2023-07-10T09:49:12Z",
    "updated_at": "2025-03-21T08:16:54Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nHi! \r\n\r\nI think it would be very useful to have the possibility to pass arguments in the `docker run` of [host_bento](https://github.com/bentoml/BentoML/blob/7fb4578e287fc86b295c82c59d08a3ab01fe2195/src/bentoml/testing/server.py#L437).\r\nThat is, to have the possibility to pass command arguments in `run_bento_server_container` [here](https://github.com/bentoml/BentoML/blob/7fb4578e287fc86b295c82c59d08a3ab01fe2195/src/bentoml/testing/server.py#L215).\r\n\r\nRight now the possibility of passing arguments is only supported for the \"containerize\" phase with the [container_mode_options](https://github.com/bentoml/BentoML/blob/7fb4578e287fc86b295c82c59d08a3ab01fe2195/src/bentoml/testing/server.py#L529). It would be nice to have something similar but for the `docker run`.\n\n### Motivation\n\nThis feature would be especially useful to pass environment variables with secrets during the run of the service container.\n\n### Other\n\nI wouldn't mind helping with the implementation if you also think that it is an important feature to add.",
    "comments": [
      {
        "user": "aarnphm",
        "body": "we are yet to decide on how we want to provide a testing API for bentoml atm. We had some discussion in the past internally, but currently this is within our backlog. It would be great if you can provide a usecase for using this, since `host_bento` is currently still considered as internal API"
      },
      {
        "user": "mkmenta",
        "body": "I see... I began using `host_bento` because I saw it in the repo examples and I found it very useful. \r\n\r\nIn my case, I am facing a situation where I need to add some secrets and credentials as environment variables during the `docker run` of the service container. I need to run some requests to an external service as a pre-processing before running the model inference. However, I can't do that right now in `host_bento` because it does not allow passing extra arguments to the docker run."
      }
    ]
  },
  {
    "issue_number": 3974,
    "title": "feature: Change configuration by using Windows command prompt",
    "author": "Yuanlim0919",
    "state": "closed",
    "created_at": "2023-06-21T05:58:10Z",
    "updated_at": "2025-03-21T08:16:54Z",
    "labels": [],
    "body": "### Feature request\n\nI am using Anaconda in Windows, and I am about to change the bentoml default configuration by using methods provided in [providing-configuration-during-serve-runtime](https://docs.bentoml.org/en/latest/guides/configuration.html#providing-configuration-during-serve-runtime) and [overriding-configuration-with-environment-variables](https://docs.bentoml.org/en/latest/guides/configuration.html#overriding-configuration-with-environment-variables). However, both of these methods aren't working. After I print out the environment parameters, it shows that both `BENTOML_CONFIG` and `BENTOML_CONFIG_OPTION` are under bash.\r\n\r\n\r\n```\r\nbash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n\n\n### Motivation\n\nI think that this feature is attractive to developers who are using Windows, especially in local dev and testing.\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3962,
    "title": "feature: deploy on Local, similar to gradio demo.launch(share=True)",
    "author": "fadamsyah",
    "state": "closed",
    "created_at": "2023-06-15T10:29:37Z",
    "updated_at": "2025-03-21T08:16:54Z",
    "labels": [],
    "body": "Hi, is it possible to deploy our UI on a local machine and share a link to others as what we can do with `demo.launch(share=True)` (in `gradio`)?\r\n\r\nThank you.",
    "comments": [
      {
        "user": "bojiang",
        "body": "Great idea. bentoml used to have this feature, exposing via a third-party port forwarding service. However, it was no longer free afterwards."
      }
    ]
  },
  {
    "issue_number": 3948,
    "title": "feature: Add compile = False argument to bentoml.keras.load_model()",
    "author": "MarioMarkov",
    "state": "closed",
    "created_at": "2023-06-10T10:29:29Z",
    "updated_at": "2025-03-21T08:16:53Z",
    "labels": [],
    "body": "### Feature request\n\nWhen loading model with bentoml.keras.load_model() a warning is shown: `[WARNING] [dev_api_server] No training configuration found in save file, so the model was *not* compiled. Compile it manually.`, so It would be nice if we can set: `compile=False`\n\n### Motivation\n\nIt is annoying and probably will save execution time.\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3573,
    "title": "feature: triton runner configuration",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2023-02-15T23:54:39Z",
    "updated_at": "2025-03-21T08:16:53Z",
    "labels": [],
    "body": "### Feature request\n\nCurrently, options for triton are configured at runtime, and such options will be applied to all available triton runners.\r\n\r\nTODO: Enable configuration per triton runners\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3515,
    "title": "bug: Specifying `task` as `ner` (alias of token-classification) will cause FileNotFound error when loading transformers pipeline but specifying \"token-classification\" works fine",
    "author": "zhangyilun",
    "state": "closed",
    "created_at": "2023-02-02T18:43:33Z",
    "updated_at": "2025-03-21T08:16:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nIn transformers pipeline, `ner` is an alias for `token-classification` but when I create a pipeline with `task=\"ner\"` and save as bentoml model, it will result in error when loading the pipeline due to not able to fine `pipeline.v1.pkl` file. But exact setup will work with `task=\"token-classification\"`.\r\n\r\nThe error is from `cloudpickle` trying to load `PIPELINE_PICKLE_NAME` (https://github.com/bentoml/BentoML/blob/bedfe4ab1e7b2bc7611534cce24ac8b3b06f0473/src/bentoml/_internal/frameworks/transformers.py#L195-L212).\r\n\r\nThis is where things get interesting since `ner` is an alias for `token-classification` defined in `TASK_ALIASES` and the bentoml code checks:\r\n```\r\nif task not in SUPPORTED_TASKS and task not in TASK_ALIASES:\r\n```\r\nso theoretically it should work without any issues. But it's actually causing error which `cloudpickle` can't fine the defined pipeline pickle file.\n\n### To reproduce\n\nSave model: `save_model.py` with very simple `ner` model directly pulled from huggingface model hub. Saving 2 pipelines with different `task`s.\r\n```{python}\r\nfrom transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\r\nimport bentoml\r\n\r\nname = \"dslim/bert-base-NER\"\r\ntokenizer = AutoTokenizer.from_pretrained(name)\r\nmodel = AutoModelForTokenClassification.from_pretrained(name)\r\n\r\n# as ner task\r\npl = pipeline(task=\"ner\", tokenizer=tokenizer, model=model, aggregation_strategy='none')\r\ntag = bentoml.transformers.save_model(name=f\"test-ner-model:ner\", pipeline=pl)\r\n\r\n# as token-classification task\r\npl = pipeline(task=\"token-classification\", tokenizer=tokenizer, model=model, aggregation_strategy='none')\r\ntag = bentoml.transformers.save_model(name=f\"test-ner-model:token-classification\", pipeline=pl)\r\n```\r\n\r\nService: `service.py`. Here I'm creating a custom runnable class since that's my usecase, but simplified it to only contain init part and a simple inference code (the issue is in init part so we can ignore the inference code).\r\n```{python}\r\nimport bentoml\r\nfrom bentoml.io import JSON\r\nimport torch\r\n\r\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\r\n\r\n# switch\r\n# model_name = \"test-ner-model:token-classification\"\r\nmodel_name = \"test-ner-model:ner\"\r\n\r\nclass CustomRunnable(bentoml.Runnable):\r\n\r\n    SUPPORTED_RESOURCES = (\"nvidia.com/gpu\",)\r\n    SUPPORTS_CPU_MULTI_THREADING = False\r\n\r\n    def __init__(self):\r\n        self.torch_device = DEVICE\r\n        try:\r\n            bento_model = bentoml.transformers.get(model_name)\r\n            self.pipeline = bentoml.transformers.load_model(bento_model)\r\n            self.pipeline.model.to(self.torch_device)\r\n        except Exception as e:\r\n            print(e)\r\n\r\n    @bentoml.Runnable.method(batchable=True)\r\n    def run_batch(self, batch):\r\n        src_text = [b.get(\"text\") for b in batch]\r\n        result = self.pipeline(src_text)\r\n        return result\r\n\r\ncustom_runner = bentoml.Runner(CustomRunnable, name=\"custom_runner\", models=[bentoml.transformers.get(model_name)])\r\n\r\nsvc = bentoml.Service(\"test\", runners=[ custom_runner])\r\n\r\n@svc.api(input=JSON(),\r\n        output=JSON(),\r\n        route=\"predict-ner\")\r\nasync def run_batch(request_data):\r\n    is_single_request = False\r\n    if type(request_data) == dict:\r\n        is_single_request = True\r\n        request_data = [request_data]\r\n    \r\n    outputs = await custom_runner.run_batch.async_run(request_data)\r\n    \r\n    if is_single_request:\r\n        return outputs[0]\r\n    return outputs\r\n```\n\n### Expected behavior\n\nIn the service code, I have\r\n```{python}\r\n# switch\r\n# model_name = \"test-ner-model:token-classification\"\r\nmodel_name = \"test-ner-model:ner\"\r\n```\r\n\r\nStart server:\r\n```{bash}\r\nbentoml serve service.py:svc --reload --verbose\r\n```\r\n\r\nIf set to `token-classification` pipeline model, everything works fine.\r\nBut if set to `ner` pipeline model, it will show error looking for `pipeline.v1.pkl` file.\n\n### Environment\n\n```\r\nbentoml==1.0.4\r\ntransformers==4.21.2\r\n```",
    "comments": []
  },
  {
    "issue_number": 3417,
    "title": "feat: gRPC context",
    "author": "ssheng",
    "state": "closed",
    "created_at": "2023-01-11T23:59:44Z",
    "updated_at": "2025-03-21T08:16:52Z",
    "labels": [],
    "body": "Expose the `grpc.ServicerContext` to the service API through `InferenceApiContext`.\n\n```\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n  def classify(input_series: np.ndarray, ctx) -> np.ndarray:\n    grpc_context: grpc.ServicerContext = ctx.grpc.context\n    grpc_context.set_code(200)\n    grpc_context.set_trailing_metadata()\n    ...\n    result = iris_clf_runner.predict.run(input_series)\n    return result\n```\n\nhttps://github.com/bentoml/BentoML/pull/3511",
    "comments": []
  },
  {
    "issue_number": 3403,
    "title": "bug: bentoml.bentos.build changes the cwd when passing build_ctx",
    "author": "fwindolf",
    "state": "closed",
    "created_at": "2023-01-09T07:46:41Z",
    "updated_at": "2025-03-21T08:16:52Z",
    "labels": [
      "bug",
      "help-wanted"
    ],
    "body": "### Describe the bug\n\nAfter running `bento.bentoml.build` and passing a `build_ctx` argument the cwd is changed to the build_ctx directory.\r\n\r\nI would have expected to return to cwd set before that after finishing the build.\n\n### To reproduce\n\n```\r\nbuild_ctx = tempfile.TemporaryDirectory(prefix=f\"bento_{MODEL_NAME}__\")\r\n\r\ncwd = os.getcwd()\r\n\r\nbento = bentoml.bentos.build(..., build_ctx=build_ctx)\r\n\r\nassert cwd == os.getcwd(), \"Bentoml changed cwd\"\r\n```\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: 1.0.12\r\npython: 3.9.10\r\nplatform: ubuntu (WSL2)",
    "comments": [
      {
        "user": "aarnphm",
        "body": "I believe this is not a bug,  as we are aware of this, from this PR https://github.com/bentoml/BentoML/pull/2737.\r\n\r\n@bojiang iirc the reason for changing the cwd to make the imports order from service.py works correctly.\r\n\r\nWe will have to try importing the service when creating the bento, therefore it is considered as a `standalone_load`.\r\n\r\nMaybe we want to fix this behaviour. Probably want to do something under https://github.com/bentoml/BentoML/blob/931dedcd1fb1d5a054898236898c5dde4ac54588/src/bentoml/bentos.py#L268\r\n\r\n```python\r\ntry:\r\n\tbuild_config = ...\r\nfinally:\r\n\trestore_cwd\r\n```\r\n\r\nBut this would be a breaking change?\r\n\r\ncc @sauyon for thoughts on this as well."
      },
      {
        "user": "fwindolf",
        "body": "Maybe some more context: I use wandb and build the bento directly after training it. This leads to wandb failing on finish with an error, due to the cwd being set to the build_ctx directory, which is deleted after building.\r\n\r\nAs a user, this seems odd as there is no indication in build that the cwd is changed. "
      },
      {
        "user": "sauyon",
        "body": "Er, this looks like a bug to me; the point of the `standalone_load` change was to prevent this behavior."
      }
    ]
  },
  {
    "issue_number": 3201,
    "title": "bug: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Gather node",
    "author": "Matthieu-Tinycoaching",
    "state": "closed",
    "created_at": "2022-11-08T13:54:17Z",
    "updated_at": "2025-03-21T08:15:51Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nHi,\r\n\r\nWhile running locust tests (100 users with spawn=100) on the ONNX model of `cross-encoder/ms-marco-minilm-l-2-v2`, it failed near to the begining with the following message:\r\n\r\n```\r\n2022-11-08T12:29:36.521235725Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/server/runner_app.py\", line 271, in _request_handler\r\n2022-11-08T12:29:36.521240198Z     payload = await infer(params)\r\n2022-11-08T12:29:36.521244217Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/marshal/dispatcher.py\", line 166, in _func\r\n2022-11-08T12:29:36.521248521Z     raise r\r\n2022-11-08T12:29:36.521252449Z   File \"/usr/local/lib/python3.8/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 407, in run_asgi\r\n2022-11-08T12:29:36.521256722Z     result = await app(  # type: ignore[func-returns-value]\r\n2022-11-08T12:29:36.521260777Z   File \"/usr/local/lib/python3.8/dist-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\r\n2022-11-08T12:29:36.521265065Z     return await self.app(scope, receive, send)\r\n2022-11-08T12:29:36.521269072Z   File \"/usr/local/lib/python3.8/dist-packages/uvicorn/middleware/message_logger.py\", line 86, in __call__\r\n2022-11-08T12:29:36.521273401Z     raise exc from None\r\n2022-11-08T12:29:36.521277350Z   File \"/usr/local/lib/python3.8/dist-packages/uvicorn/middleware/message_logger.py\", line 82, in __call__\r\n2022-11-08T12:29:36.521281714Z     await self.app(scope, inner_receive, inner_send)\r\n2022-11-08T12:29:36.521285768Z   File \"/usr/local/lib/python3.8/dist-packages/starlette/applications.py\", line 124, in __call__\r\n2022-11-08T12:29:36.521290086Z     await self.middleware_stack(scope, receive, send)\r\n2022-11-08T12:29:36.521294453Z   File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n2022-11-08T12:29:36.521298872Z     raise exc\r\n2022-11-08T12:29:36.521302747Z   File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n2022-11-08T12:29:36.521307103Z     await self.app(scope, receive, _send)\r\n2022-11-08T12:29:36.521311166Z   File \"/usr/local/lib/python3.8/dist-packages/opentelemetry/instrumentation/asgi/__init__.py\", line 482, in __call__\r\n2022-11-08T12:29:36.521315504Z     await self.app(scope, otel_receive, otel_send)\r\n2022-11-08T12:29:36.521319514Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/server/http/instruments.py\", line 293, in __call__\r\n2022-11-08T12:29:36.521323867Z     await self.app(scope, receive, wrapped_send)\r\n2022-11-08T12:29:36.521331276Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/server/http/access.py\", line 126, in __call__\r\n2022-11-08T12:29:36.521335629Z     await self.app(scope, receive, wrapped_send)\r\n2022-11-08T12:29:36.521339689Z   File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n2022-11-08T12:29:36.521344028Z     raise exc\r\n2022-11-08T12:29:36.521347955Z   File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n2022-11-08T12:29:36.521352304Z     await self.app(scope, receive, sender)\r\n2022-11-08T12:29:36.521356401Z   File \"/usr/local/lib/python3.8/dist-packages/starlette/routing.py\", line 706, in __call__\r\n2022-11-08T12:29:36.521360725Z     await route.handle(scope, receive, send)\r\n2022-11-08T12:29:36.521365010Z   File \"/usr/local/lib/python3.8/dist-packages/starlette/routing.py\", line 276, in handle\r\n2022-11-08T12:29:36.521369362Z     await self.app(scope, receive, send)\r\n2022-11-08T12:29:36.521373425Z   File \"/usr/local/lib/python3.8/dist-packages/starlette/routing.py\", line 66, in app\r\n2022-11-08T12:29:36.521377719Z     response = await func(request)\r\n2022-11-08T12:29:36.521381621Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/server/runner_app.py\", line 271, in _request_handler\r\n2022-11-08T12:29:36.521385992Z     payload = await infer(params)\r\n2022-11-08T12:29:36.521389971Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/marshal/dispatcher.py\", line 166, in _func\r\n2022-11-08T12:29:36.521394285Z     raise r\r\n2022-11-08T12:29:36.521398173Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/marshal/dispatcher.py\", line 232, in outbound_call\r\n2022-11-08T12:29:36.521402705Z     outputs = await self.callback(tuple(d for _, d, _ in inputs_info))\r\n2022-11-08T12:29:36.521406909Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/server/runner_app.py\", line 239, in infer_batch\r\n2022-11-08T12:29:36.521411213Z     batch_ret = await runner_method.async_run(\r\n2022-11-08T12:29:36.521415893Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/runner/runner.py\", line 51, in async_run\r\n2022-11-08T12:29:36.521420169Z     return await self.runner._runner_handle.async_run_method(  # type: ignore\r\n2022-11-08T12:29:36.521424325Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/runner/runner_handle/local.py\", line 57, in async_run_method\r\n2022-11-08T12:29:36.521428742Z     return await anyio.to_thread.run_sync(\r\n2022-11-08T12:29:36.521432889Z   File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\r\n2022-11-08T12:29:36.521437169Z     return await get_asynclib().run_sync_in_worker_thread(\r\n2022-11-08T12:29:36.521441226Z   File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n2022-11-08T12:29:36.521445552Z     return await future\r\n2022-11-08T12:29:36.521452925Z   File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n2022-11-08T12:29:36.521457182Z     result = context.run(func, *args)\r\n2022-11-08T12:29:36.521461192Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/runner/runnable.py\", line 139, in method\r\n2022-11-08T12:29:36.521466675Z     return self.func(obj, *args, **kwargs)\r\n2022-11-08T12:29:36.521470680Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/frameworks/onnx.py\", line 421, in _run\r\n2022-11-08T12:29:36.521475041Z     return self.predict_fns[method_name](output_names, input_names)[0]\r\n2022-11-08T12:29:36.521479112Z   File \"/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 200, in run\r\n2022-11-08T12:29:36.521483497Z     return self._sess.run(output_names, input_feed, run_options)\r\n2022-11-08T12:29:36.521487662Z onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Gather node. Name:'/bert/embeddings/word_embeddings/Gather' Status Message: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:342 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool) Failed to allocate memory for requested buffer of size 132422400\r\n2022-11-08T12:29:36.521493032Z \r\n2022-11-08T12:29:36.524426491Z 2022-11-08T12:29:36+0000 [ERROR] [api_server:3] Exception on /minilm_l_2_v2_similarities_async [POST] (trace=1e309291e1fb7257b9664268bd47dcdd,span=22c844b1ba1a20b1,sampled=0)\r\n2022-11-08T12:29:36.524439840Z Traceback (most recent call last):\r\n2022-11-08T12:29:36.524443224Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/server/http_app.py\", line 311, in api_func\r\n2022-11-08T12:29:36.524446307Z     output = await api.func(input_data)\r\n2022-11-08T12:29:36.524449023Z   File \"/home/bentoml/bento/src/bentoml_gpu_onnx_ct2_service.py\", line 230, in minilm_l_2_v2_similarities_async\r\n2022-11-08T12:29:36.524452020Z     return await onnx_ms_marco_minilm_l_2_v2_runner.run.async_run(encoded_input_onnx.get('input_ids'), encoded_input_onnx.get('token_type_ids'), encoded_input_onnx.get('attention_mask'))\r\n2022-11-08T12:29:36.524454946Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/runner/runner.py\", line 51, in async_run\r\n2022-11-08T12:29:36.524457768Z     return await self.runner._runner_handle.async_run_method(  # type: ignore\r\n2022-11-08T12:29:36.524460452Z   File \"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/runner/runner_handle/remote.py\", line 163, in async_run_method\r\n2022-11-08T12:29:36.524463339Z     raise RemoteException(\r\n2022-11-08T12:29:36.524465911Z bentoml.exceptions.RemoteException: An exception occurred in remote runner ms-marco-minilm-l-2-v2: [500] Internal Server Error\r\n2022-11-08T12:29:36.525132624Z 2022-11-08T12:29:36+0000 [INFO] [api_server:3] 109.205.64.66:9066 (scheme=http,method=POST,path=/minilm_l_2_v2_similarities_async,type=application/json,length=9321) (status=500,type=application/json,length=2) 6564.692ms (trace=1e309291e1fb7257b9664268bd47dcdd,span=22c844b1ba1a20b1,sampled=0)\r\n```\r\n\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\n`bentoml`: 1.0.7\r\n`python`: 3.8.13\r\n`platform`: Linux-5.4.0-65-generic-x86_64-with-glibc2.17\r\n`uid:gid`: 1000:1000\r\n`conda`: 22.9.0\r\n`in_conda_env`: True\r\n\r\n",
    "comments": [
      {
        "user": "aarnphm",
        "body": "cc @larme "
      },
      {
        "user": "larme",
        "body": "@Matthieu-Tinycoaching This seems like a memory allocation error. Do you serve the model on CPU or GPU?"
      },
      {
        "user": "Matthieu-Tinycoaching",
        "body": "Hi @larme I serve it on GPU. This seems weird since this model is lighter and faster than other models that run well in same conditions "
      }
    ]
  },
  {
    "issue_number": 3181,
    "title": "feature: portable containerization with lima",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-11-02T21:22:25Z",
    "updated_at": "2025-03-21T08:15:50Z",
    "labels": [],
    "body": "### Feature request\n\nRight now, containerize depends on user setup docker. #3164 adds the ability to use whatever backend user desire to.\r\n\r\nBut this leaves new users who are confused and don't know how to setup these tools a lot of trouble just to get started.\r\n\r\nContainerize could eventually download a binary of said backend to local bento home directory and run it locally from there.\r\n\r\nI'm not sure if this would solve the case for Windows users. This is probably mainly a Unix-only features.\n\n### Motivation\n\nWe can use something like lima to initialize VM and run said tools.\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3119,
    "title": "bug: MyPy compatibility",
    "author": "judahrand",
    "state": "closed",
    "created_at": "2022-10-19T09:49:24Z",
    "updated_at": "2025-03-21T08:15:50Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nBentoML doesn't play nice with `mypy` due to this issue: python/mypy#12299\r\n\r\nThe issue is that a lot of `# type: ignore` comments in the codebase have additional comments after them (ie. `# type: ignore (reason for ignore)`). This results in packages that use BentoML and `mypy` throwing `error: Invalid \"type: ignore\" comment  [syntax]` for `mypy`.\r\n\r\n\r\n\r\n\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\nBentoML should work with `mypy`\n\n### Environment\n\nDoesn't really matter.",
    "comments": [
      {
        "user": "aarnphm",
        "body": "We made a conscious decision to use pyright instead of mypy. https://github.com/bentoml/BentoML/blob/main/DEVELOPMENT.md#style-check-auto-formatting-type-checking\r\n\r\nWe don't currently have intention on supporting two different types of type checker."
      },
      {
        "user": "judahrand",
        "body": "@aarnphm I don't think you're understanding. This isn't about the development of BentoML... It's about using it elsewhere. Any project which uses BentoML can't currently use `mypy`. You're forcing the choice for others."
      },
      {
        "user": "judahrand",
        "body": "I have BentoML installed in a `venv` and I import and use it in a `service.py` file which will be included in my Bento, right? What if I want to lint that `service.py` file with `mypy` because it is a standard tool in my organization? Currently, it throws errors. "
      }
    ]
  },
  {
    "issue_number": 3061,
    "title": "bug: TypeError: cannot pickle 'ctranslate2.translator.Translator' object",
    "author": "Matthieu-Tinycoaching",
    "state": "closed",
    "created_at": "2022-10-03T13:29:42Z",
    "updated_at": "2025-03-21T08:15:50Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nI have used the CTranslate2 inference within bentoML (https://github.com/OpenNMT/CTranslate2) with custom runner.\r\n\r\nThis library implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to [accelerate and reduce the memory usage](https://github.com/OpenNMT/CTranslate2#benchmarks) of Transformer models on CPU and GPU.\r\n\r\nBut, when trying to save the model with `cloudpickle`, it results in the following error: \r\n\r\n> converting 'CT2_default_opus_mt_fr_en' to lowercase: 'ct2_default_opus_mt_fr_en'\r\n> Traceback (most recent call last):\r\n>   File \"/home/matthieu/Code/Python/CTranslate2/MarianMT.py\", line 125, in <module>\r\n>     saved_model = bentoml.picklable_model.save_model(\r\n>   File \"/home/matthieu/anaconda3/envs/ctranslate2_py3.8/lib/python3.8/site-packages/bentoml/_internal/frameworks/picklable.py\", line 145, in save_model\r\n>     cloudpickle.dump(model, f)\r\n>   File \"/home/matthieu/anaconda3/envs/ctranslate2_py3.8/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py\", line 55, in dump\r\n>     CloudPickler(\r\n>   File \"/home/matthieu/anaconda3/envs/ctranslate2_py3.8/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py\", line 632, in dump\r\n>     return Pickler.dump(self, obj)\r\n> TypeError: cannot pickle 'ctranslate2.translator.Translator' object\r\n\r\n\r\n### To reproduce\r\n\r\n```\r\nimport ctranslate2\r\nimport transformers\r\n\r\nmodel_path = \"/home/matthieu/Deployment/CTranslate2/opus-mt-fr-en/default\"\r\n\r\ntranslator = ctranslate2.Translator(model_path, device=\"cuda\")\r\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\r\n\r\nsaved_model = bentoml.picklable_model.save_model(\r\n    \"CT2_default_opus-mt-fr-en\",    # model name in the local model store \r\n    translator,    # model instance being saved\r\n    signatures={    # model signatures for runner inference\r\n        \"__call__\": {\r\n            \"batchable\": True,\r\n            }\r\n    }\r\n)\r\n\r\nprint(f\"Model saved: {saved_model}\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nModel should be pickled into bentoml models repository.\r\n\r\n### Environment\r\n\r\nbentoml: 1.0.6\r\npython: 3.8.13\r\nubuntu: 18.04.LTS",
    "comments": [
      {
        "user": "aarnphm",
        "body": "`external_modules` can fix this issue here.\r\n\r\n```python\r\nsave_model = bentoml.pickable_model.save_model(..., external_modules=[ctranslate2])\r\n```"
      },
      {
        "user": "Matthieu-Tinycoaching",
        "body": "Hi @aarnphm,\r\n\r\nI followed your suggestion above:\r\n```\r\nsaved_model = bentoml.picklable_model.save_model(\r\n    \"CT2_default_opus-mt-fr-en\",    # model name in the local model store \r\n    translator,    # model instance being saved\r\n    custom_objects={    # save additional user-defined python objects\r\n        \"tokenizer\": tokenizer,\r\n    },\r\n    signatures={    # model signatures for runner inference\r\n        \"__call__\": {\r\n            \"batchable\": True,\r\n            \"batch_dim\": 0,\r\n            }\r\n    },\r\n    external_modules=[ctranslate2],\r\n)\r\n```\r\nBut got the same error as previously:\r\n```\r\nconverting 'CT2_default_opus-mt-fr-en' to lowercase: 'ct2_default_opus-mt-fr-en'\r\nTraceback (most recent call last):\r\n  File \"/home/matthieu/Code/Python/CTranslate2/MarianMT.py\", line 81, in <module>\r\n    saved_model = bentoml.picklable_model.save_model(\r\n  File \"/home/matthieu/anaconda3/envs/ctranslate2_py3.8/lib/python3.8/site-packages/bentoml/_internal/frameworks/picklable.py\", line 145, in save_model\r\n    cloudpickle.dump(model, f)\r\n  File \"/home/matthieu/anaconda3/envs/ctranslate2_py3.8/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py\", line 55, in dump\r\n    CloudPickler(\r\n  File \"/home/matthieu/anaconda3/envs/ctranslate2_py3.8/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py\", line 632, in dump\r\n    return Pickler.dump(self, obj)\r\nTypeError: cannot pickle 'ctranslate2.translator.Translator' object\r\n```"
      },
      {
        "user": "kvasilopoulos",
        "body": "Any update here?"
      }
    ]
  },
  {
    "issue_number": 2860,
    "title": "Error when using torch_hub_yolov5 Custom Mode",
    "author": "darkking-park",
    "state": "closed",
    "created_at": "2022-08-04T11:41:48Z",
    "updated_at": "2025-03-21T08:15:49Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nI inquired about the issue before ==>   https://github.com/bentoml/BentoML/issues/2602\r\n\r\nFirst of all, thank you so much for answering my inquiry and giving me a solution. https://github.com/bentoml/gallery/tree/main/custom_runner/torch_hub_yolov5\r\n\r\nSo I implemented your proposal.\r\nI checked that it went normally if I did as you suggested.\r\n```console\r\n2022-08-04T11:18:51+0000 [WARNING] [cli] Using lowercased runnable class name 'yolov5runnable' for runner.\r\n2022-08-04T11:18:52+0000 [INFO] [cli] Starting development BentoServer from \"service.py:svc\" running on http://127.0.0.1:3000 (Press CTRL+C to quit)\r\n2022-08-04T11:18:53+0000 [WARNING] [dev_api_server] Using lowercased runnable class name 'yolov5runnable' for runner.\r\nstart=========================================\r\nDownloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\r\n2022-08-04T11:18:57+0000 [INFO] [dev_api_server] YOLOv5  2022-8-4 Python-3.7.5 torch-1.11.0+cu102 CUDA:0 (Tesla P40, 24452MiB)\r\n\r\nYOLOv5  2022-8-4 Python-3.7.5 torch-1.11.0+cu102 CUDA:0 (Tesla P40, 24452MiB)\r\n\r\n2022-08-04T11:18:58+0000 [INFO] [dev_api_server] Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\r\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14.1M/14.1M [00:00<00:00, 22.3MB/s]\r\n2022-08-04T11:18:59+0000 [INFO] [dev_api_server] \r\n\r\n2022-08-04T11:19:03+0000 [INFO] [dev_api_server] Fusing layers... \r\nFusing layers... \r\n2022-08-04T11:19:03+0000 [INFO] [dev_api_server] YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\r\nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\r\n2022-08-04T11:19:03+0000 [INFO] [dev_api_server] Adding AutoShape... \r\nAdding AutoShape... \r\n-----------------------------------------------\r\nGPU available\r\n-----------------------------------------------\r\n2022-08-04T11:19:03+0000 [INFO] [dev_api_server] Application startup complete.\r\nApplication startup complete.\r\n\r\n```\r\n### But Error occurs when running with custom model. \r\n\r\nI only changed the following from your service.py source.\r\nservice .py\r\n```python \r\ntorch.hub.load('ultralytics/yolov5', 'yolov5s') \r\n==> torch.hub.load(\"ultralytics/yolov5\", 'custom', path='my_custom_yolov5_model.pt', force_reload=True)\r\n```\r\n\r\nError occurs when using custom model as follows.\r\n```console\r\n2022-08-04T11:20:33+0000 [WARNING] [cli] Using lowercased runnable class name 'yolov5runnable' for runner.\r\n2022-08-04T11:20:33+0000 [INFO] [cli] Starting development BentoServer from \"service.py:svc\" running on http://127.0.0.1:3000 (Press CTRL+C to quit)\r\n2022-08-04T11:20:35+0000 [WARNING] [dev_api_server] Using lowercased runnable class name 'yolov5runnable' for runner.\r\nstart=========================================\r\nDownloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\r\n2022-08-04T11:20:37+0000 [INFO] [dev_api_server] YOLOv5  2022-8-4 Python-3.7.5 torch-1.11.0+cu102 CUDA:0 (Tesla P40, 24452MiB)\r\n\r\nYOLOv5  2022-8-4 Python-3.7.5 torch-1.11.0+cu102 CUDA:0 (Tesla P40, 24452MiB)\r\n\r\n2022-08-04T11:20:38+0000 [ERROR] [dev_api_server] Traceback (most recent call last):\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 47, in _create\r\n    model = DetectMultiBackend(path, device=device, fuse=autoshape)  # detection model\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py\", line 334, in __init__\r\n    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py\", line 81, in attempt_load\r\n    ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1186, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'DetectMultiBackend' object has no attribute 'get'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 51, in _create\r\n    model = attempt_load(path, device=device, fuse=False)  # arbitrary model\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py\", line 81, in attempt_load\r\n    ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1186, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'DetectMultiBackend' object has no attribute 'get'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/starlette/routing.py\", line 645, in lifespan\r\n    async with self.lifespan_context(app):\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/starlette/routing.py\", line 540, in __aenter__\r\n    await self._router.startup()\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/starlette/routing.py\", line 624, in startup\r\n    handler()\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/bentoml/_internal/runner/runner.py\", line 226, in init_local\r\n    self._init_local()\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/bentoml/_internal/runner/runner.py\", line 217, in _init_local\r\n    self._init(LocalRunnerRef)\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/bentoml/_internal/runner/runner.py\", line 211, in _init\r\n    runner_handle = handle_class(self)\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/bentoml/_internal/runner/runner_handle/local.py\", line 25, in __init__\r\n    self._runnable = runner.runnable_class(**runner.runnable_init_params)  # type: ignore\r\n  File \"/home/bentoml_pytorch/golf_service.py\", line 14, in __init__\r\n    self.model = torch.hub.load(\"ultralytics/yolov5\", 'custom', path='/home/yolov5_torchserve_v1/saved_model.pt', force_reload=True, autoshape=True)\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/torch/hub.py\", line 404, in load\r\n    model = _load_local(repo_or_dir, model, *args, **kwargs)\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/torch/hub.py\", line 433, in _load_local\r\n    model = entry(*args, **kwargs)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 74, in custom\r\n    return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 69, in _create\r\n    raise Exception(s) from e\r\nException: 'DetectMultiBackend' object has no attribute 'get'. Cache may be out of date, try `force_reload=True` or see https://github.com/ultralytics/yolov5/issues/36 for help.\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 47, in _create\r\n    model = DetectMultiBackend(path, device=device, fuse=autoshape)  # detection model\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py\", line 334, in __init__\r\n    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py\", line 81, in attempt_load\r\n    ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1186, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'DetectMultiBackend' object has no attribute 'get'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 51, in _create\r\n    model = attempt_load(path, device=device, fuse=False)  # arbitrary model\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py\", line 81, in attempt_load\r\n    ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1186, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'DetectMultiBackend' object has no attribute 'get'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/starlette/routing.py\", line 645, in lifespan\r\n    async with self.lifespan_context(app):\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/starlette/routing.py\", line 540, in __aenter__\r\n    await self._router.startup()\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/starlette/routing.py\", line 624, in startup\r\n    handler()\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/bentoml/_internal/runner/runner.py\", line 226, in init_local\r\n    self._init_local()\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/bentoml/_internal/runner/runner.py\", line 217, in _init_local\r\n    self._init(LocalRunnerRef)\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/bentoml/_internal/runner/runner.py\", line 211, in _init\r\n    runner_handle = handle_class(self)\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/bentoml/_internal/runner/runner_handle/local.py\", line 25, in __init__\r\n    self._runnable = runner.runnable_class(**runner.runnable_init_params)  # type: ignore\r\n  File \"/home/bentoml_pytorch/golf_service.py\", line 14, in __init__\r\n    self.model = torch.hub.load(\"ultralytics/yolov5\", 'custom', path='/home/yolov5_torchserve_v1/saved_model.pt', force_reload=True, autoshape=True)\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/torch/hub.py\", line 404, in load\r\n    model = _load_local(repo_or_dir, model, *args, **kwargs)\r\n  File \"/opt/venv/tf2.3/lib/python3.7/site-packages/torch/hub.py\", line 433, in _load_local\r\n    model = entry(*args, **kwargs)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 74, in custom\r\n    return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 69, in _create\r\n    raise Exception(s) from e\r\nException: 'DetectMultiBackend' object has no attribute 'get'. Cache may be out of date, try `force_reload=True` or see https://github.com/ultralytics/yolov5/issues/36 for help.\r\n```\r\n### Please review the error\r\n\r\nFor your information, I confirmed that my model(my_custom_yolov5_model.pt) is serving normally with flask.\n\n### To reproduce\n\nSame as the guide \r\nhttps://github.com/bentoml/gallery/tree/main/custom_runner/torch_hub_yolov5 \n\n### Expected behavior\n\nSame as the guide \r\nhttps://github.com/bentoml/gallery/tree/main/custom_runner/torch_hub_yolov5 \n\n### Environment\n\nSame as the guide \r\nhttps://github.com/bentoml/gallery/tree/main/custom_runner/torch_hub_yolov5 ",
    "comments": [
      {
        "user": "parano",
        "body": "@darkking-park does it work if you run `torch.hub.load(\"ultralytics/yolov5\", 'custom', path='my_custom_yolov5_model.pt', force_reload=True)` in a python shell?"
      },
      {
        "user": "darkking-park",
        "body": "@parano \r\nThe python flask is in use as below.\r\n```python\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser(description=\"Flask api exposing yolov5 model\")\r\n    parser.add_argument(\"--port\", default=5000, type=int, help=\"port number\")\r\n    args = parser.parse_args()\r\n    model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path='./model/best.pt',  force_reload=True, autoshape=True)\r\n    model.eval()\r\n    app.run(host=\"0.0.0.0\", port=args.port) \r\n```\r\n\r\n### I want to use bento rather than flask"
      },
      {
        "user": "parano",
        "body": "@darkking-park were you able to run the example with a brand new torch hub model downloaded from the git repo? \r\n\r\nWhere did you put the `'my_custom_yolov5_model.pt` file? is it under the same directory?"
      }
    ]
  },
  {
    "issue_number": 2593,
    "title": "build_config(env): not escape space accordingly",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-06-15T19:38:14Z",
    "updated_at": "2025-03-21T08:15:49Z",
    "labels": [
      "bug"
    ],
    "body": "**Describe the bug**\r\n<!--- A clear and concise description of what the bug is. -->\r\nCurrently if users pass in space environment variables `BuildConfig` wouldn't be able to process it accordingly\r\n```yaml\r\nservice: \"service.py:svc\"\r\nenv:\r\n  my space: 1\r\n  ENV: hello world\r\n``` \r\n\r\n**To Reproduce**\r\n<!--\r\nSteps to reproduce the issue:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n-->\r\n\r\n**Expected behavior**\r\n<!--- A clear and concise description of what you expected to happen. -->\r\n\r\n**Screenshots/Logs**\r\n<!--- \r\nIf applicable, add screenshots, logs or error outputs to help explain your problem.\r\n\r\nTo give us more information for diagnosing the issue, make sure to enable debug logging:\r\n\r\nEnable via environment variable, e.g.:\r\n```\r\n$ git clone git@github.com:bentoml/BentoML.git && cd bentoml\r\n$ BENTOML_DEBUG=TRUE python guides/quick-start/main.py\r\n```\r\n\r\nOr set debug logging in your Python code:\r\n```python\r\nfrom bentoml.configuration import set_debug_mode\r\nset_debug_mode(True)\r\n```\r\n\r\nFor BentoML CLI commands, simply add the `--verbose` flag, e.g.:\r\n```bash\r\nbentoml get IrisClassifier --verbose\r\n```\r\n\r\n-->\r\n\r\n\r\n**Environment:**\r\n - OS: [e.g. MacOS 10.14.3]\r\n - Python Version [e.g. Python 3.7.1]\r\n - BentoML Version [e.g. BentoML-0.8.6]\r\n\r\n\r\n**Additional context**\r\n<!-- Add any other context about the problem here. e.g. links to related discussion. -->",
    "comments": []
  },
  {
    "issue_number": 3173,
    "title": "bug: how can i run scheduling tasks using bentoml  ",
    "author": "math-sasso",
    "state": "closed",
    "created_at": "2022-11-02T01:50:08Z",
    "updated_at": "2025-03-21T08:15:41Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nIn the code below I schedule a task to run every 3 seconds.\r\n\r\n```\r\nimport time\r\nfrom datetime import datetime\r\nimport bentoml\r\nfrom apscheduler.schedulers.background import BackgroundScheduler\r\nfrom fastapi import FastAPI, APIRouter\r\n\r\nsched = BackgroundScheduler()\r\napp = FastAPI()\r\nservice = bentoml.Service(\"bentoml-service\")\r\n\r\n\r\nclass Router:\r\n    def __init__(self):\r\n        self.router = APIRouter()\r\n        self.router.add_api_route(\"tick\", self.tick, methods=[\"POST\"])\r\n        self.router.add_api_route(\"job\", self.job, methods=[\"POST\"])\r\n        self.router.on_startup.append(self.job)\r\n\r\n    def tick(self):\r\n        print('Tick! The time is: %s' % datetime.now())\r\n\r\n    def job(self):\r\n        sched.add_job(self.tick, 'interval', seconds=3)\r\n        sched.start()\r\n\r\n        try:\r\n            while True:\r\n                time.sleep(2)\r\n        except (KeyboardInterrupt, SystemExit):\r\n            sched.shutdown()\r\n\r\napp.include_router(Router().router)\r\nservice.mount_asgi_app(app)\r\n```\r\n\r\nIf I use FastAPI, we see the scheduled task result\r\n\r\n`\r\nuvicorn code:app\r\n`\r\n\r\nWe can see the scheduled task\r\n`\r\nTick! The time is: 2022-11-01 22:47:51.878470\r\n`\r\n\r\nOtherwise if we run it with bentoml, we can not schdule the tick method. It does not print Ticks.\r\n`\r\nbentoml serve code:service\r\n` \n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\nTicks should bre printed using bentoml.\n\n### Environment\n\nbentoml: 1.0.8\r\npytohn: 3.8",
    "comments": [
      {
        "user": "ssheng",
        "body": "Though running scheduled task in a service is not well supported use case. I think you can use FastAPI to achieve scheduled task. In FastAPI, you can add startup and shutdown event to an app. You can schedule the task against a BackgroundScheduler in the startup and shutdown the scheduler in the shutdown event. https://fastapi.tiangolo.com/advanced/events/"
      },
      {
        "user": "mathbr10",
        "body": "@ssheng it is perfecly running iwth fastapi, The problem is that it is not running with bentoml, and that is the problem to me because I have a bento app that mounts a fastapi app"
      },
      {
        "user": "aarnphm",
        "body": "since this is a bit stale,\n\nbentoml can mount asgi app via `asgi_app` decorator"
      }
    ]
  },
  {
    "issue_number": 4506,
    "title": "bug: bentlml containerize groupadd: Permission denied",
    "author": "amybachir",
    "state": "open",
    "created_at": "2024-02-18T23:53:16Z",
    "updated_at": "2025-03-20T00:56:39Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWhen running `bentoml containerize` with a custom docker base_image, the command throws an error\r\n```\r\n => => transferring context: 47.64MB                                                                                                                             0.3s\r\n => ERROR [base-container 2/8] RUN groupadd -g 1034 -o bentoml && useradd -m -u 1034 -g 1034 -o -r bentoml                                                       0.4s\r\n------                                                                                                                                                                \r\n > [base-container 2/8] RUN groupadd -g 1034 -o bentoml && useradd -m -u 1034 -g 1034 -o -r bentoml:\r\n0.283 groupadd: Permission denied.\r\n0.284 groupadd: cannot lock /etc/group; try again later.\r\n------\r\nDockerfile:22\r\n--------------------\r\n  20 |     ARG BENTO_USER_UID=1034\r\n  21 |     ARG BENTO_USER_GID=1034\r\n  22 | >>> RUN groupadd -g $BENTO_USER_GID -o $BENTO_USER && useradd -m -u $BENTO_USER_UID -g $BENTO_USER_GID -o -r $BENTO_USER\r\n  23 |     ARG BENTO_PATH=/home/bentoml/bento\r\n  24 |     ENV BENTO_PATH=$BENTO_PATH\r\n--------------------\r\nERROR: failed to solve: process \"/bin/sh -c groupadd -g $BENTO_USER_GID -o $BENTO_USER && useradd -m -u $BENTO_USER_UID -g $BENTO_USER_GID -o -r $BENTO_USER\" did not complete successfully: exit code: 10\r\n```\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml env\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.1.11\r\n`python`: 3.9.13\r\n`platform`: macOS-14.3-arm64-arm-64bit\r\n`uid_gid`: 503:20\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naccelerate==0.27.2\r\naiohttp==3.9.3\r\naiosignal==1.3.1\r\nannotated-types==0.6.0\r\nanyio==4.2.0\r\nappdirs==1.4.4\r\nasgiref==3.7.2\r\nasync-timeout==4.0.3\r\nattrs==23.2.0\r\nbentoml==1.1.11\r\nbitsandbytes==0.41.3.post2\r\nbuild==0.10.0\r\ncattrs==23.1.2\r\ncertifi==2024.2.2\r\ncharset-normalizer==3.3.2\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==3.0.0\r\ncoloredlogs==15.0.1\r\ncontextlib2==21.6.0\r\ndatasets==2.17.0\r\ndeepmerge==1.1.1\r\nDeprecated==1.2.14\r\ndill==0.3.8\r\ndistlib==0.3.8\r\ndistro==1.9.0\r\neinops==0.7.0\r\nexceptiongroup==1.2.0\r\nfastcore==1.5.29\r\nfilelock==3.13.1\r\nfiletype==1.2.0\r\nfrozenlist==1.4.1\r\nfs==2.4.16\r\nfsspec==2023.10.0\r\nghapi==1.0.4\r\nh11==0.14.0\r\nhttpcore==1.0.3\r\nhttpx==0.26.0\r\nhuggingface-hub==0.20.3\r\nhumanfriendly==10.0\r\nidna==3.6\r\nimportlib-metadata==6.11.0\r\ninflection==0.5.1\r\nJinja2==3.1.3\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmdurl==0.1.2\r\nmpmath==1.3.0\r\nmultidict==6.0.5\r\nmultiprocess==0.70.16\r\nmypy-extensions==1.0.0\r\nnetworkx==3.2.1\r\nnumpy==1.26.4\r\nnvidia-ml-py==11.525.150\r\nopenllm==0.4.44\r\nopenllm-client==0.4.44\r\nopenllm-core==0.4.44\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\noptimum==1.17.0\r\norjson==3.9.14\r\npackaging==23.2\r\npandas==2.2.0\r\npathspec==0.12.1\r\npillow==10.2.0\r\npip-requirements-parser==32.0.1\r\npip-tools==7.3.0\r\nplatformdirs==4.2.0\r\nprometheus_client==0.20.0\r\nprotobuf==4.25.3\r\npsutil==5.9.8\r\npyarrow==15.0.0\r\npyarrow-hotfix==0.6\r\npydantic==1.10.14\r\npydantic_core==2.16.2\r\nPygments==2.17.2\r\npyparsing==3.1.1\r\npyproject_hooks==1.0.0\r\npython-dateutil==2.8.2\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.9\r\npytz==2024.1\r\nPyYAML==6.0.1\r\npyzmq==25.1.2\r\nregex==2023.12.25\r\nrequests==2.31.0\r\nrich==13.7.0\r\nsafetensors==0.4.2\r\nschema==0.7.5\r\nscipy==1.12.0\r\nsentencepiece==0.1.99\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nstarlette==0.37.1\r\nsympy==1.12\r\ntokenizers==0.15.2\r\ntomli==2.0.1\r\ntorch==2.2.0\r\ntornado==6.4\r\ntqdm==4.66.2\r\ntransformers==4.37.2\r\ntyping_extensions==4.9.0\r\ntzdata==2024.1\r\nurllib3==2.2.0\r\nuvicorn==0.27.1\r\nvirtualenv==20.25.0\r\nwatchfiles==0.21.0\r\nwrapt==1.16.0\r\nxxhash==3.4.1\r\nyarl==1.9.4\r\nzipp==3.17.0\r\n```\r\n\r\n</details>\r\n\r\n",
    "comments": [
      {
        "user": "felixgao",
        "body": "is there any progress on this?"
      },
      {
        "user": "frostming",
        "body": "Since this is a year old issue, does it reproduce on the latest bentoml?"
      }
    ]
  },
  {
    "issue_number": 5245,
    "title": "bug: bentoml build --containerize cannot run pip install uv",
    "author": "ishandhanani",
    "state": "closed",
    "created_at": "2025-02-23T23:17:07Z",
    "updated_at": "2025-03-18T01:41:08Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWhen running bentoml build --containerize, I receive an error on \n\n```\nRUN pip install uv && UV_PYTHON_INSTALL_DIR=/app/python/ uv venv --python 3.10 /app/.venv  \n```\n\nStating that `line 1: pip: command not found`. I am using a base image that does not contain pip or pip3. I instead use uv already. Is there anyway to skip that line?\n\n### To reproduce\n\nRun containerize and specify a base image that does not have pip or pip3\n\n### Expected behavior\n\nThe container is properly built\n\n### Environment\n\n#### Environment variable\n\n```bash\nBENTOML_DEBUG=''\nBENTOML_QUIET=''\nBENTOML_BUNDLE_LOCAL_BUILD=''\nBENTOML_DO_NOT_TRACK=''\nBENTOML_CONFIG=''\nBENTOML_CONFIG_OPTIONS=''\nBENTOML_PORT=''\nBENTOML_HOST=''\nBENTOML_API_WORKERS=''\n```\n\n#### System information\n\n`bentoml`: 1.4.0\n`python`: 3.10.12\n`platform`: Linux-6.8.0-1021-gcp-x86_64-with-glibc2.35\n`uid_gid`: 1000:1000\n<details><summary><code>pip_packages</code></summary>\n\n<br>\n\n```\na2wsgi==1.10.8\naiohappyeyeballs==2.4.6\naiohttp==3.11.12\naiohttp-cors==0.7.0\naiosignal==1.3.2\naiosqlite==0.21.0\nairportsdata==20241001\nannotated-types==0.7.0\nanyio==4.8.0\nappdirs==1.4.4\nasgiref==3.8.1\nastor==0.8.1\nasync-timeout==5.0.1\nattrs==25.1.0\nbentoml==1.4.0\nblake3==1.0.4\ncachetools==5.5.1\ncattrs==23.1.2\ncertifi==2025.1.31\ncharset-normalizer==3.4.1\nclick==8.1.8\nclick-option-group==0.5.6\ncloudpickle==3.1.1\ncolorful==0.5.6\ncompressed-tensors==0.9.1\ndeepmerge==2.0\ndeprecated==1.2.18\ndepyf==0.18.0\ndill==0.3.9\ndiskcache==5.6.3\ndistlib==0.3.9\ndistro==1.9.0\neinops==0.8.1\nexceptiongroup==1.2.2\nfastapi==0.115.8\nfilelock==3.17.0\nfrozenlist==1.5.0\nfs==2.4.16\nfsspec==2025.2.0\ngguf==0.10.0\ngoogle-api-core==2.24.1\ngoogle-auth==2.38.0\ngoogleapis-common-protos==1.67.0\ngrpcio==1.70.0\nh11==0.14.0\nhttpcore==1.0.7\nhttptools==0.6.4\nhttpx==0.28.1\nhttpx-ws==0.7.1\nhuggingface-hub==0.29.0\nidna==3.10\nimportlib-metadata==8.5.0\ninflection==0.5.1\niniconfig==2.0.0\ninteregular==0.3.3\njinja2==3.1.5\njiter==0.8.2\njsonschema==4.23.0\njsonschema-specifications==2024.10.1\nkantoku==0.18.1\nlark==1.2.2\nlm-format-enforcer==0.10.10\nmarkdown-it-py==3.0.0\nmarkupsafe==3.0.2\nmdurl==0.1.2\nmistral-common==1.5.3\nmpmath==1.3.0\nmsgpack==1.1.0\nmsgspec==0.19.0\nmultidict==6.1.0\nnest-asyncio==1.6.0\nnetworkx==3.4.2\nnumpy==1.26.4\nnvidia-cublas-cu12==12.4.5.8\nnvidia-cuda-cupti-cu12==12.4.127\nnvidia-cuda-nvrtc-cu12==12.4.127\nnvidia-cuda-runtime-cu12==12.4.127\nnvidia-cudnn-cu12==9.1.0.70\nnvidia-cufft-cu12==11.2.1.3\nnvidia-curand-cu12==10.3.5.147\nnvidia-cusolver-cu12==11.6.1.9\nnvidia-cusparse-cu12==12.3.1.170\nnvidia-ml-py==12.570.86\nnvidia-nccl-cu12==2.21.5\nnvidia-nvjitlink-cu12==12.4.127\nnvidia-nvtx-cu12==12.4.127\nopenai==1.63.2\nopencensus==0.11.4\nopencensus-context==0.1.3\nopencv-python-headless==4.11.0.86\nopentelemetry-api==1.30.0\nopentelemetry-instrumentation==0.51b0\nopentelemetry-instrumentation-aiohttp-client==0.51b0\nopentelemetry-instrumentation-asgi==0.51b0\nopentelemetry-sdk==1.30.0\nopentelemetry-semantic-conventions==0.51b0\nopentelemetry-util-http==0.51b0\noutlines==0.1.11\noutlines-core==0.1.26\npackaging==24.2\npartial-json-parser==0.2.1.1.post5\npathspec==0.12.1\npillow==11.1.0\npip-requirements-parser==32.0.1\nplatformdirs==4.3.6\npluggy==1.5.0\nprometheus-client==0.21.1\nprometheus-fastapi-instrumentator==7.0.2\nprompt-toolkit==3.0.50\npropcache==0.2.1\nproto-plus==1.26.0\nprotobuf==5.29.3\npsutil==7.0.0\npy-cpuinfo==9.0.0\npy-spy==0.4.0\npyasn1==0.6.1\npyasn1-modules==0.4.1\npybind11==2.13.6\npycountry==24.6.1\npydantic==2.10.6\npydantic-core==2.27.2\npygments==2.19.1\npyparsing==3.2.1\npytest==8.3.4\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.1\npython-json-logger==3.2.1\npython-multipart==0.0.20\npyyaml==6.0.2\npyzmq==26.2.1\nquestionary==2.1.0\nray==2.42.1\nreferencing==0.36.2\nregex==2024.11.6\nrequests==2.32.3\nrich==13.9.4\nrpds-py==0.22.3\nrsa==4.9\nsafetensors==0.5.2\nschema==0.7.7\nsentencepiece==0.2.0\nsetuptools==75.8.0\nsimple-di==0.1.5\nsix==1.17.0\nsmart-open==7.1.0\nsniffio==1.3.1\nstarlette==0.45.3\nsympy==1.13.1\ntiktoken==0.9.0\ntokenizers==0.21.0\ntomli==2.2.1\ntomli-w==1.2.0\ntorch==2.5.1\ntorchaudio==2.5.1\ntorchvision==0.20.1\ntornado==6.4.2\ntqdm==4.67.1\ntransformers==4.49.0\ntriton==3.1.0\ntyping-extensions==4.12.2\nurllib3==2.3.0\nuv==0.6.2\nuvicorn==0.34.0\nuvloop==0.21.0\nvirtualenv==20.29.2\nvllm==0.7.2\nwatchfiles==1.0.4\nwcwidth==0.2.13\nwebsockets==15.0\nwrapt==1.17.2\nwsproto==1.2.0\nxformers==0.0.28.post3\nxgrammar==0.1.13\nyarl==1.18.3\nzipp==3.21.0\n```\n\n</details>",
    "comments": [
      {
        "user": "frostming",
        "body": "> Is there anyway to skip that line?\n\nNo for now, as a workaround you can add `python3-pip` to the `system_packages`"
      }
    ]
  },
  {
    "issue_number": 5246,
    "title": "bug: Bentoml mounted FastAPI app doesn't apply timeout value",
    "author": "jtong99",
    "state": "open",
    "created_at": "2025-02-24T01:21:51Z",
    "updated_at": "2025-03-18T01:36:49Z",
    "labels": [
      "bug",
      "feedback-wanted"
    ],
    "body": "### Describe the bug\n\nHi, I am running Bentoml and mount with FastAPI application.\n\n```\napp = FastAPI()\n\n@bentoml.service(\n    name=\"ai_service\",\n    resources={\n        \"cpu\": CPU_THREADS,\n        \"memory\": MEMORY,\n    },\n    traffic={\"timeout\": 3600}\n)\n@bentoml.asgi_app(app, path=\"/api\")\n\n@app.post('/v1/test')\nasync def ai_api(self, request: Request):\n```\n\nEven though I added timeout value to bentoml and run the service with timeout `bentoml serve service:svc  --timeout=3600`, the API `/v1/test` only has 60s timeout. Is there anyway to make the API route from FastAPI which is mounted to Bentoml can accept the timeout value?\n\nThanks\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\nThe mounted FastAPI service accepted timeout value from Bentoml\n\n### Environment\n\nBentoml: latest",
    "comments": [
      {
        "user": "frostming",
        "body": "I can't reproduce using the latest version, here is the testing code:\n\n```python\nimport asyncio\n\nimport bentoml\nimport fastapi\n\napp = fastapi.FastAPI()\n\n\n@app.post(\"/greet\")\nasync def greet():\n    await asyncio.sleep(120)\n    return \"hello world\"\n\n\n@bentoml.service(traffic={\"timeout\": 360})\n@bentoml.asgi_app(app)\nclass MyService:\n    pass\n```\nServe with `bentoml serve` and request with `curl -XPOST http://localhost:3000/greet`\nThe response returns after around 120s"
      },
      {
        "user": "jtong99",
        "body": "Actually, it's about multiple APIs comes and it's timeout on processing after 60s, I thought it was about timeout variable but I checked the bentoml library and found it's because of this line at `/bentoml/_internal/server/http/traffic.py`:\n\n> loop.call_later(self.timeout, self._set_timer_out, waiter)\n\nAfter I commented out this line, the issue is gone. The `TimeoutMiddleware` did not read the environment variable `timeout`, correct? Is there anyway to control this? \n\nThis is the error from bentoml regarding above error:\n\n> {\"error\":\"Not able to process the request in 60.0 seconds\"}"
      },
      {
        "user": "frostming",
        "body": "> After I commented out this line, the issue is gone. The `TimeoutMiddleware` did not read the environment variable `timeout`, correct? Is there anyway to control this?\n\nWhen you comment out this line, the middleware never times out.\n\n`self.timeout` is retrieved from the config value, you can add some debug prints to verify the value."
      }
    ]
  },
  {
    "issue_number": 4193,
    "title": "bug: zombie process for running service",
    "author": "oppokui",
    "state": "open",
    "created_at": "2023-09-15T08:50:44Z",
    "updated_at": "2025-03-16T22:36:13Z",
    "labels": [],
    "body": "### Describe the bug\n\nWhen I run \"openllm start xxx\" to play with it, I can easily use crtl+C to stop it, no http server left.\r\nBut once I want to start a backend process by \"nohup openllm start llama 2>&1 &\", I will lose the control of backend process. It will launch the server as well as lot of http server (per core) at backend. When I kill the main process, those http listener process will be still alive, controlled by some circusd. I am frustrating about it. Everytime I need to reboot OS to restore the env.\r\n\r\nHow to make the server easy to stop?\n\n### To reproduce\n\n_No response_\n\n### Logs\n\n_No response_\n\n### Environment\n\nopenllm, 0.3.3 (compiled: no)\r\nPython (CPython) 3.10.4\r\n\n\n### System information (Optional)\n\n_No response_",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Ye, so we definitely want to improve on this\r\n\r\nyou can do \r\n\r\n```bash\r\npgrep circusd | xargs kill -9\r\n```"
      },
      {
        "user": "aarnphm",
        "body": "Will translate to bentoml since this is mainly on BentoML improvement"
      },
      {
        "user": "sauyon",
        "body": "I would advise against running kill 9 as a rule. How are you terminating the processes normally?"
      }
    ]
  },
  {
    "issue_number": 4410,
    "title": "bug: Docker build includes an enormous amount of context data",
    "author": "wmeints",
    "state": "open",
    "created_at": "2024-01-17T07:13:00Z",
    "updated_at": "2025-03-11T16:59:00Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nThe command `bentoml build --containerize` copies a serious amount of data to the docker build. I'm not entirely sure where it comes from, but it's 29GB while my project directory is less then 100MB. \r\n\r\nThe amount of data is so huge that my root file system is out of space by the time the build tries to copy over the project files.\n\n### To reproduce\n\nThis is the log I'm getting:\r\n\r\n```\r\n❯ bentoml build --containerize > /tmp/bento-build.log\r\nWARNING: --strip-extras is becoming the default in version 8.0.0. To silence this warning, either use --strip-extras to opt into the new default or use --no-strip-extras to retain the existing behavior.\r\n[+] Building 54.5s (13/15)                                                                                                                                                                                               docker:default\r\n => [internal] load .dockerignore                                                                                                                                                                                                  0.0s\r\n => => transferring context: 2B                                                                                                                                                                                                    0.0s\r\n => [internal] load build definition from Dockerfile                                                                                                                                                                               0.0s\r\n => => transferring dockerfile: 1.71kB                                                                                                                                                                                             0.0s\r\n => [internal] load metadata for docker.io/library/python:3.11-slim                                                                                                                                                                1.4s\r\n => [base-container  1/11] FROM docker.io/library/python:3.11-slim@sha256:637774748f62b832dc11e7b286e48cd716727ed04b45a0322776c01bc526afc3                                                                                         0.0s\r\n => [internal] load build context                                                                                                                                                                                                 31.8s\r\n => => transferring context: 29.29GB                                                                                                                                                                                              31.8s\r\n => CACHED [base-container  2/11] RUN rm -f /etc/apt/apt.conf.d/docker-clean; echo 'Binary::apt::APT::Keep-Downloaded-Packages \"true\";' > /etc/apt/apt.conf.d/keep-cache                                                           0.0s\r\n => CACHED [base-container  3/11] RUN --mount=type=cache,target=/var/lib/apt --mount=type=cache,target=/var/cache/apt set -eux &&     apt-get update -y &&     apt-get install -q -y --no-install-recommends --allow-remove-essen  0.0s\r\n => CACHED [base-container  4/11] RUN groupadd -g 1034 -o bentoml && useradd -m -u 1034 -g 1034 -o -r bentoml                                                                                                                      0.0s\r\n => CACHED [base-container  5/11] RUN mkdir /home/bentoml/bento && chown bentoml:bentoml /home/bentoml/bento -R                                                                                                                    0.0s\r\n => CACHED [base-container  6/11] WORKDIR /home/bentoml/bento                                                                                                                                                                      0.0s\r\n => CACHED [base-container  7/11] COPY --chown=bentoml:bentoml ./env/python ./env/python/                                                                                                                                          0.0s\r\n => CACHED [base-container  8/11] RUN --mount=type=cache,target=/root/.cache/pip bash -euxo pipefail /home/bentoml/bento/env/python/install.sh                                                                                     0.0s\r\n => ERROR [base-container  9/11] COPY --chown=bentoml:bentoml . ./                                                                                                                                                                21.2s\r\n------\r\n > [base-container  9/11] COPY --chown=bentoml:bentoml . ./:\r\n------\r\nDockerfile:44\r\n--------------------\r\n  42 |     # install python packages with install.sh\r\n  43 |     RUN --mount=type=cache,target=/root/.cache/pip bash -euxo pipefail /home/bentoml/bento/env/python/install.sh\r\n  44 | >>> COPY --chown=bentoml:bentoml . ./\r\n  45 |     \r\n  46 |     # Block SETUP_BENTO_ENTRYPOINT\r\n--------------------\r\nERROR: failed to solve: failed to copy files: userspace copy failed: write /var/lib/docker/overlay2/ookbcxyphwifanadslwb9dq6w/merged/home/bentoml/bento/models/reviews_classifier_rf/jeqarfvu7w5tn4gu/saved_model.pkl: no space left on device\r\n\r\nEncountered exception while trying to building image: Command '['/usr/bin/docker', 'build', '--tag', 'review_classifier:7l2y44fva2xu34gu', '--file', '/tmp/tmp30elvep5fsTempFS/env/docker/Dockerfile', '/tmp/tmp30elvep5fsTempFS/']' returned non-zero exit status 1.\r\nTraceback (most recent call last):\r\n  File \"/home/willem/repos/review-classification/.venv/lib/python3.11/site-packages/bentoml/_internal/container/__init__.py\", line 234, in build\r\n    return builder.build(**kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/willem/repos/review-classification/.venv/lib/python3.11/site-packages/bentoml/_internal/container/base.py\", line 190, in build\r\n    raise BentoMLException(str(e)) from None\r\nbentoml.exceptions.BentoMLException: Command '['/usr/bin/docker', 'build', '--tag', 'review_classifier:7l2y44fva2xu34gu', '--file', '/tmp/tmp30elvep5fsTempFS/env/docker/Dockerfile', '/tmp/tmp30elvep5fsTempFS/']' returned non-zero exit status 1.\r\n```\n\n### Expected behavior\n\nI expect bentoml not to copy my whole root filesystem?!\n\n### Environment\n\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.1.11\r\n`python`: 3.11.6\r\n`platform`: Linux-6.7.0-arch3-1-x86_64-with-glibc2.38\r\n`uid_gid`: 1000:1000\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiohttp==3.9.1\r\naiosignal==1.3.1\r\nalembic==1.13.1\r\naniso8601==9.0.1\r\nannotated-types==0.6.0\r\nanyio==4.2.0\r\nappdirs==1.4.4\r\nasgiref==3.7.2\r\nattrs==23.2.0\r\nbackoff==2.2.1\r\nbentoml==1.1.11\r\nblack==23.12.1\r\nblinker==1.7.0\r\nblis==0.7.11\r\nbuild==1.0.3\r\ncatalogue==2.0.10\r\ncattrs==23.1.2\r\ncertifi==2023.11.17\r\ncharset-normalizer==3.3.2\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpathlib==0.16.0\r\ncloudpickle==3.0.0\r\ncoloredlogs==14.0\r\nconfection==0.1.4\r\ncontextlib2==21.6.0\r\ncontourpy==1.2.0\r\ncroniter==2.0.1\r\ncycler==0.12.1\r\ncymem==2.0.8\r\ndagster==1.6.0\r\ndagster-graphql==1.6.0\r\ndagster-pipes==1.6.0\r\ndagster-webserver==1.6.0\r\ndatabricks-cli==0.18.0\r\ndeepmerge==1.1.1\r\nDeprecated==1.2.14\r\ndocker==6.1.3\r\ndocstring-parser==0.15\r\nentrypoints==0.4\r\nflake8==7.0.0\r\nFlask==3.0.0\r\nfonttools==4.47.2\r\nfrozenlist==1.4.1\r\nfs==2.4.16\r\nfsspec==2023.12.2\r\ngitdb==4.0.11\r\nGitPython==3.1.41\r\ngql==3.5.0\r\ngraphene==3.3\r\ngraphql-core==3.2.3\r\ngraphql-relay==3.2.0\r\ngreenlet==3.0.3\r\ngrpcio==1.60.0\r\ngrpcio-health-checking==1.60.0\r\ngunicorn==21.2.0\r\nh11==0.14.0\r\nhttpcore==1.0.2\r\nhttptools==0.6.1\r\nhttpx==0.26.0\r\nhumanfriendly==10.0\r\nidna==3.6\r\nimportlib-metadata==6.11.0\r\ninflection==0.5.1\r\niniconfig==2.0.0\r\nisort==5.13.2\r\nitsdangerous==2.1.2\r\nJinja2==3.1.3\r\njoblib==1.3.2\r\nkiwisolver==1.4.5\r\nlangcodes==3.3.0\r\nMako==1.3.0\r\nMarkdown==3.5.2\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.3\r\nmatplotlib==3.8.2\r\nmccabe==0.7.0\r\nmdurl==0.1.2\r\nmlflow==2.9.2\r\nmultidict==6.0.4\r\nmurmurhash==1.0.10\r\nmypy-extensions==1.0.0\r\nnumpy==1.26.3\r\nnvidia-ml-py==11.525.150\r\noauthlib==3.2.2\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\npackaging==23.2\r\npandas==2.1.4\r\npathspec==0.12.1\r\npendulum==2.1.2\r\npillow==10.2.0\r\npip-requirements-parser==32.0.1\r\npip-tools==7.3.0\r\nplatformdirs==4.1.0\r\npluggy==1.3.0\r\npreshed==3.0.9\r\nprometheus-client==0.19.0\r\nprotobuf==4.25.2\r\npsutil==5.9.7\r\npyarrow==14.0.2\r\npycodestyle==2.11.1\r\npydantic==2.5.3\r\npydantic_core==2.14.6\r\npyflakes==3.2.0\r\nPygments==2.17.2\r\nPyJWT==2.8.0\r\npyparsing==3.1.1\r\npyproject_hooks==1.0.0\r\npytest==7.4.4\r\npython-dateutil==2.8.2\r\npython-dotenv==1.0.0\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.6\r\npytz==2023.3.post1\r\npytzdata==2020.1\r\nPyYAML==6.0.1\r\npyzmq==25.1.2\r\nquerystring-parser==1.2.4\r\nrequests==2.31.0\r\nrequests-toolbelt==1.0.0\r\n-e git+ssh://git@github.com/wmeints/mlops-with-dagster.git@3469f3a4fb579d9bdd11c62c4023bb0b26834e24#egg=review_classification\r\nrich==13.7.0\r\nschema==0.7.5\r\nscikit-learn==1.3.2\r\nscipy==1.11.4\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsmart-open==6.4.0\r\nsmmap==5.0.1\r\nsniffio==1.3.0\r\nspacy==3.7.2\r\nspacy-legacy==3.0.12\r\nspacy-loggers==1.0.5\r\nSQLAlchemy==2.0.25\r\nsqlparse==0.4.4\r\nsrsly==2.4.8\r\nstarlette==0.35.1\r\nstructlog==24.1.0\r\ntabulate==0.9.0\r\nthinc==8.2.2\r\nthreadpoolctl==3.2.0\r\ntomli==2.0.1\r\ntoposort==1.10\r\ntornado==6.4\r\ntqdm==4.66.1\r\ntyper==0.9.0\r\ntyping_extensions==4.9.0\r\ntzdata==2023.4\r\nuniversal-pathlib==0.1.4\r\nurllib3==2.1.0\r\nuvicorn==0.25.0\r\nuvloop==0.19.0\r\nwasabi==1.1.2\r\nwatchdog==3.0.0\r\nwatchfiles==0.21.0\r\nweasel==0.3.4\r\nwebsocket-client==1.7.0\r\nwebsockets==12.0\r\nWerkzeug==3.0.1\r\nwrapt==1.16.0\r\nyarl==1.9.4\r\nzipp==3.17.0\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "jeffmarshall",
        "body": "it's copying the contents of the working directory to the working directory in the container. `copy . ./`"
      },
      {
        "user": "aarnphm",
        "body": "Do you hae a virtualenv in this directory?\n\nCan you add a `.bentoignore` and ignore this directory if you have one?"
      }
    ]
  },
  {
    "issue_number": 4940,
    "title": "bug: pynvml.NVMLError_DriverNotLoaded: Driver Not Loaded",
    "author": "rlleshi",
    "state": "closed",
    "created_at": "2024-08-26T18:51:12Z",
    "updated_at": "2025-03-10T17:12:43Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nWhen starting the server the following cuda-related error is thrown:\r\n\r\n```\r\n❯ bentoml serve . --verbose --debug\r\nTraceback (most recent call last):\r\n  File \"/home/user/.pyenv/versions/neoface-swapper/bin/bentoml\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/bentoml_cli/utils.py\", line 372, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/bentoml_cli/utils.py\", line 343, in wrapper\r\n    return_value = func(*args, **kwargs)\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/bentoml_cli/env_manager.py\", line 123, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/bentoml_cli/serve.py\", line 313, in serve\r\n    serve_http(\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/_bentoml_impl/server/serving.py\", line 204, in serve_http\r\n    allocator = ResourceAllocator()\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/_bentoml_impl/server/allocator.py\", line 21, in __init__\r\n    self.system_resources = system_resources()\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/bentoml/_internal/resource.py\", line 46, in system_resources\r\n    res[resource_kind] = resource.from_system()\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/bentoml/_internal/resource.py\", line 248, in from_system\r\n    pynvml.nvmlInit()\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/pynvml.py\", line 1793, in nvmlInit\r\n    nvmlInitWithFlags(0)\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/pynvml.py\", line 1783, in nvmlInitWithFlags\r\n    _nvmlCheckReturn(ret)\r\n  File \"/home/user/.pyenv/versions/3.9.0/envs/neoface-swapper/lib/python3.9/site-packages/pynvml.py\", line 855, in _nvmlCheckReturn\r\n    raise NVMLError(ret)\r\npynvml.NVMLError_DriverNotLoaded: Driver Not Loaded\r\n\r\n```\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\nI have nvidia drivers and nvidia toolkit installed because I sometimes use an eGPU. But bento only runs when the eGPU is attached. The current application code is supposed to work both with CPU & GPU. Actually it was working a couple of weeks ago when I first developed the solution in another machine .\r\n\r\n### Environment\r\n\r\nbentoml: 1.3.0\r\npython: 3.9.0\r\nubuntu 22.04",
    "comments": []
  },
  {
    "issue_number": 5233,
    "title": "feature: How to use multi GPU",
    "author": "Euraxluo",
    "state": "open",
    "created_at": "2025-02-20T03:24:44Z",
    "updated_at": "2025-02-25T01:53:47Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nAn example of a model that uses multiple gpus at the same time, or we should have a way to make this easy to use\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "```python\n@bentoml.service(resources={\"gpu\": 4})\nclass MyService:\n    ...\n```\nAnd it will inject `CUDA_VISIBLE_DEVICES=0,1,2,3` into the environment"
      },
      {
        "user": "Euraxluo",
        "body": "Will it automatically schedule and use all gpus at the same time?\n"
      },
      {
        "user": "frostming",
        "body": "What automation do you mean, that is all it does and it depends on how the framework respects the env var."
      }
    ]
  },
  {
    "issue_number": 5248,
    "title": "Fill \"try it out\" swagger section with examples data",
    "author": "Spoutnik97",
    "state": "closed",
    "created_at": "2025-02-24T15:10:53Z",
    "updated_at": "2025-02-25T01:10:42Z",
    "labels": [],
    "body": "I would like to fill default values for the \"try it out\" swagger section, but I cannot achieve to do this. \n\nWhat I tried this, but didn't work : \n\n```python\n    @bentoml.api\n    def predict(\n        self,\n        companyId: str = Field(examples=[\"abcde\"]),\n        qty: int = Field(examples=[10]),\n        json: List[Dict] = Field(examples=[mock_features]),\n    ):\n```\n\nAny Idea to fill this section ? ",
    "comments": [
      {
        "user": "frostming",
        "body": "Use `default` instead of `examples`"
      }
    ]
  },
  {
    "issue_number": 5235,
    "title": "feature: add time-series models with statsmodels and river library",
    "author": "jsamantaucd",
    "state": "open",
    "created_at": "2025-02-20T16:36:53Z",
    "updated_at": "2025-02-21T11:17:35Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nFeature request\nI have implemented and tested two examples that trains a time series model such as ARIMA or River ML model and serves it using BentoML's custom runner to make predictions.\n\nThis feature was tested on Bentoml v1.4\n\n\n\n### Motivation\n\nI believe it would be a nice guide to someone exploring time series models with custom runner implementation for models like ARIMA.\nRiver offers a diverse selection of ML models for online learning that are much more efficient than the sklearn models, and integrating this MLflow would create a comprehensive ML solution.\n\n### Other\n\nThis was already requested as a feature in:\nhttps://github.com/bentoml/BentoML/pull/4979\nhttps://github.com/bentoml/BentoML/pull/4896\n\nAnd as suggested I have created two standalone repositories for the above two examples:\nhttps://github.com/jsamantaucd/BentoStatsmodel\nhttps://github.com/jsamantaucd/BentoRiverModel\n\nPlease kindly review and link it in the README.",
    "comments": [
      {
        "user": "frostming",
        "body": "Thanks for that, Please test if they work on bentoml 1.4"
      },
      {
        "user": "jsamantaucd",
        "body": "Hi @frostming, I have tested with the latest bentoml 1.4 version, updated requirements, and this works fine. Regards!"
      }
    ]
  },
  {
    "issue_number": 5229,
    "title": "bug: multi-arch parallel builds fails to obtain apt lock",
    "author": "aas47",
    "state": "open",
    "created_at": "2025-02-19T17:47:20Z",
    "updated_at": "2025-02-19T17:50:07Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWhen using `buildx`, `--platform=linux/amd64,linux/arm64` and `{% block SETUP_BENTO_BASE_IMAGE %}`, `bentoml containerize` fails with `apt` unable to obtain lock.\n\n```\n# short snippet\n0.583 E: Could not get lock /var/lib/apt/lists/lock. It is held by process 0\n0.584 E: Unable to lock directory /var/lib/apt/lists/\n```\n\nI'm new to bentoml so unsure where this config is coming from and how to update it. I haven't found a way to add something like `sharing=shared` or `sharing=locked` to this particular instruction. I found [this](https://github.com/bentoml/BentoML/blob/main/src/bentoml/_internal/container/frontend/dockerfile/templates/_macros.j2#L1) is maybe where it needs to be added? I'm wondering how.\n\n```\n# full error\n > [linux/arm64 base-container  3/12] RUN --mount=type=cache,target=/var/lib/apt --mount=type=cache,target=/var/cache/apt set -eux &&     apt-get update -y &&     apt-get install -q -y --no-install-recommends --allow-remove-essential         ca-certificates gnupg2 bash build-essential:\n0.142 + apt-get update -y\n0.381 Reading package lists...\n0.583 E: Could not get lock /var/lib/apt/lists/lock. It is held by process 0\n0.584 E: Unable to lock directory /var/lib/apt/lists/\n------\nDockerfile:22\n--------------------\n  21 |     RUN rm -f /etc/apt/apt.conf.d/docker-clean; echo 'Binary::apt::APT::Keep-Downloaded-Packages \"true\";' > /etc/apt/apt.conf.d/keep-cache\n  22 | >>> RUN --mount=type=cache,target=/var/lib/apt --mount=type=cache,target=/var/cache/apt set -eux && \\\n  23 | >>>     apt-get update -y && \\\n  24 | >>>     apt-get install -q -y --no-install-recommends --allow-remove-essential \\\n  25 | >>>         ca-certificates gnupg2 bash build-essential\n  26 |     # Build arguments -- passed in by bento as it does not allow env vars to be passed in\n--------------------\nERROR: failed to solve: process \"/bin/sh -c set -eux &&     apt-get update -y &&     apt-get install -q -y --no-install-recommends --allow-remove-essential         ca-certificates gnupg2 bash build-essential\" did not complete successfully: exit code: 100\n```\n\n### To reproduce\n\nsetup command:\n```\ndocker buildx create --name container-builder --driver docker-container --bootstrap --use\ndocker buildx inspect --bootstrap\nbentoml containerize testing:latest \n      --backend buildx\n      --opt builder=container-builder\n      --opt platform=linux/amd64,linux/arm64\n      --opt pull\n      --opt push\n      -t \"${MYIMAGETAG}\"\n\n```\n\nand \n\n```\n# Dockerfile.template\n{% extends bento_base_template %}\n{% block SETUP_BENTO_BASE_IMAGE %}\n{{ super() }}\n{% endblock %}\n{% block SETUP_BENTO_COMPONENTS %}\n{{ super() }}\n.\n.\n.\n.\n.\n{% endblock %}\n```\n\n### Expected behavior\n\nexpect the build to just pass.\n\n### Environment\n```\ndocker:\n  dockerfile_template: \"Dockerfile.template\"\n  env:\n    BUILD_ENV: local\n```",
    "comments": []
  },
  {
    "issue_number": 5219,
    "title": "bug: AttributeError: 'Interface' object has no attribute 'max_file_size'.",
    "author": "braveapple",
    "state": "closed",
    "created_at": "2025-02-18T12:37:58Z",
    "updated_at": "2025-02-19T03:27:26Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\n```\nfrom __future__ import annotations  # I001\n\nimport bentoml\nfrom bentoml.io import Image\n\nwith bentoml.importing():\n    import gradio as gr\n\ndef run(image_file: str) -> str:\n    print(f'(2) image_file = {image_file}')\n    return image_file\n\n\nio = gr.Interface(\n    fn=run,\n    inputs=gr.Image(type='filepath', label=\"input\"),\n    outputs=gr.Textbox(label='output'),\n    title=\"Summarization\",\n    description=\"Enter text to get summarized text.\",\n)\n\n@bentoml.service(resources={\"cpu\": \"4\"})\n@bentoml.gradio.mount_gradio_app(io, path=\"/ui\")\nclass Demo:\n    def __init__(self) -> None:\n        pass\n        # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        # self.pipeline = pipeline(\"summarization\", device=device)\n\n    @bentoml.api()\n    def run(self, image_file: str) -> str:\n        print(f'(1) image_file = {image_file}')\n        return image_file\n```\nWhen upload an image file through gradio==4.44.1, the bentoml serveice obtian such an error \"AttributeError: 'Interface' object has no attribute 'max_file_size'. Did you mean: 'max_batch_size'?\"\n\n![Image](https://github.com/user-attachments/assets/2f241273-a1bd-472a-8671-5cd7763d670a)\n\n![Image](https://github.com/user-attachments/assets/c8149f86-c3f6-4c82-9d3b-9ace2d552770)\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\ngradio==4.44.1\nbentoml==1.3.21\npython==3.10.0",
    "comments": []
  },
  {
    "issue_number": 5215,
    "title": "bug: Cannot set input of API as data binary similar with previous BENTOML version",
    "author": "jtong99",
    "state": "closed",
    "created_at": "2025-02-15T05:02:27Z",
    "updated_at": "2025-02-19T01:50:13Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nHi, I just updated Bentoml to the latest version and I recognize there is no options to make the input type as data binary. In previous version, I am able to set the data binary input like this:\n\n```python\n@svc.api(\n    input=File(),\n    output=JSON(),\n    route='/image',\n)\n```\n\nBut in the new version of Bentoml:\n\n```python\n@bentoml.api(route='/image')\n    async def recognize_with_line(self, image: PIL_Image) -> Dict:\n```\n\nBentoml only required me to set input as form data.\n\nIs there anyway that I can set the input as data binary like previous version of Bentoml?\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\nI would like to set input as data binary\n\n### Environment\n\nbentoml: latest version",
    "comments": [
      {
        "user": "frostming",
        "body": "It is not supported by the new `@bentoml.service` yet"
      },
      {
        "user": "jtong99",
        "body": "Got it, thanks!!!"
      }
    ]
  },
  {
    "issue_number": 5202,
    "title": "feature: New block for Docker template",
    "author": "maxhahn",
    "state": "open",
    "created_at": "2025-02-04T11:04:56Z",
    "updated_at": "2025-02-07T01:14:02Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nRight now, I need to add a certificate to my container before any apt-gets can be made. \nBecause there is no templatable block right after the FROM step in the Dockerfile, this is a major hassle.\n\nSo, my request is to add a Block right after the FROM step.\n\nRight now, I work around this issue by templating in front of the FROM step, where a base image is defined which can the be loaded as the base image in the bentofile. This is not nice\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "We will soon deliver a new mechanism for customizing the build process, to replace and deprecate the dockerfile template, stay tuned."
      }
    ]
  },
  {
    "issue_number": 5199,
    "title": "Python",
    "author": "Techasit232000",
    "state": "closed",
    "created_at": "2025-01-28T15:39:14Z",
    "updated_at": "2025-01-31T02:17:08Z",
    "labels": [],
    "body": "\nfrom pymongo.mongo_client import MongoClient\nfrom pymongo.server_api import ServerApi\n\nuri = \"mongodb+srv://techasit14607:<db_password>@cluster1.yw0wz.mongodb.net/?retryWrites=true&w=majority&appName=Cluster1\"\n\n# Create a new client and connect to the server\nclient = MongoClient(uri, server_api=ServerApi('1'))\n\n# Send a ping to confirm a successful connection\ntry:\n    client.admin.command('ping')\n    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\nexcept Exception as e:\n    print(e)",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Hi, please open a separate issue stating your bug/problem. Thanks."
      }
    ]
  },
  {
    "issue_number": 5198,
    "title": "Failure to recognise python version in custom base_image",
    "author": "sjwalker223",
    "state": "open",
    "created_at": "2025-01-28T12:38:52Z",
    "updated_at": "2025-01-28T12:38:52Z",
    "labels": [],
    "body": "When running `bentoml containerize` with a custom base image, in previous versions of bentoml it was enough to have Python installed on this base image. With the recent move to uv as package manager, there are several related issues with this:\n\n1) The documentation states here (under `docker`):\nhttps://docs.bentoml.com/en/latest/reference/bentoml/bento-build-options.html\nthat the `base_image` parameter will override all other options. This is no longer the case (see example below): the `python_version` parameter is now not only not overriden, but actually required, when using a custom base image.\n\n2) It should be possible to use a custom base_image without the `python_version` parameter; in this case uv should use the default python version. In fact what happens is that the command `RUN uv venv -p None /app/.venv` is run, which inevitably fails due to `None`.\n\n3) Any custom base_image is now required to have uv installed. Ideally I think that uv should not be required, but should be installed as part of `bentoml containerize`, as happens when no base_image is provided.\n\nSteps to reproduce these issues:\n`bentofile.yaml`:\n```\nservice: \"service.py:MyService\"\nname: myservice\ndescription: |\n  mydescription\ninclude:\n  - 'service.py'\npython:\n  requirements_txt: \"requirements.txt\"\ndocker:\n  base_image: \"custom-base-image:latest\"\n```\n\nAnd `Dockerfile`:\n```\nFROM python:3.10-slim\n```\n\nThen run:\n```\ndocker build -t custom-base-image .\nbentoml build --version example .\nbentoml containerize myservice:example\n```\n\nThe containerize command should fail.\nThis can be fixed by adding the following:\nin bentofile.yaml `python_version: \"3.10\"`\nin Dockerfile:\n```\nUSER root\nRUN rm -f /etc/apt/apt.conf.d/docker-clean; echo 'Binary::apt::APT::Keep-Downloaded-Packages \"true\";' > /etc/apt/apt.conf.d/keep-cache\nRUN set -eux && apt-get update -y && apt-get install -q -y --no-install-recommends --allow-remove-essential ca-certificates gnupg2 bash build-essential curl\nRUN curl -LO https://astral.sh/uv/install.sh && sh install.sh && rm install.sh && mv $HOME/.local/bin/uv /usr/local/bin/\n```\n\nMy impression is that the user should not be expected to add these to their bentofile.yaml and Dockerfile; or at least, that they should be documented.\nThanks!",
    "comments": []
  },
  {
    "issue_number": 4928,
    "title": "enhancement: make timeout configurable for bento cloud pulls",
    "author": "holzweber",
    "state": "closed",
    "created_at": "2024-08-22T09:42:27Z",
    "updated_at": "2025-01-23T01:49:09Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nI try to load a bento with 50 models from yatai. I always run into timeouts.\r\nIf it would be possible to set the timeout configuration in the Client, the error would be solved:\r\nhttps://github.com/bentoml/BentoML/blob/e1a40cc67d5ad196458e916b03791448ce74a55e/src/bentoml/_internal/cloud/client.py#L854\n\n### To reproduce\n\nbentoml pull myBento:r6lk63tamwm7dit7 --verbose\n\n### Expected behavior\n\nDownload the bento without timeout\n\n### Environment\n\nbentoml: v1.3.2\r\nplatform: windows",
    "comments": [
      {
        "user": "frostming",
        "body": "Now you can customize the timeout by passing it to `BentoCloudClient`:\r\n\r\n\r\n```python\r\nimport bentoml\r\n\r\nclient = bentoml.cloud.BentoCloudClient(timeout=300)\r\nclient.bento.pull(\"myBento:r6lk63tamwm7dit7\")\r\n```"
      }
    ]
  },
  {
    "issue_number": 5174,
    "title": "bug: async initialization not supported",
    "author": "tmonty12",
    "state": "closed",
    "created_at": "2025-01-08T18:44:11Z",
    "updated_at": "2025-01-23T01:48:16Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nI am attempting to do some async initialization for my fastapi server using the FastAPI lifespan parameter: https://fastapi.tiangolo.com/advanced/events/. However, when mounting my FastAPI app to my bento service, the lifespan coroutine is not executed. \r\n\r\nFurthermore, bento does not seem to support async initialization in general. The `on_deployment` decorator does not support an async function and doesn't intake an instantiation of the class.\r\n\r\n### To reproduce\r\n\r\n```python\r\nimport asyncio\r\nimport bentoml\r\nfrom fastapi import FastAPI\r\nfrom pydantic import BaseModel\r\n\r\n\r\nasync def test_async_func():\r\n    print(\"test_async_func starting...\")\r\n    asyncio.sleep(1)\r\n    print(\"test_async_func completed...\")\r\n\r\n\r\nasync def lifespan(starting_app: FastAPI):\r\n    print(\"lifespan starting...\")\r\n    await test_async_func()\r\n\r\n    @starting_app.get(\"/health\")\r\n    def health():\r\n        return {\"status\": \"healthy\"}\r\n\r\n    print(\"lifespan completed...\")\r\n\r\n\r\napp = FastAPI(lifespan=lifespan)\r\n\r\n\r\nclass GenerateRequest(BaseModel):\r\n    payload: str\r\n\r\n\r\n@app.post(\"/generate\")\r\ndef generate(req: GenerateRequest):\r\n    return {\"result\": req.payload}\r\n\r\n\r\n@bentoml.asgi_app(app)\r\n@bentoml.service\r\nclass Service:\r\n\r\n    @bentoml.api\r\n    async def chat(self, payload: str):\r\n        return {\"result\": payload}\r\n\r\n```\r\n\r\nRun the server: `BENTOML_DEBUG=TRUE bentoml serve --working-dir ./src test-lifespan:Service --verbose`\r\n\r\n### My Output\r\n```\r\n(test-bento) (base) ➜  test-bento git:(master) ✗ BENTOML_DEBUG=TRUE bentoml serve --working-dir ./src test-lifespan:Service --verbose\r\n2025-01-08T10:44:53-0800 [WARNING] [cli] Converting 'Service' to lowercase: 'service'.\r\n2025-01-08T10:44:53-0800 [WARNING] [cli] Converting 'Service' to lowercase: 'service'.\r\n2025-01-08T10:44:53-0800 [DEBUG] [cli] GPU not detected. Unable to initialize pynvml lib.\r\n2025-01-08T10:44:53-0800 [INFO] [cli] Installing handle_callback_exception to loop\r\n2025-01-08T10:44:53-0800 [INFO] [cli] Registering signals...\r\n2025-01-08T10:44:53-0800 [INFO] [cli] Starting master on pid 88246\r\n2025-01-08T10:44:53-0800 [DEBUG] [cli] Socket bound at 0.0.0.0:3000 - fd: 3\r\n2025-01-08T10:44:53-0800 [INFO] [cli] sockets started\r\n2025-01-08T10:44:53-0800 [DEBUG] [cli] Initializing watchers\r\n2025-01-08T10:44:53-0800 [DEBUG] [cli] cmd: /Users/tmontfort/Brev/repos/test-bento/.venv/bin/python3\r\n2025-01-08T10:44:53-0800 [DEBUG] [cli] args: ['-m', '_bentoml_impl.worker.service', 'test-lifespan:Service', '--fd', '$(circus.sockets._bento_api_server)', '--service-name', 'Service', '--backlog', '2048', '--worker-id', '$(CIRCUS.WID)', '--ssl-version', '17', '--ssl-ciphers', 'TLSv1']\r\n2025-01-08T10:44:53-0800 [DEBUG] [cli] process args: ['/Users/tmontfort/Brev/repos/test-bento/.venv/bin/python3', '-m', '_bentoml_impl.worker.service', 'test-lifespan:Service', '--fd', '3', '--service-name', 'Service', '--backlog', '2048', '--worker-id', '1', '--ssl-version', '17', '--ssl-ciphers', 'TLSv1']\r\n2025-01-08T10:44:53-0800 [DEBUG] [cli] running service process [pid 88266]\r\n2025-01-08T10:44:53-0800 [INFO] [cli] Arbiter now waiting for commands\r\n2025-01-08T10:44:53-0800 [INFO] [cli] service started\r\n2025-01-08T10:44:53-0800 [INFO] [cli] Starting production HTTP BentoServer from \"test-lifespan:Service\" listening on http://localhost:3000 (Press CTRL+C to quit)\r\n2025-01-08T10:44:54-0800 [DEBUG] [entry_service:Service:1] 'tracing.sample_rate' is set to zero. No traces will be collected. Please refer to https://docs.bentoml.com/en/latest/guides/tracing.html for more details.\r\n2025-01-08T10:44:54-0800 [INFO] [entry_service:Service:1] Service Service initialized\r\n2025-01-08T10:44:54-0800 [DEBUG] [entry_service:Service:1] executing <function connect.<locals>.connector at 0x10914ee60>\r\n2025-01-08T10:44:54-0800 [DEBUG] [entry_service:Service:1] operation <function connect.<locals>.connector at 0x10914ee60> completed\r\n2025-01-08T10:44:54-0800 [DEBUG] [entry_service:Service:1] executing functools.partial(<built-in method execute of sqlite3.Connection object at 0x1088e9740>, 'CREATE TABLE IF NOT EXISTS result (\\n    task_id TEXT PRIMARY KEY,\\n    name TEXT,\\n    input BLOB,\\n    status TEXT,\\n    result BLOB,\\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n    completed_at TIMESTAMP DEFAULT NULL\\n)\\n', [])\r\n2025-01-08T10:44:54-0800 [DEBUG] [entry_service:Service:1] operation functools.partial(<built-in method execute of sqlite3.Connection object at 0x1088e9740>, 'CREATE TABLE IF NOT EXISTS result (\\n    task_id TEXT PRIMARY KEY,\\n    name TEXT,\\n    input BLOB,\\n    status TEXT,\\n    result BLOB,\\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n    completed_at TIMESTAMP DEFAULT NULL\\n)\\n', []) completed\r\n2025-01-08T10:44:54-0800 [DEBUG] [entry_service:Service:1] executing functools.partial(<built-in method commit of sqlite3.Connection object at 0x1088e9740>)\r\n2025-01-08T10:44:54-0800 [DEBUG] [entry_service:Service:1] operation functools.partial(<built-in method commit of sqlite3.Connection object at 0x1088e9740>) completed\r\n```\r\n\r\n```\r\n(test-bento) (base) ➜  test-bento git:(master) ✗ curl -X POST localhost:3000/generate -H \"Content-Type: application/json\" -d '{ \"payload\": \"test\" }'\r\n{\"result\":\"test\"}%                                                                                                \r\n(test-bento) (base) ➜  test-bento git:(master) ✗ curl -X POST localhost:3000/health -H \"Content-Type: application/json\" -d '{\"payload\": \"test\"}'  \r\n{\"detail\":\"Not Found\"}%\r\n```\r\n\r\n### Expected behavior\r\n\r\nI expect for the lifespan coroutine to execute. This would mean that I'd see the print statements from the `lifespan` and `test_async_func` and that the `health` endpoint would be exposed from my bento service.\r\n\r\n### Environment\r\n\r\n`python`: 3.10.15\r\n`bentoml`: 1.3.19",
    "comments": [
      {
        "user": "frostming",
        "body": "The FastAPI app is mounted as a subroute, so the app-level lifespan setting is not being executed.\r\n\r\nNow the official way to run service level startup hook is inside `__init__` method, which doesn't support async for now.\r\n\r\nIn fact, how to expose a user-friendly lifespan interface is still [a topic worth discussing](https://github.com/encode/starlette/issues/2845), and bentoml will also discuss a better API to allow users to customize the behavior of lifespan."
      },
      {
        "user": "tmonty12",
        "body": "> The FastAPI app is mounted as a subroute, so the app-level lifespan setting is not being executed.\r\n> \r\n> Now the official way to run service level startup hook is inside `__init__` method, which doesn't support async for now.\r\n> \r\n> In fact, how to expose a user-friendly lifespan interface is still [a topic worth discussing](https://github.com/encode/starlette/issues/2845), and bentoml will also discuss a better API to allow users to customize the behavior of lifespan.\r\n\r\nIt should be simple to add another decorator such as `@on_startup`  similar to `@on_deployment` that wraps a coroutine. Then on `ServiceAppFactory.create_instance` which is an async func, you can grab all of the coroutines on the inner class with the attr and run an `asyncio.gather` on them."
      }
    ]
  },
  {
    "issue_number": 5164,
    "title": "bug: pydantic version problem",
    "author": "gost-sniper",
    "state": "closed",
    "created_at": "2025-01-01T19:36:11Z",
    "updated_at": "2025-01-23T00:45:49Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\n```bash\r\nC:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\_bentoml_sdk\\_pydantic.py:198: UserWarning: Subclassing `GenerateSchema` is not supported. The API is highly subject to change in minor versions.\r\n  class BentoMLPydanticGenerateSchema(GenerateSchema):\r\nC:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\r\n  warnings.warn(warning.format(ret))\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\_bentoml_sdk\\io_models.py\", line 335, in from_input\r\n    create_model(\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\main.py\", line 1679, in create_model\r\n    return meta(\r\n           ^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py\", line 226, in __new__\r\n    complete_model_class(\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py\", line 658, in complete_model_class\r\n    schema = cls.__get_pydantic_core_schema__(cls, handler)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\main.py\", line 702, in __get_pydantic_core_schema__\r\n    return handler(source)\r\n           ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 84, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 610, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 879, in _generate_schema_inner\r\n    return self._model_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 691, in _model_schema\r\n    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1071, in _generate_md_field_schema\r\n    common_field = self._common_field_schema(name, field_info, decorators)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1263, in _common_field_schema\r\n    schema = self._apply_annotations(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2056, in _apply_annotations\r\n    schema = get_inner_schema(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 84, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2037, in inner_handler\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 884, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 995, in match_type\r\n    return self._unknown_type_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 513, in _unknown_type_schema\r\n    raise PydanticSchemaGenerationError(\r\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'PIL.Image.Image'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\r\n\r\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\r\n\r\nFor further information visit https://errors.pydantic.dev/2.10/u/schema-for-unknown-type\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Scripts\\bentoml.exe\\__main__.py\", line 7, in <module>\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\click\\core.py\", line 1161, in __call__\r\n    return self.main(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\click\\core.py\", line 1082, in main\r\n    rv = self.invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\click\\core.py\", line 1697, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\click\\core.py\", line 1443, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\click\\core.py\", line 788, in invoke\r\n    return __callback(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\bentoml_cli\\utils.py\", line 361, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\bentoml_cli\\utils.py\", line 332, in wrapper\r\n    return_value = func(*args, **kwargs)\r\n                   ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\bentoml_cli\\env_manager.py\", line 123, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\bentoml_cli\\serve.py\", line 261, in serve\r\n    svc = load(bento_identifier=bento, working_dir=working_dir)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\bentoml\\_internal\\service\\loader.py\", line 369, in load\r\n    _svc = import_1_2_service(_bento_identifier, _working_dir)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\_bentoml_impl\\loader.py\", line 170, in import_service\r\n    module = importlib.import_module(module_name)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\.pyenv\\pyenv-win\\versions\\3.12.5\\Lib\\importlib\\__init__.py\", line 90, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\main.py\", line 74, in <module>\r\n    class AgreggatedModels(BaseDeployableModel):\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\main.py\", line 134, in AgreggatedModels\r\n    @bentoml.api(route='/finder/perspective/image')\r\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\_bentoml_sdk\\decorators.py\", line 97, in wrapper\r\n    return APIMethod(func, **params)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<attrs generated init _bentoml_sdk.method.APIMethod>\", line 12, in __init__\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\_bentoml_sdk\\method.py\", line 93, in default_input_spec\r\n    return IODescriptor.from_input(self.func, skip_self=True)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\User1\\Desktop\\bentoml-vieweet\\venv\\Lib\\site-packages\\_bentoml_sdk\\io_models.py\", line 340, in from_input\r\n    raise TypeError(\r\nTypeError: Unable to infer the input spec for function <function AgreggatedModels.finder_perspective at 0x0000020DD6566F20>, please specify input_spec manually\r\n```\r\n\r\nthe problem is related to the Pydantic library version  `2.10.4` switching it to `2.9.2`  solves the problem\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nUsing Python 3.12.5 environment at: venv\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.3.8\r\n`python`: 3.12.5\r\n`platform`: Windows-11-10.0.26100-SP0\r\n`is_window_admin`: False\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\na2wsgi==1.10.7\r\nabsl-py==2.1.0\r\naccelerate==0.32.1\r\naddict==2.4.0\r\naiofiles==23.2.1\r\naiohappyeyeballs==2.4.4\r\naiohttp==3.11.11\r\naiosignal==1.3.2\r\naiosqlite==0.20.0\r\nalbumentations==0.5.2\r\naltair==5.5.0\r\nannotated-types==0.7.0\r\nantlr4-python3-runtime==4.9.3\r\nanyio==4.7.0\r\nappdirs==1.4.4\r\nargs==0.1.0\r\nasgiref==3.8.1\r\nastunparse==1.6.3\r\nattridict==0.0.8\r\nattrs==24.3.0\r\nbentoml==1.3.8\r\nbraceexpand==0.1.7\r\ncattrs==23.1.2\r\ncertifi==2024.12.14\r\ncffi==1.17.1\r\ncharset-normalizer==3.4.1\r\ncircus==0.18.0\r\nclick==8.1.8\r\nclick-option-group==0.5.6\r\nclint==0.5.1\r\nclip==0.2.0\r\ncloudpickle==3.1.0\r\ncolorama==0.4.6\r\ncoloredlogs==15.0.1\r\ncontourpy==1.3.1\r\ncycler==0.12.1\r\ndatasets==3.2.0\r\ndeepmerge==2.0\r\ndeprecated==1.2.15\r\n-e file:///C:/Users/Zyam/Desktop/bentoml-vieweet/diffuse/brushnet-original\r\ndill==0.3.8\r\ndistro==1.9.0\r\ndocopt==0.6.2\r\neasydict==1.13\r\neinops==0.8.0\r\nfairscale==0.4.13\r\nfastapi==0.115.6\r\nffmpy==0.5.0\r\nfilelock==3.16.1\r\nflatbuffers==24.12.23\r\nfonttools==4.55.3\r\nfrozenlist==1.5.0\r\nfs==2.4.16\r\nfsspec==2024.9.0\r\nftfy==6.3.1\r\ngast==0.6.0\r\ngoogle-pasta==0.2.0\r\ngputil==1.4.0\r\ngradio==3.41.2\r\ngradio-client==0.5.0\r\ngroundingdino-py==0.4.0\r\ngrpcio==1.68.1\r\nh11==0.14.0\r\nh5py==3.12.1\r\nhpsv2==1.2.0\r\nhttpcore==1.0.7\r\nhttpx==0.27.0\r\nhttpx-ws==0.7.0\r\nhuggingface-hub==0.25.2\r\nhumanfriendly==10.0\r\nhydra-core==1.3.2\r\nidna==3.10\r\nimage-reward==1.5\r\nimageio==2.36.1\r\nimgaug==0.4.0\r\nimportlib-metadata==6.11.0\r\nimportlib-resources==6.4.5\r\ninflection==0.5.1\r\niniconfig==2.0.0\r\niopath==0.1.10\r\njinja2==3.1.5\r\njoblib==1.4.2\r\njsonschema==4.23.0\r\njsonschema-specifications==2024.10.1\r\nkeras==3.7.0\r\nkiwisolver==1.4.8\r\nkornia==0.7.4\r\nkornia-rs==0.1.7\r\nlazy-loader==0.4\r\nlibclang==18.1.1\r\nlightning-utilities==0.11.9\r\nllvmlite==0.43.0\r\nloguru==0.7.3\r\nmarkdown==3.7\r\nmarkdown-it-py==3.0.0\r\nmarkupsafe==2.1.5\r\nmatplotlib==3.9.2\r\nmdurl==0.1.2\r\nml-dtypes==0.4.1\r\nmpmath==1.3.0\r\nmultidict==6.1.0\r\nmultiprocess==0.70.16\r\nnamex==0.0.8\r\nnarwhals==1.20.1\r\nnetworkx==3.4.2\r\nnumba==0.60.0\r\nnumpy==1.26.4\r\nnvidia-ml-py==11.525.150\r\nomegaconf==2.3.0\r\nonnxruntime==1.18.1\r\nopen-clip-torch==2.29.0\r\nopencv-contrib-python-headless==4.10.0.84\r\nopencv-python==4.10.0.84\r\nopencv-python-headless==4.10.0.84\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\nopt-einsum==3.4.0\r\noptree==0.13.1\r\norjson==3.10.13\r\npackaging==24.1\r\npandas==2.2.3\r\npathspec==0.12.1\r\npillow==10.4.0\r\npip==24.2\r\npip-requirements-parser==32.0.1\r\nplatformdirs==4.3.6\r\npluggy==1.5.0\r\npooch==1.8.2\r\nportalocker==3.1.1\r\nprometheus-client==0.21.1\r\nprompt-toolkit==3.0.48\r\npropcache==0.2.1\r\nprotobuf==3.20.3\r\npsutil==6.0.0\r\npy-cpuinfo==9.0.0\r\npyarrow==18.1.0\r\npycocotools==2.0.8\r\npycparser==2.22\r\npydantic==2.10.4\r\npydantic-core==2.27.2\r\npydub==0.25.1\r\npygit2==1.15.1\r\npygments==2.18.0\r\npylsd-nova==1.2.1\r\npymatting==1.1.13\r\npyparsing==3.2.1\r\npyreadline3==3.5.4\r\npytest==7.2.0\r\npytest-split==0.8.0\r\npython-dateutil==2.9.0.post0\r\npython-json-logger==3.2.1\r\npython-multipart==0.0.20\r\npytorch-lightning==2.3.3\r\n-e C:\\Users\\Zyam\\Desktop\\bentoml-vieweet\\pytorch3d\r\npytz==2024.2\r\npywin32==308\r\npyyaml==6.0.2\r\npyzmq==26.2.0\r\nquestionary==2.1.0\r\nreferencing==0.35.1\r\nregex==2024.11.6\r\nrembg==2.0.57\r\nrequests==2.32.3\r\nrich==13.9.4\r\nrpds-py==0.22.3\r\nruamel-base==1.0.0\r\nsafetensors==0.4.3\r\nschema==0.7.7\r\nscikit-image==0.25.0\r\nscikit-learn==1.6.0\r\nscipy==1.14.0\r\nseaborn==0.13.2\r\nsegment-anything==1.0\r\nsemantic-version==2.10.0\r\nsentencepiece==0.2.0\r\nsetuptools==75.6.0\r\nshapely==2.0.6\r\nsimple-di==0.1.5\r\nsix==1.17.0\r\nsniffio==1.3.1\r\nstarlette==0.41.3\r\nsupervision==0.6.0\r\nsympy==1.13.1\r\ntabulate==0.9.0\r\ntensorboard==2.18.0\r\ntensorboard-data-server==0.7.2\r\ntensorboardx==2.6.2.2\r\ntensorflow==2.18.0\r\ntensorflow-intel==2.18.0\r\ntermcolor==2.5.0\r\nthreadpoolctl==3.5.0\r\ntifffile==2024.12.12\r\ntimm==0.6.13\r\ntokenizers==0.19.1\r\ntomli-w==1.1.0\r\ntorch==2.5.1+cu121\r\ntorchmetrics==1.6.1\r\ntorchsde==0.2.6\r\ntorchvision==0.20.1+cu121\r\ntornado==6.4.2\r\ntqdm==4.67.1\r\ntrampoline==0.1.2\r\ntransformers==4.42.4\r\ntyping-extensions==4.12.2\r\ntzdata==2024.2\r\nultralytics==8.3.56\r\nultralytics-thop==2.0.13\r\nurllib3==2.3.0\r\nuv==0.5.13\r\nuvicorn==0.34.0\r\nwatchfiles==1.0.3\r\nwcwidth==0.2.13\r\nwebdataset==0.2.100\r\nwebsockets==11.0.3\r\nwerkzeug==3.1.3\r\nwheel==0.45.1\r\nwin32-setctime==1.2.0\r\nwldhx-yadisk-direct==0.0.6\r\nwrapt==1.17.0\r\nwsproto==1.2.0\r\nxxhash==3.5.0\r\nyacs==0.1.8\r\nyapf==0.43.0\r\nyarl==1.18.3\r\nzipp==3.21.0\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "frostming",
        "body": "It was fixed in the latest version of bentoml, upgrade it please."
      },
      {
        "user": "abayala",
        "body": "I just updated bentoml and I am having a similar issue, in my case I followed the example documentation https://docs.bentoml.com/en/latest/build-with-bentoml/iotypes.html#images \nI have an api function inside  a service which extract image embeddings, for example : \n```\nwith bentoml.importing():\n    from PIL import Image\n    from typing import List, Dict \n@bentoml.service\nclass Embedding:\n    def __init__(self):\n        self.image_model = None\n  \n   @bentoml.api\n    def encode_image(self, image: Image) -> np.ndarray:\n        return self.image_model.encode(image)\n```\nI get the similar error \n```\nTypeError: Unable to infer the input spec for function <function Embedding.encode_image at 0x0000026C5DF11120>, please specify input_spec manually\n```\nI have Pydantic 2.9.2, windows 10, python 3.10\n"
      },
      {
        "user": "frostming",
        "body": "@abayala What version of bentoml are you using? Make sure you have installed `pillow`"
      }
    ]
  },
  {
    "issue_number": 5187,
    "title": "docs: incorrect type of a parameter in docs",
    "author": "NitzanShwartz",
    "state": "closed",
    "created_at": "2025-01-18T10:58:44Z",
    "updated_at": "2025-01-22T05:42:41Z",
    "labels": [],
    "body": "I was reviewing the [documentation](https://docs.bentoml.com/en/latest/build-with-bentoml/iotypes.html) and noticed that the \"prompt\" parameter in the generate method is incorrectly defined as an int instead of a str. I have corrected this issue and submitted a pull request for the changes.",
    "comments": [
      {
        "user": "NitzanShwartz",
        "body": "https://github.com/bentoml/BentoML/pull/5188"
      }
    ]
  },
  {
    "issue_number": 5189,
    "title": "bug: I started a BentoML server, and the task_result.db keeps growing indefinitely. What could be the reason",
    "author": "yellow123Nike",
    "state": "closed",
    "created_at": "2025-01-22T02:27:08Z",
    "updated_at": "2025-01-22T04:44:51Z",
    "labels": [
      "bug",
      "feedback-wanted"
    ],
    "body": "### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/418c9276-3f16-44a9-a3e6-97b018310880)\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\n![Image](https://github.com/user-attachments/assets/43600459-3be4-4fbb-98a7-47ce78d0a16e)",
    "comments": [
      {
        "user": "frostming",
        "body": "Can you check what is stored in that file(using sqlite3) and what you run in your service code?"
      },
      {
        "user": "yellow123Nike",
        "body": "\"I have already deleted SQLite, and my service is an embedding service.\"\n\n![Image](https://github.com/user-attachments/assets/ce50b043-584a-4c4a-acf3-33e4c9c6515c)"
      },
      {
        "user": "frostming",
        "body": "So do you keep sending requests to this endpoint? The result will be stored in that file"
      }
    ]
  },
  {
    "issue_number": 5178,
    "title": "bug: python3.12-distutils no longer available.",
    "author": "jaume-ferrarons",
    "state": "closed",
    "created_at": "2025-01-10T06:29:09Z",
    "updated_at": "2025-01-10T08:18:33Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nFails to build the docker image when using containerize for debian cuda images when using python 3.12.\r\n\r\nIt fails at this Docker step with the following error:\r\n```bash\r\n > [base-container  5/15] RUN --mount=type=cache,target=/var/lib/apt --mount=type=cache,target=/var/cache/apt     set -eux &&     apt-get install -y --no-install-recommends --allow-remove-essential software-properties-common &&     add-apt-repository ppa:deadsnakes/ppa &&     apt-get update -y &&     apt-get install -y --no-install-recommends --allow-remove-essential python3.12 python3.12-dev python3.12-distutils:\r\n23.93 Hit:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease\r\n23.94 Hit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease\r\n24.05 Hit:6 http://security.ubuntu.com/ubuntu focal-security InRelease\r\n\r\n26.80 + apt-get install -y --no-install-recommends --allow-remove-essential python3.12 python3.12-dev python3.12-distutils\r\n26.85 Reading package lists...\r\n\r\n27.42 E: Unable to locate package python3.12-distutils\r\n27.42 E: Couldn't find any package by glob 'python3.12-distutils'\r\n27.42 E: Couldn't find any package by regex 'python3.12-distutils'\r\n```\r\n\r\n`python3.12-distutils` Was deprecated and now seems to be no longer available.\r\n\r\nRelated issues:\r\n- https://github.com/bentoml/BentoML/issues/4603\n\n### To reproduce\n\nUse a `bentofile.yaml` with the following docker section:\r\n\r\n```yaml\r\ndocker:\r\n  python_version: \"3.12\"\r\n  cuda_version: \"12.1.1\"\r\n```\r\n\r\nContainerize the bento after building it:\r\n```bash\r\nbentoml build\r\nbentoml containerize my_service_name:latest --opt platform=linux/amd64\r\n```\n\n### Expected behavior\n\nIt should build the image.\n\n### Environment\n\nbentoml: 1.3.19\r\npython: 3.12\r\nOS: MacOS/Linux",
    "comments": []
  },
  {
    "issue_number": 5168,
    "title": "bug: BentoML doesn't properly support lists of PIL Images for adaptive batching",
    "author": "slobodaapl",
    "state": "closed",
    "created_at": "2025-01-03T12:21:36Z",
    "updated_at": "2025-01-06T11:20:59Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWhen we use BentoML CLI with `bentoml server .` or explicitly run via `bentoml.serve()`, if we have a bento api method that is passed a list of PIL Images on input, BentoML is unable to infer the type of input, [even though the example in documentation clearly shows this should be possible](https://docs.bentoml.com/en/latest/build-with-bentoml/iotypes.html#compound)\n\n### To reproduce\n\nDefine simple BentoML service with the following api method:\r\n\r\n```python\r\n    @bentoml.api(\r\n        batchable=True,\r\n        batch_dim=0,\r\n    )\r\n    async def embed_image(self, images: list[PILImage]) -> list[list[float]]:\r\n        pass\r\n```\r\n\r\nThis will throw an error when we attempt to serve the app, being told the input specification cannot be inferred.\n\n### Expected behavior\n\nI'd expect the example from documentation to work 'out of the box' as such I can specify a list of PILImage on input\n\n### Environment\n\nbentoml: 1.3.18\r\npython: 3.11.9\r\npillow: 11.0.0\r\npydantic: 2.10.4",
    "comments": [
      {
        "user": "parano",
        "body": "@slobodaapl could you provide full code sample and error message?\r\n\r\nYou may refer to this example project where batchable image list is used as input: https://github.com/bentoml/BentoCLIP/blob/main/service.py#L27"
      },
      {
        "user": "slobodaapl",
        "body": "> @slobodaapl could you provide full code sample and error message?\r\n> \r\n> You may refer to this example project where batchable image list is used as input: https://github.com/bentoml/BentoCLIP/blob/main/service.py#L27\r\n\r\nOf course, here is an isolated runnable example @parano .. Removing `batchable` param doesn't make a difference either.  I made this simple example exactly following the example you provided in the link.\r\n\r\n```python\r\nfrom typing import List\r\n\r\nimport bentoml\r\nfrom PIL.Image import Image\r\n\r\n\r\n@bentoml.service(\r\n    name=\"test\",\r\n)\r\nclass APIService:\r\n\r\n    def __init__(self) -> None:\r\n        self.ctx = bentoml.Context()\r\n\r\n    @bentoml.api(route=\"/test_a\", batchable=True, batch_dim=(0, 0))\r\n    async def test_a(self, images: List[Image]) -> List[List[float]]:\r\n        return [[1, 2, 3], [4, 5, 6]]\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    bentoml.serve(APIService, host=\"0.0.0.0\", port=3000)\r\n```\r\n\r\nRunning this gives:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\_bentoml_sdk\\io_models.py\", line 369, in from_input\r\n    create_model(\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\main.py\", line 1679, in create_model\r\n    return meta(\r\n           ^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py\", line 226, in __new__\r\n    complete_model_class(\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py\", line 658, in complete_model_class\r\n    schema = cls.__get_pydantic_core_schema__(cls, handler)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\_bentoml_sdk\\io_models.py\", line 157, in __get_pydantic_core_schema__\r\n    return super().__get_pydantic_core_schema__(source, handler)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\main.py\", line 702, in __get_pydantic_core_schema__\r\n    return handler(source)\r\n           ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 84, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 610, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 879, in _generate_schema_inner\r\n    return self._model_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 691, in _model_schema\r\n    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 691, in <dictcomp>\r\n    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1071, in _generate_md_field_schema\r\n    common_field = self._common_field_schema(name, field_info, decorators)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1263, in _common_field_schema\r\n    schema = self._apply_annotations(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2056, in _apply_annotations\r\n    schema = get_inner_schema(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 84, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2037, in inner_handler\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 884, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 986, in match_type\r\n    return self._match_generic_type(obj, origin)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1018, in _match_generic_type\r\n    return self._list_schema(self._get_first_arg_or_any(obj))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 367, in _list_schema\r\n    return core_schema.list_schema(self.generate_schema(items_type))\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 610, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 884, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 995, in match_type\r\n    return self._unknown_type_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 513, in _unknown_type_schema\r\n    raise PydanticSchemaGenerationError(\r\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'PIL.Image.Image'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\r\n\r\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\r\n\r\nFor further information visit https://errors.pydantic.dev/2.10/u/schema-for-unknown-type\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\ai_api\\main.py\", line 10, in <module>\r\n    class APIService:\r\n  File \"c:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\ai_api\\main.py\", line 15, in APIService\r\n    @bentoml.api(route=\"/test_a\", batchable=True, batch_dim=(0, 0))\r\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\_bentoml_sdk\\decorators.py\", line 99, in wrapper\r\n    return APIMethod(func, **params)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<attrs generated init _bentoml_sdk.method.APIMethod>\", line 12, in __init__\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\_bentoml_sdk\\method.py\", line 116, in default_input_spec\r\n    return IODescriptor.from_input(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\[redacted]\\PycharmProjects\\[redacted]\\.venv\\Lib\\site-packages\\_bentoml_sdk\\io_models.py\", line 374, in from_input\r\n    raise TypeError(\r\nTypeError: Unable to infer the input spec for function <function APIService.test_a at 0x000001FD05AFA480>, please specify input_spec manually\r\n```"
      },
      {
        "user": "frostming",
        "body": "This has been fixed in [v1.3.19](https://github.com/bentoml/BentoML/releases/tag/v1.3.19), please upgrade and try"
      }
    ]
  },
  {
    "issue_number": 5163,
    "title": "bug: FastAPI responses misbehave when mounted as an ASGI app",
    "author": "slobodaapl",
    "state": "closed",
    "created_at": "2025-01-01T00:50:55Z",
    "updated_at": "2025-01-03T12:13:11Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nMounting an FastAPI ASGI app, regardless of whether its methods are defined outside of the BentoML Service class or inside, incorrectly propagates (or rather doesn't at all) returned values from GET calls. App is launched via BentoML CLI: `bentoml serve .` with a valid `bentofile.yaml`.\r\n\r\n### To reproduce\r\n\r\n```python\r\nfrom __future__ import annotations\r\n\r\nimport bentoml\r\nfrom fastapi import FastAPI\r\nfrom fastapi.responses import JSONResponse\r\nfrom pydantic import BaseModel\r\n\r\n\r\nclass ResponseModel(BaseModel):\r\n    message: str\r\n\r\n\r\napp = FastAPI()\r\n\r\n\r\n@bentoml.service(name=\"repro\")\r\n@bentoml.asgi_app(app)\r\nclass ReproService:\r\n\r\n    def __init__(self) -> None:\r\n        self.ctx = bentoml.Context()\r\n\r\n    @app.get(\"/example1\")\r\n    async def example(self):\r\n        return \"Hello\"\r\n\r\n    @app.get(\"/example2\")\r\n    async def example(self) :\r\n        return JSONResponse(status_code=200, content={\"message\": \"Hello\"})\r\n\r\n    @app.get(\"/example3\")\r\n    async def example(self):\r\n        return ResponseModel(\"Hello\")\r\n\r\n@app.get(\"/example4\")\r\nasync def example(service: ReproService = Depends(bentoml.get_current_service)):\r\n     return \"Hello\"\r\n\r\nif __name__ == \"__main__\":\r\n    bentoml.serve(APIService, host=\"0.0.0.0\", port=3000)\r\n\r\n```\r\n\r\nIn neither of examples 1 to 4 do we get the correct response. The response is always this:\r\n```\r\nResponse body: b\"\\n\"\r\n\r\nResponse headers:  content-length: 1 \r\n content-type: text/plain; charset=utf-8 \r\n date: Wed,01 Jan 2025 00:28:51 GMT \r\n```\r\n\r\n### Expected behavior\r\n\r\nI'd expect to get a proper `json` response or plaintext that matches my return.\r\n\r\n### Environment\r\n\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### Bento and Python version\r\n`bentoml`: 1.3.18\r\n`python`: 3.11.9\r\n\r\n#### pip_packages\r\n```\r\na2wsgi==1.10.7\r\naccelerate==1.2.1\r\naiohappyeyeballs==2.4.4\r\naiohttp==3.11.11\r\naiosignal==1.3.2\r\naiosqlite==0.20.0\r\nalabaster==0.7.16\r\nalembic==1.14.0\r\nannotated-types==0.7.0\r\nanyio==4.7.0\r\nappdirs==1.4.4\r\nasgiref==3.8.1\r\nasyncpg==0.30.0\r\nattrs==24.3.0\r\nbabel==2.16.0\r\nbandit==1.8.0\r\nbentoml==1.3.18\r\ncattrs==23.1.2\r\ncertifi==2024.12.14\r\ncfgv==3.4.0\r\ncharset-normalizer==3.4.1\r\nclick==8.1.8\r\nclick-option-group==0.5.6\r\ncloudpickle==3.1.0\r\ncolorama==0.4.6\r\ncoverage==7.6.10\r\ndeepmerge==2.0\r\ndeprecated==1.2.15\r\ndiffusers==0.32.1\r\ndistlib==0.3.9\r\ndocutils==0.20.1\r\ndynaconf==3.2.6\r\nfastapi==0.115.6\r\nfilelock==3.16.1\r\nfrozenlist==1.5.0\r\nfs==2.4.16\r\nfsspec==2024.12.0\r\ngoogleapis-common-protos==1.66.0\r\ngreenlet==3.1.1\r\ngrpcio==1.68.1\r\nh11==0.14.0\r\nhttpcore==1.0.7\r\nhttpx==0.28.1\r\nhttpx-ws==0.7.0\r\nhuggingface-hub==0.27.0\r\nidentify==2.6.4\r\nidna==3.10\r\nimagesize==1.4.1\r\nimportlib-metadata==8.5.0\r\ninflection==0.5.1\r\niniconfig==2.0.0\r\ninvisible-watermark==0.2.0\r\njinja2==3.1.5\r\njoblib==1.4.2\r\nkantoku==0.18.1\r\nloguru==0.7.3\r\nmako==1.3.8\r\nmarkdown-it-py==3.0.0\r\nmarkupsafe==3.0.2\r\nmdurl==0.1.2\r\nmpmath==1.3.0\r\nmultidict==6.1.0\r\nnetworkx==3.4.2\r\nnodeenv==1.9.1\r\nnumpy==2.2.1\r\nnvidia-ml-py==12.560.30\r\nopencv-python==4.10.0.84\r\nopentelemetry-api==1.29.0\r\nopentelemetry-exporter-otlp==1.29.0\r\nopentelemetry-exporter-otlp-proto-common==1.29.0\r\nopentelemetry-exporter-otlp-proto-grpc==1.29.0\r\nopentelemetry-exporter-otlp-proto-http==1.29.0\r\nopentelemetry-instrumentation==0.50b0\r\nopentelemetry-instrumentation-aiohttp-client==0.50b0\r\nopentelemetry-instrumentation-asgi==0.50b0\r\nopentelemetry-proto==1.29.0\r\nopentelemetry-sdk==1.29.0\r\nopentelemetry-semantic-conventions==0.50b0\r\nopentelemetry-util-http==0.50b0\r\npackaging==24.2\r\npathspec==0.12.1\r\npbr==6.1.0\r\npgvector==0.3.6\r\npillow==11.0.0\r\npip-requirements-parser==32.0.1\r\nplatformdirs==4.3.6\r\npluggy==1.5.0\r\npre-commit==3.8.0\r\nprometheus-client==0.21.1\r\nprompt-toolkit==3.0.48\r\npropcache==0.2.1\r\nprotobuf==5.29.2\r\npsutil==6.1.1\r\npsycopg2-binary==2.9.10\r\npydantic==2.10.4\r\npydantic-core==2.27.2\r\npygments==2.18.0\r\npyjwt==2.10.1\r\npyparsing==3.2.1\r\npytest==8.3.4\r\npytest-asyncio==0.25.0\r\npytest-cov==6.0.0\r\npytest-dependency==0.6.0\r\npytest-order==1.3.0\r\npytest-socket==0.7.0\r\npytest-timeout==2.3.1\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\npython-json-logger==3.2.1\r\npython-multipart==0.0.20\r\npywavelets==1.8.0\r\npyyaml==6.0.2\r\npyzmq==26.2.0\r\nquestionary==2.1.0\r\nregex==2024.11.6\r\nrequests==2.32.3\r\nrich==13.9.4\r\nsafetensors==0.4.5\r\nschema==0.7.7\r\nscikit-learn==1.6.0\r\nscipy==1.14.1\r\nsentence-transformers==3.3.1\r\nsetuptools==75.6.0\r\nsimple-di==0.1.5\r\nsix==1.17.0\r\nsniffio==1.3.1\r\nsnowballstemmer==2.2.0\r\nsphinx==7.4.7\r\nsphinx-autodoc-typehints==2.3.0\r\nsphinx-rtd-theme==2.0.0\r\nsphinxcontrib-applehelp==2.0.0\r\nsphinxcontrib-devhelp==2.0.0\r\nsphinxcontrib-htmlhelp==2.1.0\r\nsphinxcontrib-jquery==4.1\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==2.0.0\r\nsphinxcontrib-serializinghtml==2.0.0\r\nsqlalchemy==2.0.36\r\nstarlette==0.41.3\r\nstevedore==5.4.0\r\nsympy==1.13.1\r\nthreadpoolctl==3.5.0\r\ntokenizers==0.21.0\r\ntomli-w==1.1.0\r\ntorch==2.5.1+cu124\r\ntorchvision==0.20.1+cu124\r\ntornado==6.4.2\r\ntqdm==4.67.1\r\ntransformers==4.47.1\r\ntyping-extensions==4.12.2\r\nurllib3==2.3.0\r\nuv==0.5.13\r\nuvicorn==0.34.0\r\nvirtualenv==20.28.0\r\nwatchfiles==1.0.3\r\nwcwidth==0.2.13\r\nwin32-setctime==1.2.0\r\nwrapt==1.17.0\r\nwsproto==1.2.0\r\nyarl==1.18.3\r\nzipp==3.21.0\r\n```",
    "comments": [
      {
        "user": "frostming",
        "body": "I can't reproduce with bentoml 1.3.18, except for `/example3` which returns a 500 error because you instantiate the `ResponseModel` incorrectly, it should be `ResponseModel(message=\"Hello\")`\r\n\r\n\r\nPlease make sure you run the correct service and request the correct host and port.\r\n"
      }
    ]
  },
  {
    "issue_number": 3111,
    "title": "bug: auto-restart function for BentoML not working on a Windows Machine",
    "author": "DeleMike",
    "state": "open",
    "created_at": "2022-10-18T19:24:04Z",
    "updated_at": "2024-12-30T15:23:03Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nI am trying to enable auto-restart on Bento-ML development server and I am not seeing any server restart upon code change.\r\n\r\nI used this to start the service: `bentoml serve service.py:service --reload`\n\n### To reproduce\n\nI ran this `bentoml serve service.py:service --reload`  to test the auto-restart function\n\n### Expected behavior\n\nI expect it to restart server upon code change\n\n### Environment\n\nbentoml, version 1.0.7\r\npython 3.7\r\nWindows 10, hp",
    "comments": [
      {
        "user": "MuhammadAwon",
        "body": "Basically, I think this issue is occurring for all Windows OS. I am Win8.1, Python 3.9 user and having the same issue."
      },
      {
        "user": "ssheng",
        "body": "@MuhammadAwon could you please try without the `--reload` functionality? "
      },
      {
        "user": "MuhammadAwon",
        "body": "@ssheng  Basically it is same for me, if I run the command with or without `--reload`. I have to stop the server and rerun it after every time I am making changes in the `service.py`. "
      }
    ]
  },
  {
    "issue_number": 5141,
    "title": "bug: the @bentoml.task decorator's cancel functionality does not respond properly.",
    "author": "oogou11",
    "state": "closed",
    "created_at": "2024-12-20T06:45:30Z",
    "updated_at": "2024-12-25T12:21:19Z",
    "labels": [
      "bug"
    ],
    "body": "\r\n### Reproduction Code  \r\n\r\n```python\r\nimport bentoml\r\nimport typing as t\r\nimport time\r\n\r\n@bentoml.service(workers=1)\r\nclass ChunkTask:\r\n\r\n    @bentoml.task\r\n    def chunk_task(self) -> t.Dict:\r\n        time.sleep(10)\r\n        return {\"code\": \"Ok\"}\r\n```\r\n---\r\n\r\n### Observed Symptoms  \r\nWhen attempting to cancel a task, regardless of the task's current state:  \r\n- The application hangs for an extended period.  \r\n- Eventually, it returns a `500 Internal Server Error`.\r\n\r\n---\r\n\r\n### Initial Diagnosis  \r\nThe issue seems to be related to SQLite database operations in the task status management functionality. Specifically, the following method appears to hang when executing `self._conn.execute`:\r\n\r\n```python\r\nasync def set_status(self, task_id: str, status: ResultStatus) -> None:\r\n    await self._conn.execute(\r\n        \"UPDATE result SET status = ?, updated_at = ? WHERE task_id = ? AND status = ?\",\r\n        (\r\n            status.value,\r\n            datetime.datetime.now(tz=datetime.timezone.utc),\r\n            task_id,\r\n            ResultStatus.IN_PROGRESS.value,\r\n        ),\r\n    )\r\n    await self._conn.commit()\r\n```\r\n### Additional Notes  \r\nThe issue might stem from SQLite's locking behavior, particularly during concurrent updates or uncommitted transactions.\r\n\r\n### Environment\r\n\r\n- **Python Version**: `3.11.x`  \r\n- **BentoML Version**: `>=1.3.15`  \r\n- **Operating System**:  \r\n  - Windows  \r\n  - Ubuntu 22.04.5 LTS  ",
    "comments": [
      {
        "user": "frostming",
        "body": "We do not support interrupting a running task. The cancel endpoint only has effect on **pending** tasks(scheduled but not started yet)."
      }
    ]
  },
  {
    "issue_number": 4745,
    "title": "feature: support bentoml.depends.from_url(\"http://already.deployed-bento.com:3000\")",
    "author": "KimSoungRyoul",
    "state": "closed",
    "created_at": "2024-05-19T11:27:20Z",
    "updated_at": "2024-12-22T01:43:16Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\n~~~Python\r\n\r\n@bentoml.service(\r\n    traffic={\"timeout\": 600},\r\n    workers=8,\r\nresources={\"cpu\": \"1\"}\r\n)\r\nclass ControlNet:\r\n    # Pass the dependent Service class as an argument\r\n    # controlnet_service = bentoml.depends(SDXLControlNetService)\r\n    controlnet_service = bentoml.depends.from_url(\r\n       url= \"http://already.deployed-bento.com:3000\",\r\n       input =  Annotated[torch.Tensor, Shape((1, 4)), DType(\"float32\")],\r\n       output =  Annotated[torch.Tensor, Shape((1, 4)), DType(\"float32\")],\r\n    )  \r\n\r\n    @bentoml.api\r\n    async def generate(self, image: PIL_Image, params: Params) -> PIL_Image:\r\n\r\n\r\n~~~\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "Thanks, this will be included in 1.4, the next minor release"
      },
      {
        "user": "KimSoungRyoul",
        "body": "thx, this feature is implemented\r\n\r\nhttps://docs.bentoml.com/en/latest/build-with-bentoml/distributed-services.html#depend-on-an-external-deployment\r\n\r\n<img width=\"979\" alt=\"스크린샷 2024-12-22 오전 10 40 52\" src=\"https://github.com/user-attachments/assets/ca4165b2-5494-4218-9ae5-e6986a7814b8\" />\r\n"
      }
    ]
  },
  {
    "issue_number": 5113,
    "title": "bug: the server of new sdk is unresponsive when SSL is enabled ‼️",
    "author": "justachillguy1",
    "state": "closed",
    "created_at": "2024-12-06T16:45:09Z",
    "updated_at": "2024-12-20T02:38:36Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nWhen using the `bentoml.serve` context manager (as used in `tests/e2e/bento_new_sdk`), if the service is configured with SSL enabled, i.e., `@bentoml.service(ssl={\"enabled\": True})`, the `bentoml.serve` fails to start the server properly, resulting in a hang and unresponsive state.\r\n\r\nThe error message: `bentoml.exceptions.ServiceUnavailable: Server is not ready after 100 seconds`\r\n\r\n### To reproduce\r\n\r\nmake sure the requirements are installed.\r\n\r\n1. Configure the service of `examples/quickstart/service.py` to enable SSL. (directly edit the file)\r\n\r\n  ```python\r\n# file: examples/quickstart/service.py\r\nfrom __future__ import annotations  # I001\r\n\r\nimport bentoml\r\n\r\n@bentoml.service(ssl={\"enabled\": True}) # enable SSL\r\nclass Summarization:\r\n    def __init__(self) -> None:\r\n        import torch\r\n        from transformers import pipeline\r\n\r\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n        self.pipeline = pipeline(\"summarization\", device=device)\r\n\r\n    @bentoml.api(batchable=True)\r\n    def summarize(self, texts: list[str]) -> list[str]:\r\n        results = self.pipeline(texts)\r\n        return [item[\"summary_text\"] for item in results]\r\n```\r\n\r\n2. Use `bentoml.serve` context manager to start the server: create `test.py` in the root folder, then execute it.\r\n\r\n  ```python\r\n# file: test.py\r\nimport bentoml\r\n\r\nwith bentoml.serve(\r\n    \".\", working_dir=\"examples/quickstart\", port=5000\r\n) as server:\r\n    with bentoml.SyncHTTPClient(server.url, server_ready_timeout=100) as client:\r\n            result = client.summarize([\"test\"])[0]\r\n            print(result)\r\n```\r\n\r\n3. or just execute the command `pytest tests/e2e/bento_new_sdk`, to see the same results.\r\n\r\n### Expected behavior\r\n\r\nThe server is hanging and unresponsive, couldn't be ready.\r\n\r\n`bentoml.exceptions.ServiceUnavailable: Server is not ready after 100 seconds`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/test/test.py\", line 6, in <module>\r\n    with bentoml.SyncHTTPClient(server.url, server_ready_timeout=100) as client:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/miniconda3/lib/python3.12/site-packages/_bentoml_impl/client/__init__.py\", line 50, in __init__\r\n    super().__init__(\r\n  File \"/home/user/miniconda3/lib/python3.12/site-packages/_bentoml_impl/client/http.py\", line 181, in __init__\r\n    self.wait_until_server_ready(server_ready_timeout)\r\n  File \"/home/user/miniconda3/lib/python3.12/site-packages/_bentoml_impl/client/http.py\", line 293, in wait_until_server_ready\r\n    raise ServiceUnavailable(f\"Server is not ready after {timeout} seconds\")\r\nbentoml.exceptions.ServiceUnavailable: Server is not ready after 100 seconds\r\n```\r\n\r\nHowever, If SSL is disabled, this code works well.\r\n\r\n### Environment\r\n\r\nUsing Python 3.12.4 environment at: /home/user/miniconda3\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.3.15\r\n`python`: 3.12.4\r\n`platform`: Linux-6.8.0-49-generic-x86_64-with-glibc2.35\r\n`uid_gid`: 1004:1004\r\n`conda`: 24.7.1\r\n`in_conda_env`: True\r\n<details><summary><code>conda_packages</code></summary>\r\n\r\n<br>\r\n\r\n```yaml\r\nname: base\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=5.1=1_gnu\r\n  - anaconda-anon-usage=0.4.4=py312hfc0e8ea_100\r\n  - archspec=0.2.3=pyhd3eb1b0_0\r\n  - boltons=23.0.0=py312h06a4308_0\r\n  - brotli-python=1.0.9=py312h6a678d5_8\r\n  - bzip2=1.0.8=h5eee18b_6\r\n  - c-ares=1.19.1=h5eee18b_0\r\n  - ca-certificates=2024.7.2=h06a4308_0\r\n  - certifi=2024.7.4=py312h06a4308_0\r\n  - cffi=1.16.0=py312h5eee18b_1\r\n  - charset-normalizer=3.3.2=pyhd3eb1b0_0\r\n  - conda=24.7.1=py312h06a4308_0\r\n  - conda-content-trust=0.2.0=py312h06a4308_1\r\n  - conda-libmamba-solver=24.7.0=pyhd3eb1b0_0\r\n  - conda-package-handling=2.3.0=py312h06a4308_0\r\n  - conda-package-streaming=0.10.0=py312h06a4308_0\r\n  - cryptography=42.0.5=py312hdda0065_1\r\n  - distro=1.9.0=py312h06a4308_0\r\n  - expat=2.6.2=h6a678d5_0\r\n  - fmt=9.1.0=hdb19cb5_1\r\n  - frozendict=2.4.2=py312h06a4308_0\r\n  - icu=73.1=h6a678d5_0\r\n  - idna=3.7=py312h06a4308_0\r\n  - jsonpatch=1.33=py312h06a4308_1\r\n  - jsonpointer=2.1=pyhd3eb1b0_0\r\n  - krb5=1.20.1=h143b758_1\r\n  - ld_impl_linux-64=2.38=h1181459_1\r\n  - libarchive=3.6.2=hfab0078_4\r\n  - libcurl=8.7.1=h251f7ec_0\r\n  - libedit=3.1.20230828=h5eee18b_0\r\n  - libev=4.33=h7f8727e_1\r\n  - libffi=3.4.4=h6a678d5_1\r\n  - libgcc-ng=11.2.0=h1234567_1\r\n  - libgomp=11.2.0=h1234567_1\r\n  - libmamba=1.5.8=hfe524e5_2\r\n  - libmambapy=1.5.8=py312h2dafd23_2\r\n  - libnghttp2=1.57.0=h2d74bed_0\r\n  - libsolv=0.7.24=he621ea3_1\r\n  - libssh2=1.11.0=h251f7ec_0\r\n  - libstdcxx-ng=11.2.0=h1234567_1\r\n  - libuuid=1.41.5=h5eee18b_0\r\n  - libxml2=2.13.1=hfdd30dd_2\r\n  - lz4-c=1.9.4=h6a678d5_1\r\n  - menuinst=2.1.2=py312h06a4308_0\r\n  - ncurses=6.4=h6a678d5_0\r\n  - openssl=3.0.14=h5eee18b_0\r\n  - packaging=24.1=py312h06a4308_0\r\n  - pcre2=10.42=hebb0a14_1\r\n  - pip=24.2=py312h06a4308_0\r\n  - platformdirs=3.10.0=py312h06a4308_0\r\n  - pluggy=1.0.0=py312h06a4308_1\r\n  - pybind11-abi=5=hd3eb1b0_0\r\n  - pycosat=0.6.6=py312h5eee18b_1\r\n  - pycparser=2.21=pyhd3eb1b0_0\r\n  - pysocks=1.7.1=py312h06a4308_0\r\n  - python=3.12.4=h5148396_1\r\n  - readline=8.2=h5eee18b_0\r\n  - reproc=14.2.4=h6a678d5_2\r\n  - reproc-cpp=14.2.4=h6a678d5_2\r\n  - requests=2.32.3=py312h06a4308_0\r\n  - ruamel.yaml=0.17.21=py312h5eee18b_0\r\n  - setuptools=72.1.0=py312h06a4308_0\r\n  - sqlite=3.45.3=h5eee18b_0\r\n  - tk=8.6.14=h39e8969_0\r\n  - tqdm=4.66.4=py312he106c6f_0\r\n  - truststore=0.8.0=py312h06a4308_0\r\n  - tzdata=2024a=h04d1e81_0\r\n  - urllib3=2.2.2=py312h06a4308_0\r\n  - wheel=0.43.0=py312h06a4308_0\r\n  - xz=5.4.6=h5eee18b_1\r\n  - yaml-cpp=0.8.0=h6a678d5_1\r\n  - zlib=1.2.13=h5eee18b_1\r\n  - zstandard=0.22.0=py312h2c38b39_0\r\n  - zstd=1.5.5=hc292b87_2\r\n  - pip:\r\n      - a2wsgi==1.10.7\r\n      - aiohappyeyeballs==2.4.4\r\n      - aiohttp==3.11.10\r\n      - aiosignal==1.3.1\r\n      - aiosqlite==0.20.0\r\n      - annotated-types==0.7.0\r\n      - anyio==4.7.0\r\n      - appdirs==1.4.4\r\n      - asgiref==3.8.1\r\n      - attrs==24.2.0\r\n      - bentoml==1.3.15\r\n      - cattrs==23.1.2\r\n      - circus==0.18.0\r\n      - click==8.1.7\r\n      - click-option-group==0.5.6\r\n      - cloudpickle==3.1.0\r\n      - deepmerge==2.0\r\n      - deprecated==1.2.15\r\n      - filelock==3.16.1\r\n      - frozenlist==1.5.0\r\n      - fs==2.4.16\r\n      - fsspec==2024.10.0\r\n      - h11==0.14.0\r\n      - httpcore==1.0.7\r\n      - httpx==0.28.1\r\n      - httpx-ws==0.6.2\r\n      - huggingface-hub==0.26.3\r\n      - importlib-metadata==6.11.0\r\n      - inflection==0.5.1\r\n      - jinja2==3.1.4\r\n      - markdown-it-py==3.0.0\r\n      - markupsafe==3.0.2\r\n      - mdurl==0.1.2\r\n      - mpmath==1.3.0\r\n      - multidict==6.1.0\r\n      - networkx==3.4.2\r\n      - numpy==2.1.3\r\n      - nvidia-cublas-cu12==12.4.5.8\r\n      - nvidia-cuda-cupti-cu12==12.4.127\r\n      - nvidia-cuda-nvrtc-cu12==12.4.127\r\n      - nvidia-cuda-runtime-cu12==12.4.127\r\n      - nvidia-cudnn-cu12==9.1.0.70\r\n      - nvidia-cufft-cu12==11.2.1.3\r\n      - nvidia-curand-cu12==10.3.5.147\r\n      - nvidia-cusolver-cu12==11.6.1.9\r\n      - nvidia-cusparse-cu12==12.3.1.170\r\n      - nvidia-ml-py==12.560.30\r\n      - nvidia-nccl-cu12==2.21.5\r\n      - nvidia-nvjitlink-cu12==12.4.127\r\n      - nvidia-nvtx-cu12==12.4.127\r\n      - opentelemetry-api==1.20.0\r\n      - opentelemetry-instrumentation==0.41b0\r\n      - opentelemetry-instrumentation-aiohttp-client==0.41b0\r\n      - opentelemetry-instrumentation-asgi==0.41b0\r\n      - opentelemetry-sdk==1.20.0\r\n      - opentelemetry-semantic-conventions==0.41b0\r\n      - opentelemetry-util-http==0.41b0\r\n      - pathspec==0.12.1\r\n      - pip-requirements-parser==32.0.1\r\n      - prometheus-client==0.21.1\r\n      - prompt-toolkit==3.0.36\r\n      - propcache==0.2.1\r\n      - psutil==6.1.0\r\n      - pydantic==2.10.3\r\n      - pydantic-core==2.27.1\r\n      - pygments==2.18.0\r\n      - pyparsing==3.2.0\r\n      - python-dateutil==2.9.0.post0\r\n      - python-dotenv==1.0.1\r\n      - python-json-logger==2.0.7\r\n      - python-multipart==0.0.19\r\n      - pyyaml==6.0.2\r\n      - pyzmq==26.2.0\r\n      - questionary==2.0.1\r\n      - regex==2024.11.6\r\n      - rich==13.9.4\r\n      - safetensors==0.4.5\r\n      - schema==0.7.7\r\n      - simple-di==0.1.5\r\n      - six==1.17.0\r\n      - sniffio==1.3.1\r\n      - starlette==0.41.3\r\n      - sympy==1.13.1\r\n      - tokenizers==0.21.0\r\n      - tomli-w==1.1.0\r\n      - torch==2.5.1\r\n      - tornado==6.4.2\r\n      - transformers==4.47.0\r\n      - triton==3.1.0\r\n      - typing-extensions==4.12.2\r\n      - uv==0.5.6\r\n      - uvicorn==0.32.1\r\n      - watchfiles==1.0.0\r\n      - wcwidth==0.2.13\r\n      - wrapt==1.17.0\r\n      - wsproto==1.2.0\r\n      - yarl==1.18.3\r\n      - zipp==3.21.0\r\nprefix: /home/user/miniconda3\r\n```\r\n\r\n</details>\r\n\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\na2wsgi==1.10.7\r\naiohappyeyeballs==2.4.4\r\naiohttp==3.11.10\r\naiosignal==1.3.1\r\naiosqlite==0.20.0\r\nanaconda-anon-usage @ file:///croot/anaconda-anon-usage_1710965072196/work\r\nannotated-types==0.7.0\r\nanyio==4.7.0\r\nappdirs==1.4.4\r\narchspec @ file:///croot/archspec_1709217642129/work\r\nasgiref==3.8.1\r\nattrs==24.2.0\r\nbentoml==1.3.15\r\nboltons @ file:///work/perseverance-python-buildout/croot/boltons_1698851177130/work\r\nbrotli @ file:///croot/brotli-split_1714483155106/work\r\ncattrs==23.1.2\r\ncertifi @ file:///croot/certifi_1720453481653/work/certifi\r\ncffi @ file:///croot/cffi_1714483155441/work\r\ncharset-normalizer @ file:///croot/charset-normalizer_1721748349566/work\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==3.1.0\r\nconda @ file:///croot/conda_1722004606466/work\r\nconda-content-trust @ file:///croot/conda-content-trust_1714483159009/work\r\nconda-libmamba-solver @ file:///croot/conda-libmamba-solver_1721662679737/work/src\r\nconda-package-handling @ file:///croot/conda-package-handling_1718138267740/work\r\nconda-package-streaming @ file:///croot/conda-package-streaming_1718136078615/work\r\ncryptography @ file:///croot/cryptography_1714660666131/work\r\ndeepmerge==2.0\r\ndeprecated==1.2.15\r\ndistro @ file:///croot/distro_1714488253808/work\r\nfilelock==3.16.1\r\nfrozendict @ file:///croot/frozendict_1713194832637/work\r\nfrozenlist==1.5.0\r\nfs==2.4.16\r\nfsspec==2024.10.0\r\nh11==0.14.0\r\nhttpcore==1.0.7\r\nhttpx==0.28.1\r\nhttpx-ws==0.6.2\r\nhuggingface-hub==0.26.3\r\nidna @ file:///croot/idna_1714398848350/work\r\nimportlib-metadata==6.11.0\r\ninflection==0.5.1\r\njinja2==3.1.4\r\njsonpatch @ file:///croot/jsonpatch_1714483231291/work\r\njsonpointer==2.1\r\nlibmambapy @ file:///croot/mamba-split_1714483352891/work/libmambapy\r\nmarkdown-it-py==3.0.0\r\nmarkupsafe==3.0.2\r\nmdurl==0.1.2\r\nmenuinst @ file:///croot/menuinst_1723567589013/work\r\nmpmath==1.3.0\r\nmultidict==6.1.0\r\nnetworkx==3.4.2\r\nnumpy==2.1.3\r\nnvidia-cublas-cu12==12.4.5.8\r\nnvidia-cuda-cupti-cu12==12.4.127\r\nnvidia-cuda-nvrtc-cu12==12.4.127\r\nnvidia-cuda-runtime-cu12==12.4.127\r\nnvidia-cudnn-cu12==9.1.0.70\r\nnvidia-cufft-cu12==11.2.1.3\r\nnvidia-curand-cu12==10.3.5.147\r\nnvidia-cusolver-cu12==11.6.1.9\r\nnvidia-cusparse-cu12==12.3.1.170\r\nnvidia-ml-py==12.560.30\r\nnvidia-nccl-cu12==2.21.5\r\nnvidia-nvjitlink-cu12==12.4.127\r\nnvidia-nvtx-cu12==12.4.127\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\npackaging @ file:///croot/packaging_1720101850331/work\r\npathspec==0.12.1\r\npip @ file:///croot/pip_1723484598856/work\r\npip-requirements-parser==32.0.1\r\nplatformdirs @ file:///work/perseverance-python-buildout/croot/platformdirs_1701732573265/work\r\npluggy @ file:///work/perseverance-python-buildout/croot/pluggy_1698805497733/work\r\nprometheus-client==0.21.1\r\nprompt-toolkit==3.0.36\r\npropcache==0.2.1\r\npsutil==6.1.0\r\npycosat @ file:///croot/pycosat_1714510623388/work\r\npycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\r\npydantic==2.10.3\r\npydantic-core==2.27.1\r\npygments==2.18.0\r\npyparsing==3.2.0\r\npysocks @ file:///work/perseverance-python-buildout/croot/pysocks_1698845478203/work\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.19\r\npyyaml==6.0.2\r\npyzmq==26.2.0\r\nquestionary==2.0.1\r\nregex==2024.11.6\r\nrequests @ file:///croot/requests_1721410876868/work\r\nrich==13.9.4\r\nruamel-yaml @ file:///work/perseverance-python-buildout/croot/ruamel.yaml_1698863605521/work\r\nsafetensors==0.4.5\r\nschema==0.7.7\r\nsetuptools==72.1.0\r\nsimple-di==0.1.5\r\nsix==1.17.0\r\nsniffio==1.3.1\r\nstarlette==0.41.3\r\nsympy==1.13.1\r\ntokenizers==0.21.0\r\ntomli-w==1.1.0\r\ntorch==2.5.1\r\ntornado==6.4.2\r\ntqdm @ file:///croot/tqdm_1716395931952/work\r\ntransformers==4.47.0\r\ntriton==3.1.0\r\ntruststore @ file:///work/perseverance-python-buildout/croot/truststore_1701735771625/work\r\ntyping-extensions==4.12.2\r\nurllib3 @ file:///croot/urllib3_1718912636303/work\r\nuv==0.5.6\r\nuvicorn==0.32.1\r\nwatchfiles==1.0.0\r\nwcwidth==0.2.13\r\nwheel==0.43.0\r\nwrapt==1.17.0\r\nwsproto==1.2.0\r\nyarl==1.18.3\r\nzipp==3.21.0\r\nzstandard @ file:///croot/zstandard_1714677652653/work\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "frostming",
        "body": "You must at least provide the keyfile and certfile as well:\r\n\r\n```python\r\n@bentoml.service(\r\n    ssl={\"enabled\": True, \"keyfile\": \"private.key\", \"certfile\": \"cert.pem\"}\r\n)\r\n```\r\n\r\nAlso if the key pairs are self-signed, turn on the insecure mode on the client. But since the bentoml built-in client doesn't support insecure mode, you will need to use your own client instead."
      }
    ]
  },
  {
    "issue_number": 4970,
    "title": "bug: Website Quickstart Instructions",
    "author": "bardicreels",
    "state": "closed",
    "created_at": "2024-09-11T11:56:48Z",
    "updated_at": "2024-12-19T12:54:09Z",
    "labels": [
      "documentation"
    ],
    "body": "### Describe the bug\r\n\r\nIt is not clear in the instructions that deploying from local (downloading torch etc.) is not necessary. \r\nThat by simply creating the service.py  bentofile.yaml and building it is all you need if using cloud. \r\nFor Users with limited local resources, clarifying this might be a huge selling point. \r\nI had to go looking for the build step after getting a config error. \r\n",
    "comments": [
      {
        "user": "frostming",
        "body": "/cc @Sherlock113 if it is worth doing or feel free to close it."
      },
      {
        "user": "Sherlock113",
        "body": "Updated the quickstart and added a note in the setup section to tell users to go to the cloud deployment doc if they don't want to do local serving/deployment: https://docs.bentoml.com/en/latest/get-started/hello-world.html#set-up-the-environment\r\n\r\nFeel free to reopen this if not clear."
      }
    ]
  },
  {
    "issue_number": 5056,
    "title": "bug: Only the Last Declared Histogram is Collected When Using Multiple Histograms",
    "author": "takhyun12",
    "state": "open",
    "created_at": "2024-11-01T00:38:03Z",
    "updated_at": "2024-12-19T08:08:47Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nI'm experiencing an issue where only the last declared Histogram metric is being collected when using multiple histograms in my BentoML service. The previously declared histograms do not collect any data, even though they are being called in the code.\r\n\r\n\n\n### To reproduce\n\nSteps to Reproduce:\r\n\r\nHere is a minimal reproducible example:\r\n\r\n```python\r\nfrom bentoml.metrics import Counter, Histogram\r\n\r\ntest_runner = bentoml.Runner(TestRunnable, name=\"test_runner\")\r\nservice = bentoml.Service(name=\"test_service\", runners=[test_runner, (...)])\r\n\r\nREQUEST_COUNT = Counter(name=\"request_count\", documentation=\"Total number of requests\")\r\nREQUEST_LATENCY = Histogram(name=\"request_latency_seconds\", documentation=\"Request latency in seconds\")\r\nREQUEST_IMAGE_WIDTH = Histogram(name=\"request_image_width\", documentation=\"Width of input images in pixels\")\r\nREQUEST_IMAGE_HEIGHT = Histogram(name=\"request_image_height\", documentation=\"Height of input images in pixels\")\r\n\r\n\r\nasync def api(data: BytesIO):\r\n    REQUEST_COUNT.inc()\r\n    with REQUEST_LATENCY.time():\r\n        image = PILImage.open(BytesIO(data.read())).convert(\"RGB\")\r\n        image = exif_transpose(image=image)\r\n\r\n        original_size = image.size\r\n        REQUEST_IMAGE_WIDTH.observe(original_size[0])\r\n        REQUEST_IMAGE_HEIGHT.observe(original_size[1])\r\n```\r\n\r\nWhen I run the service and make requests, only the last declared histogram (REQUEST_IMAGE_HEIGHT) collects data.\r\nThe other histograms (REQUEST_LATENCY and REQUEST_IMAGE_WIDTH) do not show any collected data in the metrics endpoint.\r\n\r\nIf I declare only one histogram (regardless of which one), it collects data as expected.\r\nDeclaring two or more histograms results in only the last one collecting data.\r\n\r\nI have tried changing the order of the histogram declarations, and it's always the last one that collects data.\r\nNo errors or warnings are logged during runtime.\r\n\r\nCould you please help investigate this issue? Is there a known limitation or a specific way to declare multiple histograms in BentoML so that they all collect data correctly?\r\n\r\nThank you for your assistance.\n\n### Expected behavior\n\nAll declared Histogram metrics should collect and display data when their observe() method is called, regardless of the number of histograms declared.\r\n\n\n### Environment\n\nbentoml: 1.2.16\r\nPython: 3.10\r\nOS: Ubuntu 20.04.6 LTS",
    "comments": [
      {
        "user": "frostming",
        "body": "Can you upgrade to the latest bentoml version and see if the issue exists? Reporting against an old version is not helpful"
      },
      {
        "user": "takhyun12",
        "body": "@frostming \r\n\r\nI've upgraded to the latest version of BentoML (1.3.10), but I'm still encountering the same issue. "
      },
      {
        "user": "frostming",
        "body": "Run this code on the latest BentoML and it reports all metrics correctly:\r\n\r\n```python\r\nfrom pathlib import Path\r\n\r\nimport bentoml\r\nimport PIL.Image as PILImage\r\nfrom bentoml.metrics import Counter, Histogram\r\n\r\nREQUEST_COUNT = Counter(name=\"request_count\", documentation=\"Total number of requests\")\r\nREQUEST_LATENCY = Histogram(\r\n    name=\"request_latency_seconds\", documentation=\"Request latency in seconds\"\r\n)\r\nREQUEST_IMAGE_WIDTH = Histogram(\r\n    name=\"request_image_width\", documentation=\"Width of input images in pixels\"\r\n)\r\nREQUEST_IMAGE_HEIGHT = Histogram(\r\n    name=\"request_image_height\", documentation=\"Height of input images in pixels\"\r\n)\r\n\r\n\r\n@bentoml.service\r\nclass MyService:\r\n    @bentoml.api\r\n    def generate(self, src_image: Path) -> Path:\r\n        REQUEST_COUNT.inc()\r\n        with REQUEST_LATENCY.time():\r\n            image = PILImage.open(src_image).convert(\"RGB\")\r\n\r\n            original_size = image.size\r\n            REQUEST_IMAGE_WIDTH.observe(original_size[0])\r\n            REQUEST_IMAGE_HEIGHT.observe(original_size[1])\r\n        return src_image\r\n```"
      }
    ]
  },
  {
    "issue_number": 5132,
    "title": "bug: Can't save bento with custom transformers pipeline",
    "author": "beatanyari",
    "state": "open",
    "created_at": "2024-12-16T13:04:32Z",
    "updated_at": "2024-12-17T12:29:46Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nThe support for custom transformers pipeline seems to be broken, I believe introduced by [this commit](https://github.com/bentoml/BentoML/commit/bdaa41b9d35835d862d3725ca444dbd521d0ccaf) When trying to save a custom pipeline, I'm getting this error:\r\n\r\n```\r\nBentoMLException: Argument 'pipeline' is not an instance of <class '__main__.MyPipeline'>. It is an instance of <class '__main__.MyPipeline'>.\r\n```\r\n\r\n### To reproduce\r\n\r\n```\r\nfrom transformers import pipeline\r\nfrom transformers import Pipeline\r\nfrom transformers import AutoTokenizer\r\nfrom transformers import AutoModelForSequenceClassification\r\nfrom transformers.pipelines import SUPPORTED_TASKS\r\n\r\nimport typing as t\r\n\r\nimport bentoml\r\n\r\n\r\nclass MyPipeline(Pipeline):\r\n    def _sanitize_parameters(self, **kwargs):\r\n        preprocess_kwargs = {}\r\n        if \"maybe_arg\" in kwargs:\r\n            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\r\n        return preprocess_kwargs, {}, {}\r\n\r\n    def preprocess(self, text, maybe_arg=2):\r\n        input_ids = self.tokenizer(text, return_tensors=\"pt\")\r\n        return input_ids\r\n\r\n    def _forward(self, model_inputs):\r\n        outputs = self.model(**model_inputs)\r\n        return outputs\r\n\r\n    def postprocess(self, model_outputs):\r\n        return model_outputs[\"logits\"].softmax(-1).numpy()\r\n\r\n\r\nTASK_NAME: str = \"my-classification-task\"\r\nTASK_DEFINITION: t.Dict[str, t.Any] = {\r\n    \"impl\": MyPipeline,\r\n    \"tf\": (),\r\n    \"pt\": (AutoModelForSequenceClassification,),\r\n    \"default\": {},\r\n    \"type\": \"text\",\r\n}\r\nSUPPORTED_TASKS[TASK_NAME] = TASK_DEFINITION\r\n\r\npipe = pipeline(\r\n    task=TASK_NAME,\r\n    model=AutoModelForSequenceClassification.from_pretrained(\r\n        \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n    ),\r\n    tokenizer=AutoTokenizer.from_pretrained(\r\n        \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n    ),\r\n)\r\n\r\nsaved_pipe = bentoml.transformers.save_model(\r\n    \"my_classification_model\",\r\n    pipeline=pipe,\r\n    task_name=TASK_NAME,\r\n    task_definition=TASK_DEFINITION,\r\n)\r\n```\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nbentoml: 1.3.16",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Did you register your pipeline class with huggingface?\r\n\r\nCan you send a small reproducer here?"
      }
    ]
  },
  {
    "issue_number": 5094,
    "title": "bug: Not able to deploy crewai app to BentoCloud",
    "author": "ambareeshav",
    "state": "closed",
    "created_at": "2024-11-21T09:08:27Z",
    "updated_at": "2024-12-17T08:39:43Z",
    "labels": [
      "bug",
      "feedback-wanted"
    ],
    "body": "### Describe the bug\r\n\r\nFollowing this [blog post](https://bentoml.com/blog/building-a-multi-agent-system-with-crewai-and-bentoml) to deploy the crewai service to BentoCloud.\r\nThe service is running and working locally using:\r\n`bentoml serve .`\r\nBut when trying to deploy with:\r\n`bentoml deploy . --secret openai -n crewai-demo --scaling-min 0 --scaling-max 3`\r\nI am persistently running into this error:\r\nhttps://pastebin.com/ygYa7AzD\r\n\r\nEDIT - updated Bentoml to 1.3.14, error still persists\r\n\r\n\r\n\r\n### To reproduce\r\n\r\n1. Follow the steps as given in this [blog post](https://bentoml.com/blog/building-a-multi-agent-system-with-crewai-and-bentoml)\r\n2. Deploy to BentoCloud using the default GPT model option.:\r\n`bentoml deploy . --secret openai -n crewai-demo --scaling-min 0 --scaling-max 3`\r\n\r\n\r\n### Expected behavior\r\n\r\nCrewAI service to be deployed to BentoCloud\r\n\r\n### Environment\r\n\r\nUsing Python 3.11.0 environment at venv\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.3.14\r\n`python`: 3.11.0\r\n`platform`: Windows-10-10.0.19045-SP0\r\n`is_window_admin`: True\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\na2wsgi==1.10.7\r\naiofiles==24.1.0\r\naiohappyeyeballs==2.4.3\r\naiohttp==3.10.9\r\naiosignal==1.3.1\r\naiosqlite==0.20.0\r\nalembic==1.13.3\r\naniso8601==9.0.1\r\nannotated-types==0.7.0\r\nanyio==4.6.0\r\nappdirs==1.4.4\r\nasgiref==3.8.1\r\nasttokens==2.4.1\r\nasync-timeout==4.0.3\r\nattrs==23.2.0\r\nauth0-python==4.7.2\r\nbackoff==2.2.1\r\nbcrypt==4.2.0\r\nbeautifulsoup4==4.12.3\r\nbentoml==1.3.14\r\nblinker==1.8.2\r\nboto3==1.35.34\r\nbotocore==1.35.34\r\nbuild==1.2.2.post1\r\ncachecontrol==0.14.0\r\ncachetools==5.5.0\r\ncattrs==23.1.2\r\ncertifi==2024.8.30\r\ncffi==1.17.1\r\ncharset-normalizer==3.3.2\r\nchroma-hnswlib==0.7.3\r\nchromadb==0.4.24\r\ncircus==0.18.0\r\ncleo==2.1.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==2.2.1\r\ncohere==5.11.0\r\ncolorama==0.4.6\r\ncoloredlogs==15.0.1\r\ncomposio-core==0.5.44\r\ncomposio-crewai==0.5.44\r\ncomposio-langchain==0.5.44\r\ncontourpy==1.3.0\r\ncrashtest==0.4.1\r\ncrewai==0.80.0\r\ncrewai-tools==0.14.0\r\ncryptography==43.0.1\r\ncycler==0.12.1\r\ndatabricks-sdk==0.33.0\r\ndataclasses-json==0.6.7\r\ndecorator==5.1.1\r\ndeepmerge==2.0\r\ndeprecated==1.2.14\r\ndeprecation==2.1.0\r\ndill==0.3.9\r\ndistlib==0.3.8\r\ndistro==1.9.0\r\ndocker==7.1.0\r\ndocstring-parser==0.16\r\ndocx2txt==0.8\r\ndulwich==0.21.7\r\ndurationpy==0.9\r\nembedchain==0.1.124\r\nexceptiongroup==1.2.2\r\nexecuting==2.1.0\r\nfastapi==0.115.0\r\nfastavro==1.9.7\r\nfastjsonschema==2.20.0\r\nfilelock==3.16.1\r\nflask==3.0.3\r\nflatbuffers==24.3.25\r\nfonttools==4.54.1\r\nfrozenlist==1.4.1\r\nfs==2.4.16\r\nfsspec==2024.9.0\r\ngitdb==4.0.11\r\ngitpython==3.1.43\r\ngoogle-api-core==2.20.0\r\ngoogle-auth==2.35.0\r\ngoogle-cloud-aiplatform==1.69.0\r\ngoogle-cloud-bigquery==3.26.0\r\ngoogle-cloud-core==2.4.1\r\ngoogle-cloud-resource-manager==1.12.5\r\ngoogle-cloud-storage==2.18.2\r\ngoogle-crc32c==1.6.0\r\ngoogle-pasta==0.2.0\r\ngoogle-resumable-media==2.7.2\r\ngoogleapis-common-protos==1.65.0\r\ngptcache==0.1.44\r\ngraphene==3.3\r\ngraphql-core==3.2.4\r\ngraphql-relay==3.2.0\r\ngreenlet==3.1.1\r\ngrpc-google-iam-v1==0.13.1\r\ngrpcio==1.66.2\r\ngrpcio-status==1.62.3\r\ngrpcio-tools==1.62.3\r\ngunicorn==23.0.0\r\nh11==0.14.0\r\nh2==4.1.0\r\nhpack==4.0.0\r\nhttpcore==1.0.6\r\nhttptools==0.6.1\r\nhttpx==0.27.2\r\nhttpx-sse==0.4.0\r\nhttpx-ws==0.6.1\r\nhuggingface-hub==0.25.1\r\nhumanfriendly==10.0\r\nhyperframe==6.0.1\r\nidna==3.10\r\nimportlib-metadata==6.11.0\r\nimportlib-resources==6.4.5\r\ninflection==0.5.1\r\niniconfig==2.0.0\r\ninstaller==0.7.0\r\ninstructor==1.3.3\r\nipython==8.28.0\r\nitsdangerous==2.2.0\r\njaraco-classes==3.4.0\r\njedi==0.19.1\r\njinja2==3.1.4\r\njiter==0.4.2\r\njmespath==1.0.1\r\njoblib==1.4.2\r\njson-repair==0.25.3\r\njsonpatch==1.33\r\njsonpickle==3.3.0\r\njsonpointer==3.0.0\r\njsonref==1.1.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2023.12.1\r\nkeyring==24.3.1\r\nkiwisolver==1.4.7\r\nkubernetes==31.0.0\r\nlancedb==0.5.7\r\nlangchain==0.3.7\r\nlangchain-cohere==0.3.2\r\nlangchain-community==0.3.7\r\nlangchain-core==0.3.19\r\nlangchain-experimental==0.3.3\r\nlangchain-openai==0.2.9\r\nlangchain-text-splitters==0.3.2\r\nlangchainhub==0.1.21\r\nlangsmith==0.1.131\r\nlitellm==1.48.17\r\nmako==1.3.5\r\nmarkdown==3.7\r\nmarkdown-it-py==3.0.0\r\nmarkupsafe==2.1.5\r\nmarshmallow==3.22.0\r\nmatplotlib==3.9.2\r\nmatplotlib-inline==0.1.7\r\nmdurl==0.1.2\r\nmem0ai==0.1.30\r\nmlflow==2.16.2\r\nmlflow-skinny==2.16.2\r\nmmh3==5.0.1\r\nmock==4.0.3\r\nmonotonic==1.6\r\nmore-itertools==10.5.0\r\nmpmath==1.3.0\r\nmsgpack==1.1.0\r\nmultidict==6.1.0\r\nmultiprocess==0.70.17\r\nmypy-extensions==1.0.0\r\nneo4j==5.25.0\r\nnetworkx==3.3\r\nnodeenv==1.9.1\r\nnumpy==1.26.4\r\nnvidia-ml-py==11.525.150\r\noauthlib==3.2.2\r\nonnxruntime==1.19.2\r\nopenai==1.55.0\r\nopentelemetry-api==1.27.0\r\nopentelemetry-exporter-otlp-proto-common==1.27.0\r\nopentelemetry-exporter-otlp-proto-grpc==1.27.0\r\nopentelemetry-exporter-otlp-proto-http==1.27.0\r\nopentelemetry-instrumentation==0.48b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.48b0\r\nopentelemetry-instrumentation-fastapi==0.48b0\r\nopentelemetry-proto==1.27.0\r\nopentelemetry-sdk==1.27.0\r\nopentelemetry-semantic-conventions==0.48b0\r\nopentelemetry-util-http==0.48b0\r\norjson==3.10.7\r\noutcome==1.3.0.post0\r\noverrides==7.7.0\r\npackaging==24.1\r\npandas==2.2.3\r\nparameterized==0.9.0\r\nparamiko==3.5.0\r\nparso==0.8.4\r\npathos==0.3.3\r\npathspec==0.12.1\r\npexpect==4.9.0\r\npillow==10.4.0\r\npip==22.3\r\npip-requirements-parser==32.0.1\r\npkginfo==1.11.1\r\nplatformdirs==4.3.6\r\npluggy==1.5.0\r\npoetry==1.8.3\r\npoetry-core==1.9.0\r\npoetry-plugin-export==1.8.0\r\nportalocker==2.10.1\r\nposthog==3.7.0\r\npox==0.3.5\r\nppft==1.7.6.9\r\nprometheus-client==0.21.0\r\nprompt-toolkit==3.0.48\r\nproto-plus==1.24.0\r\nprotobuf==4.25.5\r\npsutil==6.0.0\r\nptyprocess==0.7.0\r\npulsar-client==3.5.0\r\npure-eval==0.2.3\r\npy==1.11.0\r\npyarrow==17.0.0\r\npyasn1==0.6.1\r\npyasn1-modules==0.4.1\r\npycparser==2.22\r\npydantic==2.9.2\r\npydantic-core==2.23.4\r\npydantic-settings==2.6.1\r\npygments==2.18.0\r\npyjwt==2.9.0\r\npylance==0.9.18\r\npynacl==1.5.0\r\npyparsing==3.1.4\r\npypdf==5.1.0\r\npyperclip==1.9.0\r\npypika==0.48.9\r\npyproject-hooks==1.2.0\r\npyreadline3==3.5.4\r\npyright==1.1.383\r\npysbd==0.3.4\r\npysher==1.0.8\r\npysocks==1.7.1\r\npytest==8.3.3\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.12\r\npytube==15.0.0\r\npytz==2024.2\r\npyvis==0.3.2\r\npywin32==308\r\npywin32-ctypes==0.2.3\r\npyyaml==6.0.2\r\npyzmq==26.2.0\r\nqdrant-client==1.11.3\r\nquestionary==2.0.1\r\nrank-bm25==0.2.2\r\nrapidfuzz==3.10.0\r\nratelimiter==1.2.0.post0\r\nreferencing==0.35.1\r\nregex==2024.9.11\r\nrequests==2.32.3\r\nrequests-oauthlib==2.0.0\r\nrequests-toolbelt==1.0.0\r\nretry==0.9.2\r\nrich==13.9.2\r\nrpds-py==0.20.0\r\nrsa==4.9\r\ns3transfer==0.10.2\r\nsagemaker==2.232.2\r\nsagemaker-core==1.0.10\r\nsagemaker-mlflow==0.1.0\r\nschema==0.7.7\r\nscikit-learn==1.5.2\r\nscipy==1.14.1\r\nselenium==4.25.0\r\nsemver==3.0.2\r\nsentry-sdk==2.18.0\r\nsetuptools==65.5.0\r\nshapely==2.0.6\r\nshellingham==1.5.4\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsmdebug-rulesconfig==1.0.1\r\nsmmap==5.0.1\r\nsniffio==1.3.1\r\nsortedcontainers==2.4.0\r\nsoupsieve==2.6\r\nsqlalchemy==2.0.35\r\nsqlparse==0.5.1\r\nstack-data==0.6.3\r\nstarlette==0.38.6\r\nsympy==1.13.3\r\ntabulate==0.9.0\r\ntblib==3.0.0\r\ntenacity==8.5.0\r\nthreadpoolctl==3.5.0\r\ntiktoken==0.7.0\r\ntokenizers==0.20.0\r\ntomli==2.0.2\r\ntomli-w==1.1.0\r\ntomlkit==0.13.2\r\ntornado==6.4.1\r\ntqdm==4.66.5\r\ntraitlets==5.14.3\r\ntrio==0.26.2\r\ntrio-websocket==0.11.1\r\ntrove-classifiers==2024.9.12\r\ntyper==0.12.5\r\ntypes-requests==2.32.0.20240914\r\ntyping-extensions==4.12.2\r\ntyping-inspect==0.9.0\r\ntzdata==2024.2\r\nurllib3==2.2.3\r\nuv==0.5.4\r\nuvicorn==0.31.0\r\nvirtualenv==20.26.6\r\nwaitress==3.0.2\r\nwatchfiles==0.24.0\r\nwcwidth==0.2.13\r\nwebsocket-client==1.8.0\r\nwebsockets==13.1\r\nwerkzeug==3.0.4\r\nwrapt==1.16.0\r\nwsproto==1.2.0\r\nyarl==1.13.1\r\nzipp==3.20.2\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "frostming",
        "body": "Do you have a repro repo? Giving the blog post is not sufficient to know what is going on on your side."
      },
      {
        "user": "parano",
        "body": "@frostming looks like the same issue that has been reported, regarding `bentoml push` failure on windows OS."
      }
    ]
  },
  {
    "issue_number": 5070,
    "title": "feature: Update dependency opentelemetry-api (1.20.0) to latest ",
    "author": "nseppi",
    "state": "closed",
    "created_at": "2024-11-08T09:35:14Z",
    "updated_at": "2024-12-17T08:24:33Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nHey guys, it would be great if you could update the opentelemetry-api to the latest available version\n\n### Motivation\n\nThe current dependency of 1.20 leads to a lot of dependency issues with other packages that are used in our project, hindering us to use bento. \n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "mstrika",
        "body": "Yeah getting the same error"
      }
    ]
  },
  {
    "issue_number": 5099,
    "title": "bug: conda-forge version of BentoML outdated",
    "author": "stream129",
    "state": "closed",
    "created_at": "2024-11-25T11:04:57Z",
    "updated_at": "2024-12-13T02:40:40Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nCurrent version of BentoML provided by conda-forge channel is significantly outdated (1.2.20 updated 4 month ago). Looks like something broke in the release pipeline. This is extremely inconvenient because you have to mix Conda and pip environments with all the associated problems.\n\n### To reproduce\n\n1. conda create --name bentoml_version_test pydantic=2.8.2 bentoml --channel conda-forge --yes\r\n2. conda activate bentoml_version_test\r\n3. bentoml --version\r\n\r\nBentoML reports version 1.2.20 released 4 month ago\r\npydantic=2.8.2 used due to [#4974](https://github.com/bentoml/BentoML/issues/4974), without this BentoML not working at all\n\n### Expected behavior\n\nBentoML reports version >= 1.3.14\n\n### Environment\n\nnot necessary for this kind of problem",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Hi there, the BentoML team is currently not maintaining the conda-forge version of BentoML, and we don't have any plans for maintaining the forge going forward.\r\n\r\nWe would kindly ask users to use PyPI going forward. You can still install PyPI packages in your conda environment."
      }
    ]
  },
  {
    "issue_number": 5120,
    "title": "bug: bentoml not taking into account latest version of Dockerfile.template",
    "author": "rlleshi",
    "state": "closed",
    "created_at": "2024-12-10T10:00:43Z",
    "updated_at": "2024-12-10T14:53:23Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nIf I run `cat \"$(bentoml get <bento>:<tag> -o path)/env/docker/Dockerfile\"` I can see its still based the old version of my `Dockerfile.template`. \r\n\r\nWhat I tried:\r\n- `docker system prune -a` to clear everything in my docker cache\r\n- `bentoml list --quiet | awk '{print $1}' | xargs -I {} bentoml delete {} --yes` to delete all of my bentos\r\n- `bentoml containerize my_bento:latest --no-cache` to build bento without cache\r\n\r\nAfter doing the above the container builds from scratch as expected but it's nevertheless still on the old version of the `Dockerfile.template`. Seems like bentoml must be having another caching mechanism though I haven't found anything from perusing the docs. Also it appears like there's almost no documentation whatsoever on `dockerfile_template` anymore. I remember there being some couple of months ago.\r\n\r\nActually I somehow got it working with the new version of `Dockerfile.template` once but wasn't able to reproduce it later.\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nbentoml: 1.3.15\r\npython: 3.9.0\r\nplatform: ubuntu 22.04, 6.5.0-45-generic",
    "comments": [
      {
        "user": "frostming",
        "body": "Did you rebuild your bento? You didn't mention that in the steps."
      },
      {
        "user": "rlleshi",
        "body": "Yeah ofc, I'm usually doing that with `bentoml build . --containerize` but I also tried `bentoml build .` & `bentoml containerize my_bento:latest --no-cache` separately. "
      },
      {
        "user": "frostming",
        "body": "Can you provide a reproducible example?"
      }
    ]
  },
  {
    "issue_number": 4919,
    "title": "bug: Unable to send monitoring logs to Elastic APM through OTLPHttpExporter",
    "author": "ConsciousML",
    "state": "open",
    "created_at": "2024-08-14T14:58:26Z",
    "updated_at": "2024-11-27T08:40:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nHi folks,\r\n\r\nI'm having a issue to push monitoring logs to Elastic APM through OTLPHttpExporter.\r\n\r\nI'm able to send dummy logs with this standalone code and see them in Elastic APM:\r\n```python\r\nimport os\r\nimport logging\r\nfrom dotenv import load_dotenv\r\nfrom opentelemetry import trace\r\nfrom opentelemetry.sdk.trace import TracerProvider\r\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\r\n\r\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\r\nfrom opentelemetry.sdk.resources import Resource\r\n\r\nif __name__ == '__main__':\r\n    load_dotenv()\r\n\r\n    logging.basicConfig(level=logging.INFO)\r\n    logger = logging.getLogger(__name__)\r\n\r\n    otel_exporter_otlp_endpoint = os.environ.get('OTEL_EXPORTER_OTLP_ENDPOINT') + \"/v1/traces\"\r\n    otel_exporter_otlp_headers = os.environ.get('OTEL_EXPORTER_OTLP_HEADERS')\r\n\r\n    key, value = otel_exporter_otlp_headers.split('=')\r\n    otel_headers_dict = {key: value}\r\n\r\n    exporter = OTLPSpanExporter(endpoint=otel_exporter_otlp_endpoint, headers=otel_headers_dict)\r\n\r\n    resource_attributes = os.environ.get('OTEL_RESOURCE_ATTRIBUTES')\r\n\r\n    key_value_pairs = resource_attributes.split(',')\r\n    result_dict = {}\r\n\r\n    for pair in key_value_pairs:\r\n        key, value = pair.split('=')\r\n        result_dict[key] = value\r\n\r\n    otel_service_name = \"reconciliation-ranker\"\r\n    resource_attributes = {\r\n        \"service.name\": otel_service_name,\r\n        \"service.version\": result_dict['service.version'],\r\n        \"deployment.environment\": result_dict['deployment.environment'],\r\n    }\r\n\r\n    resource = Resource.create(resource_attributes)\r\n\r\n    provider = TracerProvider(resource=resource)\r\n    processor = BatchSpanProcessor(exporter)\r\n    provider.add_span_processor(processor)\r\n\r\n    trace.set_tracer_provider(provider)\r\n\r\n    tracer = trace.get_tracer(otel_service_name)\r\n\r\n    try:\r\n        with tracer.start_as_current_span(\"test-span\") as span:\r\n            logger.info(\"Hello from OpenTelemetry!\")\r\n            span.set_attribute(\"custom.attribute\", \"test-value\")\r\n\r\n        provider.force_flush()\r\n\r\n        logger.info(\"Traces sent to Elastic APM\")\r\n    except Exception as e:\r\n        logger.error(f\"An error occurred: {str(e)}\")\r\n\r\n    provider.shutdown()\r\n```\r\n\r\nI load an `.env` file in the following format:\r\n```\r\nOTEL_EXPORTER_OTLP_ENDPOINT=my_elastic_apm_url\r\nOTEL_EXPORTER_OTLP_HEADERS=\"Authorization=Bearer my_token\"\r\nOTEL_METRICS_EXPORTER=otlp\r\nOTEL_LOGS_EXPORTER=otlp\r\nOTEL_RESOURCE_ATTRIBUTES=\"service.name=my-service,service.version=0.0.1,deployment.environment=production\"\r\n```\r\n\r\nFirst, I tried to input the same arguments in my BentoML `service.py`:\r\n```python\r\nclass InferenceInput(BaseModel):  # pylint: disable=too-few-public-methods\r\n    \"\"\"Pydantic class for the inputs of the inference API method of the ReconciliationService\"\"\"\r\n\r\n    site_ids: List[str] = Field(description='The IDs of the reporting site data to reconciliate.')\r\n    provider_ids: List[str] = Field(\r\n        description='The IDs of the reporting provider data to reconciliate.'\r\n    )\r\n    top_k: int = Field(default=sys.maxsize, description='Number of recommendations to return.')\r\n    only_recommend: bool = Field(\r\n        default=False, description='Whether to only perform recommendation and not auto-match.'\r\n    )\r\n    remove_matched: bool = Field(\r\n        default=True,\r\n        description='Whether to remove the auto-match data from the recommendations.',\r\n    )\r\n\r\n\r\nenv_variables = load_service_env_vars()\r\n\r\nkey, value = env_variables[\"OTEL_EXPORTER_OTLP_HEADERS\"].split('=')\r\notel_headers_dict = {key: value}\r\n\r\n\r\n@bentoml.service(  # pylint: disable=no-member\r\n    name=\"reconciliation-ranker\",\r\n    resources={\"cpu\": \"cpu_count\"},  # Adjust as needed\r\n    traffic={\"timeout\": 10},\r\n    monitoring={\r\n        \"enabled\": True,\r\n        \"type\": \"otlp\",\r\n        \"options\": {\r\n            \"endpoint\": env_variables['OTEL_EXPORTER_OTLP_ENDPOINT'] + \"/v1/traces\",\r\n            \"headers\": otel_headers_dict,\r\n            \"insecure\": False,\r\n            \"timeout\": 10,\r\n            \"meta_sample_rate\": 1.0,\r\n        },\r\n    },\r\n)\r\nclass ReconciliationService:\r\n    \"\"\"\r\n    Service for the reconciliation model to create APIs that return auto-match and recommendations\r\n    \"\"\"\r\n\r\n    def __init__(self) -> None:\r\n        \"\"\"\r\n        Initializes the Reconciliation Runner.\r\n\r\n        Loads the model based on the provided paths and environment variables.\r\n        \"\"\"\r\n        # Load the model\r\n        env_vars = load_service_env_vars()\r\n\r\n        self.model = load_reconciliation_engine(env_vars)\r\n\r\n        with open(env_vars['ONLINE_STORE_CONFIG'], 'r') as json_file:\r\n            online_store_config = json.load(json_file)\r\n\r\n        self.feature_site_id = online_store_config['storage']['embedding_site']\r\n        self.feature_provider_id = online_store_config['storage']['embedding_provider']\r\n\r\n        self.online_store = create_online_store(env_vars['ONLINE_STORE_CONFIG'])\r\n\r\n    @bentoml.api(input_spec=InferenceInput)  # pylint: disable=no-member\r\n    def inference(self, **params: Any) -> Dict:\r\n        \"\"\"\r\n        Predicts the reconciliation between the site and the provider tables\r\n        \"\"\"\r\n        with bentoml.monitor(\"bentoml_reconciliation_ranker\") as mon:\r\n\r\n            mon.log(\"dummy_text\", name=\"input_text\", role=\"original_text\", data_type=\"text\")\r\n\r\n            site_ids = params['site_ids']\r\n            provider_ids = params['provider_ids']\r\n\r\n            site_ids_array = np.array(site_ids)\r\n            provider_ids_array = np.array(provider_ids)\r\n\r\n            df_site = self.fetch_online_features(\r\n                site_ids_array, feature_view_id=self.feature_site_id\r\n            )\r\n            df_provider = self.fetch_online_features(\r\n                provider_ids_array, feature_view_id=self.feature_provider_id\r\n            )\r\n\r\n            df_site['encoded_date'] = encode_str_date(df_site.date.tolist())\r\n            df_provider['encoded_date'] = encode_str_date(df_provider.date.tolist())\r\n\r\n            response_dict = self.model.predict(\r\n                df_site,\r\n                df_provider,\r\n                top_k=params['top_k'],\r\n                remove_matched=params['remove_matched'],\r\n                only_recommend=params['only_recommend'],\r\n            )\r\n            return response_dict\r\n```\r\n\r\nWhen tried to make an HTTP request, I got the following error:\r\n```bash\r\n2024-08-14 15:07:53,424 - bentoml._internal.server.http_app - ERROR - Exception on /inference [POST]\r\nTraceback (most recent call last):\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/_bentoml_impl/server/app.py\", line 561, in api_endpoint_wrapper\r\n    resp = await self.api_endpoint(name, request)\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/_bentoml_impl/server/app.py\", line 655, in api_endpoint\r\n    output = await self._to_thread(func, *input_args, **input_params)\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/_bentoml_impl/server/app.py\", line 507, in _to_thread\r\n    output = await anyio.to_thread.run_sync(func, limiter=self._limiter)\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\r\n    result = context.run(func, *args)\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/_bentoml_sdk/method.py\", line 132, in wrapped\r\n    return self.func(instance, *args, **kwargs)\r\n  File \"/home/consciousml/ml-reconciliation/bentoml_ranker/service.py\", line 206, in inference\r\n    with bentoml.monitor(\"bentoml_reconciliation_ranker\") as mon:\r\n  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\r\n    next(self.gen)\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/bentoml/_internal/monitoring/api.py\", line 136, in monitor\r\n    mon.stop_record()\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/bentoml/_internal/monitoring/base.py\", line 75, in stop_record\r\n    self.export_data(datas)\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/bentoml/_internal/monitoring/otlp.py\", line 241, in export_data\r\n    self._init_logger()\r\n  File \"/home/consciousml/.virtualenvs/bentoml-rec-1.3/lib/python3.10/site-packages/bentoml/_internal/monitoring/otlp.py\", line 175, in _init_logger\r\n    os.environ[OTEL_EXPORTER_OTLP_HEADERS] = self.headers\r\n  File \"/usr/lib/python3.10/os.py\", line 685, in __setitem__\r\n    value = self.encodevalue(value)\r\n  File \"/usr/lib/python3.10/os.py\", line 757, in encode\r\n    raise TypeError(\"str expected, not %s\" % type(value).__name__)\r\nTypeError: str expected, not dict\r\n```\r\n\r\nI realized that it was coming from the [OTLPMonitor._init_logger](https://github.com/bentoml/BentoML/blob/c323ef44ea772edef4c1f15ee47d0263c1e0fb19/src/bentoml/_internal/monitoring/otlp.py#L175) when trying to set the environment variable.\r\n\r\nI was confused as in the [opentelemetry source code](https://github.com/open-telemetry/opentelemetry-python/blob/9a20504b2ccf9194266699ee4eff41939dfbd188/exporter/opentelemetry-exporter-otlp-proto-http/src/opentelemetry/exporter/otlp/proto/http/_log_exporter/__init__.py#L62) the OTLPLogExporter takes a dictionary as input: `headers: Optional[Dict[str, str]] = None`.\r\n\r\nI found my way around it by not passing the header in the monitoring config as so:\r\n```python\r\n@bentoml.service(  # pylint: disable=no-member\r\n    name=\"reconciliation-ranker\",\r\n    resources={\"cpu\": \"cpu_count\"},  # Adjust as needed\r\n    traffic={\"timeout\": 10},\r\n    monitoring={\r\n        \"enabled\": True,\r\n        \"type\": \"otlp\",\r\n        \"options\": {\r\n            \"endpoint\": env_variables['OTEL_EXPORTER_OTLP_ENDPOINT'] + \"/v1/traces\",\r\n            \"insecure\": False,\r\n            \"timeout\": 10,\r\n            \"meta_sample_rate\": 1.0,\r\n        },\r\n    },\r\n)\r\n```\r\n\r\nThe `OTEL_EXPORTER_OTLP_ENDPOINT` environment variable is used in the [OLTPLogExporter](https://github.com/open-telemetry/opentelemetry-python/blob/9a20504b2ccf9194266699ee4eff41939dfbd188/exporter/opentelemetry-exporter-otlp-proto-http/src/opentelemetry/exporter/otlp/proto/http/_log_exporter/__init__.py#L85) anyway.\r\n\r\nWhen I make a request, I get the error:\r\n```bash\r\n2024-08-14 15:18:30,234 - bentoml.access - INFO - 127.0.0.1:55220 (scheme=http,method=POST,path=/inference,type=multipart/form-data; boundary=03ad167cc00f4c3abdb5f98eae823407,length=137883) (status=200,type=application/json,length=127393) 2359.814ms\r\n2024-08-14T15:18:34+0200 [INFO] [cli] Got signal SIG_WINCH\r\n2024-08-14T15:18:35+0200 [DEBUG] [entry_service:reconciliation-ranker:1] Starting new HTTPS connection (1): 14175a94be434fad8b6c58a81013bd1d.apm.europe-west9.gcp.elastic-cloud.com:443\r\n2024-08-14T15:18:35+0200 [DEBUG] [entry_service:reconciliation-ranker:1] https://14175a94be434fad8b6c58a81013bd1d.apm.europe-west9.gcp.elastic-cloud.com:443 \"POST /v1/traces HTTP/11\" 400 81\r\n2024-08-14T15:18:35+0200 [DEBUG] [entry_service:reconciliation-ranker:1] Encoding detection: utf_8 will be used as a fallback match\r\n2024-08-14T15:18:35+0200 [DEBUG] [entry_service:reconciliation-ranker:1] Encoding detection: Found utf_8 as plausible (best-candidate) for content. With 0 alternatives.\r\n2024-08-14T15:18:35+0200 [ERROR] [entry_service:reconciliation-ranker:1] Failed to export logs batch code: 400, reason:Mfailed to unmarshal request body: proto: wrong wireType = 1 for field TraceId\r\n```\r\n\r\nBtw, I can't debug [the parse_env_headers](https://github.com/open-telemetry/opentelemetry-python/blob/9a20504b2ccf9194266699ee4eff41939dfbd188/opentelemetry-api/src/opentelemetry/util/re.py#L65) because when I run my `service.py` in debug mode, the log is added to the batch_log of the exporter but does not get sent.\r\n\r\nIs there a way to access the `TracerProvider` from a BentoML method to force_flush?\r\n\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\nThe logs should be uploaded without error to Elastic APM.\r\n\r\n### Environment\r\n\r\n`bentoml`: 1.3.0\r\n`python`: 3.10.12\r\n`platform`: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35",
    "comments": [
      {
        "user": "ConsciousML",
        "body": "I don't think that it is related to library versions as I was able to send logs to Elastic APM successfully with manual instrumentation."
      },
      {
        "user": "stephenhoran",
        "body": "I believe not specifying otlp to use the http protocol will use grpc calls. We use the open telemetry operator to inject instrumentation into Benton’s via env variables that forward to an Otel collector and then from an otel collector forward to ES AP M but we has to be very specific about avoiding grpc. "
      },
      {
        "user": "stephenhoran",
        "body": "https://docs.bentoml.com/en/latest/guides/observability/tracing.html"
      }
    ]
  },
  {
    "issue_number": 5050,
    "title": "bug: bentoml task runs progressively slower",
    "author": "rlleshi",
    "state": "closed",
    "created_at": "2024-10-29T07:51:08Z",
    "updated_at": "2024-11-21T17:19:28Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nTask definition:\r\n```\r\n    @bentoml.task(\r\n        batchable=True,\r\n        batch_dim=(0, 0),\r\n        max_batch_size=15,\r\n        max_latency_ms=1000)\r\n```\r\nNormally a batch of 15 image frames will run in about 0.9s when executed synchronously. From [the docs](https://docs.bentoml.com/en/latest/guides/tasks.html) I discovered that bentoml offers running background tasks out of the box, which made me go for this instead of executing the sub-service asynchronously (as this would be the only async sub-service of the main service).\r\n\r\nHowever, the execution gets progressively slower after each batch. Not only that but it seems to actually block the rest of the synchronous execution (at least for some time). Regarding the latency, it starts off executing in milliseconds (as it should), but eventually by the time we get to the final batch these are the execution times:\r\n\r\n```\r\n2024-10-29T08:20:18+0100 [INFO] [service:Service4:1] _ (scheme=http,method=GET,path=/postprocess/status,type=application/vnd.bentoml+pickle,length=50506324) (status=200,type=application/json,length=136) 9169.608ms (trace=b48c2f58c7a620a03df685f7863429f8,span=74a96aa3f968054f,sampled=0,service.name=Service4)\r\n2024-10-29T08:20:20+0100 [INFO] [service:Service4:1] _ (scheme=http,method=GET,path=/postprocess/get,type=application/vnd.bentoml+pickle,length=50506324) (status=200,type=application/json,length=165888126) 921.565ms (trace=b48c2f58c7a620a03df685f7863429f8,span=5fda59432b2011ac,sampled=0,service.name=Service4)\r\n2024-10-29T08:20:20+0100 [INFO] [service:Service4:1] Task(postprocess) ad42439e754744f5872db94e88aa8350 is submitted (trace=b48c2f58c7a620a03df685f7863429f8,span=f84919adaacaef22,sampled=0,service.name=Service4)\r\n2024-10-29T08:20:20+0100 [INFO] [service:Service4:1] _ (scheme=http,method=POST,path=/postprocess/submit,type=application/vnd.bentoml+pickle,length=50506324) (status=200,type=application/json,length=69) 432.818ms (trace=b48c2f58c7a620a03df685f7863429f8,span=f84919adaacaef22,sampled=0,service.name=Service4)\r\n```\r\nthat is more than 10 seconds for a batch that should take ~0.9s to process, so more than an order of magnitude! As a result of this the entire execution time more than doubles.\r\n\r\nMy main service looks sth like this:\r\n```\r\niterate over all the frames of the image in batches\r\nexecute service 1, takes about 0.9s\r\nexecute service 2, takes about 0.1s\r\nexecute service 3, takes about 1.5s\r\nexecute service 4 (this is the task), which should take about 0.9s\r\n```\r\nSo by the time the next task starts executing there is at least 2.5s of execution time (excluding overhead), which should be more than enough for the previous task to have finished their execution.\r\n\r\nRegarding the execution of the task in the main service it looks sth like this:\r\n\r\n```\r\nself.task = self.service4.postprocess.submit(\r\n    [BatchInput(image=frame, ...)\r\n    for frame, ..., in zip(batch_frames, ...)\r\n])\r\n```\r\nand then accessing it (this happens after service 1-3 finished executing)\r\n\r\n```\r\nif self.task:\r\n    status = self.task.get_status()\r\n    if status.value == 'failure':\r\n        # do sth...\r\n    else:\r\n        results = self.task.get()\r\n        # do sth...\r\n```\r\n\r\nAm I missing something? Also, other than the above-linked documentation page, is there more documentation I could get on tasks in bentoml?\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\nThe task should execute in the background & not block the main flow, and therefore it should in the end it should speed up the overall execution instead of slowing it down.\r\n\r\n### Environment\r\n\r\nbentoml: 1.3.3\r\npython: 3.9.0\r\nplatform: ubuntu 22.04, 6.5.0-45-generic ",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Can you provide your service definition here? obv you should remove anything that is sensitive there?\r\n\r\nBut this seems, strange."
      },
      {
        "user": "rlleshi",
        "body": "Hey, ty for the quick follow-up. \r\n\r\nSure: \r\n\r\n```\r\n@bentoml.service(\r\n    traffic={\r\n        'timeout': 10,\r\n        'concurrency': 15,\r\n    },\r\n    metrics={\r\n        'enabled': True,\r\n    },\r\n    workers=2,\r\n)\r\nclass Service4(BentoWrapper):\r\n\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n        # do stuff\r\n\r\n    @bentoml.task(\r\n        batchable=True,\r\n        batch_dim=(0, 0),\r\n        max_batch_size=15,\r\n        max_latency_ms=1000)\r\n    def postprocess(self,\r\n                inputs: list[BatchInput]) -> torch.Tensor:\r\n```\r\nAlso updated the issue with some more info regarding the task execution from the main service.\r\n"
      },
      {
        "user": "aarnphm",
        "body": "What does this BentoWrapper class do?\r\n"
      }
    ]
  },
  {
    "issue_number": 5069,
    "title": "bug: bentoml containerize failed by uv installation location change",
    "author": "lingw777",
    "state": "closed",
    "created_at": "2024-11-08T07:44:44Z",
    "updated_at": "2024-11-21T07:43:57Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nMy CICD build is failing at `bentoml containerize`\r\n```\r\nRUN curl -LO https://astral.sh/uv/install.sh &&     sh install.sh && rm install.sh && mv $HOME/.cargo/bin/uv /usr/local/bin/\r\n#12 0.858 downloading uv 0.5.0 x86_64-unknown-linux-gnu\r\n#12 1.514 installing to /root/.local/bin\r\n#12 1.553 mv: cannot stat '/root/.cargo/bin/uv': No such file or directory\r\n```\r\n\r\nIt's probably related to recent uv release. See https://github.com/astral-sh/uv/issues/8913\r\n\r\n\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: 1.3.10\r\npython: 3.10\r\nplatform: --opt platform=linux/amd64,linux/arm64",
    "comments": [
      {
        "user": "lingw777",
        "body": "Found this breaking change in `uv` release note\r\n\r\n> Use XDG (i.e. ~/.local/bin) instead of the Cargo home directory in the installer (#8420)\r\n> \r\n> Previously, uv's installer used $CARGO_HOME or ~/.cargo/bin for its target install directory. It's been a longstanding complaint that uv uses this directory, as there's no relationship to Cargo. Now, uv will be installed into $XDG_BIN_HOME, $XDG_DATA_HOME/../bin, or ~/.local/bin (in that order). Note that $UV_INSTALL_DIR can always be used to override the target directory. Upgrading with uv self update will not move uv to the new install directory.\r\n\r\nand bentoML base docker image scripts pull from latest installation scripts https://astral.sh/uv/install.sh but still trying to `mv $HOME/.cargo/bin/uv`  \r\nSee https://github.com/bentoml/BentoML/blob/0088c75f0c58a8fec1b147678756213e44007617/src/bentoml/_internal/container/frontend/dockerfile/templates/base_debian.j2#L17"
      },
      {
        "user": "frostming",
        "body": "The fix is on the way to the next release."
      },
      {
        "user": "mobecks",
        "body": "Workaround until next release (worked for me): \r\nchange above-mentioned \"line 17\" to\r\n`sh install.sh && rm install.sh && mv $HOME/.local/bin/uv /usr/local/bin/`"
      }
    ]
  },
  {
    "issue_number": 5058,
    "title": "feature: Stop and resume tasks",
    "author": "Leon-Sander",
    "state": "closed",
    "created_at": "2024-11-01T11:05:18Z",
    "updated_at": "2024-11-20T01:11:00Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nI would like to be able to manage stopping and resuming of tasks.\n\n### Motivation\n\nIf I have a long running task like large batch processing and the server needs a reboot for example, then I would want to save the intermediate results and resume later on.\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "A task can be interrupted only if it can respond to interrupt signals and save the state of the scene for resumption.\r\n\r\nThis requires considerable efforts to implement and hard to make it generic. You can implement for specific frameworks but not for all.\r\n\r\nFor example, a simple fibonacci task like:\r\n\r\n```python\r\ndef fib(n):\r\n    if n <= 1:\r\n        return 1\r\n    return fib(n-1) + fib(n-2)\r\n```\r\nis not stoppable."
      },
      {
        "user": "Leon-Sander",
        "body": "@frostming  thanks for the response, and yes absolutely, I agree. \r\n\r\nI thought about it and here is the idea I had. Making a generic pause and resuming, which can be activated by choice if one wants to implement the logic for his framework.\r\n\r\nSo @betoml.task(is_pausable=True).\r\n\r\nIt would just be necessary to make the status of the task_id accessible. \r\nis_pausable=True would make the api endpoints of pause and resume available. If the user does not want to implement it, is_pausable default is False and everything else stays the same."
      },
      {
        "user": "frostming",
        "body": "Hi @Leon-Sander  After our discussion, we decided not to do this because `bentoml.task` is not positioned as a mechanism with complete task management."
      }
    ]
  },
  {
    "issue_number": 5082,
    "title": "bug: `AttributeError`: `close()` called on Service dependency in `TestClient`",
    "author": "yxtay",
    "state": "closed",
    "created_at": "2024-11-13T03:15:32Z",
    "updated_at": "2024-11-13T23:57:42Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nHi,\r\n\r\nI am encountering `AttributeError` when using `TestClient` due to `close` being called on service dependencies. Below is the reproducible example.\r\n\r\nIn the documentation, a simple service is demoed. But this doesn't work with model composition.\r\nhttps://docs.bentoml.com/en/latest/guides/testing.html#http-behavior-tests\r\n\r\nI believe this is linked to using `TestClient` as a context manager calls the lifespan handler.\r\nhttps://www.starlette.io/lifespan/#running-lifespan-in-tests\r\n\r\nThe service file\r\n```\r\n# service.py\r\n\r\nimport bentoml\r\n\r\n\r\n@bentoml.service(metrics={\"enabled\": False})\r\nclass Model:\r\n    model = {\"hello\": \"world\"}\r\n\r\n    @bentoml.api()\r\n    def get(self, key: str) -> str | None:\r\n        return self.model.get(key)\r\n\r\n    # workaround: add close method in dependent model\r\n    # async def close(self) -> None:\r\n    #     pass\r\n\r\n\r\n@bentoml.service(metrics={\"enabled\": False})\r\nclass Service:\r\n    model = bentoml.depends(Model)\r\n\r\n    @bentoml.api()\r\n    def get(self, key: str) -> dict[str, str | None]:\r\n        return {\"value\": self.model.get(key)}\r\n```\r\n\r\nUsing `TestClient` directly on the `Model` is fine.\r\n```\r\n# test model\r\n\r\nfrom starlette.testclient import TestClient\r\n\r\nfrom service import Model\r\n\r\n\r\nwith TestClient(app=Model.to_asgi()) as client:\r\n    resp = client.post(\"/get\", json={\"key\": \"hello\"})\r\n    print(resp.text)\r\n    resp.raise_for_status()\r\n\r\n# output: world\r\n```\r\n\r\nBut using `TestClient` on `Service` throws `AttributeError`. Stack trace is below.\r\n```\r\n# test service\r\n\r\nfrom starlette.testclient import TestClient\r\n\r\nfrom service import Service\r\n\r\n\r\nwith TestClient(app=Service.to_asgi()) as client:\r\n    resp = client.post(\"/get\", json={\"key\": \"hello\"})\r\n    print(resp.text)\r\n    resp.raise_for_status()\r\n\r\n# output: {\"value\": \"world\"}\r\n# also attribute error is thrown on exiting the client context manager\r\n\r\nTraceback (most recent call last):\r\n  File \"project/test_service.py\", line 5, in <module>\r\n    with TestClient(app=Service.to_asgi()) as client:\r\n  File \"venv/lib/python3.10/site-packages/starlette/testclient.py\", line 750, in __exit__\r\n    self.exit_stack.close()\r\n  File \"venv/lib/python3.10/contextlib.py\", line 584, in close\r\n    self.__exit__(None, None, None)\r\n  File \"venv/lib/python3.10/contextlib.py\", line 576, in __exit__\r\n    raise exc_details[1]\r\n  File \"venv/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"venv/lib/python3.10/site-packages/anyio/from_thread.py\", line 495, in start_blocking_portal\r\n    yield portal\r\n  File \"venv/lib/python3.10/contextlib.py\", line 561, in __exit__\r\n    if cb(*exc_details):\r\n  File \"venv/lib/python3.10/contextlib.py\", line 449, in _exit_wrapper\r\n    callback(*args, **kwds)\r\n  File \"venv/lib/python3.10/site-packages/starlette/testclient.py\", line 743, in wait_shutdown\r\n    portal.call(self.wait_shutdown)\r\n  File \"venv/lib/python3.10/site-packages/anyio/from_thread.py\", line 290, in call\r\n    return cast(T_Retval, self.start_task_soon(func, *args).result())\r\n  File \"venv/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\r\n    return self.__get_result()\r\n  File \"venv/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"venv/lib/python3.10/site-packages/anyio/from_thread.py\", line 221, in _call_func\r\n    retval = await retval_or_awaitable\r\n  File \"venv/lib/python3.10/site-packages/starlette/testclient.py\", line 791, in wait_shutdown\r\n    await receive()\r\n  File \"venv/lib/python3.10/site-packages/starlette/testclient.py\", line 780, in receive\r\n    self.task.result()\r\n  File \"venv/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\r\n    return self.__get_result()\r\n  File \"venv/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"venv/lib/python3.10/site-packages/anyio/from_thread.py\", line 221, in _call_func\r\n    retval = await retval_or_awaitable\r\n  File \"venv/lib/python3.10/site-packages/starlette/testclient.py\", line 755, in lifespan\r\n    await self.app(scope, self.stream_receive.receive, self.stream_send.send)\r\n  File \"venv/lib/python3.10/site-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 152, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"venv/lib/python3.10/site-packages/opentelemetry/instrumentation/asgi/__init__.py\", line 543, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"venv/lib/python3.10/site-packages/bentoml/_internal/server/http/access.py\", line 69, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"venv/lib/python3.10/site-packages/bentoml/_internal/server/http/traffic.py\", line 26, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"venv/lib/python3.10/site-packages/_bentoml_impl/server/app.py\", line 62, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 48, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"venv/lib/python3.10/site-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"venv/lib/python3.10/site-packages/starlette/routing.py\", line 724, in app\r\n    await self.lifespan(scope, receive, send)\r\n  File \"venv/lib/python3.10/site-packages/starlette/routing.py\", line 693, in lifespan\r\n    async with self.lifespan_context(app) as maybe_state:\r\n  File \"venv/lib/python3.10/contextlib.py\", line 206, in __aexit__\r\n    await anext(self.gen)\r\n  File \"venv/lib/python3.10/site-packages/bentoml/_internal/server/base_app.py\", line 81, in lifespan\r\n    await ret\r\n  File \"venv/lib/python3.10/site-packages/_bentoml_impl/server/app.py\", line 329, in destroy_instance\r\n    await cleanup()\r\n  File \"venv/lib/python3.10/site-packages/_bentoml_sdk/service/dependency.py\", line 26, in cleanup\r\n    await asyncio.gather(*tasks)\r\n  File \"venv/lib/python3.10/site-packages/_bentoml_sdk/service/dependency.py\", line 106, in close\r\n    await t.cast(\"RemoteProxy[t.Any]\", self._resolved).close()\r\nAttributeError: 'Model' object has no attribute 'close'\r\n```\r\n\r\nA possible workaround is to add an empty async `close` method to the dependent model as I added as a comment. I think this should be better handled in the `bentoml.depends` decorator instead.\r\n\r\n### To reproduce\r\n\r\nExample as provided above.\r\n\r\n### Expected behavior\r\n\r\n`AttributeError` should not be thrown when using `TestClient` on service with model composition.\r\n\r\n### Environment\r\n\r\n`bentoml==1.3.11`\r\n`python==3.10.13`\r\n\r\nplatform: ubuntu 22.04",
    "comments": [
      {
        "user": "frostming",
        "body": "Should guard agains missing `close` method. Can you send a PR for this?"
      },
      {
        "user": "yxtay",
        "body": "Will you be able to point me to the code where this should be done?"
      },
      {
        "user": "frostming",
        "body": "Along at this very line:\r\n\r\n>```\r\n> File \"venv/lib/python3.10/site-packages/_bentoml_sdk/service/dependency.py\", line 106, in close\r\n>    await t.cast(\"RemoteProxy[t.Any]\", self._resolved).close()\r\n>```\r\n\r\nCheck the `.close()` exists before calling it, you can add a condition check to the `if` statement just above it"
      }
    ]
  },
  {
    "issue_number": 3813,
    "title": "bugs: Exception during shutdown",
    "author": "ssheng",
    "state": "closed",
    "created_at": "2023-05-02T06:30:56Z",
    "updated_at": "2024-11-12T08:23:01Z",
    "labels": [],
    "body": "Issuing SIGTERM (Ctrl+C) to a runner server causes the following exception to be raised. \n\n```\n023-05-01T23:27:09-0700 [ERROR] [cli] Exception in callback <bound method Arbiter.manage_watchers of <circus.arbiter.Arbiter object at 0x10719e2c0>>\nTraceback (most recent call last):\n  File \"/opt/miniconda3/envs/bentoml/lib/python3.10/site-packages/tornado/ioloop.py\", line 921, in _run\n    val = self.callback()\n  File \"/opt/miniconda3/envs/bentoml/lib/python3.10/site-packages/circus/util.py\", line 1038, in wrapper\n    raise ConflictError(\"arbiter is already running %s command\"\ncircus.exc.ConflictError: arbiter is already running arbiter_stop command\n```",
    "comments": [
      {
        "user": "ConsciousML",
        "body": "I still have this error when I'm running BentoML containers on CloudRun.\r\nAny clue how to fix it?"
      },
      {
        "user": "parano",
        "body": "@ConsciousML what's your BentoML version?"
      },
      {
        "user": "Bowser1704",
        "body": "I meet same error in `bentoml, version 1.3.10`."
      }
    ]
  },
  {
    "issue_number": 5067,
    "title": "bentoml 1.3.10 depends on nvidia-ml-py<12, vllm 0.6.3.dev  need nvidia-ml-py 12.5",
    "author": "frei-x",
    "state": "closed",
    "created_at": "2024-11-07T17:46:22Z",
    "updated_at": "2024-11-08T09:27:03Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\npip install https://vllm-wheels.s3.us-west-2.amazonaws.com/nightly/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml 1.3910",
    "comments": []
  },
  {
    "issue_number": 5051,
    "title": "bug: bentoml 1.3.10 containerize run failed but bentoml serve succeed",
    "author": "cceasy",
    "state": "closed",
    "created_at": "2024-10-29T09:26:40Z",
    "updated_at": "2024-10-30T01:25:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nHi there, I have encountered a problem with bentoml containerized run, but I could successfully run by using bentoml serve.\n\n### To reproduce\n\nI could use `bentoml serve service:ModelService` to run the model below, and I could build docker image by `bentoml containerize --opt platform=linux/amd64 xxx`, but I failed to run it with `docker run --rm --platform linux/amd64 xxx`, here is the error message while doing docker run:\r\n```\r\n% docker run --rm --platform linux/amd64 -v ~/.config/gcloud:/home/bentoml/.config/gcloud -p 8080:8080 -e BENTOML_PORT='8080' -e MODEL_NAME='my-keras-bert-model' -e MODEL_VERSION='v0.0.1' my-keras-bert-model:stzwfhev2o2jphsf\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/bentoml\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/bentoml_cli/utils.py\", line 361, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/bentoml_cli/utils.py\", line 332, in wrapper\r\n    return_value = func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/bentoml_cli/env_manager.py\", line 126, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/bentoml_cli/serve.py\", line 261, in serve\r\n    svc = load(bento_identifier=bento, working_dir=working_dir)\r\n  File \"/usr/local/lib/python3.10/site-packages/bentoml/_internal/service/loader.py\", line 369, in load\r\n    _svc = import_1_2_service(_bento_identifier, _working_dir)\r\n  File \"/usr/local/lib/python3.10/site-packages/_bentoml_impl/loader.py\", line 198, in import_service\r\n    svc.on_load_bento(bento)\r\n  File \"/usr/local/lib/python3.10/site-packages/_bentoml_sdk/service/factory.py\", line 402, in on_load_bento\r\n    service_info = next(svc for svc in bento.info.services if svc.name == self.name)\r\nStopIteration\r\n```\r\n\r\nFiles as below:\r\n\r\n- bentofile.yaml\r\n```\r\nservice: \"service:ModelService\"\r\n\r\ndescription: \"A Bento Service for my keras bert model.\" \r\n\r\npython:\r\n  requirements_txt: \"requirements.txt\"\r\n  lock_packages: false\r\n\r\ninclude:\r\n  - \"**/*.py\"\r\n  - \"**/*.txt\"\r\n\r\n```\r\n\r\n- service.py\r\n```\r\nimport bentoml\r\nfrom starlette.responses import JSONResponse\r\nfrom starlette.status import HTTP_500_INTERNAL_SERVER_ERROR\r\nfrom pydantic import RootModel\r\nfrom pathlib import Path\r\nimport os\r\n\r\nmodel_name = Path(__file__).parent.parent.parent.stem\r\n\r\nclass Input(RootModel[list]):\r\n    pass\r\n\r\n@bentoml.service(\r\n    name=model_name,\r\n    traffic={\"timeout\": 10},\r\n)\r\nclass ModelService:\r\n    def __init__(self):\r\n        super().__init__()\r\n        model_tag = \"{}:{}\".format(os.environ.get(\"MODEL_NAME\"), os.environ.get(\"MODEL_VERSION\"))\r\n        self.model = bentoml.keras.load_model(model_tag)\r\n\r\n    @bentoml.on_deployment\r\n    def load_model_artifact():\r\n        # import bentoml model from GCS to local\r\n\r\n    @bentoml.api(input_spec=Input)\r\n    def predict(self, root: list) -> list:\r\n        # You need to modify the input pre-processing and output post-processing according to your requirements.\r\n        try:\r\n            input = self.preprocess(root)\r\n            output = self.model.predict(input)\r\n            res = self.postprocess(output)\r\n            return res\r\n        except Exception as e:\r\n            print(\"failed to predict\", str(e))\r\n            return JSONResponse(\r\n                content={\"message\": f\"An error occurred during prediction: {str(e)}\"},\r\n                status_code=HTTP_500_INTERNAL_SERVER_ERROR\r\n            )\r\n\r\n    def preprocess(self, input):\r\n        input_feed = [x[\"review\"] for x in input]\r\n        print(\"debug model input:\", input_feed)\r\n        return input_feed\r\n\r\n    def postprocess(self, output):\r\n        print(\"debug model output:\", type(output), output)\r\n        res = [\r\n            {\r\n                \"raw_score\": x[1],\r\n                \"score\": x[1] * 100\r\n            } for x in output.tolist()\r\n        ]\r\n        return res\r\n\r\n```\r\n\r\n- how to gen model\r\n```\r\nimport os\r\nimport bentoml\r\n\r\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\"\r\n\r\nimport tensorflow_datasets as tfds\r\nimport keras_nlp\r\n\r\nimdb_train, imdb_test = tfds.load(\r\n    \"imdb_reviews\",\r\n    split=[\"train\", \"test\"],\r\n    as_supervised=True,\r\n    batch_size=16,\r\n)\r\n\r\n# Load a model.\r\nclassifier = keras_nlp.models.BertClassifier.from_preset(\r\n    \"bert_tiny_en_uncased\",\r\n    num_classes=2,\r\n    activation=\"softmax\",\r\n)\r\nclassifier.fit(imdb_train.take(250), validation_data=imdb_test.take(250))\r\n# Predict new examples.\r\nclassifier.predict([\"What an amazing movie!\", \"A total waste of my time.\"])\r\n\r\nbentoml.keras.save_model(\"my-test-model:v0.0.1\", classifier)\r\n```\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml==1.3.10\r\nkeras-nlp~=0.17.0\r\nkeras-hub-nightly==0.16.1.*\r\ntensorflow~=2.18.0",
    "comments": [
      {
        "user": "frostming",
        "body": "> model_name = Path(__file__).parent.parent.parent.stem\r\n\r\nI think the problem is caused by this line, you get the service name dynamically from the parent directory name, but the directory won't be packed into the docker image. So when containerized, the service name is not likely the same as what is frozen in the bento. Therefore the error is explained.\r\n\r\nYou can close this issue if no other concerns."
      },
      {
        "user": "cceasy",
        "body": "Thanks for the quick response @frostming "
      }
    ]
  },
  {
    "issue_number": 4143,
    "title": "Support https in bentoml client",
    "author": "MattiasDC",
    "state": "open",
    "created_at": "2023-08-23T10:12:05Z",
    "updated_at": "2024-10-29T11:20:10Z",
    "labels": [],
    "body": "I wanted to use the bentoml client, but I only have an https connection available, which is currently not supported. I can create a PR for this if this is ok to work on. Relevant [code](https://github.com/bentoml/BentoML/blob/38536aaec30d8f15912f631cef2dc0d2435c9ea1/src/bentoml/_internal/client/http.py#L94C16-L94C16)",
    "comments": []
  },
  {
    "issue_number": 4651,
    "title": "bug: Unable to request PILImage with Content-Type image/jpeg with new Service decorator",
    "author": "adrian-arapiles",
    "state": "closed",
    "created_at": "2024-04-11T18:23:36Z",
    "updated_at": "2024-10-25T03:20:39Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nUnable to get an endpoint with `PILImage` without `Content-Type:multipart/form-data` with new Service decorator\n\n### To reproduce\n\nI already have working code with bentoml 1.1.11, runners, yolo/pytorch model.\r\nI'm trying to migrate to new decorators `@bentoml.service` and `@bentoml.api` to use concurrency feature introduced.\r\n\r\nBentoml version tested: [1.2.9, 1.2.10].\r\n\r\nAll examples running with `bentoml serve -p 8000 service.py:Service`.\r\n\r\nMy working code with 1.1.11 and 1.2.10 with Runner/Runnable looks like\r\n```python\r\nfrom bentoml.io import Image, JSON\r\n\r\n\r\nmy_runnable = bentoml.Runner(\r\n    MyRunnable,\r\n    name=\"runnable\"\r\n)\r\nsvc = bentoml.Service(\"runnable\", runners=[my_runnable])\r\n\r\n\r\n@svc.api(input=Image(), output=JSON(), route=\"/v1/inference\")\r\nasync def inference(image: Image):\r\n        #ofuscated code for prepare image, inference, build response and return\r\n        return output_json\r\n```\r\n\r\nAn example curl to before code is\r\n```sh\r\ncurl -X 'POST' \\\r\n        'http://localhost:8000/v1/inference' \\\r\n        -H 'accept: application/json' \\\r\n           -H 'Content-Type: image/jpeg' \\\r\n              --data-binary '@/path/to/my/image.jpg'\r\n```\r\nNow, I'm trying to migrate to new decorators and my code looks like\r\n```python\r\n\r\nfrom src.Service import Service\r\nfrom PIL.Image import Image as PILImage\r\n\r\n\r\n@bentoml.service()\r\nclass Service:\r\n    # this class is a sustitute of MyRunnable class. \r\n    my_new_service = bentoml.depends(Service)\r\n\r\n    @bentoml.api(\r\n        route=\"/v1/debug\"\r\n    )\r\n    async def debug(\r\n            self,\r\n            image: PILImage = Field(description=\"Image to be processed\")\r\n    ):\r\n        return {\"width\": image.width, \"height\": image.height}\r\n```\r\nNow my curl for this code (extracted from swagger) is\r\n```\r\ncurl -X 'POST' \\\r\n  'http://localhost:8000/v1/inference' \\\r\n  -H 'accept: application/json' \\\r\n  -H 'Content-Type: multipart/form-data' \\\r\n  -F 'image=@/path/to/my/image.jpg'\r\n```\r\nThe code is working, but I don't want to change the endpoint call from `Content-Type: image/jpeg` to `Content-Type: multipart/form-data`.\r\n\r\n\r\n\r\n\r\nI tried this:\r\n```\r\n# This is the using the same Image() that `@svc.api(input=Image()` old decorator uses\r\n\r\nfrom bentoml.io import Image\r\nfrom PIL.Image import Image as PILImage\r\nfrom src.Service import Service\r\n\r\n@bentoml.service()\r\nclass Service:\r\n    # this class is a sustitute of MyRunnable class. \r\n    my_new_service = bentoml.depends(Service)\r\n\r\n    @bentoml.api(\r\n        input_spec=Image(),\r\n        route=\"/v1/debug\"\r\n    )\r\n    async def debug(\r\n            self,\r\n            image: PILImage = Field(description=\"Image to be processed\")\r\n    ):\r\n        return {\"width\": image.width, \"height\": image.height}\r\n\r\n```\r\nWith the following errors:\r\n```\r\n  File \"/.../service.py\", line 34, in <module>\r\n    class Service:\r\n  File \"/.../service.py\", line 37, in Service\r\n    @bentoml.api(\r\n     ^^^^^^^^^^^^\r\n  File \"/.../_bentoml_sdk/decorators.py\", line 101, in wrapper\r\n    return APIMethod(func, **params)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<attrs generated init _bentoml_sdk.method.APIMethod>\", line 13, in __init__\r\n  File \"/.../_bentoml_sdk/method.py\", line 36, in _io_descriptor_converter\r\n    raise ValueError(f\"{it} must be a class type\")\r\nValueError: Image must be a class type\r\n```\r\n\r\n\r\n\r\nI already tried this:\r\n```\r\n# This is the using the Annotated example in documentation\r\n\r\nfrom typing import Annotated\r\nfrom bentoml.validators import ContentType\r\nfrom PIL.Image import Image as PILImage\r\nfrom src.Service import Service\r\n\r\n@bentoml.service()\r\nclass Service:\r\n    # this class is a sustitute of MyRunnable class. \r\n    my_new_service = bentoml.depends(Service)\r\n\r\n    @bentoml.api(\r\n        route=\"/v1/debug\"\r\n    )\r\n    async def debug(\r\n            self,\r\n            image: Annotated[PILImage, ContentType('image/jpeg')]\r\n    ):\r\n        return {\"width\": image.width, \"height\": image.height}\r\n\r\n```\r\nThis code is working too but the same mentioned problem, changed Content-Type to multipart.\r\n\r\nI'm not sure if it is a bug or not. It's possible just I was missing something.\r\n\r\nThe motivation to upgrade to new decorators is the use of concurrency feature introduced.\r\nIs there any way to use with old Runners way?\r\n\r\nI really appreciated some help there. \r\n\r\nThanks in advance,\r\nAdrián.\r\n\r\n\n\n### Expected behavior\n\nUsing PILImage or bentoml.io.Image in the same way and with the same endpoint result.\n\n### Environment\n\nbentoml: 1.2.9 and 1.2.10\r\npython: 3.11.8\r\nplatform: Macos Sonoma 14.4.1. M1 PRO cpu.",
    "comments": [
      {
        "user": "frostming",
        "body": "@adrian-arapiles It's related to how we translate the API annotations to wire format and the current design is the input must be a dictionary with inputs as field values.\r\n\r\nWe are not going to change the wire format shortly because of the efforts it needs. Anyway you can make it work by posting with a multipart body, it's easy in any of our supported clients. For example:\r\n\r\ncURL:\r\n```\r\ncurl -XPOST http://localhost:3000/v1/debug -H content-type:multipart/form -F image=@/path/to/image.jpeg\r\n```\r\n(comparing to the curl command you gave with `content-type=image/jpeg`, I don't think it is harder to read or use)\r\nAnd `requests`:\r\n```python\r\nimport requests\r\nrequests.post('http://localhost:3000/v1/debug', files=('image', open('/path/to/image.jpeg')))\r\n```\r\n"
      },
      {
        "user": "bojiang",
        "body": "@adrian-arapiles  Here is the full list about how type mapped to http requests:\r\nhttps://docs.bentoml.org/en/latest/guides/iotypes.html#appendix"
      }
    ]
  },
  {
    "issue_number": 4990,
    "title": "Daemon functionality is currently not supported on Windows.",
    "author": "Syed-Mansoor",
    "state": "closed",
    "created_at": "2024-09-23T13:59:20Z",
    "updated_at": "2024-10-23T08:22:35Z",
    "labels": [
      "feedback-wanted"
    ],
    "body": null,
    "comments": [
      {
        "user": "frostming",
        "body": "Can you explain a bit more? What did you try and what did you observe and what is the expected output?"
      },
      {
        "user": "frostming",
        "body": "Closing due to insufficient information"
      }
    ]
  },
  {
    "issue_number": 5015,
    "title": "feature: Add Support for Saving Quantized PyTorch Models (INT8) Using Picklable Model in BentoML",
    "author": "sebasmos",
    "state": "closed",
    "created_at": "2024-10-09T14:23:15Z",
    "updated_at": "2024-10-14T16:21:35Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nBentoML's `bentoml.pytorch.save_model` function is currently **not compatible with saving quantized PyTorch models (INT8) or less**. When attempting to save a quantized model, it raises errors related to the non-picklable nature of quantized tensors in PyTorch.\r\n\n\n### Motivation\n\n\r\nI temporal solution is introducing **picklable version** of the quantized model saving function for the Pytorch model. Now, developers can save and reload quantized PyTorch models in BentoML using the `bentoml.picklable_model.save_model` function.\r\n\r\n### **What was added:**\r\n- Introduced a picklable model-saving approach using:\r\n   ```python\r\n   bento_model = bentoml.picklable_model.save_model('model', model)\r\n   ```\r\n- This ensures that **quantized models (such as INT8 models)** can be saved without errors, preserving the quantization process and model architecture.\r\n  \r\n### **Why this improvement is important:**\r\n- Enables BentoML users to **save and deploy quantized PyTorch models** (such as INT8), which were previously not supported.\r\n- Simplifies the workflow for developers needing to deploy quantized models without modifying their existing pipelines.\r\n\r\n### **Testing:**\r\n- Verified the new functionality by saving and loading quantized INT8 models, ensuring that the model states and inference remain intact.\r\n- Conducted tests on various PyTorch quantized models (both statically and dynamically quantized) to validate the picklable saving and loading process.\r\n\r\n Looking forward to feedback and further suggestions for improvement!\r\n\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "jianshen92",
        "body": "Hello @sebasmos , thanks for raising the feature request.\r\n\r\nAs of BentoML 1.2, the idiomatic way to save PyTorch models is to use the `bentoml.models` API.\r\nHere's the [official documentation](https://docs.bentoml.org/en/latest/guides/model-store.html)\r\nTo be specific, you should use `bentoml.models.create` to save your quantised PyTorch model.\r\n\r\nFor example\r\n```python\r\nimport bentoml\r\nimport cloudpickle\r\n\r\nmodel = ...\r\n\r\nwith bentoml.models.create(name=\"new-pytorch-model\") as model_ref:\r\n    # Copy the PyTorch model to the model directory\r\n    model_path = model_ref.path_of(\"model.pth\")\r\n    torch.save(model, model_path, pickle_module=cloudpickle)\r\n```\r\n\r\nMoving forward, we will raise deprecation warnings for general frameworks support such as PyTorch and TensorFlow."
      },
      {
        "user": "sebasmos",
        "body": "> Hello @sebasmos , thanks for raising the feature request.\r\n> \r\n> As of BentoML 1.2, the idiomatic way to save PyTorch models is to use the `bentoml.models` API. Here's the [official documentation](https://docs.bentoml.org/en/latest/guides/model-store.html) To be specific, you should use `bentoml.models.create` to save your quantised PyTorch model.\r\n> \r\n> For example\r\n> \r\n> ```python\r\n> import bentoml\r\n> import cloudpickle\r\n> \r\n> model = ...\r\n> \r\n> with bentoml.models.create(name=\"new-pytorch-model\") as model_ref:\r\n>     # Copy the PyTorch model to the model directory\r\n>     model_path = model_ref.path_of(\"model.pth\")\r\n>     torch.save(model, model_path, pickle_module=cloudpickle)\r\n> ```\r\n> \r\n> Moving forward, we will raise deprecation warnings for general frameworks support such as PyTorch and TensorFlow.\r\n\r\nThanks @aarnphm ! I noticed `custom_objects` is [deprecated](https://docs.bentoml.org/en/latest/reference/stores.html#bentoml.models.create:~:text=bentoml.models.create) with `bentoml.models.create`. I’ve managed to manually define everything using custom pickle files, but am I overlooking something? With `bentoml.picklable_model.save_model`, adding custom objects as a dict was straightforward."
      },
      {
        "user": "aarnphm",
        "body": "you can just use cloudpickle directly in conjunction with `bentoml.models.create`."
      }
    ]
  },
  {
    "issue_number": 5006,
    "title": "bug: Unable to upload audio file with Content-Type: audio/mpeg in io-descriptors example",
    "author": "e1ijah1",
    "state": "closed",
    "created_at": "2024-09-29T07:01:59Z",
    "updated_at": "2024-10-05T02:28:39Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nWhen using the `AudioSpeedUp` example from the io-descriptors folder, uploading an audio file fails with both `Content-Type: audio/mpeg` and without a specified Content-Type. This indicates a problem with audio file handling in the io-descriptors implementation.\r\n\r\n### To reproduce\r\n\r\n1. Clone the BentoML repository and navigate to the examples/io-descriptors folder.\r\n2. Start the `AudioSpeedUp` service:\r\n```bash\r\nbentoml serve service:AudioSpeedUp\r\n```\r\n\r\n3. Attempt to upload an audio file using the following curl command:\r\n```bash\r\ncurl -i -XPOST 127.0.0.1:3000/speed_up_audio -H 'Content-Type: audio/mpeg' -F 'audio=@/path/to/audio.mp3'\r\n```\r\n\r\nThe server returns a `500` status code with the following traceback:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/infer_dev/lib/python3.10/site-packages/_bentoml_impl/server/app.py\", line 562, in api_endpoint_wrapper\r\n    resp = await self.api_endpoint(name, request)\r\n  File \"/root/miniconda3/envs/infer_dev/lib/python3.10/site-packages/_bentoml_impl/server/app.py\", line 621, in api_endpoint\r\n    serde = ALL_SERDE[media_type]()\r\nKeyError: 'audio/mpeg'\r\n```\r\n\r\nIf the Content-Type header is omitted, a different error occurs:\r\n```bash\r\ncurl -i -XPOST 127.0.0.1:3000/speed_up_audio -F 'audio=@/path/to/audio.mp3'\r\n```\r\n\r\nThis results in a 400 error with the following traceback:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/infer_dev/lib/python3.10/site-packages/_bentoml_impl/server/app.py\", line 562, in api_endpoint_wrapper\r\n    resp = await self.api_endpoint(name, request)\r\n  File \"/root/miniconda3/envs/infer_dev/lib/python3.10/site-packages/_bentoml_impl/server/app.py\", line 622, in api_endpoint\r\n    input_data = await method.input_spec.from_http_request(request, serde)\r\n  File \"/root/miniconda3/envs/infer_dev/lib/python3.10/site-packages/_bentoml_sdk/io_models.py\", line 171, in from_http_request\r\n    return await serde.parse_request(request, t.cast(t.Type[IODescriptor], cls))\r\n  File \"/root/miniconda3/envs/infer_dev/lib/python3.10/site-packages/_bentoml_impl/serde.py\", line 205, in parse_request\r\n    return cls.model_validate(data)\r\n  File \"/root/miniconda3/envs/infer_dev/lib/python3.10/site-packages/pydantic/main.py\", line 532, in model_validate\r\n    return cls.__pydantic_validator__.validate_python(\r\npydantic_core._pydantic_core.ValidationError: 2 validation errors for Input\r\naudio\r\n  Value error, Invalid content type application/octet-stream, expected audio/mpeg [type=value_error, input_value=UploadFile(filename='audio...ication/octet-stream'})), input_type=UploadFile]\r\n    For further information visit https://errors.pydantic.dev/2.7/v/value_error\r\nvelocity\r\n  Field required [type=missing, input_value={'audio': UploadFile(file...cation/octet-stream'}))}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.7/v/missing\r\n   ```\r\n   \r\n   This issue seems to be related to a lack of proper validation for audio file uploads in BentoML. A similar issue has been reported previously: https://github.com/bentoml/BentoML/issues/4651\r\n\r\n### Expected behavior\r\n\r\nThe service should accept the audio file and process it as intended.\r\n\r\n### Environment\r\n\r\nbentoml==1.3.5\r\n\r\nPython 3.10.14\r\n\r\nPlatform: CentOS 7",
    "comments": [
      {
        "user": "parano",
        "body": "The client CURL command for that example service should be:\r\n\r\n```bash\r\ncurl -X 'POST' \\\r\n  'http://localhost:3000/speed_up_audio' \\\r\n  -H 'accept: audio/mp3' \\\r\n  -H 'Content-Type: multipart/form-data' \\\r\n  -F 'audio=@example.mp3;type=audio/mpeg' \\\r\n  -F 'velocity=2' \\\r\n  -o output.mp3\r\n```\r\n\r\nIf you're not sure what's the API specification, try visit `http://localhost:3000/` to send request with the built-in OpenAPI UI, or download the OpenAPI spec json from `http://localhost:3000/docs.json`.\r\n\r\n<img width=\"1282\" alt=\"Screenshot 2024-09-29 at 9 31 54 PM\" src=\"https://github.com/user-attachments/assets/437b7d14-de1a-4857-a2f3-a0ed8dc59520\">\r\n\r\n\r\n\r\nBentoML's built-in Python client is also very easy to use. The client call simply matches the API function definition in the service:\r\n\r\n```python\r\nimport bentoml\r\nfrom pathlib import Path\r\n\r\nwith bentoml.SyncHTTPClient(\"http://localhost:3000\") as client:\r\n    result = client.speed_up_audio(\r\n        audio=Path(\"example.mp3\"),\r\n       velocity=2,\r\n    )\r\n```"
      }
    ]
  },
  {
    "issue_number": 4974,
    "title": "bug: Regression with handling of numpy input types in a class that inherits from pydantic.BaseModel",
    "author": "adamliter",
    "state": "closed",
    "created_at": "2024-09-12T22:27:00Z",
    "updated_at": "2024-09-17T19:48:09Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nThe following code works fine with `bentoml==1.3.3` and `pydantic<2.9`:\r\n\r\n```python\r\nfrom typing import Annotated, Any\r\n\r\nimport bentoml\r\nimport numpy as np\r\nfrom pydantic import BaseModel, Field\r\nfrom bentoml.validators import DType, Shape\r\n\r\n\r\nclass MyModelInput(BaseModel):\r\n    data: Annotated[np.ndarray, Shape((-1, 2)), DType(\"float32\")] = Field(\r\n        default=[[[1, 2], [2, 4]]], description=\"A (-1, 2) np.ndarray\"\r\n    )\r\n\r\n\r\n@bentoml.service\r\nclass MyModelService:\r\n    @bentoml.api(input_spec=MyModelInput)\r\n    def predict(self, **params: Any) -> str:\r\n        return \"model prediction\"\r\n```\r\n\r\nWith `bentoml==1.3.3` and `pydantic>=2.9`, `bentoml serve .` gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/adamliter/git/bentoml-numpy-test/.venv/bin/bentoml\", line 5, in <module>\r\n    from bentoml_cli.cli import cli\r\n  File \"/Users/adamliter/git/bentoml-numpy-test/.venv/lib/python3.11/site-packages/bentoml_cli/cli.py\", line 59, in <module>\r\n    cli = create_bentoml_cli()\r\n          ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/adamliter/git/bentoml-numpy-test/.venv/lib/python3.11/site-packages/bentoml_cli/cli.py\", line 8, in create_bentoml_cli\r\n    from bentoml._internal.configuration import BENTOML_VERSION\r\n  File \"/Users/adamliter/git/bentoml-numpy-test/.venv/lib/python3.11/site-packages/bentoml/__init__.py\", line 210, in <module>\r\n    import _bentoml_sdk\r\n  File \"/Users/adamliter/git/bentoml-numpy-test/.venv/lib/python3.11/site-packages/_bentoml_sdk/__init__.py\", line 11, in <module>\r\n    add_custom_preparers()\r\n  File \"/Users/adamliter/git/bentoml-numpy-test/.venv/lib/python3.11/site-packages/_bentoml_sdk/_pydantic.py\", line 194, in add_custom_preparers\r\n    *_std_types_schema.PREPARE_METHODS,\r\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: module 'pydantic._internal._std_types_schema' has no attribute 'PREPARE_METHODS'\r\n```\r\n\r\nWith `bentoml==1.3.5` and `pydantic<2.9`, you get this a different (and very long error). The error being raised is:\r\n\r\n```\r\nraise PydanticSchemaGenerationError(\r\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'numpy.ndarray'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\r\n```\r\n\r\nWith `bentoml==1.3.5` and `pydantic>=2.9`, you get the same error as you get with `bentoml==1.3.5` and `pydantic<2.9`.\r\n\r\nIt also seems to be an issue when passing in the subclass of `BaseModel` to the method directly, not just through the `@bentoml.api` decorator:\r\n\r\n```python\r\nfrom typing import Annotated\r\n\r\nimport bentoml\r\nimport numpy as np\r\nfrom pydantic import BaseModel, Field\r\nfrom bentoml.validators import DType, Shape\r\n\r\n\r\nclass MyModelInput(BaseModel):\r\n    data: Annotated[np.ndarray, Shape((-1, 2)), DType(\"float32\")] = Field(\r\n        default=[[[1, 2], [2, 4]]], description=\"A (-1, 2) np.ndarray\"\r\n    )\r\n\r\n\r\n@bentoml.service\r\nclass MyModelService:\r\n    @bentoml.api()\r\n    def predict(self, data: MyModelInput) -> str:\r\n        return \"model prediction\"\r\n```\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.3.5\r\n`python`: 3.11.10\r\n`platform`: macOS-14.6.1-arm64-arm-64bit\r\n`uid_gid`: 501:20\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiohappyeyeballs==2.4.0\r\naiohttp==3.10.5\r\naiosignal==1.3.1\r\naiosqlite==0.20.0\r\nannotated-types==0.7.0\r\nanyio==4.4.0\r\nappdirs==1.4.4\r\nasgiref==3.8.1\r\nattrs==24.2.0\r\nbentoml==1.3.5\r\n-e file:///Users/adamliter/git/bentoml-numpy-test\r\ncattrs==23.1.2\r\ncertifi==2024.8.30\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==3.0.0\r\ndeepmerge==2.0\r\ndeprecated==1.2.14\r\nfrozenlist==1.4.1\r\nfs==2.4.16\r\nh11==0.14.0\r\nhttpcore==1.0.5\r\nhttpx==0.27.2\r\nhttpx-ws==0.6.0\r\nidna==3.8\r\nimportlib-metadata==6.11.0\r\ninflection==0.5.1\r\ninquirerpy==0.3.4\r\njinja2==3.1.4\r\nmarkdown-it-py==3.0.0\r\nmarkupsafe==2.1.5\r\nmdurl==0.1.2\r\nmultidict==6.1.0\r\nnumpy==2.1.1\r\nnvidia-ml-py==11.525.150\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\npackaging==24.1\r\npathspec==0.12.1\r\npfzy==0.3.4\r\npip-requirements-parser==32.0.1\r\nprometheus-client==0.20.0\r\nprompt-toolkit==3.0.47\r\npsutil==6.0.0\r\npydantic==2.8.2\r\npydantic-core==2.20.1\r\npygments==2.18.0\r\npyparsing==3.1.4\r\npython-dateutil==2.9.0.post0\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.9\r\npyyaml==6.0.2\r\npyzmq==26.2.0\r\nrich==13.8.1\r\nruff==0.6.4\r\nschema==0.7.7\r\nsetuptools==74.1.2\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.1\r\nstarlette==0.38.5\r\ntomli-w==1.0.0\r\ntornado==6.4.1\r\ntyping-extensions==4.12.2\r\nuv==0.4.9\r\nuvicorn==0.30.6\r\nwatchfiles==0.24.0\r\nwcwidth==0.2.13\r\nwrapt==1.16.0\r\nwsproto==1.2.0\r\nyarl==1.11.1\r\nzipp==3.20.1\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "frostming",
        "body": "The previous behavior is to **implicitly** patch pydantic to serialize numpy arrays which are not supported by `pydantic.BaseModel`. That is not optimal and may lead to [suprises](https://github.com/bentoml/BentoML/issues/4728)\r\n\r\nSo in the new version, users should use `bentoml.IODescriptor` instead of `pydantic.BaseModel` if there exist non-standard data types in field annotations:\r\n\r\n\r\n```python\r\nclass MyModelInput(bentoml.IODescriptor):\r\n    data: Annotated[np.ndarray, Shape((-1, 2)), DType(\"float32\")] = Field(\r\n        default=[[[1, 2], [2, 4]]], description=\"A (-1, 2) np.ndarray\"\r\n    )\r\n```\r\n\r\n@adamliter tell me if that works for you"
      },
      {
        "user": "adamliter",
        "body": "Hi @frostming yes, that works. Thank you! ~Maybe this issue should just be a documentation request then? I don't even really see `bentoml.IODescriptor` in the current docs when looking at the API reference section, let alone the Guides section.~ Sorry, just seeing the linked PR for the docs now. Thanks again! 😄 "
      },
      {
        "user": "lucasew",
        "body": "Can replicate this on latest version available in Conda"
      }
    ]
  },
  {
    "issue_number": 4971,
    "title": "bug: Intermittent httpx.RemoteProtocolError: Server disconnected without sending a response.",
    "author": "shihgianlee",
    "state": "open",
    "created_at": "2024-09-11T17:23:38Z",
    "updated_at": "2024-09-12T16:09:41Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nThe error occurs intermittently, approximately once every 5,000 requests. It appears to be a well-known httpx [issue](https://github.com/encode/httpx/discussions/2056).\r\n\r\nWe increased the timeout from 15 to 30 seconds but are still encountering the error. From our observations, it seems the timeout setting in BentoML is not being propagated to the httpx client. A [user](https://github.com/encode/httpx/discussions/2056#discussioncomment-2471687) increased the timeout on the service and client, which appeared to resolve the issue.\r\n\r\nSee stacktrace below:\r\n```\r\nhttpx.RemoteProtocolError: Server disconnected without sending a response.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/httpx/_transports/default.py\", line 72, in map_httpcore_exceptions\r\n    yield\r\n  File \"/usr/local/lib/python3.9/site-packages/httpx/_transports/default.py\", line 377, in handle_async_request\r\n    resp = await self._pool.handle_async_request(req)\r\n  File \"/usr/local/lib/python3.9/site-packages/httpcore/_async/connection_pool.py\", line 216, in handle_async_request\r\n    raise exc from None\r\n  File \"/usr/local/lib/python3.9/site-packages/httpcore/_async/connection_pool.py\", line 196, in handle_async_request\r\n    response = await connection.handle_async_request(\r\n  File \"/usr/local/lib/python3.9/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\r\n    return await self._connection.handle_async_request(request)\r\n  File \"/usr/local/lib/python3.9/site-packages/httpcore/_async/http11.py\", line 143, in handle_async_request\r\n    raise exc\r\n  File \"/usr/local/lib/python3.9/site-packages/httpcore/_async/http11.py\", line 113, in handle_async_request\r\n    ) = await self._receive_response_headers(**kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/httpcore/_async/http11.py\", line 186, in _receive_response_headers\r\n    event = await self._receive_event(timeout=timeout)\r\n  File \"/usr/local/lib/python3.9/site-packages/httpcore/_async/http11.py\", line 238, in _receive_event\r\n    raise RemoteProtocolError(msg)\r\nhttpcore.RemoteProtocolError: Server disconnected without sending a response.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/ddtrace/contrib/httpx/patch.py\", line 142, in _wrapped_async_send\r\n    resp = await wrapped(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/httpx/_client.py\", line 1674, in send\r\n    response = await self._send_handling_auth(\r\n  File \"/usr/local/lib/python3.9/site-packages/httpx/_client.py\", line 1702, in _send_handling_auth\r\n    response = await self._send_handling_redirects(\r\n  File \"/usr/local/lib/python3.9/site-packages/httpx/_client.py\", line 1739, in _send_handling_redirects\r\n    response = await self._send_single_request(request)\r\n  File \"/usr/local/lib/python3.9/site-packages/httpx/_client.py\", line 1776, in _send_single_request\r\n    response = await transport.handle_async_request(request)\r\n  File \"/usr/local/lib/python3.9/site-packages/httpx/_transports/default.py\", line 377, in handle_async_request\r\n    resp = await self._pool.handle_async_request(req)\r\n  File \"/usr/local/lib/python3.9/contextlib.py\", line 137, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/usr/local/lib/python3.9/site-packages/httpx/_transports/default.py\", line 89, in map_httpcore_exceptions\r\n    raise mapped_exc(message) from exc\r\nhttpx.RemoteProtocolError: Server disconnected without sending a response.\r\n```\r\n\r\n### To reproduce\r\n\r\nSee [httpx error reproduction](https://github.com/encode/httpx/discussions/2056#discussioncomment-2477597).\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=True\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=5000\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.3.2\r\n`python`: 3.9.20\r\n`platform`: Linux-6.1.85+-x86_64-with-glibc2.36\r\n`uid_gid`: 1034:1034\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiohappyeyeballs==2.4.0\r\naiohttp==3.10.5\r\naiosignal==1.3.1\r\naiosqlite==0.20.0\r\nannotated-types==0.7.0\r\nanyio==4.4.0\r\nappdirs==1.4.4\r\nasgiref==3.8.1\r\nasync-timeout==4.0.3\r\nattrs==24.2.0\r\nbentoml==1.3.2\r\nbowler==0.9.0\r\nbytecode==0.15.1\r\ncachetools==5.5.0\r\ncattrs==23.1.2\r\ncertifi==2024.8.30\r\ncharset-normalizer==3.3.2\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==3.0.0\r\ncolorama==0.4.6\r\ncontextlib2==21.6.0\r\ndask==2024.6.2\r\ndb-dtypes==1.3.0\r\nddsketch==3.0.1\r\nddtrace==1.20.15\r\ndeepmerge==2.0\r\ndeprecated==1.2.14\r\ndill==0.3.8\r\nenvier==0.5.2\r\nexceptiongroup==1.2.2\r\nfastapi==0.114.0\r\nfastavro==1.9.7\r\nfeast==0.36.0\r\nfissix==24.4.24\r\nfrozenlist==1.4.1\r\nfs==2.4.16\r\nfsspec==2023.12.2\r\ngoogle-api-core==2.19.2\r\ngoogle-auth==2.34.0\r\ngoogle-cloud-bigquery==3.12.0\r\ngoogle-cloud-bigquery-storage==2.26.0\r\ngoogle-cloud-bigtable==2.26.0\r\ngoogle-cloud-core==2.4.1\r\ngoogle-cloud-datastore==2.20.1\r\ngoogle-cloud-pubsub==2.21.0\r\ngoogle-cloud-secret-manager==2.16.1\r\ngoogle-cloud-storage==2.18.2\r\ngoogle-crc32c==1.6.0\r\ngoogle-resumable-media==2.7.2\r\ngoogleapis-common-protos==1.65.0\r\ngreenlet==3.1.0\r\ngrpc-google-iam-v1==0.13.1\r\ngrpcio==1.66.1\r\ngrpcio-health-checking==1.62.3\r\ngrpcio-reflection==1.62.3\r\ngrpcio-status==1.62.3\r\ngrpcio-tools==1.62.3\r\ngunicorn==23.0.0\r\nh11==0.14.0\r\nhttpcore==1.0.5\r\nhttptools==0.6.1\r\nhttpx==0.27.2\r\nhttpx-ws==0.6.0\r\nidna==3.8\r\nimportlib-metadata==6.11.0\r\nimportlib-resources==6.4.5\r\ninflection==0.5.1\r\niniconfig==2.0.0\r\ninquirerpy==0.3.4\r\njinja2==3.1.4\r\njoblib==1.4.2\r\njsonschema==4.23.0\r\njsonschema-specifications==2023.12.1\r\nlimits==3.13.0\r\nlocket==1.0.0\r\nmarkdown-it-py==3.0.0\r\nmarkupsafe==2.1.5\r\nmdurl==0.1.2\r\nmmh3==4.1.0\r\nmoreorless==0.4.0\r\nmultidict==6.1.0\r\nmypy==1.11.2\r\nmypy-extensions==1.0.0\r\nmypy-protobuf==3.1.0\r\nnumpy==1.24.4\r\nnvidia-ml-py==11.525.150\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\npackaging==24.1\r\npandas==1.5.3\r\npandavro==1.5.2\r\npartd==1.4.2\r\npathspec==0.12.1\r\npfzy==0.3.4\r\npip==23.0.1\r\npip-requirements-parser==32.0.1\r\npluggy==1.5.0\r\nprometheus-client==0.20.0\r\nprompt-toolkit==3.0.47\r\nproto-plus==1.24.0\r\nprotobuf==4.23.3\r\npsutil==6.0.0\r\npy==1.11.0\r\npyarrow==17.0.0\r\npyasn1==0.6.0\r\npyasn1-modules==0.4.0\r\npydantic==2.8.2\r\npydantic-core==2.20.1\r\npygments==2.18.0\r\npylogbeat==2.0.1\r\npyparsing==3.1.4\r\npytest==7.1.3\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\npython-json-logger==2.0.7\r\npython-logstash-async==2.6.0\r\npython-multipart==0.0.9\r\npython-service-common==0.3.1\r\npytz==2024.1\r\npyyaml==6.0.2\r\npyzmq==26.2.0\r\nreferencing==0.35.1\r\nrequests==2.32.3\r\nrich==13.8.1\r\nrpds-py==0.20.0\r\nrsa==4.9\r\nschema==0.7.5\r\nscikit-learn==0.24.2\r\nscipy==1.13.1\r\nsetuptools==74.1.2\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.1\r\nsqlalchemy==1.4.54\r\nsqlalchemy2-stubs==0.0.2a38\r\nstarlette==0.38.5\r\ntabulate==0.9.0\r\ntenacity==8.5.0\r\nthreadpoolctl==3.5.0\r\ntoml==0.10.2\r\ntomli==2.0.1\r\ntomli-w==1.0.0\r\ntoolz==0.12.1\r\ntornado==6.4.1\r\ntqdm==4.66.5\r\ntypeguard==4.3.0\r\ntypes-protobuf==5.27.0.20240907\r\ntyping-extensions==4.12.2\r\nurllib3==2.2.2\r\nuv==0.4.8\r\nuvicorn==0.30.6\r\nuvloop==0.20.0\r\nvolatile==2.1.0\r\nwatchfiles==0.24.0\r\nwcwidth==0.2.13\r\nwebsockets==13.0.1\r\nwheel==0.44.0\r\nwrapt==1.16.0\r\nwsproto==1.2.0\r\nxgboost==1.6.1\r\nxmltodict==0.13.0\r\nyarl==1.11.1\r\nzipp==3.20.1\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "parano",
        "body": "Hi @shihgianlee, thanks for reporting the issue. Could you share more about your service code structure and where the timeout value was set?"
      },
      {
        "user": "shihgianlee",
        "body": "> Hi @shihgianlee, thanks for reporting the issue. Could you share more about your service code structure and where the timeout value was set?\r\n\r\nSure. We have 2 services defined where service A depends on service B. We have adaptive batching turned on in service B. Also, we have async defined in service A. However, we are getting intermittent Server disconnected without sending a response errors when calling service B from service A in our production Kubernetes cluster.  \r\n\r\n```python\r\n@bentoml.service(\r\n    traffic={\"timeout\": 30},\r\n    workers=1,\r\n    logging={\r\n        \"access\": {\r\n            \"enabled\": False,\r\n        }\r\n    }\r\n)\r\nclass AService:\r\n    b_service = bentoml.depends(BService)\r\n\r\n   ...\r\n\r\n    @bentoml.api(input_spec=InputFeaturesV2, route=\"/v2/predict\")\r\n    async def predict_v2(self, ctx: bentoml.Context, **params: t.Any):\r\n        input_data = InputFeaturesV2(**params)\r\n        try:\r\n           results = await self.b_service.to_async.predict([features_processed])\r\n           ...\r\n```\r\n\r\n```python\r\n@bentoml.service(\r\n    traffic={\"timeout\": 30},\r\n    workers=1,\r\n    logging={\r\n        \"access\": {\r\n            \"enabled\": False,\r\n        }\r\n    }\r\n)\r\nclass BService:\r\n\r\n      ...\r\n\r\n    @bentoml.api(batchable=True)\r\n    def predict(self, input_list: list[Features]):\r\n        features = pd.DataFrame(input_list)\r\n        try:\r\n            predictions = self.predict(features)\r\n            return predictions\r\n        except Exception:\r\n            logger.exception(\"Unexpected exception caught\")\r\n            raise\r\n```\r\n"
      }
    ]
  },
  {
    "issue_number": 4964,
    "title": "bug: Error run production bentoml with grpc",
    "author": "jtong99",
    "state": "closed",
    "created_at": "2024-09-10T02:55:53Z",
    "updated_at": "2024-09-10T07:45:47Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nI am deploying a bentoml service with gRPC, it worked normally some weeks ago, but now I am facing an unexpected error. \r\n\r\nNow I am facing this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/bentoml\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/usr/local/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml_cli/utils.py\", line 359, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml_cli/utils.py\", line 330, in wrapper\r\n    return_value = func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml_cli/env_manager.py\", line 123, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml_cli/serve.py\", line 529, in serve_grpc\r\n    serve_grpc_production(\r\n  File \"/usr/local/lib/python3.8/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml/serving.py\", line 687, in serve_grpc_production\r\n    loop.run_until_complete(on_service_deployment(svc))\r\n  File \"/usr/local/lib/python3.8/asyncio/base_events.py\", line 595, in run_until_complete\r\n    future = tasks.ensure_future(future, loop=self)\r\n  File \"/usr/local/lib/python3.8/asyncio/tasks.py\", line 684, in ensure_future\r\n    raise TypeError('An asyncio.Future, a coroutine or an awaitable is '\r\nTypeError: An asyncio.Future, a coroutine or an awaitable is required\r\n```\r\n\r\nIt worked normally in HTTP but the error will happen when I run with gRPC.\n\n### To reproduce\n\nThis is how I build and run:\r\n\r\n1. build: `bentoml build`\r\n2. containerize: `bentoml containerize --enable-features=grpc`\r\n3. docker run with `serve-grpc`\n\n### Expected behavior\n\nIt worked with gRPC\n\n### Environment\n\nbentoml: 1.1.6\r\npython: 3.8\r\nplatform: Ubuntu 21",
    "comments": [
      {
        "user": "jtong99",
        "body": "@frostming Thanks for the fix. Previously, I installed the Bentoml from PIP, do I need to re-install Bentoml from Git to apply this PR?"
      },
      {
        "user": "frostming",
        "body": "@jtong99 it's available on PyPI now, just upgrade it."
      }
    ]
  },
  {
    "issue_number": 4885,
    "title": "bug: env variables not injected in bento container",
    "author": "rlleshi",
    "state": "open",
    "created_at": "2024-07-28T17:52:08Z",
    "updated_at": "2024-09-04T13:00:01Z",
    "labels": [
      "feedback-wanted"
    ],
    "body": "### Describe the bug\n\nAs per the docs [here](https://docs.bentoml.org/en/latest/guides/build-options.html#envs), env variables should be injected in the resulting docker container. However, these variables are not actually being injected and are not available inside the container at runtime.\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\nEnv variables defined via the bentofile should be injected inside the container. \r\n\r\nAlthough it appears that they are actually injected when defined inside the `env` attribute of the `docker` attribute inside the bentofile.\r\n\r\nIs the documentation perhaps outdated?\n\n### Environment\n\nbentoml: 1.2.16\r\npython: 3.9.0\r\nplatform: ubuntu 22.04",
    "comments": [
      {
        "user": "aarnphm",
        "body": "I can't seem to reproduce this with bentoml 1.3\r\n<img width=\"1008\" alt=\"Screenshot 2024-07-28 at 16 14 54\" src=\"https://github.com/user-attachments/assets/ddccee81-c3ad-41cc-9a25-95bf6458998b\">\r\n\r\nAs you can see here, the env `HF_TOKEN` is injected correctly from `bentofile.yaml`:\r\n\r\n```yaml\r\nservice: 'service:VLLM'\r\nlabels:\r\n  owner: bentoml-team\r\n  stage: demo\r\ninclude:\r\n  - '*.py'\r\n  - 'bentovllm_openai/*.py'\r\npython:\r\n  requirements_txt: './requirements.txt'\r\n  lock_packages: false\r\nenvs:\r\n  - name: HF_TOKEN\r\ndocker:\r\n  python_version: 3.11\r\n```"
      },
      {
        "user": "rlleshi",
        "body": "Hi @aarnphm this seems to be happening when you define the `docker.env` field. \r\n\r\nSo if you have a different set of variables on `envs` and you define another set of variables on `docker.env`, then the variables of `envs` are not taken into account.\r\n\r\n```\r\nservice: \"service:TestService\"\r\ninclude:\r\n  - \"*.py\"\r\nenvs:\r\n  - name: \"TEST_ENV\"\r\n    value: \"test\"\r\ndocker:\r\n  distro: debian\r\n  python_version: \"3.10\"\r\n  env:\r\n    TEST_ENV_2: \"test\"\r\n```\r\n\r\n`TEST_ENV` won't be available during runtime.\r\n"
      },
      {
        "user": "aarnphm",
        "body": "@frostming iirc the `docker.env` will be deprecated soon in light of `envs`? \r\n\r\nShould we print out a warning then for now just merge the docker env to the above env?"
      }
    ]
  },
  {
    "issue_number": 4678,
    "title": "bug: bentoml serve error pynvml.nvml.NVMLError_DriverNotLoaded: Driver Not Loaded",
    "author": "TomRivero",
    "state": "closed",
    "created_at": "2024-04-18T09:31:49Z",
    "updated_at": "2024-08-28T01:58:09Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWhen running `bentoml serve` after building my bento I get the following error: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/bin/bentoml\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml_cli/utils.py\", line 362, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml_cli/utils.py\", line 333, in wrapper\r\n    return_value = func(*args, **kwargs)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/click/decorators.py\", line 33, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml_cli/utils.py\", line 290, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml_cli/env_manager.py\", line 122, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml_cli/serve.py\", line 260, in serve\r\n    serve_http_production(\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml/serve.py\", line 327, in serve_http_production\r\n    json.dumps(runner.scheduled_worker_env_map),\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml/_internal/runner/runner.py\", line 356, in scheduled_worker_env_map\r\n    for worker_id in range(self.scheduled_worker_count)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml/_internal/runner/runner.py\", line 341, in scheduled_worker_count\r\n    return self.scheduling_strategy.get_worker_count(\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml/_internal/runner/strategy.py\", line 68, in get_worker_count\r\n    resource_request = system_resources()\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml/_internal/resource.py\", line 46, in system_resources\r\n    res[resource_kind] = resource.from_system()\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/bentoml/_internal/resource.py\", line 248, in from_system\r\n    pynvml.nvmlInit()\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/pynvml/nvml.py\", line 1770, in nvmlInit\r\n    nvmlInitWithFlags(0)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/pynvml/nvml.py\", line 1760, in nvmlInitWithFlags\r\n    _nvmlCheckReturn(ret)\r\n  File \"/home/tom/Desktop/ml-reconciliation/venv/lib/python3.10/site-packages/pynvml/nvml.py\", line 833, in _nvmlCheckReturn\r\n    raise NVMLError(ret)\r\npynvml.nvml.NVMLError_DriverNotLoaded: Driver Not Loaded\r\n```\n\n### To reproduce\n\n`bentoml serve`\n\n### Expected behavior\n\n`bentoml serve` running without error\n\n### Environment\n\nBentoML: 1.1.11\r\nPython: 3.10\r\ntorch: 2.2.1\r\nUbuntu: 22.04\r\nno Nvidia GPU",
    "comments": [
      {
        "user": "akashlp27",
        "body": "Hi, any updates with this issue?\r\n"
      },
      {
        "user": "TomRivero",
        "body": "I tried to rerun it today, still the same issue"
      }
    ]
  },
  {
    "issue_number": 4902,
    "title": "feature: add to documentation how to mock services and api endpoints",
    "author": "smidm",
    "state": "open",
    "created_at": "2024-08-06T20:55:17Z",
    "updated_at": "2024-08-27T13:34:34Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nI really appreciated https://docs.bentoml.com/en/latest/guides/testing.html#unit-tests. It would be helpful to include also docs on mocking the bentoml decorated API methods. \r\n\r\nThe api method defined at `package.module.Service.api_endpoint` is located after the application of `@bentoml.service` and `@bentoml.api` decorators at `package.module.Service.inner.api_endpoint.func`, which is not clear. \r\n\r\nI use:\r\n```\r\ndef test_sample(monkeypatch):\r\n    def mock_func():\r\n        return None\r\n\r\n    monkeypatch.setattr('package.module.Service.inner.api_endpoint.func', mock_func)\r\n```\r\n\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "It depends on how you test your API, can you show an example of testing function?"
      },
      {
        "user": "smidm",
        "body": "I'm mocking following:\r\n- package `pose_service` \r\n- module `pose` \r\n- @bentoml.service `MMPoseModel` \r\n- @bentoml.api endpoint `pose` \r\n\r\n```python\r\n    def mock_pose(self, image, persons):\r\n        return [\r\n            Person(\r\n                top_left=person.top_left,\r\n                bottom_right=person.bottom_right,\r\n                detection_confidence=person.detection_confidence,\r\n                keypoints=[],\r\n            ) for person in persons\r\n        ]\r\n\r\n    # bypass the pose estimation and return the detection results\r\n    monkeypatch.setattr('pose_service.pose.MMPoseModel.inner.pose.func', mock_pose)\r\n\r\n    sample_image = Image.new('RGB', (200, 100))  # width, height\r\n    service = poseservice()\r\n\r\n    # force both\r\n    img, rotation_ccw, persons = service.detect_pose_and_image_rotation(\r\n        sample_image,\r\n        rotation_ccw=90,\r\n        bounding_box=BoundingBox(\r\n            top_left=Coordinate(x=0.1, y=0.1),\r\n            bottom_right=Coordinate(x=0.2, y=0.2),\r\n        )\r\n    )\r\n    assert img.shape == (200, 100, 3)\r\n    assert rotation_ccw == 90\r\n    assert len(persons) == 1\r\n    assert isinstance(persons[0], Person)\r\n    assert persons[0].top_left == Coordinate(x=0.1, y=0.1)\r\n    assert persons[0].bottom_right == Coordinate(x=0.2, y=0.2)\r\n```"
      },
      {
        "user": "frostming",
        "body": "> ```python\r\n> service = poseservice()\r\n> ```\r\n\r\nWhat is `poseservice()` and how does it interact with the mocked service?"
      }
    ]
  },
  {
    "issue_number": 4702,
    "title": "Bug: 'bentoml containerize' doesn't include models in the image",
    "author": "mohsenim",
    "state": "closed",
    "created_at": "2024-05-03T08:34:51Z",
    "updated_at": "2024-08-27T13:31:47Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\n'bentoml containerize' doesn't include the model specified in 'bentofile.yaml' in the image. Running the command:\r\n\r\n```\r\ndocker run -it --rm -p 3000:3000 predictor:lkpxx2u5o24wpxjr serve\r\n```\r\n\r\nresults in this error:\r\n\r\n```\r\nbentoml.exceptions.NotFound: no Models with name 'my_model' exist in BentoML store <osfs '/home/bentoml/bento/models'>\r\n```\r\n\r\nI also checked inside the docker container and noticed the `models` folder is empty. However, when a runner is used in `service.py`, as shown in this [example](https://github.com/bentoml/BentoML/tree/main/examples/monitoring/task_classification), the model is automatically included in the image without needing to add it to 'benotfile.yaml'. \r\nCurrently, my workaround involves saving and loading the model myself and using the `include` option in 'benotfile.yaml' to ensure it's included in the image. But if this is the intended method, it raises questions about the purpose and usage of the BentoML Model Store.\n\n### To reproduce\n\n1- `service.py` looks like this:\r\n\r\n```\r\n@bentoml.service(\r\n    resources={\"cpu\": \"1\"},\r\n    traffic={\"timeout\": 10},\r\n)\r\nclass Predictor:\r\n    def __init__(self) -> None:\r\n        self.model = bentoml.sklearn.load_model('my_model:latest')\r\n\r\n   @bentoml.api\r\n    def predict(self, \r\n                      input: np.ndarray\r\n                     ) -> np.ndarray:\r\n        result = self.model.predict(input)\r\n        return result\r\n```\r\n\r\n2- `benofile.yaml` includes this option:\r\n\r\n```\r\nmodels:\r\n  - \"my_model:latest\"\r\n```\r\n\r\n3- `bentoml build` successfully builds the Bento\r\n\r\n4- `bentoml containerize summarization:latest` containerizes the Bento\r\n\r\n5- `docker run --rm -p 3000:3000 predictor:ep3yr7aiwgczsoaa`\r\n\r\n6- Any request to the service returns this error\r\n\r\n```\r\nbentoml.exceptions.NotFound: no Models with name 'my_model' exist in BentoML store <osfs '/home/bentoml/bento/models'>\r\n``` \n\n### Expected behavior\n\n_No response_\n\n### Environment\n\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.2.12\r\n`python`: 3.12.2\r\n`platform`: Linux-6.5.0-28-generic-x86_64-with-glibc2.35\r\n`uid_gid`: 1000:1000\r\n`conda`: 24.3.0\r\n`in_conda_env`: True\r\n",
    "comments": [
      {
        "user": "vmallya-123",
        "body": "I am facing a similar issue, how was this resolved? I have my model in the model store but it throws the same error"
      },
      {
        "user": "frostming",
        "body": "@vmallya-123 this is a legacy bug that has been resolved in previous versions.\r\n\r\nYou should share your configuration and what you observed in order for us to investigate."
      }
    ]
  },
  {
    "issue_number": 4936,
    "title": "bug: uv pip install results in \"Error no virtual environment found\"",
    "author": "poudrouxj",
    "state": "closed",
    "created_at": "2024-08-23T07:10:33Z",
    "updated_at": "2024-08-24T00:04:25Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWhen running `bentoml cointainerize` on a `bento` with a custom base image which was built using `bentoml==1.3.2` in a build environment with `bentoml==1.3.2` we receive the following error:\r\n```bash\r\n#11 [base-container 6/8] RUN --mount=type=cache,target=/root/.cache/uv bash -euxo pipefail /home/bentoml/bento/env/python/install.sh\r\n#11 0.054 + set -exuo pipefail\r\n#11 0.055 +++ dirname -- /home/bentoml/bento/env/python/install.sh\r\n#11 0.057 ++ cd -- /home/bentoml/bento/env/python\r\n#11 0.057 ++ pwd\r\n#11 0.058 + BASEDIR=/home/bentoml/bento/env/python\r\n#11 0.058 + PIP_ARGS=()\r\n#11 0.058 + REQUIREMENTS_TXT=/home/bentoml/bento/env/python/requirements.txt\r\n#11 0.058 + REQUIREMENTS_LOCK=/home/bentoml/bento/env/python/requirements.lock.txt\r\n#11 0.058 + WHEELS_DIR=/home/bentoml/bento/env/python/wheels\r\n#11 0.058 + BENTOML_VERSION=1.3.2\r\n#11 0.058 + pushd /home/bentoml/bento/env/python\r\n#11 0.059 + '[' -f /home/bentoml/bento/env/python/requirements.lock.txt ']'\r\n#11 0.059 + '[' -f /home/bentoml/bento/env/python/requirements.txt ']'\r\n#11 0.059 Installing pip packages from 'requirements.txt'..\r\n#11 0.059 + echo 'Installing pip packages from '\\''requirements.txt'\\''..'\r\n#11 0.059 + pip_install -r /home/bentoml/bento/env/python/requirements.txt\r\n#11 0.059 + command -v uv\r\n#11 0.059 + uv pip install -r /home/bentoml/bento/env/python/requirements.txt\r\n#11 0.199 error: No virtual environment found\r\n#11 ERROR: process \"/bin/sh -c bash -euxo pipefail /home/bentoml/bento/env/python/install.sh\" did not complete successfully: exit code: 2\r\n------\r\n > [base-container 6/8] RUN --mount=type=cache,target=/root/.cache/uv bash -euxo pipefail /home/bentoml/bento/env/python/install.sh:\r\n: No virtual environment found\r\n```\r\n\r\nPerhaps by adding a `--system`  as disclosed in https://github.com/astral-sh/uv/issues/1374#issuecomment-2218077141 will help? \r\n\r\nNote we're using a custom base image.\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml==1.3.2\r\nplatform=linux/amd64\r\n\r\n(We're running this in the CI on a github workflow runner)",
    "comments": [
      {
        "user": "frostming",
        "body": "You can set `ENV UV_SYSTEM_PYTHON=1` which is set on our built-in docker template:\r\n\r\nhttps://github.com/bentoml/BentoML/blob/1764c6945797d63a061b072888b7eb5dfb9030f7/src/bentoml/_internal/container/frontend/dockerfile/templates/base_debian.j2#L15"
      },
      {
        "user": "poudrouxj",
        "body": "> You can set `ENV UV_SYSTEM_PYTHON=1` which is set on our built-in docker template:\r\n> \r\n> https://github.com/bentoml/BentoML/blob/1764c6945797d63a061b072888b7eb5dfb9030f7/src/bentoml/_internal/container/frontend/dockerfile/templates/base_debian.j2#L15\r\n\r\nStill not working - no idea what to do @frostming , open issue again ?"
      },
      {
        "user": "frostming",
        "body": "> Still not working - no idea what to do @frostming , open issue again ?\r\n\r\nWhat does your Dockerfile look like? You can find it at `$BENTOML_HOME/bentos/$BENTO_NAME/$BENTO_VERSION/env/docker/Dockerfile`"
      }
    ]
  },
  {
    "issue_number": 4937,
    "title": "bug: Unable to create custom duration metrics with buckets",
    "author": "poudrouxj",
    "state": "closed",
    "created_at": "2024-08-23T07:14:36Z",
    "updated_at": "2024-08-23T08:10:16Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nCreating a bentoml service with this definition:\r\n```python\r\n@bentoml.service(\r\n    resources={\"cpu\":1},\r\n    metrics={\r\n    \"enabled\": True,\r\n    \"duration\": {\r\n        \"buckets\": [0.1, 0.2, 0.5, 1, 2, 5, 10, 15, 20, 30, 40]\r\n        }\r\n    }\r\n)\r\n```\r\n\r\nyields the following error when trying to run `bentoml.bentos.build`\r\n\r\n```bash\r\nbentoml.exceptions.BentoMLConfigException: Invalid configuration file was given:\r\nKey 'services' error:\r\nKey 'BoxReleaseOrderService' error:\r\nKey 'metrics' error:\r\nKey 'duration' error:\r\nKey 'buckets' error:\r\nOr(And(<class 'list'>, <function ensure_iterable_type.<locals>.v at 0x7f32e5a013a0>), None) did not validate [0.1, 0.2, 0.5, 1, 2, 5, 10, 15, 20, 30, 40]\r\nv([0.1, 0.2, 0.5, 1, 2, 5, 10, 15, 20, 30, 40]) should evaluate to True\r\nNone does not match [0.1, 0.2, 0.5, 1, 2, 5, 10, 15, 20, 30, 40]\r\n```\r\n\r\nHowever the following works:\r\n```python\r\n@bentoml.service(\r\n    resources={\"cpu\":1},\r\n    metrics={\r\n    \"enabled\": True,\r\n    \"duration\": {\r\n        \"min\" : 0.2,\r\n        \"max\" : 40.0,\r\n        \"factor\": 2.5\r\n        }\r\n    }\r\n)\r\n```\r\n\r\nI'm pretty sure its a typing issue https://github.com/bentoml/BentoML/blob/v1.3.2/src/_bentoml_sdk/service/config.py#L96 and it should be like:\r\n```python\r\nclass MetricDuration(TypedDict, total=False):\r\n    buckets: Annotated[Tuple[float, ...], Unique()]\r\n    min: Annotated[float, Gt(0)]\r\n    max: Annotated[float, Gt(0)]\r\n    factor: Annotated[float, Gt(1.0)]\r\n```\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml==1.3.2\r\npython=3.11\r\nplatform=linux/amd64",
    "comments": [
      {
        "user": "frostming",
        "body": "There is no typing issue, all numbers must be float:\r\n\r\n```python\r\n@bentoml.service(\r\n    resources={\"cpu\":1},\r\n    metrics={\r\n    \"enabled\": True,\r\n    \"duration\": {\r\n        \"buckets\": [0.1, 0.2, 0.5, 1., 2., 5., 10., 15., 20., 30., 40.]  # note the dot added\r\n        }\r\n    }\r\n)\r\n```\r\n"
      }
    ]
  },
  {
    "issue_number": 4926,
    "title": "bug: pydantic/numpy patching causes `Cannot interpret 'numpy.float16 | numpy.float32' as a data type`",
    "author": "jamt9000",
    "state": "closed",
    "created_at": "2024-08-20T10:53:00Z",
    "updated_at": "2024-08-23T06:09:06Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nImporting bentoml can cause errors in unrelated code when using `numpy.typing` annotations.\r\n\r\nThis seems to have the same underlying cause as https://github.com/bentoml/BentoML/issues/4728, due to bentoml monkey-patching the private pydantic `pydantic._internal._std_types_schema` in an unsafe way that is incompatible with general type annotations.\r\n\r\n### To reproduce\r\n\r\n```py\r\nfrom pydantic import BaseModel, ConfigDict\r\nfrom numpy.typing import NDArray\r\nimport numpy as np\r\n\r\n# Without the import there is no error\r\nimport bentoml\r\n\r\nclass MyModel(BaseModel):\r\n    vector: NDArray[np.float16 | np.float32]\r\n    model_config = ConfigDict(arbitrary_types_allowed=True)\r\n```\r\n\r\n```\r\nFile .venv/lib/python3.12/site-packages/_bentoml_sdk/_pydantic.py:50, in numpy_prepare_pydantic_annotations(source, annotations, config)\r\n     47     return None\r\n     49 args = get_args(source)\r\n---> 50 dtype = np.dtype(get_args(args[1])[0]).name if args else None\r\n     51 shape: tuple[int, ...] | None = None\r\n     53 _, other_annotations = _known_annotated_metadata.collect_known_metadata(annotations)\r\n\r\nTypeError: Cannot interpret 'numpy.float16 | numpy.float32' as a data type\r\n```\r\n\r\n### Expected behavior\r\n\r\nImporting bentoml shouldn't cause errors with unrelated pydantic code.\r\n\r\n### Environment\r\n\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.3.2\r\n`python`: 3.12.3\r\n`platform`: macOS-14.5-x86_64-i386-64bit\r\n`uid_gid`: 501:20\r\n\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\n\r\nbentoml==1.3.2\r\nnumpy==1.26.4\r\npydantic==2.8.2\r\npydantic-core==2.20.1\r\n```\r\n\r\n</details>",
    "comments": []
  },
  {
    "issue_number": 4931,
    "title": "feature: Pass torch_load_args to bentoml.torch.load_model",
    "author": "TokeReines",
    "state": "closed",
    "created_at": "2024-08-22T14:11:47Z",
    "updated_at": "2024-08-23T02:09:47Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nThere was the option of passing kwargs torch_load_args to bentoml.torch.load_model, such that I can pass `weights_only=False` and suppress pytorch warnings like the following:\r\n\r\n> FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n### Motivation\n\nRunning emsemble models means this warning is printed many many times on startup and clutters my terminal.\n\n### Other\n\nI have already made a PR here: https://github.com/bentoml/BentoML/pull/4930",
    "comments": []
  },
  {
    "issue_number": 4541,
    "title": "bug: Transforms API references a Transformers Guide that doesn't exist",
    "author": "ProVega",
    "state": "open",
    "created_at": "2024-02-29T05:58:50Z",
    "updated_at": "2024-08-20T21:04:48Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nI am new to Bento ML and trying to learn how to deploy a Hugging Face Transformer. I see various example and I want to learn more about the transformers.pipeline method (ex: What is task). I find this page:\r\n\r\nhttps://github.com/bentoml/BentoML/blob/main/docs/source/reference/frameworks/transformers.rst\r\nhttps://docs.bentoml.com/en/latest/reference/frameworks/transformers.html\r\n\r\nBut it just says this \"Please refer to Transformers guide for more information about how to use Hugging Face Transformers in BentoML.\" \r\n\r\nUnder Guides there is no such article and there is no link. I know I am a noob - I THINK it wants to point to here:\r\n\r\nhttps://huggingface.co/docs/transformers/quicktour\r\n\r\nIs that right?\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nN/A",
    "comments": [
      {
        "user": "cserpellZF",
        "body": "I don't think it really wants to point to huggingface's page, I think in fact there is a framework documentation missing in BentoML doc."
      }
    ]
  },
  {
    "issue_number": 4921,
    "title": "bug: model saving and loading does not work with Keras 3.x model format",
    "author": "rmarquis",
    "state": "closed",
    "created_at": "2024-08-16T14:58:23Z",
    "updated_at": "2024-08-17T13:42:46Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nSince Keras 3 (released Novembre 28, 2023), Keras model saving and loading API changed and now requires a filepath for the model ending iwith `.keras` as opposed to a folder. The `.keras` format is simply a compressed folder.\r\n\r\n### Error\r\n\r\nAs such, the `bentoml.keras.save_model` and `bentoml.keras.load_model` API are not working as expected anymore, since they try to save and load from a folder path.\r\n\r\n```\r\nValueError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=/tmp/tmp394vyky1bentoml_model_my_keras_model/.\r\n```\r\n\r\nAs far as I can tell, BentoML's Keras framework layer API have been broken since Keras 3 release.\r\n\r\n### Fix\r\n\r\nStarting with Keras 3.4.0 (released June 25, 2024), Keras allows to pass a new `zipped` boolean parameter which allows the older behaviour (see https://github.com/keras-team/keras/commit/e0eaac473791f0481c2b292492a4af81eef14153).\r\n\r\nThis can be used to restore the compatibility with BentoML when saving and loading Keras models.\r\n\r\n### To reproduce\r\n\r\nSave a keras model with BentoML in its model store, using a recent version of Keras (3.x) or Tensorflow.\r\n\r\n```\r\n    # Save the model using BentoML to its model store\r\n    # https://docs.bentoml.com/en/latest/reference/frameworks/keras.html#bentoml.keras.save_model\r\n    bentoml.keras.save_model(\r\n        \"my_keras_model\",\r\n        model,\r\n        include_optimizer=True,\r\n        custom_objects={\r\n            \"preprocess\": preprocess,\r\n            \"postprocess\": postprocess,\r\n        },\r\n    )\r\n``` \r\n\r\nTraceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/remy/my_project/src/train.py\", line 134, in <module>\r\n    main()\r\n  File \"/home/remy/my_project/src/train.py\", line 111, in main\r\n    bentoml.keras.save_model(\r\n  File \"/home/remy/my_project/.venv/lib/python3.11/site-packages/bentoml/_internal/frameworks/keras.py\", line 272, in save_model\r\n    model.save(bento_model.path, include_optimizer=include_optimizer, **kwargs)\r\n  File \"/home/remy/my_project/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/home/remy/my_project/.venv/lib/python3.11/site-packages/keras/src/saving/saving_api.py\", line 114, in save_model\r\n    raise ValueError(\r\nValueError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=/tmp/tmp394vyky1bentoml_model_my_keras_model/.\r\n```\r\n\r\n### Expected behavior\r\n\r\nModel is correctly saved with `bentoml.keras.save_model`.\r\n\r\n### Environment\r\n\r\nbentoml: 1.3.2\r\nkeras: 3.5.0 (transitively through tensorflow 2.17)\r\npython: 3.11\r\nos: Linux",
    "comments": []
  },
  {
    "issue_number": 4908,
    "title": "bug: What's the reason why this concurrency configuration hasn't taken effect",
    "author": "woshitieren",
    "state": "closed",
    "created_at": "2024-08-08T07:43:36Z",
    "updated_at": "2024-08-09T07:09:29Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\n![image](https://github.com/user-attachments/assets/2e08c79c-2c78-4da2-b8c6-8631247441a2)\r\nWhat is the reason why concurrency configuration thread testing is not effective\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\nConcurrent configuration enables concurrent processing of data\n\n### Environment\n\nbentoml",
    "comments": [
      {
        "user": "frostming",
        "body": "It's a config item to control the auto-scaling behavior, not to denote it can process 4 requests concurrently.\r\n\r\nDepending on the workload of a single request, raise the number of `workers` to achieve that."
      }
    ]
  },
  {
    "issue_number": 4900,
    "title": "bug: pip not found when using cuda based image with my own Dockerfile template",
    "author": "shanirosen-airis",
    "state": "closed",
    "created_at": "2024-08-06T11:08:39Z",
    "updated_at": "2024-08-07T09:59:54Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nHi there,\r\nwhen creating a new bento service with cuda (any cuda version) in the new version (1.3.1), my bentoml build started to fail on:\r\n`pip not found` when trying to run the pip install here in my dockerfile tmeplate:\r\n\r\n```\r\n{% extends bento_base_template %}\r\n{% block SETUP_BENTO_BASE_IMAGE %}\r\nARG CODEARTIFACT_USER\r\n{{ super() }}\r\n\r\nARG CODEARTIFACT_USER\r\nENV CODEARTIFACT_USER=$CODEARTIFACT_USER\r\nADD src/requirements.txt .\r\n\r\n{% endblock %}\r\n{% block SETUP_BENTO_COMPONENTS %}\r\n{{ super() }}\r\n\r\nRUN --mount=type=secret,id=code_artifact_token \\\r\n    export CODEARTIFACT_TOKEN=$(cat /run/secrets/code_artifact_token) && \\\r\n    pip config set global.extra-index-url https://${CODEARTIFACT_USER}:${CODEARTIFACT_TOKEN}@######.d.codeartifact.us-east-1.amazonaws.com/pypi/######/simple/ && \\\r\n    pip install -r src/requirements.txt\r\n\r\n{% endblock %}\r\n```\r\n\r\nWhen comparing to 1.2.20 version I noticed these lines:\r\n```\r\nRUN curl -O https://bootstrap.pypa.io/get-pip.py && \\\r\n    python3 get-pip.py && \\\r\n    rm -rf get-pip.py\r\n```\r\nwere **missing** from the cuda template file (for example `bentoml/_internal/container/frontend/dockerfile/templates/cuda_debian.j2`)\r\n\r\nThese lines existed in the templates under 1.2.20 and in that version I had no problems running pip install through my own template.\r\n\r\nexample `bentofile.yaml`:\r\n```\r\nservice: \"service:myservice\"\r\ndescription: |\r\n  ## This is a sample module\r\ninclude:\r\n  - \"src/*\"\r\n  - \"*.py\"\r\n  - \"requirements.txt\"\r\nexclude:\r\n  - \"tests/*\"\r\n  - \"*venv/*\"\r\ndocker:\r\n  distro: debian\r\n  dockerfile_template: ./Dockerfile.template.jinja\r\n  cuda_version: \"12.1.1\"\r\n```\r\n\r\nI don't mind opening a pr for that fix  if you this that is indeed the problem :)\r\n\r\n### To reproduce\r\n\r\n1. create a simple bentofile with a simple project like the bentofile.yaml above with a cuda version\r\n2. create a Dockerfile template jinja file like the provided above (can do anything involving pip, even just `RUN pip --version`)\r\n3. run bentoml build and see the error\r\n\r\n### Expected behavior\r\n\r\nIn 1.2.20 the build is perfectly fine and the pip install works without any problems.\r\n\r\n### Environment\r\n\r\n`bentoml`: 1.3.1\r\n`python`: 3.10.9\r\n`platform`: Linux-5.15.0-1051-aws-x86_64-with-glibc2.31",
    "comments": [
      {
        "user": "frostming",
        "body": "We have switched to `uv` as the package installer, please either use `uv` or install `pip` in the template yourself."
      },
      {
        "user": "shanirosen-airis",
        "body": "alright, good to know, thanks!\r\n"
      }
    ]
  },
  {
    "issue_number": 4859,
    "title": "Cannot define custom duration histogram buckets via `@bentoml.service(metrics=...)` ",
    "author": "rob-apella",
    "state": "closed",
    "created_at": "2024-07-13T00:07:58Z",
    "updated_at": "2024-08-06T18:23:51Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nI'm having issues defining custom histogram buckets for my service, following the documentation [here](https://docs.bentoml.com/en/latest/guides/configurations.html#metrics). I'm running `bentoml` v1.2.20\r\n\r\nTo reproduce I created a simple toy service:\r\n\r\n```python\r\nimport bentoml\r\n\r\n@bentoml.service(metrics={\"duration\": {\"buckets\": [1.0, 2.0, 5.0, 10.0]}})\r\nclass Summarization:\r\n\r\n    @bentoml.api\r\n    def summarize(self) -> str:\r\n        return \"ok\"\r\n```\r\n\r\nThen,\r\n\r\n1. run locally with `bentoml serve service:Summarization`\r\n2. hit the `summarize` endpoint a few times\r\n3. hit `metrics` endpoint, inspect buckets\r\n\r\nThe buckets returned are the default buckets defined [here](https://github.com/bentoml/BentoML/blob/65b22ef51b874b4669331879ff4323ba3ee10cb4/src/bentoml/_internal/utils/metrics.py#L5), not the custom buckets I defined in the service decorator\r\n\r\nAm I missing a step in the configuration somewhere?\r\n\r\n### To reproduce\r\n\r\nSee above\r\n\r\n### Expected behavior\r\n\r\n`/metrics` endpoint returns latencies bucketed into the custom bins that I provided via the `service` decorator\r\n\r\n### Environment\r\n\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.2.20\r\n`python`: 3.10.0\r\n`platform`: macOS-14.5-arm64-arm-64bit\r\n`uid_gid`: 501:20\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiohttp==3.9.5\r\naiosignal==1.3.1\r\nannotated-types==0.7.0\r\nanyio==4.4.0\r\nappdirs==1.4.4\r\nasgiref==3.8.1\r\nasync-timeout==4.0.3\r\nattrs==23.2.0\r\nbentoml==1.2.20\r\nbuild==1.2.1\r\ncattrs==23.1.2\r\ncertifi==2024.7.4\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==3.0.0\r\ndeepmerge==1.1.1\r\nDeprecated==1.2.14\r\nexceptiongroup==1.2.2\r\nfrozenlist==1.4.1\r\nfs==2.4.16\r\nh11==0.14.0\r\nhttpcore==1.0.5\r\nhttpx==0.27.0\r\nhttpx-ws==0.6.0\r\nidna==3.7\r\nimportlib-metadata==6.11.0\r\ninflection==0.5.1\r\nJinja2==3.1.4\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmdurl==0.1.2\r\nmultidict==6.0.5\r\nnumpy==2.0.0\r\nnvidia-ml-py==11.525.150\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\npackaging==24.1\r\npathspec==0.12.1\r\npip-requirements-parser==32.0.1\r\npip-tools==7.4.1\r\nprometheus_client==0.20.0\r\npsutil==6.0.0\r\npydantic==2.8.2\r\npydantic_core==2.20.1\r\nPygments==2.18.0\r\npyparsing==3.1.2\r\npyproject_hooks==1.1.0\r\npython-dateutil==2.9.0.post0\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.9\r\nPyYAML==6.0.1\r\npyzmq==26.0.3\r\nrich==13.7.1\r\nschema==0.7.7\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.1\r\nstarlette==0.37.2\r\ntomli==2.0.1\r\ntomli_w==1.0.0\r\ntornado==6.4.1\r\ntyping_extensions==4.12.2\r\nuvicorn==0.30.1\r\nwatchfiles==0.22.0\r\nwrapt==1.16.0\r\nwsproto==1.2.0\r\nyarl==1.9.4\r\nzipp==3.19.2\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "rob-apella",
        "body": "@frostming thank you for the fix. I can confirm that this is working now for when defining buckets like:\r\n\r\n```python\r\n{\"buckets\": [1.0, 2.0, 5.0, 10.0]}}\r\n```\r\n\r\nhowever, the API for defining min/max bucket and a scale factor still does not work. e.g.,\r\n\r\n```python\r\n\"duration\": {\r\n    \"min\": 1,\r\n    \"max\": 15,\r\n    \"factor\": 1.5,\r\n}\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 4234,
    "title": "bug: Doctest fails on modules using `bentoml.metrics.Histogram` on Linux",
    "author": "connorbrinton",
    "state": "closed",
    "created_at": "2023-10-12T18:55:51Z",
    "updated_at": "2024-08-02T07:26:42Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nWe recently noticed CI failing for our BentoML app. Digging into the error, it seems like the root cause was enabling doctesting using PyTest with `pytest --doctest-modules`. Trying to produce a reproducible example indicated that (i) the error only occurs on Linux and (ii) only occurs when a module-level `bentoml.metrics` object is present. It seems that when `doctest` is introspecting the module-level metrics object to look for docstrings, it accesses a property of the object that raises an error.\r\n\r\nIt would be nice if the metrics object definition could be fixed so that it can be introspected by doctest. This would allow us to write quick tests in our docstrings, which can be a convenient way to both test and explain a function.\r\n\r\n### To reproduce\r\n\r\n1. Run a Linux container with Python installed: `docker run -it --entrypoint /bin/bash python:3.11`\r\n2. Install BentoML: `pip install bentoml`\r\n3. Create the test file: `echo \"import bentoml; histogram = bentoml.metrics.Histogram()\" > test.py`\r\n4. Run `doctest`: `python -m doctest test.py`\r\n\r\n**Expected result:**\r\nDoctest should run successfully without any errors\r\n\r\n**Actual result:**\r\n```\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/usr/local/lib/python3.11/doctest.py\", line 2811, in <module>\r\n    sys.exit(_test())\r\n             ^^^^^^^\r\n  File \"/usr/local/lib/python3.11/doctest.py\", line 2801, in _test\r\n    failures, _ = testmod(m, verbose=verbose, optionflags=options)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/doctest.py\", line 1970, in testmod\r\n    for test in finder.find(m, name, globs=globs, extraglobs=extraglobs):\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/doctest.py\", line 940, in find\r\n    self._find(tests, obj, name, module, source_lines, globs, {})\r\n  File \"/usr/local/lib/python3.11/doctest.py\", line 1012, in _find\r\n    if ((self._is_routine(val) or inspect.isclass(val)) and\r\n         ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/doctest.py\", line 983, in _is_routine\r\n    maybe_routine = inspect.unwrap(maybe_routine)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/inspect.py\", line 762, in unwrap\r\n    while _is_wrapper(func):\r\n          ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/inspect.py\", line 753, in _is_wrapper\r\n    return hasattr(f, '__wrapped__')\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/repo/.venv/lib/python3.11/site-packages/bentoml/metrics.py\", line 412, in __getattr__\r\n    self._proxy = self._load_proxy()\r\n                  ^^^^^^^^^^^^^^^^^^\r\n  File \"/repo/.venv/lib/python3.11/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/repo/.venv/lib/python3.11/site-packages/bentoml/metrics.py\", line 432, in _load_proxy\r\n    proxy = getattr(client_impl, self._attr)(*self._args, **self._kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/repo/.venv/lib/python3.11/site-packages/bentoml/_internal/server/metrics/prometheus.py\", line 149, in Histogram\r\n    return partial(self.prometheus_client.Histogram, registry=self.registry)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/repo/.venv/lib/python3.11/site-packages/bentoml/_internal/server/metrics/prometheus.py\", line 57, in prometheus_client\r\n    assert os.path.isdir(self.multiproc_dir)\r\nAssertionError\r\n```\r\n\r\n### Expected behavior\r\n\r\nSee above, no error\r\n\r\n### Environment\r\n\r\nWithin the Docker container:\r\n\r\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.1.7\r\n`python`: 3.11.3\r\n`platform`: Linux-6.4.16-linuxkit-aarch64-with-glibc2.31\r\n`uid_gid`: 0:0\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiohttp==3.8.6\r\naiosignal==1.3.1\r\nanyio==4.0.0\r\nappdirs==1.4.4\r\nasgiref==3.7.2\r\nasync-timeout==4.0.3\r\nattrs==23.1.0\r\nbentoml==1.1.7\r\nbuild==1.0.3\r\ncattrs==23.1.2\r\ncertifi==2023.7.22\r\ncharset-normalizer==3.3.0\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==2.2.1\r\ncontextlib2==21.6.0\r\ndeepmerge==1.1.0\r\nDeprecated==1.2.14\r\nfrozenlist==1.4.0\r\nfs==2.4.16\r\nh11==0.14.0\r\nhttpcore==0.18.0\r\nhttpx==0.25.0\r\nidna==3.4\r\nimportlib-metadata==6.8.0\r\ninflection==0.5.1\r\niniconfig==2.0.0\r\nJinja2==3.1.2\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.3\r\nmdurl==0.1.2\r\nmultidict==6.0.4\r\nnumpy==1.25.2\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\npackaging==23.2\r\npathspec==0.11.2\r\npip-requirements-parser==32.0.1\r\npip-tools==7.3.0\r\npluggy==1.3.0\r\nprometheus-client==0.17.1\r\npsutil==5.9.5\r\nPygments==2.16.1\r\npynvml==11.5.0\r\npyparsing==3.1.1\r\npyproject_hooks==1.0.0\r\npytest==7.4.2\r\npython-dateutil==2.8.2\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.6\r\nPyYAML==6.0.1\r\npyzmq==25.1.1\r\nrequests==2.31.0\r\nrich==13.6.0\r\nschema==0.7.5\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nstarlette==0.31.1\r\ntornado==6.3.3\r\ntyping_extensions==4.8.0\r\nurllib3==2.0.6\r\nuvicorn==0.23.2\r\nwatchfiles==0.20.0\r\nwrapt==1.15.0\r\nyarl==1.9.2\r\nzipp==3.17.0\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "bojiang",
        "body": "Hi, should be fixed in 1.3.1 by @frostming .  Feel free to reopen if issue persists."
      }
    ]
  },
  {
    "issue_number": 4626,
    "title": "feature: expose prometheus collector registry client option.",
    "author": "almog2065",
    "state": "closed",
    "created_at": "2024-03-31T11:02:58Z",
    "updated_at": "2024-08-02T07:25:41Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nWhile testing multiple services, I got a bug of duplicated Collector Registry of Prometheus, Because I am on the same process. In testing, I don't need the metrics. So it will help to configure and expose this parameter to enable metrics to not get this bug.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "bojiang",
        "body": "Hi, should be fixed in 1.3.1 by @frostming ."
      }
    ]
  },
  {
    "issue_number": 4856,
    "title": "bug:  Adaptative batching with max_batch_size=1 crashes the API",
    "author": "bruno-hays",
    "state": "closed",
    "created_at": "2024-07-11T15:47:25Z",
    "updated_at": "2024-08-02T03:16:11Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWith this decorator on my function:\r\n```python\r\n    @bentoml.api(batchable=True,\r\n                 max_batch_size=1,\r\n                 max_latency_ms=3600000)\r\n```\r\nI get this incomprehensible error:\r\n```\r\n  File \"/home/gcpuser/sky_workdir/whisperapi/venv/lib/python3.11/site-packages/bentoml/_internal/utils/metrics.py\", line 44, in exponential_buckets\r\n    assert start < end\r\n           ^^^^^^^^^^^\r\nAssertionError\r\n```\r\n\r\nnow use this header:\r\n```python\r\n    @bentoml.api(batchable=True,\r\n                 max_batch_size=2,\r\n                 max_latency_ms=3600000)\r\n```\r\n\r\nAnd it works.\r\n\r\nAmusingly, this happens when I run my API on debian and I am not able to reproduce this error on my mac M1\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml==1.2.19\r\npython==3.11.9",
    "comments": []
  },
  {
    "issue_number": 4640,
    "title": "feature: Custom runner support for time series models like ARIMA using BentoML",
    "author": "jsamanta",
    "state": "open",
    "created_at": "2024-04-08T09:52:29Z",
    "updated_at": "2024-07-31T11:43:11Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Feature request\n\nI have implemented and tested a feature that trains a time series model - ARIMA and serves it using BentoML's custom runner to make predictions.\n\n### Motivation\n\nI believe it would be a nice guide to someone exploring time series models with custom runner implementation for models like ARIMA.\n\n### Other\n\nI would like to submit a PR but it would be nice if you could point me to as where I can push this code so that its available to users.",
    "comments": [
      {
        "user": "jsamantaucd",
        "body": "Dear team, did you get a chance to review this request if not please do, as I would be glad to upload a PR with the relevant code changes. "
      },
      {
        "user": "jsamantaucd",
        "body": "Submitted a PR request https://github.com/bentoml/BentoML/pull/4894 for tag v1.0.20"
      }
    ]
  },
  {
    "issue_number": 4886,
    "title": "question: How to use replica in BentoML?",
    "author": "Dawnfz-Lenfeng",
    "state": "closed",
    "created_at": "2024-07-29T03:20:47Z",
    "updated_at": "2024-07-29T03:33:10Z",
    "labels": [],
    "body": "I am using the [BentoVLLM](https://github.com/bentoml/BentoVLLM). How do I configure the [service](https://github.com/bentoml/BentoVLLM/blob/main/llama3-8b-instruct/service.py) file to use the `replica` feature?\r\n\r\nDo I need to use the Ray engine for this? I'm not very familiar with these configurations.",
    "comments": []
  },
  {
    "issue_number": 4879,
    "title": "bug: transformers vs 1.3.0 \"TypeError: __init__() got an unexpected keyword argument 'task'\"",
    "author": "smidm",
    "state": "closed",
    "created_at": "2024-07-24T09:42:08Z",
    "updated_at": "2024-07-25T00:42:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nThe 1.3.0 doesn't work with recent version of hugginface `transformers`, 1.2.19 is ok.\r\n\r\n```\r\n2024-07-23T18:18:57+0200 [ERROR] [service:TransformersSegmentationModel:1] Initializing service error\r\nTraceback (most recent call last):\r\n  File \"/home/matej/local/conda/envs/pose_service/lib/python3.9/site-packages/_bentoml_sdk/service/factory.py\", line 257, in __call__\r\n    instance = self.inner()\r\n  File \"/home/matej/prace/egoscue/mmpose_service/src/pose_service/segmentation.py\", line 21, in __init__\r\n    self.extractor_model = self.extractor_model_ref.load_model()\r\n  File \"/home/matej/local/conda/envs/pose_service/lib/python3.9/site-packages/bentoml/_internal/models/model.py\", line 377, in load_model\r\n    self._model = self.info.imported_module.load_model(self, *args, **kwargs)\r\n  File \"/home/matej/local/conda/envs/pose_service/lib/python3.9/site-packages/bentoml/_internal/frameworks/transformers.py\", line 402, in load_model\r\n    options = t.cast(TransformersOptions, bento_model.info.options)\r\n  File \"/home/matej/local/conda/envs/pose_service/lib/python3.9/site-packages/bentoml/_internal/models/model.py\", line 635, in options\r\n    self.imported_module.ModelOptions(**self._options),\r\nTypeError: __init__() got an unexpected keyword argument 'task'\r\n```\r\n\r\nI can add more info if necessary. \n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.3.0\r\n`python`: 3.9.18\r\n`platform`: Linux-6.9.7-200.fc40.x86_64-x86_64-with-glibc2.39\r\n`uid_gid`: 1000:1000\r\n`conda`: 22.9.0\r\n`in_conda_env`: True\r\n<details><summary><code>conda_packages</code></summary>\r\n\r\n<br>\r\n\r\n```yaml\r\nname: pose_service\r\nchannels:\r\n  - conda-forge\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=conda_forge\r\n  - _openmp_mutex=4.5=2_kmp_llvm\r\n  - argon2-cffi=21.3.0=pyhd3eb1b0_0\r\n  - argon2-cffi-bindings=21.2.0=py39h7f8727e_0\r\n  - asttokens=2.0.5=pyhd3eb1b0_0\r\n  - async-lru=2.0.4=py39h06a4308_0\r\n  - attrs=23.1.0=py39h06a4308_0\r\n  - aws-c-auth=0.7.11=h0b4cabd_1\r\n  - aws-c-cal=0.6.9=h14ec70c_3\r\n  - aws-c-common=0.9.12=hd590300_0\r\n  - aws-c-compression=0.2.17=h572eabf_8\r\n  - aws-c-event-stream=0.4.1=h97bb272_2\r\n  - aws-c-http=0.8.0=h9129f04_2\r\n  - aws-c-io=0.14.0=hf8f278a_1\r\n  - aws-c-mqtt=0.10.1=h2b97f5f_0\r\n  - aws-c-s3=0.4.9=hca09fc5_0\r\n  - aws-c-sdkutils=0.1.13=h572eabf_1\r\n  - aws-checksums=0.1.17=h572eabf_7\r\n  - aws-crt-cpp=0.26.0=h04327c0_8\r\n  - aws-sdk-cpp=1.11.210=hba3e011_10\r\n  - babel=2.11.0=py39h06a4308_0\r\n  - backcall=0.2.0=pyhd3eb1b0_0\r\n  - beautifulsoup4=4.12.2=py39h06a4308_0\r\n  - bleach=4.1.0=pyhd3eb1b0_0\r\n  - brotli-python=1.0.9=py39h6a678d5_7\r\n  - bzip2=1.0.8=hd590300_5\r\n  - c-ares=1.25.0=hd590300_0\r\n  - ca-certificates=2023.11.17=hbcca054_0\r\n  - certifi=2023.11.17=pyhd8ed1ab_0\r\n  - cffi=1.16.0=py39h5eee18b_0\r\n  - comm=0.1.2=py39h06a4308_0\r\n  - cryptography=41.0.7=py39hdda0065_0\r\n  - cyrus-sasl=2.1.28=h52b45da_1\r\n  - dbus=1.13.18=hb2f20db_0\r\n  - debugpy=1.6.7=py39h6a678d5_0\r\n  - decorator=5.1.1=pyhd3eb1b0_0\r\n  - defusedxml=0.7.1=pyhd3eb1b0_0\r\n  - executing=0.8.3=pyhd3eb1b0_0\r\n  - expat=2.5.0=h6a678d5_0\r\n  - fontconfig=2.14.1=h4c34cd2_2\r\n  - freetype=2.12.1=h4a9f257_0\r\n  - gflags=2.2.2=he1b5a44_1004\r\n  - giflib=5.2.1=h0b41bf4_3\r\n  - glib=2.69.1=he621ea3_2\r\n  - glog=0.6.0=h6f12383_0\r\n  - gst-plugins-base=1.14.1=h6a678d5_1\r\n  - gstreamer=1.14.1=h5eee18b_1\r\n  - icu=73.1=h6a678d5_0\r\n  - idna=3.4=py39h06a4308_0\r\n  - importlib_metadata=7.0.1=hd3eb1b0_0\r\n  - ipykernel=6.28.0=py39h06a4308_0\r\n  - ipython=8.15.0=py39h06a4308_0\r\n  - ipywidgets=8.0.4=py39h06a4308_0\r\n  - jedi=0.18.1=py39h06a4308_1\r\n  - jinja2=3.1.2=py39h06a4308_0\r\n  - jpeg=9e=h5eee18b_1\r\n  - json5=0.9.6=pyhd3eb1b0_0\r\n  - jupyter=1.0.0=py39h06a4308_8\r\n  - jupyter-lsp=2.2.0=py39h06a4308_0\r\n  - jupyter_client=8.6.0=py39h06a4308_0\r\n  - jupyter_console=6.6.3=py39h06a4308_0\r\n  - jupyter_core=5.5.0=py39h06a4308_0\r\n  - jupyter_events=0.8.0=py39h06a4308_0\r\n  - jupyter_server=2.10.0=py39h06a4308_0\r\n  - jupyter_server_terminals=0.4.4=py39h06a4308_1\r\n  - jupyterlab_pygments=0.1.2=py_0\r\n  - jupyterlab_server=2.25.1=py39h06a4308_0\r\n  - jupyterlab_widgets=3.0.9=py39h06a4308_0\r\n  - krb5=1.20.1=h143b758_1\r\n  - lcms2=2.12=h3be6417_0\r\n  - ld_impl_linux-64=2.38=h1181459_1\r\n  - lerc=3.0=h295c915_0\r\n  - libabseil=20230802.1=cxx17_h59595ed_0\r\n  - libarrow=14.0.2=h84dd17c_2_cpu\r\n  - libarrow-acero=14.0.2=h59595ed_2_cpu\r\n  - libarrow-dataset=14.0.2=h59595ed_2_cpu\r\n  - libarrow-flight=14.0.2=h120cb0d_2_cpu\r\n  - libarrow-flight-sql=14.0.2=h61ff412_2_cpu\r\n  - libarrow-gandiva=14.0.2=hacb8726_2_cpu\r\n  - libarrow-substrait=14.0.2=h61ff412_2_cpu\r\n  - libblas=3.9.0=21_linux64_openblas\r\n  - libbrotlicommon=1.1.0=hd590300_1\r\n  - libbrotlidec=1.1.0=hd590300_1\r\n  - libbrotlienc=1.1.0=hd590300_1\r\n  - libcblas=3.9.0=21_linux64_openblas\r\n  - libclang13=14.0.6=default_he11475f_1\r\n  - libcrc32c=1.1.2=h9c3ff4c_0\r\n  - libcups=2.4.2=h2d74bed_1\r\n  - libcurl=8.5.0=h251f7ec_0\r\n  - libdeflate=1.17=h5eee18b_1\r\n  - libedit=3.1.20230828=h5eee18b_0\r\n  - libev=4.33=hd590300_2\r\n  - libevent=2.1.12=hf998b51_1\r\n  - libffi=3.4.4=h6a678d5_0\r\n  - libgcc-ng=13.2.0=h807b86a_3\r\n  - libgfortran-ng=13.2.0=h69a702a_3\r\n  - libgfortran5=13.2.0=ha4646dd_3\r\n  - libgoogle-cloud=2.12.0=h5206363_4\r\n  - libgrpc=1.59.3=hd6c4280_0\r\n  - liblapack=3.9.0=21_linux64_openblas\r\n  - libllvm14=14.0.6=hdb19cb5_3\r\n  - libllvm15=15.0.7=hadd5161_1\r\n  - libnghttp2=1.58.0=h47da74e_1\r\n  - libnl=3.9.0=hd590300_0\r\n  - libnuma=2.0.16=h0b41bf4_1\r\n  - libopenblas=0.3.26=pthreads_h413a1c8_0\r\n  - libparquet=14.0.2=h352af49_2_cpu\r\n  - libpng=1.6.39=h5eee18b_0\r\n  - libpq=12.15=hdbd6064_1\r\n  - libprotobuf=4.24.4=hf27288f_0\r\n  - libre2-11=2023.06.02=h7a70373_0\r\n  - libsodium=1.0.18=h7b6447c_0\r\n  - libssh2=1.11.0=h0841786_0\r\n  - libstdcxx-ng=13.2.0=h7e041cc_3\r\n  - libthrift=0.19.0=hb90f79a_1\r\n  - libtiff=4.5.1=h6a678d5_0\r\n  - libutf8proc=2.8.0=h166bdaf_0\r\n  - libuuid=1.41.5=h5eee18b_0\r\n  - libwebp=1.3.2=h11a3e52_0\r\n  - libwebp-base=1.3.2=hd590300_0\r\n  - libxcb=1.15=h7f8727e_0\r\n  - libxkbcommon=1.0.1=h5eee18b_1\r\n  - libxml2=2.10.4=hf1b16e4_1\r\n  - libzlib=1.2.13=hd590300_5\r\n  - llvm-openmp=17.0.6=h4dfa4b3_0\r\n  - lz4-c=1.9.4=h6a678d5_0\r\n  - matplotlib-inline=0.1.6=py39h06a4308_0\r\n  - mistune=2.0.4=py39h06a4308_0\r\n  - mysql=5.7.24=h721c034_2\r\n  - nbclient=0.8.0=py39h06a4308_0\r\n  - nbconvert=7.10.0=py39h06a4308_0\r\n  - nbformat=5.9.2=py39h06a4308_0\r\n  - ncurses=6.4=h6a678d5_0\r\n  - nest-asyncio=1.5.6=py39h06a4308_0\r\n  - notebook=7.0.6=py39h06a4308_0\r\n  - notebook-shim=0.2.3=py39h06a4308_0\r\n  - openjpeg=2.5.0=hfec8fc6_2\r\n  - openssl=3.2.0=hd590300_1\r\n  - orc=1.9.2=h4b38347_0\r\n  - overrides=7.4.0=py39h06a4308_0\r\n  - packaging=23.1=py39h06a4308_0\r\n  - pandocfilters=1.5.0=pyhd3eb1b0_0\r\n  - parso=0.8.3=pyhd3eb1b0_0\r\n  - pcre=8.45=h295c915_0\r\n  - pexpect=4.8.0=pyhd3eb1b0_3\r\n  - pickleshare=0.7.5=pyhd3eb1b0_1003\r\n  - ply=3.11=py39h06a4308_0\r\n  - prometheus_client=0.14.1=py39h06a4308_0\r\n  - prompt-toolkit=3.0.43=py39h06a4308_0\r\n  - prompt_toolkit=3.0.43=hd3eb1b0_0\r\n  - ptyprocess=0.7.0=pyhd3eb1b0_2\r\n  - pure_eval=0.2.2=pyhd3eb1b0_0\r\n  - pycparser=2.21=pyhd3eb1b0_0\r\n  - pyopenssl=23.2.0=py39h06a4308_0\r\n  - pyqt=5.15.10=py39h6a678d5_0\r\n  - pyqt5-sip=12.13.0=py39h5eee18b_0\r\n  - pysocks=1.7.1=py39h06a4308_0\r\n  - python=3.9.18=h955ad1f_0\r\n  - python-dateutil=2.8.2=pyhd3eb1b0_0\r\n  - python-fastjsonschema=2.16.2=py39h06a4308_0\r\n  - python-json-logger=2.0.7=py39h06a4308_0\r\n  - python_abi=3.9=2_cp39\r\n  - pyzmq=25.1.2=py39h6a678d5_0\r\n  - qt-main=5.15.2=h53bd1ea_10\r\n  - qtconsole=5.5.0=py39h06a4308_0\r\n  - qtpy=2.4.1=py39h06a4308_0\r\n  - rdma-core=49.0=hd3aeb46_2\r\n  - re2=2023.06.02=h2873b5e_0\r\n  - readline=8.2=h5eee18b_0\r\n  - rerun-sdk=0.12.0=py39h3d6467e_2\r\n  - rfc3339-validator=0.1.4=py39h06a4308_0\r\n  - rfc3986-validator=0.1.1=py39h06a4308_0\r\n  - s2n=1.4.1=h06160fa_0\r\n  - send2trash=1.8.2=py39h06a4308_0\r\n  - sip=6.7.12=py39h6a678d5_0\r\n  - six=1.16.0=pyhd3eb1b0_1\r\n  - snappy=1.1.10=h9fff704_0\r\n  - sniffio=1.3.0=py39h06a4308_0\r\n  - soupsieve=2.5=py39h06a4308_0\r\n  - sqlite=3.41.2=h5eee18b_0\r\n  - stack_data=0.2.0=pyhd3eb1b0_0\r\n  - terminado=0.17.1=py39h06a4308_0\r\n  - tinycss2=1.2.1=py39h06a4308_0\r\n  - tk=8.6.12=h1ccaba5_0\r\n  - tomli=2.0.1=py39h06a4308_0\r\n  - tornado=6.3.3=py39h5eee18b_0\r\n  - traitlets=5.7.1=py39h06a4308_0\r\n  - ucx=1.15.0=h75e419f_3\r\n  - webencodings=0.5.1=py39h06a4308_1\r\n  - websocket-client=0.58.0=py39h06a4308_4\r\n  - wheel=0.41.2=py39h06a4308_0\r\n  - widgetsnbextension=4.0.5=py39h06a4308_0\r\n  - xz=5.4.5=h5eee18b_0\r\n  - yaml=0.2.5=h7b6447c_0\r\n  - zeromq=4.3.5=h6a678d5_0\r\n  - zlib=1.2.13=hd590300_5\r\n  - zstd=1.5.5=hc292b87_0\r\n  - pip:\r\n    - absl-py==1.4.0\r\n    - addict==2.4.0\r\n    - aiofiles==23.1.0\r\n    - aiohttp==3.8.4\r\n    - aiohttp-retry==2.8.3\r\n    - aiosignal==1.3.1\r\n    - aiosqlite==0.20.0\r\n    - aliyun-python-sdk-core==2.14.0\r\n    - aliyun-python-sdk-kms==2.16.2\r\n    - amqp==5.1.1\r\n    - annotated-types==0.6.0\r\n    - antlr4-python3-runtime==4.9.3\r\n    - anyio==4.4.0\r\n    - appdirs==1.4.4\r\n    - argcomplete==3.1.1\r\n    - array-record==0.5.0\r\n    - arrow==1.3.0\r\n    - asgiref==3.6.0\r\n    - astunparse==1.6.3\r\n    - async-timeout==4.0.2\r\n    - asyncssh==2.14.0\r\n    - atpublic==4.0\r\n    - av==9.2.0\r\n    - backoff==2.2.1\r\n    - bentoml==1.3.0\r\n    - billiard==4.1.0\r\n    - blinker==1.7.0\r\n    - boto3==1.26.109\r\n    - botocore==1.29.109\r\n    - build==1.1.1\r\n    - bump-pydantic==0.8.0\r\n    - cachetools==5.3.1\r\n    - cattrs==22.2.0\r\n    - celery==5.3.4\r\n    - charset-normalizer==2.1.1\r\n    - chex==0.1.7\r\n    - chumpy==0.70\r\n    - circus==0.18.0\r\n    - click==8.1.3\r\n    - click-didyoumean==0.3.0\r\n    - click-option-group==0.5.5\r\n    - click-plugins==1.1.1\r\n    - click-repl==0.3.0\r\n    - cloudpickle==2.2.1\r\n    - colorama==0.4.6\r\n    - configargparse==1.7\r\n    - configobj==5.0.8\r\n    - contextlib2==21.6.0\r\n    - contourpy==1.0.7\r\n    - coverage==7.3.2\r\n    - crcmod==1.7\r\n    - cuda-python==12.3.0\r\n    - cycler==0.11.0\r\n    - cython==0.29.34\r\n    - dacite==1.7.0\r\n    - dash==2.15.0\r\n    - dash-core-components==2.0.0\r\n    - dash-html-components==2.0.0\r\n    - dash-table==5.0.0\r\n    - deepmerge==1.1.0\r\n    - deprecated==1.2.13\r\n    - dictdiffer==0.9.0\r\n    - dill==0.3.6\r\n    - diskcache==5.6.3\r\n    - distlib==0.3.8\r\n    - distro==1.8.0\r\n    - dm-tree==0.1.8\r\n    - dnspython==2.3.0\r\n    - docker-pycreds==0.4.0\r\n    - dpath==2.1.6\r\n    - dulwich==0.21.6\r\n    - dvc==3.28.0\r\n    - dvc-data==2.20.0\r\n    - dvc-http==2.30.2\r\n    - dvc-objects==1.1.0\r\n    - dvc-render==0.6.0\r\n    - dvc-studio-client==0.15.0\r\n    - dvc-task==0.3.0\r\n    - editables==0.5\r\n    - et-xmlfile==1.1.0\r\n    - etils==1.4.1\r\n    - eventlet==0.33.3\r\n    - exceptiongroup==1.1.1\r\n    - fiftyone==0.23.8\r\n    - fiftyone-brain==0.16.1\r\n    - fiftyone-db==0.4.0\r\n    - fiftyone-desktop==0.33.7\r\n    - filelock==3.13.1\r\n    - flake8==6.1.0\r\n    - flask==3.0.0\r\n    - flask-basicauth==0.2.0\r\n    - flask-cors==4.0.0\r\n    - flask-login==0.6.3\r\n    - flatbuffers==23.5.26\r\n    - flatten-dict==0.4.2\r\n    - flax==0.7.4\r\n    - flufl-lock==7.1.1\r\n    - fonttools==4.39.3\r\n    - fqdn==1.5.1\r\n    - frozenlist==1.3.3\r\n    - fs==2.4.16\r\n    - fs-s3fs==1.1.1\r\n    - fsspec==2023.10.0\r\n    - ftfy==6.1.1\r\n    - funcy==2.0\r\n    - future==0.18.3\r\n    - gast==0.4.0\r\n    - gevent==23.9.1\r\n    - geventhttpclient==2.3.1\r\n    - gin-config==0.5.0\r\n    - gitdb==4.0.11\r\n    - gitpython==3.1.40\r\n    - glob2==0.7\r\n    - google-api-core==2.18.0\r\n    - google-api-python-client==2.125.0\r\n    - google-auth==2.22.0\r\n    - google-auth-httplib2==0.2.0\r\n    - google-auth-oauthlib==1.0.0\r\n    - google-pasta==0.2.0\r\n    - googleapis-common-protos==1.59.0\r\n    - grandalf==0.8\r\n    - graphql-core==3.2.3\r\n    - greenlet==2.0.2\r\n    - grpcio==1.54.2\r\n    - grpcio-health-checking==1.48.2\r\n    - gto==1.5.0\r\n    - h11==0.14.0\r\n    - h2==4.1.0\r\n    - h5py==3.10.0\r\n    - hatch==1.9.3\r\n    - hatchling==1.21.1\r\n    - hpack==4.0.0\r\n    - httpcore==1.0.5\r\n    - httplib2==0.22.0\r\n    - httpx==0.27.0\r\n    - httpx-ws==0.6.0\r\n    - huggingface-hub==0.24.1\r\n    - humanize==4.8.0\r\n    - hydra-core==1.3.2\r\n    - hypercorn==0.14.3\r\n    - hyperframe==6.0.1\r\n    - hyperlink==21.0.0\r\n    - imageio==2.28.1\r\n    - imgaug==0.4.0\r\n    - immutabledict==4.2.0\r\n    - importlib-metadata==6.2.0\r\n    - importlib-resources==5.12.0\r\n    - inflate64==0.3.1\r\n    - inflection==0.5.1\r\n    - iniconfig==2.0.0\r\n    - inquirerpy==0.3.4\r\n    - interrogate==1.5.0\r\n    - ipympl==0.9.3\r\n    - ipython-genutils==0.2.0\r\n    - isoduration==20.11.0\r\n    - isort==4.3.21\r\n    - iterative-telemetry==0.0.8\r\n    - itsdangerous==2.1.2\r\n    - jaraco-classes==3.3.1\r\n    - jax==0.4.14\r\n    - jaxlib==0.4.14\r\n    - jeepney==0.8.0\r\n    - jmespath==0.10.0\r\n    - joblib==1.2.0\r\n    - json-stream==2.3.2\r\n    - json-stream-rs-tokenizer==0.4.25\r\n    - json-tricks==3.16.1\r\n    - jsonlines==3.1.0\r\n    - jsonpointer==2.4\r\n    - jsonschema==4.20.0\r\n    - jsonschema-specifications==2023.11.1\r\n    - jstyleson==0.0.2\r\n    - jupyter-events==0.9.0\r\n    - jupyter-server==2.10.1\r\n    - jupyterlab==4.0.9\r\n    - kaggle==1.6.11\r\n    - kaleido==0.2.1\r\n    - keras==2.15.0\r\n    - keyring==24.3.1\r\n    - kiwisolver==1.4.4\r\n    - kombu==5.3.2\r\n    - kornia==0.7.2\r\n    - kornia-moons==0.2.9\r\n    - kornia-rs==0.1.3\r\n    - lazy-loader==0.2\r\n    - libclang==16.0.6\r\n    - libcst==1.1.0\r\n    - llvmlite==0.41.1\r\n    - locust==2.29.1\r\n    - lxml==5.2.1\r\n    - markdown==3.4.3\r\n    - markdown-it-py==2.2.0\r\n    - markupsafe==2.1.2\r\n    - matplotlib==3.8.0\r\n    - mccabe==0.7.0\r\n    - mdurl==0.1.2\r\n    - mediapipe==0.10.3\r\n    - ml-dtypes==0.3.2\r\n    - mmcls==0.25.0\r\n    - mmcv==2.1.0\r\n    - mmdet==3.2.0\r\n    - mmsegmentation==0.30.0\r\n    - model-index==0.1.11\r\n    - mongoengine==0.24.2\r\n    - more-itertools==10.2.0\r\n    - motor==3.1.2\r\n    - mpmath==1.2.1\r\n    - msgpack==1.0.5\r\n    - multidict==6.0.4\r\n    - multivolumefile==0.2.3\r\n    - munkres==1.1.4\r\n    - mypy-extensions==1.0.0\r\n    - namex==0.0.7\r\n    - networkx==2.8.8\r\n    - nibabel==5.1.0\r\n    - numba==0.58.1\r\n    - numpy==1.26.4\r\n    - nvidia-ml-py==11.525.150\r\n    - oauth2client==4.1.3\r\n    - oauthlib==3.2.2\r\n    - omegaconf==2.3.0\r\n    - open3d==0.18.0\r\n    - open3d-cpu==0.18.0\r\n    - opencv-contrib-python==4.9.0.80\r\n    - opendatalab==0.0.10\r\n    - openmim==0.3.9\r\n    - openpyxl==3.1.2\r\n    - opentelemetry-api==1.20.0\r\n    - opentelemetry-exporter-otlp-proto-http==1.14.0\r\n    - opentelemetry-instrumentation==0.41b0\r\n    - opentelemetry-instrumentation-aiohttp-client==0.41b0\r\n    - opentelemetry-instrumentation-asgi==0.41b0\r\n    - opentelemetry-instrumentation-grpc==0.38b0\r\n    - opentelemetry-proto==1.14.0\r\n    - opentelemetry-sdk==1.20.0\r\n    - opentelemetry-semantic-conventions==0.41b0\r\n    - opentelemetry-util-http==0.41b0\r\n    - openvino==2023.1.0\r\n    - openvino-dev==2023.1.0\r\n    - openvino-telemetry==2023.2.1\r\n    - openxlab==0.0.26\r\n    - opt-einsum==3.3.0\r\n    - optax==0.1.7\r\n    - optree==0.11.0\r\n    - orbax-checkpoint==0.3.5\r\n    - ordered-set==4.1.0\r\n    - orjson==3.9.1\r\n    - oss2==2.17.0\r\n    - ovmsclient==2023.1\r\n    - pandas==2.0.0\r\n    - parameterized==0.9.0\r\n    - pathspec==0.11.1\r\n    - pfzy==0.3.4\r\n    - pillow==9.5.0\r\n    - pillow-heif==0.10.1\r\n    - pip==24.0\r\n    - pip-requirements-parser==32.0.1\r\n    - pip-tools==7.4.0\r\n    - pipdeptree==2.19.0\r\n    - platformdirs==3.11.0\r\n    - plotly==5.15.0\r\n    - pluggy==1.5.0\r\n    - plyfile==0.9\r\n    - portalocker==2.8.2\r\n    - pose-service==0.0.1\r\n    - pprintpp==0.4.0\r\n    - prettytable==3.6.0\r\n    - priority==2.0.0\r\n    - prometheus-client==0.16.0\r\n    - promise==2.3\r\n    - proto-plus==1.23.0\r\n    - protobuf==3.20.3\r\n    - psutil==5.9.4\r\n    - py==1.11.0\r\n    - py-cpuinfo==9.0.0\r\n    - py7zr==0.20.5\r\n    - pyarrow==14.0.2\r\n    - pyasn1==0.5.0\r\n    - pyasn1-modules==0.3.0\r\n    - pybcj==1.0.1\r\n    - pycocotools==2.0.6\r\n    - pycodestyle==2.11.1\r\n    - pycryptodome==3.19.0\r\n    - pycryptodomex==3.18.0\r\n    - pydantic==2.7.1\r\n    - pydantic-core==2.18.2\r\n    - pydot==1.4.2\r\n    - pyemd==1.0.0\r\n    - pyflakes==3.1.0\r\n    - pygit2==1.13.2\r\n    - pygments==2.16.1\r\n    - pygtrie==2.5.0\r\n    - pyinquirer==1.0.3\r\n    - pymongo==4.3.3\r\n    - pynndescent==0.5.11\r\n    - pynvml==11.5.0\r\n    - pyodbc==5.0.1\r\n    - pyparsing==3.0.9\r\n    - pyppmd==1.0.0\r\n    - pyproject-hooks==1.0.0\r\n    - pyquaternion==0.9.9\r\n    - pyrealsense2==2.54.2.5684\r\n    - pytest==8.2.2\r\n    - pytest-runner==6.0.0\r\n    - python-dotenv==1.0.1\r\n    - python-multipart==0.0.6\r\n    - python-rapidjson==1.12\r\n    - python-slugify==8.0.4\r\n    - pytz==2023.3\r\n    - pywavelets==1.4.1\r\n    - pyyaml==6.0\r\n    - pyzstd==0.15.8\r\n    - rarfile==4.0\r\n    - referencing==0.33.0\r\n    - regex==2023.3.23\r\n    - requests==2.28.2\r\n    - requests-oauthlib==1.3.1\r\n    - requests-toolbelt==1.0.0\r\n    - retrying==1.3.4\r\n    - rich==13.4.2\r\n    - roundrobin==0.0.4\r\n    - rpds-py==0.13.1\r\n    - rsa==4.9\r\n    - ruamel-yaml==0.17.32\r\n    - ruamel-yaml-clib==0.2.7\r\n    - s3transfer==0.6.0\r\n    - sacrebleu==2.4.1\r\n    - safetensors==0.4.3\r\n    - schema==0.7.5\r\n    - scikit-image==0.21.0\r\n    - scikit-learn==1.1.2\r\n    - scipy==1.10.1\r\n    - scmrepo==1.4.1\r\n    - seaborn==0.13.2\r\n    - secretstorage==3.3.3\r\n    - semver==3.0.2\r\n    - sentencepiece==0.2.0\r\n    - sentry-sdk==1.45.0\r\n    - seqeval==1.2.2\r\n    - setproctitle==1.3.3\r\n    - setuptools==60.2.0\r\n    - shapely==2.0.1\r\n    - shellingham==1.5.4\r\n    - shortuuid==1.0.11\r\n    - shtab==1.6.4\r\n    - simple-di==0.1.5\r\n    - smmap==5.0.1\r\n    - sortedcontainers==2.4.0\r\n    - sounddevice==0.4.6\r\n    - sqltrie==0.8.0\r\n    - sse-starlette==0.10.3\r\n    - sseclient-py==1.7.2\r\n    - starlette==0.25.0\r\n    - strawberry-graphql==0.138.1\r\n    - sympy==1.11.1\r\n    - tabulate==0.9.0\r\n    - tenacity==8.2.2\r\n    - tensorboard==2.15.2\r\n    - tensorboard-data-server==0.7.1\r\n    - tensorboardx==2.6.2.2\r\n    - tensorflow==2.15.1\r\n    - tensorflow-addons==0.23.0\r\n    - tensorflow-datasets==4.9.3\r\n    - tensorflow-decision-forests==1.8.1\r\n    - tensorflow-estimator==2.15.0\r\n    - tensorflow-hub==0.16.1\r\n    - tensorflow-io-gcs-filesystem==0.34.0\r\n    - tensorflow-metadata==1.14.0\r\n    - tensorflow-model-optimization==0.8.0\r\n    - tensorflowjs==4.18.0\r\n    - tensorstore==0.1.43\r\n    - termcolor==2.3.0\r\n    - terminaltables==3.1.10\r\n    - text-unidecode==1.3\r\n    - texttable==1.6.7\r\n    - tf-keras==2.15.1\r\n    - tf-slim==1.1.0\r\n    - threadpoolctl==3.1.0\r\n    - tifffile==2023.4.12\r\n    - tokenizers==0.19.1\r\n    - toml==0.10.2\r\n    - tomli-w==1.0.0\r\n    - tomlkit==0.12.2\r\n    - toolz==0.12.0\r\n    - torch==2.0.0+cpu\r\n    - torchvision==0.15.1+cpu\r\n    - tqdm==4.65.0\r\n    - transformers==4.43.1\r\n    - tritonclient==2.38.0\r\n    - trove-classifiers==2024.2.23\r\n    - typeguard==2.13.3\r\n    - typer==0.9.0\r\n    - types-python-dateutil==2.8.19.14\r\n    - typing-extensions==4.11.0\r\n    - typing-inspect==0.9.0\r\n    - tzdata==2023.3\r\n    - tzlocal==5.0.1\r\n    - umap-learn==0.5.5\r\n    - universal-analytics-python3==1.1.1\r\n    - uri-template==1.3.0\r\n    - uritemplate==4.1.1\r\n    - urllib3==1.26.13\r\n    - userpath==1.9.2\r\n    - uv==0.2.27\r\n    - uvicorn==0.30.1\r\n    - vine==5.0.0\r\n    - virtualenv==20.25.1\r\n    - voluptuous==0.13.1\r\n    - voxel51-eta==0.12.6\r\n    - wandb==0.16.6\r\n    - watchfiles==0.19.0\r\n    - wcwidth==0.2.6\r\n    - webcolors==1.13\r\n    - werkzeug==3.0.1\r\n    - wrapt==1.14.1\r\n    - wsproto==1.2.0\r\n    - wurlitzer==3.0.3\r\n    - xdoctest==1.1.2\r\n    - xmltodict==0.13.0\r\n    - xtcocotools==1.13\r\n    - yapf==0.32.0\r\n    - yarl==1.8.2\r\n    - zc-lockfile==3.0.post1\r\n    - zipp==3.15.0\r\n    - zope-event==5.0\r\n    - zope-interface==6.1\r\n    - zstandard==0.22.0\r\nprefix: /home/matej/local/conda/envs/pose_service\r\n```\r\n\r\n</details>\r\n\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\nabsl-py==1.4.0\r\naddict==2.4.0\r\naiofiles==23.1.0\r\naiohttp==3.8.4\r\naiohttp-retry==2.8.3\r\naiosignal==1.3.1\r\naiosqlite==0.20.0\r\naliyun-python-sdk-core==2.14.0\r\naliyun-python-sdk-kms==2.16.2\r\namqp==5.1.1\r\nannotated-types==0.6.0\r\nantlr4-python3-runtime==4.9.3\r\nanyio==4.4.0\r\nappdirs==1.4.4\r\nargcomplete==3.1.1\r\nargon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work\r\nargon2-cffi-bindings @ file:///tmp/build/80754af9/argon2-cffi-bindings_1644569679365/work\r\narray-record==0.5.0\r\narrow==1.3.0\r\nasgiref==3.6.0\r\nasttokens @ file:///opt/conda/conda-bld/asttokens_1646925590279/work\r\nastunparse==1.6.3\r\nasync-lru @ file:///croot/async-lru_1699554519285/work\r\nasync-timeout==4.0.2\r\nasyncssh==2.14.0\r\natpublic==4.0\r\nattrs @ file:///croot/attrs_1695717823297/work\r\nav==9.2.0\r\nbabel @ file:///croot/babel_1671781930836/work\r\nbackcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work\r\nbackoff==2.2.1\r\nbeautifulsoup4 @ file:///croot/beautifulsoup4-split_1681493039619/work\r\nbentoml==1.3.0\r\nbilliard==4.1.0\r\nbleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work\r\nblinker==1.7.0\r\nboto3==1.26.109\r\nbotocore==1.29.109\r\nbrotli @ file:///tmp/abs_ecyw11_7ze/croots/recipe/brotli-split_1659616059936/work\r\nbuild==1.1.1\r\nbump-pydantic==0.8.0\r\ncachetools==5.3.1\r\ncattrs==22.2.0\r\ncelery==5.3.4\r\ncertifi @ file:///home/conda/feedstock_root/build_artifacts/certifi_1700303426725/work/certifi\r\ncffi @ file:///croot/cffi_1700254295673/work\r\ncharset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work\r\ncharset-normalizer==2.1.1\r\nchex==0.1.7\r\nchumpy==0.70\r\ncircus==0.18.0\r\nclick==8.1.3\r\nclick-didyoumean==0.3.0\r\nclick-option-group==0.5.5\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\ncloudpickle==2.2.1\r\ncolorama==0.4.6\r\ncomm @ file:///croot/comm_1671231121260/work\r\nconfigargparse==1.7\r\nconfigobj==5.0.8\r\ncontextlib2==21.6.0\r\ncontourpy==1.0.7\r\ncoverage==7.3.2\r\ncrcmod==1.7\r\ncryptography @ file:///croot/cryptography_1702070282333/work\r\ncuda-python==12.3.0\r\ncycler==0.11.0\r\ncython==0.29.34\r\ndacite==1.7.0\r\ndash==2.15.0\r\ndash-core-components==2.0.0\r\ndash-html-components==2.0.0\r\ndash-table==5.0.0\r\ndebugpy @ file:///croot/debugpy_1690905042057/work\r\ndecorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work\r\ndeepmerge==1.1.0\r\ndefusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\r\ndeprecated==1.2.13\r\ndictdiffer==0.9.0\r\ndill==0.3.6\r\ndiskcache==5.6.3\r\ndistlib==0.3.8\r\ndistro==1.8.0\r\ndm-tree==0.1.8\r\ndnspython==2.3.0\r\ndocker-pycreds==0.4.0\r\ndpath==2.1.6\r\ndulwich==0.21.6\r\ndvc==3.28.0\r\ndvc-data==2.20.0\r\ndvc-http==2.30.2\r\ndvc-objects==1.1.0\r\ndvc-render==0.6.0\r\ndvc-studio-client==0.15.0\r\ndvc-task==0.3.0\r\neditables==0.5\r\net-xmlfile==1.1.0\r\netils==1.4.1\r\neventlet==0.33.3\r\nexceptiongroup==1.1.1\r\nexceptiongroup @ file:///croot/exceptiongroup_1706031385326/work\r\nexecuting @ file:///opt/conda/conda-bld/executing_1646925071911/work\r\nfastjsonschema @ file:///opt/conda/conda-bld/python-fastjsonschema_1661371079312/work\r\nfiftyone==0.23.8\r\nfiftyone-brain==0.16.1\r\nfiftyone-db==0.4.0\r\nfiftyone-desktop==0.33.7\r\nfilelock==3.13.1\r\nflake8==6.1.0\r\nflask==3.0.0\r\nflask-basicauth==0.2.0\r\nflask-cors==4.0.0\r\nflask-login==0.6.3\r\nflatbuffers==23.5.26\r\nflatten-dict==0.4.2\r\nflax==0.7.4\r\nflufl-lock==7.1.1\r\nfonttools==4.39.3\r\nfqdn==1.5.1\r\nfrozenlist==1.3.3\r\nfs==2.4.16\r\nfs-s3fs==1.1.1\r\nfsspec==2023.10.0\r\nftfy==6.1.1\r\nfuncy==2.0\r\nfuture==0.18.3\r\ngast==0.4.0\r\ngevent==23.9.1\r\ngeventhttpclient==2.3.1\r\ngin-config==0.5.0\r\ngitdb==4.0.11\r\ngitpython==3.1.40\r\nglob2==0.7\r\ngoogle-api-core==2.18.0\r\ngoogle-api-python-client==2.125.0\r\ngoogle-auth==2.22.0\r\ngoogle-auth-httplib2==0.2.0\r\ngoogle-auth-oauthlib==1.0.0\r\ngoogle-pasta==0.2.0\r\ngoogleapis-common-protos==1.59.0\r\ngrandalf==0.8\r\ngraphql-core==3.2.3\r\ngreenlet==2.0.2\r\ngrpcio==1.54.2\r\ngrpcio-health-checking==1.48.2\r\ngto==1.5.0\r\nh11==0.14.0\r\nh2==4.1.0\r\nh5py==3.10.0\r\nhatch==1.9.3\r\nhatchling==1.21.1\r\nhpack==4.0.0\r\nhttpcore==1.0.5\r\nhttplib2==0.22.0\r\nhttpx==0.27.0\r\nhttpx-ws==0.6.0\r\nhuggingface-hub==0.24.1\r\nhumanize==4.8.0\r\nhydra-core==1.3.2\r\nhypercorn==0.14.3\r\nhyperframe==6.0.1\r\nhyperlink==21.0.0\r\nidna @ file:///croot/idna_1666125576474/work\r\nimageio==2.28.1\r\nimgaug==0.4.0\r\nimmutabledict==4.2.0\r\nimportlib-metadata==6.2.0\r\nimportlib-metadata @ file:///croot/importlib_metadata-suite_1704813515092/work\r\nimportlib-resources==5.12.0\r\ninflate64==0.3.1\r\ninflection==0.5.1\r\niniconfig==2.0.0\r\ninquirerpy==0.3.4\r\ninterrogate==1.5.0\r\nipykernel @ file:///croot/ipykernel_1705933831282/work\r\nipympl==0.9.3\r\nipython @ file:///croot/ipython_1694181358621/work\r\nipython-genutils==0.2.0\r\nipywidgets @ file:///croot/ipywidgets_1679394798311/work\r\nisoduration==20.11.0\r\nisort==4.3.21\r\niterative-telemetry==0.0.8\r\nitsdangerous==2.1.2\r\njaraco-classes==3.3.1\r\njax==0.4.14\r\njaxlib==0.4.14\r\njedi @ file:///tmp/build/80754af9/jedi_1644297102865/work\r\njeepney==0.8.0\r\njinja2 @ file:///croot/jinja2_1666908132255/work\r\njmespath==0.10.0\r\njoblib==1.2.0\r\njson-stream==2.3.2\r\njson-stream-rs-tokenizer==0.4.25\r\njson-tricks==3.16.1\r\njson5 @ file:///tmp/build/80754af9/json5_1624432770122/work\r\njsonlines==3.1.0\r\njsonpointer==2.4\r\njsonschema @ file:///croot/jsonschema_1699041609003/work\r\njsonschema==4.20.0\r\njsonschema-specifications @ file:///croot/jsonschema-specifications_1699032386549/work\r\njsonschema-specifications==2023.11.1\r\njstyleson==0.0.2\r\njupyter @ file:///tmp/abs_33h4eoipez/croots/recipe/jupyter_1659349046347/work\r\njupyter-client @ file:///croot/jupyter_client_1699455897726/work\r\njupyter-console @ file:///croot/jupyter_console_1679999630278/work\r\njupyter-core @ file:///croot/jupyter_core_1698937308754/work\r\njupyter-events @ file:///croot/jupyter_events_1699282461638/work\r\njupyter-events==0.9.0\r\njupyter-lsp @ file:///croot/jupyter-lsp-meta_1699978238815/work\r\njupyter-server @ file:///croot/jupyter_server_1699466442171/work\r\njupyter-server==2.10.1\r\njupyter-server-terminals @ file:///croot/jupyter_server_terminals_1686870725608/work\r\njupyterlab @ file:///croot/jupyterlab_1700518272623/work\r\njupyterlab==4.0.9\r\njupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\r\njupyterlab-server @ file:///croot/jupyterlab_server_1699555425460/work\r\njupyterlab-widgets @ file:///croot/jupyterlab_widgets_1700168618520/work\r\nkaggle==1.6.11\r\nkaleido==0.2.1\r\nkeras==2.15.0\r\nkeyring==24.3.1\r\nkiwisolver==1.4.4\r\nkombu==5.3.2\r\nkornia==0.7.2\r\nkornia-moons==0.2.9\r\nkornia-rs==0.1.3\r\nlazy-loader==0.2\r\nlibclang==16.0.6\r\nlibcst==1.1.0\r\nllvmlite==0.41.1\r\nlocust==2.29.1\r\nlxml==5.2.1\r\nmarkdown==3.4.3\r\nmarkdown-it-py==2.2.0\r\nmarkupsafe==2.1.2\r\nmarkupsafe @ file:///croot/markupsafe_1704205993651/work\r\nmatplotlib==3.8.0\r\nmatplotlib-inline @ file:///opt/conda/conda-bld/matplotlib-inline_1662014470464/work\r\nmccabe==0.7.0\r\nmdurl==0.1.2\r\nmediapipe==0.10.3\r\nmistune @ file:///opt/conda/conda-bld/mistune_1661496219659/work\r\nml-dtypes==0.3.2\r\nmmcls==0.25.0\r\nmmcv==2.1.0\r\nmmdet==3.2.0\r\n-e /home/matej/prace/egoscue/3rd_party/mmengine\r\n-e /home/matej/prace/egoscue/mmpose\r\nmmsegmentation==0.30.0\r\nmodel-index==0.1.11\r\nmongoengine==0.24.2\r\nmore-itertools==10.2.0\r\nmotor==3.1.2\r\nmpmath==1.2.1\r\nmsgpack==1.0.5\r\nmultidict==6.0.4\r\nmultivolumefile==0.2.3\r\nmunkres==1.1.4\r\nmypy-extensions==1.0.0\r\nnamex==0.0.7\r\nnbclient @ file:///croot/nbclient_1698934205032/work\r\nnbconvert @ file:///croot/nbconvert_1699022732553/work\r\nnbformat @ file:///croot/nbformat_1694616755618/work\r\nnest-asyncio @ file:///croot/nest-asyncio_1672387112409/work\r\nnetworkx==2.8.8\r\nnibabel==5.1.0\r\nnotebook @ file:///croot/notebook_1700582094678/work\r\nnotebook-shim @ file:///croot/notebook-shim_1699455894279/work\r\nnumba==0.58.1\r\nnumpy==1.24.3\r\nnumpy==1.26.4\r\nnvidia-ml-py==11.525.150\r\noauth2client==4.1.3\r\noauthlib==3.2.2\r\nomegaconf==2.3.0\r\nopen3d==0.18.0\r\nopen3d-cpu==0.18.0\r\nopencv-contrib-python==4.9.0.80\r\nopendatalab==0.0.10\r\nopenmim==0.3.9\r\nopenpyxl==3.1.2\r\nopentelemetry-api==1.20.0\r\nopentelemetry-exporter-otlp-proto-http==1.14.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-instrumentation-grpc==0.38b0\r\nopentelemetry-proto==1.14.0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\nopenvino==2023.1.0\r\nopenvino-dev==2023.1.0\r\nopenvino-telemetry==2023.2.1\r\nopenxlab==0.0.26\r\nopt-einsum==3.3.0\r\noptax==0.1.7\r\noptree==0.11.0\r\norbax-checkpoint==0.3.5\r\nordered-set==4.1.0\r\norjson==3.9.1\r\noss2==2.17.0\r\noverrides @ file:///croot/overrides_1699371140756/work\r\novmsclient==2023.1\r\npackaging @ file:///croot/packaging_1693575174725/work\r\npandas==2.0.0\r\npandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work\r\nparameterized==0.9.0\r\nparso @ file:///opt/conda/conda-bld/parso_1641458642106/work\r\npathspec==0.11.1\r\npexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\r\npfzy==0.3.4\r\npickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\r\npillow==9.5.0\r\npillow @ file:///croot/pillow_1696580024257/work\r\npillow-heif==0.10.1\r\npip==24.0\r\npip-requirements-parser==32.0.1\r\npip-tools==7.4.0\r\npipdeptree==2.19.0\r\n-e /home/matej/prace/equiscore/3rd_party/pixelstitch\r\nplatformdirs @ file:///croot/platformdirs_1692205439124/work\r\nplatformdirs==3.11.0\r\nplotly==5.15.0\r\npluggy==1.5.0\r\nply==3.11\r\nplyfile==0.9\r\nportalocker==2.8.2\r\n-e file:///home/matej/prace/egoscue/mmpose_service\r\npprintpp==0.4.0\r\nprettytable==3.6.0\r\npriority==2.0.0\r\nprometheus-client @ file:///tmp/abs_d3zeliano1/croots/recipe/prometheus_client_1659455100375/work\r\nprometheus-client==0.16.0\r\npromise==2.3\r\nprompt-toolkit==3.0.43\r\nproto-plus==1.23.0\r\nprotobuf==3.20.3\r\npsutil==5.9.4\r\npsutil==6.0.0\r\nptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\r\npure-eval @ file:///opt/conda/conda-bld/pure_eval_1646925070566/work\r\npy==1.11.0\r\npy-cpuinfo==9.0.0\r\npy7zr==0.20.5\r\npyarrow==14.0.2\r\npyarrow==14.0.2\r\npyasn1==0.5.0\r\npyasn1-modules==0.3.0\r\npybcj==1.0.1\r\npycocotools==2.0.6\r\npycodestyle==2.11.1\r\npycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\r\npycryptodome==3.19.0\r\npycryptodomex==3.18.0\r\npydantic==2.7.1\r\npydantic-core==2.18.2\r\npydot==1.4.2\r\npyemd==1.0.0\r\npyflakes==3.1.0\r\npygit2==1.13.2\r\npygments==2.14.0\r\npygments @ file:///croot/pygments_1684279966437/work\r\npygments==2.16.1\r\npygtrie==2.5.0\r\npyinquirer==1.0.3\r\npymongo==4.3.3\r\npynndescent==0.5.11\r\npynvml==11.5.0\r\npyodbc==5.0.1\r\npyopenssl @ file:///croot/pyopenssl_1690223430423/work\r\npyparsing==3.0.9\r\npyppmd==1.0.0\r\npyproject-hooks==1.0.0\r\npyqt5==5.15.10\r\npyqt5-sip @ file:///croot/pyqt-split_1698769088074/work/pyqt_sip\r\npyquaternion==0.9.9\r\npyrealsense2==2.54.2.5684\r\npysocks @ file:///tmp/build/80754af9/pysocks_1605305812635/work\r\npytest==8.2.2\r\npytest-runner==6.0.0\r\npython-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\r\npython-dotenv==1.0.1\r\npython-json-logger @ file:///croot/python-json-logger_1683823803357/work\r\npython-multipart==0.0.6\r\npython-rapidjson==1.12\r\npython-slugify==8.0.4\r\npytz==2023.3\r\npytz @ file:///croot/pytz_1695131579487/work\r\npywavelets==1.4.1\r\npyyaml==6.0\r\npyyaml @ file:///croot/pyyaml_1698096049011/work\r\npyzmq @ file:///croot/pyzmq_1705605076900/work\r\npyzstd==0.15.8\r\nqtconsole @ file:///croot/qtconsole_1700160644874/work\r\nqtpy @ file:///croot/qtpy_1700144840038/work\r\nrarfile==4.0\r\nreferencing==0.31.0\r\nreferencing==0.33.0\r\nregex==2023.3.23\r\nrequests==2.28.2\r\nrequests @ file:///croot/requests_1690400202158/work\r\nrequests-oauthlib==1.3.1\r\nrequests-toolbelt==1.0.0\r\nrerun-sdk @ file:///home/conda/feedstock_root/build_artifacts/rerun-sdk_1705065223242/work/rerun_py\r\nretrying==1.3.4\r\nrfc3339-validator @ file:///croot/rfc3339-validator_1683077044675/work\r\nrfc3986-validator @ file:///croot/rfc3986-validator_1683058983515/work\r\nrich==13.4.2\r\nroundrobin==0.0.4\r\nrpds-py @ file:///croot/rpds-py_1698945930462/work\r\nrpds-py==0.13.1\r\nrsa==4.9\r\nruamel-yaml==0.17.32\r\nruamel-yaml-clib==0.2.7\r\ns3transfer==0.6.0\r\nsacrebleu==2.4.1\r\nsafetensors==0.4.3\r\nschema==0.7.5\r\nscikit-image==0.21.0\r\nscikit-learn==1.1.2\r\nscipy==1.10.1\r\nscmrepo==1.4.1\r\nseaborn==0.13.2\r\nsecretstorage==3.3.3\r\nsemver==3.0.2\r\nsend2trash @ file:///croot/send2trash_1699371139552/work\r\nsentencepiece==0.2.0\r\nsentry-sdk==1.45.0\r\nseqeval==1.2.2\r\nsetproctitle==1.3.3\r\nsetuptools==60.2.0\r\nsetuptools==68.2.2\r\nshapely==2.0.1\r\nshellingham==1.5.4\r\nshortuuid==1.0.11\r\nshtab==1.6.4\r\nsimple-di==0.1.5\r\nsip @ file:///croot/sip_1698675935381/work\r\nsix @ file:///tmp/build/80754af9/six_1644875935023/work\r\nsmmap==5.0.1\r\nsniffio @ file:///croot/sniffio_1705431295498/work\r\nsortedcontainers==2.4.0\r\nsounddevice==0.4.6\r\nsoupsieve @ file:///croot/soupsieve_1696347547217/work\r\nsqltrie==0.8.0\r\nsse-starlette==0.10.3\r\nsseclient-py==1.7.2\r\nstack-data @ file:///opt/conda/conda-bld/stack_data_1646927590127/work\r\nstarlette==0.25.0\r\nstrawberry-graphql==0.138.1\r\nsympy==1.11.1\r\ntabulate==0.9.0\r\ntenacity==8.2.2\r\ntensorboard==2.15.2\r\ntensorboard-data-server==0.7.1\r\ntensorboardx==2.6.2.2\r\ntensorflow==2.15.1\r\ntensorflow-addons==0.23.0\r\ntensorflow-datasets==4.9.3\r\ntensorflow-decision-forests==1.8.1\r\ntensorflow-estimator==2.15.0\r\ntensorflow-hub==0.16.1\r\ntensorflow-io-gcs-filesystem==0.34.0\r\ntensorflow-metadata==1.14.0\r\ntensorflow-model-optimization==0.8.0\r\ntensorflowjs==4.18.0\r\ntensorstore==0.1.43\r\ntermcolor==2.3.0\r\nterminado @ file:///croot/terminado_1671751832461/work\r\nterminaltables==3.1.10\r\ntext-unidecode==1.3\r\ntexttable==1.6.7\r\ntf-keras==2.15.1\r\ntf-slim==1.1.0\r\nthreadpoolctl==3.1.0\r\ntifffile==2023.4.12\r\ntinycss2 @ file:///croot/tinycss2_1668168815555/work\r\ntokenizers==0.19.1\r\ntoml==0.10.2\r\ntomli @ file:///opt/conda/conda-bld/tomli_1657175507142/work\r\ntomli-w==1.0.0\r\ntomlkit==0.12.2\r\ntoolz==0.12.0\r\ntorch==2.0.0+cpu\r\ntorchvision==0.15.1+cpu\r\ntornado @ file:///croot/tornado_1696936946304/work\r\ntqdm==4.65.0\r\ntraitlets @ file:///croot/traitlets_1671143879854/work\r\ntransformers==4.43.1\r\ntritonclient==2.38.0\r\ntrove-classifiers==2024.2.23\r\ntypeguard==2.13.3\r\ntyper==0.9.0\r\ntypes-python-dateutil==2.8.19.14\r\ntyping-extensions==4.11.0\r\ntyping-inspect==0.9.0\r\ntzdata==2023.3\r\ntzlocal==5.0.1\r\numap-learn==0.5.5\r\nuniversal-analytics-python3==1.1.1\r\nuri-template==1.3.0\r\nuritemplate==4.1.1\r\nurllib3==1.26.13\r\nurllib3 @ file:///croot/urllib3_1698257533958/work\r\nuserpath==1.9.2\r\nuv==0.2.27\r\nuvicorn==0.30.1\r\nvine==5.0.0\r\nvirtualenv==20.25.1\r\nvoluptuous==0.13.1\r\nvoxel51-eta==0.12.6\r\nwandb==0.16.6\r\nwatchfiles==0.19.0\r\nwcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work\r\nwcwidth==0.2.6\r\nwebcolors==1.13\r\nwebencodings==0.5.1\r\nwebsocket-client @ file:///tmp/build/80754af9/websocket-client_1614803975924/work\r\nwerkzeug==3.0.1\r\nwheel==0.41.2\r\nwidgetsnbextension @ file:///croot/widgetsnbextension_1679313860248/work\r\nwrapt==1.14.1\r\nwsproto==1.2.0\r\nwurlitzer==3.0.3\r\nxdoctest==1.1.2\r\nxmltodict==0.13.0\r\nxtcocotools==1.13\r\nyapf==0.32.0\r\nyarl==1.8.2\r\nzc-lockfile==3.0.post1\r\nzipp==3.15.0\r\nzipp @ file:///croot/zipp_1704206909481/work\r\nzope-event==5.0\r\nzope-interface==6.1\r\nzstandard==0.22.0\r\n```\r\n\r\n</details>\r\n",
    "comments": []
  },
  {
    "issue_number": 4789,
    "title": "bug: timeout configuration not work ",
    "author": "BangDaeng",
    "state": "open",
    "created_at": "2024-06-11T07:32:31Z",
    "updated_at": "2024-07-18T03:19:12Z",
    "labels": [
      "bug",
      "feedback-wanted"
    ],
    "body": "### Describe the bug\r\n\r\nThe timeout setting of api_server and runner is not working in bentoml.\r\ni'm using bentoml 1.0.20.post11 version\r\nThe default configuration is as follows\r\n\r\n```yaml\r\nversion: 1\r\napi_server:\r\n  workers: ~ # cpu_count() will be used when null\r\n  timeout: 60\r\n  backlog: 2048\r\n  # the maximum number of connections that will be made to any given runner server at once\r\n  max_runner_connections: 16\r\n  metrics:\r\n    enabled: true\r\n    namespace: bentoml_api_server\r\n    duration:\r\n      # https://github.com/prometheus/client_python/blob/f17a8361ad3ed5bc47f193ac03b00911120a8d81/prometheus_client/metrics.py#L544\r\n      buckets: [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0, 7.5, 10.0]\r\n      min: ~\r\n      max: ~\r\n      factor: ~\r\n  logging:\r\n    access:\r\n      enabled: true\r\n      request_content_length: true\r\n      request_content_type: true\r\n      response_content_length: true\r\n      response_content_type: true\r\n      format:\r\n        trace_id: 032x\r\n        span_id: 016x\r\n  ssl:\r\n    enabled: false\r\n    certfile: ~\r\n    keyfile: ~\r\n    keyfile_password: ~\r\n    ca_certs: ~\r\n    version: 17 # ssl.PROTOCOL_TLS_SERVER\r\n    cert_reqs: 0 # ssl.CERT_NONE\r\n    ciphers: TLSv1 # default ciphers\r\n  http:\r\n    host: 0.0.0.0\r\n    port: 3000\r\n    cors:\r\n      enabled: false\r\n      access_control_allow_origins: ~\r\n      access_control_allow_credentials: ~\r\n      access_control_allow_methods: ~\r\n      access_control_allow_headers: ~\r\n      access_control_allow_origin_regex: ~\r\n      access_control_max_age: ~\r\n      access_control_expose_headers: ~\r\n    response:\r\n      trace_id: false\r\n  grpc:\r\n    host: 0.0.0.0\r\n    port: 3000\r\n    max_concurrent_streams: ~\r\n    maximum_concurrent_rpcs: ~\r\n    max_message_length: -1\r\n    reflection:\r\n      enabled: false\r\n    channelz:\r\n      enabled: false\r\n    metrics:\r\n      host: 0.0.0.0\r\n      port: 3001\r\n  runner_probe: # configure whether the API server's health check endpoints (readyz, livez, healthz) also check the runners\r\n    enabled: true\r\n    timeout: 1\r\n    period: 10\r\nrunners:\r\n  resources: ~\r\n  workers_per_resource: 1\r\n  timeout: 300\r\n  batching:\r\n    enabled: true\r\n    max_batch_size: 100\r\n    max_latency_ms: 10000\r\n  logging:\r\n    access:\r\n      enabled: true\r\n      request_content_length: true\r\n      request_content_type: true\r\n      response_content_length: true\r\n      response_content_type: true\r\n  metrics:\r\n    enabled: true\r\n    namespace: bentoml_runner\r\ntracing:\r\n  exporter_type: ~\r\n  sample_rate: ~\r\n  excluded_urls: ~\r\n  timeout: ~\r\n  max_tag_value_length: ~\r\n  zipkin:\r\n    endpoint: ~\r\n    local_node_ipv4: ~\r\n    local_node_ipv6: ~\r\n    local_node_port: ~\r\n  jaeger:\r\n    protocol: thrift\r\n    collector_endpoint: ~\r\n    thrift:\r\n      agent_host_name: ~\r\n      agent_port: ~\r\n      udp_split_oversized_batches: ~\r\n    grpc:\r\n      insecure: ~\r\n  otlp:\r\n    protocol: ~\r\n    endpoint: ~\r\n    compression: ~\r\n    http:\r\n      certificate_file: ~\r\n      headers: ~\r\n    grpc:\r\n      headers: ~\r\n      insecure: ~\r\nmonitoring:\r\n  enabled: true\r\n  type: default\r\n  options:\r\n    log_config_file: ~\r\n    log_path: monitoring\r\n```\r\n--------------\r\nI modified the timeout of the api_server and the timeout of the runners, but the settings are not applied\r\nWhen I write service.py as below and enter kentoml serve and test it, it doesn't work normally.\r\n\r\nI tried changing the default configuration\r\nI wrote configuration.yaml and tried changing it.\r\nI also checked that it changed properly by printing the config setting.\r\n\r\nbut not work, please help me\r\n\r\n```python\r\n@svc.api(\r\n    input=JSON.from_sample(samples),\r\n    output=NumpyNdarray(),\r\n    route=BUILD_NAME,\r\n)\r\nasync def predict(sentences):\r\n    time.sleep(350)\r\n    output = await scappy_runner.predict.async_run(sentences)\r\n    return output\r\n```\r\n\r\nPlease Help Me\r\n\r\n\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nbentoml: 1.0.20.post11\r\npython: 3.10.12",
    "comments": [
      {
        "user": "frostming",
        "body": "Can you try on the latest version of BentoML?"
      }
    ]
  },
  {
    "issue_number": 4839,
    "title": "bug: torch tensor converted to numpy array during serialization & non-writable warning",
    "author": "rlleshi",
    "state": "closed",
    "created_at": "2024-07-01T18:26:50Z",
    "updated_at": "2024-07-17T06:34:47Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nI'm passing a torch tensor to another bentoml service but it appears to be first converted into a numpy array during serialization.\r\n\r\nThis is then the cause of the following warning:\r\n\r\n```\r\n/home/user/.pyenv/versions/3.10.0/envs/tensor_np_conversion_bug/lib/python3.10/site-packages/_bentoml_sdk/validators.py:276: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\r\n  return torch.from_numpy(obj)\r\n```\r\n\n\n### To reproduce\n\nMinimal reproducible example:\r\n\r\n`service.py`\r\n\r\n```\r\nfrom __future__ import annotations\r\n\r\nimport torch\r\nimport bentoml\r\n\r\n@bentoml.service()\r\nclass TestService:\r\n\r\n    def __init__(self) -> None:\r\n        pass\r\n\r\n    @bentoml.api\r\n    def test(self, image: torch.Tensor) -> torch.Tensor:\r\n        print(f'Type inside TestService.test: {type(image)}')\r\n        return image\r\n```\r\n\r\n`service2.py`\r\n\r\n```\r\nfrom __future__ import annotations\r\n\r\nimport torch\r\nimport bentoml\r\nimport cv2\r\nfrom .service import TestService\r\n\r\n@bentoml.service()\r\nclass TestService2:\r\n\r\n    test_service = bentoml.depends(TestService)\r\n\r\n    def __init__(self) -> None:\r\n        pass\r\n\r\n    @bentoml.api\r\n    def test2(self, image_path: str) -> str:\r\n        img = cv2.imread(image_path)\r\n        print('Writeable ', img.flags.writeable)\r\n        img_tensor = torch.from_numpy(img).permute(2, 0, 1)\r\n        print(f'Type inside TestService2.test2 (before call): {type(img_tensor)}')\r\n        result = self.test_service.test(img_tensor)\r\n        print(f'Type inside TestService2.test2 (after call): {type(result)}')\r\n        return 'OK'\r\n```\r\n\r\n`bentofile.yaml`\r\n\r\n```\r\nservice: \"src.service2:TestService2\"\r\ninclude:\r\n  - \"*.py\"\r\npython:\r\n  requirements_txt: requirements.txt\r\n```\r\n\r\n`requirements.txt`\r\n```\r\nbentoml\r\nopencv-python\r\ntorch\r\n```\r\n\r\nHere's the logs: \r\n\r\n❯ bentoml serve .\r\n2024-07-01T19:52:45+0200 [INFO] [cli] Starting production HTTP BentoServer from \"src.service2:TestService2\" listening on http://localhost:3000 (Press CTRL+C to quit)\r\nWriteable  True\r\nType inside TestService2.test2 (before call): <class 'torch.Tensor'>\r\n<class 'torch.Tensor'>, torch.Size([3, 958, 640]), File: /home/user/.pyenv/versions/3.10.0/envs/tensor_np_conversion_bug/lib/python3.10/site-packages/pydantic/main.py, Line: 176\r\n<class 'numpy.ndarray'>, (3, 958, 640), File: /home/user/.pyenv/versions/3.10.0/envs/tensor_np_conversion_bug/lib/python3.10/site-packages/pydantic/main.py, Line: 551\r\nType inside TestService.test: <class 'torch.Tensor'>\r\n<class 'torch.Tensor'>, torch.Size([3, 958, 640]), File: /home/user/.pyenv/versions/3.10.0/envs/tensor_np_conversion_bug/lib/python3.10/site-packages/pydantic/root_model.py, Line: 71\r\n2024-07-01T19:52:47+0200 [INFO] [service:TestService:1] _ (scheme=http,method=POST,path=/test,type=application/vnd.bentoml+pickle,length=1839537) (status=200,type=application/json,length=1839484) 13.497ms (trace=7ad7af6d592d6d9d5bad7ac796537c24,span=5920153c7d24fc1a,sampled=0,service.name=TestService)\r\n<class 'numpy.ndarray'>, (3, 958, 640), File: /home/user/.pyenv/versions/3.10.0/envs/tensor_np_conversion_bug/lib/python3.10/site-packages/pydantic/main.py, Line: 551\r\n/home/user/.pyenv/versions/3.10.0/envs/tensor_np_conversion_bug/lib/python3.10/site-packages/_bentoml_sdk/validators.py:276: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\r\n  return torch.from_numpy(obj)\r\nType inside TestService2.test2 (after call): <class 'torch.Tensor'>\r\n2024-07-01T19:52:47+0200 [INFO] [entry_service:TestService2:1] 127.0.0.1:41972 (scheme=http,method=POST,path=/test2,type=application/json,length=120) (status=200,type=text/plain; charset=utf-8,length=2) 72.050ms (trace=bf613730b60be2b0d0d8cd6da854c21b,span=543b6aa1b67d6475,sampled=0,service.name=TestService2)\r\n\r\nSample curl request: \r\n\r\n```\r\ncurl --location --request POST 'http://localhost:3000/test2' \\\r\n--header 'Content-Type: application/json' \\\r\n--data-raw '{\r\n    \"image_path\": \"path_to_image\"\r\n}'\r\n```\n\n### Expected behavior\n\nPerhaps I'm still not super familiar with the documentation as I just started using bentoml. But I had thought that it supported sending tensors. \r\n\r\nAlso, if they have to be converted to numpy first, they should be writable when converted back to tensor, no?\n\n### Environment\n\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.2.19\r\n`python`: 3.10.0\r\n`platform`: Linux-6.5.0-42-generic-x86_64-with-glibc2.38\r\n`uid_gid`: 1000:1000\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiohttp==3.9.5\r\naiosignal==1.3.1\r\nannotated-types==0.7.0\r\nanyio==4.4.0\r\nappdirs==1.4.4\r\nasgiref==3.8.1\r\nasync-timeout==4.0.3\r\nattrs==23.2.0\r\nbentoml==1.2.19\r\nbuild==1.2.1\r\ncattrs==23.1.2\r\ncertifi==2024.6.2\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==3.0.0\r\ndeepmerge==1.1.1\r\nDeprecated==1.2.14\r\nexceptiongroup==1.2.1\r\nfilelock==3.15.4\r\nfrozenlist==1.4.1\r\nfs==2.4.16\r\nfsspec==2024.6.1\r\nh11==0.14.0\r\nhttpcore==1.0.5\r\nhttpx==0.27.0\r\nhttpx-ws==0.6.0\r\nidna==3.7\r\nimportlib-metadata==6.11.0\r\ninflection==0.5.1\r\nJinja2==3.1.4\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmdurl==0.1.2\r\nmpmath==1.3.0\r\nmultidict==6.0.5\r\nnetworkx==3.3\r\nnumpy==2.0.0\r\nnvidia-cublas-cu12==12.1.3.1\r\nnvidia-cuda-cupti-cu12==12.1.105\r\nnvidia-cuda-nvrtc-cu12==12.1.105\r\nnvidia-cuda-runtime-cu12==12.1.105\r\nnvidia-cudnn-cu12==8.9.2.26\r\nnvidia-cufft-cu12==11.0.2.54\r\nnvidia-curand-cu12==10.3.2.106\r\nnvidia-cusolver-cu12==11.4.5.107\r\nnvidia-cusparse-cu12==12.1.0.106\r\nnvidia-ml-py==11.525.150\r\nnvidia-nccl-cu12==2.20.5\r\nnvidia-nvjitlink-cu12==12.5.40\r\nnvidia-nvtx-cu12==12.1.105\r\nopencv-python==4.10.0.84\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\npackaging==24.1\r\npathspec==0.12.1\r\npip-requirements-parser==32.0.1\r\npip-tools==7.4.1\r\nprometheus_client==0.20.0\r\npsutil==6.0.0\r\npydantic==2.7.4\r\npydantic_core==2.18.4\r\nPygments==2.18.0\r\npyparsing==3.1.2\r\npyproject_hooks==1.1.0\r\npython-dateutil==2.9.0.post0\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.9\r\nPyYAML==6.0.1\r\npyzmq==26.0.3\r\nrich==13.7.1\r\nschema==0.7.7\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.1\r\nstarlette==0.37.2\r\nsympy==1.12.1\r\ntomli==2.0.1\r\ntomli_w==1.0.0\r\ntorch==2.3.1\r\ntornado==6.4.1\r\ntriton==2.3.1\r\ntyping_extensions==4.12.2\r\nuvicorn==0.30.1\r\nwatchfiles==0.22.0\r\nwrapt==1.16.0\r\nwsproto==1.2.0\r\nyarl==1.9.4\r\nzipp==3.19.2\r\n```\r\n\r\n</details>\r\n",
    "comments": [
      {
        "user": "frostming",
        "body": "> Also, if they have to be converted to numpy first, they should be writable when converted back to tensor, no?\r\n\r\nNo, the underlying data are mapped to a block of memory to avoid copy during (de)serialization.\r\n\r\nWe'll try to suppress the warning for users, but for now you can just ignore it and don't write data to the tensor, or copy the tensor yourself, if you do want to write to it."
      },
      {
        "user": "rlleshi",
        "body": "Thanks for the quick response!\r\n\r\nI see. Could bentoml then preserve the state of the writeable flag under the hood perhaps? I have a bunch of services and would have to take care to do this everywhere as the tensor flows from one service to another. "
      },
      {
        "user": "frostming",
        "body": "> Could bentoml then preserve the state of the writeable flag under the hood perhaps\r\n\r\nbentoml doesn't change anything, and the writeable flag isn't \"set\" by anyone but from the inspection of the underlying data. This warning can be suppressed by copying that tensor but we don't copy intentionally for performance purposes."
      }
    ]
  },
  {
    "issue_number": 4863,
    "title": "bug: Async Return Latency Issues with BentoML Image IO API",
    "author": "takhyun12",
    "state": "open",
    "created_at": "2024-07-16T08:27:59Z",
    "updated_at": "2024-07-16T08:27:59Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nHello,\r\n\r\nI'm facing an issue with BentoML API serving where significant delays occur during the async return of images. \r\n\r\nHere’s the simplified code:\r\n\r\n```python\r\nfrom io import BytesIO\r\nimport time\r\nfrom bentoml.io import Multipart, File, Image\r\nfrom PIL import Image as PILImage\r\n\r\n\r\n@service.api(\r\n    input=Multipart(data=File()),\r\n    output=Image(mime_type=\"image/png\"),\r\n    route=\"/inpaint/test/wrinkles\",\r\n)\r\nasync def api_test(data: BytesIO):\r\n    start_time = time.time()\r\n    image = PILImage.open(BytesIO(data.read())).convert(\"RGB\")\r\n    print(\"Image loaded in\", time.time() - start_time, \"seconds\")\r\n    return image\r\n````\r\n\r\nLoading the image is efficient (0.08-0.1 seconds), but returning it asynchronously incurs a delay of up to 10 seconds. \r\nAttempting to resolve this with a runner leads to a format error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/firm/lib/python3.10/site-packages/bentoml/_internal/server/http_app.py\", line 334, in api_func\r\n    output = await api.func(**input_data)\r\n  File \"/root/snowflake/backend/python/firm/services/inpaint/service.py\", line 215, in api\r\n    return await post_process_runner.forward.async_run(source=image, target=output_image)\r\n  File \"/root/miniconda3/envs/firm/lib/python3.10/site-packages/bentoml/_internal/runner/runner.py\", line 56, in async_run\r\n    return await self.runner._runner_handle.async_run_method(self, *args, **kwargs)\r\n  File \"/root/miniconda3/envs/firm/lib/python3.10/site-packages/bentoml/_internal/runner/runner_handle/remote.py\", line 201, in async_run_method\r\n    payload_params = Params[Payload](*args, **kwargs).map(\r\n  File \"/root/miniconda3/envs/firm/lib/python3.10/site-packages/bentoml/_internal/runner/utils.py\", line 65, in map\r\n    kwargs = {k: function(v) for k, v in self.kwargs.items()}\r\n  File \"/root/miniconda3/envs/firm/lib/python3.10/site-packages/bentoml/_internal/runner/utils.py\", line 65, in <dictcomp>\r\n    kwargs = {k: function(v) for k, v in self.kwargs.items()}\r\n  File \"/root/miniconda3/envs/firm/lib/python3.10/site-packages/bentoml/_internal/runner/container.py\", line 700, in to_payload\r\n    return container_cls.to_payload(batch, batch_dim)\r\n  File \"/root/miniconda3/envs/firm/lib/python3.10/site-packages/bentoml/_internal/runner/container.py\", line 490, in to_payload\r\n    batch.save(buffer, format=batch.format)\r\n  File \"/root/miniconda3/envs/firm/lib/python3.10/site-packages/PIL/Image.py\", line 2546, in save\r\n    raise ValueError(msg) from e\r\nValueError: unknown file extension:\r\n```\r\n\r\nHow can I properly handle async returns with bentoml.io.Image() to avoid these delays?\r\n\r\nThank you for your assistance.\r\n\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.2.16\r\n`python`: 3.10.14\r\n`platform`: Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.31\r\n`uid_gid`: 0:0\r\n`conda`: 23.5.0\r\n`in_conda_env`: True\r\n<details><summary><code>conda_packages</code></summary>\r\n\r\n<br>\r\n\r\n```yaml\r\nname: firm\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=5.1=1_gnu\r\n  - bzip2=1.0.8=h5eee18b_6\r\n  - ca-certificates=2024.3.11=h06a4308_0\r\n  - ld_impl_linux-64=2.38=h1181459_1\r\n  - libffi=3.4.4=h6a678d5_1\r\n  - libgcc-ng=11.2.0=h1234567_1\r\n  - libgomp=11.2.0=h1234567_1\r\n  - libstdcxx-ng=11.2.0=h1234567_1\r\n  - libuuid=1.41.5=h5eee18b_0\r\n  - ncurses=6.4=h6a678d5_0\r\n  - openssl=3.0.14=h5eee18b_0\r\n  - pip=24.0=py310h06a4308_0\r\n  - python=3.10.14=h955ad1f_1\r\n  - readline=8.2=h5eee18b_0\r\n  - setuptools=69.5.1=py310h06a4308_0\r\n  - sqlite=3.45.3=h5eee18b_0\r\n  - tk=8.6.14=h39e8969_0\r\n  - wheel=0.43.0=py310h06a4308_0\r\n  - xz=5.4.6=h5eee18b_1\r\n  - zlib=1.2.13=h5eee18b_1\r\n  - pip:\r\n      - absl-py==2.1.0\r\n      - aenum==3.1.15\r\n      - aiofiles==24.1.0\r\n      - aiohttp==3.9.5\r\n      - aioresponses==0.7.6\r\n      - aiosignal==1.3.1\r\n      - annotated-types==0.7.0\r\n      - anyio==4.4.0\r\n      - appdirs==1.4.4\r\n      - apscheduler==3.10.1\r\n      - asgiref==3.8.1\r\n      - asttokens==2.4.1\r\n      - async-timeout==4.0.3\r\n      - attrs==23.2.0\r\n      - awscli==1.29.54\r\n      - backoff==2.2.1\r\n      - bentoml==1.2.16\r\n      - blendmodes==2024.1\r\n      - boto3==1.28.23\r\n      - botocore==1.31.54\r\n      - build==1.2.1\r\n      - cachetools==5.3.3\r\n      - cattrs==23.1.2\r\n      - certifi==2024.7.4\r\n      - cffi==1.16.0\r\n      - charset-normalizer==3.3.2\r\n      - circus==0.18.0\r\n      - click==8.1.7\r\n      - click-option-group==0.5.6\r\n      - cloudpickle==3.0.0\r\n      - cmake==3.30.0\r\n      - colorama==0.4.4\r\n      - coloredlogs==15.0.1\r\n      - comm==0.2.2\r\n      - contourpy==1.2.1\r\n      - coverage==7.5.1\r\n      - cryptography==42.0.8\r\n      - cycler==0.12.1\r\n      - cython==3.0.0\r\n      - dataclasses-json==0.6.7\r\n      - decorator==5.1.1\r\n      - deepmerge==1.1.1\r\n      - defusedxml==0.7.1\r\n      - deprecated==1.2.14\r\n      - distro==1.9.0\r\n      - docker==6.1.3\r\n      - docutils==0.16\r\n      - exceptiongroup==1.2.1\r\n      - executing==2.0.1\r\n      - fastapi==0.110.3\r\n      - filelock==3.15.4\r\n      - flatbuffers==24.3.25\r\n      - fonttools==4.53.1\r\n      - frozenlist==1.4.1\r\n      - fs==2.4.16\r\n      - fsspec==2024.6.1\r\n      - google-api-core==2.19.1\r\n      - google-api-python-client==2.136.0\r\n      - google-auth==2.31.0\r\n      - google-auth-httplib2==0.2.0\r\n      - google-cloud-core==2.4.1\r\n      - google-cloud-storage==2.17.0\r\n      - google-crc32c==1.5.0\r\n      - google-resumable-media==2.7.1\r\n      - googleapis-common-protos==1.63.2\r\n      - gputil==1.4.0\r\n      - h11==0.14.0\r\n      - httpcore==1.0.5\r\n      - httplib2==0.22.0\r\n      - httpx==0.27.0\r\n      - huggingface-hub==0.23.4\r\n      - humanfriendly==10.0\r\n      - idna==3.7\r\n      - imagehash==4.3.1\r\n      - imageio==2.34.2\r\n      - importlib-metadata==6.11.0\r\n      - inference-gpu==0.12.0\r\n      - inflection==0.5.1\r\n      - iniconfig==2.0.0\r\n      - ipython==8.26.0\r\n      - ipywidgets==8.1.3\r\n      - jax==0.4.30\r\n      - jaxlib==0.4.30\r\n      - jedi==0.19.1\r\n      - jinja2==3.1.4\r\n      - jmespath==1.0.1\r\n      - jsonschema==4.22.0\r\n      - jsonschema-specifications==2023.12.1\r\n      - jupyterlab-widgets==3.0.11\r\n      - kiwisolver==1.4.5\r\n      - lazy-loader==0.4\r\n      - line-profiler==4.1.3\r\n      - lit==18.1.8\r\n      - markdown-it-py==3.0.0\r\n      - markupsafe==2.1.5\r\n      - marshmallow==3.21.3\r\n      - matplotlib==3.9.1\r\n      - matplotlib-inline==0.1.7\r\n      - mdurl==0.1.2\r\n      - mediapipe==0.10.14\r\n      - ml-dtypes==0.4.0\r\n      - mpmath==1.3.0\r\n      - multidict==6.0.5\r\n      - mypy-extensions==1.0.0\r\n      - natsort==8.4.0\r\n      - networkx==3.3\r\n      - numpy==1.26.4\r\n      - nvidia-cublas-cu11==11.10.3.66\r\n      - nvidia-cuda-cupti-cu11==11.7.101\r\n      - nvidia-cuda-nvrtc-cu11==11.7.99\r\n      - nvidia-cuda-runtime-cu11==11.7.99\r\n      - nvidia-cudnn-cu11==8.5.0.96\r\n      - nvidia-cufft-cu11==10.9.0.58\r\n      - nvidia-curand-cu11==10.2.10.91\r\n      - nvidia-cusolver-cu11==11.4.0.1\r\n      - nvidia-cusparse-cu11==11.7.4.91\r\n      - nvidia-ml-py==11.525.150\r\n      - nvidia-nccl-cu11==2.14.3\r\n      - nvidia-nvtx-cu11==11.7.91\r\n      - onnxruntime-gpu==1.15.1\r\n      - openai==1.35.10\r\n      - opencv-contrib-python==4.10.0.84\r\n      - opencv-python==4.8.0.76\r\n      - opencv-python-headless==4.10.0.84\r\n      - opentelemetry-api==1.20.0\r\n      - opentelemetry-instrumentation==0.41b0\r\n      - opentelemetry-instrumentation-aiohttp-client==0.41b0\r\n      - opentelemetry-instrumentation-asgi==0.41b0\r\n      - opentelemetry-sdk==1.20.0\r\n      - opentelemetry-semantic-conventions==0.41b0\r\n      - opentelemetry-util-http==0.41b0\r\n      - opt-einsum==3.3.0\r\n      - packaging==24.1\r\n      - pandas==2.2.2\r\n      - parso==0.8.4\r\n      - pathspec==0.12.1\r\n      - pendulum==3.0.0\r\n      - pexpect==4.9.0\r\n      - piexif==1.1.3\r\n      - pillow==10.4.0\r\n      - pillow-heif==0.14.0\r\n      - pip-requirements-parser==32.0.1\r\n      - pip-tools==7.4.1\r\n      - pluggy==1.5.0\r\n      - prettytable==3.10.0\r\n      - prometheus-client==0.20.0\r\n      - prometheus-fastapi-instrumentator==6.0.0\r\n      - prompt-toolkit==3.0.47\r\n      - proto-plus==1.24.0\r\n      - protobuf==4.25.3\r\n      - psutil==6.0.0\r\n      - ptyprocess==0.7.0\r\n      - pulp==2.8.0\r\n      - pure-eval==0.2.2\r\n      - py-cpuinfo==9.0.0\r\n      - pyasn1==0.6.0\r\n      - pyasn1-modules==0.4.0\r\n      - pybase64==1.3.2\r\n      - pycparser==2.22\r\n      - pydantic==2.8.2\r\n      - pydantic-core==2.20.1\r\n      - pydot==2.0.0\r\n      - pyfacer==0.0.4\r\n      - pygments==2.18.0\r\n      - pyparsing==3.1.2\r\n      - pyproject-hooks==1.1.0\r\n      - pytest==8.2.2\r\n      - pytest-asyncio==0.21.1\r\n      - python-dateutil==2.9.0.post0\r\n      - python-dotenv==1.0.1\r\n      - python-json-logger==2.0.7\r\n      - python-multipart==0.0.9\r\n      - pytz==2024.1\r\n      - pywavelets==1.6.0\r\n      - pyyaml==6.0.1\r\n      - pyzmq==26.0.3\r\n      - redis==5.0.7\r\n      - referencing==0.35.1\r\n      - requests==2.31.0\r\n      - requests-toolbelt==1.0.0\r\n      - rich==13.5.2\r\n      - rpds-py==0.18.1\r\n      - rsa==4.7.2\r\n      - s3transfer==0.6.2\r\n      - safetensors==0.4.3\r\n      - schema==0.7.7\r\n      - scikit-image==0.24.0\r\n      - scipy==1.14.0\r\n      - seaborn==0.13.2\r\n      - shapely==2.0.1\r\n      - simple-di==0.1.5\r\n      - six==1.16.0\r\n      - skypilot==0.5.0\r\n      - sniffio==1.3.1\r\n      - sounddevice==0.4.7\r\n      - stack-data==0.6.3\r\n      - starlette==0.37.2\r\n      - structlog==24.2.0\r\n      - supervision==0.21.0\r\n      - sympy==1.12.1\r\n      - tabulate==0.9.0\r\n      - thop==0.1.1-2209072238\r\n      - tifffile==2024.7.2\r\n      - time-machine==2.14.2\r\n      - timm==1.0.3\r\n      - tomli==2.0.1\r\n      - tomli-w==1.0.0\r\n      - torch==2.0.1\r\n      - torchaudio==2.0.2\r\n      - torchvision==0.15.2\r\n      - tornado==6.4.1\r\n      - tqdm==4.66.4\r\n      - traitlets==5.14.3\r\n      - triton==2.0.0\r\n      - typer==0.9.0\r\n      - typing-extensions==4.12.2\r\n      - typing-inspect==0.9.0\r\n      - tzdata==2024.1\r\n      - tzlocal==5.2\r\n      - ultralytics==8.2.18\r\n      - uritemplate==4.1.1\r\n      - urllib3==1.26.19\r\n      - uvicorn==0.30.1\r\n      - validators==0.30.0\r\n      - watchfiles==0.22.0\r\n      - wcwidth==0.2.13\r\n      - websocket-client==1.8.0\r\n      - widgetsnbextension==4.0.11\r\n      - wrapt==1.16.0\r\n      - yarl==1.9.4\r\n      - zipp==3.19.2\r\n      - zxing-cpp==2.2.0\r\nprefix: /root/miniconda3/envs/firm\r\n```\r\n\r\n</details>\r\n\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\nabsl-py==2.1.0\r\naenum==3.1.15\r\naiofiles==24.1.0\r\naiohttp==3.9.5\r\naioresponses==0.7.6\r\naiosignal==1.3.1\r\nannotated-types==0.7.0\r\nanyio==4.4.0\r\nappdirs==1.4.4\r\nAPScheduler==3.10.1\r\nasgiref==3.8.1\r\nasttokens==2.4.1\r\nasync-timeout==4.0.3\r\nattrs==23.2.0\r\nawscli==1.29.54\r\nbackoff==2.2.1\r\nbentoml==1.2.16\r\nblendmodes==2024.1\r\nboto3==1.28.23\r\nbotocore==1.31.54\r\nbuild==1.2.1\r\ncachetools==5.3.3\r\ncattrs==23.1.2\r\ncertifi==2024.7.4\r\ncffi==1.16.0\r\ncharset-normalizer==3.3.2\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==3.0.0\r\ncmake==3.30.0\r\ncolorama==0.4.4\r\ncoloredlogs==15.0.1\r\ncomm==0.2.2\r\ncontourpy==1.2.1\r\ncoverage==7.5.1\r\ncryptography==42.0.8\r\ncycler==0.12.1\r\nCython==3.0.0\r\ndataclasses-json==0.6.7\r\ndecorator==5.1.1\r\ndeepmerge==1.1.1\r\ndefusedxml==0.7.1\r\nDeprecated==1.2.14\r\ndistro==1.9.0\r\ndocker==6.1.3\r\ndocutils==0.16\r\nexceptiongroup==1.2.1\r\nexecuting==2.0.1\r\nfastapi==0.110.3\r\nfilelock==3.15.4\r\nflatbuffers==24.3.25\r\nfonttools==4.53.1\r\nfrozenlist==1.4.1\r\nfs==2.4.16\r\nfsspec==2024.6.1\r\ngoogle-api-core==2.19.1\r\ngoogle-api-python-client==2.136.0\r\ngoogle-auth==2.31.0\r\ngoogle-auth-httplib2==0.2.0\r\ngoogle-cloud-core==2.4.1\r\ngoogle-cloud-storage==2.17.0\r\ngoogle-crc32c==1.5.0\r\ngoogle-resumable-media==2.7.1\r\ngoogleapis-common-protos==1.63.2\r\nGPUtil==1.4.0\r\nh11==0.14.0\r\nhttpcore==1.0.5\r\nhttplib2==0.22.0\r\nhttpx==0.27.0\r\nhuggingface-hub==0.23.4\r\nhumanfriendly==10.0\r\nidna==3.7\r\nImageHash==4.3.1\r\nimageio==2.34.2\r\nimportlib-metadata==6.11.0\r\ninference-gpu==0.12.0\r\ninflection==0.5.1\r\niniconfig==2.0.0\r\nipython==8.26.0\r\nipywidgets==8.1.3\r\njax==0.4.30\r\njaxlib==0.4.30\r\njedi==0.19.1\r\nJinja2==3.1.4\r\njmespath==1.0.1\r\njsonschema==4.22.0\r\njsonschema-specifications==2023.12.1\r\njupyterlab_widgets==3.0.11\r\nkiwisolver==1.4.5\r\nlazy_loader==0.4\r\nline_profiler==4.1.3\r\nlit==18.1.8\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmarshmallow==3.21.3\r\nmatplotlib==3.9.1\r\nmatplotlib-inline==0.1.7\r\nmdurl==0.1.2\r\nmediapipe==0.10.14\r\nml-dtypes==0.4.0\r\nmpmath==1.3.0\r\nmultidict==6.0.5\r\nmypy-extensions==1.0.0\r\nnatsort==8.4.0\r\nnetworkx==3.3\r\nnumpy==1.26.4\r\nnvidia-cublas-cu11==11.10.3.66\r\nnvidia-cuda-cupti-cu11==11.7.101\r\nnvidia-cuda-nvrtc-cu11==11.7.99\r\nnvidia-cuda-runtime-cu11==11.7.99\r\nnvidia-cudnn-cu11==8.5.0.96\r\nnvidia-cufft-cu11==10.9.0.58\r\nnvidia-curand-cu11==10.2.10.91\r\nnvidia-cusolver-cu11==11.4.0.1\r\nnvidia-cusparse-cu11==11.7.4.91\r\nnvidia-ml-py==11.525.150\r\nnvidia-nccl-cu11==2.14.3\r\nnvidia-nvtx-cu11==11.7.91\r\nonnxruntime-gpu==1.15.1\r\nopenai==1.35.10\r\nopencv-contrib-python==4.10.0.84\r\nopencv-python==4.8.0.76\r\nopencv-python-headless==4.10.0.84\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\nopt-einsum==3.3.0\r\npackaging==24.1\r\npandas==2.2.2\r\nparso==0.8.4\r\npathspec==0.12.1\r\npendulum==3.0.0\r\npexpect==4.9.0\r\npiexif==1.1.3\r\npillow==10.4.0\r\npillow-heif==0.14.0\r\npip-requirements-parser==32.0.1\r\npip-tools==7.4.1\r\npluggy==1.5.0\r\nprettytable==3.10.0\r\nprometheus-fastapi-instrumentator==6.0.0\r\nprometheus_client==0.20.0\r\nprompt_toolkit==3.0.47\r\nproto-plus==1.24.0\r\nprotobuf==4.25.3\r\npsutil==6.0.0\r\nptyprocess==0.7.0\r\nPuLP==2.8.0\r\npure-eval==0.2.2\r\npy-cpuinfo==9.0.0\r\npyasn1==0.6.0\r\npyasn1_modules==0.4.0\r\npybase64==1.3.2\r\npycparser==2.22\r\npydantic==2.8.2\r\npydantic_core==2.20.1\r\npydot==2.0.0\r\npyfacer==0.0.4\r\nPygments==2.18.0\r\npyparsing==3.1.2\r\npyproject_hooks==1.1.0\r\npytest==8.2.2\r\npytest-asyncio==0.21.1\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.9\r\npytz==2024.1\r\nPyWavelets==1.6.0\r\nPyYAML==6.0.1\r\npyzmq==26.0.3\r\nredis==5.0.7\r\nreferencing==0.35.1\r\nrequests==2.31.0\r\nrequests-toolbelt==1.0.0\r\nrich==13.5.2\r\nrpds-py==0.18.1\r\nrsa==4.7.2\r\ns3transfer==0.6.2\r\nsafetensors==0.4.3\r\nschema==0.7.7\r\nscikit-image==0.24.0\r\nscipy==1.14.0\r\nseaborn==0.13.2\r\nshapely==2.0.1\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nskypilot==0.5.0\r\nsniffio==1.3.1\r\nsounddevice==0.4.7\r\nstack-data==0.6.3\r\nstarlette==0.37.2\r\nstructlog==24.2.0\r\nsupervision==0.21.0\r\nsympy==1.12.1\r\ntabulate==0.9.0\r\nthop==0.1.1.post2209072238\r\ntifffile==2024.7.2\r\ntime-machine==2.14.2\r\ntimm==1.0.3\r\ntomli==2.0.1\r\ntomli_w==1.0.0\r\ntorch==2.0.1\r\ntorchaudio==2.0.2\r\ntorchvision==0.15.2\r\ntornado==6.4.1\r\ntqdm==4.66.4\r\ntraitlets==5.14.3\r\ntriton==2.0.0\r\ntyper==0.9.0\r\ntyping-inspect==0.9.0\r\ntyping_extensions==4.12.2\r\ntzdata==2024.1\r\ntzlocal==5.2\r\nultralytics==8.2.18\r\nuritemplate==4.1.1\r\nurllib3==1.26.19\r\nuvicorn==0.30.1\r\nvalidators==0.30.0\r\nwatchfiles==0.22.0\r\nwcwidth==0.2.13\r\nwebsocket-client==1.8.0\r\nwidgetsnbextension==4.0.11\r\nwrapt==1.16.0\r\nyarl==1.9.4\r\nzipp==3.19.2\r\nzxing-cpp==2.2.0\r\n```\r\n\r\n</details>",
    "comments": []
  },
  {
    "issue_number": 3481,
    "title": "Improve PandasSeries example",
    "author": "nickolasrm",
    "state": "closed",
    "created_at": "2023-01-25T17:57:54Z",
    "updated_at": "2024-07-12T09:55:38Z",
    "labels": [],
    "body": "https://github.com/bentoml/BentoML/blob/b2184b3ff471760cdea2a7b8ec837bc9da8a9377/src/bentoml/_internal/io_descriptors/pandas.py#L800\r\n\r\nIn the example above, the request works, but it seems more suitable as a `DataFrame` example. Wouldn't it be better if it was replaced by `[5,4,3,2]`?",
    "comments": []
  },
  {
    "issue_number": 3797,
    "title": "bug: Client requires grpcio dependency even if HTTP only is used",
    "author": "nadworny",
    "state": "closed",
    "created_at": "2023-04-27T12:23:33Z",
    "updated_at": "2024-07-12T09:54:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nThis happens when I use bentoml HTTPClient. The following error is thrown:\r\n`bentoml.exceptions.MissingDependencyException: 'grpcio' is required for gRPC support. Install with 'pip install bentoml[grpc]'. (reason: No module named 'grpc')`\r\n\r\n### To reproduce\r\n\r\n1. Install bentoml `poetry add bentoml@1.0.8`\r\n2. Initialize client `HTTPClient.from_url(\"\")`\r\n3. Run the code (I'm using simple fastapi server which invokes the bentoml server url)\r\n\r\n### Expected behavior\r\n\r\nI would expect that when I only want to use HTTPClient, I don't need to install the grpc extras.\r\n\r\n### Environment\r\n\r\nbentoml: 1.0.19\r\npython: 3.9",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Hi there, can you try with the latest release 1.0.19?"
      },
      {
        "user": "nadworny",
        "body": "Hey, just did it, exactly the same problem"
      }
    ]
  },
  {
    "issue_number": 3787,
    "title": "bug: Http Server is not worked with Pytorch-Lightning without 'del sys.modules[\"prometheus_client\"]'",
    "author": "bobby-ohouse",
    "state": "open",
    "created_at": "2023-04-25T02:15:40Z",
    "updated_at": "2024-07-12T09:53:41Z",
    "labels": [
      "bug",
      "fixed-in-main"
    ],
    "body": "### Describe the bug\r\n\r\nbenotml serve {my_service}.py:svc --port 3001 is loaded and stdout shows log\r\n\r\n```bash\r\nPrometheus metrics for HTTP BentoServer from \"{my_service}.py:svc\" can be accessed at http://localhost:3001/metrics\r\n2023-04-25T10:49:47+0900 [INFO] [cli] Starting development HTTP BentoServer from \"{my_service}.py:svc\" listening on http://0.0.0.0:3001 (Press CTRL+C to quit)\r\n```\r\nHowever, when I access to 'http://0.0.0.0:3001', it falls in infinite loading loop without any kinds of warning or error message.\r\n\r\nWhen I add 'del sys.modules[\"prometheus_client\"]', the issue is resolved and I can access http server via localhost web.\r\n\r\nI guess it is caused when we import 3rd party library using prometheus client (e.g. pytorch lightning)\r\n\r\n### To reproduce\r\n\r\nI find that just adding a single line \r\n\r\n```python\r\nimport torch\r\nimport pytorch_lightning as pl\r\n\r\n...\r\n```\r\nin your example of 'custom_runner/torch_hub_yolov5/service.py' triggers this bug.\r\n\r\n(please run  bentoml serve service.py:svc)\r\n\r\nHowever, after add this single line, it will resolve this issue\r\n\r\n```python\r\nimport torch\r\nimport pytorch_lightning as pl\r\n\r\ndel sys.modules[\"prometheus_client\"]\r\n...\r\n```\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nbentoml: 1.0.18\r\npython: 3.8.9\r\npytorch-lightning: 2.0.0",
    "comments": [
      {
        "user": "sauyon",
        "body": "That's strange. @aarnphm any ideas?"
      },
      {
        "user": "bobby-ohouse",
        "body": "Any updates or do you find any insights? \r\nI think it might be related to your custom prometheus client object checking sys.modules separately for edge case handling?"
      },
      {
        "user": "aarnphm",
        "body": "I couldn't find reference of prometheus_client within pytorch_lightning. \r\n\r\nCan you send your service.py definition here?\r\n\r\nIf any of the other modules in your service.py that uses the default prometheus_client, the metrics won't work since it will the client here is not setup in multiprocess mode. Read more about multiprocess mode: https://github.com/prometheus/client_python#multiprocess-mode-eg-gunicorn "
      }
    ]
  },
  {
    "issue_number": 3784,
    "title": "Update Open Telemetry integration packages",
    "author": "PeterJCLaw",
    "state": "closed",
    "created_at": "2023-04-24T18:03:29Z",
    "updated_at": "2024-07-12T09:51:35Z",
    "labels": [],
    "body": "The currently pinned versions of Open Telemetry's packages are somewhat outdated, meaning that the dependencies they pull in are also somewhat outdated. In turn this can cause conflicts with other libraries.\r\n\r\nI appreciate that Open Telemetry's packages aren't themselves great at their version spellings/matching, however it's not clear that BentoML pinning specific versions is the best solution.\r\n\r\nA particular example of package version conflicts is that `opentelemetry-proto==1.14.0` requires `protobuf~=3.13`, yet the most recent Google Cloud packages require `protobuf` >= 4.",
    "comments": [
      {
        "user": "sauyon",
        "body": "@aarnphm do you have context on why we locked the otel dep versions?"
      },
      {
        "user": "aarnphm",
        "body": "Because different otlp packages have conflict dependencies. Also newer version of otlp also breaks our monitoring API, cc @bojiang"
      },
      {
        "user": "sauyon",
        "body": "What conflicting dependencies?"
      }
    ]
  },
  {
    "issue_number": 4631,
    "title": "Trouble in saving the Keras model to the Bentoml model store",
    "author": "rxiaogit",
    "state": "closed",
    "created_at": "2024-04-01T17:54:47Z",
    "updated_at": "2024-07-12T09:48:26Z",
    "labels": [],
    "body": "Versions installed on Windows 11 desktop:\r\nbentoml 1.2.9\r\nkeras 3.1.1\r\ntensorflow 2.16.1\r\n\r\nThe Issue: \r\nAfter training a Keras model of CNN with the MNIST dataset, need to save the trained Keras model \r\nto the Bentoml model store, but always got the ValueError complaining about the arguments of ['signatures', 'options']\r\nnot supported, though those were never passed as arguments in the save_model() call.  \r\n\r\nSee the python codes below:\r\n\r\ndef main():\r\n    \r\n    import bentoml\r\n    import tensorflow as tf\r\n    from tensorflow import keras\r\n    \r\n    \"\"\"Trains a model for classifying digits using the MNIST dataset.\"\"\"\r\n    train_images, train_labels, test_images, test_labels = prepare_mnist_training_data()\r\n\r\n    model = build_convnet_model(INPUT_SHAPE, NUM_CLASSES)\r\n\r\n    model = train_model(\r\n        model, train_images, train_labels, test_images, test_labels, NUM_EPOCHS\r\n    )\r\n\r\n    # save the model to bento model store \r\n    # model.save(\"cnn_model.keras\")\r\n    \r\n    bentoml.keras.save_model(\"keras_cnn\", model)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\nAnd the error messages when running above codes:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\rxiao\\Tensorflow-Tutorial\\mldeployment\\mldeployment\\training.py\", line 138, in <module>\r\n    main()\r\n  File \"C:\\Users\\rxiao\\Tensorflow-Tutorial\\mldeployment\\mldeployment\\training.py\", line 134, in main\r\n    bentoml.keras.save_model(\"keras_cnn\", model)\r\n  File \"C:\\Users\\rxiao\\anaconda3\\envs\\tf\\Lib\\site-packages\\bentoml\\_internal\\frameworks\\keras.py\", line 267, in save_model\r\n    model.save(\r\n  File \"C:\\Users\\rxiao\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 123, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"C:\\Users\\rxiao\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\models\\model.py\", line 309, in save\r\n    raise ValueError(\r\nValueError: The following argument(s) are not supported: ['signatures', 'options']\r\n\r\nIs there a bug in the bentoml._internal.frameworks.keras library? \r\nOr What else do I miss here?\r\n\r\nThanks in advance for your help!!\r\n\r\n\r\n\r\n",
    "comments": [
      {
        "user": "dhj-git",
        "body": "i have the same problem"
      },
      {
        "user": "kascesar",
        "body": "same here! "
      },
      {
        "user": "kascesar",
        "body": "@rxiaogit @dhj-git Hello, i solved the problem. but you need to use bentoml.mlflow instead of bentoml.keras:\r\n\r\n1. Instantiate a server of [mlflow](https://www.mlflow.org/docs/latest/introduction/index.html)\r\n2. save your model keras model into mlflow -lots of tutorials in web, just googleit-\r\n### My code\r\n```python\r\nfrom datetime import datetime\r\n\r\nimport mlflow\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom hydra import compose, initialize\r\nfrom hydra.utils import instantiate\r\nfrom mlflow.models.signature import infer_signature\r\nfrom tensorflow.keras.callbacks import CSVLogger\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nfrom src.utils import parse_dataset\r\n\r\n\r\ndef main():\r\n    with initialize(version_base=None, config_path=\"config\", job_name=\"test_app\"):\r\n        cfg = compose(config_name=\"main\")\r\n        mlflow.set_tracking_uri(cfg.mlflow.uri)\r\n        mlflow.set_experiment(\"anomaly-det\")\r\n        # mlflow.keras.autolog()\r\n        with mlflow.start_run(run_name=\"anomaly-csv2d-lstm-autoenconder\"):\r\n            model = instantiate(cfg.model)\r\n\r\n            dataset = tf.data.TFRecordDataset(cfg.dataset.train)\r\n            dataset = dataset.map(parse_dataset)\r\n            dataset = dataset.batch(cfg.train.batch_size)\r\n            opt = Adam(\r\n                learning_rate=1e-4,\r\n                epsilon=1e-6,\r\n            )\r\n            model.compile(loss=\"mse\", optimizer=opt)\r\n            csv_logger = CSVLogger(\"training.csv\")\r\n            model.fit(\r\n                dataset,\r\n                epochs=cfg.train.epocs,\r\n                callbacks=[csv_logger],\r\n            )\r\n            # Login mlflow\r\n            now = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\r\n            model_name = f\"Conv2D_LSTM_Anomlay_Fatigue_Model_{now}\"\r\n            registered_model_name = f\"Conv2D_LSTM_Anomaly_Fatigue_Model_{now}\"\r\n\r\n            df_loger = pd.read_csv(\"training.csv\")\r\n\r\n            # crea el signature para el modelo\r\n            example_input = tf.random.normal((1, 1, 5, 4, 4))\r\n            example_output = model.predict(example_input)\r\n            signature = infer_signature(example_input.numpy(), example_output)\r\n\r\n            mlflow.keras.log_model(\r\n                model,\r\n                artifact_path=model_name,\r\n                registered_model_name=registered_model_name,\r\n                signature=signature,\r\n            )\r\n            mlflow.log_table(data=df_loger, artifact_file=\"train-log.csv\")\r\n            mlflow.log_param(\"train-date\", now.replace(\"_\", \" \"))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n3. create a bento model\r\n\r\n```python\r\n\r\nimport bentoml\r\nimport mlflow\r\nfrom hydra import compose, initialize\r\n\r\nfrom anomaly_x_builder import XBuilderAnomaly, anomaly_mesure\r\n\r\n\r\ninitialize(version_base=None, config_path=\"../../config\", job_name=\"bento-create-model\")\r\ncfg = compose(config_name=\"main\")\r\nmlflow.set_tracking_uri(cfg.mlflow.uri)\r\n\r\n\r\nbentoml.mlflow.import_model(\r\n    \"fatiguelog-anomaly-detector\",\r\n    model_uri=cfg.model.mlflow_uri,\r\n    signatures={\"predict\": {\"batchable\": False, \"batch_dim\": 0}},\r\n    custom_objects={\r\n        \"x_builder\": XBuilderAnomaly(),\r\n        \"anomaly_mesure\": anomaly_mesure,\r\n    },\r\n)\r\n```\r\n\r\n5. build a bento\r\n### My code for serve\r\n```python\r\nfrom io import StringIO\r\nimport bentoml\r\nimport pandas as pd\r\nfrom bentoml.io import JSON, Text\r\n\r\nmodel_ref = bentoml.mlflow.get(\"fatiguelog-anomaly-detector:latest\")\r\nrunner = model_ref.to_runner()\r\nxbuilder = model_ref.custom_objects[\"x_builder\"]\r\nmesure = model_ref.custom_objects[\"anomaly_mesure\"]\r\nsample_input = pd.read_csv(\r\n    \"sample_input.csv\",\r\n    parse_dates=[\"shift_start_timestamp_utc\", \"fatigue_log_timestamp_utc\"],\r\n)\r\n\r\nsvc = bentoml.Service(\r\n    name=\"fatiguelog-anomaly-detector\",\r\n    runners=[runner],\r\n)\r\n\r\n\r\n@svc.api(input=Text.from_sample(sample_input.to_csv(index=False)), output=JSON())\r\nasync def predict(data):\r\n    data = pd.read_csv(StringIO(data), engine=\"pyarrow\")\r\n    X, dts = xbuilder(data)\r\n    result = await runner.async_run(X)\r\n    return {\"anomaly-score\": [f\"{i}\" for i in mesure(X, result)], \"timestamp\": dts}\r\n```\r\n```bash\r\nbentoml build fatiguelog-anomaly-detector:latest\r\n```\r\n7. serve the model\r\nrun the following code in the folder where are the serve.py\r\n```bash\r\nbentoml serve .\r\n```\r\n---\r\nPreviously i had a problem with deplyment to AWS sagemaker , the version of bentoml thaths works for that is the 1.1.11, y don't know why.\r\n\r\nEnjoy! \r\nHope this help! :D "
      }
    ]
  },
  {
    "issue_number": 3769,
    "title": "bug: runner/service doesn't include mlflow `code_path` modules?",
    "author": "nadworny",
    "state": "closed",
    "created_at": "2023-04-18T15:08:46Z",
    "updated_at": "2024-07-12T09:34:33Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nMy mlflow model has additional dependencies (i.e. util classes) which are included through the `code_path` parameter (see https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.log_model). The included code doesn't seem to be available when the inference takes place leading to the following error:\r\n```\r\n[api_server:7] Exception on /predict [POST] (trace=ccfa8725f497d488a34b399b6ff94103,span=0e83cdfcd3b8955d,sampled=0)\r\nTraceback (most recent call last):                                                                                             \r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml/_internal/server/http_app.py\", line 324, in api_func                    \r\n    output = await api.func(input_data)                                                                                        \r\n  File \"/home/bentoml/bento/src/my_model/Model.py\", line 15, in predict                                     \r\n    return await runner.predict.async_run(input_arr)                                                                           \r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml/_internal/runner/runner.py\", line 55, in async_run                      \r\n    return await self.runner._runner_handle.async_run_method(self, *args, **kwargs)                                            \r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml/_internal/runner/runner_handle/remote.py\", line 279, in async_run_method\r\n    return AutoContainer.from_payload(payload)                                                                                 \r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml/_internal/runner/container.py\", line 628, in from_payload               \r\n    return container_cls.from_payload(payload)                                                                                 \r\n  File \"/usr/local/lib/python3.8/site-packages/simple_di/__init__.py\", line 139, in _                                          \r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))                                                       \r\n  File \"/usr/local/lib/python3.8/site-packages/bentoml/_internal/runner/container.py\", line 500, in from_payload               \r\n    return pickle.loads(payload.data)                                                                                          \r\nModuleNotFoundError: No module named 'my_python_project'\r\n```\r\n\r\nThe module `my_python_project` is available in the model itself and I verified that inside the running container:\r\n```bash\r\n$ ls models/my_model/1/mlflow_model/code/\r\nmy_python_project\r\n```\r\n\r\nPrediction works when I'm loading the model using mlflow.pyfunc flavor.\r\n\r\n### To reproduce\r\n\r\n1. Build a mlflow model using pyfunc flavor (i.e. https://medium.com/@pennyqxr/how-save-and-load-fasttext-model-in-mlflow-format-37e4d6017bf0) which uses external dependencies during prediction. Those dependencies are logged within the model using `code_path` parameter\r\n2. Load the model into bentoml and run predict\r\n\r\n### Expected behavior\r\n\r\nClasses/modules included through the `code_path` are resolved properly in the same way as the mlflow.pyfunc would work.\r\n\r\n### Environment\r\n\r\nbentoml, version 1.0.18\r\nPython 3.8.16\r\nm1 / k8s",
    "comments": [
      {
        "user": "aarnphm",
        "body": "You need to pass `my_python_project` into `custom_modules` when saving the model with `bentoml.mlflow`."
      },
      {
        "user": "nadworny",
        "body": "I did some further investigation. I was wrong, the problem is not that the *code_path* isn't being loaded because this works for another example that I used. \r\n\r\nI think the problem is when the response of the predict api is a json which is a pydantic model. If that model isn't within the container's Python path, it will fail.\r\n\r\nThe strange thing is though that the same bentoml service works with `bentoml serve` but not once containerized... I made sure to have exactly the same python env & dependencies for both cases.\r\n\r\n> You need to pass `my_python_project` into `custom_modules` when saving the model with `bentoml.mlflow`.\r\n\r\n@aarnphm the thing is I don't want to introduce another code dependency in the service itself since the model already has the required code."
      },
      {
        "user": "frostming",
        "body": "yes, you have to make sure all objects are importable by your service, we don't allow to manipulate `sys.path` at runtime. Closing it now"
      }
    ]
  },
  {
    "issue_number": 3770,
    "title": "BentoML not loading pytorch object with .model module",
    "author": "AceMcAwesome77",
    "state": "open",
    "created_at": "2023-04-18T19:00:11Z",
    "updated_at": "2024-07-12T08:08:19Z",
    "labels": [
      "feedback-wanted",
      "questions"
    ],
    "body": "Hi, I am trying to save a RetinaNet model with the following code:\r\n```python\r\nnet = torch.jit.load(_my_model_path, map_location=torch.device('cpu'))\r\n\r\ndef getstate():\r\n    pass\r\n\r\nnet.__getstate__ = getstate\r\n\r\ndef __init__(self):\r\n    super().__init__()\r\n    \r\nnet.__init__ = __init__\r\n\r\n#those extra init and getstate functions were necessary to get the code working.  the bolded text should have two underscores on either side but is not formatting properly\r\n\r\ndetector = RetinaNetDetector(network=net, anchor_generator=anchor_generator, debug=False)\r\n\r\ndetector.set_box_selector_parameters(\r\n    score_thresh=0.02,\r\n    topk_candidates_per_level=1000,\r\n    nms_thresh=0.22,\r\n    detections_per_img=1,\r\n)\r\ndetector.set_sliding_window_inferer(\r\n    roi_size=(192,192,96),\r\n    overlap=0.25,\r\n    sw_batch_size=1,\r\n    mode=\"gaussian\",\r\n    # device=\"cpu\",\r\n    device=device,\r\n)\r\ndetector.eval()\r\n\r\nbentoml.pytorch.save_model(model_name, detector, signatures={\"__call__\": {\"batchable\": False},\r\n                                        \"__init__\": {\"batchable\": False},\r\n                                        \"model\": {\"batchable\": False}})\r\n```\r\ndetector is of type monai.apps.detection.networks.retinanet_detector.RetinaNetDetector.\r\nI am then trying to load the model to a runner in a different file using:\r\n```python\r\nbento_model = bentoml.pytorch.get(model_name)\r\ntorch_model_runner = bento_model.to_runner()\r\nsvc = bentoml.Service(model_name, runners=[torch_model_runner])\r\n```\r\nBut when I call the runner like this:\r\n```\r\nprediction = await torch_model_runner.async_run(data)\r\n```\r\nI get this error:\r\n```pytb\r\n\"{\"time\": \"2023-04-18 17:43:51,259\", \"name\": \"bentoml._internal.server.runner_app\", \"level\": \"ERROR\", \"message\": \"Exception on runner 'smao_230409' method '__call__'\\nTraceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/server/runner_app.py\\\", line 331, in _run\\n    ret = await runner_method.async_run(*params.args, **params.kwargs)\\n  File \\\"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/runner/runner.py\\\", line 55, in async_run\\n    return await self.runner._runner_handle.async_run_method(self, *args, **kwargs)\\n  File \\\"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/runner/runner_handle/local.py\\\", line 59, in async_run_method\\n    return await anyio.to_thread.run_sync(\\n  File \\\"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\\\", line 31, in run_sync\\n    return await get_asynclib().run_sync_in_worker_thread(\\n  File \\\"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\\\", line 937, in run_sync_in_worker_thread\\n    return await future\\n  File \\\"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\\\", line 867, in run\\n    result = context.run(func, *args)\\n  File \\\"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/runner/runnable.py\\\", line 139, in method\\n    return self.func(obj, *args, **kwargs)\\n  File \\\"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/frameworks/common/pytorch.py\\\", line 107, in _run\\n    return getattr(self.model, method_name)(\\n  File \\\"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/runner/runnable.py\\\", line 139, in method\\n    return self.func(obj, *args, **kwargs)\\n  File \\\"/usr/local/lib/python3.8/dist-packages/bentoml/_internal/frameworks/common/pytorch.py\\\", line 107, in _run\\n    return getattr(self.model, method_name)(\\nAttributeError: 'function' object has no attribute 'model'\"}\"\r\n```\r\nif I print `dir(bento_model)` I see that is has an attribute of \"_model\" rather than \"model\", but it is equal to None.  Am I doing something wrong in saving this model to the store and then loading it into a runner?  I don't know how to get the model attribute to propagate all the way to the runner.  This is for bentoml==1.0.13 but I could change to whatever version if it helps. \r\n Thanks!",
    "comments": [
      {
        "user": "frostming",
        "body": "Does `.model()` actually exist on the `RetinaNetDetector` ? `__init__` also shouldn't be in the signatures mapping since it is not meant to be an inference method."
      }
    ]
  },
  {
    "issue_number": 3828,
    "title": "docs: add documentation for HTTPServer/GRPCServer Python API usage",
    "author": "parano",
    "state": "open",
    "created_at": "2023-05-04T22:57:49Z",
    "updated_at": "2024-07-12T08:02:22Z",
    "labels": [
      "documentation"
    ],
    "body": "Add documentation related to launching Bento server from Python API, e.g.:\r\n\r\n```python\r\nserver = HTTPServer(bento, production=True, host=host, port=server_port)\r\nclient = server.get_client()\r\nclient.predict(..)\r\n```",
    "comments": []
  },
  {
    "issue_number": 3930,
    "title": "feature: `bentoml.io.AttrsClass`",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2023-06-05T06:48:46Z",
    "updated_at": "2024-07-12T08:01:44Z",
    "labels": [],
    "body": "### Feature request\n\nAn extension of `bentoml.io.JSON` that supports `attrs` class.\r\n\r\nSimilar to `bentoml.io.PydanticModel` described under #3929 \n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3758,
    "title": "bug: Server dont load on Docker but on native terminal it works",
    "author": "enmanuelmag",
    "state": "open",
    "created_at": "2023-04-16T21:26:45Z",
    "updated_at": "2024-07-12T08:01:23Z",
    "labels": [
      "feedback-wanted",
      "questions"
    ],
    "body": "### Describe the bug\n\nI am trying to start a server with docker. The problem is that I am getting the following error:\r\n```\r\n[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"application.src.serve_model\": libGL.so.1: cannot open shared object file: No such file or directory\r\n```\r\n\r\nHowever, if I run are docker the server, that is, from my native terminal `bentoml serve` in my project folder, the server does work.\r\n\r\nClearly, I have previously executed the commands `bento build` and `bentoml containerize`.\r\n\r\nThis is the enire output error on docker:\r\n```\r\nError: [bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"application.src.serve_model\": libGL.so.1: cannot open shared object file: No such file or directory\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/bentoml\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml_cli/utils.py\", line 339, in wrapper\r\n    raise err from None\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml_cli/utils.py\", line 334, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml_cli/utils.py\", line 305, in wrapper\r\n    return_value = func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml_cli/utils.py\", line 262, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml_cli/env_manager.py\", line 122, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml_cli/serve.py\", line 195, in serve\r\n    serve_http_production(\r\n  File \"/usr/local/lib/python3.9/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml/serve.py\", line 322, in serve_http_production\r\n    svc = load(bento_identifier, working_dir=working_dir, standalone_load=True)\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml/_internal/service/loader.py\", line 334, in load\r\n    raise BentoMLException(\r\nbentoml.exceptions.BentoMLException: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"application.src.serve_model\": libGL.so.1: cannot open shared object file: No such file or directory\r\n```\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml==1.0.17\r\npython==3.9\r\nplatflow=windows 11 (docker on WSL 2.0)",
    "comments": [
      {
        "user": "parano",
        "body": "Hi @enmanuelmag, could you share more about your project structure, the content in `bentofile.yaml`, and especially what's the `service` filed in there?"
      },
      {
        "user": "enmanuelmag",
        "body": "@parano this is my bentofile.yaml\r\n```\r\nservice: 'application/src/create_service.py:service'\r\ninclude:\r\n  - config\r\n  - application/src/\r\n  - Procfile\r\npython:\r\n  packages:\r\n    - bentoml==1.0.17\r\n    - hydra-core==1.3.2\r\n    - numpy==1.23.5\r\n    - patsy==0.5.3\r\n    - pydantic==1.10.7\r\n    - opencv-python==4.7.0.72\r\n    - tensorflow==2.10.1\r\n    - pytesseract==0.3.10\r\n\r\n```\r\nAnd this is the prokect structure:\r\n```\r\nProject-folder/\r\n┣ application/\r\n┃ ┣ src/\r\n┃ ┃ ┣ create_service.py\r\n┃ ┃ ┗ __init__.py\r\n┃ ┣ tests/\r\n┃ ┃ ┣ input.png\r\n┃ ┃ ┣ test_create_service.py\r\n┃ ┃ ┗ __init__.py\r\n┃ ┃ ┗ __init__.cpython-39.pyc\r\n┃ ┣ requirements.txt\r\n┃ ┗ __init__.py\r\n┣ config/\r\n┃ ┗ main.yaml\r\n┣ .dvcignore\r\n┣ .flake8\r\n┣ .gitignore\r\n┣ .pre-commit-config.yaml\r\n┣ bentofile.yaml\r\n┣ data.dvc\r\n┣ dev-requirements.txt\r\n┣ Makefile\r\n┣ metrics.csv\r\n┣ models.dvc\r\n┣ outputs.dvc\r\n┣ params.yml\r\n┣ poetry.lock\r\n┣ pyproject.toml\r\n┣ README.md\r\n┣ requirements.txt\r\n┗ server.bat\r\n```"
      },
      {
        "user": "enmanuelmag",
        "body": "@parano \r\nIn addition, how I could hanlder mutiple images? I have been trying is this way but only detect or the server recieved one image:\r\n```python\r\nfrom bentoml.io import Multipart\r\nfrom bentoml.io import Image\r\nfrom bentoml.io import NumpyNdarray, JSON\r\n\r\nINPUT_SPEC = Multipart(images=Image())\r\nOUTPUT_SPEC = JSON()\r\n\r\n\r\n@service.api(input=INPUT_SPEC, output=OUTPUT_SPEC)\r\ndef predict(images: np.ndarray):\r\n    \"\"\"Transform the data then make predictions\"\"\"\r\n    print(images)\r\n    # ...rest of the predictor code\r\n```\r\n\r\nI sent the images is this way:\r\n\r\n```python\r\nimport requests\r\n\r\nfrom requests_toolbelt.multipart.encoder import MultipartEncoder\r\n\r\nm = MultipartEncoder(\r\n    fields={\r\n        \"field1\": (\"filename\", open(\"./multi/input_1.png\", \"rb\"), \"image/png\"),\r\n        \"field2\": (\"filename\", open(\"./multi/input_2.png\", \"rb\"), \"image/png\"),\r\n        \"field3\": (\"filename\", open(\"./multi/input_3.png\", \"rb\"), \"image/png\"),\r\n        \"field4\": (\"filename\", open(\"./multi/input_4.png\", \"rb\"), \"image/png\"),\r\n    }\r\n)\r\n\r\nresponse = requests.post(\r\n    \"http://localhost:3000/predict\",\r\n    data=m,\r\n    headers={\"Content-Type\": m.content_type},\r\n)\r\n\r\nprint(response.json())\r\n```\r\n\r\nThe response is this, because is not a iterable\r\n![image](https://user-images.githubusercontent.com/7991036/233869343-7e6d6833-b2af-42e2-8ec9-70198a561653.png)\r\n\r\nI was searching an example with multiple images on your repo but only found with once.\r\n"
      }
    ]
  },
  {
    "issue_number": 3755,
    "title": "bug: support environment variables for python build extra_index_url",
    "author": "nadworny",
    "state": "closed",
    "created_at": "2023-04-14T14:08:59Z",
    "updated_at": "2024-07-12T07:55:45Z",
    "labels": [
      "questions"
    ],
    "body": "### Describe the bug\r\n\r\nBuild bento using an extra index url that contains an environment variable. Instead, the following error happens:\r\n```\r\n#12 0.282 Installing pip packages from 'requirements.txt'..\r\n#12 0.284 + pip3 install -r /home/bentoml/bento/env/python/requirements.txt --no-warn-script-location --extra-index-url 'https://$PYPI_TOKEN:@pkgs.dev.azure.com/xxx/_packaging/xxx/pypi/simple'\r\n#12 9.132 Looking in indexes: https://pypi.org/simple, https://%24PYPI_TOKEN:****@pkgs.dev.azure.com/xxx/_packaging/xxx/pypi/simple\r\n#12 10.00 WARNING: 401 Error, Credentials not correct for https://pkgs.dev.azure.com/xxx/_packaging/xxx/pypi/simple/mlflow/\r\n```\r\n\r\n### To reproduce\r\n\r\n1. Have the following `bentofile.yaml`\r\n```yaml\r\nservice: \"sevice.Model:svc\"\r\npython:\r\n  requirements_txt: \"$BENTOML_MLFLOW_MODEL_PATH/mlflow_model/requirements.txt\"\r\n  extra_index_url:\r\n  - \"https://$PYPI_TOKEN:@pkgs.dev.azure.com/xxx/_packaging/xxx/pypi/simple\"\r\n```\r\n2. Export the token `export PYPI_TOKEN=abc`\r\n3. Export model `export BENTOML_MLFLOW_MODEL_PATH=$(bentoml models get demo:latest -o path)`\r\n4. Run\r\n```console\r\nbentoml build\r\nbentoml containerize demo_service:latest \r\n```\r\n\r\n### Expected behavior\r\n\r\nRequirements are installed successfully and bentoml image is created.\r\n\r\n### Environment\r\n\r\nbentoml, version 1.0.18\r\nPython 3.8.12 (default, Mar 16 2023, 16:38:07) \r\nm1 macos",
    "comments": [
      {
        "user": "ssheng",
        "body": "@nadworny I'm not too familiar with using environment variables directly in the `--extra-index-url`. Could you please point me to the specification of this syntax? Do you think using `PIP_EXTRA_INDEX_URL` environment variable could help with your case?"
      },
      {
        "user": "nadworny",
        "body": "hi @ssheng , I assumed that since you can use env variable in the *benotfile.yml* for *requirements_txt*, this should also work the same way for the *extra_index_url* but it's not.\r\n\r\nAre you suggesting that I should pass instead the `PIP_EXTRA_INDEX_URL` to the `bentoml containerize` command? Would it work?"
      },
      {
        "user": "aarnphm",
        "body": "We will have to support parsing this environment variable within our install script.\r\n\r\nRight now can you try putting this env under the docker `env` argument?"
      }
    ]
  },
  {
    "issue_number": 3728,
    "title": "bug: runner in production mode does not get input of type dict",
    "author": "seyong92",
    "state": "closed",
    "created_at": "2023-03-29T03:50:25Z",
    "updated_at": "2024-07-12T07:53:52Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nI am now using a python dict with PyTorch Tensors as an input of the model. When I serve the model in debug mode, it works correctly.\r\n\r\nHowever, when I serve the model in production mode, the runner does not pass the dict to the model. Instead of dict, the runner gives a list to the model, and the list is composed of keys of dict.\r\n\r\nHere is the example.\r\n\r\n```python\r\nbento_model = bentoml.pytorch.get('my_model')\r\nconfig = bento_model.info.metadata\r\ndecoder = bento_model.custom_objects['decoder']\r\n\r\n\r\n_BuiltinRunnable = bento_model.to_runnable()\r\n\r\n\r\nclass CustomRunnable(_BuiltinRunnable):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    @bentoml.Runnable.method(batchable=True, batch_dim=0)\r\n    def __call__(self, input_data: dict) -> np.ndarray:\r\n        for k, v in input_data.items():\r\n            input_data[k] = v.to(self.device_id)\r\n        output = super().__call__(input_data)\r\n\r\n        return output\r\n```\r\n\r\nWhen I use the runner, I use async_run function, and get the following error in production mode.\r\n\r\n```shell\r\n  File \"MY_PROJECT_DIR/bentoml_serve.py\", line 40, in __call__\r\n    for k, v in input_data.items():\r\nAttributeError: 'list' object has no attribute 'items'\r\n```\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: 1.0.16\r\npython: 3.10.9",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Hi there, can you send your service definition here?\r\n\r\nI suspect you probably need to do something like\r\n```python\r\n@svc.api()\r\ndef predict(input):\r\n    runnable.run([input])\r\n```"
      },
      {
        "user": "seyong92",
        "body": "@aarnphm \r\n\r\nOkay, I will share the service definition part.\r\n\r\nShould I use a list as an input instead of using dict?\r\n\r\n```python\r\nrunner = bentoml.Runner(CustomRunnable, models=[bento_model])\r\n\r\n\r\nsvc = bentoml.Service(name='my_service_name', runners=[runner])\r\n\r\n\r\n@svc.api(input=File(), output=custom_pydantic_model)\r\nasync def predict(input_audio):\r\n    audio, sr = torchaudio.load(input_audio)\r\n\r\n    if audio.ndim > 1:  # change to mono\r\n        audio = audio.mean(0).unsqueeze(0)\r\n\r\n    if sr != config['sample_rate']:\r\n        audio = torchaudio.functional.resample(audio, sr, config['sample_rate'])\r\n\r\n    with torch.no_grad():\r\n        y = await runner.async_run({'audio': audio})\r\n\r\n    ...\r\n\r\n```"
      },
      {
        "user": "aarnphm",
        "body": "try \r\n```python\r\ny = await runner.async_run([{\"audio\": audio}])\r\n```\r\n\r\nSince this runners has batchable=True, the input should be a list"
      }
    ]
  },
  {
    "issue_number": 3727,
    "title": "ci: coverage",
    "author": "aarnphm",
    "state": "open",
    "created_at": "2023-03-28T23:57:07Z",
    "updated_at": "2024-07-12T07:53:17Z",
    "labels": [
      "ci"
    ],
    "body": "Probably need to fix our coverage config and make it more consistent on CI",
    "comments": []
  },
  {
    "issue_number": 3718,
    "title": "feature: Pass runtime configuration when serving with Python API",
    "author": "AndriiG13",
    "state": "closed",
    "created_at": "2023-03-27T10:35:55Z",
    "updated_at": "2024-07-12T07:52:55Z",
    "labels": [
      "questions"
    ],
    "body": "### Feature request\n\nAs I understand, the way to pass runtime config is by exporting an env var `BENTOML_CONFIG`. However this only seems to work when serving with the CLI, that is  `bentoml serve bento_tag`. \r\n\r\nMy team would like to serve with the Python API instead, for instance \r\n\r\n```python\r\n# main.py\r\nfrom bentoml.serve import serve_grpc_development\r\n\r\nserve_grpc_development(bento_tag)\r\n```\r\n\r\nHowever, when testing, I found that when using Python API the runtime config is not picked up. \r\n\r\nSo we would like to be able to both use the Python API **and** the runtime config.\r\n\n\n### Motivation\n\nFor our use case, we find it easier/cleaner to use the Python API instead of the CLI. But right now we have to use the CLI because seemingly that's the only way to pass the runtime config. \n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "You have to set the env var `BENTOML_CONFIG` when executing `main.py`, not inside it."
      }
    ]
  },
  {
    "issue_number": 3692,
    "title": "bug: can't save a fastai unet_learner model ",
    "author": "RihabFekii",
    "state": "open",
    "created_at": "2023-03-21T07:17:12Z",
    "updated_at": "2024-07-12T07:48:48Z",
    "labels": [
      "bug",
      "framework"
    ],
    "body": "### Describe the bug\r\n\r\n## Training code: \r\nlearner = unet_learner(data,params)\r\nlearner.fine_tune(epochs=int(params['epochs']),\r\n                      base_lr=float(params['learning_rate']))\r\n\r\nlearner.save('model')\r\nbento_model = bentoml.fastai.save_model('model', learner)\r\n\r\n## Bug:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/rihabfeki/Desktop/SavtaDepth/src/code/training.py\", line 47, in <module>\r\n    bento_model = bentoml.fastai.save_model('model', learner)\r\n  File \"/Users/rihabfeki/Desktop/SavtaDepth/env/lib/python3.9/site-packages/bentoml/_internal/frameworks/fastai.py\", line 224, in save_model\r\n    learner_.export(bento_model.path_of(MODEL_FILENAME), pickle_module=cloudpickle)\r\n  File \"/Users/rihabfeki/Desktop/SavtaDepth/env/lib/python3.9/site-packages/fastai/learner.py\", line 436, in export\r\n    torch.save(self, self.path/fname, pickle_module=pickle_module, pickle_protocol=pickle_protocol)\r\n  File \"/Users/rihabfeki/Desktop/SavtaDepth/env/lib/python3.9/site-packages/torch/serialization.py\", line 423, in save\r\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\r\n  File \"/Users/rihabfeki/Desktop/SavtaDepth/env/lib/python3.9/site-packages/torch/serialization.py\", line 635, in _save\r\n    pickler.dump(obj)\r\n  File \"/Users/rihabfeki/Desktop/SavtaDepth/env/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py\", line 632, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"/Users/rihabfeki/Desktop/SavtaDepth/env/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py\", line 343, in _file_reduce\r\n    raise pickle.PicklingError(\r\n_pickle.PicklingError: Cannot pickle files that are not opened for reading: w\r\n```\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nbentoml==1.0.16\r\nfastai==2.7.11\r\npython==3.9.6",
    "comments": []
  },
  {
    "issue_number": 4238,
    "title": "bug: aiohttp.client_exceptions.ServerDisconnectedError: Server disconnected",
    "author": "nadimintikrish",
    "state": "open",
    "created_at": "2023-10-16T02:46:21Z",
    "updated_at": "2024-07-12T07:47:42Z",
    "labels": [
      "bug",
      "feedback-wanted"
    ],
    "body": "### Describe the bug\n\nThis keeps happening with mainly transformer based models. However, this error keeps prevailing only after containerizing like using \r\n`bentoml containerize bento_svc:latest` and running the container like `docker run --rm -p 3000:3000 bento_svc:t5mqzwdlzoxbjdx2`\r\n\r\nI have worked on multiple models like the one in documentation\r\nhttps://docs.bentoml.com/en/latest/quickstarts/deploy-a-transformer-model-with-bentoml.html\r\neven distil bert models and also sentence embedding. \r\n\r\nI cloned the source code of this sentence embedding and tried using containerize and run. it still fails but works like charm if used docker container out of the box.\r\n\r\nhttps://github.com/bentoml/sentence-embedding-bento/blob/main/requirements.txt\r\n\r\nThis happened with bentoml 1.1.6 and updated version 1.1.7 as well.\r\n\r\nAlso, I am using Mac OS with Apple Silicon chip if that matters.\r\n\r\nI am able to run with scikit learn models and even torch based models that I trained. \r\n\r\nWonder what am I doing wrong here.\r\n\r\nstrack-trace for further introspection:\r\n\r\n`2023-10-16T02:32:33+0000 [ERROR] [api_server:4] Exception on /encode [POST] (trace=f755596b7d185db8d620262d333fa9ae,span=2c59e1cd9a90f9cb,sampled=0,service.name=sentence-embedding-svc)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml/_internal/server/http_app.py\", line 341, in api_func\r\n    output = await api.func(*args)\r\n  File \"/home/bentoml/bento/src/service.py\", line 38, in encode\r\n    return await embed_runner.encode.async_run(docs.dict())\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml/_internal/runner/runner.py\", line 55, in async_run\r\n    return await self.runner._runner_handle.async_run_method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml/_internal/runner/runner_handle/remote.py\", line 216, in async_run_method\r\n    async with self._client.post(\r\n  File \"/usr/local/lib/python3.9/site-packages/aiohttp/client.py\", line 1167, in __aenter__\r\n    self._resp = await self._coro\r\n  File \"/usr/local/lib/python3.9/site-packages/aiohttp/client.py\", line 586, in _request\r\n    await resp.start(conn)\r\n  File \"/usr/local/lib/python3.9/site-packages/aiohttp/client_reqrep.py\", line 905, in start\r\n    message, payload = await protocol.read()  # type: ignore[union-attr]\r\n  File \"/usr/local/lib/python3.9/site-packages/aiohttp/streams.py\", line 616, in read\r\n    await self._waiter\r\naiohttp.client_exceptions.ServerDisconnectedError: Server disconnected\r\n`\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.1.7\r\n`python`: 3.9.18\r\n`platform`: macOS-13.6-arm64-i386-64bit\r\n`uid_gid`: 502:20\r\n`conda`: 23.1.0\r\n`in_conda_env`: True\r\n<details><summary><code>conda_packages</code></summary>\r\n\r\n<br>\r\n\r\n```yaml\r\nname: bento_39\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - ca-certificates=2023.08.22=hca03da5_0\r\n  - libcxx=14.0.6=h848a8c0_0\r\n  - libffi=3.4.4=hca03da5_0\r\n  - ncurses=6.4=h313beb8_0\r\n  - openssl=3.0.11=h1a28f6b_2\r\n  - pip=23.2.1=py39hca03da5_0\r\n  - python=3.9.18=hb885b13_0\r\n  - readline=8.2=h1a28f6b_0\r\n  - setuptools=68.0.0=py39hca03da5_0\r\n  - sqlite=3.41.2=h80987f9_0\r\n  - tk=8.6.12=hb8d0fd4_0\r\n  - tzdata=2023c=h04d1e81_0\r\n  - wheel=0.41.2=py39hca03da5_0\r\n  - xz=5.4.2=h80987f9_0\r\n  - zlib=1.2.13=h5a0b063_0\r\n  - pip:\r\n      - absl-py==2.0.0\r\n      - aiohttp==3.8.6\r\n      - aiosignal==1.3.1\r\n      - annotated-types==0.6.0\r\n      - anyio==4.0.0\r\n      - appdirs==1.4.4\r\n      - asgiref==3.7.2\r\n      - astunparse==1.6.3\r\n      - async-timeout==4.0.3\r\n      - attrs==23.1.0\r\n      - bentoml==1.1.7\r\n      - build==1.0.3\r\n      - cachetools==5.3.1\r\n      - cattrs==23.1.2\r\n      - certifi==2023.7.22\r\n      - charset-normalizer==3.3.0\r\n      - circus==0.18.0\r\n      - click==8.1.7\r\n      - click-option-group==0.5.6\r\n      - cloudpickle==2.2.1\r\n      - contextlib2==21.6.0\r\n      - deepmerge==1.1.0\r\n      - deprecated==1.2.14\r\n      - exceptiongroup==1.1.3\r\n      - filelock==3.12.4\r\n      - flatbuffers==23.5.26\r\n      - frozenlist==1.4.0\r\n      - fs==2.4.16\r\n      - fsspec==2023.9.2\r\n      - gast==0.5.4\r\n      - google-auth==2.23.3\r\n      - google-auth-oauthlib==1.0.0\r\n      - google-pasta==0.2.0\r\n      - grpcio==1.59.0\r\n      - h11==0.14.0\r\n      - h5py==3.10.0\r\n      - httpcore==0.18.0\r\n      - httpx==0.25.0\r\n      - huggingface-hub==0.17.3\r\n      - idna==3.4\r\n      - importlib-metadata==6.0.1\r\n      - inflection==0.5.1\r\n      - jinja2==3.1.2\r\n      - joblib==1.3.2\r\n      - keras==2.14.0\r\n      - libclang==16.0.6\r\n      - markdown==3.5\r\n      - markdown-it-py==3.0.0\r\n      - markupsafe==2.1.3\r\n      - mdurl==0.1.2\r\n      - ml-dtypes==0.2.0\r\n      - mpmath==1.3.0\r\n      - multidict==6.0.4\r\n      - networkx==3.1\r\n      - numpy==1.26.0\r\n      - oauthlib==3.2.2\r\n      - opentelemetry-api==1.20.0\r\n      - opentelemetry-instrumentation==0.41b0\r\n      - opentelemetry-instrumentation-aiohttp-client==0.41b0\r\n      - opentelemetry-instrumentation-asgi==0.41b0\r\n      - opentelemetry-sdk==1.20.0\r\n      - opentelemetry-semantic-conventions==0.41b0\r\n      - opentelemetry-util-http==0.41b0\r\n      - opt-einsum==3.3.0\r\n      - packaging==23.2\r\n      - pathspec==0.11.2\r\n      - pillow==10.0.1\r\n      - pip-requirements-parser==32.0.1\r\n      - pip-tools==7.3.0\r\n      - prometheus-client==0.17.1\r\n      - protobuf==4.24.4\r\n      - psutil==5.9.5\r\n      - pyasn1==0.5.0\r\n      - pyasn1-modules==0.3.0\r\n      - pydantic==2.4.2\r\n      - pydantic-core==2.10.1\r\n      - pygments==2.16.1\r\n      - pynvml==11.5.0\r\n      - pyparsing==3.1.1\r\n      - pyproject-hooks==1.0.0\r\n      - python-dateutil==2.8.2\r\n      - python-json-logger==2.0.7\r\n      - python-multipart==0.0.6\r\n      - pyyaml==6.0.1\r\n      - pyzmq==25.1.1\r\n      - regex==2023.10.3\r\n      - requests==2.31.0\r\n      - requests-oauthlib==1.3.1\r\n      - rich==13.6.0\r\n      - rsa==4.9\r\n      - safetensors==0.4.0\r\n      - schema==0.7.5\r\n      - scikit-learn==1.3.1\r\n      - scipy==1.11.3\r\n      - simple-di==0.1.5\r\n      - six==1.16.0\r\n      - sniffio==1.3.0\r\n      - starlette==0.31.1\r\n      - sympy==1.12\r\n      - tensorboard==2.14.1\r\n      - tensorboard-data-server==0.7.1\r\n      - tensorflow==2.14.0\r\n      - tensorflow-estimator==2.14.0\r\n      - tensorflow-io-gcs-filesystem==0.34.0\r\n      - tensorflow-macos==2.14.0\r\n      - termcolor==2.3.0\r\n      - threadpoolctl==3.2.0\r\n      - tokenizers==0.14.1\r\n      - tomli==2.0.1\r\n      - torch==2.1.0\r\n      - torchvision==0.16.0\r\n      - tornado==6.3.3\r\n      - tqdm==4.66.1\r\n      - transformers==4.34.0\r\n      - typing-extensions==4.8.0\r\n      - urllib3==2.0.6\r\n      - uvicorn==0.23.2\r\n      - watchfiles==0.20.0\r\n      - werkzeug==3.0.0\r\n      - wrapt==1.14.1\r\n      - xgboost==2.0.0\r\n      - yarl==1.9.2\r\n      - zipp==3.17.0\r\nprefix: /Users/user/miniconda3/envs/bento_39\r\n```\r\n\r\n</details>\r\n\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\nabsl-py==2.0.0\r\naiohttp==3.8.6\r\naiosignal==1.3.1\r\nannotated-types==0.6.0\r\nanyio==4.0.0\r\nappdirs==1.4.4\r\nasgiref==3.7.2\r\nastunparse==1.6.3\r\nasync-timeout==4.0.3\r\nattrs==23.1.0\r\nbentoml==1.1.7\r\nbuild==1.0.3\r\ncachetools==5.3.1\r\ncattrs==23.1.2\r\ncertifi==2023.7.22\r\ncharset-normalizer==3.3.0\r\ncircus==0.18.0\r\nclick==8.1.7\r\nclick-option-group==0.5.6\r\ncloudpickle==2.2.1\r\ncontextlib2==21.6.0\r\ndeepmerge==1.1.0\r\nDeprecated==1.2.14\r\nexceptiongroup==1.1.3\r\nfilelock==3.12.4\r\nflatbuffers==23.5.26\r\nfrozenlist==1.4.0\r\nfs==2.4.16\r\nfsspec==2023.9.2\r\ngast==0.5.4\r\ngoogle-auth==2.23.3\r\ngoogle-auth-oauthlib==1.0.0\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.59.0\r\nh11==0.14.0\r\nh5py==3.10.0\r\nhttpcore==0.18.0\r\nhttpx==0.25.0\r\nhuggingface-hub==0.17.3\r\nidna==3.4\r\nimportlib-metadata==6.0.1\r\ninflection==0.5.1\r\nJinja2==3.1.2\r\njoblib==1.3.2\r\nkeras==2.14.0\r\nlibclang==16.0.6\r\nMarkdown==3.5\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.3\r\nmdurl==0.1.2\r\nml-dtypes==0.2.0\r\nmpmath==1.3.0\r\nmultidict==6.0.4\r\nnetworkx==3.1\r\nnumpy==1.26.0\r\noauthlib==3.2.2\r\nopentelemetry-api==1.20.0\r\nopentelemetry-instrumentation==0.41b0\r\nopentelemetry-instrumentation-aiohttp-client==0.41b0\r\nopentelemetry-instrumentation-asgi==0.41b0\r\nopentelemetry-sdk==1.20.0\r\nopentelemetry-semantic-conventions==0.41b0\r\nopentelemetry-util-http==0.41b0\r\nopt-einsum==3.3.0\r\npackaging==23.2\r\npathspec==0.11.2\r\nPillow==10.0.1\r\npip-requirements-parser==32.0.1\r\npip-tools==7.3.0\r\nprometheus-client==0.17.1\r\nprotobuf==4.24.4\r\npsutil==5.9.5\r\npyasn1==0.5.0\r\npyasn1-modules==0.3.0\r\npydantic==2.4.2\r\npydantic_core==2.10.1\r\nPygments==2.16.1\r\npynvml==11.5.0\r\npyparsing==3.1.1\r\npyproject_hooks==1.0.0\r\npython-dateutil==2.8.2\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.6\r\nPyYAML==6.0.1\r\npyzmq==25.1.1\r\nregex==2023.10.3\r\nrequests==2.31.0\r\nrequests-oauthlib==1.3.1\r\nrich==13.6.0\r\nrsa==4.9\r\nsafetensors==0.4.0\r\nschema==0.7.5\r\nscikit-learn==1.3.1\r\nscipy==1.11.3\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nstarlette==0.31.1\r\nsympy==1.12\r\ntensorboard==2.14.1\r\ntensorboard-data-server==0.7.1\r\ntensorflow==2.14.0\r\ntensorflow-estimator==2.14.0\r\ntensorflow-io-gcs-filesystem==0.34.0\r\ntensorflow-macos==2.14.0\r\ntermcolor==2.3.0\r\nthreadpoolctl==3.2.0\r\ntokenizers==0.14.1\r\ntomli==2.0.1\r\ntorch==2.1.0\r\ntorchvision==0.16.0\r\ntornado==6.3.3\r\ntqdm==4.66.1\r\ntransformers==4.34.0\r\ntyping_extensions==4.8.0\r\nurllib3==2.0.6\r\nuvicorn==0.23.2\r\nwatchfiles==0.20.0\r\nWerkzeug==3.0.0\r\nwrapt==1.14.1\r\nxgboost==2.0.0\r\nyarl==1.9.2\r\nzipp==3.17.0\r\n```\r\n\r\n</details>\r\n\r\n",
    "comments": [
      {
        "user": "sudeepg545",
        "body": "As some additional notes to the above issue, it definitely seems to be memory issue, we were also facing the same issue and could resolve it for now by adding more memory to the container. In our case, we are deploying in ECS and running multiple models (i.e. multiple runners) in each EC2 instance."
      },
      {
        "user": "jianshen92",
        "body": "> As some additional notes to the above issue, it definitely seems to be memory issue, we were also facing the same issue and could resolve it for now by adding more memory to the container. In our case, we are deploying in ECS and running multiple models (i.e. multiple runners) in each EC2 instance.\r\n\r\nThanks for your finding! Do you find the memory slowly building up over time? Can you share a graph of memory consumption with respect to request volume?"
      },
      {
        "user": "nadimintikrish",
        "body": "Thanks @sudeepg545 ! I will look into this POV.\r\nWill update on this @jianshen92 ."
      }
    ]
  },
  {
    "issue_number": 3629,
    "title": "lightgbm and predict_contrib not working when using get - shapley not available with get method",
    "author": "whatisAI",
    "state": "closed",
    "created_at": "2023-03-01T23:23:29Z",
    "updated_at": "2024-07-12T07:45:26Z",
    "labels": [],
    "body": "Hello, \r\nI trained a lightgbm model which I have on disk. I would like to use the predict_proba parameter, but this does not work. \r\n\r\nThese are the steps I followed\r\n1. I trained a lightgbm model, saved it as a pkl file. \r\n2. In the training step I load it and save it using   bentoml.lightgbm.save_model(\"lgb_mymod\",model_lgb)\r\n3. If I do model  = bentoml.lightgbm.load_model(\"lgb_rr1\") and then model.predict(sample_x, pred_contrib=True) it works\r\n4. But if I load the model like we are supposed to with model_runner = bentoml.lightgbm.get(\"lgb_mymod:latest\").to_runner() and then do model_runner.predict.run(sample_x3, predict_contrib=True) then I get the error TypeError: _run() got an unexpected keyword argument 'predict_contrib'\r\n\r\nPlease could you help me?\r\n\r\nMany thanks\r\n\r\n",
    "comments": [
      {
        "user": "whatisAI",
        "body": "Do you think to be able to access the predict_contrib parameter I need to define a new runner?"
      },
      {
        "user": "parano",
        "body": "Hi @whatisAI, thanks for reporting the issue, yes this is limited by the default lightgbm runner implementation, which doesn't allow users to easily pass through additional parameters to `predict` method.\r\n\r\nAn easy workaround is to use BentoML's custom runner interface, see this example here: https://docs.bentoml.org/en/latest/concepts/runner.html#custom-model-runner\r\n\r\nYou could do something like this:\r\n\r\n```python\r\nbento_model = bentoml.lightgbm.get(\"my_model:latest\")\r\n\r\nclass MyLightGBMRunner(bentoml.Runnable):\r\n    SUPPORTED_RESOURCES = (\"cpu\",)\r\n    SUPPORTS_CPU_MULTI_THREADING = True\r\n\r\n    def __init__(self):\r\n        self.model = bentoml.lightbm.load_model(bento_model)\r\n\r\n    @bentoml.Runnable.method(batchable=False)\r\n    def predict(self, input_data):\r\n        return self.model.predict(input_data, predict_contrib=True)\r\n\r\nmodel_runner = bentoml.Runner(MyLightGBMRunner, models=[bento_model])\r\n\r\n```"
      }
    ]
  },
  {
    "issue_number": 3623,
    "title": "feature: worker exception message preserved when worker throws exception handled as `RemoteException` by api_server.",
    "author": "avicennax",
    "state": "closed",
    "created_at": "2023-02-28T01:06:46Z",
    "updated_at": "2024-07-12T07:44:18Z",
    "labels": [],
    "body": "### Feature request\n\nditto title.\r\n\r\nCurrently when a service runner predict function is executed by a worker any Python exceptions (AFAICT) will handled via api_server as `RemoteException`. This is fine except all information about the exception is lost with this generic exception message: \r\n\r\n```\r\nbentoml.exceptions.RemoteException: An exception occurred in remote runner my_model_tag: [500] Internal Server Error\r\n```\n\n### Motivation\n\nThis would enable users you handle worker exceptions more effectively (return non-500s when appropriate) without forcing those functions (in my case service runner predict functions) to have to know about Bento by letting them raise whatever exceptions make sense and letting the api_server handle them accordingly.\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "This is no longer an issue on the new service API"
      }
    ]
  },
  {
    "issue_number": 3578,
    "title": "feature: S3 Compatible Storage Endpoint configuration",
    "author": "herunyu",
    "state": "open",
    "created_at": "2023-02-16T09:33:13Z",
    "updated_at": "2024-07-12T07:43:48Z",
    "labels": [
      "feedback-wanted",
      "questions"
    ],
    "body": "### Feature request\n\nHI~ I am trying to export model and bento to our S3 compatible storage. I tried setting the \"BENTOML_S3_ENDPOINT_URL\"  environment variable but still cannot export the models. Is there any instruction about how to use bentoml cli with S3 compatible storage?\r\n\r\nThanks\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": ">I tried setting the \"BENTOML_S3_ENDPOINT_URL\" environment variable but still cannot export the models.\r\n\r\nWhere does this env var name come from? Did you try `bentoml export s3://<s3_endpoint>/<bucket>`?"
      }
    ]
  },
  {
    "issue_number": 3391,
    "title": "bug: Exception raised when creating bentoml directory with home directory containing @ character",
    "author": "niits",
    "state": "closed",
    "created_at": "2023-01-05T05:58:53Z",
    "updated_at": "2024-07-12T00:02:31Z",
    "labels": [
      "bug",
      "help-wanted",
      "good-first-issue"
    ],
    "body": "### Describe the bug\n\nException raised when creating `bentoml` directory with home directory containing `@` character\r\n\r\n```bash\r\n~/AI/pet_projects/BentoML/examples/quickstart on main took 21s\r\n% python train.py                  \r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 21, in <module>\r\n    saved_model = bentoml.sklearn.save_model(\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/bentoml/_internal/frameworks/sklearn.py\", line 163, in save_model\r\n    with bentoml.models.create(\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/simple_di/__init__.py\", line 110, in _inject_kwargs\r\n    return {k: v.get() if isinstance(v, Provider) else v for k, v in kwargs.items()}\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/simple_di/__init__.py\", line 110, in <dictcomp>\r\n    return {k: v.get() if isinstance(v, Provider) else v for k, v in kwargs.items()}\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/simple_di/__init__.py\", line 72, in get\r\n    return self._provide()\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/simple_di/providers.py\", line 124, in _provide\r\n    value = super()._provide()\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/simple_di/providers.py\", line 103, in _provide\r\n    return inject(self._func)(\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/bentoml/_internal/configuration/containers.py\", line 203, in model_store\r\n    return ModelStore(base_dir)\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/bentoml/_internal/models/model.py\", line 371, in __init__\r\n    super().__init__(base_path, Model)\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/bentoml/_internal/store.py\", line 69, in __init__\r\n    self._fs = fs.open_fs(base_path)\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/fs/opener/registry.py\", line 220, in open_fs\r\n    _fs, _path = self.open(\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/fs/opener/registry.py\", line 177, in open\r\n    open_fs = opener.open_fs(fs_url, parse_result, writeable, create, cwd)\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/fs/opener/osfs.py\", line 40, in open_fs\r\n    osfs = OSFS(path, create=create)\r\n  File \"/home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8/lib/python3.8/site-packages/fs/osfs.py\", line 141, in __init__\r\n    raise errors.CreateFailed(message)\r\nfs.errors.CreateFailed: root path '/home/tran.duc.trungb@sun-asterisk.com/AI/pet_projects/BentoML/examples/quickstart/sun-asterisk.com/bentoml/models' does not exist\r\n```\n\n### To reproduce\n\nSteps to reproduce bugs:\r\n- Create new `linux` user containing `@` character such as `someone@somewhere`\r\n- Clone repo `https://github.com/bentoml/BentoML.git` with commit hash: `a30d95e372daa646a2e644a90e0fe337512c6f82`\r\n- Run quickstart demo placed in `BentoML/examples/quickstart` with command `python3 train.py`\n\n### Expected behavior\n\n- `bentoml` dir should be created\r\n- model was saved successfully with following message:\r\n\r\n```bash\r\nroot@0510a83917a1:~/AI/pet_projects/BentoML/examples/quickstart# python train.py \r\nModel saved: Model(tag=\"iris_clf:a7yvf5emxsrjqasc\")\r\n```\n\n### Environment\n\nBentoML: `1.0.12`\r\nPython: `3.8.13`\r\n\r\nPlatform:\r\n```bash\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 20.04.5 LTS\r\nRelease:        20.04\r\nCodename:       focal\r\n```\r\nRoot dir: `/home/tran.duc.trungb@sun-asterisk.com/AI/pet_projects/BentoML/examples/quickstart`\r\n\r\nConda env:\r\n\r\n```yaml\r\nname: bentoml-3.8\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=5.1=1_gnu\r\n  - ca-certificates=2022.10.11=h06a4308_0\r\n  - certifi=2022.9.24=py38h06a4308_0\r\n  - ld_impl_linux-64=2.38=h1181459_1\r\n  - libffi=3.3=he6710b0_2\r\n  - libgcc-ng=11.2.0=h1234567_1\r\n  - libgomp=11.2.0=h1234567_1\r\n  - libstdcxx-ng=11.2.0=h1234567_1\r\n  - ncurses=6.3=h5eee18b_3\r\n  - openssl=1.1.1s=h7f8727e_0\r\n  - pip=22.2.2=py38h06a4308_0\r\n  - python=3.8.13=haa1d7c7_1\r\n  - readline=8.2=h5eee18b_0\r\n  - setuptools=65.5.0=py38h06a4308_0\r\n  - sqlite=3.39.3=h5082296_0\r\n  - tk=8.6.12=h1ccaba5_0\r\n  - wheel=0.37.1=pyhd3eb1b0_0\r\n  - xz=5.2.6=h5eee18b_0\r\n  - zlib=1.2.13=h5eee18b_0\r\n  - pip:\r\n    - aiohttp==3.8.3\r\n    - aiohttp-cors==0.7.0\r\n    - aiosignal==1.2.0\r\n    - alembic==1.8.1\r\n    - anyio==3.6.2\r\n    - appdirs==1.4.4\r\n    - asgiref==3.6.0\r\n    - async-timeout==4.0.2\r\n    - attrs==22.1.0\r\n    - audioread==3.0.0\r\n    - backoff==2.2.1\r\n    - beautifulsoup4==4.11.1\r\n    - bentoml==1.0.12\r\n    - boto3==1.26.3\r\n    - botocore==1.29.3\r\n    - build==0.9.0\r\n    - cattrs==22.2.0\r\n    - cerberus==1.3.4\r\n    - cffi==1.15.1\r\n    - chardet==5.0.0\r\n    - charset-normalizer==2.1.1\r\n    - circus==0.18.0\r\n    - click==8.1.3\r\n    - click-option-group==0.5.5\r\n    - cloudpickle==2.2.0\r\n    - commonmark==0.9.1\r\n    - configparser==5.3.0\r\n    - contextlib2==21.6.0\r\n    - cycler==0.11.0\r\n    - cython==0.29.32\r\n    - decorator==5.1.1\r\n    - deepmerge==1.1.0\r\n    - deprecated==1.2.13\r\n    - distance==0.1.3\r\n    - docker==5.0.2\r\n    - einops==0.3.0\r\n    - exceptiongroup==1.1.0\r\n    - filelock==3.8.0\r\n    - flask==2.2.2\r\n    - frozenlist==1.3.1\r\n    - fs==2.4.16\r\n    - g2p-en==2.1.0\r\n    - gdown==4.4.0\r\n    - googleapis-common-protos==1.57.1\r\n    - grpcio==1.50.0\r\n    - gunicorn==20.1.0\r\n    - h11==0.14.0\r\n    - humanfriendly==10.0\r\n    - idna==3.4\r\n    - importlib-metadata==5.0.0\r\n    - importlib-resources==5.10.0\r\n    - inflect==5.0.2\r\n    - itsdangerous==2.1.2\r\n    - jinja2==3.1.2\r\n    - jmespath==1.0.1\r\n    - joblib==1.2.0\r\n    - kiwisolver==1.4.4\r\n    - librosa==0.8.0\r\n    - llvmlite==0.31.0\r\n    - mako==1.2.3\r\n    - markupsafe==2.1.1\r\n    - matplotlib==3.3.3\r\n    - multidict==6.0.2\r\n    - nltk==3.7\r\n    - numba==0.48.0\r\n    - numpy==1.22.3\r\n    - opentelemetry-api==1.14.0\r\n    - opentelemetry-exporter-otlp-proto-http==1.14.0\r\n    - opentelemetry-instrumentation==0.35b0\r\n    - opentelemetry-instrumentation-aiohttp-client==0.35b0\r\n    - opentelemetry-instrumentation-asgi==0.35b0\r\n    - opentelemetry-proto==1.14.0\r\n    - opentelemetry-sdk==1.14.0\r\n    - opentelemetry-semantic-conventions==0.35b0\r\n    - opentelemetry-util-http==0.35b0\r\n    - packaging==21.3\r\n    - pandas==1.5.2\r\n    - pathspec==0.10.3\r\n    - pep517==0.13.0\r\n    - pillow==9.3.0\r\n    - pip-requirements-parser==32.0.1\r\n    - pip-tools==6.12.1\r\n    - pooch==1.6.0\r\n    - prometheus-client==0.15.0\r\n    - protobuf==3.20.3\r\n    - psutil==5.9.3\r\n    - pycparser==2.21\r\n    - pygments==2.14.0\r\n    - pynvml==11.4.1\r\n    - pyparsing==3.0.9\r\n    - pypinyin==0.39.0\r\n    - pysocks==1.7.1\r\n    - python-dateutil==2.8.2\r\n    - python-json-logger==2.0.4\r\n    - python-multipart==0.0.5\r\n    - pytz==2022.7\r\n    - pyword==1.0\r\n    - pyworld==0.2.10\r\n    - pyyaml==5.4.1\r\n    - pyzmq==24.0.1\r\n    - regex==2022.10.31\r\n    - requests==2.28.1\r\n    - resampy==0.3.1\r\n    - rich==13.0.0\r\n    - ruamel-yaml==0.17.21\r\n    - ruamel-yaml-clib==0.2.7\r\n    - s3transfer==0.6.0\r\n    - schema==0.7.5\r\n    - scikit-learn==0.23.2\r\n    - scipy==1.5.4\r\n    - simple-di==0.1.5\r\n    - six==1.16.0\r\n    - sniffio==1.3.0\r\n    - soundfile==0.10.3.post1\r\n    - soupsieve==2.3.2.post1\r\n    - sqlalchemy==1.3.24\r\n    - sqlalchemy-utils==0.36.5\r\n    - starlette==0.23.1\r\n    - tabulate==0.9.0\r\n    - tgt==1.4.4\r\n    - threadpoolctl==3.1.0\r\n    - tomli==2.0.1\r\n    - torch==1.9.0\r\n    - tornado==6.2\r\n    - tqdm==4.64.1\r\n    - typing-extensions==4.4.0\r\n    - unidecode==1.1.2\r\n    - urllib3==1.25.11\r\n    - uvicorn==0.20.0\r\n    - watchfiles==0.18.1\r\n    - websocket-client==1.4.2\r\n    - werkzeug==2.2.2\r\n    - wrapt==1.14.1\r\n    - yarl==1.8.1\r\n    - zipp==3.10.0\r\nprefix: /home/tran.duc.trungb@sun-asterisk.com/anaconda3/envs/bentoml-3.8\r\n```",
    "comments": [
      {
        "user": "aarnphm",
        "body": "cc @sauyon "
      },
      {
        "user": "sudoandros",
        "body": "Yeah, I have the same problem with 1.0.21. Was there any progress? "
      },
      {
        "user": "sauyon",
        "body": "Ah, whoops, sorry. Missed that ping. This is a known bug due to how we handle paths; for now the workaround is setting your `BENTOML_HOME=/some/path/without/at`; we haven't resolved this. If anybody would like to contribute this should be a good first issue!"
      }
    ]
  },
  {
    "issue_number": 3625,
    "title": "bug: containerize fails with alpine base image",
    "author": "smidm",
    "state": "closed",
    "created_at": "2023-03-01T05:43:49Z",
    "updated_at": "2024-07-12T00:01:33Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nwhen using `alpine` as a baseimage the `bentoml containerize` fails on `groupadd` command:\r\n\r\n```\r\n > [base-container 2/8] RUN groupadd -g 1034 -o bentoml && useradd -m -u 1034 -g 1034 -o -r bentoml:\r\n#5 0.318 /bin/sh: groupadd: not found\r\n```\r\n\r\nIt seems that `groupadd` is not supported on busybox based alpine linux. It is recommended to use `adduser` and `addgroup`, see https://stackoverflow.com/questions/49955097/how-do-i-add-a-user-when-im-using-alpine-as-a-base-image\n\n### To reproduce\n\n1. set \r\n```\r\ndocker:\r\n    base_image: alpine\r\n```\r\nin `bentofile.yaml`\r\n\r\n2. `bentoml build`\r\n3. `bentoml containerize`\r\n\n\n### Expected behavior\n\ndocker image will be build\n\n### Environment\n\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.0.15.post15+gf9ebcc37.d20230301\r\n`python`: 3.8.0\r\n`platform`: Linux-5.4.0-1068-aws-x86_64-with-glibc2.10\r\n`uid_gid`: 1000:1000\r\n`conda`: 4.8.4\r\n`in_conda_env`: True\r\n<details><summary><code>conda_packages</code></summary>\r\n\r\n<br>\r\n\r\n```yaml\r\nname: bentoml_dev\r\nchannels:\r\n  - pytorch\r\n  - conda-forge\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=conda_forge\r\n  - _openmp_mutex=4.5=2_kmp_llvm\r\n  - blas=1.0=mkl\r\n  - brotli=1.0.9=h5eee18b_7\r\n  - brotli-bin=1.0.9=h5eee18b_7\r\n  - brotlipy=0.7.0=py38h27cfd23_1003\r\n  - bzip2=1.0.8=h7b6447c_0\r\n  - ca-certificates=2023.01.10=h06a4308_0\r\n  - certifi=2022.12.7=py38h06a4308_0\r\n  - cffi=1.15.0=py38h7f8727e_0\r\n  - charset-normalizer=2.0.4=pyhd3eb1b0_0\r\n  - contourpy=1.0.5=py38hdb19cb5_0\r\n  - cryptography=38.0.4=py38h9ce1e76_0\r\n  - cudatoolkit=11.3.1=h2bc3f7f_2\r\n  - cycler=0.11.0=pyhd3eb1b0_0\r\n  - cython=0.29.33=py38h6a678d5_0\r\n  - ffmpeg=4.3=hf484d3e_0\r\n  - flit-core=3.6.0=pyhd3eb1b0_0\r\n  - fonttools=4.25.0=pyhd3eb1b0_0\r\n  - freetype=2.12.1=h4a9f257_0\r\n  - giflib=5.2.1=h5eee18b_3\r\n  - gmp=6.2.1=h295c915_3\r\n  - gnutls=3.6.15=he1e5248_0\r\n  - idna=3.4=py38h06a4308_0\r\n  - intel-openmp=2021.4.0=h06a4308_3561\r\n  - jpeg=9e=h7f8727e_0\r\n  - kiwisolver=1.4.4=py38h6a678d5_0\r\n  - lame=3.100=h7b6447c_0\r\n  - lcms2=2.12=h3be6417_0\r\n  - lerc=3.0=h295c915_0\r\n  - libbrotlicommon=1.0.9=h5eee18b_7\r\n  - libbrotlidec=1.0.9=h5eee18b_7\r\n  - libbrotlienc=1.0.9=h5eee18b_7\r\n  - libdeflate=1.8=h7f8727e_5\r\n  - libedit=3.1.20221030=h5eee18b_0\r\n  - libffi=3.2.1=hf484d3e_1007\r\n  - libgcc-ng=12.2.0=h65d4601_19\r\n  - libiconv=1.16=h7f8727e_2\r\n  - libidn2=2.3.2=h7f8727e_0\r\n  - libpng=1.6.37=hbc83047_0\r\n  - libstdcxx-ng=11.2.0=h1234567_1\r\n  - libtasn1=4.16.0=h27cfd23_0\r\n  - libtiff=4.5.0=h6a678d5_1\r\n  - libunistring=0.9.10=h27cfd23_0\r\n  - libuv=1.44.2=h5eee18b_0\r\n  - libwebp=1.2.4=h11a3e52_1\r\n  - libwebp-base=1.2.4=h5eee18b_1\r\n  - llvm-openmp=14.0.6=h9e868ea_0\r\n  - lz4-c=1.9.4=h6a678d5_0\r\n  - matplotlib-base=3.6.2=py38h945d387_0\r\n  - mkl=2021.4.0=h06a4308_640\r\n  - mkl-service=2.4.0=py38h7f8727e_0\r\n  - mkl_fft=1.3.1=py38hd3c417c_0\r\n  - mkl_random=1.2.2=py38h51133e4_0\r\n  - munkres=1.1.4=py_0\r\n  - ncurses=6.4=h6a678d5_0\r\n  - nettle=3.7.3=hbbd107a_1\r\n  - numpy=1.23.5=py38h14f4228_0\r\n  - numpy-base=1.23.5=py38h31eccc5_0\r\n  - openh264=2.1.1=h4ff587b_0\r\n  - openssl=1.1.1t=h7f8727e_0\r\n  - packaging=22.0=py38h06a4308_0\r\n  - pillow=9.3.0=py38h6a678d5_2\r\n  - pip=22.3.1=py38h06a4308_0\r\n  - pycocotools=2.0.6=py38h26c90d9_1\r\n  - pycparser=2.21=pyhd3eb1b0_0\r\n  - pyopenssl=22.0.0=pyhd3eb1b0_0\r\n  - pyparsing=3.0.9=py38h06a4308_0\r\n  - pysocks=1.7.1=py38h06a4308_0\r\n  - python=3.8.0=h0371630_2\r\n  - python-dateutil=2.8.2=pyhd3eb1b0_0\r\n  - python_abi=3.8=2_cp38\r\n  - pytorch=1.11.0=py3.8_cuda11.3_cudnn8.2.0_0\r\n  - pytorch-mutex=1.0=cuda\r\n  - readline=7.0=h7b6447c_5\r\n  - requests=2.28.1=py38h06a4308_0\r\n  - setuptools=65.6.3=py38h06a4308_0\r\n  - six=1.16.0=pyhd3eb1b0_1\r\n  - sqlite=3.33.0=h62c20be_0\r\n  - tk=8.6.12=h1ccaba5_0\r\n  - torchvision=0.12.0=py38_cu113\r\n  - typing_extensions=4.4.0=py38h06a4308_0\r\n  - urllib3=1.26.14=py38h06a4308_0\r\n  - wheel=0.38.4=py38h06a4308_0\r\n  - xz=5.2.10=h5eee18b_1\r\n  - zlib=1.2.13=h5eee18b_0\r\n  - zstd=1.5.2=ha4553b6_0\r\n  - pip:\r\n    - addict==2.4.0\r\n    - aiohttp==3.8.4\r\n    - aiosignal==1.3.1\r\n    - anyio==3.6.2\r\n    - appdirs==1.4.4\r\n    - asgiref==3.6.0\r\n    - async-timeout==4.0.2\r\n    - attrs==22.2.0\r\n    - backoff==2.2.1\r\n    - bentoml==1.0.15.post15+gf9ebcc37.d20230226\r\n    - boto3==1.26.78\r\n    - botocore==1.29.78\r\n    - build==0.10.0\r\n    - cattrs==22.2.0\r\n    - chumpy==0.70\r\n    - circus==0.18.0\r\n    - click==8.1.3\r\n    - click-option-group==0.5.5\r\n    - cloudpickle==2.2.1\r\n    - colorama==0.4.6\r\n    - contextlib2==21.6.0\r\n    - deepmerge==1.1.0\r\n    - deprecated==1.2.13\r\n    - exceptiongroup==1.1.0\r\n    - frozenlist==1.3.3\r\n    - fs==2.4.16\r\n    - googleapis-common-protos==1.58.0\r\n    - h11==0.14.0\r\n    - importlib-metadata==6.0.0\r\n    - jinja2==3.1.2\r\n    - jmespath==1.0.1\r\n    - joblib==1.2.0\r\n    - json-tricks==3.16.1\r\n    - markdown==3.4.1\r\n    - markdown-it-py==2.2.0\r\n    - markupsafe==2.1.2\r\n    - mdurl==0.1.2\r\n    - mmcv-full==1.7.0\r\n    - mmdet==2.28.1\r\n    - mmpose==0.29.0\r\n    - model-index==0.1.11\r\n    - multidict==6.0.4\r\n    - opencv-python==4.7.0.72\r\n    - openmim==0.3.6\r\n    - opentelemetry-api==1.14.0\r\n    - opentelemetry-exporter-otlp-proto-http==1.14.0\r\n    - opentelemetry-instrumentation==0.35b0\r\n    - opentelemetry-instrumentation-aiohttp-client==0.35b0\r\n    - opentelemetry-instrumentation-asgi==0.35b0\r\n    - opentelemetry-proto==1.14.0\r\n    - opentelemetry-sdk==1.14.0\r\n    - opentelemetry-semantic-conventions==0.35b0\r\n    - opentelemetry-util-http==0.35b0\r\n    - ordered-set==4.1.0\r\n    - pandas==1.5.3\r\n    - pathspec==0.11.0\r\n    - pillow-heif==0.10.0\r\n    - pip-requirements-parser==32.0.1\r\n    - pip-tools==6.12.2\r\n    - prometheus-client==0.16.0\r\n    - protobuf==3.20.3\r\n    - psutil==5.9.4\r\n    - pydantic==1.10.5\r\n    - pygments==2.14.0\r\n    - pynvml==11.5.0\r\n    - pyproject-hooks==1.0.0\r\n    - python-json-logger==2.0.7\r\n    - python-multipart==0.0.5\r\n    - pytz==2022.7.1\r\n    - pyyaml==6.0\r\n    - pyzmq==25.0.0\r\n    - rich==13.3.1\r\n    - s3transfer==0.6.0\r\n    - schema==0.7.5\r\n    - scikit-learn==1.2.1\r\n    - scipy==1.10.1\r\n    - simple-di==0.1.5\r\n    - sniffio==1.3.0\r\n    - starlette==0.25.0\r\n    - tabulate==0.9.0\r\n    - terminaltables==3.1.10\r\n    - threadpoolctl==3.1.0\r\n    - tomli==2.0.1\r\n    - tornado==6.2\r\n    - uvicorn==0.20.0\r\n    - watchfiles==0.18.1\r\n    - wrapt==1.14.1\r\n    - xtcocotools==1.13\r\n    - yapf==0.32.0\r\n    - yarl==1.8.2\r\n    - zipp==3.14.0\r\nprefix: /home/ubuntu/anaconda3/envs/bentoml_dev\r\n```\r\n\r\n</details>\r\n\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naddict==2.4.0\r\naiohttp==3.8.4\r\naiosignal==1.3.1\r\nanyio==3.6.2\r\nappdirs==1.4.4\r\nasgiref==3.6.0\r\nasync-timeout==4.0.2\r\nattrs==22.2.0\r\nbackoff==2.2.1\r\n-e git+https://github.com/smidm/BentoML.git@f9ebcc37339346e6e10cb9545a24e3233dbdc181#egg=bentoml\r\nboto3==1.26.78\r\nbotocore==1.29.78\r\nbrotlipy==0.7.0\r\nbuild==0.10.0\r\ncattrs==22.2.0\r\ncertifi @ file:///croot/certifi_1671487769961/work/certifi\r\ncffi @ file:///tmp/build/80754af9/cffi_1636541934635/work\r\ncharset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work\r\nchumpy==0.70\r\ncircus==0.18.0\r\nclick==8.1.3\r\nclick-option-group==0.5.5\r\ncloudpickle==2.2.1\r\ncolorama==0.4.6\r\ncontextlib2==21.6.0\r\ncontourpy @ file:///opt/conda/conda-bld/contourpy_1663827406301/work\r\ncryptography @ file:///croot/cryptography_1673298753778/work\r\ncycler @ file:///tmp/build/80754af9/cycler_1637851556182/work\r\nCython @ file:///croot/cython_1676568029361/work\r\ndeepmerge==1.1.0\r\nDeprecated==1.2.13\r\nexceptiongroup==1.1.0\r\nflit_core @ file:///opt/conda/conda-bld/flit-core_1644941570762/work/source/flit_core\r\nfonttools==4.25.0\r\nfrozenlist==1.3.3\r\nfs==2.4.16\r\ngoogleapis-common-protos==1.58.0\r\nh11==0.14.0\r\nidna @ file:///croot/idna_1666125576474/work\r\nimportlib-metadata==6.0.0\r\nJinja2==3.1.2\r\njmespath==1.0.1\r\njoblib==1.2.0\r\njson-tricks==3.16.1\r\nkiwisolver @ file:///croot/kiwisolver_1672387140495/work\r\nMarkdown==3.4.1\r\nmarkdown-it-py==2.2.0\r\nMarkupSafe==2.1.2\r\nmatplotlib @ file:///croot/matplotlib-suite_1670466153205/work\r\nmdurl==0.1.2\r\nmkl-fft==1.3.1\r\nmkl-random @ file:///tmp/build/80754af9/mkl_random_1626186064646/work\r\nmkl-service==2.4.0\r\nmmcv-full==1.7.0\r\nmmdet==2.28.1\r\nmmpose==0.29.0\r\nmodel-index==0.1.11\r\nmultidict==6.0.4\r\nmunkres==1.1.4\r\nnumpy @ file:///croot/numpy_and_numpy_base_1672336185480/work\r\nopencv-python==4.7.0.72\r\nopenmim==0.3.6\r\nopentelemetry-api==1.14.0\r\nopentelemetry-exporter-otlp-proto-http==1.14.0\r\nopentelemetry-instrumentation==0.35b0\r\nopentelemetry-instrumentation-aiohttp-client==0.35b0\r\nopentelemetry-instrumentation-asgi==0.35b0\r\nopentelemetry-proto==1.14.0\r\nopentelemetry-sdk==1.14.0\r\nopentelemetry-semantic-conventions==0.35b0\r\nopentelemetry-util-http==0.35b0\r\nordered-set==4.1.0\r\npackaging @ file:///croot/packaging_1671697413597/work\r\npandas==1.5.3\r\npathspec==0.11.0\r\nPillow==9.3.0\r\npillow-heif==0.10.0\r\npip-requirements-parser==32.0.1\r\npip-tools==6.12.2\r\nprometheus-client==0.16.0\r\nprotobuf==3.20.3\r\npsutil==5.9.4\r\npycocotools @ file:///home/conda/feedstock_root/build_artifacts/pycocotools_1667626621479/work\r\npycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\r\npydantic==1.10.5\r\nPygments==2.14.0\r\npynvml==11.5.0\r\npyOpenSSL @ file:///opt/conda/conda-bld/pyopenssl_1643788558760/work\r\npyparsing @ file:///opt/conda/conda-bld/pyparsing_1661452539315/work\r\npyproject_hooks==1.0.0\r\nPySocks @ file:///tmp/build/80754af9/pysocks_1605305779399/work\r\npython-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\r\npython-json-logger==2.0.7\r\npython-multipart==0.0.5\r\npytz==2022.7.1\r\nPyYAML==6.0\r\npyzmq==25.0.0\r\nrequests @ file:///opt/conda/conda-bld/requests_1657734628632/work\r\nrich==13.3.1\r\ns3transfer==0.6.0\r\nschema==0.7.5\r\nscikit-learn==1.2.1\r\nscipy==1.10.1\r\nsimple-di==0.1.5\r\nsix @ file:///tmp/build/80754af9/six_1644875935023/work\r\nsniffio==1.3.0\r\nstarlette==0.25.0\r\ntabulate==0.9.0\r\nterminaltables==3.1.10\r\nthreadpoolctl==3.1.0\r\ntomli==2.0.1\r\ntorch==1.11.0\r\ntorchvision==0.12.0\r\ntornado==6.2\r\ntyping_extensions @ file:///croot/typing_extensions_1669924550328/work\r\nurllib3 @ file:///croot/urllib3_1673575502006/work\r\nuvicorn==0.20.0\r\nwatchfiles==0.18.1\r\nwrapt==1.14.1\r\nxtcocotools==1.13\r\nyapf==0.32.0\r\nyarl==1.8.2\r\nzipp==3.14.0\r\n```\r\n\r\n</details>\r\n",
    "comments": []
  },
  {
    "issue_number": 3802,
    "title": "bug: HTTPS doesn't work with client",
    "author": "nadworny",
    "state": "closed",
    "created_at": "2023-04-28T12:39:10Z",
    "updated_at": "2024-07-11T12:24:35Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nUsing HTTPS url for bentoml client doesn't work.\r\n\r\nI'm getting the following error (since my service is doing https termination, it's 301):\r\n```\r\nbentoml.exceptions.RemoteException: Failed to get OpenAPI schema from the server: 301 Moved Permanently:\r\n```\n\n### To reproduce\n\n1. Initialize client from https url, i.e. `HTTPClient.from_url(\"https://your-server\")`\r\n2. Start your app\n\n### Expected behavior\n\nHTTPS urls are supported.\n\n### Environment\n\nbentoml: 1.0.19\r\npython: 3.9.16",
    "comments": [
      {
        "user": "frostming",
        "body": "This is solved by using httpx to replace the aiohttp"
      }
    ]
  },
  {
    "issue_number": 3661,
    "title": "Do metrics returned from the `/metrics` endpoint include webserver + asyncio timings?",
    "author": "Ch0ronomato",
    "state": "closed",
    "created_at": "2023-03-13T22:47:43Z",
    "updated_at": "2024-07-11T12:23:28Z",
    "labels": [],
    "body": "Hi!\r\n\r\ni was looking at the metrics endpoint the bento (0.13-LTS) api and I was wondering if the response times histogram reported includes all of the asyncio / webserver bits or if it's essentially just the model runtime? \r\n\r\nWithout burying the lead, the metric name is `BENTOML_XXX_request_duration_seconds_bucket`, where XXX is a service that gets packed, something like `XXX(BentoService)`",
    "comments": [
      {
        "user": "ssheng",
        "body": "Hi @Ch0ronomato, metrics instrumentation is implemented as a middleware on the server. It starts tracking after the server receives the requests and ends tracking before the server sends out the response. So the tracking includes everything in between and does not include the socket IO time. Any plan to move to 1.0?"
      },
      {
        "user": "Ch0ronomato",
        "body": "Yeah we do want to upgrade, I just wasn't tossing it on this. Trying to do one change at a time, ha.\r\n\r\nSo if I'm correct, any time the batching layer holds the request would not be reflected in the metrics? "
      },
      {
        "user": "ssheng",
        "body": "Yes, you are correct. In 0.13, latency incurred during batching is not reflected in the metrics, because the marshaling server is external to the server. In 1.0, latency incurred during batching is reflected in the metrics, because the batching logic is internal to the runner. Version 1.0 also provides metrics like batch size for each runner."
      }
    ]
  },
  {
    "issue_number": 3652,
    "title": "feature: Custom exception management ",
    "author": "tkaraouzene",
    "state": "closed",
    "created_at": "2023-03-10T15:32:47Z",
    "updated_at": "2024-07-11T12:22:44Z",
    "labels": [],
    "body": "### Feature request\n\nHi and thanks for this library!\r\n\r\nI would like to be able to add some custom exceptions as possible output.\n\n### Motivation\n\nI've developed a library which train and infer a model based on timeseries data. \r\nDuring these two steps, some data validation are made (mainly for dataframe index which are datetime).\r\nI've implemented some custom Exceptions i.e.: ``NotSortedTimestamps`` when timestamps are not sorted.\r\n\r\nHowever when I serve my model through ``bentoml`` these error will appears as ``InternalServerError``. \r\nSo it is not really explicit for users.\r\n\r\nIs it possible to provide a list of custom exceptions?\r\n\r\nThanks a lot\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "Make the custom exceptions subclass of `bentoml.exceptions.BentoMLException` and it will correctly returned"
      }
    ]
  },
  {
    "issue_number": 3674,
    "title": "Access query params in bentoml.Context",
    "author": "Lorenzo-N",
    "state": "closed",
    "created_at": "2023-03-15T14:08:01Z",
    "updated_at": "2024-07-11T12:21:33Z",
    "labels": [],
    "body": "### Feature request\n\nI need to retrieve the query params within an API. I saw in the documentation that I can access the request context through the bentoml.Context ctx argument, like in the following example:\r\n\r\n```\r\ndef predict(input_array: np.ndarray, ctx: bentoml.Context) -> np.ndarray:\r\n    # get request headers\r\n    request_headers = ctx.request.headers\r\n```\r\n\r\nHowever, looking at the implementation I see that only the headers are contained in ctx.request.\r\nI would like to retrieve also the query params in the context.\n\n### Motivation\n\nI think this feature is useful for being able to access additional model parameters that are separate from the data to be predicted.\r\n\r\nFor example in a timeseries forecasting model I would like to pass the series data in POST and a parameter in GET to specify the time interval to predict. Currently to do this I can:\r\n- pass everything in post in json format with params and data instead of a simple numpy array\r\n- pass the additional parameter in a header\r\n\r\nHowever, I think the more correct way would be to be able to access the query params so as to pass the numpy array in POST and the additional parameter in GET.\n\n### Other\n\nTo do this I think you just need to modify the line [context.py:104](https://github.com/bentoml/BentoML/blob/main/src/bentoml/_internal/context.py#L104) by adding `request.query_params` in the constructor and the `InferenceApiContext.RequestContext` class accordingly.\r\n",
    "comments": []
  },
  {
    "issue_number": 3789,
    "title": "bug: Non-OS export ignores path argument",
    "author": "NicoG94",
    "state": "closed",
    "created_at": "2023-04-25T07:49:21Z",
    "updated_at": "2024-07-11T12:21:19Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWhen using the export bento function with s3 and user and password the bucket gets removed from the dest_uri [here](https://github.com/bentoml/BentoML/blob/main/src/bentoml/_internal/exportable.py#L311) which causes the function to fail.\r\n\r\n\r\n```python\r\nimport bentoml\r\n\r\nbentoml.export_bento('test_service:latest',\r\n                     'test_bucket', # same happens with 'test_bucket/'\r\n                     protocol='s3', subpath='bentos',\r\n                     user='test_user', passwd='test_pw')\r\n```\r\n\r\nI think [this line](https://github.com/bentoml/BentoML/blob/main/src/bentoml/_internal/exportable.py#L272) causes the issue as the bucket name stays in subpath and resource will be an empty string.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/mnt/c/Users/some_path/export_import_test.py\", line 44, in <module>\r\n    bentoml.export_bento(bento, s3bucket)\r\n  File \"/home/nico_linux/.cache/pypoetry/virtualenvs/some-repo-COtPtbak-py3.9/lib/python3.9/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/home/nico_linux/.cache/pypoetry/virtualenvs/ some-repo -COtPtbak-py3.9/lib/python3.9/site-packages/bentoml/bentos.py\", line 224, in export_bento\r\n    return bento.export(\r\n  File \"/home/nico_linux/.cache/pypoetry/virtualenvs/ some-repo -COtPtbak-py3.9/lib/python3.9/site-packages/bentoml/_internal/exportable.py\", line 327, in export\r\n    destfs = fs.open_fs(dest_uri)\r\n  File \"/home/nico_linux/.cache/pypoetry/virtualenvs/ some-repo -COtPtbak-py3.9/lib/python3.9/site-packages/fs/opener/registry.py\", line 220, in open_fs\r\n    _fs, _path = self.open(\r\n  File \"/home/nico_linux/.cache/pypoetry/virtualenvs/ some-repo -COtPtbak-py3.9/lib/python3.9/site-packages/fs/opener/registry.py\", line 177, in open\r\n    open_fs = opener.open_fs(fs_url, parse_result, writeable, create, cwd)\r\n  File \"/home/nico_linux/.cache/pypoetry/virtualenvs/ some-repo -COtPtbak-py3.9/lib/python3.9/site-packages/fs_s3fs/opener.py\", line 22, in open_fs\r\n    raise OpenerError(\"invalid bucket name in '{}'\".format(fs_url))\r\nfs.opener.errors.OpenerError: invalid bucket name in 's3://test_user:test_pw@?'\r\n\r\n```\r\n\r\n\r\n\r\nwhen attaching \"s3://\" in front of the bucket name:\r\n\r\n```python\r\nimport bentoml\r\n\r\nbentoml.export_bento('test_service:latest',\r\n                     's3://test_bucket',\r\n                     protocol='s3', subpath='bentos',\r\n                     user='test_user', passwd='test_pw')\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/mnt/c/Program Files/JetBrains/PyCharm 2022.3.3/plugins/python/helpers/pydev/pydevconsole.py\", line 364, in runcode\r\n    coro = func()\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/home/nico_linux/miniforge3/envs/parrot-main/lib/python3.9/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/home/nico_linux/miniforge3/envs/parrot-main/lib/python3.9/site-packages/bentoml/bentos.py\", line 224, in export_bento\r\n    return bento.export(\r\n  File \"/home/nico_linux/miniforge3/envs/parrot-main/lib/python3.9/site-packages/bentoml/_internal/exportable.py\", line 203, in export\r\n    raise ValueError(\r\nValueError: An FS URL was passed as the output path; all additional information should be passed as part of the URL.\r\n\r\n```\r\n\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml[aws]: 1.0.16 (or 1.0.18)\r\npython: 3.9.5\r\nOS: wsl\r\n\r\n",
    "comments": [
      {
        "user": "NicoG94",
        "body": "also when using bentoml.export() locally without s3, username and password it works fine and takes around 2 min. But when using it in our gitlab ci pipeline it takes 50min. Would you have a guess why that is?"
      },
      {
        "user": "sauyon",
        "body": "For now you should probably pass a full FS URL:\r\n\r\n```python\r\nbentoml.export_bento('test_service:latest', 's3://test_user:test_pw@test_bucket/bentos')\r\n```\r\n\r\nBut I think this is a bug with our non-OS path handling, I'll put up a quick PR.\r\n"
      }
    ]
  },
  {
    "issue_number": 3885,
    "title": "feature(pydantic-v2): support parsing nested models",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2023-05-23T21:41:08Z",
    "updated_at": "2024-07-11T12:19:43Z",
    "labels": [],
    "body": "### Feature request\n\nCurrently, the OpenAPI schema support for pydantic v2 doesn't support parsing nested model.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3929,
    "title": "feature: `bentoml.io.PydanticModel`",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2023-06-05T06:47:20Z",
    "updated_at": "2024-07-11T12:19:34Z",
    "labels": [],
    "body": "### Feature request\r\n\r\nSupport for more extensive pydantic model\r\n\r\n### Motivation\r\n\r\n```python\r\n@svc.api(input=bentoml.io.PydanticModel(...), output=...)\r\ndef predict(inputs): ...\r\n```\r\n\r\n### Other\r\n\r\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3483,
    "title": "bug: bentoml CLI containerize --output parameter",
    "author": "mtichy92",
    "state": "closed",
    "created_at": "2023-01-26T08:23:43Z",
    "updated_at": "2024-07-11T11:41:31Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nHello, we encounter this strange behaviour which seems like bug in CLI.\r\nWhen running `bentoml containerize --opt output=type=registry --image-tag registry.gitlab.com/XXX --backend buildctl TAGGED_BENTO:TAG` it seems like the default --output parameter is not overridden and appears twice in actual command.\r\nEither switching to --output type=registry does not help. Using default `--backend docker` does not help. Using `--push` or `--opt push` does not help (throwing `unknown flag: --push` error)\r\n\r\nTraceback looks like this:\r\n```\r\nerror: currently only single Exports can be specified (or Error response from daemon: exporter \"registry\" could not be found - when using docker)\r\n\r\nEncountered exception while trying to building image: Command '['/opt/homebrew/bin/buildctl', 'build', '--local',\r\n'context=/var/folders/jz/bb9zdtl57n70bp2bw1lg69km0000gn/T/tmpex446mo8fsTempFS/', '--local',\r\n'dockerfile=/var/folders/jz/bb9zdtl57n70bp2bw1lg69km0000gn/T/tmpex446mo8fsTempFS/env/docker', '--frontend',\r\n'dockerfile.v0', '--output', 'type=registry', '--output', 'type=docker,name=docker.io/registry.gitlab.com/XXX']' returned\r\nnon-zero exit status 1.\r\n```\r\n\r\nCan you please look at this? Since Yatai is far from being production ready we are trying to containerize and deploy bentos ourselves and this is blocker. Seems like not working on either of OCI backends supported.\r\n\r\nThank you!\r\n\r\n### To reproduce\r\n\r\nSteps to reproduce:\r\n\r\nrun any `bentoml containerize` command with `--opt output=type=registry`\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nbentoml: 1.0.13 (also tested on 1.0.12)\r\npython: 3.9.16\r\ndocker: 20.10.21\r\nbuildctl: 0.11.1",
    "comments": [
      {
        "user": "KimSoungRyoul",
        "body": "Apart from the bug fixes, wouldn't this issue be solved if you did the following?\r\n\r\n~~~\r\nbentoml containerize iris_classifier:betk2ie5koavizx5 --opt output=type=image,name=registry.gitlab.com/XXX,push=true  --backend buildctl\r\n~~~\r\nhttps://github.com/moby/buildkit#output\r\nI think this issue is caused by `docker build` and `buildctl build` having different options style."
      },
      {
        "user": "aarnphm",
        "body": "I will take a look into this. Thanks for finding it."
      },
      {
        "user": "mtichy92",
        "body": "@KimSoungRyoul Hello, thanks for the suggestion - you are right. If I do quick hack in bentoml/_internal/container/base.py line `cmds.extend(self.construct_build_args(**attrs))` to `cmds.extend(self.construct_build_args(**attrs)[:-2])` then your suggested command works fine."
      }
    ]
  },
  {
    "issue_number": 3517,
    "title": "bug(ci): flaky inteceptors tests.",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2023-02-03T01:44:38Z",
    "updated_at": "2024-07-11T11:39:50Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nSometimes the interceptors unit tests failed on the main branch due to the inability to mount ports.\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nn/a",
    "comments": []
  },
  {
    "issue_number": 3477,
    "title": "feature(io): bentoml.io.ProtoMessage",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2023-01-25T05:32:16Z",
    "updated_at": "2024-07-11T11:33:51Z",
    "labels": [],
    "body": "### Feature request\r\n\r\nwould be nice if this can be a plugin contribution under [bentoml/plugins](https://github.com/bentoml/plugins)\r\n\r\n### Motivation\r\n\r\n_No response_\r\n\r\n### Other\r\n\r\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3541,
    "title": "bug: filename always set to `<request body>` when using multipart",
    "author": "sauyon",
    "state": "closed",
    "created_at": "2023-02-08T19:36:11Z",
    "updated_at": "2024-07-11T10:53:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nWith the API\r\n\r\n```\r\n@svc.api(\r\n    input=Multipart(f=File()),\r\n    output=...\r\n)\r\n```\r\n\r\n`f.name` will always be `<request body>` instead of its real name; this is a bug in the way we handle files inside the multipart descriptor.\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nN/A",
    "comments": [
      {
        "user": "frostming",
        "body": "No longer an issue on the new service API"
      }
    ]
  },
  {
    "issue_number": 3488,
    "title": "bug: long build times when using in-project venv",
    "author": "FirefoxMetzger",
    "state": "closed",
    "created_at": "2023-01-28T09:21:19Z",
    "updated_at": "2024-07-11T10:53:09Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nI like to use in-project venvs during model development (a folder called `.venv` inside `project_root` which among other things contains the `site-packages` for the project). However, if I do so with BentoML then the build time for `bentoml build` increases several fold. This is because bentoml will walk over the entire build context and copy files.\r\n\r\nThe problem is that even if I exclude the `.venv` folder explicitly, it will still be walked over. No files are added to the virtual context - ofc - but it still takes time to traverse this tree, which could be saved by simply skipping it. If I am not mistaken, this traces back to the following loop:\r\n\r\nhttps://github.com/bentoml/BentoML/blob/fb08105b304d2f95dacb23c397e7fbb1f909d583/src/bentoml/_internal/bento/bento.py#L225-L236\r\n\r\nThe problem here is that we do a generic `ctx_fs.walk()` and filter manually instead of handing over the include/exclude filters to `fs` and have it worry about the rest. In fact, with the following change builds are fast again:\r\n\r\n```python\r\nfor dir_path, _, files in ctx_fs.walk(\r\n            filter=build_config.include,\r\n            filter_dirs=build_config.include,\r\n            exclude=build_config.exclude,\r\n            exclude_dirs=build_config.exclude,\r\n        ):\r\n            for f in files:\r\n                path = fs.path.combine(dir_path, f.name).lstrip(\"/\")\r\n                if specs.includes(\r\n                    path,\r\n                    recurse_exclude_spec=filter(\r\n                        lambda s: fs.path.isparent(s[0], dir_path),\r\n                        specs.from_path(build_ctx),\r\n                    ),\r\n                ):\r\n                    target_fs.makedirs(dir_path, recreate=True)\r\n                    copy_file(ctx_fs, path, target_fs, path)\r\n```\n\n### To reproduce\n\n<details>\r\n<summary>Example files</summary>\r\n\r\n```python\r\n# train.py\r\n\r\nimport bentoml\r\n\r\nfrom sklearn import svm\r\nfrom sklearn import datasets\r\n\r\n# Load training data set\r\niris = datasets.load_iris()\r\nX, y = iris.data, iris.target\r\n\r\n# Train the model\r\nclf = svm.SVC(gamma='scale')\r\nclf.fit(X, y)\r\n\r\n# Save model to the BentoML local model store\r\nsaved_model = bentoml.sklearn.save_model(\"iris_clf\", clf)\r\nprint(f\"Model saved: {saved_model}\")\r\n\r\n```\r\n\r\n```python\r\nservice.py\r\n\r\nimport numpy as np\r\nimport bentoml\r\nfrom bentoml.io import NumpyNdarray\r\n\r\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\r\n\r\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\r\n\r\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\r\ndef classify(input_series: np.ndarray) -> np.ndarray:\r\n    result = iris_clf_runner.predict.run(input_series)\r\n    return result\r\n\r\n```\r\n\r\n```python\r\nservice: \"service:svc\"  # Same as the argument passed to `bentoml serve`\r\nlabels:\r\n   owner: bentoml-team\r\n   stage: dev\r\ninclude:\r\n- \"*.py\"  # A pattern for matching which files to include in the bento\r\nexclude:\r\n- \".venv/\"  # Note the exclusion of venv\r\npython:\r\n   packages:  # Additional pip packages required by the service\r\n   - scikit-learn\r\n   - pandas\r\n\r\n```\r\n\r\n</details>\r\n\r\nSetup a sample project (files above) with the following structure:\r\n\r\n```\r\nproject_root\r\n|___bentofile.yml\r\n|___service.py\r\n|___train.py\r\n```\r\n\r\nBuild the project in the usual way. This should run reasonably fast (a few sec on my machine).\r\n\r\n```bash\r\npython train.py \r\nbentoml build\r\n```\r\n\r\nNow inside `project_root`, create an in-project venv and add the needed dependencies: \r\n\r\n```bash\r\npython -m venv .venv\r\n.venv/Scripts/activate  # assuming you are on linux, on Windows the file is called activate.bash or Activate.ps1\r\npip install bentoml scikit-learn pandas\r\n```\r\n\r\nwhen you build the project again using the just created venv, you will notice a significantly longer build time. If you then apply the patch suggested above, build time will be back to normal again :)\n\n### Expected behavior\n\nWe respect exclusion rules and don't walk into folders that are explicitly excluded.\n\n### Environment\n\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.0.13\r\n`python`: 3.8.8\r\n`platform`: Windows-10-10.0.22000-SP0\r\n`is_window_admin`: False\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiohttp==3.7.4.post0\r\nanyio==3.6.2\r\nappdirs==1.4.4\r\nasgiref==3.6.0\r\nasync-timeout==3.0.1\r\nattrs==22.2.0\r\nbackoff==2.2.1\r\nbentoml==1.0.13\r\nblack==23.1a1\r\nbuild==0.10.0\r\ncattrs==22.2.0\r\ncertifi==2022.12.7\r\nchardet==4.0.0\r\ncharset-normalizer==3.0.1\r\ncircus==0.18.0\r\nclick==8.1.3\r\nclick-option-group==0.5.5\r\ncloudpickle==2.2.1\r\ncolorama==0.4.6\r\ncontextlib2==21.6.0\r\ndeepmerge==1.1.0\r\nDeprecated==1.2.13\r\nexceptiongroup==1.1.0\r\nfs==2.4.16\r\ngoogleapis-common-protos==1.58.0\r\nh11==0.14.0\r\nidna==3.4\r\nJinja2==3.1.2\r\njoblib==1.2.0\r\nmarkdown-it-py==2.1.0\r\nMarkupSafe==2.1.2\r\nmdurl==0.1.2\r\nmultidict==6.0.4\r\nmypy-extensions==0.4.3\r\nnumpy==1.24.1\r\nopentelemetry-api==1.14.0\r\nopentelemetry-exporter-otlp-proto-http==1.14.0\r\nopentelemetry-instrumentation==0.35b0\r\nopentelemetry-instrumentation-aiohttp-client==0.35b0\r\nopentelemetry-instrumentation-asgi==0.35b0\r\nopentelemetry-proto==1.14.0\r\nopentelemetry-sdk==1.14.0\r\nopentelemetry-semantic-conventions==0.35b0\r\nopentelemetry-util-http==0.35b0\r\npackaging==21.3\r\npandas==1.5.3\r\npathspec==0.11.0\r\npip-requirements-parser==32.0.1\r\npip-tools==6.12.1\r\nplatformdirs==2.6.2\r\nprometheus-client==0.16.0\r\nprotobuf==3.20.3\r\npsutil==5.9.4\r\nPygments==2.14.0\r\npynvml==11.4.1\r\npyparsing==3.0.9\r\npyproject_hooks==1.0.0\r\npython-dateutil==2.8.2\r\npython-json-logger==2.0.4\r\npython-multipart==0.0.5\r\npytz==2022.7.1\r\nPyYAML==6.0\r\npyzmq==25.0.0\r\nrequests==2.28.2\r\nrich==13.3.0\r\nschema==0.7.5\r\nscikit-learn==1.2.1\r\nscipy==1.10.0\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nstarlette==0.23.1\r\nthreadpoolctl==3.1.0\r\ntomli==2.0.1\r\ntornado==6.2\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.14\r\nuvicorn==0.20.0\r\nwatchfiles==0.18.1\r\nwrapt==1.14.1\r\nyarl==1.8.2\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "FirefoxMetzger",
        "body": "I didn't explicitly test this, but with the include/exclude rules given to `fs`, we should be able to simplify this loop and can likely drop the custom logic for checking if the file should be included or not.\r\n\r\n(Also, if this sounds like a good change, I'd be happy to submit a PR.)"
      },
      {
        "user": "frostming",
        "body": "This is fixed on the latest BentoML version"
      }
    ]
  },
  {
    "issue_number": 3464,
    "title": "bug: Win 11, Cuda, bento serve. Api service significantly slows SD generation.",
    "author": "pr1ntr",
    "state": "open",
    "created_at": "2023-01-20T18:11:57Z",
    "updated_at": "2024-07-11T10:52:33Z",
    "labels": [
      "bug",
      "feedback-wanted"
    ],
    "body": "### Describe the bug\n\nSD generation is incredibly slow running pytorch 1.13.1 + cuda 11.7 when I run Run `BENTOML_CONFIG=configuration.yaml PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  bentoml serve service:svc --production` locally.`\r\n\r\nWhen I submit the request through postman, while its processing the request its like 100x slower than if I just cancel the request. Once the request is cancelled the generation time goes from 10min to about 10-20 seconds.\r\n\r\nI can confirm that `self.device = \"cuda\"`\n\n### To reproduce\n\n1. BENTOML_CONFIG=configuration.yaml PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  bentoml serve service:svc --production\r\n2. Submit request to txt2img through postman\r\n3. observe that it takes a looong time\r\n4. cancel the request\r\n5. sd generation goes much faster.\r\n\r\nI put `image.save('./img.jpg')` before the response so that I can test the image and it works.\n\n### Expected behavior\n\nSD Should generate in less than 15 seconds for this hardware.\n\n### Environment\n\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.0.13\r\n`python`: 3.10.5\r\n`platform`: Windows-10-10.0.22621-SP0\r\n`is_window_admin`: False\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naccelerate==0.15.0\r\naiohttp==3.8.3\r\naiosignal==1.3.1\r\nanyio==3.6.2\r\nappdirs==1.4.4\r\nasgiref==3.6.0\r\nasync-timeout==4.0.2\r\nattrs==22.2.0\r\nbackoff==2.2.1\r\nbentoml==1.0.13\r\nbuild==0.10.0\r\ncattrs==22.2.0\r\ncertifi==2022.12.7\r\ncharset-normalizer==2.1.1\r\ncircus==0.18.0\r\nclick==8.1.3\r\nclick-option-group==0.5.5\r\ncloudpickle==2.2.1\r\ncolorama==0.4.6\r\ncontextlib2==21.6.0\r\ndeepmerge==1.1.0\r\nDeprecated==1.2.13\r\ndiffusers==0.11.1\r\nexceptiongroup==1.1.0\r\nfastapi==0.89.1\r\nfilelock==3.9.0\r\nfrozenlist==1.3.3\r\nfs==2.4.16\r\nftfy==6.1.1\r\ngoogleapis-common-protos==1.58.0\r\nh11==0.14.0\r\nhuggingface-hub==0.11.1\r\nidna==3.4\r\nimportlib-metadata==6.0.0\r\nJinja2==3.1.2\r\nmarkdown-it-py==2.1.0\r\nMarkupSafe==2.1.2\r\nmdurl==0.1.2\r\nmultidict==6.0.4\r\nnumpy==1.24.1\r\nopentelemetry-api==1.14.0\r\nopentelemetry-exporter-otlp-proto-http==1.14.0\r\nopentelemetry-instrumentation==0.35b0\r\nopentelemetry-instrumentation-aiohttp-client==0.35b0\r\nopentelemetry-instrumentation-asgi==0.35b0\r\nopentelemetry-proto==1.14.0\r\nopentelemetry-sdk==1.14.0\r\nopentelemetry-semantic-conventions==0.35b0\r\nopentelemetry-util-http==0.35b0\r\npackaging==21.3\r\npathspec==0.10.3\r\nPillow==9.4.0\r\npip-requirements-parser==32.0.1\r\npip-tools==6.12.1\r\nprometheus-client==0.15.0\r\nprotobuf==3.20.3\r\npsutil==5.9.4\r\npydantic==1.10.4\r\nPygments==2.14.0\r\npynvml==11.4.1\r\npyparsing==3.0.9\r\npyproject_hooks==1.0.0\r\npython-dateutil==2.8.2\r\npython-json-logger==2.0.4\r\npython-multipart==0.0.5\r\nPyYAML==6.0\r\npyzmq==25.0.0\r\nregex==2022.10.31\r\nrequests==2.28.2\r\nrich==13.2.0\r\nschema==0.7.5\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nstarlette==0.22.0\r\ntokenizers==0.13.2\r\ntomli==2.0.1\r\ntorch==1.13.1+cu117\r\ntorchaudio==0.13.1+cu117\r\ntorchvision==0.14.1+cu117\r\ntornado==6.2\r\ntqdm==4.64.1\r\ntransformers==4.25.1\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.14\r\nuvicorn==0.20.0\r\nwatchfiles==0.18.1\r\nwcwidth==0.2.6\r\nwrapt==1.14.1\r\nyarl==1.8.2\r\nzipp==3.11.0\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "frostming",
        "body": "Can you try it on the latest version of BentoML? Thanks"
      }
    ]
  },
  {
    "issue_number": 3462,
    "title": "feature: bentoml.io.PandasDataFrame dtype enhancement",
    "author": "aarnphm",
    "state": "open",
    "created_at": "2023-01-20T08:04:47Z",
    "updated_at": "2024-07-11T10:47:34Z",
    "labels": [
      "enhancement",
      "io-descriptor"
    ],
    "body": "### Feature request\n\ncleanup some implementation for PandasDataFrame as well.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3446,
    "title": "refactor(grpc): `enable_so_reuseport` for `reserve_free_port` should be handled separately",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2023-01-18T01:33:28Z",
    "updated_at": "2024-07-11T10:47:06Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nRight now, `enable_so_reuseport` is set under `reserve_free_port`, which is not correct. It should be a different function to set the existing port instead of using `reserve_free_port`.\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nn/a",
    "comments": [
      {
        "user": "frostming",
        "body": "I reviewed the function and find `enable_so_reuseport` takes effect on a `socket` object, so it is fine to keep it there.\r\n\r\nClose this first, we can revisit it in the future."
      }
    ]
  },
  {
    "issue_number": 3437,
    "title": "triton: supports unload/load model on demand",
    "author": "aarnphm",
    "state": "open",
    "created_at": "2023-01-17T08:39:52Z",
    "updated_at": "2024-07-11T10:44:35Z",
    "labels": [
      "enhancement",
      "framework"
    ],
    "body": "### Feature request\n\nImplement MODEL_CONTROL_MODE to be explicit and allow given model to be loaded on demand.\r\n\r\nWe should also provide ability to teardown model after a period of time, that can be configured via configuration.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3393,
    "title": "bug: ClientOSError aiohttp.streams in read",
    "author": "junyeongchoi-29cm",
    "state": "open",
    "created_at": "2023-01-06T03:25:45Z",
    "updated_at": "2024-07-11T10:14:25Z",
    "labels": [
      "bug",
      "feedback-wanted"
    ],
    "body": "### Describe the bug\r\n\r\nI am using the pytorch and ResNet model serving with bentoml in production. Using this model, I developed an API to extract features of two images and compare similarities. \r\nHowever, sometimes ClientOSError occurs, so I wonder if it is possible to correct the cause on the code.\r\n`ClientOSErroraiohttp.streams in read`\r\n![image](https://user-images.githubusercontent.com/61039908/210923006-ad939b28-4513-4b3f-837d-560db3ed7e9b.png)\r\n\r\n\r\n### To reproduce\r\n\r\n[1] save_model.py\r\n```python\r\nweights = ResNet50_Weights.DEFAULT\r\nmodel = resnet50(weights=weights)\r\nmodel.eval()\r\n\r\nreturn_nodes = {'avgpool': 'avgpool'}  # extract feature to save it to variable\r\nnew_model = create_feature_extractor(model, return_nodes=return_nodes)\r\nbentoml.pytorch.save_model(\"model\", new_model)\r\n```\r\n\r\n[2] service.py\r\n```python\r\nrunner = bentoml.pytorch.get(\"model:latest\").to_runner()\r\nsvc = bentoml.Service(\"service\", runners=[runner])\r\n\r\n@svc.api(input=input_spec, output=output_spec)\r\nasync def predict(input_data: ImageModelFeatures) -> dict:\r\n    \"\"\"\r\n    input: {\"source_url\": \"...jpg\", \"target_url\": \"....jpg\"}\r\n    return: similarity\r\n    \"\"\"\r\n    ZERO_DISTANCE = 0.0\r\n    try:\r\n        source_img = url_to_processed_img(input_data.source_url)\r\n        source_embedding = await runner.async_run(source_img)\r\n        source_embedding = torch.flatten(source_embedding['avgpool'])\r\n        source_embedding = source_embedding.detach().numpy()\r\n\r\n        target_img = url_to_processed_img(input_data.source_url)\r\n        target_embedding = await runner.async_run(target_img)\r\n        target_embedding = torch.flatten(target_embedding['avgpool'])\r\n        target_embedding = target_embedding.detach().numpy()\r\n\r\n        distance = cos_sim(source_embedding, target_embedding)\r\n        logging.info(f\"[Predict] distance: {str(distance)}\")\r\n        await asyncio.sleep(0.001)\r\n        return {\"distance\": distance}\r\n    except (RuntimeError, UnidentifiedImageError) as e:\r\n        logging.error(f\"[Predict][RuntimeError] {str(e)}\", extra=dict(error=str(e)))\r\n        return {\"distance\": ZERO_DISTANCE}\r\n    except (ServerDisconnectedError, RemoteException) as e:\r\n        logging.error(f\"[Predict][DisconnectedError] {str(e)}\", extra=dict(error=str(e)))\r\n        return {\"distance\": ZERO_DISTANCE}\r\n```\r\n\r\n[3] save model and run\r\n```python\r\npython save_model.py\r\nbentoml serve service.py:svc\r\n```\r\n\r\n[4] Outout\r\nIt is working well, but sometimes return the error.\r\n\r\n\r\n### Expected behavior\r\n\r\nI wonder if the model and service code are well-organized to distribute to the production environment.\r\nAnd, I wonder what kind of defense code will be needed to prevent the error from occurring.\r\n\r\n### Environment\r\n\r\n[1] python: python3.9-slim-buster docker image\r\n[2] requirements.txt\r\n```text\r\nnumpy==1.23.*\r\nrequests==2.28.*\r\nbentoml==1.0.7\r\ntorch==1.12.*\r\ntorchvision==0.13.*\r\npydantic==1.10.*\r\nsentry-sdk==1.11.*",
    "comments": [
      {
        "user": "frostming",
        "body": "Hi, does it reproduce on the latest version of BentoML? you can use the new service APIs."
      }
    ]
  },
  {
    "issue_number": 3387,
    "title": "bug: RuntimeError: Found no NVIDIA driver on your system.",
    "author": "BEpresent",
    "state": "closed",
    "created_at": "2023-01-03T19:10:36Z",
    "updated_at": "2024-07-11T10:12:51Z",
    "labels": [
      "questions"
    ],
    "body": "### Describe the bug\n\nI'm not sure if this is actually a bug or an error from my side, so please excuse the latter. \r\n\r\nI am able to successfully build a bento that uses the gpu with no problems. However, containerizing it leads to the following error (it does not find the Nvidia GPU drivers):\r\n\r\n`RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx`\r\n\r\n\r\nDid I forget to specify more information to use nvidia drivers, may be in the `Dockerfile.template` ?  Note that it runs in a local miniconda environment. Could this be the issue ? \r\n\r\n\r\nHere is the `bentofile.yaml` :\r\n\r\n```\r\nservice: \"service:svc\"  # Same as the argument passed to `bentoml serve`\r\n#include:\r\n#- \"*.py\"  # A pattern for matching which files to include in the bento\r\nexclude:\r\n- \"examples/\"\r\n- \"*.png\"\r\n- \"*.gif\"\r\n- \"venv/\"\r\n- \"venv\"\r\ndocker:\r\n  distro: debian\r\n  dockerfile_template: ./Dockerfile.template\r\n  python_version: \"3.10.8\"\r\n  cuda_version: \"11.6.2\"\r\npython:\r\n   packages:  # Additional pip packages required by the service\r\n   - filelock\r\n   - Pillow\r\n   - torch\r\n   - fire\r\n   - humanize\r\n   - requests\r\n   - tqdm\r\n   - matplotlib\r\n   - scikit-image\r\n   - scipy\r\n   - numpy\r\n```\r\n\r\nThis is the Dockerfile.template\r\n\r\n\r\n```\r\n{% extends bento_base_template %}\r\n{% block SETUP_BENTO_COMPONENTS %}\r\n{{ super() }}\r\nRUN echo \"We are running this during bentoml containerize!\"\r\nRUN apt-get update && \\\r\n    apt-get upgrade -y && \\\r\n    apt-get install -y git\r\nRUN pip install git+https://github.com/openai/CLIP.git\r\nRUN echo \"CLIP installed!\"\r\n{% endblock %} \r\n```\r\n\r\n\r\n\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\n#### Environment variable\r\n\r\n```bash\r\nBENTOML_DEBUG=''\r\nBENTOML_QUIET=''\r\nBENTOML_BUNDLE_LOCAL_BUILD=''\r\nBENTOML_DO_NOT_TRACK=''\r\nBENTOML_CONFIG=''\r\nBENTOML_CONFIG_OPTIONS=''\r\nBENTOML_PORT=''\r\nBENTOML_HOST=''\r\nBENTOML_API_WORKERS=''\r\n```\r\n\r\n#### System information\r\n\r\n`bentoml`: 1.0.12\r\n`python`: 3.10.8\r\n`platform`: Linux-5.15.85-1-MANJARO-x86_64-with-glibc2.36\r\n`uid_gid`: 1000:1000\r\n`conda`: 22.9.0\r\n`in_conda_env`: True\r\n<details><summary><code>conda_packages</code></summary>\r\n\r\n<br>\r\n\r\n```yaml\r\nname: pointe\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=5.1=1_gnu\r\n  - bzip2=1.0.8=h7b6447c_0\r\n  - ca-certificates=2022.10.11=h06a4308_0\r\n  - certifi=2022.12.7=py310h06a4308_0\r\n  - ld_impl_linux-64=2.38=h1181459_1\r\n  - libffi=3.4.2=h6a678d5_6\r\n  - libgcc-ng=11.2.0=h1234567_1\r\n  - libgomp=11.2.0=h1234567_1\r\n  - libstdcxx-ng=11.2.0=h1234567_1\r\n  - libuuid=1.41.5=h5eee18b_0\r\n  - ncurses=6.3=h5eee18b_3\r\n  - openssl=1.1.1s=h7f8727e_0\r\n  - pip=22.3.1=py310h06a4308_0\r\n  - python=3.10.8=h7a1cb2a_1\r\n  - readline=8.2=h5eee18b_0\r\n  - setuptools=65.5.0=py310h06a4308_0\r\n  - sqlite=3.40.0=h5082296_0\r\n  - tk=8.6.12=h1ccaba5_0\r\n  - tzdata=2022g=h04d1e81_0\r\n  - wheel=0.37.1=pyhd3eb1b0_0\r\n  - xz=5.2.8=h5eee18b_0\r\n  - zlib=1.2.13=h5eee18b_0\r\n  - pip:\r\n    - aiohttp==3.8.3\r\n    - aiosignal==1.3.1\r\n    - anyio==3.6.2\r\n    - appdirs==1.4.4\r\n    - asgiref==3.6.0\r\n    - async-timeout==4.0.2\r\n    - attrs==22.2.0\r\n    - backoff==2.2.1\r\n    - bentoml==1.0.12\r\n    - build==0.9.0\r\n    - cattrs==22.2.0\r\n    - charset-normalizer==2.1.1\r\n    - circus==0.18.0\r\n    - click==8.1.3\r\n    - click-option-group==0.5.5\r\n    - clip==1.0\r\n    - cloudpickle==2.2.0\r\n    - commonmark==0.9.1\r\n    - contextlib2==21.6.0\r\n    - contourpy==1.0.6\r\n    - cycler==0.11.0\r\n    - deepmerge==1.1.0\r\n    - deprecated==1.2.13\r\n    - exceptiongroup==1.1.0\r\n    - filelock==3.9.0\r\n    - fire==0.5.0\r\n    - fonttools==4.38.0\r\n    - frozenlist==1.3.3\r\n    - fs==2.4.16\r\n    - ftfy==6.1.1\r\n    - googleapis-common-protos==1.57.0\r\n    - h11==0.14.0\r\n    - humanize==4.4.0\r\n    - idna==3.4\r\n    - imageio==2.23.0\r\n    - jinja2==3.1.2\r\n    - kiwisolver==1.4.4\r\n    - markupsafe==2.1.1\r\n    - matplotlib==3.6.2\r\n    - multidict==6.0.4\r\n    - networkx==2.8.8\r\n    - numpy==1.24.1\r\n    - nvidia-cublas-cu11==11.10.3.66\r\n    - nvidia-cuda-nvrtc-cu11==11.7.99\r\n    - nvidia-cuda-runtime-cu11==11.7.99\r\n    - nvidia-cudnn-cu11==8.5.0.96\r\n    - opentelemetry-api==1.14.0\r\n    - opentelemetry-exporter-otlp-proto-http==1.14.0\r\n    - opentelemetry-instrumentation==0.35b0\r\n    - opentelemetry-instrumentation-aiohttp-client==0.35b0\r\n    - opentelemetry-instrumentation-asgi==0.35b0\r\n    - opentelemetry-proto==1.14.0\r\n    - opentelemetry-sdk==1.14.0\r\n    - opentelemetry-semantic-conventions==0.35b0\r\n    - opentelemetry-util-http==0.35b0\r\n    - packaging==21.3\r\n    - pathspec==0.10.3\r\n    - pep517==0.13.0\r\n    - pillow==9.4.0\r\n    - pip-requirements-parser==32.0.1\r\n    - pip-tools==6.12.1\r\n    - prometheus-client==0.15.0\r\n    - protobuf==3.20.3\r\n    - psutil==5.9.4\r\n    - pygments==2.14.0\r\n    - pynvml==11.4.1\r\n    - pyparsing==3.0.9\r\n    - python-dateutil==2.8.2\r\n    - python-json-logger==2.0.4\r\n    - python-multipart==0.0.5\r\n    - pywavelets==1.4.1\r\n    - pyyaml==6.0\r\n    - pyzmq==24.0.1\r\n    - regex==2022.10.31\r\n    - requests==2.28.1\r\n    - rich==13.0.0\r\n    - schema==0.7.5\r\n    - scikit-image==0.19.3\r\n    - scipy==1.9.3\r\n    - simple-di==0.1.5\r\n    - six==1.16.0\r\n    - sniffio==1.3.0\r\n    - starlette==0.23.1\r\n    - termcolor==2.1.1\r\n    - tifffile==2022.10.10\r\n    - tomli==2.0.1\r\n    - torch==1.13.1\r\n    - torchvision==0.14.1\r\n    - tornado==6.2\r\n    - tqdm==4.64.1\r\n    - typing-extensions==4.4.0\r\n    - urllib3==1.26.13\r\n    - uvicorn==0.20.0\r\n    - watchfiles==0.18.1\r\n    - wcwidth==0.2.5\r\n    - wrapt==1.14.1\r\n    - yarl==1.8.2\r\nprefix: /home/be/miniconda3/envs/pointe\r\n```\r\n\r\n</details>\r\n\r\n<details><summary><code>pip_packages</code></summary>\r\n\r\n<br>\r\n\r\n```\r\naiohttp==3.8.3\r\naiosignal==1.3.1\r\nanyio==3.6.2\r\nappdirs==1.4.4\r\nasgiref==3.6.0\r\nasync-timeout==4.0.2\r\nattrs==22.2.0\r\nbackoff==2.2.1\r\nbentoml==1.0.12\r\nbuild==0.9.0\r\ncattrs==22.2.0\r\ncertifi @ file:///croot/certifi_1671487769961/work/certifi\r\ncharset-normalizer==2.1.1\r\ncircus==0.18.0\r\nclick==8.1.3\r\nclick-option-group==0.5.5\r\nclip @ git+https://github.com/openai/CLIP.git@d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\r\ncloudpickle==2.2.0\r\ncommonmark==0.9.1\r\ncontextlib2==21.6.0\r\ncontourpy==1.0.6\r\ncycler==0.11.0\r\ndeepmerge==1.1.0\r\nDeprecated==1.2.13\r\nexceptiongroup==1.1.0\r\nfilelock==3.9.0\r\nfire==0.5.0\r\nfonttools==4.38.0\r\nfrozenlist==1.3.3\r\nfs==2.4.16\r\nftfy==6.1.1\r\ngoogleapis-common-protos==1.57.0\r\nh11==0.14.0\r\nhumanize==4.4.0\r\nidna==3.4\r\nimageio==2.23.0\r\nJinja2==3.1.2\r\nkiwisolver==1.4.4\r\nMarkupSafe==2.1.1\r\nmatplotlib==3.6.2\r\nmultidict==6.0.4\r\nnetworkx==2.8.8\r\nnumpy==1.24.1\r\nnvidia-cublas-cu11==11.10.3.66\r\nnvidia-cuda-nvrtc-cu11==11.7.99\r\nnvidia-cuda-runtime-cu11==11.7.99\r\nnvidia-cudnn-cu11==8.5.0.96\r\nopentelemetry-api==1.14.0\r\nopentelemetry-exporter-otlp-proto-http==1.14.0\r\nopentelemetry-instrumentation==0.35b0\r\nopentelemetry-instrumentation-aiohttp-client==0.35b0\r\nopentelemetry-instrumentation-asgi==0.35b0\r\nopentelemetry-proto==1.14.0\r\nopentelemetry-sdk==1.14.0\r\nopentelemetry-semantic-conventions==0.35b0\r\nopentelemetry-util-http==0.35b0\r\npackaging==21.3\r\npathspec==0.10.3\r\npep517==0.13.0\r\nPillow==9.4.0\r\npip-requirements-parser==32.0.1\r\npip-tools==6.12.1\r\n-e git+https://github.com/openai/point-e.git@fc8a607c08a3ea804cc82bf1ef8628f88a3a5d2f#egg=point_e\r\nprometheus-client==0.15.0\r\nprotobuf==3.20.3\r\npsutil==5.9.4\r\nPygments==2.14.0\r\npynvml==11.4.1\r\npyparsing==3.0.9\r\npython-dateutil==2.8.2\r\npython-json-logger==2.0.4\r\npython-multipart==0.0.5\r\nPyWavelets==1.4.1\r\nPyYAML==6.0\r\npyzmq==24.0.1\r\nregex==2022.10.31\r\nrequests==2.28.1\r\nrich==13.0.0\r\nschema==0.7.5\r\nscikit-image==0.19.3\r\nscipy==1.9.3\r\nsimple-di==0.1.5\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nstarlette==0.23.1\r\ntermcolor==2.1.1\r\ntifffile==2022.10.10\r\ntomli==2.0.1\r\ntorch==1.13.1\r\ntorchvision==0.14.1\r\ntornado==6.2\r\ntqdm==4.64.1\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.13\r\nuvicorn==0.20.0\r\nwatchfiles==0.18.1\r\nwcwidth==0.2.5\r\nwrapt==1.14.1\r\nyarl==1.8.2\r\n```\r\n\r\n</details>\r\n",
    "comments": [
      {
        "user": "sauyon",
        "body": "Hiya! I think you need to use the CUDA base image or install CUDA manually."
      },
      {
        "user": "BEpresent",
        "body": "Thanks! So I tried to specify a CUDA version (  `cuda_version: \"11.6.2\"`) in the Dockerfile.template , however it still shows the same error. \r\n\r\nIs there another way to force a CUDA base image ? \r\n\r\n"
      },
      {
        "user": "aarnphm",
        "body": "Hi there, after choosing cuda base image make sure to setup https://github.com/NVIDIA/nvidia-docker so that you can run it with GPU for container."
      }
    ]
  },
  {
    "issue_number": 3377,
    "title": "bug: transformers save_model and load_model tasks conflict",
    "author": "aarnphm",
    "state": "open",
    "created_at": "2022-12-22T01:10:33Z",
    "updated_at": "2024-07-11T02:55:02Z",
    "labels": [
      "bug",
      "framework"
    ],
    "body": "### Describe the bug\n\nTransformers `save_model` currently check for task_name and task_definition to be not None to pickle custom pipelines.\r\n\r\nThis behaviour should be consistent with load_model\n\n### To reproduce\n\nSee bentoml/_internal/frameworks/transformers.py#load_model,save_model\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nna",
    "comments": []
  },
  {
    "issue_number": 3250,
    "title": "feat(grpc): specify protocol version for serving",
    "author": "aarnphm",
    "state": "open",
    "created_at": "2022-11-16T01:42:18Z",
    "updated_at": "2024-07-11T02:52:37Z",
    "labels": [
      "enhancement"
    ],
    "body": "allow user to fallback to `v1alpha1` with\r\n```bash\r\nbentoml serve-grpc --production --protocol-version v1alpha1\r\n```\r\n",
    "comments": []
  },
  {
    "issue_number": 3055,
    "title": "feature: CTranslated2 Framework + Adaptive-batching for custom runner",
    "author": "Matthieu-Tinycoaching",
    "state": "closed",
    "created_at": "2022-09-30T07:16:36Z",
    "updated_at": "2024-07-11T02:52:07Z",
    "labels": [],
    "body": "### Feature request\n\nHi,\r\n\r\nIt would be nice to enable CTranslate2 inference within bentoML (https://github.com/OpenNMT/CTranslate2). This library implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to [accelerate and reduce the memory usage](https://github.com/OpenNMT/CTranslate2#benchmarks) of Transformer models on CPU and GPU.\r\n\r\nFor example, for MarianMT transformer model the following code is used (https://opennmt.net/CTranslate2/guides/transformers.html#marianmt):\r\n\r\n`ct2-transformers-converter --model Helsinki-NLP/opus-mt-en-de --output_dir opus-mt-en-de`\r\n\r\n```\r\nimport ctranslate2\r\nimport transformers\r\n\r\ntranslator = ctranslate2.Translator(\"opus-mt-en-de\")\r\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\r\n\r\nsource = tokenizer.convert_ids_to_tokens(tokenizer.encode(\"Hello world!\"))\r\nresults = translator.translate_batch([source])\r\ntarget = results[0].hypotheses[0]\r\n\r\nprint(tokenizer.decode(tokenizer.convert_tokens_to_ids(target)))\r\n```\r\n\r\nMaybe this already possible with custom runner?\n\n### Motivation\n\nCTranslate2 is a C++ and Python library for efficient inference with Transformer models\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "Matthieu-Tinycoaching",
        "body": "Hi,\r\n\r\nI managed to make a bentoml service worked on my local machine with custom runner indeed.\r\n\r\nHowever, how could I use adaptive-batching since I cannot enable batching for the target model signature during `save_model`, which doesn't exist for this framework.... \r\n\r\nThe converted model consist of a directory with 2 files :\r\n\r\n- model.bin\r\n- shared_vocabulary.txt\r\n\r\nThe tokenizer is used from a call to `transformers` `AutoTokenizer.from_pretrained()` method\r\n\r\nI tried to specify `@bentoml.Runnable.method(batchable=True)` in the custom runner class but this didn't work. Is there any way to activate adaptive-batching for custom runner?"
      },
      {
        "user": "frostming",
        "body": "This is possible, and you can use the new service APIs to make a BentoML service, read the latest docs for how to do it."
      }
    ]
  },
  {
    "issue_number": 2731,
    "title": "Better error messages",
    "author": "sauyon",
    "state": "closed",
    "created_at": "2022-07-12T20:29:35Z",
    "updated_at": "2024-07-11T02:44:57Z",
    "labels": [
      "documentation"
    ],
    "body": "A large tracking issue for things that should probably have specific error messages and suggested fixes:\r\n - [ ] Passing a non-supported type to / from runners via DataContainer\r\n - [ ] Running a normal `run` method instead of `async_run` in an async function",
    "comments": []
  },
  {
    "issue_number": 2790,
    "title": "docs: Service lifecycle event customization",
    "author": "ssheng",
    "state": "closed",
    "created_at": "2022-07-22T22:19:06Z",
    "updated_at": "2024-07-11T02:44:37Z",
    "labels": [
      "documentation"
    ],
    "body": "- Health checking (https://github.com/bentoml/BentoML/issues/2630)\r\n- Start/stop hooks",
    "comments": []
  },
  {
    "issue_number": 3304,
    "title": "feature: custom objects/models accepting serialization functions instead of relying on cloudpickle",
    "author": "cadmusthefounder",
    "state": "open",
    "created_at": "2022-12-03T07:49:29Z",
    "updated_at": "2024-07-11T02:44:16Z",
    "labels": [
      "questions"
    ],
    "body": "### Feature request\n\nIn the docs, it is mentioned that for custom objects like tokenizer, the object is being pickled\r\n\r\n```\r\nbentoml.pytorch.save_model(\r\n    \"demo_mnist\",   # model name in the local model store\r\n    trained_model,  # model instance being saved\r\n    labels={    # user-defined labels for managing models in Yatai\r\n        \"owner\": \"nlp_team\",\r\n        \"stage\": \"dev\",\r\n    },\r\n    metadata={  # user-defined additional metadata\r\n        \"acc\": acc,\r\n        \"cv_stats\": cv_stats,\r\n        \"dataset_version\": \"20210820\",\r\n    },\r\n    custom_objects={    # save additional user-defined python objects\r\n        \"tokenizer\": tokenizer_object,\r\n    }\r\n)\r\n```\r\n\r\nand it is similar for custom models. This may not be desirable due to the explanation in the next section. Suggestion would be to allow the saving of other file types in addition to the model file, as well as allowing users to define their own functions to convert those files into python objects.\n\n### Motivation\n\nMain problem with `pickle` is that things may break when the python version changes. To maintain backwards compatibility, users should be able to define the serialization and deserialization logic, as well as handling other file types.\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "SebastianScherer88",
        "body": "related suggestion: extend the way those extra files appear in the resulting model store entry to something more along the lines of the `bento` builds' `include` argument. i.e. instead of having an intransparent `custom_objects.pkl` pickle file as part of the exported _model_, it would be nice to have:\r\n- the ability to pass files, not just python objects\r\n- the resulting object files saved into a flat directory structure, much like what is already happening at the `bento` build phase.\r\n\r\nthis fits in with the original request in that the huggingface tokenizers's native `save` method exports a collection of files to a specified directory. the tokenizer's `from_pretrained` method can then be pointed at the same directory, reading in whats needed and discarding non-tokenizer related files that are also placed in that directory.\r\n\r\nmy team cant use the `Transformer` framework as we require our models to be in the neuron torchscript format, which the huggingface `pipeline` currently doesnt support afaik.\r\n\r\neven without the neuron torchscript motivation this would be a nice extension of the current model export functionality, making the storage of the pre-/postprocessing artifacts in the bento model store a bit more transparent imo :)"
      },
      {
        "user": "frostming",
        "body": "We recommend saving these objects as separate bentoml models, and load them together into the service when it starts up."
      }
    ]
  },
  {
    "issue_number": 3299,
    "title": "defined python object  lost  after certain duration",
    "author": "MAYUR192",
    "state": "closed",
    "created_at": "2022-12-02T08:02:54Z",
    "updated_at": "2024-07-11T02:41:45Z",
    "labels": [],
    "body": "Hello BentoMl Team\nI am trying to use BentoMl without a runner.\nI have configured 2 APIs  load model and predict.\nLoad model api will take model path as input and loads model into memory.\n\nIssue:-\nThis configuration works fine and prediction also works. But after certain duration, loaded model object initialised to default value that is defined at start.\nIn my case at the start of code i defined the model object to None so it assigns that value after a certain duration.\n\nAttached screenshot of sample code.\n\nConfiguration:-\nBentoMl version:- 1.0.5\nEnv:- production\nApi workers :- 1\n\nCommand to start server:-\n\nbentoml serve \" service_filename\":svc --production --api-workers 1\n\n[Slack Message](https://bentoml.slack.com/archives/CKRANBHPH/p1669875219783629?thread_ts=1669875219.783629&cid=CKRANBHPH)",
    "comments": []
  },
  {
    "issue_number": 3293,
    "title": "feature: test with local project",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-11-30T00:49:36Z",
    "updated_at": "2024-07-11T02:40:11Z",
    "labels": [],
    "body": "### Feature request\n\nto run `bentoml test` with local bento project directory.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3277,
    "title": "feature: support positional kwargs assignment",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-11-25T01:24:50Z",
    "updated_at": "2024-07-11T02:39:48Z",
    "labels": [
      "good-first-issue",
      "enhancement",
      "io-descriptor"
    ],
    "body": "### Feature request\n\nClient should also support assigning positional args with kwargs (this is also a Python behaviour):\r\n\r\nLet say that \r\n```python\r\n@svc.api(\r\n    input=NumpyNdarray.from_sample(np.array([[4.9, 3.0, 1.4, 0.2]], dtype=np.double)),\r\n    output=NumpyNdarray(),\r\n)\r\nasync def classify(input_series: np.ndarray) -> np.ndarray:\r\n    return await iris_clf_runner.predict.async_run(input_series)\r\n```\r\n\r\nThen the below should also be valid\r\n```python\r\nclient = Client.from_url(\"localhost:3000\")\r\nclient.classify(input_series=np.array(...))\r\n```\r\n\r\nRight now this will raise an error due to the actual input being assigned via `inp` (in https://github.com/bentoml/BentoML/blob/4e771b6a458eef182815c6f7797606841aca5244/src/bentoml/client.py#L62)\n\n### Motivation\n\nThis can be done. We can save the given signature of the decorated function to `InferenceAPI`.\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "Srivathsan-V",
        "body": "Hi!. I would like to work on this. Could you provide some insights about it?"
      },
      {
        "user": "y1450",
        "body": "Hi @Srivathsan-V , if you are not working on it can I take it over?"
      },
      {
        "user": "aarnphm",
        "body": "@y1450 feel free to take this on."
      }
    ]
  },
  {
    "issue_number": 3249,
    "title": "feature: bazel code coverage",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-11-15T23:11:17Z",
    "updated_at": "2024-07-11T02:38:36Z",
    "labels": [],
    "body": "### Feature request\r\n\r\nBazel does provide coverage, but it didn't work well with Python. We probably have to apply a patch to `py_library`.",
    "comments": []
  },
  {
    "issue_number": 3246,
    "title": "feature: bazel cli run",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-11-14T23:45:40Z",
    "updated_at": "2024-07-11T02:38:35Z",
    "labels": [],
    "body": "### Feature request\n\nwe can run bentoml cli from bazel\r\n\r\nThis will further reduce the setup time and developer can easily run bentoml without having the need to setup Python.\r\n\r\n```bash\r\nbazel run //:cli -- serve --production\r\n```\r\n\r\nAll operations except `serve` and `containerize` already work with bazel.\n\n### Motivation\n\nbetter devex\n\n### Other\n\nIn conjunction to #3074 ",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Note that we can also use bazel rules from tensorflow, jax, and pytorch directly"
      }
    ]
  },
  {
    "issue_number": 3230,
    "title": "feature: Support pandera.SchemaModel in bentoml.io.PandasDataFrame",
    "author": "cosmicBboy",
    "state": "open",
    "created_at": "2022-11-12T16:05:37Z",
    "updated_at": "2024-07-11T02:37:30Z",
    "labels": [
      "enhancement",
      "io-descriptor"
    ],
    "body": "### Feature request\n\nJust like a `pydantic_model` can be specified in the `bentoml.io.JSON` descriptor, providing an option to specify a `pandera.SchemaModel` in the `PandasDataFrame` descriptor would enable users to validate incoming/outgoing dataframes in their service apis with more complex, potentially custom validation checks defined in pandera SchemaModels.\r\n\r\nSomething like:\r\n\r\n```python\r\nimport pandera as pa\r\nfrom pandera.typing import Series\r\n\r\nclass Features(pa.SchemaModel):\r\n    feature1: Series[int] = pa.Field(gt=0)\r\n    feature2: Series[str] = pa.Field(isin=[\"A\", \"B\", \"C\"])\r\n    feature3: Series[float] = pa.Field(in_range={\"min_value\": -1000, \"max_value\": 1000})\r\n\r\n    @pa.check(\"feature3\")\r\n    def custom_check(cls, series):\r\n        return -10 <= series.mean() <= 10\r\n\r\n    class Config:\r\n        coerce = True # coerce dtypes automatically\r\n\r\n\r\n@svc.api(\r\n    input=PandasDataFrame(\r\n        pandera_model=Features,\r\n        enforce_dtype=True),\r\n    output=PandasDataFrame()\r\n)\r\ndef predict(input_df: pd.DataFrame) -> pd.DataFrame:\r\n    ...\r\n```\n\n### Motivation\n\nThe current `PandasDataFrame` io descriptor allows for `dtype` enforcement but it's up to the user to implement other statistical validation checks, like range values, allowable values, potentially more complex checks that can be expressed by pandera (see [here](https://pandera.readthedocs.io/en/stable/reference/generated/pandera.checks.Check.html#pandera.checks.Check) and [here](https://pandera.readthedocs.io/en/stable/hypothesis.html).\r\n\r\nBy supporting this feature, bentoml api services would be able to automatically validate these statistical properties.\n\n### Other\n\nI'm the author of pandera 👋 and love the bentoml project!",
    "comments": [
      {
        "user": "tkaraouzene",
        "body": "Hi ! \r\nAny update on this issue ? \r\nI'm using pandera to validate dataframes.\r\nI would like to deploy services with bentoml.\r\nIt would be wonderful to be able to validate dataframe schema with pandera.\r\nIt is a bit linked with an issue I've opened: https://github.com/bentoml/BentoML/issues/3652\r\nSince it would at least allow to have fine validation and exceptions for inputs"
      }
    ]
  },
  {
    "issue_number": 3224,
    "title": "feature: allow saving framework dependencies alongside with `save_model`",
    "author": "aarnphm",
    "state": "open",
    "created_at": "2022-11-10T05:49:29Z",
    "updated_at": "2024-07-11T02:36:30Z",
    "labels": [
      "enhancement",
      "framework"
    ],
    "body": "### Feature request\n\n`save_model` should have the ability to add model dependencies (esp library deps)\r\n\r\n`bentoml.build` then can automatically include this into the requirements.txt\n\n### Motivation\n\nbetter qol\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3207,
    "title": "feature: make it possible to set custom path for folder `prometheus_multiproc_dir`",
    "author": "ALee008",
    "state": "open",
    "created_at": "2022-11-09T16:18:59Z",
    "updated_at": "2024-07-11T02:36:07Z",
    "labels": [
      "fixed-in-main",
      "enhancement"
    ],
    "body": "### Feature request\n\nMake it possible to set the path of `prometheus_multiproc_dir` using the environment variable `PROMETHEUS_MULTIPROC_DIR`.\n\n### Motivation\n\nOS: SLES 15\r\nPython: 3.8.13\r\nBentoML version: 1.0.5\r\n\r\nOur BentoML service is running with user `A` but the bento contents belong to user `B`. The idea is to separate running a service from the actual code. When running in production mode the folder `prometheus_multiproc_dir` is created in `BENTOML_HOME` path, which belongs to user `B`. But this fails because user `A` does not have write access in `B`'s folders.\r\nWhile reading through the BentoML code we found an environment variable, `PROMETHEUS_MULTIPROC_DIR` and tried setting it. Unfortunately without success. The service would still try to write to `BENTOML_HOME/prometheus_multiproc_dir`.\r\nThe relevant part seems to be [here](https://github.com/bentoml/BentoML/blob/99040e4ecd2262bd4ce11ae867cb63e1a2c36cee/src/bentoml/_internal/configuration/containers.py#L469) where `bentoml` is used to create `prometheus_multiproc_dir` path.\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3232,
    "title": "feature: Explainable AI supports for BentoML",
    "author": "yangwenz",
    "state": "closed",
    "created_at": "2022-11-14T09:36:49Z",
    "updated_at": "2024-07-11T02:29:24Z",
    "labels": [
      "enhancement",
      "framework"
    ],
    "body": "### Feature request\n\nWe have developed an open source project OmniXAI for explainable AI: https://github.com/salesforce/OmniXAI, and implemented an interface for deploying OmniXAI explainers using BentoML: https://github.com/salesforce/OmniXAI/blob/main/omnixai/deployment/bentoml/omnixai.py. Is it possible to integrate it directly in BentoML?\r\n\r\nWe are happy to provide more supports for it to extend BentoML's capabilities. \n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "We will not maintain this framework as a build-in support, instead, community can support is as an extension. Thanks"
      }
    ]
  },
  {
    "issue_number": 3235,
    "title": "bug: \"bentoml build\" command takes too long time but there is no additional message with \"--verbose\" tag.",
    "author": "seyong92",
    "state": "closed",
    "created_at": "2022-11-14T13:50:48Z",
    "updated_at": "2024-07-11T02:28:29Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nIn \"bentofile.yaml\", we only include \"*.py\" file and some text files, but it takes a too long time to build bento.\r\n\r\nIt takes more than 1 hour for building bentos, and the created bento is just around 50 MB.\r\n\r\nWhen I use \"--verbose\" tag when using \"bento build\", it does not return anything, so I cannot know the reason.\n\n### To reproduce\n\nI just share the \"bentofile.yaml\" file of my project.\r\n\r\n```yaml\r\nservice: \"bentoml_service:svc\"\r\nlabels:\r\n  owner: our-team\r\n  stage: dev\r\ninclude:\r\n  - \"*.py\"\r\n  - \"*.csv\"\r\nexclude:\r\n  - \"log_save/\"\r\n  - \"model_save/\"\r\npython:\r\n  requirements_txt: \"./requirements.txt\"\r\ndocker:\r\n  distro: debian\r\n  python_version: \"3.9.13\"\r\n  cuda_version: \"11.6.2\"\r\n  system_packages:\r\n    - libsndfile-dev\r\n    - ffmpeg\r\n```\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: 1.0.10\r\npython: 3.9.13",
    "comments": [
      {
        "user": "knoll-fabio",
        "body": "I have the same problem, `bentoml build` just takes too long and it is hard to debug this. My build ctx is the project root and it contains many directories with a lot of files. I did exclude these in bentofile.yml but maybe this is the problem."
      },
      {
        "user": "aarnphm",
        "body": "Hi there, is there a virtualenv folder under this build directory?"
      },
      {
        "user": "knoll-fabio",
        "body": "Hi, yes there is. I looked at the code and noticed that for each file to be included, the entire directory structure from the build context is traversed to look for and parse `.bentoignore` files, which is very time consuming if the build directory contains a lot of files and directories. Wouldn't it make sense here to only step through the included directories?"
      }
    ]
  },
  {
    "issue_number": 3193,
    "title": "feature: Support For Microsoft FLAML",
    "author": "VikramxD",
    "state": "closed",
    "created_at": "2022-11-06T11:08:06Z",
    "updated_at": "2024-07-11T02:22:26Z",
    "labels": [
      "enhancement",
      "framework"
    ],
    "body": "### Feature request\n\nFeature request\r\nSupport the ML framework  Microsoft Flaml [microsoft.github.io/FLAML/](https://microsoft.github.io/FLAML/)\r\n\r\n\r\n\n\n### Motivation\n\nMotivation\r\nMicrosoft FLAML is a popular AutoML library currently there is no native support present for it in Bentoml , we have to use the models for inference by fitting it into a sklearn pipeline , if native support for Flaml objects can be added that would be helpful\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "frostming",
        "body": "We will not maintain this framework as a build-in support, instead, community can support is as an extension. Thanks"
      }
    ]
  },
  {
    "issue_number": 3190,
    "title": "feat: medical image IO descriptor support",
    "author": "ssheng",
    "state": "closed",
    "created_at": "2022-11-04T18:45:15Z",
    "updated_at": "2024-07-11T02:11:52Z",
    "labels": [],
    "body": "https://imageio.readthedocs.io/en/v2.8.0/format_dicom.html\nhttps://openslide.org/api/python/",
    "comments": [
      {
        "user": "frostming",
        "body": "Duplicate with #2805 "
      }
    ]
  },
  {
    "issue_number": 3159,
    "title": "test: improve general workflow",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-10-28T20:19:28Z",
    "updated_at": "2024-07-11T02:11:04Z",
    "labels": [
      "bug"
    ],
    "body": "Currently, unit tests are very flaky and hard to run locally.",
    "comments": []
  },
  {
    "issue_number": 3158,
    "title": "race in batch dispatcher",
    "author": "sauyon",
    "state": "closed",
    "created_at": "2022-10-28T19:25:16Z",
    "updated_at": "2024-07-11T02:10:52Z",
    "labels": [],
    "body": "Currently, with the way dispatcher is written, there is a minor race where multiple training requests of the same type may be made.\r\n\r\nNote that the worst that happens is that the three training requests all have batch size 1, which may occur anyway if there aren't enough requests when the runner is starting up, so this is a very minor issue.",
    "comments": []
  },
  {
    "issue_number": 3146,
    "title": "bug: failed to containerize when using mlflow",
    "author": "SohelKabir",
    "state": "closed",
    "created_at": "2022-10-26T16:50:00Z",
    "updated_at": "2024-07-11T02:10:21Z",
    "labels": [
      "bug"
    ],
    "body": "Trying to integrate Mlflow with my current bentoml workflow and following this example\r\n`https://github.com/bentoml/BentoML/tree/main/examples/mlflow/pytorch`\r\nBut getting error when i try to deploy model with docker, \r\n\r\nwhen i run  `bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\n```Building docker image for Bento(tag=\"mlflow_pytorch_mnist_demo:3utxjn2vbgxh5gbc\")...\r\nERROR: failed to solve: executor failed running [/bin/sh -c bash <<EOF\r\nset -euxo pipefail\r\n\r\nif [ -f /home/bentoml/bento/env/conda/environment.yml ]; then\r\n   set pip_interop_enabled to improve conda-pip interoperability. Conda can use\r\n   pip-installed packages to satisfy dependencies.\r\n  echo \"Updating conda base environment with environment.yml\"\r\n  /opt/conda/bin/conda config --set pip_interop_enabled True\r\n  /opt/conda/bin/conda env update -n base -f /home/bentoml/bento/env/conda/environment.yml\r\n  /opt/conda/bin/conda clean --all\r\nfi\r\nEOF]: exit code: 1\r\nFailed building docker image: Command '['docker', 'buildx', 'build', '--progress', 'auto', '--tag', 'mlflow_pytfile', 'env\\\\docker\\\\Dockerfile', '--load', '.']' returned non-zero exit status 1.\r\n```\r\n### To reproduce\r\n\r\nBug recreation steps:\r\nClone the repo `https://github.com/bentoml/BentoML/tree/main/examples/mlflow/pytorch` \r\nGoto the folder `examples/mlflow/pytorch`\r\n`python mnist.py`\r\n`bentoml build`\r\n`bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\nP.S. `bentoml serve service.py:svc`  works fine`\r\n\r\n\r\n### Environment\r\n\r\nbentoml version 1.0.7\r\nPython version  3.9.12\r\nDocker Engine 20.10.17",
    "comments": [
      {
        "user": "aarnphm",
        "body": "Hi there, can you send the full error output?"
      },
      {
        "user": "SohelKabir",
        "body": "Full error given already, that was all @aarnphm "
      },
      {
        "user": "SohelKabir",
        "body": "![image](https://user-images.githubusercontent.com/33397287/198088425-81fdd1dd-2187-4e83-a05d-7a376108888b.png)\r\n"
      }
    ]
  },
  {
    "issue_number": 3145,
    "title": "bug: Given model is not a onnx.ModelProto.",
    "author": "sarmientoj24",
    "state": "open",
    "created_at": "2022-10-26T07:51:13Z",
    "updated_at": "2024-07-11T02:05:26Z",
    "labels": [
      "bug",
      "feedback-wanted",
      "framework"
    ],
    "body": "### Describe the bug\n\nReceiving this error that my model is not an onnx.ModelProto\r\n\r\n```\r\n❯ bentoml serve image_classification_onnx:svc --reload\r\n/home/user/workspace/ws/models/test_image_classification.onnx\r\n2022-10-26T15:42:39+0800 [INFO] [cli] Using the default model signature for onnx ({'run': {'batchable': False}}) for model image_classification_onnx.\r\nTraceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/yolov5/bin/bentoml\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/click/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/click/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/click/core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/click/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/click/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/bentoml_cli/utils.py\", line 256, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/bentoml_cli/utils.py\", line 230, in wrapper\r\n    return_value = func(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/bentoml_cli/utils.py\", line 191, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/bentoml_cli/serve.py\", line 217, in serve\r\n    ssl_ciphers=ssl_ciphers,\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/bentoml/serve.py\", line 192, in serve_http_development\r\n    svc = load(bento_identifier, working_dir=working_dir)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/bentoml/_internal/service/loader.py\", line 351, in load\r\n    standalone_load=standalone_load,\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/simple_di/__init__.py\", line 139, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/bentoml/_internal/service/loader.py\", line 137, in import_service\r\n    module = importlib.import_module(module_name, package=working_dir)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/user/workspace/ws/scripts/deploy/image_classification_onnx.py\", line 32, in <module>\r\n    bentoml.onnx.save_model(f\"{BENTO_MODEL}\", model_path)\r\n  File \"/home/user/anaconda3/envs/yolov5/lib/python3.7/site-packages/bentoml/_internal/frameworks/onnx.py\", line 299, in save_model\r\n    raise TypeError(f\"Given model ({model}) is not a onnx.ModelProto.\")\r\nTypeError: Given model (/home/user/workspace/ws/models/test_image_classification.onnx) is not a onnx.ModelProto.\r\n```\r\n\r\nNetron app\r\n![image](https://user-images.githubusercontent.com/8830319/197966722-e325fead-f6e5-4e0a-98ea-d2b4f16877a8.png)\r\n\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: 1.0.7\r\nenvironment: Ubuntu 20.04",
    "comments": [
      {
        "user": "sarmientoj24",
        "body": "![image](https://user-images.githubusercontent.com/8830319/197969052-84d60726-952c-4720-ac5b-d37c1ced740c.png)\r\n"
      },
      {
        "user": "frostming",
        "body": "> bentoml.onnx.save_model(f\"{BENTO_MODEL}\", model_path)\r\n\r\nSo what is a `model_path`? From the naming it looks like a path, then it is incorrect."
      }
    ]
  },
  {
    "issue_number": 3206,
    "title": "test: BentoBuildConfig",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-11-09T14:40:47Z",
    "updated_at": "2024-07-11T01:29:12Z",
    "labels": [
      "good-first-issue"
    ],
    "body": "Right now we don't have a test suite for our `bentofile.yaml`.\r\n\r\nUnit test would be good. Hypothesis suite will probably be the best case scenario.",
    "comments": []
  },
  {
    "issue_number": 3141,
    "title": "No better performance for concurring requests using batching",
    "author": "katerinafrid",
    "state": "open",
    "created_at": "2022-10-25T19:15:53Z",
    "updated_at": "2024-07-11T01:28:41Z",
    "labels": [
      "bug",
      "feedback-wanted"
    ],
    "body": "### Discussed in https://github.com/bentoml/BentoML/discussions/3137\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **katerinafrid** October 24, 2022</sup>\r\nI'm trying to speed up the processing of requests to my pytorch model, but I do not see any improvements compared to the standard sequential processing. Am I doing something wrong? \r\nFirst, I execute utils.py file, then I start a server by running server.py.\r\n\r\nutils.py\r\n```python\r\nclass NLUPipeline(transformers.Pipeline):\r\n    def preprocess(self, inputs):\r\n        return self.tokenizer(inputs['premise_list'], inputs['hypothesis_list'], truncation='only_first',\r\n                              max_length=128, padding=True, return_tensors='pt').to(self.device)\r\n\r\n    def _sanitize_parameters(self, **kwargs):\r\n        return {}, {}, {}\r\n\r\n    def _forward(self, model_inputs):\r\n        model_out = self.model(**model_inputs)\r\n        return model_out.logits\r\n\r\n    @classmethod\r\n    def postprocess(cls, model_outputs):\r\n        return model_outputs\r\n\r\n\r\nclass NLURunnable(bentoml.Runnable):\r\n    SUPPORT_NVIDIA_GPU = True\r\n    SUPPORTED_RESOURCES = ('cuda' if torch.cuda.is_available() else 'cpu',)\r\n    SUPPORTS_CPU_MULTI_THREADING = True\r\n\r\n    def __init__(self):\r\n        # load the model instance\r\n        self.nlu_model = bentoml.transformers.load_model(\"nlu_pl:latest\",  device=0 if torch.cuda.is_available() else -1)\r\n\r\n    @bentoml.Runnable.method(batchable=True, batch_dim=0)\r\n    def predict(self, input_data):\r\n        return self.nlu_model(input_data)\r\n\r\n\r\n\r\n\r\n\r\ndef register_nlu_pipeline():\r\n    TASK_NAME = \"zero-shot-classification\"\r\n    TASK_DEFINITION = {\r\n        \"impl\": NLUPipeline,\r\n        \"tf\": (),\r\n        \"pt\": (transformers.AutoModelForSequenceClassification,),\r\n        \"default\": {},\r\n        \"type\": \"text\",\r\n    }\r\n    SUPPORTED_TASKS[TASK_NAME] = TASK_DEFINITION\r\n\r\ndef create_nlu_pipeline(nlu_model_path: str = default_nlu_model_path):\r\n    classifier = transformers.pipeline(\r\n        task=\"zero-shot-classification\",\r\n        model=transformers.AutoModelForSequenceClassification.from_pretrained(\r\n            nlu_model_path\r\n        ),\r\n        tokenizer=transformers.AutoTokenizer.from_pretrained(\r\n            nlu_model_path\r\n        ),\r\n    )\r\n    return classifier\r\n\r\nnlu_pl = create_nlu_pipeline('path')\r\nbentoml.transformers.save_model(\r\n       'nlu_pl',\r\n        pipeline=nlu_pl,\r\n        signatures={\r\n            \"predict\": {\r\n                \"batchable\": True,\r\n                \"batch_dim\": 0,\r\n            },\r\n        },\r\n    )\r\n```\r\n\r\nserver.py\r\n```python\r\nnlu_model = bentoml.transformers.get(\"nlu_pl:latest\")\r\nnlu_runner = bentoml.Runner(NLURunnable, models=[nlu_model],\r\n                            method_configs={\"predict\": {\"max_batch_size\": 16, \"max_latency_ms\": 600}}\r\n                            )\r\nregister_nlu_pipeline()\r\nsvc = bentoml.Service(\"server\", runners=[nlu_runner])\r\n\r\n\r\nclass NLURequest(BaseModel):\r\n    premise_list: List[str]\r\n    hypothesis_list: List[str]\r\n\r\n@svc.api(input=JSON(pydantic_model=NLURequest), output=JSON())\r\nasync def nlu_request(json: NLURequest) -> Dict[str, Any]:\r\n    req_body = {\"premise_list\": json.premise_list, \"hypothesis_list\": json.hypothesis_list}\r\n    response = await nlu_runner.predict.async_run(req_body)\r\n    return {\"result\": response.cpu().numpy()}\r\n\r\n```",
    "comments": [
      {
        "user": "pi2cto",
        "body": "I am experiencing error with batching enabled, 2022-11-08T13:07:08+0000 [INFO] [api_server:1] 127.0.0.1:33304 (scheme=http,method=POST,path=/v1/get_intents,type=application/json,length=91) (status=200,type=application/json,length=20) 1450.984ms (trace=f8472c38b374f57a7213989491a40acc,span=c32005903caa5b5f,sampled=0)\r\nTraceback (most recent call last):\r\n  File \"/workspace/personality_framework/personality_service/bento_service.py\", line 238, in get_intent\r\n    result=await runner1.is_positive.async_run([{\"sentence\":query}])\r\n  File \"/tmp/e2/lib/python3.8/site-packages/bentoml/_internal/runner/runner.py\", line 53, in async_run\r\n    return await self.runner._runner_handle.async_run_method(  # type: ignore\r\n  File \"/tmp/e2/lib/python3.8/site-packages/bentoml/_internal/runner/runner_handle/remote.py\", line 207, in async_run_method\r\n    raise ServiceUnavailable(body.decode()) from None  \r\n\r\nwithout batching the same code works well , \r\n\r\nmy batching configuration is \r\n    enabled: true\r\n    max_batch_size: 100\r\n    max_latency_ms: 1000\r\n\r\nwithout batching with load testing i get reply to my 100 simultaneous requests without error , with batching im facing the above error\r\n\r\n\r\n"
      },
      {
        "user": "frostming",
        "body": "Can you try on the latest bentoml version, with the new service API?"
      }
    ]
  },
  {
    "issue_number": 3093,
    "title": "bug: Detectron2 installation error during bentoml containerize",
    "author": "shinilseop",
    "state": "closed",
    "created_at": "2022-10-12T08:55:35Z",
    "updated_at": "2024-07-11T01:26:22Z",
    "labels": [
      "questions"
    ],
    "body": "### Describe the bug\n\nI am going to serve the Detectron2 model using BentoML.\r\nAlso, I want to image through bentoml containerize function.\r\n\r\nHowever, an error occurs during containerize when installing Python dependencies.\r\nbentofile.yaml is as follows:\r\n```yaml\r\nservice: \"main:svc\"\r\nlabels:\r\n  owner: so1s\r\n  stage: dev\r\ninclude:\r\n  - \"*.py\"\r\n  - \"*.txt\"\r\ndocker:\r\n  distro: debian\r\n  python_version: \"3.8.12\"\r\n  cuda_version: \"11.6.2\"\r\n  system_packages:\r\n    - git\r\n  env:\r\n    - INPUT_TYPE=image\r\n    - OUTPUT_TYPE=image\r\n    - MODEL_NAME=detectron2basic\r\n    - LIBRARY=detectron2\r\npython:\r\n  packages:\r\n    - \"torch==1.12.0+cu116\"\r\n    - \"torchvision==0.13.0+cu116\"\r\n    - \"torchaudio==0.12.0\"\r\n    - \"pyyaml==5.1\"\r\n    - \"opencv-python==4.6.0.66\"\r\n    - \"git+https://github.com/facebookresearch/detectron2.git\"\r\n  extra_index_url:\r\n    - \"https://download.pytorch.org/whl/cu116\"\r\n```\r\n\r\n```\r\nbentoml build\r\nbentoml containerize modelname:tag --network host --verbose --platform=linux/amd64\r\n```\r\n\r\nContainerize ends with an error that there is no 'torch' module while checking the dependency of detectron2 :\r\n```bash\r\nCollecting detectron2@ git+https://github.com/facebookresearch/detectron2.git\r\n  Cloning https://github.com/facebookresearch/detectron2.git to /private/var/folders/2n/bdz7z2vj7179hs0n6zy3pwbc0000gn/T/pip-install-vwe148tw/detectron2_3d76682006014f2f8d7a5e69c14bfad6\r\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /private/var/folders/2n/bdz7z2vj7179hs0n6zy3pwbc0000gn/T/pip-install-vwe148tw/detectron2_3d76682006014f2f8d7a5e69c14bfad6\r\n  Resolved https://github.com/facebookresearch/detectron2.git to commit 7c2c8fb168a2093ce06a531c1208fba48d2984ec\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × python setup.py egg_info did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [6 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"/private/var/folders/2n/bdz7z2vj7179hs0n6zy3pwbc0000gn/T/pip-install-vwe148tw/detectron2_3d76682006014f2f8d7a5e69c14bfad6/setup.py\", line 10, in <module>\r\n          import torch\r\n      ModuleNotFoundError: No module named 'torch'\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n```\r\n\r\nI can't set the order for pip installation. \r\nSo I couldn't find a way to resolve the error.\r\n\r\nThe only way I found it is to set up the detector2 module separately after bentoml containerize through Dockerfile.template.\r\nhttps://docs.bentoml.org/en/latest/guides/containerization.html#dockerfile-template\r\n\r\n\r\nIs there a better way than this?\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: 1.0.5\r\npython: 3.8.12\r\nos: mac os (m1)",
    "comments": [
      {
        "user": "aarnphm",
        "body": "This is not a bug, rather than a usage question\r\n\r\nSee https://github.com/bentoml/OCR-as-a-Service/blob/main/Dockerfile.template and https://github.com/bentoml/OCR-as-a-Service/blob/main/bentofile.yaml for setting up detectron."
      },
      {
        "user": "aarnphm",
        "body": "This has to do with detectron requiring torch first. With the requirements.txt, it will always respect the dependencies from git before everything, hence the error"
      }
    ]
  },
  {
    "issue_number": 3000,
    "title": "grpc(io): support numpy ndarray fields for PandasDataFrame",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-09-14T23:58:50Z",
    "updated_at": "2024-07-11T01:25:43Z",
    "labels": [],
    "body": "### Feature request\n\nPandasDataFrame support for `ndarray` proto field.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2998,
    "title": "feature: accepts multiple proto IO fields",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-09-14T22:46:45Z",
    "updated_at": "2024-07-11T01:25:32Z",
    "labels": [],
    "body": "### Feature request\n\nIO descriptor should have ability to accept multiple proto fields.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2993,
    "title": "feature(io): pandas validation ",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-09-13T22:31:49Z",
    "updated_at": "2024-07-11T01:25:20Z",
    "labels": [],
    "body": "### Feature request\n\nCurrently pandas IO descriptor doesn't have any validation in `from_http_request` and `to_http_response`\r\n\r\nIt should.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 3077,
    "title": "feature: Make the timeout of health probes configurable",
    "author": "qu8n",
    "state": "closed",
    "created_at": "2022-10-08T00:35:47Z",
    "updated_at": "2024-07-11T01:24:58Z",
    "labels": [],
    "body": "After this [PR](https://github.com/bentoml/BentoML/pull/3050) is done, API health checks will be added to runners in the same container, and health check requests to the server will have a timeout of 5s which is modeled after k8s. Ideally, we want to make this timeout value configurable by the users.",
    "comments": []
  },
  {
    "issue_number": 3086,
    "title": "bug: pandas IO descriptor",
    "author": "jiewpeng",
    "state": "closed",
    "created_at": "2022-10-11T00:31:09Z",
    "updated_at": "2024-07-11T01:23:50Z",
    "labels": [
      "bug",
      "io-descriptor"
    ],
    "body": "### Describe the bug\n\nWhen a user constructs a `PandasDataFrame` IODescriptor using `from_sample()`, it creates a broken IODescriptor.\r\n\r\n```python\r\ninst = cls(\r\n    orient=orient,\r\n    enforce_shape=enforce_shape,\r\n    shape=sample_input.shape,\r\n    apply_column_names=apply_column_names,\r\n    columns=[str(x) for x in list(sample_input.columns)],\r\n    enforce_dtype=enforce_dtype,\r\n    dtype=True,  # set to True to infer from given input\r\n    default_format=default_format,\r\n)\r\n```\r\n\r\nSpecifically, when calling `from_http_request`, there is a line that asserts that `self._dtype` is not a boolean, but when we create the IODescriptor using `from_sample`, it sets `dtype` to `True`.\r\n\r\n```python\r\nobj = await request.body()\r\nif serialization_format is SerializationFormat.JSON:\r\n    assert not isinstance(self._dtype, bool)\r\n```\r\n\r\nCurrently, I work around this by setting `self._dtype = None` after constructing the IODescriptor using `from_sample()`.\n\n### To reproduce\n\n_No response_\n\n### Expected behavior\n\n_No response_\n\n### Environment\n\nbentoml: v1.0.7\r\npython: 3.8.14\r\nplatform: Linux-5.4.209-116.367.amzn2.x86_64-x86_64-with-glibc2.2.5",
    "comments": [
      {
        "user": "frostming",
        "body": "Use the new BentoML IO descriptors please"
      }
    ]
  },
  {
    "issue_number": 3085,
    "title": "feature: ONNX service with multiple inputs",
    "author": "wyhanz",
    "state": "open",
    "created_at": "2022-10-10T11:03:39Z",
    "updated_at": "2024-07-11T01:22:59Z",
    "labels": [
      "feedback-wanted",
      "questions",
      "io-descriptor"
    ],
    "body": "### Feature request\r\n\r\nI exported an onnx model that accept multiple inputs (\"input_ids\", \"input_mask\", \"input_seg\"). \r\n\r\nAnd the docs for bentoml(ONNX) only give a simple example, `runner.run.run(test_input)`.\r\nHowever, an error ouccred after i wrapped these inputs with dict:  \r\n`TypeError: run of ONNXRunnable only takes numpy.ndarray or pd.DataFrame, tf.Tensor, or torch.Tensor as input parameters`\r\n\r\nI wonder if i use it in a wrong way? \r\n\r\n\r\n### Motivation\r\nI think ONNX is useful for AI inference so can you improve the docs about using bentoml on onnx service? \r\n\r\n### Other\r\n\r\n_No response_",
    "comments": [
      {
        "user": "wyhanz",
        "body": "After reading the source code, i found that bentoml may convert my input to np.float32. But my onnx model takes int64 as its input. It may be a bug?"
      },
      {
        "user": "wyhanz",
        "body": "\r\n```python\r\ndef inf(inputs):\r\n    inputs = json.loads(inputs)\r\n    inputs['input_ids'] = np.array(inputs['input_ids'], dtype = np.float32)\r\n    inputs['input_mask'] = np.array(inputs['input_mask'], dtype = np.float32)\r\n    inputs['input_seg'] = np.array(inputs['input_seg'], dtype = np.float32)\r\n\r\n    print(np.shape(inputs['input_ids']), np.shape(inputs['input_mask']), np.shape(inputs['input_seg']))\r\n    outputs = onnx_runner.run.run(\r\n                     inputs['input_ids'].astype(np.int64), \r\n                     inputs['input_mask'].astype(np.int64),\r\n                     inputs['input_seg'].astype(np.int64))\r\n\r\n    print('---------------------------------')\r\n    print(outputs)\r\n    return outputs\r\n```\r\nThis code raise error: `onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(int64))`. But I explicitly converted variable types to int64......\r\n\r\n"
      },
      {
        "user": "larme",
        "body": "@wyzhangyuhan This is a limitation for current onnx implementation (we are converting all inputs into float32). We will improve this by infer the input type from the onnx model"
      }
    ]
  },
  {
    "issue_number": 3048,
    "title": "bug: bentoml push should fail if there's MinIO errors",
    "author": "benjamintanweihao",
    "state": "closed",
    "created_at": "2022-09-29T08:50:37Z",
    "updated_at": "2024-07-11T01:21:18Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nWhen there are MinIO errors, `bentoml push` seems to fail silently:\r\n\r\n```\r\n██████╗░███████╗███╗░░██╗████████╗░█████╗░███╗░░░███╗██╗░░░░░\r\n██╔══██╗██╔════╝████╗░██║╚══██╔══╝██╔══██╗████╗░████║██║░░░░░\r\n██████╦╝█████╗░░██╔██╗██║░░░██║░░░██║░░██║██╔████╔██║██║░░░░░\r\n██╔══██╗██╔══╝░░██║╚████║░░░██║░░░██║░░██║██║╚██╔╝██║██║░░░░░\r\n██████╦╝███████╗██║░╚███║░░░██║░░░╚█████╔╝██║░╚═╝░██║███████╗\r\n╚═════╝░╚══════╝╚═╝░░╚══╝░░░╚═╝░░░░╚════╝░╚═╝░░░░░╚═╝╚══════╝\r\nSuccessfully built Bento(tag=\"aml:3ccqifb7xkgtzosg\")\r\n$ bentoml yatai login --api-token xxx --endpoint http://yatai.yatai-system.svc.cluster.local/\r\nlogin successfully! user: Benjamin Tan, organization: default\r\n$ export BENTO_DEPLOYMENT_NAME=\"$BENTO_MODEL-${CI_COMMIT_SHORT_SHA}\"\r\n$ bentoml push $BENTO_MODEL\r\n╭──────────────────────────────────────────────────────────────────────────────╮\r\n│ Failed to upload model \"aml:24da2qb7xk5mjosg\"                                │\r\n│ Failed pushing model \"aml:24da2qb7xk5mjosg\" : <?xml version=\"1.0\" encoding=… │\r\n│ <Error><Code>XMinioStorageFull</Code><Message>Storage backend has reached i… │\r\n│ Failed to upload Bento \"aml:3ccqifb7xkgtzosg\"                                │\r\n│ Failed pushing Bento \"aml:3ccqifb7xkgtzosg\": <?xml version=\"1.0\" encoding=\"… │\r\n│ <Error><Code>XMinioStorageFull</Code><Message>Storage backend has reached i… │\r\n╰──────────────────────────────────────────────────────────────────────────────╯\r\n  Pushing Bento \"aml:3ccqifb7xkgtzosg\" ━━━━━━ 100.0% • 29.8/… • 178.7  • 0:00:00\r\n                                                       kB       MB/s            \r\nUploading model \"aml:24da2qb7xk5mjosg\" ━━━━━━ 100.0% • 18.4/… • 103.2  • 0:00:00\r\n                                                       kB       MB/s\r\n $ bentoml list $BENTO_MODEL -o json | jq '.[0][\"tag\"]'\r\n\"aml:3ccqifb7xkgtzosg\"\r\n```\r\n\r\n\r\n\r\n### To reproduce\r\n\r\n_No response_\r\n\r\n### Expected behavior\r\n\r\nWhat I would have expected would be `bentoml push` exit and the subsequent command (i.e. `bentoml list`) not to execute.\r\n\r\n### Environment\r\n\r\nbentoml: 1.0.5\r\nenvironment: GKE",
    "comments": [
      {
        "user": "frostming",
        "body": "Please try with the latest bentoml version"
      }
    ]
  },
  {
    "issue_number": 3118,
    "title": "bug: UnicodeEncodeError is occured",
    "author": "smartdolphin",
    "state": "closed",
    "created_at": "2022-10-19T07:49:33Z",
    "updated_at": "2024-07-11T01:20:42Z",
    "labels": [
      "bug",
      "pre-1.0"
    ],
    "body": "### Describe the bug\r\n\r\nWhen utf-16 characters are sended to api, an UnicodeEncodeError error is occured with running encoding utf-8.\r\n\r\n```\r\nUnicodeEncodeError: 'utf-8' codec can't encode character '\\udc4f' in position 13: surrogates not allowed\r\n```\r\n\r\n### To reproduce\r\n\r\n1. text = \"\\udc4f\"\r\n2. send text to predict API\r\n3. see error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml/server/api_server.py\", line 394, in api_func\r\n    response = resp.to_flask_response()\r\n  File \"/usr/local/lib/python3.9/site-packages/bentoml/types.py\", line 288, in to_flask_response\r\n    return flask.Response(\r\n  File \"/usr/local/lib/python3.9/site-packages/werkzeug/wrappers/response.py\", line 206, in __init__\r\n    self.set_data(response)\r\n  File \"/usr/local/lib/python3.9/site-packages/werkzeug/wrappers/response.py\", line 332, in set_data\r\n    value = value.encode(self.charset)\r\nUnicodeEncodeError: 'utf-8' codec can't encode character '\\udc4f' in position 13: surrogates not allowed\r\n```\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nbentoml: 0.13-LTS\r\npython: 3.9\r\nplatform: linux",
    "comments": [
      {
        "user": "frostming",
        "body": "This issue is pre-1.0 and is no longer considered valid."
      }
    ]
  },
  {
    "issue_number": 2988,
    "title": "grpc: exception handling",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-09-12T22:36:22Z",
    "updated_at": "2024-07-11T01:19:36Z",
    "labels": [],
    "body": "### Feature request\n\nException raised via IO descriptor should be `ValueError` instead of currently implemented `InvalidArgument`.\r\n\r\nSince `InvalidArgument` eventually end up throwing a server error, this would be a bit awkward when handling exception on the client side. \r\nIf these error happens during server runtime, then we should raise Server exception there.\n\n### Motivation\n\n_No response_\n\n### Other\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2987,
    "title": "grpc: specific content type",
    "author": "aarnphm",
    "state": "closed",
    "created_at": "2022-09-12T22:27:36Z",
    "updated_at": "2024-07-11T01:19:01Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\r\n\r\nCurrently, content type for grpc are being set via our interceptor to `application/grpc`. While this conform with gRPC standard, this is not really useful for sysadmin. We should specify the correct CONTENT_TYPE for any given request.\r\n\r\nCurrently each io descriptor as `_rpc_content_type`, this should be specific to each IO descriptor.\r\n\r\nAccess log should use this to determine the appropriate content-type",
    "comments": []
  },
  {
    "issue_number": 2975,
    "title": "feature: make it easier to override / set fields in the openapi docs",
    "author": "creativedutchmen",
    "state": "open",
    "created_at": "2022-09-08T07:40:56Z",
    "updated_at": "2024-07-11T01:18:32Z",
    "labels": [
      "feedback-wanted",
      "questions"
    ],
    "body": "### Feature request\n\nI would like if I could set details that are relevant to our organisation in the openapi docs for my models (contact information, summary, git link) without having to wrap the bento in another API.\n\n### Motivation\n\nWhen consumers of my API visit the docs for my deployed models, they don't care that I used BentoML to deploy the model - they want to know what the model does, how to use it and who to contact if they run into issues.\r\n\r\nSome of the fields in the docs can be overwritten, but the GitHub link on the top right, the summary (`A ML Service created with BentoML`) and contact `Contact(name=\"BentoML Team\", email=\"contact@bentoml.ai\")` are hardcoded without an easy way to set them myself.\r\n\r\nThe only way to change this behaviour now is to override the `openapi_spec` property and set those fields manually - which is fragile and cumbersome. It would be great if we could take these values from the bentofile instead.\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "aarnphm",
        "body": "I think it is a great idea for user to override these fields. However, I'm a bit more hesistant for putting this into bentofile.yaml\r\n\r\nFastAPI does allow user to extend [OpenAPI](https://fastapi.tiangolo.com/advanced/extending-openapi/), but I think if we want to design this feature we would probably will provide a Python API for this. What do you think?"
      },
      {
        "user": "creativedutchmen",
        "body": "We already allow some of it to be overwritten in the bentofile though, by setting `description` for instance, so having it in one place makes sense to me.\r\n\r\nBut yeah, if the goal is to allow much more customisation, then python is the way to go. (I would _love_ to be able to replace the BentoML docs with a version that doesn't include google tag manager, for instance)"
      },
      {
        "user": "aarnphm",
        "body": "I believe this would be also a great features for community contribution. I would love to help you with desigining this if you are interested in contributing. We also have #2900 that also touches a bit of OpenAPI.\r\n\r\nWould love to chat more on slack. But also feel free to discuss here as well 😃 "
      }
    ]
  },
  {
    "issue_number": 2955,
    "title": "feature: allow runner signatures without arguments",
    "author": "mqk",
    "state": "closed",
    "created_at": "2022-08-30T16:49:51Z",
    "updated_at": "2024-07-11T01:15:45Z",
    "labels": [
      "enhancement",
      "io-descriptor"
    ],
    "body": "### Feature request\n\nI would it if runner signatures could be called without an `input_data` argument.\n\n### Motivation\n\nWe want to be able to access the feature names from a trained LightGBM classifier. The model has a method called `feature_name()` on it (https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster.feature_name). We tried to add a signature for it to the runner, so that we could access it in the bento service, but that failed because runner signatures require an argument.\r\n\r\n\n\n### Other\n\n_No response_",
    "comments": [
      {
        "user": "ssheng",
        "body": "Thanks for reporting this issue, @mqk . I don’t think there is a strong requirement for runner method to have at least one argument. cc: @bojiang "
      },
      {
        "user": "parano",
        "body": "Thanks for the feedback @mqk!\r\n\r\nAnother approach is to save the feature names in the model's `custom_objects` field, here's an example: https://github.com/bentoml/BentoML/tree/main/examples/sklearn/pipeline\r\n\r\n```python\r\n...\r\n\r\nbento_model = bentoml.sklearn.save_model(\r\n    \"20_news_group\",\r\n    grid_search.best_estimator_,\r\n    signatures={\r\n        \"predict\": {\"batchable\": True, \"batch_dim\": 0},\r\n        \"predict_proba\": {\"batchable\": True, \"batch_dim\": 0},\r\n    },\r\n    custom_objects={\r\n        \"target_names\": data.target_names,\r\n    },\r\n    metadata=best_parameters,\r\n)\r\nprint(f\"Model saved: {bento_model}\")\r\n```\r\n\r\n```python\r\nbento_model = bentoml.sklearn.get(\"20_news_group:latest\")\r\nmodel_runner = bento_model.to_runner()\r\ntarget_names = bento_model.custom_objects[\"target_names\"]\r\n\r\nsvc = bentoml.Service(\"doc_classifier\", runners=[model_runner])\r\n\r\n\r\n@svc.api(input=Text(), output=JSON())\r\ndef predict(input_doc: str):\r\n    prediction = model_runner.predict.run([input_doc])[0]\r\n    return {\"result\": target_names[prediction]}\r\n```"
      },
      {
        "user": "mqk",
        "body": "Thanks both. `custom_objects` is an interesting suggestion, @parano. Thanks, I'll investigate that solution. Still think it might be useful to allow users to expose methods on the model that don't require input."
      }
    ]
  },
  {
    "issue_number": 2913,
    "title": "feat: Dynamic padding based on adaptive batching",
    "author": "ssheng",
    "state": "closed",
    "created_at": "2022-08-17T03:55:41Z",
    "updated_at": "2024-07-11T01:14:26Z",
    "labels": [
      "enhancement"
    ],
    "body": null,
    "comments": []
  }
]