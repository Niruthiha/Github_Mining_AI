[
  {
    "issue_number": 48846,
    "title": "[data] importing ray.data closes logging handlers, breaking custom logging ",
    "author": "Liquidmasl",
    "state": "open",
    "created_at": "2024-11-21T15:17:09Z",
    "updated_at": "2025-06-17T14:26:57Z",
    "labels": [
      "bug",
      "P1",
      "windows",
      "data",
      "community-backlog"
    ],
    "body": "### What happened + What you expected to happen\n\nI am debugging for 2 days now why our Loki logging handler will not log some lines. \n\nI think I understand now that ray is removing all logging handlers and just.. readding (?) these?\n```\nHandler.__init__ <class 'ray._private.log.PlainRayHandler'>\nHandler.__init__ <class 'logging._StderrHandler'>\nHandler.__init__ <class 'logging.StreamHandler'>\n```\n\nMeaning the handler that is responsible for sending our logs to our logging server just disappears. I have not figured out why this happens. \n\nI added a debug print in `logging.Handerl.__init__` and `logging.Handler.close`, this is what i get:\n\n```\nHandler.close <class 'processing.intern_depend.cirq_logger.loki_logger.CustomLokiHandler'>\nHandler.close <class 'processing.intern_depend.cirq_logger.loki_logger.CustomLokiQueueHandler'>\nHandler.close <class 'logging.StreamHandler'>\nHandler.close <class 'logging.NullHandler'>\nHandler.close <class 'logging.NullHandler'>\nHandler.close <class 'logging.StreamHandler'>\nHandler.close <class 'logging.NullHandler'>\nHandler.close <class 'logging._StderrHandler'>\nHandler.__init__ <class 'ray._private.log.PlainRayHandler'>\nHandler.__init__ <class 'logging._StderrHandler'>\nHandler.__init__ <class 'logging.StreamHandler'>\n```  \n\nSo my loki handler is just... gone. And that without any notice, I just noticed that logs are missing on the server.\n\nWhen I debug into the close method I see in the call stack that is being triggered by `generate_logging_config()` in `ray/private/_log` and it seams it is called always from `ray/__init__.py` in line 7\n\nThe only thing i can do is ensure that i ALWAYS import ray before i initialize my logging. \na little change in linter configuration can lead to the ray import slipping below the logger initialization, silently shutting of our custom logger. \n\nI think this is quite the severe issue! And it was a very hard and strange issue to find. \n\n(might be related to https://github.com/ray-project/ray/issues/22312 ?)\n\n\nAfter more trial and error I am even more confused. Using a queue handler, or another custom handler, it still works, even though apparently the handlers are closed. \nbut when the custom handler has some custom shut down or `close()` code. stuff breaks. \nThe 'minimal' reproducer is not so minimal, but it manages to reproduce without any libraries. \n\n\n\n### Versions / Dependencies\n\nWindows 11\nRay 2.39\nPython 3.11\n\n### Reproduction script\n```py\nimport logging\nimport queue\nimport threading\nimport time\nfrom logging.handlers import QueueHandler\nfrom logging.handlers import QueueListener\n\nroot_logger = logging.getLogger()\nroot_logger.setLevel(logging.DEBUG)\n\n\nclass CustomQueueHandler(QueueHandler):\n\n    def __init__(self, queue, **kwargs):\n        super().__init__(queue)\n        self.custom_handler = BatchHandler(**kwargs)  # noqa: WPS110\n        self.listener = QueueListener(self.queue, self.custom_handler)\n        self.listener.start()\n\n    def close(self):\n        self.listener.stop()\n        self.custom_handler.stop()\n        super().close()\n\n    def flush(self):\n        self.custom_handler.flush()\n        super().flush()\n\n\nclass BatchHandler(logging.Handler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.lock = threading.Lock()\n        self.last_handle_time = time.time()\n        self.stop_flag = False\n        self.batch = []\n        self.thread = threading.Thread(target=self._monitor)\n        self.thread.daemon = True\n        self.thread.start()\n\n\n    def handle(self, record):\n        with self.lock:\n            self.batch.append(record)\n            print(self.batch)\n\n    def _monitor(self):\n        while True:\n            try:\n                if len(self.batch) > 3:\n                    self.handle_batch()\n\n                if time.time() - self.last_handle_time > 2:\n                    self.handle_batch()\n\n                if self.stop_flag:\n                    self.handle_batch()\n                    break\n\n                time.sleep(1)\n            except queue.Empty:\n                pass\n\n    def handle_batch(self):\n\n        with self.lock:\n            b = self.batch\n            self.batch = []\n\n        for i, record in enumerate(b):\n            print(f'{i}: {record.msg}')\n\n    def stop(self):\n        self.stop_flag = True\n        self.thread.join()\n\n\nq = queue.Queue()\nroot_logger.addHandler(CustomQueueHandler(q))\n\nroot_logger.info('I will be printed 1')\n\nimport ray  # this does not break it anymore since 2.41\nfrom ray.data import from_pandas_refs  # but this still does break it! Every log after this line will be missing!\n\n\nroot_logger.info('I will be missing when this is below the ray import ')\nlogging.getLogger('some.other.logger').info('I will also be missing if i am below the ray import.')\n```\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.\n\n# EDIT: \n### 2.41 fixed the loggers being closed on `import ray` but did **not** fix the issue when doing `from ray.data import from_pandas_refs`. This still leads to the same issue. I updated the reproducer with the different import\n",
    "comments": [
      {
        "user": "Liquidmasl",
        "body": "Actualy now deeper down in my software, when i thought it finally works. it seams some modin df initialisation REIMPORTS ray, or uses some other import, or dunno, which leads to the same method being called, again destroying my handlers...\r\n\r\nSo now i have to put `from ray.data import from_pandas_refs` into my logging module aswell. so to make it work"
      },
      {
        "user": "alexeykudinkin",
        "body": "This is a duplicate of https://github.com/ray-project/ray/issues/48732"
      },
      {
        "user": "kevin85421",
        "body": "I tested it manually. I have already been fixed. Close this issue.\r\n\r\n<img width=\"1728\" alt=\"image\" src=\"https://github.com/user-attachments/assets/79786a3b-7712-46d2-88e0-8aa52bea4ffa\" />\r\n"
      }
    ]
  },
  {
    "issue_number": 47289,
    "title": "[core] aggregated metrics for `ray_tasks`/`ray_actors`",
    "author": "hongchaodeng",
    "state": "closed",
    "created_at": "2024-08-23T00:25:28Z",
    "updated_at": "2025-06-17T13:36:35Z",
    "labels": [
      "enhancement",
      "P1",
      "core",
      "observability",
      "stability"
    ],
    "body": "### Description\n\nCurrently the the ray_tasks/actors metrics could be of huge volume. This is fine for single cluster. But for aggregated platform view this could be a problem of excessive load on Prometheus & Grafana server.\r\n\r\nFor these aggregated view, we don't need to know the `NAME`, `WorkerId`, etc. But these tags lead to high cardinality in output metrics. Due to the current limit of GAUGE type of these metrics, dropping labels is not ideal either.\r\n\r\nWe should add a new aggregated metrics for `ray_tasks`/`ray_actors`.\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "alanwguo",
        "body": "For this, we should also take into account that Prometheus is not designed to show \"absolute\" count values very well. We should design our metric and the visualization of that metric around rates/changes of the value.\r\n\r\nThe existing \"number of tasks in each state\" graph performs poorly in prometheus. "
      },
      {
        "user": "alanwguo",
        "body": "One idea is we do a rate for finished tasks and absolute counts for running tasks.\r\n\r\nThen we do a counter for finished tasks state and gauges for the rest."
      }
    ]
  },
  {
    "issue_number": 29127,
    "title": "[Core]  Ray Autoscaler does not restart a worker node on setup failure",
    "author": "mzakharocsc",
    "state": "closed",
    "created_at": "2022-10-06T20:08:59Z",
    "updated_at": "2025-06-17T12:36:11Z",
    "labels": [
      "bug",
      "P3",
      "core-autoscaler",
      "infra",
      "core",
      "Ray-2.4",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nIn a ray aws cluster, dynamically added worker nodes that sometimes fail to execute statements listed inside `setup_commands` configuration block will not recover. Command execution failure is reported on the monitor console, but the failed node will continue to show up as `uninitialized` pending status indefinitely.  \r\nExpected behaviour is to restart/retry setting up the worker after the failure has occurred/some timeout. \r\n\r\nExample of command failure:\r\n```\r\n2022-10-06 11:40:16,543\tERR updater.py:158 -- New status:update-failed\r\n2022-10-06 11:40:16,544\tERR updater.py:160 -- !!!\r\n2022-10-06 11:40:16,544\tVERR updater.py:168 -- {'message': 'SSH command failed.'}\r\n2022-10-06 11:40:16,544\tERR updater.py:170 -- SSH command failed.\r\n2022-10-06 11:40:16,544\tERR updater.py:172 -- !!!\r\n```\r\n\r\nExample of monitor status after node setup failure has occured:\r\n```\r\n======== Autoscaler status: 2022-10-06 11:58:17.648737 ========\r\nNode status\r\n---------------------------------------------------------------\r\nHealthy:\r\n 1 ray.head.default\r\nPending:\r\n<IP>: ray.worker.default, uninitialized\r\nRecent failures:\r\n (no failures)\r\n```\r\n\r\n### Versions / Dependencies\r\n\r\nRay 2.0.0\r\nPython 3.8.0\r\n\r\n\r\n### Reproduction script\r\n\r\n```\r\ncluster_name: default\r\nmax_workers: 8\r\nupscaling_speed: 1.0\r\n\r\ndocker:\r\n    image: rayproject/ray:latest-cpu   # use this one if you don't need ML dependencies, it's faster to pull\r\n    container_name: \"ray_container\"\r\n    pull_before_run: True\r\n    run_options:   # Extra options to pass into \"docker run\"\r\n        - --ulimit nofile=65536:65536\r\n   \r\nidle_timeout_minutes: 5\r\n\r\n# Cloud-provider specific configuration.\r\nprovider:\r\n    type: aws\r\n    region: us-east-1\r\n    availability_zone: us-east-1a\r\n    cache_stopped_nodes: True # If not present, the default is True.\r\n    use_internal_ips: true\r\n\r\n# How Ray will authenticate with newly launched nodes.\r\nauth:\r\n    ssh_user: ubuntu\r\n\r\navailable_node_types:\r\n    ray.head.default:\r\n        resources: {}\r\n        node_config:\r\n            InstanceType: m5.large\r\n            ImageId: ami-x \r\n\r\n    ray.worker.default:\r\n        min_workers: 0\r\n        max_workers: 8\r\n        # resources: {\"CPU\": 1, \"GPU\": 1, \"custom\": 5}\r\n        resources: {}\r\n        node_config:\r\n            InstanceType: m5.large\r\n            ImageId: ami-x\r\n \r\nhead_node_type: ray.head.default\r\n\r\n\r\n# List of shell commands to run to set up nodes.\r\nsetup_commands:\r\n    - SOME_SOMETIMES_FAILING_COMMAND\r\n\r\n# Command to start ray on the head node. You don't need to change this.\r\nhead_start_ray_commands:\r\n    - ray stop\r\n    - ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml\r\n\r\n# Command to start ray on worker nodes. You don't need to change this.\r\nworker_start_ray_commands:\r\n    - ray stop\r\n    -  ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\r\n```\r\n### Issue Severity\r\n\r\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "Hello! This P3 issue has seen no activity in at least a year. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 53881,
    "title": "[Ray Data]Pylint detection found some Python code defects in ray data",
    "author": "daiping8",
    "state": "open",
    "created_at": "2025-06-17T12:17:56Z",
    "updated_at": "2025-06-17T12:17:56Z",
    "labels": [],
    "body": "Pylint scan found some Python code defects in ray data, such as:\n\nray/python/data/datasource/datasink.py:13:0: C0103: Type variable name \"WriteReturnType\" doesn't conform to predefined naming style (invalid-name)\nray/python/ray/data/datasource/file_based_datasource.py:276:20: R1730: Consider using 'num_threads = min(num_threads, len(read_paths))' instead of unnecessary if block (consider-using-min-builtin)\nray/python/ray/data/read_api.py:3214:4: R1705: Unnecessary \"elif\" after \"return\", remove the leading \"el\" from \"elif\" (no-else-return)\n/python/ray/data/dataset.py:845:20: R1701: Consider merging these isinstance calls to isinstance(column, (pd.DataFrame, pd.Index, pd.Series)) (consider-merging-isinstance)\nray/python/ray/data/dataset.py:867:16: R1705: Unnecessary \"else\" after \"return\", remove the \"else\" and de-indent the code inside it (no-else-return)\n\n   All these issues do not affect the logic of program execution, but we can fix these defects, improve code readability, reduce loop load, or remove useless code, and so on.\n   For example, the corresponding source code for ray/python/ray/data/dataset.py: 845:20 above is:\n“if (\nisinstance(column, pd.Series)\nor isinstance(column, pd.DataFrame)\nor isinstance(column, pd.Index)\n):\n”\nWe can completely merge the above three judgments into one isinstance (column, (pd. DataFrame, pd. Index, pd. Series)) without affecting the logic, and the code is more concise.",
    "comments": []
  },
  {
    "issue_number": 43200,
    "title": "[data] verbose_progress=True doesn't work in client mode",
    "author": "danielgafni",
    "state": "open",
    "created_at": "2024-02-15T16:45:13Z",
    "updated_at": "2025-06-17T11:24:32Z",
    "labels": [
      "bug",
      "P3",
      "data",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nNo progress bar appears \r\n\r\nMy app has the following structure: \r\n\r\n```python\r\n@ray.remote\r\ndef main():\r\n    ray.data.Dataset.....map_batches.materialize()\r\n\r\nray.get(main.remote())\r\n```\r\n\r\nIf I'm running it in client mode on KubeRay cluster, the progress bars do not show up.\r\n\r\nAll the bars are shown correctly when running in local mode without connecting to a cluster.\r\n\r\n### Versions / Dependencies\r\n\r\nPython 3.9.13\r\nRay 2.9.1\r\n\r\n### Reproduction script\r\n\r\n```python\r\nimport time\r\n\r\nimport ray\r\nimport polars as pl\r\n\r\ndef batch_fn(batch):\r\n    print(f\"hello {min(batch['i'])}!\")\r\n    time.sleep(0.1)\r\n    return batch\r\n\r\n\r\n@ray.remote\r\ndef main():\r\n    ray.data.DataContext.get_current().execution_options.verbose_progress = True\r\n\r\n    print(\"Before map_batches...\")\r\n\r\n    ray.data.from_arrow(\r\n        pl.DataFrame({\r\n            \"i\": list(range(1000))\r\n        }).to_arrow()\r\n    ).map_batches(batch_fn, batch_size=50, concurrency=10).materialize()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ray.get(main.remote())\r\n```\r\n\r\n### Issue Severity\r\n\r\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "Hello! This P3 issue has seen no activity in at least a year. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      },
      {
        "user": "danielgafni",
        "body": "I believe it's a valid issue. I am using Ray with client mode all the time. "
      }
    ]
  },
  {
    "issue_number": 53879,
    "title": "[dashboard] Support to overwrite the _client_max_size of http request entity",
    "author": "caican00",
    "state": "open",
    "created_at": "2025-06-17T11:14:22Z",
    "updated_at": "2025-06-17T11:14:22Z",
    "labels": [
      "bug",
      "triage"
    ],
    "body": "### What happened + What you expected to happen\n\nWe should support adjusting the max size of the request entity in the upload_package in job_head, some packages can easily exceed the default limit of 1m\n\n### Versions / Dependencies\n\n2.47.0\n\n### Reproduction script\n\nNone\n\n### Issue Severity\n\nNone",
    "comments": []
  },
  {
    "issue_number": 53878,
    "title": "[RLlib] Significant drop in DQN training reward when resuming from checkpoint",
    "author": "WhizZest",
    "state": "open",
    "created_at": "2025-06-17T10:03:18Z",
    "updated_at": "2025-06-17T10:05:48Z",
    "labels": [
      "bug",
      "triage"
    ],
    "body": "### What happened + What you expected to happen\n\nI ran DQN training on CartPole-v1 for 10 iterations and observed steadily increasing rewards, reaching around 123.54 before saving a checkpoint and exiting. Everything looked stable:\n```\n[Training] Iteration 0: reward = 20.04, sampled_timesteps = 32\n[Training] Iteration 1: reward = 23.34, sampled_timesteps = 16032\n[Training] Iteration 2: reward = 30.50, sampled_timesteps = 32032\n[Training] Iteration 3: reward = 32.42, sampled_timesteps = 48032\n[Training] Iteration 4: reward = 40.64, sampled_timesteps = 64032\n[Training] Iteration 5: reward = 57.04, sampled_timesteps = 80032\n[Training] Iteration 6: reward = 69.62, sampled_timesteps = 96032\n[Training] Iteration 7: reward = 91.88, sampled_timesteps = 112032\n[Training] Iteration 8: reward = 110.36, sampled_timesteps = 128032\n[Training] Iteration 9: reward = 123.54, sampled_timesteps = 144032\n```\nWhen I resumed training by loading the checkpoint and continued for another 10 iterations (iterations 11–20), the reward collapsed immediately—iteration 11 dropped to 22.78, despite the previous checkpoint’s reward being 123.54:\n```\n[Training] Iteration 10: reward = 22.78, sampled_timesteps = 16000\n[Training] Iteration 11: reward = 27.72, sampled_timesteps = 32000\n[Training] Iteration 12: reward = 36.40, sampled_timesteps = 48000\n[Training] Iteration 13: reward = 41.04, sampled_timesteps = 64000\n[Training] Iteration 14: reward = 45.60, sampled_timesteps = 80000\n[Training] Iteration 15: reward = 50.84, sampled_timesteps = 96000\n[Training] Iteration 16: reward = 59.66, sampled_timesteps = 112000\n[Training] Iteration 17: reward = 67.42, sampled_timesteps = 128000\n[Training] Iteration 18: reward = 72.24, sampled_timesteps = 144000\n[Training] Iteration 19: reward = 80.78, sampled_timesteps = 160000\n```\nIf I train continuously from scratch for 20 iterations, rewards increase normally (with iteration 11 around ~154):\n```\n[Training] Iteration 0: reward = 24.18, sampled_timesteps = 32\n[Training] Iteration 1: reward = 20.68, sampled_timesteps = 16032\n[Training] Iteration 2: reward = 28.98, sampled_timesteps = 32032\n[Training] Iteration 3: reward = 36.06, sampled_timesteps = 48032\n[Training] Iteration 4: reward = 46.12, sampled_timesteps = 64032\n[Training] Iteration 5: reward = 56.12, sampled_timesteps = 80032\n[Training] Iteration 6: reward = 75.36, sampled_timesteps = 96032\n[Training] Iteration 7: reward = 90.38, sampled_timesteps = 112032\n[Training] Iteration 8: reward = 104.32, sampled_timesteps = 128032\n[Training] Iteration 9: reward = 125.52, sampled_timesteps = 144032\n[Training] Iteration 10: reward = 142.24, sampled_timesteps = 160032\n[Training] Iteration 11: reward = 154.98, sampled_timesteps = 176032\n[Training] Iteration 12: reward = 170.42, sampled_timesteps = 192032\n[Training] Iteration 13: reward = 191.34, sampled_timesteps = 208032\n[Training] Iteration 14: reward = 202.32, sampled_timesteps = 224032\n[Training] Iteration 15: reward = 218.48, sampled_timesteps = 240032\n[Training] Iteration 16: reward = 224.14, sampled_timesteps = 256032\n[Training] Iteration 17: reward = 221.78, sampled_timesteps = 272032\n[Training] Iteration 18: reward = 220.42, sampled_timesteps = 288032\n[Training] Iteration 19: reward = 211.40, sampled_timesteps = 304032\n```\nAdditionally, I suspect the replay buffer isn't being correctly saved/restored. I tried setting ```store_buffer_in_checkpoints=True```, but it appears to have no effect.\nI suspect one or more of the following are being reset when loading from the checkpoint:\n1. Epsilon (exploration rate), possibly resetting back to 1.0\n2. Timestep counters, possibly resetting to 0, which affects epsilon decay\n3. Replay buffer, possibly lost upon restore\n\nI haven't found a reliable way to monitor or verify any of these states after resuming.\n\n### Versions / Dependencies\n\nRay 2.44.1\nPython 3.10.16\nwindows 11\n\n### Reproduction script\n\n```\nimport ray\nfrom ray.rllib.algorithms.dqn import DQNConfig\nfrom ray.rllib.algorithms.algorithm import Algorithm\nfrom pathlib import Path\n\nCHECKPOINT_DIR = \"my_dqn_checkpoints\"\nCONTINUE_TRAIN_ITER = 10\n\ndef continue_training(checkpoint_path):\n    try:\n        algo = Algorithm.from_checkpoint(checkpoint_path)\n        print(f\"Loaded checkpoint from: {checkpoint_path}\")\n    except Exception as e:\n        print(f\"Error loading checkpoint: {e}, starting fresh training.\")\n        config = (\n            DQNConfig()\n            .environment(\"CartPole-v1\")\n            .env_runners(num_env_runners=2)\n            .framework(\"torch\")\n            .training(replay_buffer_config={\n                \"type\": \"PrioritizedEpisodeReplayBuffer\",\n                \"capacity\": 60000,\n                \"alpha\": 0.5,\n                \"beta\": 0.5,\n            },store_buffer_in_checkpoints=True)\n        )\n        print(f\"exploration_config: {config['exploration_config']}\")\n        algo = config.build_algo()\n\n    print(f\"exploration_config: {algo.config['exploration_config']}\")\n    for i in range(algo.iteration, algo.iteration + CONTINUE_TRAIN_ITER):\n        result = algo.train()\n        reward = result.get(\"env_runners\", {}).get(\"episode_return_mean\")\n        if reward is None:\n            reward = 0.0\n        print(f\"[Training] Iteration {i}: reward = {reward:.2f}, sampled_timesteps = {algo.local_replay_buffer.sampled_timesteps}\")\n\n    new_checkpoint = algo.save_to_path(checkpoint_path)\n    print(f\"\\n Continued checkpoint saved to: {checkpoint_path}\\n\")\n\n    return new_checkpoint\n\nif __name__ == \"__main__\":\n    checkpoint_dir = Path(CHECKPOINT_DIR).absolute()\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n    checkpoint = str(checkpoint_dir)\n    print(f\"Checkpoint directory: {checkpoint}\")\n\n    new_checkpoint = continue_training(checkpoint)\n\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": []
  },
  {
    "issue_number": 53877,
    "title": "[RLlib] Checkpoint metrics loading with Tune is broken in 2.47.0",
    "author": "maybe-otski",
    "state": "open",
    "created_at": "2025-06-17T09:50:35Z",
    "updated_at": "2025-06-17T09:50:35Z",
    "labels": [
      "bug",
      "triage"
    ],
    "body": "### What happened + What you expected to happen\n\nIf you are using PPO with tune.fit() and loading an existing checkpoint, the metrics are incorrect. For example `num_env_steps_sampled_lifetime` is not initialized from the checkpoint. As a side effect for example the learning rate schedule starts from the beginning instead of continuing from the checkpoint value in the next tune run.\n\nThis appears to be a regression in ray 2.47.0 release as it works with 2.46.0.\n\n### Expected result\nUsing the reproduction script, the output should be:\n```\nTune run 1 (initial):\nNum env steps sampled lifetime:  4000\nLearning rate 0.000255\n\nTune run 2 (load from checkpoint):\nNum env steps sampled lifetime:  8000\nLearning rate 1e-05\n```\nThis is what happens with ray 2.46.0\n\n### Actual result\n```\nTune run 1 (initial):\nNum env steps sampled lifetime:  4000\nLearning rate 0.000255\n\nTune run 2 (load from checkpoint):\nNum env steps sampled lifetime:  4000.0\nLearning rate 0.000255\nTraceback (most recent call last):\n  File \"/home/otski/external_src/ray/rllib/examples/debugging/temp.py\", line 67, in <module>\n    assert int(lifetime_steps_after_second) == 2*int(lifetime_steps_after_first), \\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: Expected lifetime steps to be incremented, now: 4000 vs. 4000\n```\n\n### Versions / Dependencies\n\nPython 3.12.8\nray 2.47.0\nPop!_OS 22.04 LTS x86_64\n\n### Reproduction script\n\n```\nfrom pathlib import Path\n\nimport ray\nfrom ray import tune\nfrom ray.rllib.algorithms.ppo import PPOConfig\n\n\ndef _run_with_tune(algo_config):\n    tuner = tune.Tuner(\n        algo_config.algo_class,\n        param_space=algo_config.to_dict(),\n        run_config=tune.RunConfig(\n            storage_path=Path.cwd() / \"ray_results\",\n            name=\"example\",\n            stop={\"iterations_since_restore\": 1},\n            checkpoint_config=tune.CheckpointConfig(\n                checkpoint_at_end=True,\n            )\n        )\n    )\n    return tuner.fit()\n\nif __name__ == \"__main__\":\n    ray.init(ignore_reinit_error=True)\n\n    config = (\n        PPOConfig()\n        .api_stack(\n            enable_rl_module_and_learner=True,\n            enable_env_runner_and_connector_v2=True)\n        .framework(\"torch\")\n        .environment(\"CartPole-v1\")\n        .env_runners(\n            num_env_runners=1,\n        )\n        .training(\n            lr=[\n                [0, 0.0005],\n                [8000, 0.00001],\n            ]\n        )\n    )\n    tune_results = _run_with_tune(config)\n\n    checkpoint_dir = tune_results[0].checkpoint.path\n    lifetime_steps_after_first = tune_results[0].metrics['num_env_steps_sampled_lifetime']\n    learning_rate_after_first = tune_results[0].metrics['learners']['default_policy']['default_optimizer_learning_rate']\n\n    # Load the first run checkpoint for the second run\n    config.callbacks(\n        on_algorithm_init=(\n            lambda algorithm, _dir=str(checkpoint_dir), **kw: algorithm.restore_from_path(_dir)\n        )\n    )\n    tune_results = _run_with_tune(config)\n    lifetime_steps_after_second = tune_results[0].metrics['num_env_steps_sampled_lifetime']\n    learning_rate_after_second = tune_results[0].metrics['learners']['default_policy']['default_optimizer_learning_rate']\n\n    print('Tune run 1 (initial):')\n    print('Num env steps sampled lifetime: ', lifetime_steps_after_first)\n    print('Learning rate', learning_rate_after_first)\n    print()\n    print('Tune run 2 (load from checkpoint):')\n    print('Num env steps sampled lifetime: ', lifetime_steps_after_second)\n    print('Learning rate', learning_rate_after_second)\n\n    assert int(lifetime_steps_after_second) == 2*int(lifetime_steps_after_first), \\\n        f\"Expected lifetime steps to be incremented, now: {int(lifetime_steps_after_first)} vs. {int(lifetime_steps_after_second)}\"\n    assert abs(learning_rate_after_first - learning_rate_after_second) > 1e-7, \\\n        f\"Expected learning rates to differ, now: {learning_rate_after_first} vs. {learning_rate_after_second}\"\n```\n\n### Issue Severity\n\nNone",
    "comments": []
  },
  {
    "issue_number": 53848,
    "title": "[Core] Ray 2.47 regression: All tasks hang when using `uv`",
    "author": "juhaszp95",
    "state": "open",
    "created_at": "2025-06-16T09:15:21Z",
    "updated_at": "2025-06-17T09:32:51Z",
    "labels": [
      "bug",
      "regression",
      "triage",
      "core",
      "stability"
    ],
    "body": "### What happened + What you expected to happen\n\nRunning any Ray jobs just hangs after I upgraded to Ray 2.47. I use `uv` for environment management, which may be  part of the issue judging by the error messages below. I can confirm the script below runs with Ray 2.46.\n\nI have no idea what the issue might be, happy to give more information as needed. This may be related to https://github.com/ray-project/ray/pull/53060 as well.\n\nPlease see logs below:\n```\n2025-06-16 05:07:01,222\tINFO worker.py:1917 -- Started a local Ray instance.\n2025-06-16 05:07:01,240\tINFO packaging.py:588 -- Creating a file package for local module '/home/coder/Research/Setup/uv_base'.\n2025-06-16 05:07:01,263\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_dbbf75d97baeb29d.zip' (1.63MiB) to Ray cluster...\n2025-06-16 05:07:01,272\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_dbbf75d97baeb29d.zip'.\n(raylet) error: unexpected argument '--node-ip-address' found\n(raylet) \n(raylet)   tip: a similar argument exists: '--no-editable'\n(raylet) \n(raylet) Usage: uv run --with <WITH> --no-editable\n(raylet) \n(raylet) For more information, try '--help'.\n(raylet) \n(raylet) \n(raylet) Usage: uv run --with <WITH> --no-editable\n(raylet) \n(raylet) \n(raylet) \n(raylet) Usage: uv run --with <WITH> --no-editable\n(raylet) \n(raylet) [2025-06-16 05:08:02,689 E 2371813 2371813] (raylet) worker_pool.cc:586: Some workers of the worker process(2372264) have not registered within the timeout. The process is dead, probably it crashed during start.\n(raylet) error: unexpected argument '--node-ip-address' found [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(raylet)   tip: a similar argument exists: '--no-editable' [repeated 2x across cluster]\n(raylet) For more information, try '--help'. [repeated 2x across cluster]\n(raylet) \n(raylet) \n(raylet) Usage: uv run --with <WITH> --no-editable\n(raylet) \n(raylet) \n(raylet) \n(raylet) Usage: uv run --with <WITH> --no-editable\n(raylet) \n(raylet) \n(raylet) \n(raylet) Usage: uv run --with <WITH> --no-editable\n(raylet) \n```\n\n### Versions / Dependencies\n\nRay 2.47\nPython 3.11\nuv 0.7.13\n\n### Reproduction script\n\nThe script is taken from the [docs](https://docs.ray.io/en/latest/ray-core/examples/map_reduce.html):\n\n```\nimport subprocess\nimport ray\n\nzen_of_python = subprocess.check_output([\"python\", \"-c\", \"import this\"])\ncorpus = zen_of_python.split()\n\nnum_partitions = 3\nchunk = len(corpus) // num_partitions\npartitions = [corpus[i * chunk : (i + 1) * chunk] for i in range(num_partitions)]\n\n\ndef map_function(document):\n    for word in document.lower().split():\n        yield word, 1\n\n\n@ray.remote\ndef apply_map(corpus, num_partitions=3):\n    map_results = [list() for _ in range(num_partitions)]\n    for document in corpus:\n        for result in map_function(document):\n            first_letter = result[0].decode(\"utf-8\")[0]\n            word_index = ord(first_letter) % num_partitions\n            map_results[word_index].append(result)\n    return map_results\n\n\nmap_results = [\n    apply_map.options(num_returns=num_partitions).remote(data, num_partitions)\n    for data in partitions\n]\n\nfor i in range(num_partitions):\n    mapper_results = ray.get(map_results[i])\n    for j, result in enumerate(mapper_results):\n        print(f\"Mapper {i}, return value {j}: {result[:2]}\")\n```\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "pcmoritz",
        "body": "Thanks for reporting this -- can you say more about the exact uv environment you are running in (e.g. are you using a pyproject.toml and if yes, how does it look like).\n\nI've tried to repro your issue on Ray 2.47 on the Ray 2.47 docker image `anyscale/ray:2.47.0-py312-cu128` with just `uv run` and that works for me.\n\nI'm currently working on making the parsing of the uv run command line more robust (https://github.com/ray-project/ray/pull/53838), so if it is related to that I'd love to have a look if that fixes it or if anything more is needed."
      },
      {
        "user": "j93hahn",
        "body": "+1, I get something similar:\n```\n>>> uv run proteus describe ...\n/Users/jjahn/Desktop/Exa/monorepo/python/shared/exa_ml/.venv/lib/python3.10/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n2025-06-16 17:13:02,778 INFO worker.py:1908 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8266\n2025-06-16 17:13:02,817 INFO packaging.py:588 -- Creating a file package for local module '/Users/jjahn/Desktop/Exa/monorepo/python/shared/exa_ml'.\n2025-06-16 17:13:02,857 INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_b181dc551557e89f.zip' (2.49MiB) to Ray cluster...\n2025-06-16 17:13:02,860 INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_b181dc551557e89f.zip'.\n2025-06-16 17:13:03,998 INFO parquet_datasource.py:226 -- Filtered out 655 paths\n(raylet) warning: `VIRTUAL_ENV=/Users/jjahn/Desktop/Exa/monorepo/python/shared/exa_ml/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\n(raylet) Using CPython 3.10.15\n(raylet) Creating virtual environment at: .venv\n(raylet) error: Failed to generate package metadata for `dataset==0.1.0 @ directory+../dataset`\n(raylet)   Caused by: Distribution not found at: file:///private/tmp/ray/session_2025-06-16_17-13-01_243361_52182/runtime_resources/working_dir_files/dataset\n(raylet)   Caused by: error: Failed to generate package metadata for `dataset==0.1.0 @ directory+../dataset`\n(raylet) Distribution not found at: file:///private/tmp/ray/session_2025-06-16_17-13-01_243361_52182/runtime_resources/working_dir_files/dataset  Caused by: Distribution not found at: file:///private/tmp/ray/session_2025-06-16_17-13-01_243361_52182/runtime_resources/working_dir_files/dataset\n(raylet)\n(raylet) error: Failed to generate package metadata for `dataset==0.1.0 @ directory+../dataseterror: Failed to generate package metadata for ``\n(raylet) dataset==0.1.0 @ directory+../dataset`\n(raylet)   Caused by: Distribution not found at: file:///private/tmp/ray/session_2025-06-16_17-13-01_243361_52182/runtime_resources/working_dir_files/dataseterror: Failed to generate package metadata for `dataset==0.1.0 @ directory+../dataset\n(raylet) `\n(raylet)     Caused by: Caused by: Distribution not found at: file:///private/tmp/ray/session_2025-06-16_17-13-01_243361_52182/runtime_resources/working_dir_files/dataset\n(raylet) Distribution not found at: file:///private/tmp/ray/session_2025-06-16_17-13-01_243361_52182/runtime_resources/working_dir_files/dataset\n(raylet) [2025-06-16 17:14:04,172 E 52203 22370359] (raylet) worker_pool.cc:586: Some workers of the worker process(52223) have not registered within the timeout. The process is dead, probably it crashed during start.\n(raylet) warning: `VIRTUAL_ENV=/Users/jjahn/Desktop/Exa/monorepo/python/shared/exa_ml/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(raylet) error: Failed to generate package metadata for `dataset==0.1.0 @ directory+../dataset` [repeated 5x across cluster]\n(raylet)   Caused by: Distribution not found at: file:///private/tmp/ray/session_2025-06-16_17-13-01_243361_52182/runtime_resources/working_dir_files/dataset [repeated 4x across cluster]\nMetadata Fetch Progress 0:   0%|                                                                                                                                                        | 0.00/21.0 [01:39<?, ? task/s]Metadata Fetch Progress 0:   0%|                                                                                                                                                        | 0.00/21.0 [01:40<?, ? task/s]\n```"
      },
      {
        "user": "chrisk314",
        "body": "I am also encountering hanging with Ray 2.47.0 when running in Gitlab Actions on an ubuntu image with a uv venv. Both python 3.12.10 and 3.13.3 affected. Pytest starts but no tests run. No log output to speak of so difficult to know the exact cause; however, pinning Ray to 2.46.0 resolves the issue. Don't know if this is related to uv. Interestingly, I can run the test suite locally with the same setup (uv 0.7.13, python 3.13.3, ray 2.47.0) and it works fine. Quite mysterious...\n\nIn case it helps, for context, we have this pytest fixture in our top-level conftest.py which creates and tears down a Ray cluster for use throughout the test suite. Based on the fact that I can see the pytest message stating that it has \"collected X tests\", but I do not see a single log message with test progress info, I'm guessing it's hanging whilst setting up the fixtures (I could be wrong; I'm no expert on pytest internals).\n\n```python\n@pytest.fixture(scope=\"session\")\ndef ray_ctx() -> _t.Iterator[None]:\n    \"\"\"Initialises and shuts down Ray.\"\"\"\n    ray.init(num_cpus=2, num_gpus=0, include_dashboard=False)\n    yield\n    ray.shutdown()\n```"
      }
    ]
  },
  {
    "issue_number": 53876,
    "title": "Issue: Ray Dashboard Links to Grafana Return \"Dashboard Not Found\" (Windows)",
    "author": "Fatima-khanjar",
    "state": "open",
    "created_at": "2025-06-17T09:10:03Z",
    "updated_at": "2025-06-17T09:11:51Z",
    "labels": [
      "bug",
      "triage"
    ],
    "body": "Hi all,\n\nI'm running Ray on Windows, and I’ve successfully set up:\n- Ray (`ray start --head --metrics-export-port=8080`)\n- Prometheus, which works at `http://localhost:9090`\n- Grafana, accessible at `http://localhost:3000`\n- I also imported the default_dashboard.json manually from:\n`C:\\Users\\<my_user>\\AppData\\Local\\Temp\\ray\\session_latest\\metrics\\grafana\\default_dashboard.json`\n\nWhat’s working:\nThe Ray dashboard loads fine at `http://localhost:8265`\nPrometheus is running and collecting metrics\nGrafana loads and displays plots after manually importing the dashboard JSON\n\nThe issue:\nWhen I go to the Ray Dashboard, then click on \"View metrics in Grafana\", it opens Grafana but says \"Dashboard not found\"\n\nThe problem:\nFrom the Ray dashboard, when I click \"View metrics in Grafana\", Grafana opens but shows:\nDashboard not found\nSo the built-in Ray → Grafana link is broken, or the dashboard is not auto-registered.\n\nMy question:\nHow can I fix the broken Grafana link from the Ray dashboard?\n\nThanks in advance!",
    "comments": []
  },
  {
    "issue_number": 37293,
    "title": "[Core] Submitted containerized job is stuck in pending mode",
    "author": "stwerner97",
    "state": "open",
    "created_at": "2023-07-11T16:27:02Z",
    "updated_at": "2025-06-17T09:06:06Z",
    "labels": [
      "bug",
      "P3",
      "core",
      "core-runtime-env"
    ],
    "body": "### What happened + What you expected to happen\n\nHi, I want to use ray to submit containerized jobs to a kubernetes cluster. I've tried scheduling non-containerized jobs and it works fine. However, once I submit a containerized job, it is stuck in pending mode forever. The command below submits the job successfully, but is stuck in pending mode forever.\r\n\r\n```\r\nray job submit --address http://localhost:8265 --runtime-env-json='{\"container\": {\"image\": \"<my-cuda-docker-image>\", \"worker_path\": \"/root\"}}' -- nvidia-smi\r\n\r\nJob submission server address: http://localhost:8265\r\n\r\n-------------------------------------------------------\r\nJob 'raysubmit_KKgyZumhXYm1y3ng' submitted successfully\r\n-------------------------------------------------------\r\n\r\nNext steps\r\n  Query the logs of the job:\r\n    ray job logs raysubmit_KKgyZumhXYm1y3ng\r\n  Query the status of the job:\r\n    ray job status raysubmit_KKgyZumhXYm1y3ng\r\n  Request the job to be stopped:\r\n    ray job stop raysubmit_KKgyZumhXYm1y3ng\r\n\r\nTailing logs until the job exits (disable with --no-wait)\r\n```\r\n\r\nChecking the job status confirms this issue.\r\n```\r\nray job status raysubmit_KKgyZumhXYm1y3ng --address http://localhost:8265\r\n\r\nStatus for job 'raysubmit_KKgyZumhXYm1y3ng': PENDING\r\nStatus message: Job has not started yet. It may be waiting for the runtime environment to be set up.\r\n```\r\n\r\nTerminating the submitted job also does not work for me.\r\n```\r\nray job stop raysubmit_KKgyZumhXYm1y3ng --address http://localhost:8265\r\n\r\nJob submission server address: http://localhost:8265\r\nAttempting to stop job 'raysubmit_KKgyZumhXYm1y3ng'\r\nWaiting for job 'raysubmit_KKgyZumhXYm1y3ng' to exit (disable with --no-wait):\r\nJob has not exited yet. Status: PENDING\r\nJob has not exited yet. Status: PENDING\r\nJob has not exited yet. Status: PENDING\r\n```\r\n\n\n### Versions / Dependencies\n\nSome information on the Ray Kubernetes cluster that I am using.\r\n\r\n- The ``raycluster-kuberay-head`` uses the image ``rayproject/ray:2.3.0``.\r\n- The ``kuberay-operator`` uses the image ``kuberay/operator:v0.5.0``.\r\n- The ``raycluster-kuberay-head-svc`` service has the following targets\r\n    - ``app.kubernetes.io/created-by=kuberay-operator``: ``<ip-address>:10001  10001/TCP`` \r\n    - ``app.kubernetes.io/name=kuberay``: ``<ip-address>:6379  6379/TCP`` (this is the forwarded port)\r\n    - ``ray.io/cluster=raycluster-kuberay``: ``<ip-address>:8265  8265/TCP`` \r\n    - ``ray.io/identifier=raycluster-kuberay-head``: ``8080/TCP``\r\n    - ``ray.io/node-type=head``: ``<ip-address>:8000  8000/TCP``\r\n\r\nI forward the dashboard port via `` kubectl port-forward --address 0.0.0.0 service/raycluster-kuberay-head-svc 8265:8265``.\r\n\r\nAlso, I use ``ray, version 2.5.1``. \n\n### Reproduction script\n\nSetting up the Ray Kubernetes cluster:\r\n```\r\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.5.0\r\nhelm install raycluster kuberay/ray-cluster --version 0.5.0\r\n```\r\n\r\nSetting up the port forwarding:\r\n\r\n```\r\nkubectl port-forward --address 0.0.0.0 service/raycluster-kuberay-head-svc 8265:8265\r\n```\r\n\r\nSubmitting the job (the submitted job also remains pending with other images):\r\n\r\n```\r\nray job submit --address http://localhost:8265 --runtime-env-json='{\"container\": {\"image\": \"<my-cuda-docker-image>\", \"worker_path\": \"/root\"}}' -- nvidia-smi\r\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "kevin85421",
        "body": "I don't have experience setting up the `container` for the runtime environment. Based on the [Ray doc](https://docs.ray.io/en/latest/ray-core/api/doc/ray.runtime_env.RuntimeEnv.html#ray.runtime_env.RuntimeEnv), the Ray worker process will run in a container with the image specified by `container.image`. In my understanding, if you want to launch a container in a Pod, you may require setting some configurations for Pod's `securityContext` config. Would you mind sharing your use cases for launching a container in a Pod? \r\n"
      },
      {
        "user": "kevin85421",
        "body": "cc @architkulkarni "
      },
      {
        "user": "stwerner97",
        "body": "Hi @kevin85421, thanks for responding! 😊 \r\n\r\nI would like to train a neural network that needs some non-standard system packages to be installed, requires a specific cuda version, and so on. In my limited experience, this would be difficult to do in ray otherwise, correct? \r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 46043,
    "title": "CI test windows://python/ray/serve/tests:test_logging is flaky",
    "author": "can-anyscale",
    "state": "closed",
    "created_at": "2024-06-14T18:35:56Z",
    "updated_at": "2025-06-17T07:10:37Z",
    "labels": [
      "bug",
      "triage",
      "serve",
      "flaky-tracker",
      "ray-test-bot",
      "ci-test",
      "weekly-release-blocker",
      "stability"
    ],
    "body": "CI test **windows://python/ray/serve/tests:test_logging** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/4923#019017e1-a382-48c9-bc9a-59a170567313\n\t- https://buildkite.com/ray-project/postmerge/builds/4923#0190178e-516d-4e86-be1e-f5fdf9c8642f\n\nDataCaseName-windows://python/ray/serve/tests:test_logging-END\nManaged by OSS Test Policy",
    "comments": [
      {
        "user": "can-anyscale",
        "body": "Test passed on latest run: https://buildkite.com/ray-project/postmerge/builds/4953#01901a4f-531c-4b43-95a1-f984d59c7f14"
      },
      {
        "user": "can-anyscale",
        "body": "CI test **windows://python/ray/serve/tests:test_logging** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/6727#0192d9b8-3ae8-4964-9f1f-27198eecdf01\n\t- https://buildkite.com/ray-project/postmerge/builds/6727#0192d909-76b7-4832-a8da-b7294c71ca7c\n\nDataCaseName-windows://python/ray/serve/tests:test_logging-END\nManaged by OSS Test Policy"
      },
      {
        "user": "can-anyscale",
        "body": "passing now"
      }
    ]
  },
  {
    "issue_number": 53827,
    "title": "Release test llm_batch_vllm failed",
    "author": "can-anyscale",
    "state": "closed",
    "created_at": "2025-06-14T07:33:19Z",
    "updated_at": "2025-06-17T07:10:14Z",
    "labels": [
      "bug",
      "P0",
      "triage",
      "release-test",
      "llm",
      "ray-test-bot",
      "weekly-release-blocker",
      "stability"
    ],
    "body": "Release test **llm_batch_vllm** failed. See https://buildkite.com/ray-project/release/builds/45678#01976d4a-d4f4-4652-b4f8-bcb2a701ab70 for more details.\n\nManaged by OSS Test Policy",
    "comments": [
      {
        "user": "can-anyscale",
        "body": "Test passed on latest run: https://buildkite.com/ray-project/release/builds/45851#01977c8c-6d28-4187-88cf-a4f4d33fa910"
      }
    ]
  },
  {
    "issue_number": 25433,
    "title": "[Serve] Add timeout parameter for `deploy`  ",
    "author": "ArzelaAscoIi",
    "state": "closed",
    "created_at": "2022-06-03T07:13:51Z",
    "updated_at": "2025-06-17T06:57:27Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### Description\n\nIt would be helpful to have a public-facing timeout for [deploy](https://github.com/ray-project/ray/blob/99429b7a9201173fbd147202cec7d2018d295543/python/ray/serve/client.py#L240).\r\n\r\nOptionally: rollback the deployment request. \n\n### Use case\n\nSince this parameter is currently set to -1 by default and it is not passed to the deploy method. Without being able to set this parameter your process might end up waiting for more worker nodes, but your cluster already started 10/10 nodes. \r\nIn this case it would be helpful to set this timeout and retry later, since other deployments might have been removed. \r\n\r\nYou can find a draft of this here: https://github.com/ray-project/ray/compare/master...ArzelaAscoIi:enhancement/timeoutForServeDeployments\r\n(This does not include some rollback of the request) \r\n\r\nWorkarounds are: \r\n    1. First checking if there are sufficient available resources\r\n    2. Adding a manual timeout implementation <- this would not roll back the deployment request",
    "comments": [
      {
        "user": "cszhu",
        "body": "Hello! This P2 issue has seen no activity in at least 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 53806,
    "title": "Release test random_shuffle_fixed_size failed",
    "author": "can-anyscale",
    "state": "open",
    "created_at": "2025-06-13T16:07:16Z",
    "updated_at": "2025-06-17T06:40:04Z",
    "labels": [
      "bug",
      "P0",
      "triage",
      "data",
      "release-test",
      "jailed-test",
      "ray-test-bot",
      "weekly-release-blocker",
      "stability"
    ],
    "body": "Release test **random_shuffle_fixed_size** failed. See https://buildkite.com/ray-project/release/builds/45565#019767fd-8dae-45bd-bdbf-f65e5e7fa8a7 for more details.\n\nManaged by OSS Test Policy",
    "comments": [
      {
        "user": "can-anyscale",
        "body": "Blamed commit: 50fce2329d51ab19764b9dfe6540b6935baf01d0 found by bisect job https://buildkite.com/ray-project/release-tests-bisect/builds/2516"
      },
      {
        "user": "can-anyscale",
        "body": "Test has been failing for far too long. Jailing."
      }
    ]
  },
  {
    "issue_number": 53871,
    "title": "CI test linux://python/ray/tests:test_gpu_objects_nccl is consistently_failing",
    "author": "can-anyscale",
    "state": "open",
    "created_at": "2025-06-17T01:56:50Z",
    "updated_at": "2025-06-17T06:04:21Z",
    "labels": [
      "bug",
      "triage",
      "core",
      "flaky-tracker",
      "ray-test-bot",
      "ci-test",
      "weekly-release-blocker",
      "stability"
    ],
    "body": "CI test **linux://python/ray/tests:test_gpu_objects_nccl** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/10906#01977b68-a504-4b82-b15b-bb70f9a7e683\n\t- https://buildkite.com/ray-project/postmerge/builds/10906#01977b34-31f8-4bb8-a9fb-5387a6b6a839\n\nDataCaseName-linux://python/ray/tests:test_gpu_objects_nccl-END\nManaged by OSS Test Policy",
    "comments": [
      {
        "user": "can-anyscale",
        "body": "This test is now considered as flaky because it has been failing on postmerge for too long. Flaky tests do not run on premerge."
      }
    ]
  },
  {
    "issue_number": 53255,
    "title": "CI test linux://rllib:examples/evaluation/evaluation_parallel_to_training_multi_agent_duration_auto is consistently_failing",
    "author": "can-anyscale",
    "state": "closed",
    "created_at": "2025-05-23T02:24:23Z",
    "updated_at": "2025-06-17T05:47:21Z",
    "labels": [
      "bug",
      "triage",
      "rllib",
      "flaky-tracker",
      "ray-test-bot",
      "ci-test",
      "weekly-release-blocker",
      "stability"
    ],
    "body": "CI test **linux://rllib:examples/evaluation/evaluation_parallel_to_training_multi_agent_duration_auto** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/10325#0196fab5-a2f6-4d02-befc-5afcde8d7626\n\t- https://buildkite.com/ray-project/postmerge/builds/10325#0196fa77-0b71-445e-98a5-a255207aab53\n\t- https://buildkite.com/ray-project/postmerge/builds/10315#0196f99a-e612-43bf-8b78-5a739f0c4658\n\t- https://buildkite.com/ray-project/postmerge/builds/10166#0196d77d-0954-4a9b-987e-d1ae63ca4755\n\nDataCaseName-linux://rllib:examples/evaluation/evaluation_parallel_to_training_multi_agent_duration_auto-END\nManaged by OSS Test Policy",
    "comments": [
      {
        "user": "can-anyscale",
        "body": "Blamed commit: 2c1b283d5de69533d1356f9c9af129b13b72f043 found by bisect job https://buildkite.com/ray-project/release-tests-bisect/builds/2436"
      },
      {
        "user": "can-anyscale",
        "body": "This test is now considered as flaky because it has been failing on postmerge for too long. Flaky tests do not run on premerge."
      },
      {
        "user": "can-anyscale",
        "body": "Test passed on latest run: https://buildkite.com/ray-project/postmerge/builds/10332#0196fbb5-967c-4973-ac31-e3a3ff9c2a70"
      }
    ]
  },
  {
    "issue_number": 53079,
    "title": "Ray Serve Replica Initialization Timeout: STDOUT \"Failed to load\", RequestCancelledError, Likely Due to Slow/Crashing RLModule.from_checkpoint()",
    "author": "Sun-Moon-1314",
    "state": "open",
    "created_at": "2025-05-16T11:12:06Z",
    "updated_at": "2025-06-17T05:42:14Z",
    "labels": [
      "triage",
      "serve",
      "community-backlog"
    ],
    "body": "**Ray version:** 2.44.0\n**Python version:** 3.9.22\n**Operating System:** macOS Sequoia 15.4.1\n**Relevant library versions:**\n*   RLlib 2.44.0 (as part of Ray 2.44.0)\n*   PyTorch 2.5.1\n*   TensorFlow 2.19.0\n*   Gymnasium 1.0.0\n*   NumPy 1.26.0\n*   PyArrow 19.0.1\n\n**Bug Description:**\n\nI have a Ray Serve deployment named `ServeRLlibRLModule`. When attempting to view the STDOUT logs for its replicas in the Ray Dashboard, it shows \"Failed to load\".\n\nConcurrently, I observe two key error/warning messages:\n\n1.  From the `ServeController`:\n    ```\n    (ServeController pid=8814) WARNING 2025-05-16 19:04:44,408 controller 8814 -- Deployment 'ServeRLlibRLModule' in application 'default' has 1 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed.\n    ```\n    This indicates that the replica(s) for `ServeRLlibRLModule` are taking longer than 30 seconds to become healthy and ready.\n\n2.  From a `ProxyActor`:\n    ```\n    (ProxyActor pid=8813) Task exception was never retrieved\n    (ProxyActor pid=8813) future: <Task finished name='Task-64' coro=<ProxyResponseGenerator._await_response_anext() done, defined at .../ray/serve/_private/proxy_response_generator.py:115> exception=RequestCancelledError('8253f5dd-a217-4db5-ac8c-d5893e2e2d7e')>\n    (ProxyActor pid=8813) Traceback (most recent call last):\n    (ProxyActor pid=8813)   File \".../ray/serve/_private/proxy_response_generator.py\", line 116, in _await_response_anext\n    (ProxyActor pid=8813)     return await self._response.__anext__()\n    (ProxyActor pid=8813)   File \".../ray/serve/handle.py\", line 566, in __anext__\n    (ProxyActor pid=8813)     replica_result = await self._fetch_future_result_async()\n    (ProxyActor pid=8813)   File \".../ray/serve/handle.py\", line 287, in _fetch_future_result_async\n    (ProxyActor pid=8813)     raise RequestCancelledError(self.request_id) from None\n    (ProxyActor pid=8813) ray.serve.exceptions.RequestCancelledError: Request 8253f5dd-a217-4db5-ac8c-d5893e2e2d7e was cancelled.\n    ```\n    This suggests that requests to the `ServeRLlibRLModule` deployment are being cancelled, likely because no healthy replicas are available to process them due to the initialization timeout.\n\n**Key Finding:**\nIf I comment out the line `self.rl_module = RLModule.from_checkpoint(rl_module_checkpoint_path)` within the `__init__` method of my `ServeRLlibRLModule` deployment, and also comment out any code that uses `self.rl_module`, the deployment starts successfully, and STDOUT logs are viewable in the Dashboard. This strongly points to the `RLModule.from_checkpoint()` call as the source of the problem.\n\n**Expected Behavior:**\n\n1.  Replicas of `ServeRLlibRLModule` should initialize successfully within a reasonable timeframe.\n2.  STDOUT logs for the replicas should be accessible via the Ray Dashboard.\n3.  Requests to the deployment should be processed, not cancelled due to unavailable replicas.\n\n**Steps to Reproduce or Code Snippet:**\n\nMy `ServeRLlibRLModule` deployment is structured as follows. The issue occurs during the `RLModule.from_checkpoint()` call in `__init__`.\n\n```python\nfrom ray import serve\nfrom ray.rllib.core.rl_module.rl_module import RLModule # Ensure this import path is correct for your RLlib version\nimport traceback\nimport time # For debugging init duration\n\n@serve.deployment # (Potentially other configs like num_replicas, ray_actor_options)\nclass ServeRLlibRLModule:\n    def __init__(self, rl_module_checkpoint_path: str):\n        init_start_time = time.time()\n        print(f\"[DEBUG] ServeRLlibRLModule replica __init__ started. Attempting to load checkpoint from: {rl_module_checkpoint_path}\", flush=True)\n        try:\n            # CRITICAL STEP: Loading the RLModule from checkpoint\n            # This is where the process seems to hang or crash\n            self.rl_module = RLModule.from_checkpoint(rl_module_checkpoint_path)\n            print(f\"[DEBUG] ServeRLlibRLModule replica RLModule loaded successfully in {time.time() - init_start_time:.2f}s.\", flush=True)\n        except Exception as e:\n            error_message = f\"CRITICAL_ERROR: Failed to load RLModule from checkpoint '{rl_module_checkpoint_path}' after {time.time() - init_start_time:.2f}s. Error: {e}\\n\"\n            error_message += traceback.format_exc()\n            print(error_message, flush=True)\n            # Re-raise to ensure Serve knows the replica failed to initialize\n            raise RuntimeError(f\"Failed to initialize RLModule: {e}\") from e\n\n        print(f\"[DEBUG] ServeRLlibRLModule replica __init__ finished in {time.time() - init_start_time:.2f}s.\", flush=True)\n\n    async def __call__(self, request_data):\n        if not hasattr(self, 'rl_module') or self.rl_module is None:\n            print(\"[ERROR] RLModule not loaded, cannot process request.\", flush=True)\n            # Depending on application logic, return an error response\n            return {\"error\": \"Model not loaded\"}\n\n        print(\"[DEBUG] ServeRLlibRLModule replica __call__ received request.\", flush=True)\n        # ... inference using self.rl_module ...\n        # result = self.rl_module.forward_inference(request_data) # Example\n        # print(\"[DEBUG] ServeRLlibRLModule replica __call__ processed request.\", flush=True)\n        return {\"result\": \"some_output\"} # Example response\n\n# Example deployment (ensure path is correct)\n# rl_checkpoint_path = \"/path/to/your/actual/checkpoint\"\n# deployment = ServeRLlibRLModule.bind(rl_module_checkpoint_path=rl_checkpoint_path)\n# serve.run(deployment)\n```\n\n**Hypothesis / Analysis:**\n\nI suspect that the `RLModule.from_checkpoint()` call is either:\n1.  Extremely time-consuming, exceeding the default health check / initialization timeouts for Ray Serve replicas.\n2.  Hanging indefinitely due to an internal issue (e.g., deadlock, waiting for a resource).\n3.  Crashing the replica process silently (e.g., segmentation fault in underlying C++ code of PyTorch/TensorFlow, or OOM kill by the OS) without raising a Python exception that my `try...except` block can catch and print.\n\nThis failure/slowness in `__init__` prevents the replica from becoming healthy, leading to the `ServeController` warning, the \"Failed to load\" STDOUT in the dashboard (as the STDOUT stream might not be properly established or the replica dies too quickly), and subsequent `RequestCancelledError` for any incoming requests.\n\nEven with the added `try...except Exception as e:` block and `traceback.print_exc()`, I am not seeing the \"CRITICAL_ERROR\" messages in the `serve run` console output or easily in the dashboard STDOUT when the checkpoint loading is *not* commented out. This suggests the failure might be happening at a lower level or the logs are only going to specific worker files.\n\n**Troubleshooting Steps Taken:**\n\n1.  Confirmed that commenting out `RLModule.from_checkpoint()` allows the replica to start and STDOUT to be visible.\n2.  Added detailed `try...except` logging with `traceback.print_exc()` and `flush=True` to the `__init__` method.\n3.  Observed the `ServeController` timeout warning and `ProxyActor` `RequestCancelledError`.\n4.  Attempted to find the \"CRITICAL_ERROR\" logs, but they are not apparent in the main `serve run` output or dashboard when the issue occurs. I understand these might be in specific `worker-*.out` or `worker-*.err` files, which I will try to locate.\n5.  Plan to test `RLModule.from_checkpoint()` in a standalone Python script (outside of Serve) to measure its duration and observe for any direct errors/crashes.\n\n**Request:**\n\nCould you provide guidance on:\n*   How to better debug silent crashes or hangs within a Serve replica's `__init__` method, especially if standard Python `try...except` doesn't catch the issue?\n*   Are there known issues or best practices for loading large/complex RLlib `RLModule` checkpoints within Ray Serve replicas, particularly regarding initialization time, resource allocation (memory, CPU), or potential deadlocks?\n*   How to effectively retrieve detailed logs from a replica that fails very early in its lifecycle?\n\nAny insights or suggestions would be greatly appreciated. I can provide full log files if needed.\n\n[dashboard_agent.log](https://github.com/user-attachments/files/20242898/dashboard_agent.log)\n[dashboard.log](https://github.com/user-attachments/files/20242900/dashboard.log)\n[debug_state.txt](https://github.com/user-attachments/files/20242899/debug_state.txt)",
    "comments": []
  },
  {
    "issue_number": 53873,
    "title": "[serve.llm] LLM serving seems not working with mistral tokenizer.",
    "author": "kanwang",
    "state": "open",
    "created_at": "2025-06-17T04:04:20Z",
    "updated_at": "2025-06-17T05:19:26Z",
    "labels": [
      "bug",
      "triage"
    ],
    "body": "### What happened + What you expected to happen\n\nI tried serving a few mistral models like https://huggingface.co/mistralai/Devstral-Small-2505 or https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503. I've install are necessary dependencies (vllm==0.8.5.post1) but serving still wasn't working. From stack trace looks like it's from https://github.com/ray-project/ray/blob/master/python/ray/llm/_internal/serve/deployments/utils/node_initialization_utils.py#L151-L154 where we are trying to load the tokenizer. Could be because mistral models requires different tokenizer. \n\nThe suggestion vllm args are:\n```\nvllm serve mistralai/Devstral-Small-2505 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\n```\n\nStacktrace:\n```\nTraceback (most recent call last):\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/serve/_private/deployment_state.py\", line 694, in check_ready\n    ) = ray.get(self._ready_obj_ref)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2822, in get\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 930, in get_objects\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ServeReplica:llm_app:LLMDeployment:mistralai--devstral-small-2505.initialize_and_get_metadata()\u001b[39m (pid=1570, ip=10.112.221.167, actor_id=ab8257849af39417e37e0ae801000000, repr=<ray.serve._private.replica.ServeReplica:llm_app:LLMDeployment:mistralai--devstral-small-2505 object at 0x7a8cba05ab70>)\n  File \"/home/ray/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/serve/_private/replica.py\", line 984, in initialize_and_get_metadata\n    await self._replica_impl.initialize(deployment_config)\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/serve/_private/replica.py\", line 713, in initialize\n    raise RuntimeError(traceback.format_exc()) from None\nRuntimeError: Traceback (most recent call last):\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/serve/_private/replica.py\", line 690, in initialize\n    self._user_callable_asgi_app = await asyncio.wrap_future(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/serve/_private/replica.py\", line 1384, in initialize_callable\n    await self._call_func_or_gen(\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/serve/_private/replica.py\", line 1347, in _call_func_or_gen\n    result = await result\n             ^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/llm/_internal/serve/deployments/llm/llm_server.py\", line 440, in __init__\n    await asyncio.wait_for(self._start_engine(), timeout=ENGINE_START_TIMEOUT_S)\n  File \"/home/ray/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n    return await fut\n           ^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/llm/_internal/serve/deployments/llm/llm_server.py\", line 486, in _start_engine\n    await self.engine.start()\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/llm/_internal/serve/deployments/llm/vllm/vllm_engine.py\", line 232, in start\n    self.engine = await self._start_engine()\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/llm/_internal/serve/deployments/llm/vllm/vllm_engine.py\", line 271, in _start_engine\n    return await self._start_engine_v0()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/llm/_internal/serve/deployments/llm/vllm/vllm_engine.py\", line 364, in _start_engine_v0\n    ) = await self._prepare_engine_config(use_v1=False)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/llm/_internal/serve/deployments/llm/vllm/vllm_engine.py\", line 287, in _prepare_engine_config\n    node_initialization = await self.initialize_node(self.llm_config)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/llm/_internal/serve/deployments/llm/vllm/vllm_engine.py\", line 218, in initialize_node\n    return await initialize_node_util(llm_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/llm/_internal/serve/deployments/utils/node_initialization_utils.py\", line 109, in initialize_node\n    await _initialize_local_node(\n  File \"/home/ray/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/ray/llm/_internal/serve/deployments/utils/node_initialization_utils.py\", line 155, in _initialize_local_node\n    _ = transformers.AutoTokenizer.from_pretrained(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py\", line 1032, in from_pretrained\n    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2025, in from_pretrained\n    return cls._from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2063, in _from_pretrained\n    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2278, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/transformers/models/llama/tokenization_llama.py\", line 171, in __init__\n    self.sp_model = self.get_spm_processor(kwargs.pop(\"from_slow\", False))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/transformers/models/llama/tokenization_llama.py\", line 198, in get_spm_processor\n    tokenizer.Load(self.vocab_file)\n  File \"/home/ray/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py\", line 961, in Load\n    return self.LoadFromFile(model_file)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py\", line 316, in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: not a string\n```\n\n### Versions / Dependencies\n\n* vllm==0.8.5.post1\n* ray==2.46.0\n\n### Reproduction script\n\nserving config we used:\n```\nserveConfigV2:\n  http_options:\n    host: 0.0.0.0\n    port: 8000\n    request_timeout_s: 300\n    keep_alive_timeout_s: 10\n  logging_config:\n    encoding: JSON\n    log_level: INFO\n    logs_dir: null\n    enable_access_log: true\n  applications:\n  - name: llm_app\n    args:\n      llm_configs:\n        - model_loading_config:\n            model_id: \"mistralai/devstral-small-2505\"\n            model_source: \"/home/ray/llm/mistralai/devstral-small-2505\"\n          accelerator_type: \"L4\"\n          deployment_config:\n            max_ongoing_requests: 16\n            autoscaling_config:\n              target_ongoing_requests: 10\n              min_replicas: 2\n              max_replicas: 4\n              downscale_delay_s: 1200\n          engine_kwargs:\n            tensor_parallel_size: 4\n            pipeline_parallel_size: 1\n            gpu_memory_utilization: 0.95\n            max_model_len: 100000\n            tokenizer_mode: \"mistral\"\n            config_format: \"mistral\"\n            load_format: \"mistral\"\n            enable_chunked_prefill: true\n            enable_prefix_caching: true\n    import_path: ray.serve.llm:build_openai_app\n    name: llm_app\n    route_prefix: \"/\"\n```\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": []
  },
  {
    "issue_number": 52739,
    "title": "[Core] `ray.init()` and `ray start` fails on Windows 11 in ray 2.45+",
    "author": "PhilippWillms",
    "state": "open",
    "created_at": "2025-05-02T15:01:46Z",
    "updated_at": "2025-06-17T04:24:16Z",
    "labels": [
      "P0",
      "windows",
      "core",
      "stability"
    ],
    "body": "### What happened + What you expected to happen\n\nHello,\n\nafter installing latest release 2.45 - no upgrade, removed older release before - I notice following fundamental issue.\n\nCalling ray start --head in CLI leads to following console output.\n\n> (py311-raynew) C:\\Users\\Philipp>ray start --head\n> Usage stats collection is disabled.\n> \n> Local node IP: 192.168.178.26\n> 2025-05-02 16:53:36,888 ERROR services.py:1362 -- Failed to start the dashboard , return code 3221226505\n> 2025-05-02 16:53:36,888 ERROR services.py:1387 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.\n> 2025-05-02 16:53:36,888 ERROR services.py:1397 -- Couldn't read dashboard.log file. Error: 'utf-8' codec can't decode byte 0xfc in position 40: invalid start byte. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.\n> 2025-05-02 16:53:36,888 ERROR services.py:1431 --\n> The last 20 lines of C:\\Users\\Philipp\\AppData\\Local\\Temp\\ray\\session_2025-05-02_16-53-32_217596_35728\\logs\\dashboard.err (it contains the error message from the dashboard):\n> \n> Ray runtime started.\n\nI am surprised that the app says \"Ray runtime started\", but I do not get` ray status` feedback.\n\n> (py311-raynew) C:\\Users\\Philipp>ray status\n> No cluster status. It may take a few seconds for the Ray internal services to start up.\n\nContent of `dashboard.log` file\n\n> 2025-05-02 16:53:33,639\tERROR dashboard.py:305 -- The dashboard on node PHILIPP-MAIN failed with the following error:\n> Traceback (most recent call last):\n>   File \"C:\\Users\\Philipp\\anaconda3\\envs\\py311-raynew\\Lib\\site-packages\\ray\\dashboard\\dashboard.py\", line 247, in <module>\n>     logging_utils.redirect_stdout_stderr_if_needed(\n>   File \"C:\\Users\\Philipp\\anaconda3\\envs\\py311-raynew\\Lib\\site-packages\\ray\\_private\\logging_utils.py\", line 48, in redirect_stdout_stderr_if_needed\n>     sys.stderr = open_log(stderr_fileno, unbuffered=True, closefd=False)\n>                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>   File \"C:\\Users\\Philipp\\anaconda3\\envs\\py311-raynew\\Lib\\site-packages\\ray\\_private\\utils.py\", line 446, in open_log\n>     stream = open(path, **kwargs)\n>              ^^^^^^^^^^^^^^^^^^^^\n> OSError: [WinError 6] Das Handle ist ungültig\n\nContent of `dashboard.error` file\n\n> \u001b[0m\n\n### Versions / Dependencies\n\nray == 2.45\nWindows == 26100.3915\n\n### Reproduction script\n\n`> ray start --head`\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "kolcker",
        "body": "I'm experiencing the same issue. Some addtional information from raylet.log \n```\n[2025-05-04 12:06:23,685 I 32292 25604] (raylet.exe) agent_manager.cc:83: Agent process with name dashboard_agent exited, exit code 1.\n[2025-05-04 12:06:23,685 E 32292 25604] (raylet.exe) agent_manager.cc:87: The raylet exited immediately because one Ray agent failed, agent_name = dashboard_agent.\nThe raylet fate shares with the agent. This can happen because\n- The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.\n- The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/{dashboard_agent|runtime_env_agent}.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure.\n- The agent is killed by the OS (e.g., out of memory).\n[2025-05-04 12:06:23,686 I 32292 35296] (raylet.exe) main.cc:307: Raylet graceful shutdown triggered, reason = UNEXPECTED_TERMINATION, reason message = dashboard_agent failed and raylet fate-shares with it.\n[2025-05-04 12:06:23,686 I 32292 35296] (raylet.exe) main.cc:310: Shutting down...\n[2025-05-04 12:06:23,686 I 32292 35296] (raylet.exe) accessor.cc:515: Unregistering node node_id=2547f168cb62f79248f1ebdb4d408b1f1ac93d33f1d41244737eb966\n[2025-05-04 12:06:23,689 I 32292 35296] (raylet.exe) accessor.cc:528: Finished unregistering node info, status = OK node_id=2547f168cb62f79248f1ebdb4d408b1f1ac93d33f1d41244737eb966\n[2025-05-04 12:06:23,689 W 32292 19800] (raylet.exe) store.cc:368: Disconnecting client due to connection error with code 2: End of file\n[2025-05-04 12:06:23,695 I 32292 35296] (raylet.exe) agent_manager.cc:116: Killing agent dashboard_agent, pid 8496.\n[2025-05-04 12:06:23,695 I 32292 35296] (raylet.exe) agent_manager.cc:116: Killing agent runtime_env_agent, pid 6348.\n[2025-05-04 12:06:23,696 I 32292 9940] (raylet.exe) agent_manager.cc:83: Agent process with name runtime_env_agent exited, exit code 1067.\n[2025-05-04 12:06:23,697 I 32292 35296] (raylet.exe) io_service_pool.cc:48: IOServicePool is stopped.\n\u001b[0m\u001b[0m[2025-05-04 12:06:23,915 I 32292 35296] (raylet.exe) stats.h:120: Stats module has shutdown.\n```\nUpdating of grpcio version to 1.72 doesn't help. \nCurrent environment works fine with ray 2.44.1. \n\n**Versions / Dependencies**\nray == 2.46\nWindows == 26100.3915\n\n**Issue Severity**\nHigh: It blocks me from completing my task.\n\nLet me know if any additional information will be helpful. "
      },
      {
        "user": "kolcker",
        "body": "Dear @masoudcharkhabi, @sven1977, @simonsays1980, \nIt looks like, this issue is a **blocker** for users that are making fast experiments on the Windows OS. Issue appears from ray version 2.45. As I said in my previous post ray 2.44.1 is stable. This is for sure not **utf-8 decoding issue**.\n\nScript to reproduce issue. Disabling the dashboard doesn't help. The same behavior for ray.init()\n```\nimport ray\nray.init(include_dashboard=False)\n```\nScript error. The same exception could be found in the dashboard.log\n```\n2025-05-14 12:45:04,169\tERROR services.py:1362 -- Failed to start the dashboard , return code 3221226505\n2025-05-14 12:45:04,169\tERROR services.py:1387 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.\n2025-05-14 12:45:04,174\tERROR services.py:1431 -- \nThe last 20 lines of C:\\WINDOWS\\TEMP\\ray\\session_2025-05-14_12-44-59_804590_5292\\logs\\dashboard.log (it contains the error message from the dashboard): \nTraceback (most recent call last):\n  File \"E:\\python_venvs\\prs\\.venv\\Lib\\site-packages\\ray\\dashboard\\dashboard.py\", line 247, in <module>\n    logging_utils.redirect_stdout_stderr_if_needed(\n  File \"E:\\python_venvs\\prs\\.venv\\Lib\\site-packages\\ray\\_private\\logging_utils.py\", line 48, in redirect_stdout_stderr_if_needed\n    sys.stderr = open_log(stderr_fileno, unbuffered=True, closefd=False)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\python_venvs\\prs\\.venv\\Lib\\site-packages\\ray\\_private\\utils.py\", line 446, in open_log\n    stream = open(path, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^\nOSError: [WinError 6] The handle is invalid\n\n2025-05-14 12:45:04,326\tINFO worker.py:1888 -- Started a local Ray instance.\n[2025-05-14 12:45:07,189 E 5292 16792] core_worker.cc:513: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Unknown error worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff\n```\nLog raylet.err\n```\n....\n[2025-05-14 13:08:45,360 I 32516 14120] (raylet.exe) worker_pool.cc:527: Started worker process with pid 27440, the token is 26\n[2025-05-14 13:08:45,364 I 32516 14120] (raylet.exe) worker_pool.cc:527: Started worker process with pid 9280, the token is 27\n[2025-05-14 13:08:45,368 I 32516 14120] (raylet.exe) worker_pool.cc:527: Started worker process with pid 2608, the token is 28\n[2025-05-14 13:08:45,372 I 32516 14120] (raylet.exe) worker_pool.cc:527: Started worker process with pid 6768, the token is 29\n[2025-05-14 13:08:45,376 I 32516 14120] (raylet.exe) worker_pool.cc:527: Started worker process with pid 31592, the token is 30\n[2025-05-14 13:08:45,380 I 32516 14120] (raylet.exe) worker_pool.cc:527: Started worker process with pid 32684, the token is 31\n[2025-05-14 13:08:47,073 I 32516 11484] (raylet.exe) agent_manager.cc:83: Agent process with name dashboard_agent exited, exit code -1073740791.\n[2025-05-14 13:08:47,073 E 32516 11484] (raylet.exe) agent_manager.cc:87: The raylet exited immediately because one Ray agent failed, agent_name = dashboard_agent.\nThe raylet fate shares with the agent. This can happen because\n- The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.\n- The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/{dashboard_agent|runtime_env_agent}.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure.\n- The agent is killed by the OS (e.g., out of memory).\n[2025-05-14 13:08:47,075 I 32516 14120] (raylet.exe) main.cc:307: Raylet graceful shutdown triggered, reason = UNEXPECTED_TERMINATION, reason message = dashboard_agent failed and raylet fate-shares with it.\n[2025-05-14 13:08:47,075 I 32516 14120] (raylet.exe) main.cc:310: Shutting down...\n[2025-05-14 13:08:47,075 I 32516 14120] (raylet.exe) accessor.cc:515: Unregistering node node_id=b98f0b68d473db175d08c36ea95ca35ce7abbea5dd07edb0f301695f\n[2025-05-14 13:08:47,080 I 32516 14120] (raylet.exe) accessor.cc:528: Finished unregistering node info, status = OK node_id=b98f0b68d473db175d08c36ea95ca35ce7abbea5dd07edb0f301695f\n[2025-05-14 13:08:47,080 W 32516 31440] (raylet.exe) store.cc:368: Disconnecting client due to connection error with code 2: End of file\n[2025-05-14 13:08:47,115 I 32516 14120] (raylet.exe) agent_manager.cc:116: Killing agent dashboard_agent, pid 27564.\n[2025-05-14 13:08:47,115 I 32516 14120] (raylet.exe) agent_manager.cc:116: Killing agent runtime_env_agent, pid 15252.\n[2025-05-14 13:08:47,122 I 32516 9552] (raylet.exe) agent_manager.cc:83: Agent process with name runtime_env_agent exited, exit code 1067.\n[2025-05-14 13:08:47,123 I 32516 14120] (raylet.exe) io_service_pool.cc:48: IOServicePool is stopped.\n[2025-05-14 13:08:47,277 I 32516 14120] (raylet.exe) stats.h:120: Stats module has shutdown.\n[2025-05-14 13:08:47,278 E 32516 14120] (raylet.exe) logging.cc:496: *** SIGSEGV received at time=1747217327 ***\n....\n```\nhere is results from pip freeze\n```\nabsl-py==2.2.2\naiohappyeyeballs==2.6.1\naiohttp==3.11.18\naiohttp-cors==0.8.1\naiosignal==1.3.2\nannotated-types==0.7.0\nattrs==25.3.0\ncachetools==5.5.2\ncertifi==2025.4.26\ncharset-normalizer==3.4.2\nclick==8.2.0\ncloudpickle==3.1.1\ncolorama==0.4.6\ncolorful==0.5.6\ncontourpy==1.3.2\ncycler==0.12.1\ndistlib==0.3.9\ndm-tree==0.1.9\nFarama-Notifications==0.0.4\nfilelock==3.18.0\nfonttools==4.58.0\nfrozenlist==1.6.0\nfsspec==2025.3.2\ngoogle-api-core==2.24.2\ngoogle-auth==2.40.1\ngoogleapis-common-protos==1.70.0\ngrpcio==1.71.0\ngymnasium==1.0.0\nidna==3.10\nJinja2==3.1.4\njoblib==1.5.0\njsonschema==4.23.0\njsonschema-specifications==2025.4.1\nkiwisolver==1.4.8\nlz4==4.4.4\nMarkupSafe==2.1.5\nmatplotlib==3.10.3\nmpmath==1.3.0\nmsgpack==1.1.0\nmultidict==6.4.3\nnetworkx==3.4.2\nnumpy==2.2.5\nopencensus==0.11.4\nopencensus-context==0.1.3\normsgpack==1.7.0\npackaging==25.0\npandas==2.2.3\npillow==11.2.1\nplatformdirs==4.3.8\nprometheus_client==0.21.1\npropcache==0.3.1\nproto-plus==1.26.1\nprotobuf==6.30.2\npy-spy==0.4.0\npyarrow==20.0.0\npyasn1==0.6.1\npyasn1_modules==0.4.2\npydantic==2.11.4\npydantic_core==2.33.2\npyparsing==3.2.3\npython-dateutil==2.9.0.post0\npytz==2025.2\nPyYAML==6.0.2\nray==2.46.0\nreferencing==0.36.2\nrequests==2.32.3\nrpds-py==0.24.0\nrsa==4.9.1\nscikit-learn==1.6.1\nscipy==1.15.3\nsix==1.17.0\nsmart-open==7.1.0\nsympy==1.13.3\nta==0.11.0\ntensorboardX==2.6.2.2\nthreadpoolctl==3.6.0\ntorch==2.8.0.dev20250512+cu128\ntorchaudio==2.6.0.dev20250513+cu128\ntorchvision==0.22.0.dev20250513+cu128\ntyping-inspection==0.4.0\ntyping_extensions==4.13.2\ntzdata==2025.2\nurllib3==2.4.0\nvirtualenv==20.31.2\nwrapt==1.17.2\nyarl==1.20.0\n```\nAdditional information:\n\n- Windows OS Build: 26100.2605\n- Experiment isolation: I made a clean environment and reinstalled all ray packages and dependencies to potentially prevent problems with particular updates of packages.\n- Updating grpcio from 1.71.0 to 1.72.0 or decreasing down to grpcio==1.66.2 doesn't help.\n- This is not an antivirus, firewall, out of memory or permission issue.\n- This is for sure not an utf-8 decoding issue.\n\nExpected result:\n\n- Increase priority for this issue. I guess it's P0/P1. It blocks migration to 2.46 for several departments. \n- Fix regression issue and unblock possibility of using ray 2.45+ on the Windows OS\n- Fix issue with include_dashboard=False for ray.init. I guess it should disable the dashboard.\n- Please provide a workaround, if fixing this issue requires a lot of time.\n\nPlease let me know if any additional information will be helpful.\nThanks in advance."
      },
      {
        "user": "PhilippWillms",
        "body": "@kolcker: Thanks for your contribution to the error defintion. Changed the title of the GH issue accordingly."
      }
    ]
  },
  {
    "issue_number": 52571,
    "title": "[Core] Read-only buffer error in some scikit-learn models",
    "author": "wingkitlee0",
    "state": "closed",
    "created_at": "2025-04-24T02:49:37Z",
    "updated_at": "2025-06-17T03:22:04Z",
    "labels": [
      "question",
      "P2",
      "core",
      "community-backlog"
    ],
    "body": "### What happened + What you expected to happen\n\nI got some \"buffer source array is read-only\" error when using certain scikit-learn models.\n* related to Cython / serialization? it seems to affect when the sklearn model is a C library.\n* If I pickled the model explicity it works (see example)\n* ray.put does not help\n* multiprocessing.Pool would work; but Ray's Pool does not\n\n```bash\nray.exceptions.RayTaskError(ValueError): ray::func() (pid=45108, ip=172.25.209.242)\n  File \"repro1.py\", line 23, in func\n    y_pred = model.predict_proba(X_eval)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"lib/python3.12/site-packages/sklearn/svm/_base.py\", line 869, in predict_proba\n    return pred_proba(X)\n           ^^^^^^^^^^^^^\n  File \"lib/python3.12/site-packages/sklearn/svm/_base.py\", line 909, in _dense_predict_proba\n    pprob = libsvm.predict_proba(\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"sklearn/svm/_libsvm.pyx\", line 475, in sklearn.svm._libsvm.predict_proba\n  File \"stringsource\", line 660, in View.MemoryView.memoryview_cwrapper\n  File \"stringsource\", line 350, in View.MemoryView.memoryview.__cinit__\nValueError: buffer source array is read-only\n```\n\nSo there is a workaround (pickle). but I am not sure if it's a Ray problem or scikit-learn. I cannot reproduce the same error in pure scikit-learn despite there were similar (now fixed) bugs in sklearn.\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\nThe following script works fine when the pickling option is used.\n\n```python\nimport argparse\nimport pickle\nimport ray\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\n\n\ndef create_model():\n    X, y = make_classification(n_samples=1000)\n\n    model = SVC(kernel=\"linear\", probability=True)\n    model.fit(X, y)\n\n    return model\n\n@ray.remote\ndef func(model: SVC | bytes):\n    if isinstance(model, bytes):\n        model = pickle.loads(model)\n\n    X_eval, _ = make_classification(n_samples=1000)\n\n    y_pred = model.predict_proba(X_eval)\n\n    return y_pred\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-p\", action=\"store_true\", help=\"use pickle to load model\")\n    args = parser.parse_args()\n\n    model = create_model()\n\n    if args.p:\n        model = pickle.dumps(model)\n\n    refs = [\n        func.remote(model)\n        for _ in range(2)\n    ]\n\n    ray.get(refs)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "chuang0221",
        "body": "I think the issue occurs because Ray's serialization system intentionally makes all buffers immutable (read-only) for distributed computing safety as shown in the `python/ray/includes/serialization.pxi`:\n\n```python\ndef __getbuffer__(self, Py_buffer* buffer, int flags):\n    if flags & cpython.PyBUF_WRITABLE:\n        # Ray ensures all buffers are immutable.\n        raise BufferError\n    buffer.readonly = self.readonly\n```"
      },
      {
        "user": "wingkitlee0",
        "body": "Thanks for the info. Is it correct to say that it's a scikit-learn bug that those variables were not declared as read-only?"
      },
      {
        "user": "chuang0221",
        "body": "Well, I don't think so. I think it's an intentional design choice on both sides that happens to conflict:\n\n- Ray intentionally makes all shared buffers read-only for memory safety in distributed computing.\n- scikit-learn's libsvm implementation in Cython requires writable buffers for its internal operations during `predict_proba`. I think it's because it needs to perform calculations in-place for efficiency.\n\nThis type of conflict could occur with other libraries that use Cython or C extensions that expect writable buffers for in-place operations."
      }
    ]
  },
  {
    "issue_number": 53872,
    "title": "[Core] ray.ActorID.nil().job_id",
    "author": "minerharry",
    "state": "open",
    "created_at": "2025-06-17T02:37:33Z",
    "updated_at": "2025-06-17T03:13:29Z",
    "labels": [
      "bug",
      "triage"
    ],
    "body": "### What happened + What you expected to happen\n\nAccessing the job_id field of a nil ActorID (just the getattribute, not even executing the method) results in a full python crash instead of raising an appropriate exception.\n\n### Versions / Dependencies\n\nTested on ray 3.0.0.dev0 and 2.47.0, python 3.9 and 3.10\n\n### Reproduction script\n\nRepro code:\n```python\nfrom ray import ActorID\nActorId.nil().job_id\n```\n\n\nOutput: [sometimes the stacktrace just says \"Unknown\", the following is from a self-built install)\n```\n[2025-06-16 22:29:27,301 C 92497 92497] id.cc:160:  Check failed: !IsNil() \n*** StackTrace Information ***\n/home/miner/github/ray/python/ray/_raylet.so(+0x1612377) [0x76e597612377] ray::operator<<()\n/home/miner/github/ray/python/ray/_raylet.so(_ZN3ray6RayLogD1Ev+0x514) [0x76e597616124] ray::RayLog::~RayLog()\n/home/miner/github/ray/python/ray/_raylet.so(_ZNK3ray7ActorID5JobIdEv+0x10f) [0x76e59745888f] ray::ActorID::JobId()\n/home/miner/github/ray/python/ray/_raylet.so(+0x859dbb) [0x76e596859dbb] __pyx_getprop_3ray_7_raylet_7ActorID_job_id()\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyObject_GenericGetAttrWithDict+0x2d6) [0x5ffe508c5cd6] _PyObject_GenericGetAttrWithDict\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x733) [0x5ffe508b7763] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5ffe508b61e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalCodeWithName+0x48) [0x5ffe508b5e98] _PyEval_EvalCodeWithName\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyEval_EvalCodeEx+0x39) [0x5ffe508b5e49] PyEval_EvalCodeEx\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyEval_EvalCode+0x1b) [0x5ffe5096074b] PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1d4324) [0x5ffe50965324] builtin_exec\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x137163) [0x5ffe508c8163] cfunction_vectorcall_FASTCALL\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x3af) [0x5ffe508b73df] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1d6cd2) [0x5ffe50967cd2] gen_send_ex\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x68e4) [0x5ffe508bd914] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1d6cd2) [0x5ffe50967cd2] gen_send_ex\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x68e4) [0x5ffe508bd914] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1d6cd2) [0x5ffe50967cd2] gen_send_ex\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1421ac) [0x5ffe508d31ac] method_vectorcall_O\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5ffe508b7694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5ffe508c79fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x3af) [0x5ffe508b73df] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5ffe508c79fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5ffe508b7694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5ffe508b61e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x143b6b) [0x5ffe508d4b6b] method_vectorcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x1167) [0x5ffe508b8197] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5ffe508c79fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5ffe508b7694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5ffe508c79fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5ffe508b7694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5ffe508c79fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5ffe508b7694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5ffe508b61e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x143b6b) [0x5ffe508d4b6b] method_vectorcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyObject_Call+0xbc) [0x5ffe508d52ec] PyObject_Call\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x369d) [0x5ffe508ba6cd] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5ffe508b61e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyFunction_Vectorcall+0xd9) [0x5ffe508c7759] _PyFunction_Vectorcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x3af) [0x5ffe508b73df] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5ffe508b61e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalCodeWithName+0x48) [0x5ffe508b5e98] _PyEval_EvalCodeWithName\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyEval_EvalCodeEx+0x39) [0x5ffe508b5e49] PyEval_EvalCodeEx\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyEval_EvalCode+0x1b) [0x5ffe5096074b] PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1fcc2a) [0x5ffe5098dc2a] run_eval_code_obj\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1f90f3) [0x5ffe5098a0f3] run_mod\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x942c4) [0x5ffe508252c4] pyrun_file.cold\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyRun_SimpleFileExFlags+0x1c0) [0x5ffe50984020] PyRun_SimpleFileExFlags\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(Py_RunMain+0x344) [0x5ffe509815e4] Py_RunMain\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(Py_BytesMain+0x37) [0x5ffe50954577] Py_BytesMain\n/lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca) [0x76e59f82a1ca]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b) [0x76e59f82a28b] __libc_start_main\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1c348e) [0x5ffe5095448e]\n```\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "minerharry",
        "body": "Interestingly, this also happens for TaskID.nil().job_id(), but only when you execute the method - unlike ActorID.nil().job_id, simply accessing TaskID.nil().job_id doesn't crash. It also doesn't crash on TaskID.nil().actor_id or actor_id() - those just return a nil actorid. perhaps a nil TaskID / ActorID should also return nil JobIDs?\n\nStack trace for TaskID.nil().job_id():\n```\n[2025-06-16 23:09:58,727 C 95161 95161] id.cc:160:  Check failed: !IsNil() \n*** StackTrace Information ***\n/home/miner/github/ray/python/ray/_raylet.so(+0x1612377) [0x787217812377] ray::operator<<()\n/home/miner/github/ray/python/ray/_raylet.so(_ZN3ray6RayLogD1Ev+0x514) [0x787217816124] ray::RayLog::~RayLog()\n/home/miner/github/ray/python/ray/_raylet.so(_ZNK3ray7ActorID5JobIdEv+0x10f) [0x78721765888f] ray::ActorID::JobId()\n/home/miner/github/ray/python/ray/_raylet.so(_ZNK3ray6TaskID5JobIdEv+0x33) [0x787217658933] ray::TaskID::JobId()\n/home/miner/github/ray/python/ray/_raylet.so(+0x86e600) [0x787216a6e600] __pyx_pw_3ray_7_raylet_6TaskID_15job_id()\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5b0914579694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5b09145781e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalCodeWithName+0x48) [0x5b0914577e98] _PyEval_EvalCodeWithName\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyEval_EvalCodeEx+0x39) [0x5b0914577e49] PyEval_EvalCodeEx\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyEval_EvalCode+0x1b) [0x5b091462274b] PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1d4324) [0x5b0914627324] builtin_exec\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x137163) [0x5b091458a163] cfunction_vectorcall_FASTCALL\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x3af) [0x5b09145793df] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1d6cd2) [0x5b0914629cd2] gen_send_ex\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x68e4) [0x5b091457f914] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1d6cd2) [0x5b0914629cd2] gen_send_ex\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x68e4) [0x5b091457f914] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1d6cd2) [0x5b0914629cd2] gen_send_ex\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1421ac) [0x5b09145951ac] method_vectorcall_O\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5b0914579694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5b09145899fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x3af) [0x5b09145793df] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5b09145899fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5b0914579694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5b09145781e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x143b6b) [0x5b0914596b6b] method_vectorcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x1167) [0x5b091457a197] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5b09145899fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5b0914579694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5b09145899fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5b0914579694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1369fa) [0x5b09145899fa] function_code_fastcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x664) [0x5b0914579694] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5b09145781e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x143b6b) [0x5b0914596b6b] method_vectorcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyObject_Call+0xbc) [0x5b09145972ec] PyObject_Call\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x369d) [0x5b091457c6cd] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5b09145781e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyFunction_Vectorcall+0xd9) [0x5b0914589759] _PyFunction_Vectorcall\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalFrameDefault+0x3af) [0x5b09145793df] _PyEval_EvalFrameDefault\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1251e5) [0x5b09145781e5] _PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(_PyEval_EvalCodeWithName+0x48) [0x5b0914577e98] _PyEval_EvalCodeWithName\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyEval_EvalCodeEx+0x39) [0x5b0914577e49] PyEval_EvalCodeEx\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyEval_EvalCode+0x1b) [0x5b091462274b] PyEval_EvalCode\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1fcc2a) [0x5b091464fc2a] run_eval_code_obj\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1f90f3) [0x5b091464c0f3] run_mod\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x942c4) [0x5b09144e72c4] pyrun_file.cold\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(PyRun_SimpleFileExFlags+0x1c0) [0x5b0914646020] PyRun_SimpleFileExFlags\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(Py_RunMain+0x344) [0x5b09146435e4] Py_RunMain\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(Py_BytesMain+0x37) [0x5b0914616577] Py_BytesMain\n/lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca) [0x78722142a1ca]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b) [0x78722142a28b] __libc_start_main\n/home/miner/miniforge3/envs/ray_wsl_develop/bin/python3.9(+0x1c348e) [0x5b091461648e]\n```"
      }
    ]
  },
  {
    "issue_number": 33997,
    "title": "[RLlib] TorchDistributionWrapper Typing Information Should Be Changed ",
    "author": "mechanyx",
    "state": "open",
    "created_at": "2023-04-02T22:36:20Z",
    "updated_at": "2025-06-17T03:01:53Z",
    "labels": [
      "bug",
      "P2",
      "rllib"
    ],
    "body": "### What happened + What you expected to happen\n\nThe code functions but the typing for TorchDistributionWrapper's constructor in torch_action_dist.py should be improved. The code in question is:\r\n\r\n```\r\n    def __init__(self, inputs: List[TensorType], model: TorchModelV2):\r\n        if not isinstance(inputs, torch.Tensor):\r\n```\r\n\r\nThe argument typing says you have to pass a list and then on the next line the code checks if it's something that's not a list. Python's isinstance will not check members of collections so isinstance([1,2,3], int) will return False.\r\n\r\nSo either this function can be expecting inputs to be a Tensor in which case you probably want something like:\r\n\r\n`    def __init__(self, inputs: Union[TensorType, List[TensorType]], model: TorchModelV2): `\r\n\r\nOr it doesn't in which case the `if not isinstance` check should be removed as it's always True as a List is not a Tensor.\r\n\n\n### Versions / Dependencies\n\nThis code is currently on HEAD and I found it reading the 2.2 source.\n\n### Reproduction script\n\nThis is a code/documentation quality/clarity problem, not a functionality issue. You just read the code.\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      },
      {
        "user": "mechanyx",
        "body": "None of the code has been updated so the type hints are still wrong."
      },
      {
        "user": "mechanyx",
        "body": "I don't believe I have permissions to remove the pending-cleanup tag."
      }
    ]
  },
  {
    "issue_number": 35976,
    "title": "[Core] DecodeError when `ray.put` a large (2GB) object",
    "author": "messense",
    "state": "open",
    "created_at": "2023-06-01T08:51:23Z",
    "updated_at": "2025-06-17T02:32:50Z",
    "labels": [
      "bug",
      "P3",
      "core-client",
      "core",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nWhen calling `ray.put` on a large object (size >= 2GB) in client mode, python process segfaults in `protobuf` 4.x library. Although it works fine with `protobuf` 3.20.\r\n\r\nI have a fix for the segfault in https://github.com/protocolbuffers/upb/pull/1338, but even with that patch `ray.put` raise `DecodeError` from `protobuf` library so it doesn't make it work on large objects.\r\n\r\nTo me it seems that ray should implement chunked put in https://github.com/ray-project/ray/blob/609b8e6151c190d1b5f18b2bfb0d2495b63e994e/python/ray/util/client/worker.py#L498-L514\n\n### Versions / Dependencies\n\n```bash\r\n$ ray --version\r\nray, version 2.4.0\r\n\r\n$ cat /etc/os-release\r\nPRETTY_NAME=\"Ubuntu 22.04.2 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.2 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n\r\n$ python -c 'from google import protobuf; print(protobuf.__version__)'\r\n4.23.2\r\n```\n\n### Reproduction script\n\n```python\r\nfrom ray.core.generated.ray_client_pb2 import PutRequest, DataRequest\r\n\r\n# data size of 2**31 bytes (2GB)\r\n# doesn't error when data size <= 2147483646\r\nreq = PutRequest(data=b\"\\0\" * 2147483648)\r\ndatareq = DataRequest(put=req)\r\n```\r\n\r\nCode minimized from https://github.com/ray-project/ray/blob/609b8e6151c190d1b5f18b2bfb0d2495b63e994e/python/ray/util/client/worker.py#L480-L514 so it doesn't need to include a call to `ray.put`.\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "rkooo567",
        "body": "cc @ckw017 "
      },
      {
        "user": "cszhu",
        "body": "Hello! This P3 issue has seen no activity in at least a year. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      },
      {
        "user": "messense",
        "body": "@cszhu usually issue creator outside of this github org does not have permission to update/remove issue tags so I can't simply remove the `pending-cleanup` label.\n\nPlease make it clear whether the removing label action is for issue creator or your team members."
      }
    ]
  },
  {
    "issue_number": 28038,
    "title": "[core] ray stop --force doesn't kill processes on worker node",
    "author": "QiTianyu-0403",
    "state": "closed",
    "created_at": "2022-08-21T14:05:34Z",
    "updated_at": "2025-06-17T02:23:23Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-worker",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nI set up a Ray cluster with multiple virtual machines. When I want to terminate the cluster, I try `ray stop`. But it tells me that some Ray processes cannot be terminated. So I try `ray stop --force`. However, the error is still reported. The error results are as follows:\r\n```\r\nStopped only 0 out of 4 Ray processes within the grace period 10 seconds. Set `-v` to see more details. Remaining processes [psutil.Process(pid=406135, name='gcs_server', status='zombie', started='2022-08-17 12:11:07'), psutil.Process(pid=406183, name='raylet', status='zombie', started='2022-08-17 12:11:09'), psutil.Process(pid=22942, name='gcs_server', status='zombie', started='2022-08-15 10:23:09'), psutil.Process(pid=22990, name='raylet', status='zombie', started='2022-08-15 10:23:12')] will be forcefully terminated.\r\nYou can also use `--force` to forcefully terminate processes or set higher `--grace-period` to wait longer time for proper termination.\r\n```\r\nNow I don't know how to kill these processes.\n\n### Versions / Dependencies\n\nOS: Ubuntu 20.04.4\r\nPython: Python 3.7\r\nRay: 1.12.1\n\n### Reproduction script\n\n1. First I start a ray cluster. In head node I run `ray start --head --port=6379`. In worker node, I run `ray start --address='xxx.xxx.xxx.xxx:6379'`. And it works.\r\n2. When I want to stop the cluster after training, I try `ray stop --force` in every nodes. But it has error:\r\n```\r\nStopped only 0 out of 4 Ray processes within the grace period 10 seconds. Set `-v` to see more details. Remaining processes [psutil.Process(pid=406135, name='gcs_server', status='zombie', started='2022-08-17 12:11:07'), psutil.Process(pid=406183, name='raylet', status='zombie', started='2022-08-17 12:11:09'), psutil.Process(pid=22942, name='gcs_server', status='zombie', started='2022-08-15 10:23:09'), psutil.Process(pid=22990, name='raylet', status='zombie', started='2022-08-15 10:23:12')] will be forcefully terminated.\r\nYou can also use `--force` to forcefully terminate processes or set higher `--grace-period` to wait longer time for proper termination.\r\n```\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 51643,
    "title": "[core][gpu-objects] Support streaming to overlap computation / communication",
    "author": "kevin85421",
    "state": "open",
    "created_at": "2025-03-24T17:06:08Z",
    "updated_at": "2025-06-17T01:27:07Z",
    "labels": [
      "enhancement",
      "P1",
      "core",
      "gpu-objects",
      "community-backlog",
      "pending-cleanup"
    ],
    "body": "### Description\n\nThis is not required to be in the POC of PD disaggregation, but it's important.\n\n```python\n# prefill actor\ndef forward(self):\n  for ....:\n     kv_cache = gen_kv_cache(...)\n     ...\n     ...\n```\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "stephanie-wang",
        "body": "@kevin85421, this issue needs more description. What exactly is it proposing and is it different from #51277?"
      }
    ]
  },
  {
    "issue_number": 51279,
    "title": "[core][gpu-objects] Allow tensor metadata to be specified ahead of time for improved performance",
    "author": "kevin85421",
    "state": "open",
    "created_at": "2025-03-11T23:00:41Z",
    "updated_at": "2025-06-17T01:25:12Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "gpu-objects",
      "community-backlog"
    ],
    "body": "### Description\n\nSpecifying shape ahead of time, so then we don’t need to wait for sender to finish the task before triggering receive.\n\n\n### Use case\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 51550,
    "title": "[core][gpu-objects] Support TensorDict",
    "author": "kevin85421",
    "state": "closed",
    "created_at": "2025-03-20T06:48:46Z",
    "updated_at": "2025-06-17T01:23:50Z",
    "labels": [
      "enhancement",
      "P1",
      "core",
      "gpu-objects",
      "verl-poc",
      "community-backlog"
    ],
    "body": "TensorDict uses a different serialization method from torch.Tensor. We should support extracting CUDA TensorDicts in the same way that we do for torch.Tensor.",
    "comments": [
      {
        "user": "stephanie-wang",
        "body": "Duplicate of #52340."
      }
    ]
  },
  {
    "issue_number": 53622,
    "title": "[core][gpu-objects] Allocate placeholder tensor on corresponding devices",
    "author": "kevin85421",
    "state": "closed",
    "created_at": "2025-06-06T20:03:38Z",
    "updated_at": "2025-06-17T01:17:37Z",
    "labels": [
      "bug",
      "P1",
      "gpu-objects"
    ],
    "body": "### What happened + What you expected to happen\n\nhttps://github.com/ray-project/ray/blob/d4dc0818e4119a76ea445e8fef5fcf5bac22ac45/python/ray/_private/gpu_object_manager.py#L125\n\nCurrently, we always allocate tensors on the CPU. However, if we use NCCL to transfer GPU tensors, dist.recv will hang indefinitely. We should allocate placeholder tensors on the correct devices depending on the situation.\n\n### Versions / Dependencies\n\nnightly\n\n### Reproduction script\n\nTODO\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "stephanie-wang",
        "body": "Closed by #53720."
      }
    ]
  },
  {
    "issue_number": 53694,
    "title": "[<Ray component: Core|RLlib|etc...>] SAC config error about framework",
    "author": "mahao18cm",
    "state": "open",
    "created_at": "2025-06-10T07:41:13Z",
    "updated_at": "2025-06-17T01:17:07Z",
    "labels": [
      "bug",
      "question",
      "P3",
      "rllib",
      "usability",
      "stability"
    ],
    "body": "### What happened + What you expected to happen\n\nWhen i tried to use SAC to train my model, the error happened. \n\n### Versions / Dependencies\n\nRay:Version: 2.46.0\npython:Python 3.10.17\n\n### Reproduction script\n\nagent, checkpoint_path = load_policy(algo, env_name, load_policy_path, env_config=env_config, seed=seed)\ndef load_policy(algo, env_name, policy_path=None, seed=0, env_config={}, eval=False):\n    if algo == 'ppo':\n        agent = ppo.PPOTrainer(setup_config(algo, seed, env_config, eval=eval), env_name,\n                               logger_creator=custom_log_creator(\"**\", env_name)\n        )\n    elif algo == 'sac':\n        config = setup_config(algo, seed, env_config, eval=eval)\n        agent = SAC(config=config,  \n                               logger_creator=custom_log_creator(\"**\", env_name)\n        )\nWhen i tried to use  agent = SAC(config=config,  logger_creator=custom_log_creator(\"**\", env_name), the error happened. The EnvRunnerGroup's setup method self._local_config = local_config_raw.framework(tf_session_args=config.copy(copy_frozen=False)). The return error TypeError: 'str' object is not callable. However my config type is <class 'ray.rllib.algorithms.sac.sac.SACConfig'>\n\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "mahao18cm",
        "body": "Anyone help?"
      }
    ]
  },
  {
    "issue_number": 51264,
    "title": "[core][gpu-objects] Driver should order all collective calls to avoid deadlock",
    "author": "kevin85421",
    "state": "closed",
    "created_at": "2025-03-11T18:58:48Z",
    "updated_at": "2025-06-17T01:17:03Z",
    "labels": [
      "enhancement",
      "P0",
      "core",
      "gpu-objects",
      "community-backlog"
    ],
    "body": "### Description\n\nSimilar to compiled graphs, the driver should order all collective calls to avoid deadlocks.\n\nExample 1:\n* Avoid passing tensors within the same actor using NCCL. Instead, we should access the in-actor store directly.\n\nExample 2: Both actors are single-threaded and synchronous. If `t1_1` is the input for `t2_2` and `t1_2` is the input for `t2_1`, both use NCCL to transfer data. In this case, we should call NCCL recv of `t2_2` before `t2_1` to avoid deadlock.\n\n```\nActor 1: t1_1, t1_2\nActor 2: t2_1, t2_2\n```\n\nNote: Check if this will work if we only have one CUDA stream.\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "stephanie-wang",
        "body": "This is already supported in the current PoC, because Ray will guarantee actor task ordering for tasks called by the same caller (the driver)."
      }
    ]
  },
  {
    "issue_number": 47264,
    "title": "CI test linux://rllib:learning_tests_multi_agent_pendulum_sac_multi_cpu is flaky",
    "author": "can-anyscale",
    "state": "open",
    "created_at": "2024-08-21T21:15:32Z",
    "updated_at": "2025-06-17T01:16:42Z",
    "labels": [
      "bug",
      "P0",
      "rllib",
      "flaky-tracker",
      "ray-test-bot",
      "ci-test",
      "stability"
    ],
    "body": "CI test **linux://rllib:learning_tests_multi_agent_pendulum_sac_multi_cpu** is flaky. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/5957#019175b2-3966-428f-8fc6-e137df2eeee8\n\nDataCaseName-linux://rllib:learning_tests_multi_agent_pendulum_sac_multi_cpu-END\nManaged by OSS Test Policy",
    "comments": [
      {
        "user": "can-anyscale",
        "body": "Test passed on latest run: https://buildkite.com/ray-project/postmerge/builds/5963#0191768d-a2ca-473c-8eac-94255acf2c28"
      },
      {
        "user": "can-anyscale",
        "body": "CI test **linux://rllib:learning_tests_multi_agent_pendulum_sac_multi_cpu** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/6041#0191979e-9232-48bf-a58d-022007f216ca\n\t- https://buildkite.com/ray-project/postmerge/builds/6041#01919761-a4d6-4797-aefd-b1462c8857f4\n\t- https://buildkite.com/ray-project/postmerge/builds/6020#01919279-36fa-4b79-bf56-ca0b293fe68e\n\nDataCaseName-linux://rllib:learning_tests_multi_agent_pendulum_sac_multi_cpu-END\nManaged by OSS Test Policy"
      },
      {
        "user": "can-anyscale",
        "body": "Test passed on latest run: https://buildkite.com/ray-project/postmerge/builds/6045#019199be-ed1b-44a5-a2cd-49501556243f"
      }
    ]
  },
  {
    "issue_number": 48859,
    "title": "CI test linux://python/ray/data:test_arrow_block is flaky",
    "author": "can-anyscale",
    "state": "open",
    "created_at": "2024-11-22T01:55:06Z",
    "updated_at": "2025-06-17T00:52:33Z",
    "labels": [
      "bug",
      "triage",
      "data",
      "flaky-tracker",
      "ray-test-bot",
      "ci-test",
      "weekly-release-blocker",
      "stability",
      "community-backlog"
    ],
    "body": "CI test **linux://python/ray/data:test_arrow_block** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/7044#0193516b-8294-4cd4-876a-3a1fb4a75b8d\n\t- https://buildkite.com/ray-project/postmerge/builds/7044#0193516b-828a-4616-bb10-361e671bac7d\n\t- https://buildkite.com/ray-project/postmerge/builds/7044#0193516b-8291-4d34-b16f-aa49c468a288\n\t- https://buildkite.com/ray-project/postmerge/builds/7039#0193508d-7303-4e19-ae91-25d603e9ea3c\n\nDataCaseName-linux://python/ray/data:test_arrow_block-END\nManaged by OSS Test Policy",
    "comments": [
      {
        "user": "can-anyscale",
        "body": "Test passed on latest run: https://buildkite.com/ray-project/postmerge/builds/7044#01935193-b59a-4a0d-bcfa-0feb962aafc8"
      },
      {
        "user": "can-anyscale",
        "body": "CI test **linux://python/ray/data:test_arrow_block** is flaky. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/7044#0193516b-8294-4cd4-876a-3a1fb4a75b8d\n\t- https://buildkite.com/ray-project/postmerge/builds/7044#0193516b-828a-4616-bb10-361e671bac7d\n\t- https://buildkite.com/ray-project/postmerge/builds/7044#0193516b-8291-4d34-b16f-aa49c468a288\n\t- https://buildkite.com/ray-project/postmerge/builds/7039#0193508d-7303-4e19-ae91-25d603e9ea3c\n\nDataCaseName-linux://python/ray/data:test_arrow_block-END\nManaged by OSS Test Policy"
      },
      {
        "user": "can-anyscale",
        "body": "Test passed on latest run: https://buildkite.com/ray-project/postmerge/builds/7051#01935372-ce67-48e0-8de8-3111974952dc"
      }
    ]
  },
  {
    "issue_number": 43796,
    "title": "CI test windows://python/ray/tests:test_object_spilling_debug_mode is consistently_failing",
    "author": "can-anyscale",
    "state": "closed",
    "created_at": "2024-03-07T22:35:40Z",
    "updated_at": "2025-06-17T00:29:52Z",
    "labels": [
      "bug",
      "P1",
      "windows",
      "core",
      "flaky-tracker",
      "ray-test-bot",
      "ci-test",
      "stability"
    ],
    "body": "CI test **windows://python/ray/tests:test_object_spilling_debug_mode** is flaky. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/3349#018e1a82-759b-4a52-8a1d-c226d8f2c781\n\t- https://buildkite.com/ray-project/postmerge/builds/3328#018e158e-2623-4866-a633-2944f9743958\n\t- https://buildkite.com/ray-project/postmerge/builds/3294#018e1066-e148-417c-9b0c-88cbbb5061fe\n\t- https://buildkite.com/ray-project/postmerge/builds/3283#018e0e47-a4ec-4338-85b4-7244a2b983bb\n\t- https://buildkite.com/ray-project/postmerge/builds/3270#018e0beb-8a71-45d1-a8b0-cee890be6c6f\n\nDataCaseName-windows://python/ray/tests:test_object_spilling_debug_mode-END\nManaged by OSS Test Policy",
    "comments": [
      {
        "user": "can-anyscale",
        "body": "Test passed on latest run: https://buildkite.com/ray-project/postmerge/builds/3484#018e3982-641e-4748-8493-8c412e2f3312"
      },
      {
        "user": "can-anyscale",
        "body": "CI test **windows://python/ray/tests:test_object_spilling_debug_mode** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/4923#0190178e-5173-477b-a1b6-75b831f139c4\n\t- https://buildkite.com/ray-project/postmerge/builds/4923#0190178e-5173-477b-a1b6-75b831f139c4\n\t- https://buildkite.com/ray-project/postmerge/builds/4814#019004c1-44c0-48c9-9cdb-02b2d71a6556\n\nDataCaseName-windows://python/ray/tests:test_object_spilling_debug_mode-END\nManaged by OSS Test Policy"
      },
      {
        "user": "can-anyscale",
        "body": "This test is now considered as flaky because it has been failing on postmerge for too long. Flaky tests do not run on premerge."
      }
    ]
  },
  {
    "issue_number": 45962,
    "title": "CI test windows://python/ray/tests:test_object_spilling_asan is consistently_failing",
    "author": "can-anyscale",
    "state": "closed",
    "created_at": "2024-06-14T17:25:11Z",
    "updated_at": "2025-06-17T00:29:48Z",
    "labels": [
      "bug",
      "triage",
      "core",
      "flaky-tracker",
      "ray-test-bot",
      "ci-test",
      "stability"
    ],
    "body": "CI test **windows://python/ray/tests:test_object_spilling_asan** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge/builds/4923#0190178e-5173-477b-a1b6-75b831f139c4\n\t- https://buildkite.com/ray-project/postmerge/builds/4923#0190178e-5173-477b-a1b6-75b831f139c4\n\nDataCaseName-windows://python/ray/tests:test_object_spilling_asan-END\nManaged by OSS Test Policy",
    "comments": [
      {
        "user": "can-anyscale",
        "body": "This test is now considered as flaky because it has been failing on postmerge for too long. Flaky tests do not run on premerge."
      },
      {
        "user": "can-anyscale",
        "body": "fix is merging now, let's close and let the next run confirm"
      },
      {
        "user": "can-anyscale",
        "body": "Test passed on latest run: https://buildkite.com/ray-project/postmerge/builds/4953#01901a4f-5320-4166-88d9-481caddeed15"
      }
    ]
  },
  {
    "issue_number": 35838,
    "title": "[air/output] Jupyter notebook trial result table keeps swapping column order",
    "author": "justinvyu",
    "state": "open",
    "created_at": "2023-05-27T02:07:55Z",
    "updated_at": "2025-06-17T00:19:55Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "UX",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nAs more trial results come in, the trial result table column order keeps ping-ponging. See video:\r\n\r\nhttps://github.com/ray-project/ray/assets/3887863/ab399226-445a-4d40-98c8-5fb758ac18bd\r\n\r\ncc @krfricke \n\n### Versions / Dependencies\n\n2.4\n\n### Reproduction script\n\nN/A\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35788,
    "title": "[RLlib] Make Learner more standalone with regards to LearnerHyperparameters",
    "author": "ArturNiederfahrenhorst",
    "state": "open",
    "created_at": "2023-05-25T19:13:49Z",
    "updated_at": "2025-06-17T00:19:54Z",
    "labels": [
      "enhancement",
      "P2",
      "rllib",
      "pending-cleanup"
    ],
    "body": "### Description\n\nToday, LearnerHyperparameters, AlgorithmConfig and Learner relate as follows:\r\n\r\n1) PPOLearner needs `PPOLearnerHyperparameters`.\r\n2) `LearnerHyperparameters` does not have sensible (only Nones there).\r\n3) An instance of `PPOLearnerHyperparameters` with sensible defaults can only be gotten from the `PPOConfig`.\r\n4) When instantiating `PPOLearner` without providing this argument, it defaults to `LearnerHyperparameters()`.\r\n5) This default is not helpful because it lacks HPs to run PPOLearner.\r\n6) Instead, we should default to `PPOLearnerHyperparameters()` inside `PPOLearner`.\r\n7) Because of 6 `PPOLearnerHyperparameters()` must have sensible defaults without 3)\r\n8) Because of 7 AlgorithmConfig should get it's defaults from PPOLearnerHyperparameters.\r\n\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35721,
    "title": "[AIR] `on_trial_complete` callback hook happens before trial resources are freed",
    "author": "justinvyu",
    "state": "open",
    "created_at": "2023-05-24T17:01:22Z",
    "updated_at": "2025-06-17T00:19:52Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nWhen implementing a custom `Callback`, users may do some long computation `on_trial_complete`. This happens before the actors associated with this trial get killed. It might make sense to call this hook after the trial has been stopped.\r\n\r\nhttps://github.com/ray-project/ray/blob/8e49d2aa54426842430f70c7060c3c6ac0f513f3/python/ray/tune/execution/trial_runner.py#L692-L710\r\n\r\n\r\nIn general, what should callbacks be recommended for?\r\n- Having long computation on ANY callback hook would lead to the Tune main event loop being blocked. This prevents **other trials** from progressing.\r\n- In the current state, callbacks are mostly meant to read some state and **not do any significant work.**\r\n- Callbacks that need to do work must do it in a separate process, which needs to be implemented by the callback itself. See `SyncerCallback` for an example.\r\n\r\n### Use case\r\n\r\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35681,
    "title": "[core] Failed to close sockets in CoreWorker when crash.",
    "author": "fishbone",
    "state": "open",
    "created_at": "2023-05-23T21:21:42Z",
    "updated_at": "2025-06-17T00:19:50Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-worker",
      "core-correctness",
      "stability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nFailed to close sockets in CoreWorker could potentially ends up with socket not being closed in gRPC in a timely way.\r\n\r\nThere are two sockets:\r\n- GcsClient\r\n- Pubsub long poling.\r\n\r\nWhen worker exits, we need to close them.\r\n\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\ntest_advanced_9 sometimes is flakey due to this.\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35600,
    "title": "[serve][dashboard] Show last line instead of first line in Serve app status message",
    "author": "edoakes",
    "state": "open",
    "created_at": "2023-05-22T13:33:43Z",
    "updated_at": "2025-06-17T00:19:48Z",
    "labels": [
      "enhancement",
      "P2",
      "dashboard",
      "pending-cleanup"
    ],
    "body": "Typically when there's an error, these status messages include a traceback. That means the last line is the most valuable. Example:\r\n\r\n<img width=\"445\" alt=\"Screen Shot 2023-05-22 at 8 32 47 AM\" src=\"https://github.com/ray-project/ray/assets/9871461/2f58b32d-c807-42ba-b7a5-94b1015b11e3\">\r\n\r\n \r\n<img width=\"988\" alt=\"Screen Shot 2023-05-22 at 8 32 55 AM\" src=\"https://github.com/ray-project/ray/assets/9871461/5e051f49-3bde-4167-8dfa-313f9fe9eff7\">\r\n\r\nIf the un-expanded version was `AttributeError: module 'foo' has no attribute 'bar'` I'd know exactly the issue at a glance.",
    "comments": [
      {
        "user": "edoakes",
        "body": "@alanwguo @scottsun94 a small UX friction I noticed while dogfooding"
      },
      {
        "user": "edoakes",
        "body": "Also I wonder if there's way to filter out the weird prefix:\r\n`\u001b[36mray::deploy_serve_application()\u001b[39m (pid=10048, ip=10.0.12.213)`\r\n\r\n@zcin thinking about this makes me realize we should do exactly the same exception handling that you did for the replica constructor for the `deploy_serve_application` task (avoid serialization errors and improve the tracebacks)."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35499,
    "title": "Ray Data - Glob/wildcard in file path",
    "author": "jbpdl22",
    "state": "open",
    "created_at": "2023-05-18T16:04:49Z",
    "updated_at": "2025-06-17T00:19:45Z",
    "labels": [
      "enhancement",
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "### Description\n\nAdd the ability to use widcards in the file path for a dataset.  I use this daily in spark.\n\n### Use case\n\nI have prefixes in s3 with 10ks of files.  When testing, I often work with a subset of these files before creating a job to process the entire prefex.  To achieve this, I would like to be able to use a wildcard.\r\n\r\nExample:\r\ns3://my_data/part-00000.<lots of stuff>.json.snappy\r\n...\r\ns3://my_data/part-50000.<lots of stuff>.json.snappy\r\n\r\nIn order to select ~100 files, I should be able to give a pattern something like:\r\ns3://my_data/part-000*.json.snappy",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35496,
    "title": "[serve] Document how to silence access logs from GradioIngress",
    "author": "zcin",
    "state": "open",
    "created_at": "2023-05-18T15:21:27Z",
    "updated_at": "2025-06-17T00:19:43Z",
    "labels": [
      "P2",
      "serve",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nAccess logs are printed at a really high rate for the serve-gradio integrated deployment, `GradioIngress`. Users will likely be interested in how to silence these messages; we should add this to the documentation.\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35492,
    "title": "[RLlib] Windows CLI, cmd.exe, powershell parsing json arguments JSONDecodeError",
    "author": "I3lacx",
    "state": "open",
    "created_at": "2023-05-18T10:24:44Z",
    "updated_at": "2025-06-17T00:19:41Z",
    "labels": [
      "bug",
      "P2",
      "windows",
      "rllib",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nFollowing the installation on the [Getting started page](https://docs.ray.io/en/latest/rllib/rllib-training.html).\r\nStarting with a clean python 3.8.16 anaconda environment on Windows 10 running in **cmd.exe**, following these steps:\r\n```bash\r\npip install \"ray[rllib]\" tensorflow\r\nrllib train --algo DQN --env CartPole-v1 --stop '{\"training_iteration\": 30}'\r\n```\r\nAfter solving some issues mentioned in #35491 I receive:\r\n```\r\nNo such command \"30}'\".\r\n```\r\n\r\nWhen running the command through powershell, the error is:\r\n```\r\nJSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\r\n```\r\nWhen removing `--stop '{\"training_iteration\": 30}'` from the arguments, the training starts. Following the same steps on a Windows 11 system causes the same issue. Following the same steps in WSL works without issue.\n\n### Versions / Dependencies\n\nTested on Windows 10, Windows 11\r\nUsing cmd.exe, powershell \r\nPython=3.8\r\n\r\n[output_pip_list.txt](https://github.com/ray-project/ray/files/11506843/output_pip_list.txt)\r\n\n\n### Reproduction script\n\nStarting with a clean python 3.8.16 anaconda environment on Windows 10 running in **cmd.exe**, following the steps from the [Getting started page](https://docs.ray.io/en/latest/rllib/rllib-training.html) + hotfix for missing dependency:\r\n```bash\r\npip install \"ray[rllib]\" tensorflow tensorflow-probabilities\r\nrllib train --algo DQN --env CartPole-v1 --stop '{\"training_iteration\": 30}'\r\n```\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "Rohan138",
        "body": "Try escaping the `\\\"`; so `'{\\\"training_iteration\\\": 30}'`"
      },
      {
        "user": "I3lacx",
        "body": "Thanks, this trick worked in powershell, so running:\r\n`rllib train --algo DQN --env CartPole-v1 --stop '{\\\"training_iteration\\\": 30}' --framework tf2`\r\nis running for me on windows.\r\n\r\nIt did not work for cmd, but I found a different workaround (which does not work for powershell). Using `\"` outside and `\"\"` inside seems to pass them correctly to the code:\r\n`rllib train --algo DQN --env CartPole-v1 --stop \"{\"\"training_iteration\"\": 30}\" --framework tf2`\r\n\r\nThe issue is resolved for me, and I guess this can not really be fixed through code, so maybe some form of documentation can be added to guide windows users?"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35448,
    "title": "[RayClient]large object transfer failure ",
    "author": "yuduber",
    "state": "open",
    "created_at": "2023-05-17T18:14:00Z",
    "updated_at": "2025-06-17T00:19:39Z",
    "labels": [
      "bug",
      "P2",
      "core-client",
      "core",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nWe are training large models and we use ray client to connect to our Ray cluster and very often we need to send large model over the wire from ray client to ray cluster.  The transfer behavior is unpredictable and easy to fail for large objects.\r\n\r\n### Versions / Dependencies\r\n\r\nRay 2.3.0\r\nPython3.6\r\n\r\n### Reproduction script\r\n\r\n```\r\n# start ray cluster on remote machines, get the ip and port of this ray cluster\r\nimport ray\r\nray.init(address='ray://ip:port')\r\n\r\n# if we try smaller b, and have a 2GB array, we will\r\n# have no issue and succeed in about 2 minutes to pass the array to \r\n# Ray cluster and complete calculation\r\n# but with larger array like 8GB, it will stuck and never succeed.\r\nb = int(1e9)\r\n# 1000,000,000 = 1000,000K = 1000M = 1G\r\n# below will create 8GB array\r\nbig_array = []\r\nfor i in range(b):\r\n    big_array.append(i)\r\n\r\n@ray.remote\r\ndef sum_array(x):\r\n    return sum(x)\r\n\r\n# will stuck here for longer than 30 minutes and fail\r\n# I expect it takes about 10 minutes to finish\r\nsum_big_array = sum_array.remote(big_array)\r\n\r\n```\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "raulchen",
        "body": "@yuduber I confirmed with @ckw017. There was no Ray Client changes between Ray 1.13 and 2.0 that may affect object transfer. \r\n\r\nIn the mean time, you can also try Ray Job, which is supposed for submitting job dependencies to the cluster. It should be more reliable than Ray Client in your case. "
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35424,
    "title": "[train] Simplify `test_transformers_trainer_steps::test_e2e_steps`",
    "author": "matthewdeng",
    "state": "open",
    "created_at": "2023-05-17T05:34:31Z",
    "updated_at": "2025-06-17T00:19:37Z",
    "labels": [
      "P2",
      "train",
      "pending-cleanup"
    ],
    "body": "#35270 causes importing `ray.train.huggingface` to import `accelerate`, which takes approximately 4.5 seconds. Since this test is parameterized to run 20x times, this causes the total time to run this test to increase the threshold from `large` to `enormous`. \r\n\r\nTo reduce the time for this test, we can take a few approaches:\r\n1. Reduce the general runtime of the test by plugging in a dummy model.\r\n2. Reduce the number of configurations we are testing.\r\n3. Move the import of `accelerate` to be further lazy and only import when it is actually needed.\r\n\r\n",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35422,
    "title": "[Core] Reducing scheduling fragmentation",
    "author": "jjyao",
    "state": "open",
    "created_at": "2023-05-17T04:18:42Z",
    "updated_at": "2025-06-17T00:19:35Z",
    "labels": [
      "enhancement",
      "P2",
      "core-scheduler",
      "pending-cleanup"
    ],
    "body": "### Description\n\nFragmentation can happen when the cluster has enough total resources to schedule a task but no single node has enough resources.\r\n\r\nWays to reduce the change of fragmentation are like avoiding GPUs node for scheduling CPU tasks.\r\n\r\nPG can also have fragmentation just like other resources: #35409\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35409,
    "title": "[Core, RLlib] Multi GPU RLlib experiment is unable to be scheduled.",
    "author": "avnishn",
    "state": "open",
    "created_at": "2023-05-16T22:32:32Z",
    "updated_at": "2025-06-17T00:19:33Z",
    "labels": [
      "bug",
      "P2",
      "rllib",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\n\r\n\r\nAbove is a script to reproduce the problem. I am running on a the following cluster:\r\nhttps://console.anyscale.com/o/anyscale-internal/workspaces/expwrk_rexsdhckwvn3wltbtxwce57a77/ses_qstkpd5ej9qjmle94esjcl6nyr\r\n\r\nwhich has the following cluster compute layout\r\n```\r\n======== Autoscaler status: 2023-05-16 15:20:23.259600 ========\r\nGCS request time: 0.001424s\r\nNode Provider non_terminated_nodes time: 0.003899s\r\n\r\nNode status\r\n---------------------------------------------------------------\r\nHealthy:\r\n 4 worker-node-type-1\r\n 1 head-node-type\r\nPending:\r\n (no pending nodes)\r\nRecent failures:\r\n (no failures)\r\n\r\nResources\r\n---------------------------------------------------------------\r\nTotal Usage:\r\n 0.0/32.0 CPU (0 used of 0.0 reserved in placement groups)\r\n 0.0/4.0 GPU (0 used of 0.0 reserved in placement groups)\r\n 0.0/4.0 accelerator_type:T4\r\n 0B/74.07GiB memory\r\n 0B/34.19GiB object_store_memory\r\n\r\nTotal Demands:\r\n (no resource demands)\r\n\r\nNode: 10.0.60.35\r\n Usage:\r\n  0.0/16.0 CPU\r\n  0B/34.23GiB memory\r\n  0B/17.11GiB object_store_memory\r\n\r\nNode: 10.0.14.120\r\n Usage:\r\n  0.0/4.0 CPU (0 used of 0.0 reserved in placement groups)\r\n  0.0/1.0 GPU (0 used of 0.0 reserved in placement groups)\r\n  0.0/1.0 accelerator_type:T4\r\n  0B/9.97GiB memory\r\n  0B/4.27GiB object_store_memory\r\n\r\nNode: 10.0.52.205\r\n Usage:\r\n  0.0/4.0 CPU (0 used of 0.0 reserved in placement groups)\r\n  0.0/1.0 GPU (0 used of 0.0 reserved in placement groups)\r\n  0.0/1.0 accelerator_type:T4\r\n  0B/9.97GiB memory\r\n  0B/4.27GiB object_store_memory\r\n\r\nNode: 10.0.35.146\r\n Usage:\r\n  0.0/4.0 CPU\r\n  0.0/1.0 GPU\r\n  0.0/1.0 accelerator_type:T4\r\n  0B/9.97GiB memory\r\n  0B/4.27GiB object_store_memory\r\n\r\nNode: 10.0.39.211\r\n Usage:\r\n  0.0/4.0 CPU\r\n  0.0/1.0 GPU\r\n  0.0/1.0 accelerator_type:T4\r\n  0B/9.95GiB memory\r\n  0B/4.26GiB object_store_memory\r\n```\r\n\r\nI'm trying to run a script that creates a placement group that looks like the following:\r\n`[{\"CPU:1, \"GPU: 0\"}, {\"CPU:1, \"GPU: 0\"}, {\"CPU:1, \"GPU: 1\"}, {\"CPU:1, \"GPU: 1\"}]` and when I run this one of my gpu actors is never created.\r\n\r\nWhen I run ray status I see the following:\r\n```\r\n======== Autoscaler status: 2023-05-16 15:25:29.495778 ========\r\nGCS request time: 0.001564s\r\nNode Provider non_terminated_nodes time: 0.004192s\r\n\r\nNode status\r\n---------------------------------------------------------------\r\nHealthy:\r\n 1 head-node-type\r\n 4 worker-node-type-1\r\nPending:\r\n (no pending nodes)\r\nRecent failures:\r\n (no failures)\r\n\r\nResources\r\n---------------------------------------------------------------\r\nTotal Usage:\r\n 3.0/32.0 CPU (3.0 used of 4.0 reserved in placement groups)\r\n 1.0/4.0 GPU (1.0 used of 2.0 reserved in placement groups)\r\n 0.0/4.0 accelerator_type:T4\r\n 0B/74.07GiB memory\r\n 0B/34.19GiB object_store_memory\r\n\r\nTotal Demands:\r\n {'CPU': 1.0, 'GPU': 1.0}: 1+ pending tasks/actors (1+ using placement groups)\r\n\r\nNode: 10.0.60.35\r\n Usage:\r\n  0.0/16.0 CPU\r\n  0B/34.23GiB memory\r\n  0B/17.11GiB object_store_memory\r\n\r\nNode: 10.0.14.120\r\n Usage:\r\n  1.0/4.0 CPU (1.0 used of 1.0 reserved in placement groups)\r\n  0.0/1.0 GPU (0.0 used of 1.0 reserved in placement groups)\r\n  0.0/1.0 accelerator_type:T4\r\n  0B/9.97GiB memory\r\n  0B/4.27GiB object_store_memory\r\n\r\nNode: 10.0.52.205\r\n Usage:\r\n  0.0/4.0 CPU (0 used of 0.0 reserved in placement groups)\r\n  0.0/1.0 GPU (0 used of 0.0 reserved in placement groups)\r\n  0.0/1.0 accelerator_type:T4\r\n  0B/9.97GiB memory\r\n  0B/4.27GiB object_store_memory\r\n\r\nNode: 10.0.35.146\r\n Usage:\r\n  2.0/4.0 CPU (2.0 used of 3.0 reserved in placement groups)\r\n  1.0/1.0 GPU (1.0 used of 1.0 reserved in placement groups)\r\n  0.0/1.0 accelerator_type:T4\r\n  0B/9.97GiB memory\r\n  0B/4.27GiB object_store_memory\r\n\r\nNode: 10.0.39.211\r\n Usage:\r\n  0.0/4.0 CPU\r\n  0.0/1.0 GPU\r\n  0.0/1.0 accelerator_type:T4\r\n  0B/9.95GiB memory\r\n  0B/4.26GiB object_store_memory\r\n```\r\n\r\nIf I run the same script, but remove the need for 1 of the actors, then it runs without hanging.\r\nThe placement group for that script has 1 less bundle:\r\n\r\n`[{\"CPU:1, \"GPU: 0\"},  {\"CPU:1, \"GPU: 1\"}, {\"CPU:1, \"GPU: 1\"}]`\r\n\r\n\r\n**This issue blocks me from being able to run experiments for a blog post on multi gpu training with RLlib in ray 2.5. I cannot train across multiple nodes without this issue appearing.**\n\n### Versions / Dependencies\n\nray 5197da2c388a731c05b54b6228fb9800b96982a5\n\n### Reproduction script\n\n```python\r\nimport argparse\r\n\r\nimport ray\r\nfrom ray import air, tune\r\nfrom ray.rllib.algorithms.ppo import PPOConfig\r\nfrom ray.rllib.examples.env.dm_control_suite import cheetah_run\r\n\r\n\r\ndef run_with_tuner_1_rollout_worker(config):\r\n    config = config.rollouts(num_rollout_workers=1)\r\n    print(\"-\" * 80)\r\n    tuner = tune.Tuner(\r\n        \"PPO\",\r\n        param_space=config,\r\n        run_config=air.RunConfig(\r\n            stop={\"timesteps_total\": 128},\r\n            failure_config=air.FailureConfig(max_failures=3),\r\n            storage_path=\"/mnt/shared_storage/avnishn/ppo_multi_gpu_benchmarking\",\r\n            checkpoint_config=air.CheckpointConfig(checkpoint_frequency=1),\r\n            sync_config=tune.SyncConfig(syncer=None)\r\n        ),\r\n    )\r\n    tuner.fit()\r\n\r\ndef run_with_tuner_0_rollout_worker(config):\r\n    print(\"-\" * 80)\r\n    config = config.rollouts(num_rollout_workers=0)\r\n    tuner = tune.Tuner(\r\n        \"PPO\",\r\n        param_space=config,\r\n        run_config=air.RunConfig(\r\n            stop={\"timesteps_total\": 128},\r\n            failure_config=air.FailureConfig(max_failures=3),\r\n            storage_path=\"/mnt/shared_storage/avnishn/ppo_multi_gpu_benchmarking\",\r\n            checkpoint_config=air.CheckpointConfig(checkpoint_frequency=1),\r\n            sync_config=tune.SyncConfig(syncer=None)\r\n        ),\r\n    )\r\n    tuner.fit()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # This experiment is run on a machine with a 8 cpu headnode, and 2, 1 gpu 4 cpu workernodes.\r\n    # Note I couldn't reproduce this bug if I made my worker nodes 2, 4 gpu 32 cpu instances.\r\n\r\n    ray.init()\r\n\r\n    tune.registry.register_env(\"HalfCheetahDmc\", lambda c: cheetah_run(from_pixels=False))\r\n\r\n    config = (\r\n        PPOConfig()\r\n        .training(_enable_learner_api=True,\r\n                  model={\"fcnet_hiddens\": [256, 256, 256], \r\n                         \"fcnet_activation\": \"relu\", \r\n                         \"vf_share_layers\": True},\r\n                  train_batch_size=128)\r\n        .rl_module(_enable_rl_module_api=True)\r\n        .environment(\"HalfCheetahDmc\")\r\n        .resources(num_gpus_per_learner_worker=1, num_learner_workers=2)\r\n        .rollouts(num_rollout_workers=1)\r\n        .reporting(min_time_s_per_iteration=0,\r\n                   min_sample_timesteps_per_iteration=10\r\n\r\n        )\r\n    )\r\n\r\n    # run_with_tuner_0_rollout_worker(config)  # this works\r\n    print(\"finished without tune\")\r\n    print(\"*\" * 100)\r\n    run_with_tuner_1_rollout_worker(config)  # this hangs\r\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "jjyao",
        "body": "```\r\n>>> ray._private.state.state._available_resources_per_node()\r\n{'bee925af8b9709467925658d4bae618fbcda9c54690facf0a7d9bdd7': {'memory': 36752714958.0, 'node:10.0.60.35': 1.0, 'object_store_memory': 18376357478.0, 'CPU': 16.0}, '436333409ff6a802f346a84ade5f27973fd59612725cc9a68f917c80': {'memory': 10700826215.0, 'node:10.0.35.146': 1.0, 'CPU': 4.0, 'object_store_memory': 4586068377.0, 'accelerator_type:T4': 1.0, 'GPU': 1.0}, 'be7ba149913b7b994eeca34d043a14ea383ad26c8a900d25d1eb1900': {'CPU': 1.0, 'node:10.0.39.211': 1.0, 'bundle_group_66ea2b543e78f3f76a0e4d20173f0c000000': 3000.0, 'accelerator_type:T4': 1.0, 'bundle_group_2_66ea2b543e78f3f76a0e4d20173f0c000000': 1000.0, 'CPU_group_2_66ea2b543e78f3f76a0e4d20173f0c000000': 1.0, 'memory': 10678697165.0, 'CPU_group_1_66ea2b543e78f3f76a0e4d20173f0c000000': 1.0, 'bundle_group_0_66ea2b543e78f3f76a0e4d20173f0c000000': 1000.0, 'bundle_group_1_66ea2b543e78f3f76a0e4d20173f0c000000': 1000.0, 'GPU_group_2_66ea2b543e78f3f76a0e4d20173f0c000000': 1.0, 'object_store_memory': 4576584499.0, 'CPU_group_66ea2b543e78f3f76a0e4d20173f0c000000': 1.0}, 'b46155e307ea475886e9edbd708426f66d29b1986a3f28c01a662952': {'bundle_group_66ea2b543e78f3f76a0e4d20173f0c000000': 1000.0, 'accelerator_type:T4': 1.0, 'memory': 10700740199.0, 'bundle_group_3_66ea2b543e78f3f76a0e4d20173f0c000000': 1000.0, 'CPU': 3.0, 'GPU_group_66ea2b543e78f3f76a0e4d20173f0c000000': 1.0, 'CPU_group_3_66ea2b543e78f3f76a0e4d20173f0c000000': 1.0, 'node:10.0.52.205': 1.0, 'GPU_group_3_66ea2b543e78f3f76a0e4d20173f0c000000': 1.0, 'object_store_memory': 4586031513.0}, '8625f254c392267afc08438899f7481e2914acf6aea670c765076c04': {'memory': 10700705792.0, 'object_store_memory': 4586016768.0, 'CPU': 4.0, 'node:10.0.14.120': 1.0, 'accelerator_type:T4': 1.0, 'GPU': 1.0}}\r\n```\r\n\r\nI think the hang is due to the PG fragmentation:\r\n\r\nThe pg has 4 bundles `[{\"CPU\": 1}, {\"CPU\": 1}, {\"CPU\": 1, \"GPU\": 1}, {\"CPU\": 1, \"GPU\": 1}]`. Bundles 0, 1, 2 are on node A and bundle 3 are on node B.\r\n\r\nWe already scheduled two `{\"CPU\": 1}` task/actor and one `{\"CPU\": 1, \"GPU\": 1}` task/actor in this PG. After that the PG has 1 free CPU on bundle 1or 2 and 1 free GPU on bundle 3 but they don't belong to the same bundle. As a result, `{\"CPU\":1, \"GPU\": 1}` request cannot be satisified. The reason why is that when we schedule the first two `{\"CPU\": 1}` task/actor, we didn't specify the bundle_index and it happened to use the CPU from bundle 3 due to bad luck.\r\n\r\n\r\nThe short-term fix now should be on the Tune side to always specify bundle_index during scheduling to avoid fragmentation. \r\n\r\nIn the long term, core can probably do a better job to reduce fragmentation automatically."
      },
      {
        "user": "jjyao",
        "body": "Also as suggested by @cadedaniel: core should provide some message on why the scheduling is pending, that way people are not left assuming it’s a bug in ray core"
      },
      {
        "user": "kouroshHakha",
        "body": "> The reason why is that when we schedule the first two {\"CPU\": 1} task/actor, we didn't specify the bundle_index and it happened to use the CPU from bundle 3 due to bad luck.\r\n\r\n@jjyao Why does that happen? The two task/actors that specify one CPU requirement and no-gpu requirements should not be assigned to the bundle that has GPU requirements. Isn't that the case? "
      }
    ]
  },
  {
    "issue_number": 35387,
    "title": "[Job] Failed to schedule supervisor actor leads to job failure",
    "author": "spolcyn",
    "state": "open",
    "created_at": "2023-05-16T14:17:25Z",
    "updated_at": "2025-06-17T00:19:30Z",
    "labels": [
      "bug",
      "P2",
      "jobs",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\n1. Submitted a job, but it failed almost immediately with no logs.\r\n2. Expected it to run as usual\r\n3. The same job ran successfully ~30min before and ~30min after, and we noticed head CPU usage spiked to 100% during the time period specified in the logs.\r\n\r\nLogs from running `grep <job ID>` in the `ray/session_latest/logs` directory:\r\n```\r\ngcs_server.out:[2023-05-16 00:00:21,097 W 18 18] (gcs_server) gcs_actor_manager.cc:417: Actor with name '_ray_internal_job_actor_RTFUOWXMKFADKFFG7RLS' was not found.\r\ngcs_server.out:[2023-05-16 00:00:21,139 I 18 18] (gcs_server) gcs_actor_manager.cc:683: Actor name _ray_internal_job_actor_RTFUOWXMKFADKFFG7RLS is cleand up.\r\nevents/event_JOBS.log:{\"event_id\": \"7bE0eD27DE95dCd48CfDd7bf2fF46EE1dbEb\", \"source_type\": \"JOBS\", \"source_hostname\": \"prod-head-8f42x\", \"source_pid\": 173, \"message\": \"Started a ray job RTFUOWXMKFADKFFG7RLS.\", \"timestamp\": \"1684195221\", \"custom_fields\": {\"submission_id\": \"RTFUOWXMKFADKFFG7RLS\"}, \"severity\": \"INFO\", \"label\": \"\"}\r\nevents/event_JOBS.log:{\"event_id\": \"19f08d61Cb9e7A4F2fEC79FB01e49aE84727\", \"source_type\": \"JOBS\", \"source_hostname\": \"prod-head-8f42x\", \"source_pid\": 173, \"message\": \"Completed a ray job RTFUOWXMKFADKFFG7RLS with a status PENDING.\", \"timestamp\": \"1684195221\", \"custom_fields\": {\"submission_id\": \"RTFUOWXMKFADKFFG7RLS\"}, \"severity\": \"INFO\", \"label\": \"\"}\r\ndashboard.log:2023-05-16 00:00:22,017   INFO web_log.py:206 -- 30.30.155.110 [16/May/2023:00:00:21 +0000] 'GET /api/jobs/RTFUOWXMKFADKFFG7RLS HTTP/1.1' 200 743 bytes 749388 us '-' 'python-requests/2.25.1'\r\ndashboard_agent.log:2023-05-16 00:00:20,991     INFO job_manager.py:891 -- Starting job with submission_id: RTFUOWXMKFADKFFG7RLS\r\ndashboard_agent.log:2023-05-16 00:00:21,142     INFO job_manager.py:684 -- Failed to schedule job RTFUOWXMKFADKFFG7RLS because the supervisor actor could not be scheduled: The actor is not schedulable: The node specified via NodeAffinitySchedulingStrategy doesn't exist any more or is infeasible, and soft=False was specified.\r\npython-core-driver-01000000ffffffffffffffffffffffffffffffffffffffffffffffff_173.log:[2023-05-16 00:00:21,098 W 173 173] actor_manager.cc:112: Failed to look up actor with name '_ray_internal_job_actor_RTFUOWXMKFADKFFG7RLS'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.\r\npython-core-driver-01000000ffffffffffffffffffffffffffffffffffffffffffffffff_173.log:[2023-05-16 00:00:21,140 I 173 248] task_manager.cc:535: Task failed: SchedulingCancelled: Actor creation cancelled.: Type=ACTOR_CREATION_TASK, Language=PYTHON, Resources: {}, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.dashboard.modules.job.job_manager, class_name=JobSupervisor, function_name=__init__, function_hash=a8fbf10378f54c2db4819596a2370709}, task_id=ffffffffffffffff89f677682ee293763251637d01000000, task_name=_ray_internal_job_actor_RTFUOWXMKFADKFFG7RLS:JobSupervisor.__init__, job_id=01000000, num_args=8, num_returns=1, depth=1, attempt_number=0, actor_creation_task_spec={actor_id=89f677682ee293763251637d01000000, max_restarts=0, max_retries=0, max_concurrency=1000, is_asyncio_actor=1, is_detached=1}, runtime_env_hash=-1400121655, eager_install=1, setup_timeout_seconds=600\r\n```\n\n### Versions / Dependencies\n\nRay 2.4\r\nPython 3.9.10\r\nUbuntu 20.04\n\n### Reproduction script\n\nOccurs randomly from our perspective, mainly looking for more insight on the error or additional debugging information to collect.\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35367,
    "title": "[Job] Show submitter of a Job on the dashboard",
    "author": "jjyao",
    "state": "open",
    "created_at": "2023-05-15T23:18:33Z",
    "updated_at": "2025-06-17T00:19:28Z",
    "labels": [
      "enhancement",
      "P2",
      "jobs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nWe should add submitter as a job metadata and show up on the dashboard. It will make it easy to track when multiple users submit jobs.\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35356,
    "title": "[Serve] Support sync function for multiplexing ",
    "author": "sihanwang41",
    "state": "open",
    "created_at": "2023-05-15T21:28:58Z",
    "updated_at": "2025-06-17T00:19:26Z",
    "labels": [
      "enhancement",
      "P2",
      "pending-cleanup"
    ],
    "body": "### Description\n\nCurrently @serve.multiplexed only support `async def`.\r\nEventually we can have\r\n\r\n```\r\n@serve.multiplexed\r\ndef load_model(mode_id:str):\r\n    return\r\n\r\n@serve.deployment\r\nclass Model:\r\n    async def __call__(self, req):\r\n        model = await load_model(req.meta.model_id)\r\n```\r\n\r\nWe can prioritize the feature till we gather enough customer signals. \n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35333,
    "title": "[AIR] [Train]  train multiple instances simultaneously on machines with specified tags",
    "author": "a123tony39",
    "state": "open",
    "created_at": "2023-05-15T13:17:42Z",
    "updated_at": "2025-06-17T00:19:24Z",
    "labels": [
      "bug",
      "P2",
      "train",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\n1. I want to train multiple instances simultaneously and place the trainers on machines with specified tags. However, it is possible that a trainer with the tag \"machine_for_GPU\" might run on a machine tagged \"machine_for_CPU\" in the Ray cluster, and similarly a trainer with the tag \"machine_for_CPU\" might run on a machine tagged \"machine_for_GPU\" in the Ray cluster.\r\n\r\n2. The expectation is that all trainers with the tag \"machine_for_GPU\" should be able to train on machines with the \"machine_for_GPU\" tag, while trainers with the tag \"machine_for_CPU\" should be able to train on machines with the \"machine_for_CPU\" tag.\r\n\r\nray status\r\n![image](https://github.com/ray-project/ray/assets/53037529/f96dc4f2-2163-4459-916e-56433e9928ba)\r\n\r\n\r\nresult\r\n![image](https://github.com/ray-project/ray/assets/53037529/49398ae6-d8b6-4ead-9e8e-5f085eb37ec1)\r\n\n\n### Versions / Dependencies\n\nray==2.3.1\r\npython==3.8.16\r\n\r\n\n\n### Reproduction script\n\n```python\r\nimport ray\r\nfrom ray import train\r\nfrom ray.train.torch import TorchTrainer\r\nfrom ray.air.config import ScalingConfig\r\nfrom ray.air.config import RunConfig\r\nfrom ray.air.config import CheckpointConfig\r\nimport sys\r\n@ray.remote(num_cpus=0)\r\ndef ray_remote_training(use_gpu, resources_per_worker):\r\n    task = {}\r\n    task['use_gpu'] = use_gpu\r\n    scaling_config = ScalingConfig(\r\n        num_workers = 1, \r\n        use_gpu = use_gpu,\r\n        _max_cpu_fraction_per_node = 0.8,\r\n        resources_per_worker = resources_per_worker,\r\n    )\r\n    run_config = RunConfig(\r\n        checkpoint_config = CheckpointConfig(num_to_keep=1),\r\n    )\r\n    # trainer\r\n    trainer = TorchTrainer(\r\n        train_loop_per_worker = train_func,\r\n        train_loop_config = task,\r\n        scaling_config = scaling_config,\r\n        run_config = run_config,\r\n    )\r\n    sys.stdout = None\r\n    trainer.fit()\r\n    sys.stdout = sys.stderr\r\n\r\ndef train_func(config):\r\n    device = train.torch.get_device()\r\n    print(\"use GPU: {} device: {}\".format(config['use_gpu'], device))\r\n\r\ndef main():\r\n    for i in range(10):\r\n        if i <= 2:\r\n            use_gpu = True\r\n            resources_per_worker = {\"machine_for_GPU\": 1}\r\n        else:\r\n            use_gpu = False\r\n            resources_per_worker = {\"machine_for_CPU\": 1}\r\n        ray_remote_training.remote(use_gpu, resources_per_worker)\r\n    input()\r\n    \r\nif __name__ == '__main__':\r\n    main()\r\n``` \n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "gjoliver",
        "body": "I traced through the code. There is a limitation of current Ray Tune implementation where ``tune.with_parameters()`` uses a cluster wide per-job registry for storing and retrieving parameters.\r\nthat caused the additional_resources_per_worker parameter from different Tuners to overwrite each other.\r\n\r\nCan I ask about your use case? Why do you want to launch multiple Trainer runs yourself?\r\nWhy not simply use Ray Tune to tune the ScalingConfig?"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35313,
    "title": "<RLlib> What is the cause of the low CPU utilization in rllib PPO? ",
    "author": "fuckingcrazy0711",
    "state": "open",
    "created_at": "2023-05-13T07:10:07Z",
    "updated_at": "2025-06-17T00:19:22Z",
    "labels": [
      "bug",
      "P2",
      "rllib",
      "rllib-system",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nThis is my ppo config / I open 30workers and 30 env per worker, but the cpu usage is only 15% , every rollout worker 0.6% cpu usage / I wanna to improve the cpu sampling speed. this is my code . the env is my custom env / how can I improve the cpu usage\n\n### Versions / Dependencies\n\nray==2.2.0\n\n### Reproduction script\n\nfrom ray.rllib.algorithms.ppo import PPOConfig\r\nray.init()\r\nconfig = PPOConfig()\r\nconfig = config.training(gamma=0.99, lr=1e-05, use_gae='True', train_batch_size =50000,\r\n                        model ={\r\n                                \"fcnet_hiddens\":[512, 1024, 512]}\r\n                                )\r\nconfig = config.rollouts(num_rollout_workers=30, num_envs_per_worker=30)\r\nconfig = config.environment(Env, env_config=config_env)\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "Rohan138",
        "body": "Hi, could you provide more details about your environment and system specs? 30 rollout workers and 30 environments per worker sounds very high unless you specifically have more than 30 cpus available AND each cpu can run 30 environments in parallel (only possible if your env is exceptionally lightweight). Otherwise, you're just going to bottleneck your sampling performance."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35254,
    "title": "[VM launcher] Document how to set up the cluster when there is UFW firewall",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-05-11T07:15:48Z",
    "updated_at": "2025-06-17T00:19:20Z",
    "labels": [
      "P2",
      "docs",
      "core",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "### Description\n\nhttps://ray-distributed.slack.com/archives/CN2RGCHRR/p1683615742329169?thread_ts=1683524599.480019&channel=CN2RGCHRR&message_ts=1683615742.329169\r\n\r\nThe user ran into the issue because of the firewall issue. I followed up in the direct message and he mentioned that when he disabled the UFW firewall, the cluster worked. We need to document what users should try to make it work if there is UFW firewall.\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35230,
    "title": "[Data] Infer the data schema in Ray Datasets",
    "author": "zhe-thoughts",
    "state": "open",
    "created_at": "2023-05-10T21:03:01Z",
    "updated_at": "2025-06-17T00:19:18Z",
    "labels": [
      "enhancement",
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "### Description\n\nRight now, with \"[strict mode](https://github.com/ray-project/enhancements/blob/main/reps/2023-04-27-data-strict-mode.md)\" enabled by default, users need to care about using the right schema when they passing in functions into `map` or `map_batches`\r\n\r\nIdeally, we should infer the schema (I guess we already do that in `ray.data.read_xxx` calls). Another reference is Dask DataFrame: https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.apply.html\n\n### Use case\n\nThis will simplify users' mental model in doing batch processing with Ray Data",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35156,
    "title": "[dashboard] how to adjust ray dashboard refresh rate?",
    "author": "hwjung2",
    "state": "open",
    "created_at": "2023-05-09T04:40:49Z",
    "updated_at": "2025-06-17T00:19:16Z",
    "labels": [
      "enhancement",
      "P2",
      "dashboard",
      "pending-cleanup"
    ],
    "body": "### Description\n\nhow to adjust ray dashboard refresh rate?\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "alanwguo",
        "body": "@hwjung2 what do you mean by refresh rate? How often the dashboard UI fetches from the API?"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35137,
    "title": "[KubeRay, dashboard] Clarify that the users can use persistent volumes for log_dir and ray dashboard can read from it.",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-05-08T18:21:08Z",
    "updated_at": "2025-06-17T00:19:13Z",
    "labels": [
      "P2",
      "dashboard",
      "docs",
      "kuberay",
      "core",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nhttps://ray-distributed.slack.com/archives/C01DLHZHRBJ/p1683569842407549?thread_ts=1682522893.195049&cid=C01DLHZHRBJ\r\n\r\nA user suggests that we should clarify that the users can use persistent volumes for log_dir and ray dashboard can read from it.\r\n\r\nQuote from the user\r\n\r\n> should be that one way to do NFS is using PVs with ReadWriteMany capability, and a little example of where to mount the PVs so that the dashboard loads them correctly\r\n(I imagine it could be anywhere for NFS but for the dashboard specifically there should be one specific path, but I suggest checking this internally)\r\n\r\n### Link\r\n\r\n_No response_",
    "comments": [
      {
        "user": "richardliaw",
        "body": "Thanks for filing this!\r\n\r\nThis should be documented in KubeRay right? \r\n\r\nWe shouldn't have infra specific configuration concepts in Tune, and we also already have a guide for using Tune with an NFS: https://docs.ray.io/en/latest/tune/tutorials/tune-storage.html"
      },
      {
        "user": "scottsun94",
        "body": "@richardliaw Not necessarily in kuberay doc\r\nThis is something that the Tune users care about. I think that there are at least two things we should improve to the tune docs:\r\n\r\n1. Talk about how to visualize the ray results in ray dashboard if they want. (I believe if the ray results are saved into the log directory, they'll be rendered automatically?)\r\n2. Maybe add a link to the `guide for using Tune with an NFS` about how to use persistent volumes as NFS"
      },
      {
        "user": "richardliaw",
        "body": "No, again this does not have anything to do with hyperparameter tuning but rather is a sysadmin / kuberay responsibility. I think there's a conflation of issues here:\r\n\r\n> ray results in ray dashboard\r\n\r\nThis isn't related to the thread right? The user is asking about Ray logs, not result visualization (which is currently a responsibility of external system like w&b/mlflow, not Ray). Ray log locations should be configured at the Ray start level (which in this case, is in kuberay configs).\r\n\r\n> Maybe add a link to the guide for using Tune with an NFS about how to use persistent volumes as NFS\r\n\r\nI couldn't quite understand why we need this, per the thread... \r\n\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 35113,
    "title": "[RLlib] Better error handling when return shape from step() mismatch in utils._flatten_multidiscrete",
    "author": "y-he2",
    "state": "open",
    "created_at": "2023-05-06T17:45:57Z",
    "updated_at": "2025-06-17T00:19:11Z",
    "labels": [
      "enhancement",
      "P2",
      "rllib",
      "rllib-env",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nHi and thanks for the hardworks so far, and an amazing distributed RL library. Hope I came to the right place for RLlib improvement requests. \r\n\r\nIm just gonna drop a quick one here as I encounter them on the road, and more details could be provided as per request.\r\nCurrently the error message could be hard to use for tracing back the issue, when the returned dimension mismatches the defined observation_space element dimensions.\r\n\r\nIn this example:\r\n* Cause of error: I defined a ```MultiDiscrete( np.one( 22 ) )``` element in my observation space, then I accidentally returned a np.one( 41 ) vector in my gym.step() function. \r\n* Current error handling:\r\n```\r\n...\\gymnasium\\spaces\\utils.py\", line 158, in _flatten_multidiscrete\r\n    onehot[offsets[:-1] + x.flatten()] = 1\r\nValueError: operands could not be broadcast together with shapes (22,) (41,)\r\n```\r\nAssuming no previous knowledge this could leave the user in some confusion as to where to start to debug. \r\n\r\nDebug code to trace the error I used in utils.py line 156:\r\n```\r\ntry:\r\n    onehot[offsets[:-1] + x.flatten()] = 1\r\nexcept ValueError as err:\r\n    print( \"space:\", space )\r\n    print( \"x:\", x )\r\n    print( \"onehot:\", onehot )\r\n    print( \"offsets[:-1]:\", offsets[:-1] )\r\n    print( \"x.flatten():\", x.flatten() )\r\n    print( \"offsets[:-1].shape:\", offsets[:-1].shape )\r\n    print( \"x.flatten().shape:\", x.flatten().shape )\r\n    print( \"space.nvec\", space.nvec )\r\n    print( \"space.nvec.size\", space.nvec.size )\r\n    raise err\r\n```\r\n\r\nUseful info to display:\r\n* Which element in which space\r\n* Defined space shape\r\n* Return vector shape from step()\r\n\r\n\r\nBest regards, \r\nIan\r\n\r\n### Use case\r\n\r\nAs the title and this could help the user to better understand/trace their error when observation element vector size mismatch in step() return and self.observation_space definition.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35051,
    "title": "The ray rsync-up cli reports no issue, but actually file is absent on remote side (Ray AWS cluster)",
    "author": "yell0w4x",
    "state": "open",
    "created_at": "2023-05-04T20:04:26Z",
    "updated_at": "2025-06-17T00:19:09Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-clusters",
      "stability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nThe `ray rsync-up` cli reports no issue, but actually file is absent on remote side\r\nWanted: `requirements.txt` on remote side `/home/ray` or an error message\r\nReady to use cluster examples are here https://github.com/yell0w4x/ray-serve-boilerplate/\r\n```\r\n(ray.venv) q@bora-bora:~/work/playground/ray-web-app$ ray rsync-up -v cluster.yaml requirements.txt /home/ray\r\n2023-05-04 21:59:08,800 INFO util.py:376 -- setting max workers for head node type to 0\r\nLoaded cached provider configuration from /tmp/ray-config-4baa24b02d729e74d31d38959a8a266c2f868f62\r\nIf you experience issues with the cloud provider, try re-running the command with --no-config-cache.\r\nCreating AWS resource `ec2` in `us-west-2`\r\nCreating AWS resource `ec2` in `us-west-2`\r\nFetched IP: 34.219.##.##\r\nRunning `mkdir -p /tmp/ray_tmp_mount/default/home && chown -R ubuntu /tmp/ray_tmp_mount/default/home`\r\nShared connection to 34.219.##.## closed.\r\nRunning `rsync --rsh ssh -i /home/q/.ssh/ray-autoscaler_us-west-2.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_7694f4a663/c21f969b5f/%C -o ControlPersist=10s -o ConnectTimeout=120s -avz --exclude **/.git --exclude **/.git/** --filter dir-merge,- .gitignore requirements.txt ubuntu@34.219.##.##:/tmp/ray_tmp_mount/default/home/ray`\r\nsending incremental file list\r\nrequirements.txt\r\n\r\nsent 129 bytes  received 35 bytes  65,60 bytes/sec\r\ntotal size is 16  speedup is 0,10\r\nRunning `docker inspect -f '{{.State.Running}}' ray_container || true`\r\nShared connection to 34.219.##.## closed.\r\nRunning `docker exec -it  ray_container /bin/bash -c 'mkdir -p /home'  && rsync -e 'docker exec -i' -avz /tmp/ray_tmp_mount/default/home/ray ray_container:/home/ray`\r\nsending incremental file list\r\nray\r\n\r\nsent 118 bytes  received 35 bytes  102.00 bytes/sec\r\ntotal size is 16  speedup is 0.10\r\nShared connection to 34.219.##.## closed.\r\n`rsync`ed requirements.txt (local) to /home/ray (remote)\r\n```\r\nThe  `working_dir` doesn't matter `ray.init(address='ray://localhost:10001', runtime_env=dict(working_dir=os.getcwd()))`.\r\nThe example is here https://github.com/yell0w4x/ray-serve-boilerplate/blob/master/src/rsync_up.py\r\n\r\n```\r\n(ray.venv) q@bora-bora:~/work/playground/ray-web-app$ python src/rsync_up.py \r\n2023-05-04 22:42:02,242 INFO util.py:376 -- setting max workers for head node type to 0\r\n2023-05-04 22:42:02,349 VWARN commands.py:324 -- Loaded cached provider configuration from /tmp/ray-config-4baa24b02d729e74d31d38959a8a266c2f868f62\r\n2023-05-04 22:42:02,349 WARN commands.py:332 -- If you experience issues with the cloud provider, try re-running the command with --no-config-cache.\r\n2023-05-04 22:42:02,350 VINFO utils.py:150 -- Creating AWS resource `ec2` in `us-west-2`\r\n2023-05-04 22:42:02,590 VINFO utils.py:150 -- Creating AWS resource `ec2` in `us-west-2`\r\n2023-05-04 22:42:03,490 INFO command_runner.py:204 -- Fetched IP: 34.219.##.##\r\n2023-05-04 22:42:03,490 INFO log_timer.py:30 -- NodeUpdater: i-005872b9b0516587d: Got IP  [LogTimer=0ms]\r\n2023-05-04 22:42:03,490 VINFO command_runner.py:371 -- Running `mkdir -p /tmp/ray_tmp_mount/default/home && chown -R ubuntu /tmp/ray_tmp_mount/default/home`\r\n2023-05-04 22:42:03,490 VVINFO command_runner.py:374 -- Full command is `ssh -tt -i /home/q/.ssh/ray-autoscaler_us-west-2.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_7694f4a663/c21f969b5f/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@34.219.##.## bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (mkdir -p /tmp/ray_tmp_mount/default/home && chown -R ubuntu /tmp/ray_tmp_mount/default/home)'`\r\nShared connection to 34.219.##.## closed.\r\n2023-05-04 22:42:03,972 VINFO command_runner.py:414 -- Running `rsync --rsh ssh -i /home/q/.ssh/ray-autoscaler_us-west-2.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_7694f4a663/c21f969b5f/%C -o ControlPersist=10s -o ConnectTimeout=120s -avz --exclude **/.git --exclude **/.git/** --filter dir-merge,- .gitignore requirements.txt ubuntu@34.219.##.##:/tmp/ray_tmp_mount/default/home/ray`\r\nsending incremental file list\r\n\r\nsent 62 bytes  received 12 bytes  29,60 bytes/sec\r\ntotal size is 16  speedup is 0,22\r\n2023-05-04 22:42:05,458 VINFO command_runner.py:371 -- Running `docker inspect -f '{{.State.Running}}' ray_container || true`\r\n2023-05-04 22:42:05,463 VVINFO command_runner.py:374 -- Full command is `ssh -tt -i /home/q/.ssh/ray-autoscaler_us-west-2.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_7694f4a663/c21f969b5f/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@34.219.##.## bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (docker inspect -f '\"'\"'{{.State.Running}}'\"'\"' ray_container || true)'`\r\nShared connection to 34.219.##.## closed.\r\n2023-05-04 22:42:06,009 VINFO command_runner.py:371 -- Running `docker exec -it  ray_container /bin/bash -c 'mkdir -p /home'  && rsync -e 'docker exec -i' -avz /tmp/ray_tmp_mount/default/home/ray ray_container:/home/ray`\r\n2023-05-04 22:42:06,019 VVINFO command_runner.py:374 -- Full command is `ssh -tt -i /home/q/.ssh/ray-autoscaler_us-west-2.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_7694f4a663/c21f969b5f/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@34.219.##.## bash --login -c -i 'source ~/.bashrc; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (docker exec -it  ray_container /bin/bash -c '\"'\"'mkdir -p /home'\"'\"'  && rsync -e '\"'\"'docker exec -i'\"'\"' -avz /tmp/ray_tmp_mount/default/home/ray ray_container:/home/ray)'`\r\nsending incremental file list\r\n\r\nsent 58 bytes  received 12 bytes  140.00 bytes/sec\r\ntotal size is 16  speedup is 0.23\r\nShared connection to 34.219.##.## closed.\r\n2023-05-04 22:42:06,767 VINFO updater.py:537 -- `rsync`ed requirements.txt (local) to /home/ray (remote)\r\n```\r\n\r\n\r\n### Versions / Dependencies\r\n\r\n```\r\n(ray.venv) q@bora-bora:~/work/playground/ray-web-app$ ray --version; conda --version; python --version; uname -a\r\nray, version 2.4.0\r\nconda 23.3.1\r\nPython 3.7.16\r\nLinux bora-bora 5.10.174-1-MANJARO #1 SMP PREEMPT Mon Mar 13 11:15:28 UTC 2023 x86_64 GNU/Linux\r\n```\r\n\r\n### Reproduction script\r\n\r\n```\r\nray rsync-up -v cluster.yaml requirements.txt /home/ray\r\n```\r\n\r\nhttps://github.com/yell0w4x/ray-serve-boilerplate/blob/master/src/rsync_up.py\r\n```\r\nimport ray\r\nfrom ray.autoscaler._private.commands import rsync\r\nimport os\r\n\r\n\r\ndef main():\r\n    ray.init(address='ray://localhost:10001', runtime_env=dict(working_dir=os.getcwd()))\r\n\r\n    local_path = \"requirements.txt\"\r\n    remote_path = \"/home/ray\"\r\n    rsync('cluster.yaml', source=local_path, target=remote_path, \r\n          override_cluster_name='default', down=False)\r\n    \r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n> Note: `ssh -i ~/.ssh/ray-autoscaler_us-west-2.pem -L 10001:localhost:10001 -nNT -v ubuntu@34.219.##.##`\r\n\r\n\r\n### Issue Severity\r\n\r\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35048,
    "title": "[Core] - GPU Support - Explanation of Results ",
    "author": "brent-anyscale",
    "state": "open",
    "created_at": "2023-05-04T17:15:50Z",
    "updated_at": "2025-06-17T00:19:07Z",
    "labels": [
      "enhancement",
      "P2",
      "docs",
      "core",
      "core-hardware",
      "pending-cleanup"
    ],
    "body": "### Description\n\nIt isn't very clear from the [documentation](https://docs.ray.io/en/latest/ray-core/tasks/using-ray-with-gpus.html) that when you only have 1 GPU, the GPU ID returned will be `0` - this leads to confusion around if GPUs are available/configured.\r\n\r\nIt would be great to provide a bit more context as what is going to be returned.\n\n### Link\n\nhttps://docs.ray.io/en/latest/ray-core/tasks/using-ray-with-gpus.html#using-gpus-in-tasks-and-actors",
    "comments": [
      {
        "user": "aavbsouza",
        "body": "I believe this is the expected behavior, due to the use of variables like `NVIDIA_VISIBLE_DEVICES` and also is common in frameworks like pytorch as in `torch.cuda.current_device()` that also would return 0 (standard case). Probable would be good to have an instruction to use others API, like `ray.cluster_resources()` to get a total available gpu counts"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35029,
    "title": "[Data] Optimize `read_datasource` setup",
    "author": "bveeramani",
    "state": "open",
    "created_at": "2023-05-04T04:43:57Z",
    "updated_at": "2025-06-17T00:19:05Z",
    "labels": [
      "P2",
      "performance",
      "data",
      "pending-cleanup"
    ],
    "body": "### Description\n\nIf you pass in many files, `read_datasource` is slow . For example, if you pass in 3M paths, `read_datasource` takes almost a minute. \r\n\r\nTo improve performance, we should optimize `_is_local_scheme`, `_resolve_paths_and_filesystem`, and partition filtering.\r\n\r\n---\r\n\r\nHere’s where the time goes:\r\n\r\n**_is_local_scheme**: 7.93s\r\nhttps://github.com/ray-project/ray/blob/2c20933d8899d3c88130aba90c8bb1fdbe40880d/python/ray/data/read_api.py#L336\r\n**_resolve_paths_and_filesystem**: 26.03s\r\nhttps://github.com/ray-project/ray/blob/2c20933d8899d3c88130aba90c8bb1fdbe40880d/python/ray/data/datasource/file_based_datasource.py#L389\r\n**expand_paths**: 6.35s (with Anyscale provider)\r\nhttps://github.com/ray-project/ray/blob/2c20933d8899d3c88130aba90c8bb1fdbe40880d/python/ray/data/datasource/file_based_datasource.py#L393\r\n**partition_filter**: 9.84s\r\nhttps://github.com/ray-project/ray/blob/2c20933d8899d3c88130aba90c8bb1fdbe40880d/python/ray/data/datasource/file_based_datasource.py#L411\r\n\r\n\n\n### Use case\n\nPerformance.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35020,
    "title": "[EC2 VM Cluster launcher] Document EC2 ssh key limit and workaround",
    "author": "pcmoritz",
    "state": "open",
    "created_at": "2023-05-03T22:24:47Z",
    "updated_at": "2025-06-17T00:19:03Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "### Description\n\nCurrently there is a key limit of I believe 20 in the EC2 VM cluster launcher. We should document how to work around that limit.\r\n\r\nSee https://github.com/ray-project/langchain-ray/blob/main/embedding_pdf_documents/llm-batch-inference.yaml#L10\r\n\r\nThis happens if a single EC2 account is used by more than 20 different people / laptops.\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35017,
    "title": "[VM launcher] Ran `Ray status` after I sshed in to the head node and it printed \"No cluster status\"",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-05-03T22:01:16Z",
    "updated_at": "2025-06-17T00:19:01Z",
    "labels": [
      "bug",
      "P2",
      "usability",
      "core",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nThe head node is started.\r\n```\r\nLocal node IP: 172.31.62.187\r\n\r\n--------------------\r\nRay runtime started.\r\n--------------------\r\n\r\nNext steps\r\n  To add another node to this Ray cluster, run\r\n    ray start --address='172.31.62.187:6379'\r\n\r\n  To connect to this Ray cluster:\r\n    import ray\r\n    ray.init()\r\n\r\n  To submit a Ray job using the Ray Jobs CLI:\r\n    RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python my_script.py\r\n\r\n  See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html\r\n  for more information on submitting Ray jobs to the Ray cluster.\r\n\r\n  To terminate the Ray runtime, run\r\n    ray stop\r\n\r\n  To view the status of the cluster, use\r\n    ray status\r\n\r\n  To monitor and debug Ray, view the dashboard at\r\n    127.0.0.1:8265\r\n\r\n  If connection to the dashboard fails, check your firewall settings and network configuration.\r\nShared connection to 34.223.114.236 closed.\r\n  New status: up-to-date\r\n```\r\n\r\nI ran `Ray status` after I sshed in to the head node and it printed \"No cluster status\". \r\n```\r\nLast login: Wed May  3 21:47:54 2023 from {my laptop ip}\r\nubuntu@ip-172-31-62-187:~$ ray status\r\nNo cluster status.\r\nubuntu@ip-172-31-62-187:~$ exit\r\n```\r\n\r\nThe yaml file is attached below. \r\n\r\n\r\n```\r\ncluster_name: 0503-3\r\nmax_workers: 2\r\nprovider:\r\n    type: aws\r\n    region: us-west-2\r\n    cache_stopped_nodes: True\r\nauth:\r\n    ssh_user: ubuntu\r\navailable_node_types:\r\n    ray.head.default:\r\n        node_config:\r\n            InstanceType: m5.2xlarge\r\n    ray.worker.default:\r\n        min_workers: 2\r\n        max_workers: 2\r\n        node_config:\r\n            InstanceType: m5.2xlarge\r\nhead_node_type: ray.head.default\r\nhead_start_ray_commands:\r\n    - ray stop\r\n    - ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --temp-dir=~/ray_temp_logs/\r\nworker_start_ray_commands:\r\n    - ray stop\r\n    - ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\r\n```\r\n\r\n\r\n\r\n### Versions / Dependencies\r\n\r\nsee yaml file above\r\n\r\n### Reproduction script\r\n\r\nsee yaml file above\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "scottsun94",
        "body": "cc: @gvspraveen @wuisawesome "
      },
      {
        "user": "wuisawesome",
        "body": "In the absence of error messages i'm assuming this is a race condition where that `ray status` is happening before the autoscaler is fully up. \r\n\r\nI assume this should get fixed in the autoscaler refactor? @scv119 "
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 35006,
    "title": "[air/tune][multi-tenancy] Parallel runs can use the same experiment directory",
    "author": "krfricke",
    "state": "open",
    "created_at": "2023-05-03T14:01:35Z",
    "updated_at": "2025-06-17T00:18:59Z",
    "labels": [
      "bug",
      "P2",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nAs uncovered in #35004, when running multiple runs (training or tuning) in parallel on the same cluster (multi-tenancy), we can be in a situation where two runs \"share\" the same experiment directory.\r\n\r\nThis can be e.g. when the experiment directory name is explicitly set to the same one, or when the runs are started within the same whole second with the same trainable name (as they will have the same \"date suffix\").\r\n\r\nIn the latter case, the experiment state files will also conflict.\r\n\r\nThese conflicts _will_ lead to problems. As a minimum, restoration will not work for at least one of the runs, as its experiment state is either overwritten, or at least older than the other run's state file.\r\n\r\nMore problems are bound to come up.\r\n\r\nWe should try to detect if an experiment directory is already actively used (e.g. using a time-based filelock and process probing) and raise an error if so.\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\n-\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34996,
    "title": "Issue on page /cluster/vms/examples/ml-example.html",
    "author": "richardliaw",
    "state": "open",
    "created_at": "2023-05-03T05:15:14Z",
    "updated_at": "2025-06-17T00:18:57Z",
    "labels": [
      "bug",
      "P2",
      "docs",
      "core",
      "observability",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "The docker image should be latest, not 2.0.0.\r\n\r\ncc @pcmoritz ",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34991,
    "title": "[AWS VM Cluster Launcher] AWS Cluster launcher installs nightly Ray by default",
    "author": "pcmoritz",
    "state": "open",
    "created_at": "2023-05-03T01:24:06Z",
    "updated_at": "2025-06-17T00:18:55Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-clusters",
      "stability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nAt the moment, the AWS VM Cluster launcher is installing nightly Ray, instead of the latest released version by default. This is pretty bad because nightly Ray versions didn't go through the release process yet so they are not sufficiently tested.\r\n\r\nSee https://github.com/ray-project/ray/blob/4f754040764353b3e555327ef19c6778d4fe317b/python/ray/autoscaler/aws/defaults.yaml#L127\r\n\r\nTheoretically this should be the \"latest\" released version but in practice it is an unreleased version (currently the latest master, you can see this by e.g. downloading the wheel and looking at the `__commit__` field in `__init__.py`, which at the time of writing is `f34cfcdd2d004a5b97a8c674023eecebed3567ce`.\r\n\r\nEDIT: It seems the convention we use here at the moment is that `latest` is a nightly, see the links in https://docs.ray.io/en/latest/ray-overview/installation.html#daily-releases-nightlies -- so we should fix the `defaults.yaml` to install the latest release.\r\n\r\n### Versions / Dependencies\r\n\r\nRay 2.4\r\n\r\n### Reproduction script\r\n\r\nN/A\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "architkulkarni",
        "body": "@wuisawesome The explanation makes sense but it looks to me like there's a small chance it's intentional, can you confirm the explanation?  Also, is it true that `defaults.yaml` is used to populate all of the unspecified fields in any user's YAML (for example `example-minimal.yaml`)? "
      },
      {
        "user": "richardliaw",
        "body": "Yes, defaults is used to populate all unspecified fields.\r\n\r\nI think probably this is historical (since Ray wasn't stable before). Given that the default docker image we recommend is 'latest', i would recommend updating the line to just say 'pip install \"ray[default]\"'"
      },
      {
        "user": "architkulkarni",
        "body": "Thanks, that clarifies things completely!"
      }
    ]
  },
  {
    "issue_number": 34980,
    "title": "[CI] Fix minimal-install python 3.11: build wheel with unsupported tags.",
    "author": "woshiyyya",
    "state": "open",
    "created_at": "2023-05-02T21:47:21Z",
    "updated_at": "2025-06-17T00:18:52Z",
    "labels": [
      "bug",
      "P2",
      "testing",
      "build",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nIn this PR #34147 , a line of `set -xe` was added into `install-minimal.sh`, which caused CI test: `Minimal install 3.11` failed.\r\n\r\n\r\nhttps://buildkite.com/ray-project/oss-ci-build-branch/builds/3652#0187c3e5-133d-4258-bfbe-43e979b29507\r\n![Screenshot 2023-05-01 at 5 07 43 PM (1)](https://user-images.githubusercontent.com/26745457/235792704-5a22564b-7b54-420a-985f-b5adbd04d756.png)\r\n\r\nWe fixed the test itself by removing `set -xe` as a workaround, but we still need to find the root bug.\r\n\r\n### Versions / Dependencies\r\n\r\nMaster branch\r\n\r\n### Reproduction script\r\n\r\nA simple reproduction PR:\r\nhttps://github.com/ray-project/ray/pull/34939\r\n\r\n### Issue Severity\r\n\r\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34953,
    "title": "[serve][docs] Add DAG building classes to the API reference",
    "author": "edoakes",
    "state": "open",
    "created_at": "2023-05-02T15:19:04Z",
    "updated_at": "2025-06-17T00:18:50Z",
    "labels": [
      "P2",
      "serve",
      "docs",
      "pending-cleanup"
    ],
    "body": "- `InputNode`\r\n- `DAGDriver`\r\n- Various adapters",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34925,
    "title": "[AIR output] Rich table gets truncated when the terminal height is smaller than it",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-05-01T20:38:32Z",
    "updated_at": "2025-06-17T00:18:48Z",
    "labels": [
      "bug",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nWhen the terminal height is smaller than the sticky rich table, the table will get truncated and users cannot see the rest of it. They have to adjust the height.\r\nI'm not sure if there is a good solution to this issue. This doesn't seem an important one to fix for now.\r\n<img width=\"896\" alt=\"Screen Shot 2023-03-29 at 10 17 22 AM\" src=\"https://user-images.githubusercontent.com/9677264/228617858-9ff0acd4-21d2-4eb3-b2b0-07a80399bc60.png\">\n\n### Versions / Dependencies\n\nnightly\n\n### Reproduction script\n\nn/a\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34923,
    "title": "[AIR output] Format of trial table with Rich enabled.",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-05-01T20:13:33Z",
    "updated_at": "2025-06-17T00:18:46Z",
    "labels": [
      "bug",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nWhen running the `tune_torch_benchmark.py` script with `ENABLE_RICH=1`\r\n- not sure if the line \" 🌟 Ray Tune Trial Status Table 🌟 \" is needed\r\n- For the table, maybe we could just `use box=box.SQUARE`. Something like this?\r\n- let's use lower cases for all the headers for consistency?\r\n<img width=\"658\" alt=\"Screen Shot 2023-03-28 at 9 40 44 PM\" src=\"https://user-images.githubusercontent.com/9677264/228428670-7f6fd2b5-7c22-4d57-9a91-8f61fb115171.png\">\n\n### Versions / Dependencies\n\nnightly\n\n### Reproduction script\n\n`tune_torch_benchmark.py` script with `ENABLE_RICH=1`\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34918,
    "title": "[AIR output]  \"iteration\" is shown in the output for RL users",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-05-01T18:53:11Z",
    "updated_at": "2025-06-17T00:18:44Z",
    "labels": [
      "bug",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nI ran the learning_tests_impala_torch with the new air output. It seems that we show \"iteration\" in the output. Not sure if it's a good thing or not because users may not be familiar with this \"iteration\" concept.\r\n\r\nWe plan to show something like `Finished 1000 timesteps [359 timesteps/s] at 2023-02-24 12:35:39. Running time: 2min 14s` in the [original design](https://gist.github.com/scottsun94/497970e3c4f1e10ad71749ce5f2dc4de).\r\n\r\n```\r\nTraining finished iter 13 at 2023-03-29 09:08:13 (running for 00:03:11.52)\r\nagent_timesteps_total: 389000\r\nconnector_metrics: {}\r\ncounters:\r\n  num_agent_steps_sampled: 389000\r\n  num_agent_steps_trained: 388500\r\n  num_env_steps_sampled: 389000\r\n  num_env_steps_trained: 388500\r\n  num_samples_added_to_queue: 389000\r\n  num_training_step_calls_since_last_synch_worker_weights: 134\r\n  num_weight_broadcasts: 964\r\ncustom_metrics: {}\r\nepisode_len_mean: 1721.88\r\nepisode_media: {}\r\nepisode_reward_max: 36.0\r\nepisode_reward_mean: 9.77\r\nepisode_reward_min: 4.0\r\nepisodes_this_iter: 67\r\nepisodes_total: 1710\r\ninfo:\r\n  learner:\r\n    default_policy:\r\n      custom_metrics: {}\r\n      diff_num_grad_updates_vs_sampler_policy: 10.0\r\n      learner_stats:\r\n        cur_lr: 0.0005\r\n        entropy: 1.0906739234924316\r\n        entropy_coeff: 0.01\r\n        policy_loss: -32.83232116699219\r\n        total_loss: -28.335006713867188\r\n        var_gnorm: 16.424108505249023\r\n        vf_explained_var: 0.6170328259468079\r\n        vf_loss: 19.683231353759766\r\n      model: {}\r\n      num_grad_updates_lifetime: 777.0\r\n  learner_queue:\r\n    size_count: 778\r\n    size_mean: 0.0\r\n    size_quantiles: [0.0, 0.0, 0.0, 0.0, 0.0]\r\n    size_std: 0.0\r\n  num_agent_steps_sampled: 389000\r\n  num_agent_steps_trained: 388500\r\n  num_env_steps_sampled: 389000\r\n  num_env_steps_trained: 388500\r\n  num_samples_added_to_queue: 389000\r\n  num_training_step_calls_since_last_synch_worker_weights: 134\r\n  num_weight_broadcasts: 964\r\n  timing_breakdown:\r\n    learner_dequeue_time_ms: 2772.957\r\n    learner_grad_time_ms: 123.634\r\n    learner_load_time_ms: 4.319\r\n    learner_load_wait_time_ms: 47.829\r\nnum_agent_steps_sampled: 389000\r\nnum_agent_steps_trained: 388500\r\nnum_env_steps_sampled: 389000\r\nnum_env_steps_sampled_this_iter: 30750\r\nnum_env_steps_trained: 388500\r\nnum_env_steps_trained_this_iter: 31000\r\nnum_faulty_episodes: 0\r\nnum_healthy_workers: 10\r\nnum_in_flight_async_reqs: 20\r\nnum_remote_worker_restarts: 0\r\nnum_steps_trained_this_iter: 31000\r\nperf:\r\n  cpu_util_percent: 34.94117647058823\r\n  ram_util_percent: 5.211764705882353\r\npolicy_reward_max: {}\r\npolicy_reward_mean: {}\r\npolicy_reward_min: {}\r\nsampler_perf:\r\n  mean_action_processing_ms: 0.6395067034460499\r\n  mean_env_render_ms: 0.0\r\n  mean_env_wait_ms: 7.840870172264184\r\n  mean_inference_ms: 6.614064184668028\r\n  mean_raw_obs_processing_ms: 2.9528597540097277\r\nsampler_results:\r\n  connector_metrics: {}\r\n  custom_metrics: {}\r\n  episode_len_mean: 1721.88\r\n  episode_media: {}\r\n  episode_reward_max: 36.0\r\n  episode_reward_mean: 9.77\r\n  episode_reward_min: 4.0\r\n  episodes_this_iter: 67\r\n  hist_stats:\r\n    episode_lengths: [1414, 1306, 1641, 1446, 1234, 2026, 1600, 2454, 1359, 1572,\r\n      1411, 1471, 1463, 1269, 1347, 1083, 2344, 1095, 1956, 1603, 1255, 2218, 1208,\r\n      1943, 1483, 1158, 2108, 1073, 1535, 2590, 1804, 1802, 2109, 1783, 1099, 1258,\r\n      1211, 1826, 2480, 1977, 1649, 1159, 1598, 1972, 2280, 2026, 1732, 1167, 1884,\r\n      1599, 1722, 2156, 1723, 1767, 1387, 1849, 2061, 2356, 1875, 1727, 2524, 1620,\r\n      1926, 1507, 1902, 1999, 1914, 1514, 1699, 1095, 2081, 1632, 1520, 1578, 2329,\r\n      985, 1681, 1719, 1836, 1306, 2122, 1726, 1804, 2020, 2076, 1235, 1074, 1970,\r\n      1853, 1836, 1228, 1431, 2112, 1946, 2793, 1822, 2044, 1946, 2200, 1880]\r\n    episode_reward: [9.0, 12.0, 7.0, 7.0, 5.0, 11.0, 7.0, 17.0, 10.0, 11.0, 9.0, 6.0,\r\n      6.0, 6.0, 9.0, 4.0, 13.0, 11.0, 12.0, 7.0, 5.0, 10.0, 5.0, 10.0, 14.0, 4.0,\r\n      10.0, 4.0, 10.0, 15.0, 7.0, 11.0, 9.0, 8.0, 11.0, 5.0, 8.0, 16.0, 13.0, 8.0,\r\n      6.0, 12.0, 6.0, 9.0, 11.0, 13.0, 7.0, 4.0, 11.0, 7.0, 10.0, 15.0, 7.0, 15.0,\r\n      6.0, 8.0, 10.0, 36.0, 8.0, 8.0, 14.0, 9.0, 11.0, 13.0, 15.0, 9.0, 8.0, 5.0,\r\n      8.0, 11.0, 14.0, 11.0, 6.0, 14.0, 20.0, 4.0, 11.0, 8.0, 8.0, 12.0, 14.0, 10.0,\r\n      10.0, 11.0, 10.0, 4.0, 4.0, 9.0, 8.0, 8.0, 4.0, 5.0, 10.0, 11.0, 20.0, 13.0,\r\n      14.0, 8.0, 13.0, 9.0]\r\n  num_faulty_episodes: 0\r\n  policy_reward_max: {}\r\n  policy_reward_mean: {}\r\n  policy_reward_min: {}\r\n  sampler_perf:\r\n    mean_action_processing_ms: 0.6395067034460499\r\n    mean_env_render_ms: 0.0\r\n    mean_env_wait_ms: 7.840870172264184\r\n    mean_inference_ms: 6.614064184668028\r\n    mean_raw_obs_processing_ms: 2.9528597540097277\r\ntime_this_iter_s: 11.578344583511353\r\ntime_total_s: 149.75555968284607\r\ntimers:\r\n  sample_time_ms: 0.242\r\n  synch_weights_time_ms: 0.027\r\n  training_iteration_time_ms: 0.354\r\ntimesteps_total: 389000\r\ntraining_iteration: 13\r\n```\n\n### Versions / Dependencies\n\nnightly\n\n### Reproduction script\n\nlearning_tests_impala_torch\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "scottsun94",
        "body": "@kouroshHakha @gjoliver \r\nDo you think this is a p1 issue? (fix it before we expose the new design to users by default in 2.5)"
      },
      {
        "user": "krfricke",
        "body": "I think we should keep this in as its an important (and default) metric for schedulers and checkpoint management."
      },
      {
        "user": "scottsun94",
        "body": "> Training finished iter 13 at 2023-03-29 09:08:13 (running for 00:03:11.52)\r\n\r\n@krfricke Actually, I'm referring to the sentence before the reported results.\r\n\r\nIn the original design, we plan to use `timesteps` instead of `iteration`, something like this\r\n\r\n> Training finished 1000 timesteps [359 timesteps/s] at 2023-02-24 12:35:39. Running time: 2min 14s\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 34917,
    "title": "[core] ray.kill doesn't guarantee resources are cleaned up",
    "author": "d4l3k",
    "state": "open",
    "created_at": "2023-05-01T18:37:53Z",
    "updated_at": "2025-06-17T00:18:42Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "core-api",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nWe recently ran into an issue where Ray actors weren't being cleaned up in time leading to CPU OOMs when we retried those actors.\r\n\r\nThe root of the problem seems to be that ray.kill doesn't wait for the actor process to be cleaned up before returning. There's currently no way to detect when the process has been cleaned up (i.e. `wait` sys call).\r\n\r\nSome projects trying to detect this with an extra remote call and from my testing it's better but still races. Ex: https://github.com/mars-project/mars/blob/19aa2d1b04d4db88e947c79096102bb615f8a13c/mars/oscar/backends/ray/utils.py#L159-L161\r\n\r\n\r\n### Versions / Dependencies\r\n\r\nray[default]==1.11.0\r\n\r\nPython 3.7\r\n\r\n### Reproduction script\r\n```py\r\nfor retry in range(max_retries):\r\n    try:\r\n        workers = []\r\n        for worker in range(workers):\r\n            workers.append(Worker.remote())\r\n        ray.get(workers)\r\n        \r\n        break\r\n    except Exception:\r\n        for worker in workers:\r\n            ray.kill(worker)\r\n```\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "d4l3k",
        "body": "Unit test:\r\n\r\n```py\r\ndef test_safe_ray_kill_trainers() -> None:\r\n    ray.shutdown()\r\n    ray.init()\r\n\r\n    trainers = [\r\n        DummyTrainer.remote(0, 0, \"\", \"\", MagicMock())  # type: ignore[attr-defined]\r\n        for i in range(2)\r\n    ]\r\n\r\n    # check pids are live\r\n    procs = [psutil.Process(ray.get(trainer.pid.remote())) for trainer in trainers]\r\n    for proc in procs:\r\n        assert proc.status() in (psutil.STATUS_RUNNING, psutil.STATUS_SLEEPING), proc\r\n\r\n    # kills and waits until trainer.sleep errors\r\n    safe_ray_kill_trainers(trainers)\r\n\r\n    for trainer in trainers:\r\n        with pytest.raises(ray.exceptions.RayActorError, match=r\".*killed.*\"):\r\n            ray.get(trainer.sleep.remote(0))\r\n\r\n    # check pids are dead\r\n    for proc in procs:\r\n        assert proc.status() in (psutil.STATUS_DEAD, psutil.STATUS_ZOMBIE), proc\r\n```"
      },
      {
        "user": "rkooo567",
        "body": "Ray.kill is a asynchronous API. If you'd like to make it blocking, try this as workaround\r\n\r\n```python3\r\n        for worker in workers:\r\n            ray.kill(worker)\r\n            while True:\r\n                try:\r\n                    # ray.kill is asynchronous, so it is possible the worker is still alive right \r\n                    # after calling ray.get. So we should retry until it fails.\r\n                    ray.get(worker.__ray_ready__.remote()) \r\n                except ray.exceptions.RayActorError:\r\n                    break\r\n            time.sleep(0.01)\r\n```\r\n\r\nWe will consider adding blocking=True to the ray.kill APi, but it may take some time until it is supported. "
      },
      {
        "user": "d4l3k",
        "body": "@rkooo567 we're doing something very similar but it still races as RayActorError is returned before the actor is completely cleaned up. There's no way to guarantee memory is freed without an explicit synchronization syscall. Things like kernel locked pages and device drivers can make cleanup take longer"
      }
    ]
  },
  {
    "issue_number": 34852,
    "title": "[Data] Add `fn_kwargs` to `BatchMapper`",
    "author": "GokuMohandas",
    "state": "open",
    "created_at": "2023-04-28T01:51:37Z",
    "updated_at": "2025-06-17T00:18:40Z",
    "labels": [
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "[`BatchMapper`](https://docs.ray.io/en/latest/ray-air/api/doc/ray.data.preprocessors.BatchMapper.html) to accept `fn_kwargs`, similar to [`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html), so that I don't have to use `functools.partial`.\n\n```python\nfrom functools import partial\nonehot_encoder = BatchMapper(partial(foo, arg=arg) batch_format=\"numpy\")\n```",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34816,
    "title": "Resource Allocation: Ray Core, Ray Client",
    "author": "ParthM-GitHub",
    "state": "open",
    "created_at": "2023-04-27T06:49:02Z",
    "updated_at": "2025-06-17T00:18:38Z",
    "labels": [
      "bug",
      "P2",
      "fix-error-msg",
      "core",
      "observability",
      "core-scheduler",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nWhen there is resource available but spanned across the multiple devices, the program hangs and throws no error.\r\n\r\nFor Ex.\r\n5 ray actors, and 2 GPUs, 40% allocation to each actor. And last actor does not have resource available on any one GPU, but spanned across both of GPUs. This throws no error, program just hangs.\r\n\r\nYour help is much appreciated.\n\n### Versions / Dependencies\n\nray==2.2.0\n\n### Reproduction script\n\n```\r\nimport ray\r\n\r\nray.init(num_gpus=2)\r\n\r\n@ray.remote(num_gpus=0.4)\r\nclass FractionalGPUActor:\r\n    def ping(self):\r\n        print(\"ray.get_gpu_ids(): {}\".format(ray.get_gpu_ids()))\r\n\r\n\r\nfractional_gpu_actors = [FractionalGPUActor.remote() for _ in range(5)]\r\n[ray.get(fractional_gpu_actors[i].ping.remote()) for i in range(5)]\r\n# FractionalGPUActor 0: GPU #1 uses 40% GPU\r\n# FractionalGPUActor 1: GPU #1 uses 80% GPU\r\n# FractionalGPUActor 2: GPU #2 uses 40% GPU\r\n# FractionalGPUActor 3: GPU #2 uses 80% GPU\r\n\r\n# Now there is no enough space available on any single CPU to allocate to 5th Actor\r\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "rkooo567",
        "body": "it is a bit unfortunate, but Ray's GPU allocation is based on \"instances\". That said, you cannot allocate resources across 2 GPUs.\r\n\r\nSo if you use 0.4 GPU, and if you have 2 GPUs, this will happen.\r\n\r\nGPU 1 -> 0.4, 0.4\r\nGPU 2 -> 0.4, 0.4\r\n\r\nAnd you can't schedule another 0.4 GPU because it cannot span to GPU 1 and 2 at the same time. \r\n\r\nThe issue should be fixed if you use the numbers like 0.25, 0.5, 1, 0.125"
      },
      {
        "user": "rkooo567",
        "body": "Basically same issue as https://github.com/ray-project/ray/issues/20933"
      },
      {
        "user": "ParthM-GitHub",
        "body": "Shouldn't ray at least give some error? It just hangs the program. "
      }
    ]
  },
  {
    "issue_number": 34794,
    "title": "[Jobs] Job agent recovers all running jobs on restart, not just those monitored by that agent",
    "author": "architkulkarni",
    "state": "open",
    "created_at": "2023-04-26T17:36:07Z",
    "updated_at": "2025-06-17T00:18:35Z",
    "labels": [
      "bug",
      "P2",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nEach node has a `JobAgent` which can submit and monitor jobs.  The Python class that monitors the jobs is `JobManager`, and there's one for each agent. \r\n\r\nNormally, `JobManager` only monitors jobs submitted from its agent. (https://github.com/ray-project/ray/blob/master/python/ray/dashboard/modules/job/job_manager.py#L934)\r\n\r\nWe have support for recovering the monitoring threads if the JobManager is restarted (https://github.com/ray-project/ray/blob/master/python/ray/dashboard/modules/job/job_manager.py#L534) but this function erroneously starts to monitor *all* jobs on the cluster, not just the ones started by this agent.  \r\n\r\nThis could cause an issue where a job is monitored multiple times by monitoring threads on different nodes. This might be fine because whichever thread happens to be the first to catch an exception when pinging the JobSupervisorActor will update the job status to FAILED accordingly, and the other threads will see that the job status is terminal and exit gracefully. But it could lead to unforeseen race conditions.\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\nWill take some time to create a reproduction script, but let me know if the issue is not clear.\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34751,
    "title": "[Doc] Autogenerated \"suggest an edit\" link doesn't work",
    "author": "vitsai",
    "state": "open",
    "created_at": "2023-04-25T18:44:32Z",
    "updated_at": "2025-06-17T00:18:33Z",
    "labels": [
      "P2",
      "docs",
      "core",
      "pending-cleanup"
    ],
    "body": "### Description\n\nIt leads to a 404 page. For example, https://docs.ray.io/en/latest/ray-core/api/doc/ray.get.html#ray.get leads to https://github.com/ray-project/ray/edit/master/doc/source/ray-core/api/doc/ray.get.rst\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "richardliaw",
        "body": "cc @maxpumperla "
      },
      {
        "user": "maxpumperla",
        "body": "upstream project issue: https://github.com/executablebooks/jupyter-book/issues/1186 "
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34745,
    "title": "[Tune] thread limit resulting in the job failure in multi-tenancy usage",
    "author": "lly-zero-one",
    "state": "open",
    "created_at": "2023-04-25T17:02:58Z",
    "updated_at": "2025-06-17T00:18:31Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "core-client",
      "core",
      "stability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nWe touched an issue related to the thread limit in Ray tune. Our application of using Ray is multi-tenancy usage and multiple clients could submit job to the cluster, and there could be many pending jobs due to the resource constraint. Each Ray tune (ray.util.client.server) process is creating several hundreds of threads for either running or pending job. When the total number of jobs reached 30+, we started seeing the following thread creation error. I have done: 1) OPENBLAS_NUM_THREADS=1 2) ulimit -u unlimited, but they didn't fully solve the issue. There seems to be a threshold of 13.5k on total number of threads. We actually didn't find a matched thread limit setting in our system. \r\n\r\n```python\r\nException in thread ray_print_logs:\r\nTraceback (most recent call last):\r\n  File \"/mnt/miniconda3/envs/ray/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\r\n    self.run()\r\n  File \"/mnt/miniconda3/envs/ray/lib/python3.7/threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/mnt/miniconda3/envs/ray/lib/python3.7/site-packages/ray/_private/worker.py\", line 787, in print_logs\r\n    data = subscriber.poll()\r\n  File \"/mnt/miniconda3/envs/ray/lib/python3.7/site-packages/ray/_private/gcs_pubsub.py\", line 357, in poll\r\n    self._poll_locked(timeout=timeout)\r\n  File \"/mnt/miniconda3/envs/ray/lib/python3.7/site-packages/ray/_private/gcs_pubsub.py\", line 247, in _poll_locked\r\n    self._poll_request(), timeout=timeout\r\n  File \"/tmp/ray/session_2023-04-25_06-34-10_629284_79518/runtime_resources/py_modules_files/XXX_QBI7JZVEGCO6S3WYUN2MIFI42MY3273F/grpc/_channel.py\", line 976, in future\r\n    (operations,), event_handler, self._context)\r\n  File \"/tmp/ray/session_2023-04-25_06-34-10_629284_79518/runtime_resources/py_modules_files/XXX_QBI7JZVEGCO6S3WYUN2MIFI42MY3273F/grpc/_channel.py\", line 1306, in create\r\n    _run_channel_spin_thread(state)\r\n  File \"/tmp/ray/session_2023-04-25_06-34-10_629284_79518/runtime_resources/py_modules_files/XXX_QBI7JZVEGCO6S3WYUN2MIFI42MY3273F/grpc/_channel.py\", line 1270, in _run_channel_spin_thread\r\n    channel_spin_thread.start()\r\n  File \"src/python/grpcio/grpc/_cython/_cygrpc/fork_posix.pyx.pxi\", line 117, in grpc._cython.cygrpc.ForkManagedThread.start\r\n  File \"/mnt/miniconda3/envs/ray/lib/python3.7/threading.py\", line 852, in start\r\n    _start_new_thread(self._bootstrap, ())\r\nRuntimeError: can't start new thread\r\n```\r\n### Versions / Dependencies\r\n\r\nray, version 2.2.0\r\nPython 3.7.11 (default, Jul 27 2021, 14:32:16)\r\n\r\n### Reproduction script\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nimport manta\r\nimport ray\r\nfrom ray import train\r\nfrom ray.air import session, Checkpoint\r\nfrom ray.train.torch import TorchTrainer\r\nfrom ray.air.config import ScalingConfig\r\nfrom ray.air.config import RunConfig\r\nfrom ray.air.config import CheckpointConfig\r\n\r\n\r\ninput_size = 1\r\nlayer_size = 32\r\noutput_size = 1\r\nnum_epochs = 200\r\nnum_workers = 3\r\n\r\nclass NeuralNetwork(nn.Module):\r\n    def __init__(self):\r\n        super(NeuralNetwork, self).__init__()\r\n        self.layer1 = nn.Linear(input_size, layer_size)\r\n        self.relu = nn.ReLU()\r\n        self.layer2 = nn.Linear(layer_size, output_size)\r\n\r\n    def forward(self, input):\r\n        return self.layer2(self.relu(self.layer1(input)))\r\n\r\ndef train_loop_per_worker():\r\n\r\n    dataset_shard = session.get_dataset_shard(\"train\")\r\n    model = NeuralNetwork()\r\n    loss_fn = nn.MSELoss()\r\n    optimizer = torch.optim.Adam(model.parameters(),\r\n                lr=0.01,\r\n                weight_decay=0.01)\r\n    model = train.torch.prepare_model(model)\r\n\r\n    for epoch in range(num_epochs):\r\n        print(epoch)\r\n\r\ntorch.manual_seed(42)\r\n\r\nray.init(address=YOURCLUSTER)\r\nscaling_config = ScalingConfig(num_workers=128, use_gpu=True)\r\n\r\ntrainer = TorchTrainer(\r\n    train_loop_per_worker=train_loop_per_worker,\r\n    scaling_config=scaling_config)\r\n\r\nresult = trainer.fit()\r\n\r\n```\r\n\r\nThen you can start the script 50+ times, and will see the thread creation issue in the header node. \r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "richardliaw",
        "body": "cc @matthewdeng and cc @rkooo567 to provide some assistance here?"
      },
      {
        "user": "richardliaw",
        "body": "@lly-zero-one So this is a threading overload primarily on the head process?"
      },
      {
        "user": "lly-zero-one",
        "body": "> @lly-zero-one So this is a threading overload primarily on the head process?\r\n\r\nYes, since we are using the underlying ray tune in 2.2, each job submission will result in a new `ray.util.client.server` process. It actually spawns hundreds of threads, even the job is pending state. \r\n\r\nA better way might defer the thread allocation to the execution stage. Not sure whether it is feasible though. "
      }
    ]
  },
  {
    "issue_number": 34710,
    "title": "Ray Job",
    "author": "pfcqlj",
    "state": "open",
    "created_at": "2023-04-24T11:19:43Z",
    "updated_at": "2025-06-17T00:18:29Z",
    "labels": [
      "bug",
      "P2",
      "serve",
      "kuberay",
      "jobs",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nwhen i use kuberay deploy a cluster, use ray submit to exec a ray job,  the job always is pending, the log shows \r\nCompleted a ray job raysubmit_xbfDLX3KAx6PQz7X with a status FAILED. Unexpected error occurred: The actor died unexpectedly before finishing this task.2\tclass_name: JobSupervisor3\tactor_id: 2ce545340f55c7ee2f1ad878010000004\tpid: 8325\tname: _ray_internal_job_actor_raysubmit_xbfDLX3KAx6PQz7X6\tnamespace: SUPERVISOR_ACTOR_RAY_NAMESPACE7\tip: 10.244.24.378The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n\n### Versions / Dependencies\n\npython3.8\r\nray==2.3.0\n\n### Reproduction script\n\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.5.0\r\nhelm install raycluster kuberay/ray-cluster --version 0.5.0\r\n\r\nkubectl port-forward --address 0.0.0.0 svc/raycluster-kuberay-head-svc 8265:8265\r\n\r\nfrom ray.job_submission import JobSubmissionClient\r\nclient = JobSubmissionClient(\"http://127.0.0.1:8265\")\r\nsub_id = client.submit_job(entrypoint=\"sleep 10\")\r\nclient.stop_job(sub_id)\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "rkooo567",
        "body": "cc @tchordia @architkulkarni "
      },
      {
        "user": "architkulkarni",
        "body": "@pfcqlj could it be a memory issue? How much memory does your cluster have?"
      },
      {
        "user": "pfcqlj",
        "body": "@architkulkarni  only a simple example,\r\na cluster, 1 head, 1cpu, 10G, 1 worker, 1cpu, 10G\r\nthe python script is\r\nencrypoint.py:\r\nimport ray\r\nray.init()\r\nprint(ray.available_resources().get('CPU', None))\r\n\r\ntest.py:\r\nfrom ray.job_submission import JobSubmissionClient, JobStatus\r\n\r\n\r\nif __name__ == '__main__':\r\n   \r\n    client = JobSubmissionClient(\"http://localhost:8265\")\r\n    sub_id = client.submit_job(entrypoint=\"python entrypoint.py\",\r\n                               runtime_env={\"working_dir\": \"./\"})\r\n "
      }
    ]
  },
  {
    "issue_number": 34681,
    "title": "[docs][infra] automate checks for common link errors ",
    "author": "angelinalg",
    "state": "open",
    "created_at": "2023-04-21T21:01:29Z",
    "updated_at": "2025-06-17T00:18:27Z",
    "labels": [
      "P2",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nWe should automate checking for simple link errors such as:\r\n1. using absolute paths to other docs pages when we should use relative paths, causing build errors or future broken links)\r\n\r\n2. When adding image files, do not include a \"/\" in before the `image/` directory. This was not caught until merging into master.\r\n\r\nThis would be a big efficiency improvement. Thanks!\r\n\r\n### Link\r\n\r\n_No response_",
    "comments": [
      {
        "user": "angelinalg",
        "body": "cc: @ijrsvt "
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34672,
    "title": "[Ray Job] Auto-shutdown of the cluster when job finished",
    "author": "yinweisu",
    "state": "open",
    "created_at": "2023-04-21T19:09:53Z",
    "updated_at": "2025-06-17T00:18:24Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "jobs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nAs stated above, exposing a flag to enable user to auto-shutdown(tear down) the cluster when the job finished\n\n### Use case\n\nThis could come in handy when the user only wants a ephemeral cluster to execute one job.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34653,
    "title": "[Core]  Ray.wait should return if task throw exception",
    "author": "shiranbi",
    "state": "open",
    "created_at": "2023-04-21T04:31:29Z",
    "updated_at": "2025-06-17T00:18:22Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "core-api",
      "pending-cleanup"
    ],
    "body": "### Description\n\nToday the only way to know if a task failed is to call ray.get on the return of the task or get an exception from the task that receives the data\r\n\r\nas in either\r\nref1 = taskA.remote()\r\nref2 = taskB.remote(ref1)\r\n\r\ncall to taskB will throw exception\r\n\r\nor\r\nref1 = taskA.remote()\r\nray.get(ref1)\r\n\r\nray.get will throw exception\r\n\r\nI would like to do\r\nref1 = taskA.remote()\r\nray.wait(ref1)\r\n\r\nand to have this throw an exception\r\n\r\nsee https://discuss.ray.io/t/how-to-find-if-an-objectref-failed-without-an-expensive-ray-get-call/2458\n\n### Use case\n\nMy use case is running a bunch of functions on actors\r\nwhose results is large in data size\r\nand if only they succeeded run another bunch of heavy functions (that take a long time)\r\n\r\nnow if in group A of tasks, some succeed and some fail\r\nand then i trigger group B of tasks, which get as input the output of group A (each output goes into one task)\r\nsome tasks in group B will fail immediately but some will run (since their input is ok). Since they are long tasks I have to wait until they finish even though they don't help me (I need all of group B tasks to be able to run)\r\nCancelling means killing the actor which is something that I don't wish to do",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34620,
    "title": "[RLlib] MAML does not work with TF2 in Ray 2.3.1",
    "author": "simonsays1980",
    "state": "open",
    "created_at": "2023-04-20T09:53:33Z",
    "updated_at": "2025-06-17T00:18:20Z",
    "labels": [
      "bug",
      "P2",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\n# What happened\r\nI ran the example script shown below with all three frameworks, while with TF1 and Torch MAML runs, it does not so for TF2. \r\n\r\nThe problems are: \r\n1. In TF2 the `_loss_input_dict` in the `initialize_loss_with_dummy_batch()` does not exist and so the `split` key is not there. \r\n2. Solving this issue in the same way the `MAMLTorchPolicy` does it, gives another error coming from `tf.split` because \r\n       - the `tf1.GraphKeys.TRAINABLE_VARIABLES` is empty for TF2\r\n       - the `self.policy_vars` do not contain the `log_std_var` from the model and therefore the logits are too \"short\". \r\n3. Solving this error by\r\n       - receiving variables via `self.model.base_model.trainable_weights` and \r\n       - checking for the `log_std_var` in the policy model and adding it to the variable  \r\n    results in another one as `tf.gradients` do not exist for TF2 and the `GradientTape`needs to be used there.\r\n \r\n# What you expected to happen\r\nThat the example below runs with all three frameworks.\n\n### Versions / Dependencies\n\nRay 2.3.1\r\nPython 3.9.12\r\nFedora Linux 37\n\n### Reproduction script\n\n```python\r\nfrom ray.rllib.examples.env.pendulum_mass import PendulumMassEnv\r\nfrom ray.rllib.algorithms.maml.maml import MAMLConfig\r\n\r\nfrom ray.tune import register_env\r\nfrom ray import air, tune\r\n\r\nregister_env(\"pendulum_mass\", lambda config: PendulumMassEnv())\r\n\r\nconfig = (\r\n    MAMLConfig()\r\n    .environment(\r\n        env=\"pendulum_mass\",\r\n        clip_actions=False,\r\n    )\r\n    .rollouts(\r\n        rollout_fragment_length=200,\r\n        num_rollout_workers=2,\r\n        num_envs_per_worker=4,                    \r\n    )\r\n    .framework(\r\n        framework=\"tf2\",\r\n        eager_tracing=False,\r\n    )\r\n    .training(\r\n        inner_adaptation_steps=1,\r\n        maml_optimizer_steps=5,\r\n        gamma=0.99,\r\n        lambda_=1.0,\r\n        lr=0.001,\r\n        vf_loss_coeff=0.5,\r\n        clip_param=0.3,\r\n        kl_target=0.1,\r\n        kl_coeff=0.001,\r\n        inner_lr=0.03,\r\n        model={\r\n            \"fcnet_hiddens\": [64, 64],\r\n            \"free_log_std\": True,\r\n        }        \r\n    )\r\n    .exploration(\r\n        explore=True,\r\n    )\r\n)\r\nimport ray\r\nray.init(local_mode=True)\r\ntuner = tune.Tuner(\r\n    \"MAML\",\r\n    param_space=config.to_dict(),\r\n    run_config=air.RunConfig(\r\n        stop={\"training_iteration\": 10}\r\n    )\r\n)\r\n\r\ntuner.fit()\r\n```\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "ArturNiederfahrenhorst",
        "body": "@simonsays1980 Thanks for the clear description of the problem.\r\nSorry for not catching this on our side. We simply don't support TF2 here.\r\nWe'd be happy to accept a contribution that fixes it. Would you be willing to make such a contribution? Otherwise, I'll put up a PR that raises a more informative error when attempting to run MAML with tf2."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34619,
    "title": "[Core] ray2.3.1 gcs_server memory keeps increasing until OOM",
    "author": "MissiontoMars",
    "state": "open",
    "created_at": "2023-04-20T08:35:30Z",
    "updated_at": "2025-06-17T00:18:18Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-gcs",
      "stability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nThe way to reproduce the issue is described here: https://discuss.ray.io/t/how-to-get-gcs-server-momery-distribution-to-debug-memory-continued-increasement/10030\n\n### Versions / Dependencies\n\npip install “ray[default]”==2.3.1\r\nwithout any code change.\n\n### Reproduction script\n\nNone\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "MissiontoMars",
        "body": "@iycheng @scv119 Could you help me address this issue?"
      },
      {
        "user": "rkooo567",
        "body": "Since it is not a big memory leak (15MB), and we stores some metadata that we don't clean up per each job run, we will downgrade the priority to P1. But @iycheng will look into it. "
      },
      {
        "user": "MissiontoMars",
        "body": "> Since it is not a big memory leak (15MB), and we stores some metadata that we don't clean up per each job run, we will downgrade the priority to P1. But @iycheng will look into it.\r\n\r\nDoes GCS have a elimination mechanism for job metadata? We treat the ray cluster as a long-running cluster, lots of job will be submitted to he cluster. So gcs oom seems unacceptable."
      }
    ]
  },
  {
    "issue_number": 34605,
    "title": "[Runtime Env/Ray Job] Job submission fails when specifing local zip file as working dir ",
    "author": "spolcyn",
    "state": "open",
    "created_at": "2023-04-20T00:21:17Z",
    "updated_at": "2025-06-17T00:18:15Z",
    "labels": [
      "bug",
      "P2",
      "jobs",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\n1. The docs (https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference) imply a local `.zip` file can be passed as the working dir in a `runtime_env`. However, doing so fails.\r\n2. Expected to be able to pass in a local `.zip` file, just like a local directory\r\n3. The error may be related to the `@client_mode_hook` decorator on the `_internal_kv_put` function. In the repro script below, `ray.init()` is never called, so a Ray Client connection is not made -- only the `JobSubmissionClient` is initialized.\r\n\r\nFull traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/venv/lib/python3.9/site-packages/ray/_private/runtime_env/working_dir.py\", line 64, in upload_working_dir_if_needed\r\n    working_dir_uri = get_uri_for_directory(working_dir, excludes=excludes)\r\n  File \"/venv/lib/python3.9/site-packages/ray/_private/runtime_env/packaging.py\", line 479, in get_uri_for_directory\r\n    raise ValueError(f\"directory {directory} must be an existing directory\")\r\nValueError: directory /tmp/local.zip must be an existing directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/venv/lib/python3.9/site-packages/ray/_private/runtime_env/packaging.py\", line 360, in _store_package_in_gcs\r\n    _internal_kv_put(pkg_uri, data)\r\n  File \"/venv/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/venv/lib/python3.9/site-packages/ray/experimental/internal_kv.py\", line 94, in _internal_kv_put\r\n    return global_gcs_client.internal_kv_put(key, value, overwrite, namespace) == 0\r\nAttributeError: 'NoneType' object has no attribute 'internal_kv_put'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/venv/lib/python3.9/site-packages/ray/_private/runtime_env/working_dir.py\", line 75, in upload_working_dir_if_needed\r\n    upload_package_to_gcs(pkg_uri, package_path.read_bytes())\r\n  File \"/venv/lib/python3.9/site-packages/ray/_private/runtime_env/packaging.py\", line 504, in upload_package_to_gcs\r\n    _store_package_in_gcs(pkg_uri, pkg_bytes)\r\n  File \"/venv/lib/python3.9/site-packages/ray/_private/runtime_env/packaging.py\", line 362, in _store_package_in_gcs\r\n    raise RuntimeError(\r\nRuntimeError: Failed to store package in the GCS.\r\n  - GCS URI: gcs://_ray_pkg_81681dbe1fcacac350b9fd95f3734ae6.zip\r\n  - Package data (0.00MiB): b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x00\\x00\\xec\\x00\\x94V\\x00'...\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp/test_ray_job.py\", line 18, in <module>\r\n    client.submit_job(entrypoint=\"foo.py\", runtime_env=runtime_env)\r\n  File \"/venv/lib/python3.9/site-packages/ray/dashboard/modules/job/sdk.py\", line 160, in submit_job\r\n    self._upload_working_dir_if_needed(runtime_env)\r\n  File \"/venv/lib/python3.9/site-packages/ray/dashboard/modules/dashboard_sdk.py\", line 373, in _upload_working_dir_if_needed\r\n    upload_working_dir_if_needed(runtime_env, upload_fn=_upload_fn)\r\n  File \"/venv/lib/python3.9/site-packages/ray/_private/runtime_env/working_dir.py\", line 77, in upload_working_dir_if_needed\r\n    raise RuntimeEnvSetupError(\r\nray.exceptions.RuntimeEnvSetupError: Failed to set up runtime environment.\r\nFailed to upload package local.zip to the Ray cluster: Failed to store package in the GCS.\r\n  - GCS URI: gcs://_ray_pkg_81681dbe1fcacac350b9fd95f3734ae6.zip\r\n  - Package data (0.00MiB): b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x00\\x00\\xec\\x00\\x94V\\x00'...\r\n```\n\n### Versions / Dependencies\n\nRay 2.3.0 \r\nPython 3.9.10\r\nUbuntu\n\n### Reproduction script\n\n```\r\nimport shutil\r\nimport os\r\nfrom ray.job_submission import JobSubmissionClient\r\n\r\nif not os.path.exists(\"foo\"):\r\n    os.mkdir(\"foo\")\r\nwith open(\"foo/file.txt\", mode=\"w\") as f:\r\n    f.write(\"foofile\")\r\nshutil.make_archive(base_name=\"local\", base_dir=\"foo\", format=\"zip\")\r\nassert os.path.exists(os.path.abspath(\"local.zip\"))\r\n\r\naddress = # insert Ray cluster address here\r\nruntime_env: dict = {\"working_dir\": \"local.zip\"}\r\nclient = JobSubmissionClient(address)\r\ntry:\r\n    client.submit_job(entrypoint=\"foo.py\", runtime_env=runtime_env)\r\nexcept:\r\n    print(\"Failed to submit with zip\")\r\n\r\n\r\nruntime_env[\"working_dir\"] = \"foo\"\r\nclient.submit_job(entrypoint=\"foo.py\", runtime_env=runtime_env)\r\nprint(\"Succeeded with non-zip\")\r\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "rkooo567",
        "body": "cc @architkulkarni I suppose the local zip is currently not supported? (Maybe because we need to support unzipping all these). "
      },
      {
        "user": "architkulkarni",
        "body": "I think this is actually a bug. The Ray Job API should support every runtime env behavior that's supported in `ray.init()`. It was probably an oversight because we have two separate code paths for uploading the local files for the Job API and for `ray.init()`."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34563,
    "title": "why ray.data.read_images cat not combine_chunks",
    "author": "yanxiaod123",
    "state": "open",
    "created_at": "2023-04-19T07:09:31Z",
    "updated_at": "2025-06-17T00:18:13Z",
    "labels": [
      "bug",
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nI use the following code test, i find apply if branch [https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_ops/transform_pyarrow.py#L283](https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_ops/transform_pyarrow.py#L283)，if i have many chunks，how can i to do？because i search [#34352](https://github.com/ray-project/ray/pull/34352) indicate many chunks due to slow.\n\n### Versions / Dependencies\n\nray==3.0.0.dev0\n\n### Reproduction script\n\ndataset: Dataset = ray.data.read_images(paths=\"\", size=(224, 224), parallelism=10)\r\nstart_time = time.time()\r\nfor data in dataset.iter_batches(batch_size=16, batch_format=\"numpy\"):\r\n    pass\r\nprint('process time is ', time.time() - start_time)\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34545,
    "title": "[Core] Add support for cancelling descendants of a completed task",
    "author": "vitsai",
    "state": "open",
    "created_at": "2023-04-18T22:21:41Z",
    "updated_at": "2025-06-17T00:18:11Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "pending-cleanup"
    ],
    "body": "### Description\n\nRight now if a task completes, ray.cancel will be unable to find its descendants. There are some cases where this would be desirable, for instance in cancelling leaked workers during Python exception bubbling. \n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34444,
    "title": "[Data] retrieve written paths from `Dataset.write_datasource`",
    "author": "harelwa",
    "state": "open",
    "created_at": "2023-04-15T08:38:55Z",
    "updated_at": "2025-06-17T00:18:09Z",
    "labels": [
      "bug",
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\n**UPDATE**\r\n\r\nTL;DR how can retrieve the actual paths of files written in a distributed execution by `Dataset.write_datasource` ( or `write_parquet` for that matter ) ?\r\n\r\n**Use case**\r\n\r\nfor spreading massive writes to S3 (physically) partitioned buckets, I have implemented a custom `BlockWritePathProvider`  paths naming function (a hash function).\r\n\r\nI need to get these paths back, as these paths do not share a path `prefix` you can later use to load the dataset. It's loaded only with an explicit list for files.\r\n\r\nI have tried to work around this by _storing_ these paths in a dedicated data member of my _custom_ `BlockWritePathProvider` class instance. \r\n\r\nAs expected, this works only for local writes (i.e. - when `RAY_DATASET_FORCE_LOCAL_METADATA` env var is set). But irrelevant for distributed writes which is my use case.\r\n\r\nHow would you suggest to solve this ?\r\n\r\nThanks,\r\nHarel\r\n\r\n### Versions / Dependencies\r\n\r\n`OS` Linux\r\n`Python` 3.8\r\n`Ray` 2.3.1\r\n\r\n### Reproduction script\r\n\r\napi usage of `Dataset.write_parquet` reproduces this\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "ericl",
        "body": "In the longer run, I think we should return those paths as output of write (cc @c21 for thoughts).\r\n\r\n> As expected, this works only for local writes (i.e. - when RAY_DATASET_FORCE_LOCAL_METADATA env var is set). But irrelevant for distributed writes which is my use case.\r\n\r\nOne workaround here is to use a named actor to collect the results (see https://docs.ray.io/en/master/ray-core/actors/named-actors.html#get-or-create-a-named-actor). Basically, create the actor beforehand, have your BlockPathWriteProvider send the computed paths to the actor, and then query the actor for the paths after the write completes."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34439,
    "title": "[Docs Infra] [RLLib] Remove \"<<<\" from code blocks",
    "author": "angelinalg",
    "state": "open",
    "created_at": "2023-04-15T01:09:51Z",
    "updated_at": "2025-06-17T00:18:06Z",
    "labels": [
      "P2",
      "rllib",
      "docs",
      "rllib-docs-or-examples",
      "pending-cleanup"
    ],
    "body": "### Description\n\nCSAT feedback from Feb 11:\r\n\"delete \">>>\" from Example\"\r\n\r\nhttps://analytics.google.com/analytics/web/#/report/content-event-events/a154591315w164762454p165427173/_u.date00=20221001&_u.date01=20230413&explorer-table.plotKeys=%5B%5D&explorer-table.secSegmentId=analytics.pagePath&_r.drilldown=analytics.eventCategory:RateTheDocs,analytics.eventAction:Suggestion,analytics.eventLabel:delete%20%22%3E%3E%3E%22%20from%20Example\n\n### Link\n\nhttps://docs.ray.io/en/master/rllib/rllib-algorithms.html",
    "comments": [
      {
        "user": "angelinalg",
        "body": "cc: @richardliaw @bveeramani "
      },
      {
        "user": "bveeramani",
        "body": "Yeah, we should absolutely remove REPL prompts (`>>>`) from longer examples like these. "
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34437,
    "title": "[Serve] Production Guide: Add instruction for non-K8s on-premise clusters",
    "author": "angelinalg",
    "state": "open",
    "created_at": "2023-04-15T01:01:23Z",
    "updated_at": "2025-06-17T00:18:04Z",
    "labels": [
      "P2",
      "serve",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nCSAT feedback from Feb 15, 2023:\r\n\"deploy ray serve on on-premise cluster is not explained. only VMs and k8s is covered.\"\r\n\r\nhttps://analytics.google.com/analytics/web/#/report/content-event-events/a154591315w164762454p165427173/_u.date00=20221001&_u.date01=20230413&explorer-table.plotKeys=%5B%5D&explorer-table.secSegmentId=analytics.pagePath&_r.drilldown=analytics.eventCategory:RateTheDocs,analytics.eventAction:Suggestion,analytics.eventLabel:deploy%20ray%20serve%20on%20on-premise%20cluster%20is%20not%20explained.%20only%20VMs%20and%20k8s%20is%20covered.\n\n### Link\n\nhttps://docs.ray.io/en/master/serve/production-guide/index.html",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34414,
    "title": "[Serve] Ray Serve hangs and becomes unresponsive when calling ffmpeg in deployment",
    "author": "movchan74",
    "state": "open",
    "created_at": "2023-04-14T20:58:23Z",
    "updated_at": "2025-06-17T00:18:02Z",
    "labels": [
      "bug",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nDescription:\r\n\r\nI am experiencing a bug when running Ray Serve with a call to ffmpeg. Specifically, the Ray Serve instance hangs and stops responding when I start Ray Serve with `ray start --head --port 6380` and then run the command `serve run --port 8001 --host 0.0.0.0 bug_test:server`. However, if I directly run serve run without starting head first, the command works fine.\r\n\r\nHere's what happens: the code calls ffmpeg to convert a video to frames and store them in a temporary directory. However, when Ray Serve is started as a head node, the code seems to hang indefinitely when it reaches the call to ffmpeg. I expected the code to run successfully and complete the conversion of the video to frames.\r\n\r\nExpected behavior:\r\n\r\nI would expect the code to run successfully and complete the conversion of the video to frames, regardless of whether Ray Serve is started as a head node or not.\r\n\r\nLogs:\r\n\r\n> Note: I have to press Ctrl+C twice to stop the ray. But even after `Aborted (core dumped)` the processes are still running in the system.\r\n\r\n```\r\n2023-04-14 20:44:58,486 INFO scripts.py:322 -- Deploying from import path: \"bug_test:server\".\r\n2023-04-14 20:44:58,508 INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 172.17.0.4:6380...\r\n2023-04-14 20:44:58,522 INFO worker.py:1544 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265\r\n(ServeController pid=1510) INFO 2023-04-14 20:44:59,376 controller 1510 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-7e3d9dc505ad32f09c17ec0b9be05d18ca62982a7bda2164c12667e9' on node '7e3d9dc505ad32f09c17ec0b9be05d18ca62982a7bda2164c12667e9' listening on '0.0.0.0:8001'\r\n(HTTPProxyActor pid=1602) INFO:     Started server process [1602]\r\n(ServeController pid=1510) INFO 2023-04-14 20:45:00,340 controller 1510 deployment_state.py:1333 - Adding 1 replica to deployment 'RequestHandler'.\r\n2023-04-14 20:45:02,309 SUCC scripts.py:352 -- Deployed Serve app successfully.\r\n^C2023-04-14 20:45:18,164       INFO scripts.py:366 -- Got KeyboardInterrupt, shutting down...\r\n^C\r\nAborted!\r\n^CError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/dist-packages/ray/_private/gcs_pubsub.py\", line 288, in close\r\n    self._stub.GcsSubscriberCommandBatch(req, timeout=5)\r\n  File \"/usr/local/lib/python3.9/dist-packages/grpc/_channel.py\", line 921, in __call__\r\n    state, call, = self._blocking(request, timeout, metadata, credentials,\r\n  File \"/usr/local/lib/python3.9/dist-packages/grpc/_channel.py\", line 910, in _blocking\r\n    event = call.next_event()\r\n  File \"src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\", line 338, in grpc._cython.cygrpc.SegregatedCall.next_event\r\n  File \"src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\", line 169, in grpc._cython.cygrpc._next_call_event\r\n  File \"src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\", line 163, in grpc._cython.cygrpc._next_call_event\r\n  File \"src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\", line 63, in grpc._cython.cygrpc._latent_event\r\n  File \"src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\", line 42, in grpc._cython.cygrpc._next\r\nKeyboardInterrupt\r\n[2023-04-14 20:45:21,165 E 1411 1411] logging.cc:104: Stack trace:\r\n /usr/local/lib/python3.9/dist-packages/ray/_raylet.so(+0xd5621a) [0x7f649a25921a] ray::operator<<()\r\n/usr/local/lib/python3.9/dist-packages/ray/_raylet.so(+0xd589d8) [0x7f649a25b9d8] ray::TerminateHandler()\r\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x92ae6) [0x7f6499004ae6]\r\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x92b21) [0x7f6499004b21]\r\n/usr/local/lib/python3.9/dist-packages/ray/_raylet.so(+0x6b7019) [0x7f6499bba019]\r\n/usr/local/lib/python3.9/dist-packages/ray/_raylet.so(_ZN3ray4core10CoreWorkerD1Ev+0x43) [0x7f6499b36753] ray::core::CoreWorker::~CoreWorker()\r\n/usr/local/lib/python3.9/dist-packages/ray/_raylet.so(+0x53041a) [0x7f6499a3341a] std::_Sp_counted_base<>::_M_release()\r\n/usr/local/lib/python3.9/dist-packages/ray/_raylet.so(_ZN3ray4core21CoreWorkerProcessImplD1Ev+0x101) [0x7f6499b73721] ray::core::CoreWorkerProcessImpl::~CoreWorkerProcessImpl()\r\n/usr/local/lib/python3.9/dist-packages/ray/_raylet.so(_ZN3ray4core17CoreWorkerProcess12HandleAtExitEv+0x29) [0x7f6499b738f9] ray::core::CoreWorkerProcess::HandleAtExit()\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x43031) [0x7f649bde2031]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x4312a) [0x7f649bde212a]\r\n/usr/bin/python3.9() [0x68a1d7]\r\n/usr/bin/python3.9() [0x68a1fb]\r\n/usr/bin/python3.9() [0x68a210]\r\n/usr/bin/python3.9(PyRun_SimpleFileExFlags+0x1cb) [0x68a6eb] PyRun_SimpleFileExFlags\r\n/usr/bin/python3.9(Py_RunMain+0x322) [0x6af962] Py_RunMain\r\n/usr/bin/python3.9(Py_BytesMain+0x29) [0x6afbd9] Py_BytesMain\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xe7) [0x7f649bdc0c87] __libc_start_main\r\n/usr/bin/python3.9(_start+0x2a) [0x60b63a] _start\r\n\r\nAborted (core dumped)\r\n```\n\n### Versions / Dependencies\n\nRay 2.3.1\r\nPython 3.9.16 (main, Dec  7 2022, 01:11:58)\r\nI'm running it in the docker from the image nvidia/cuda:10.2-cudnn8-runtime-ubuntu18.04\r\nffmpeg version 3.4.11-0ubuntu0.1\n\n### Reproduction script\n\n```\r\nimport json\r\nimport os\r\nfrom ray import serve\r\nfrom starlette.requests import Request\r\n\r\n@serve.deployment(route_prefix=\"/\", num_replicas=1)\r\nclass RequestHandler:\r\n    async def __call__(self, http_request: Request):\r\n        req = await http_request.json()\r\n        req = json.loads(req)\r\n        video_path = req[\"video_path\"]\r\n\r\n        # Create a temporary directory to store frames in /tmp/task_id.\r\n        tmp_dir = f'/tmp/video_test'\r\n        os.makedirs(tmp_dir, exist_ok=True)\r\n\r\n        # Use ffmpeg to convert the video to frames at 3 fps\r\n        # and store them in the temporary directory.\r\n        extraction_height = 360\r\n        num_fps = 3\r\n        vf_args = [f\"scale=-2:{extraction_height}\", f\"fps={num_fps}\"]\r\n\r\n        frame_path_pattern = os.path.join(tmp_dir, \"%05d.jpg\")\r\n        command = f\"ffmpeg -i {video_path} -qscale:v 2 -vf {','.join(vf_args)} {frame_path_pattern}\"\r\n        \r\n        # Run the command in async subprocess.\r\n        r = os.system(command)\r\n        if r != 0:\r\n            print(\"Error running ffmpeg\")\r\n            return json.dumps({\"status\": \"Error running ffmpeg\"})\r\n        else:\r\n            print(\"ffmpeg ran successfully\")\r\n            return json.dumps({\"status\": \"ffmpeg ran successfully\"})\r\n\r\nserver = RequestHandler.bind()\r\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "movchan74",
        "body": "And it only seems to be an issue when Ray is running in the docker. I've tried the same on the host system and it works fine. The host OS is  Ubuntu 18.04.4 LTS."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34386,
    "title": "[Serve] Deployments page tasks history is full of system tasks. Not very useful",
    "author": "alanwguo",
    "state": "open",
    "created_at": "2023-04-14T00:03:07Z",
    "updated_at": "2025-06-17T00:18:00Z",
    "labels": [
      "bug",
      "P2",
      "dashboard",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nCurrently, it prints all tasks that ran on the replica actor. But the heath check task is not important to users, and since it is periodically called, it can hide actually important information\r\n\r\n<img width=\"940\" alt=\"Screenshot 2023-04-13 at 5 02 51 PM\" src=\"https://user-images.githubusercontent.com/711935/231908504-a21f6467-aebc-44dd-bad3-c5cebc522cd3.png\">\r\n\r\n\r\nOriginally reported by @rkooo567 \r\n\r\n### Versions / Dependencies\r\n\r\n2.4.0\r\n\r\n### Reproduction script\r\n\r\nStart a basic service\r\n\r\n### Issue Severity\r\n\r\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34366,
    "title": "[Core] serialisation of dataclass in separate module fails to recognise parameter change in child dataclass, but functions correctly if in the same module",
    "author": "DrJohnDale",
    "state": "open",
    "created_at": "2023-04-13T15:51:44Z",
    "updated_at": "2025-06-17T00:17:58Z",
    "labels": [
      "bug",
      "P2",
      "usability",
      "core",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nI run multi processing where I pass a dataclass with a child dataclass which are defined in a separate module to the main function. If I change any of the values in the child dataclass these are not passed to the processes.  If the dataclasses are defined in the same module as the main function then it works correctly.\r\n\r\nA work around is to have the parent class as a standard python class and then it also works correctly\r\n\r\n### Versions / Dependencies\r\n\r\nRay 2.3.1\r\n\r\n### Reproduction script\r\n\r\ndata_class.py\r\n```\r\nfrom dataclasses import dataclass\r\n\r\n\r\n@dataclass\r\nclass Child:\r\n    c1: int\r\n    c2: float\r\n\r\n\r\n@dataclass\r\nclass Parent:\r\n    p1: int = 1.0\r\n    p2 = Child(c1=1, c2=1.0)\r\n```\r\n\r\nseparate_files.py\r\n```\r\nimport ray\r\nfrom data_classes import Parent\r\nimport time\r\n\r\n\r\n@ray.remote\r\ndef print_values(data: Parent):\r\n    print(data.p1)\r\n    print(data.p2)\r\n\r\n\r\nparent = Parent()\r\n\r\nparent.p2.c1 = 10\r\nparent.p2.c2 = 99.99\r\n\r\nprint(parent.p1)\r\nprint(parent.p2)\r\n\r\nnum_cpu = 4\r\n\r\nfutures = [print_values.remote(parent) for _ in range(num_cpu)]\r\nmatching_results = ray.get(futures)\r\n\r\ntime.sleep(1)\r\nray.shutdown()\r\n```\r\n\r\nrunning above gives incorrect output\r\n```\r\n1.0\r\nChild(c1=10, c2=99.99)\r\n2023-04-13 16:45:38,894\tINFO worker.py:1553 -- Started a local Ray instance.\r\n(print_values pid=111732) 1.0\r\n(print_values pid=111732) Child(c1=1, c2=1.0)\r\n(print_values pid=111734) 1.0\r\n(print_values pid=111734) Child(c1=1, c2=1.0)\r\n(print_values pid=111726) 1.0\r\n(print_values pid=111726) Child(c1=1, c2=1.0)\r\n(print_values pid=111730) 1.0\r\n(print_values pid=111730) Child(c1=1, c2=1.0)\r\n```\r\n\r\nsingle_file.py\r\n```\r\nimport ray\r\nfrom dataclasses import dataclass\r\nimport time\r\n\r\n\r\n@dataclass\r\nclass Child:\r\n    c1: int\r\n    c2: float\r\n\r\n\r\n@dataclass\r\nclass Parent:\r\n    p1: int = 1.0\r\n    p2 = Child(c1=1, c2=1.0)\r\n\r\n\r\n@ray.remote\r\ndef print_values(data: Parent):\r\n    print(data.p1)\r\n    print(data.p2)\r\n\r\n\r\nparent = Parent()\r\n\r\nparent.p2.c1 = 10\r\nparent.p2.c2 = 99.99\r\n\r\nprint(parent.p1)\r\nprint(parent.p2)\r\n\r\nnum_cpu = 4\r\n\r\nfutures = [print_values.remote(parent) for _ in range(num_cpu)]\r\nmatching_results = ray.get(futures)\r\n\r\ntime.sleep(1)\r\nray.shutdown()\r\n```\r\n\r\nworks as expected and gives output\r\n\r\n```\r\n1.0\r\nChild(c1=10, c2=99.99)\r\n2023-04-13 16:46:36,204\tINFO worker.py:1553 -- Started a local Ray instance.\r\n(print_values pid=113184) 1.0\r\n(print_values pid=113184) Child(c1=10, c2=99.99)\r\n(print_values pid=113172) 1.0\r\n(print_values pid=113172) Child(c1=10, c2=99.99)\r\n(print_values pid=113170) 1.0\r\n(print_values pid=113170) Child(c1=10, c2=99.99)\r\n(print_values pid=113174) 1.0\r\n(print_values pid=113174) Child(c1=10, c2=99.99)\r\n```\r\n\r\n\r\n### Issue Severity\r\n\r\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cadedaniel",
        "body": "cc @jjyao FYI"
      },
      {
        "user": "rkooo567",
        "body": "this is highly likely cloudpickle doesn't play well with the dataclass. You may need to define your own serializer https://docs.ray.io/en/master/ray-core/objects/serialization.html#customized-serialization\r\n"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34354,
    "title": "ImportError: cannot import name 'torch' from 'ray.rllib.train'",
    "author": "Na-YoungKim",
    "state": "open",
    "created_at": "2023-04-13T05:01:07Z",
    "updated_at": "2025-06-17T00:17:55Z",
    "labels": [
      "needs-repro-script",
      "P2",
      "rllib",
      "@external-author-action-required",
      "pending-cleanup"
    ],
    "body": "when i ran code in window environment, it constantly gave me same error message: **ImportError: cannot import name 'torch' from 'ray.rllib.train**",
    "comments": [
      {
        "user": "Rohan138",
        "body": "Could you provide more information on your setup and script?"
      },
      {
        "user": "Na-YoungKim",
        "body": "sure\nI send my codes to you\n\n\n김나영 드림\n\n========================================\nNayoung Kim\nM.S. Student in Multiagent Communications and Networking Lab.\nDept. of Electronic and Electrical Engineering, Ewha Womans University,\nSeoul, Republic of Korea\nEmail: ***@***.***\nOffice: +82-(0)2-3277-3894\nSeize the day!\n========================================\n\n\n2023년 4월 14일 (금) 오전 5:49, Rohan Potdar ***@***.***>님이 작성:\n\n> Could you provide more information on your setup and script?\n>\n> —\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ray-project/ray/issues/34354#issuecomment-1507593080>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQCTSEOEO7OGGKRTQVMU5EDXBBRGPANCNFSM6AAAAAAW4RRJHY>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n"
      },
      {
        "user": "Rohan138",
        "body": "Do you have pytorch installed correctly? Can you run `import torch`?"
      }
    ]
  },
  {
    "issue_number": 34333,
    "title": "[core][state] Include job info for placement group",
    "author": "rickyyx",
    "state": "open",
    "created_at": "2023-04-12T20:02:51Z",
    "updated_at": "2025-06-17T00:17:53Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "observability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nAs title\n\n### Versions / Dependencies\n\nNA\n\n### Reproduction script\n\nNA\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "jjyao",
        "body": "Also for tasks, actors."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34317,
    "title": "[Jobs] Use new API `is_head_node` to find head node",
    "author": "architkulkarni",
    "state": "open",
    "created_at": "2023-04-12T16:20:28Z",
    "updated_at": "2025-06-17T00:17:51Z",
    "labels": [
      "enhancement",
      "P2",
      "pending-cleanup"
    ],
    "body": "### Description\n\nJobs are scheduled on the head node by default.  To find which node is the head node, we store the head node ID in the internal KV when it comes up https://github.com/ray-project/ray/blob/8ce253e5c96cb6003c5fb12aa3ee1a1e46c443f2/dashboard/modules/node/node_head.py#L164-L172 and read it later when a job is submitted. \r\n\r\nWe should use the new `is_head_node` field of the State API instead, from this PR: https://github.com/ray-project/ray/pull/34299\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "architkulkarni",
        "body": "Some concerns about circular dependencies between the Jobs API and State API. It may be better to get the head node id directly from the GCS: https://github.com/ray-project/ray/pull/34299#issuecomment-1505641435"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34315,
    "title": "[Core] RFC: simplify CI testing ",
    "author": "mattip",
    "state": "open",
    "created_at": "2023-04-12T16:03:54Z",
    "updated_at": "2025-06-17T00:17:49Z",
    "labels": [
      "enhancement",
      "P2",
      "windows",
      "Devprod",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nCurrently CI runs on buildkite and calls bazel to build and test Ray. Leaving the question of building Ray aside for the moment, testing Ray suffers from at least these problems:\r\n- CI is very slow\r\n- It's not really possible/practical to reproduce the testing environment locally, so debugging CI issues can block progress\r\n- Python tests hit a lot more code than they need to, since there are few \"unit\" tests and many \"integration\" tests that call `ray.init()` without really needing to start up Ray.\r\n- Bazel is used to wrap pytest, complicating the test suites\r\n- The test infrastructure is difficult to maintain, refactor, and improve.\r\n\r\nI propose to use bazel only for the build system. Python code could be directly tested by pytest, perhaps using the [pytest-buildkite](https://pypi.org/project/pytest-buildkite/) integration for reporting. C++ code, currently tested using googletest and bazel, could be tested using a [c++ test runner for pytest](https://github.com/pytest-dev/pytest-cpp). Once python testing is more pytest-oriented, tools like coverage can indicate what code is being tested and how many times, and a larger refactoring of the tests can be used to speed up the suite.\r\n\r\nPytest is widely used in python testing. Many well-maintained plugins exist to enable developer productivity. Many developers are comfortable with pytest quirks, improving contributor experience.\r\n\r\nThe proposal is to do the refactoring in stages.\r\n1. Add a pytest test stage to the buildkite CI run and integrate the reporting with the current buildkite reports and upload logs as CI artifacts. At this stage on a single proof-of-concept test would be transitioned out of bazel.\r\n2. Transition python tests from the bazel runner to the pytest runner (this could be done slowly, one test directory at a time). Use add-ons like [flaky](https://pypi.org/project/flaky/), [pytest-timeout](https://pypi.org/project/pytest-timeout/), and [pytest-xdist](https://pypi.org/project/pytest-xdist/) to replicate the bazel testing features.\r\n3. Use pytest-cpp to run the c++ tests and integrate the reporting with buildkite.\r\n4. Remove bazel testing and Cleanup any shims used in the previous steps\r\n5. Document and refactor how to run the test suite locally, including post-installation steps to setting up the tests (file structures, creating/dwnloading) data files.\r\n\r\n### Use case\r\n\r\nAllow more reproducible and faster testing of Ray\r\n",
    "comments": [
      {
        "user": "richardliaw",
        "body": "cc @jjyao @scv119 @rkooo567 any thoughts here?"
      },
      {
        "user": "jjyao",
        "body": "@can-anyscale @krfricke for feedbacks.\r\n\r\nI'm actually not sure how this can solve some problems you mentioned like `It's not really possible/practical to reproduce the testing environment locally, so debugging CI issues can block progress`: in most cases, we can reproduce test failures locally and there are certain cases we need to run the test inside docker (maybe the slowness uncovers some concurrency issue)."
      },
      {
        "user": "can-anyscale",
        "body": "CC: @aslonnie as you have opinions about using bazel for better dependency management"
      }
    ]
  },
  {
    "issue_number": 34307,
    "title": "[air] Error while loading xgboost model in BatchPredictor",
    "author": "shashwat-nks",
    "state": "open",
    "created_at": "2023-04-12T10:44:34Z",
    "updated_at": "2025-06-17T00:17:46Z",
    "labels": [
      "question",
      "P2",
      "@external-author-action-required",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nWe are saving and loading a XGBoostTrainer trained model as below, however  facing error while loading it most of the times(it works and is able to predict some of the time). This is preventing us from predicting using a saved model.\r\n\r\n\r\n\r\n\r\n### Versions / Dependencies\r\n\r\n2.3.1\r\n\r\n### Reproduction script\r\n\r\nSave:\r\n```\r\n\"\"\"Save the model fit using the trainer\r\n\"\"\"\r\nmodel_name = self.model_cfg.get(\"model_name\")\r\npickle.dump(self.result, open(model_name,\"wb\"))\r\nckp = XGBoostCheckpoint.from_checkpoint(self.result.checkpoint)\r\nckp.get_model().save_model(model_name + \".xgb\")\r\n```\r\nLoad and Predict:\r\n```\r\nmodel = xgb.Booster()\r\nmodel_name = self.model_cfg.get(\"model_name\")\r\nprint(\"========== Loading model ===========\" , model_name)\r\nmodel.load_model(model_name + \".xgb\")\r\nckpt = XGBoostCheckpoint.from_model(model)\r\nbatch_predictor = BatchPredictor.from_checkpoint(\r\n            ckpt, XGBoostPredictor\r\n        )\r\npredicted_labels = (\r\n            batch_predictor.predict(test_ds)\r\n        )\r\n```\r\n\r\nError being faced:\r\n`split_1679035020608/work/src/tree/tree_model.cc:837: Check failed: fi->Read(dmlc::BeginPtr(nodes_), sizeof(Node) * nodes_.size()) == sizeof(Node) * nodes_.size() (980 vs. 10220) :`\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "xwjiang2010",
        "body": "what happens if you do the following?\r\n```\r\nbatch_predictor = BatchPredictor.from_checkpoint(\r\n            self.result.checkpoint, XGBoostPredictor\r\n        )\r\npredicted_labels = (\r\n            batch_predictor.predict(test_ds)\r\n        )\r\n```"
      },
      {
        "user": "shashwat-nks",
        "body": "Expected behaviour is observed in the above, i.e., when we train and predict on the go we are able to get the predictions."
      },
      {
        "user": "xwjiang2010",
        "body": "I mean what if you don't do the intermediate conversion to native xgboost model and just get `self.result.checkpoint` and feed it into batch_predictor like I showed above. Have you tried that?"
      }
    ]
  },
  {
    "issue_number": 34290,
    "title": "[RLlib] Unity 3d env tests are broken ",
    "author": "avnishn",
    "state": "open",
    "created_at": "2023-04-11T23:28:31Z",
    "updated_at": "2025-06-17T00:17:44Z",
    "labels": [
      "bug",
      "P2",
      "rllib",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\n`rllib:env/tests/test_local_inference_unity3d` and `rllib:env/tests/test_remote_inference_unity3d` are currently broken.\r\n\r\nThe reason they didn't show as broken is that even though they were failing on master, the bash script that was reporting the results of these tests, `test_policy_client_server_setup.sh` was reporting in such a way that even if the test was failing the script was passing.\r\n\r\nIts unclear how long these tests have been broken.\r\n\r\nWe're disabling the tests for now, however we should come back and fix these eventually.\r\n\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\n`rllib:env/tests/test_local_inference_unity3d` and `rllib:env/tests/test_remote_inference_unity3d`\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34271,
    "title": "[air/train] the logic to grab free ports for `tf_config` is potentially racy",
    "author": "xwjiang2010",
    "state": "open",
    "created_at": "2023-04-11T15:24:11Z",
    "updated_at": "2025-06-17T00:17:42Z",
    "labels": [
      "P2",
      "train",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "If there are multiple workers on the same node. \r\n\r\nSee logic `get_address_and_port`. \r\n\r\nhttps://ray-distributed.slack.com/archives/C01DLHZHRBJ/p1681225578908959?thread_ts=1681201809.113059&cid=C01DLHZHRBJ",
    "comments": [
      {
        "user": "xwjiang2010",
        "body": "cc @matthewdeng "
      },
      {
        "user": "xwjiang2010",
        "body": "I did some initial investigation here.\r\nSo the problem is that when multiple worker processes on the same physical node look for free port (`find_free_port`) they could potentially get the same one.\r\nThis is because that `find_free_port` is not an atomic `test_and_set` thing, which is a primitive in multi-process environment. This also means that there could also be potential racing condition between ray worker processes and non ray processes that just happen to grab free ports from OS at the same time (although this is much less of a concern).\r\n\r\nRecommendations:\r\nApproach 1: Have just one ray worker process each node to find free ports for all the ray worker processes on that node. So we don't have contention among ray worker processes themselves.\r\nApproach 2: Each worker process still finds free port individually, but the range within which they look for is segmented into different ranges according to their global rank.\r\n\r\nRegardless of 1 or 2, there could still be racing with non ray process. To make this super reliable, we may have to think about retry mechanism. This is much less priority though. \r\n\r\nreassign to @matthewdeng to find new owner."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34270,
    "title": "[Core][Object Store] Push Manager: round for object manager client and FIFO for object",
    "author": "Catch-Bull",
    "state": "open",
    "created_at": "2023-04-11T15:13:31Z",
    "updated_at": "2025-06-17T00:17:40Z",
    "labels": [
      "enhancement",
      "P2",
      "RFC",
      "core",
      "core-object-store",
      "pending-cleanup"
    ],
    "body": "### Description\n\nRegarding the round-robin algorithm of the push manager in our scenario: \r\n- for normal task scheduling with a task number in the tens of thousands\r\n- the maximum parallelism of the cluster is only about 5% of the task number calculated by resources, and each node can only run a maximum of two tasks of this kind at the same time. \r\n- The argument number is 1-2, with a total size of about 1GB. \r\n\r\nI think there were too many invalid chunk transfers. The scheduling of normal tasks is prone to conflicts, resulting in a large number of waiting tasks in the waiting task queue of a node. When these tasks simultaneously pull objects, their argument preparation time becomes similar, and only a few tasks can be dispatched to workers smoothly, while other tasks will be spilled out, leading to a waste of all these pull requests.\r\n\r\nHere is a simple test: \r\n``` python\r\nimport ray\r\nimport numpy as np\r\nimport time\r\nfrom tqdm import tqdm\r\nfrom ray.cluster_utils import Cluster\r\n\r\nSYSTEM_CONFIG = {\r\n    # force argument to be put into object store\r\n    \"max_direct_call_object_size\": 512,\r\n    \"object_manager_default_chunk_size\": 25 * 1024,\r\n    \"object_manager_max_bytes_in_flight\": 400 * 25 * 1024,\r\n    \"locality_aware_leasing_enabled\": True,\r\n}\r\n\r\ncluster = Cluster()\r\n\r\ncluster.add_node(\r\n    object_store_memory=75 * 1024 ** 2,\r\n    _system_config=SYSTEM_CONFIG,\r\n    memory=16*1024**3,\r\n    num_cpus=2,\r\n)\r\n\r\nray.init(address=\"auto\")\r\n\r\ncluster.add_node(\r\n    object_store_memory=75 * 1024 ** 2,\r\n    memory=16*1024**3,\r\n    num_cpus=2,\r\n)\r\n\r\ncluster.add_node(\r\n    object_store_memory=75 * 1024 ** 2,\r\n    memory=16*1024**3,\r\n    num_cpus=2,\r\n)\r\n\r\n# N MB\r\ndef get_obj(size=5):\r\n    return np.random.randint(low=255, size=size * 1024 * 1024, dtype=np.uint8)\r\n\r\nget_obj_remote = ray.remote(memory=8*1024**3)(get_obj)\r\n\r\nargs1 = []\r\nargs2 = []\r\nfor _ in tqdm(range(100)):\r\n    args1.append(get_obj_remote.remote())\r\n    args2.append(get_obj_remote.remote())\r\n\r\n@ray.remote(memory=8*1024**3)\r\ndef get_sum(data1, data2):\r\n    print(data1.sum(), data2.sum())\r\n    ray.put(get_obj(35))\r\n    return True\r\n\r\nrefs = []\r\nfor data1, data2 in zip(args1, args2):\r\n    refs.append(get_sum.remote(data1, data2))\r\n\r\n\r\nfor ref in tqdm(refs):\r\n    print(ray.get(ref))\r\n\r\nray.shutdown()\r\n```\r\n\r\n\n\n### Use case\n\nround for object manager client and FIFO for object\r\n\r\nprototype: https://github.com/ray-project/ray/pull/34269",
    "comments": [
      {
        "user": "rkooo567",
        "body": "Hmm what's the proposal here? You are saying we should do FIFO chunk transfer instead of round robin? "
      },
      {
        "user": "rkooo567",
        "body": "This can have issues like many tasks that require small objs cannot be scheduled because of a task that requires large objects?"
      },
      {
        "user": "Catch-Bull",
        "body": "> This can have issues like many tasks that require small objs cannot be scheduled because of a task that requires large objects?\r\n\r\n@rkooo567 Sorry, I missed your comment.. so I am replying late.\r\nActually, our ultimate goal is to resolve this issue. The details of the current issue are on the [PR](https://github.com/ray-project/ray/pull/34269), and we can discuss them on the PR.\r\n\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 34211,
    "title": "[air] xgboost/lightgbm trainer's validation result differ between online and offline",
    "author": "xwjiang2010",
    "state": "open",
    "created_at": "2023-04-10T17:53:42Z",
    "updated_at": "2025-06-17T00:17:38Z",
    "labels": [
      "P2",
      "train",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "See context [here](https://discuss.ray.io/t/ray-tune-metrics-not-consistent-with-offline-evaluation/10113/6)\r\n\r\nI think the problem is that each worker will do evaluation using its own shard of data and we just randomly pick one and report. But I would like @Yard1 to confirm my understanding when he comes back. ",
    "comments": [
      {
        "user": "Yard1",
        "body": "Let me just paste my reply from the thread:\r\n\r\n> We only report the results from rank 0 worker, but each actor should have the same evaluation set - we should be only sharding the training data, not evaluation data. If we are not doing that, this is a bug - I will need to double check the logic in LightGBMTrainer. I am not sure whether the evaluation is done before or after the gradients are synced. If it is the latte and the data is the same on all workers, then all workers should return the same metrics (I think that is the case).\r\n\r\nThis will most likely need a change on the xgboost-ray side"
      },
      {
        "user": "xwjiang2010",
        "body": "https://github.com/ray-project/xgboost_ray/blob/master/xgboost_ray/main.py#L1062\r\nthis seems assigning different eval dataset to different actors?"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34207,
    "title": "[tune] support viewing partial experiment result as tuning goes on",
    "author": "xwjiang2010",
    "state": "open",
    "created_at": "2023-04-10T16:32:11Z",
    "updated_at": "2025-06-17T00:17:35Z",
    "labels": [
      "tune",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "context: https://discuss.ray.io/t/getting-analysis-results-from-tune-before-its-finished/10141\r\n\r\ncc @matthewdeng as fyi ",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34158,
    "title": "[Workflow] Improve efficiency of Ray Workflow by returning workflow metadata and completed task information in single API call",
    "author": "lcaquot94",
    "state": "open",
    "created_at": "2023-04-07T13:07:09Z",
    "updated_at": "2025-06-17T00:17:33Z",
    "labels": [
      "enhancement",
      "P2",
      "workflow",
      "core",
      "pending-cleanup"
    ],
    "body": "### Description\n\nI would like to suggest an improvement to the ray.workflow.get_metadata() function. Currently, in order to obtain both metadata and progress information of my workflow, I need to call two separate functions: ray.workflow.get_metadata(workflow_id) and ray.workflow.get_metadata(workflow_id, task_id=task.value) for each task, and then check if \"end_time\" is present to determine if the task is complete.\r\n\r\nIt would be more convenient if I could simply call ray.workflow.get_metadata(workflow_id, task_id=\"all\"), which would return both the workflow metadata and information on completed tasks.\r\nAlternatively, it might be even better if ray.workflow.get_metadata(workflow_id) systematically included information on completed tasks, if possible, with a pattern like the following:\r\n```\r\n{\r\n    \"workflow_id\": \"cbc9ce7d-7936-4d4f-86d4-f3acfd75d6e9\",\r\n    \"status\": \"SUCCESSFUL\",\r\n    \"project_id\": \"8fe914fb-1f2f-4d0e-902e-6adfafb048bd\",\r\n    \"completed_tasks\": [\r\n        \"read_data\",\r\n        \"preprocessing\"\r\n    ],\r\n    \"start_time\": \"2023-04-07 10:02:24\",\r\n    \"end_time\": \"2023-04-07 10:03:43\"\r\n}\r\n```\r\n\r\nThe completed_tasks field would vary depending on which tasks have been completed within the workflow.\n\n### Use case\n\nI am developing a Fast API for a Process Mining Software that uses Ray Workflows to run training and inference tasks. I want to add a route to my Fast API app that allows me to quickly retrieve information on workflows and tasks.\r\n\r\nCurrently, my \"get workflow status\" route is slow because I have to call ray.workflow.get_metadata() multiple times, once for each workflow and again for each task within each workflow.\r\n\r\nTo improve the performance of my app, I am considering building a database using PostgreSQL or Redis alongside the Ray Workflow API. However, I would be thrilled if I could retrieve all the necessary information quickly and efficiently without the need for additional infrastructure.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34157,
    "title": "Issue on page /rllib/package_ref/algorithm.html",
    "author": "cheng123-123ng",
    "state": "open",
    "created_at": "2023-04-07T10:05:03Z",
    "updated_at": "2025-06-17T00:17:31Z",
    "labels": [
      "bug",
      "P2",
      "rllib",
      "pending-cleanup"
    ],
    "body": "I'm trying to get a trajectory using the trained QmixConfig() with my custom_env. I'm using the function compute_single_action(), while there are some issues. my code is below.\r\n\r\nfrom gymnasium.spaces import Tuple, Discrete, Box,Dict\r\nimport ray\r\nfrom ray import tune\r\nfrom ray.tune.registry import register_env\r\nfrom ray.rllib.env.multi_agent_env import MultiAgentEnv\r\nfrom ray.rllib.algorithms.qmix import QMixConfig\r\nfrom ray.rllib.algorithms.ppo import PPOConfig\r\nfrom ray.rllib.examples.env.two_step_game import TwoStepGame\r\nfrom model.action_mask_model import ActionMaskModel\r\nimport numpy as np\r\n\r\nfrom env.MA_uav_ED_env import MA_UavEnv0\r\n\r\n\r\n\r\ndef env_creator(args=None):\r\n    env = MA_UavEnv0()\r\n    obs_space = env.observation_space\r\n    act_space = env.action_space\r\n    n_agents = env.n_agent\r\n    obs_space = Tuple([obs_space for _ in range(n_agents)])\r\n    act_space = Tuple([act_space for _ in range(n_agents)])\r\n    grouping = {\"group_1\": [\"uav0\",\"uav1\"]}\r\n    return env.with_agent_groups(grouping, obs_space=obs_space, act_space=act_space)\r\n\r\nregister_env(\"grouped_test\", env_creator)\r\nconfig = (\r\n    QMixConfig()\r\n    .environment(\"grouped_test\",env_config={\r\n        \"MAX_Ucar_num\":2,\r\n        \"MAX_Uav_num\":2,\r\n    })\r\n    .training(mixer='vdn')\r\n    .framework(\"torch\")\r\n)\r\nalgo = config.build()\r\n\r\nmy_env = env_creator()\r\nobs,_ =  my_env.reset()\r\naction = algo.compute_single_action(obs)\r\nobs, reward, done, truncated, info = my_env.step(action)\r\n\r\nand the result:\r\nD:\\anaconda\\python.exe D:\\桌面\\课程\\毕业设计\\毕业论文\\new_project\\plot_picture\\text.py \r\nD:\\anaconda\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\r\n  if (distutils.version.LooseVersion(tf.__version__) <\r\n2023-04-07 18:01:34,174\tINFO worker.py:1553 -- Started a local Ray instance.\r\n2023-04-07 18:01:36,221\tWARNING env.py:296 -- Your MultiAgentEnv <GroupAgentsWrapper instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\r\n2023-04-07 18:01:36,232\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\r\nTraceback (most recent call last):\r\n  File \"D:\\anaconda\\lib\\site-packages\\tree\\__init__.py\", line 280, in assert_same_structure\r\n    _tree.assert_same_structure(a, b, check_types)\r\nValueError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: type=dict str={'group_1': [{'action_mask': array([1, 1, 1, 1, 1, 1, 1]), 'obs': array([10., 10., 10., 10., 10., 10.,  0.,  7.,  0.,  7.,  0.,  0.,  0.,\r\n       15.,  0., 15.,  0.,  0.,  3.,  3.,  0.,  5.,  5.])}, {'action_mask': array([1, 1, 1, 1, 1, 1, 1]), 'obs': array([10., 10., 10., 10., 10., 10.,  0.,  7.,  0.,  7.,  0.,  0.,  0.,\r\n       15.,  0., 15.,  0.,  0.,  3.,  3.,  0.,  5.,  5.])}]}\r\n\r\nSecond structure: type=tuple str=(OrderedDict([('action_mask', array([0, 1, 0, 1, 1, 1, 1])), ('obs', array([10,  7,  7,  5,  0,  7, 13,  9,  7, 14, 17, 20,  2, 18, 18, 11,  6,\r\n        5, 17,  3, 16, 20,  8]))]), OrderedDict([('action_mask', array([1, 1, 0, 1, 1, 1, 0])), ('obs', array([11,  1, 12, 20, 20, 19, 15,  1,  5,  0, 10,  9, 12,  0, 14,  1, 16,\r\n        5, 16, 15,  3,  2, 20]))]))\r\n\r\nMore specifically: Substructure \"type=dict str={'action_mask': array([1, 1, 1, 1, 1, 1, 1]), 'obs': array([10., 10., 10., 10., 10., 10.,  0.,  7.,  0.,  7.,  0.,  0.,  0.,\r\n       15.,  0., 15.,  0.,  0.,  3.,  3.,  0.,  5.,  5.])}\" is a sequence, while substructure \"type=ndarray str=[0 1 0 1 1 1 1]\" is not\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\桌面\\课程\\毕业设计\\毕业论文\\new_project\\plot_picture\\text.py\", line 43, in <module>\r\n    action = algo.compute_single_action(obs)\r\n  File \"D:\\anaconda\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 1563, in compute_single_action\r\n    ac_o = pp([acd])[0]\r\n  File \"D:\\anaconda\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in __call__\r\n    return [self.transform(d) for d in acd_list]\r\n  File \"D:\\anaconda\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in <listcomp>\r\n    return [self.transform(d) for d in acd_list]\r\n  File \"D:\\anaconda\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\obs_preproc.py\", line 56, in transform\r\n    d[SampleBatch.OBS] = self._preprocessor.transform(d[SampleBatch.OBS])\r\n  File \"D:\\anaconda\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 246, in transform\r\n    self.check_shape(observation)\r\n  File \"D:\\anaconda\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 69, in check_shape\r\n    observation = convert_element_to_space_type(\r\n  File \"D:\\anaconda\\lib\\site-packages\\ray\\rllib\\utils\\spaces\\space_utils.py\", line 378, in convert_element_to_space_type\r\n    return tree.map_structure(map_, element, sampled_element, check_types=False)\r\n  File \"D:\\anaconda\\lib\\site-packages\\tree\\__init__.py\", line 428, in map_structure\r\n    assert_same_structure(structures[0], other, check_types=check_types)\r\n  File \"D:\\anaconda\\lib\\site-packages\\tree\\__init__.py\", line 284, in assert_same_structure\r\n    raise type(e)(\"%s\\n\"\r\nValueError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: type=dict str={'group_1': [{'action_mask': array([1, 1, 1, 1, 1, 1, 1]), 'obs': array([10., 10., 10., 10., 10., 10.,  0.,  7.,  0.,  7.,  0.,  0.,  0.,\r\n       15.,  0., 15.,  0.,  0.,  3.,  3.,  0.,  5.,  5.])}, {'action_mask': array([1, 1, 1, 1, 1, 1, 1]), 'obs': array([10., 10., 10., 10., 10., 10.,  0.,  7.,  0.,  7.,  0.,  0.,  0.,\r\n       15.,  0., 15.,  0.,  0.,  3.,  3.,  0.,  5.,  5.])}]}\r\n\r\nSecond structure: type=tuple str=(OrderedDict([('action_mask', array([0, 1, 0, 1, 1, 1, 1])), ('obs', array([10,  7,  7,  5,  0,  7, 13,  9,  7, 14, 17, 20,  2, 18, 18, 11,  6,\r\n        5, 17,  3, 16, 20,  8]))]), OrderedDict([('action_mask', array([1, 1, 0, 1, 1, 1, 0])), ('obs', array([11,  1, 12, 20, 20, 19, 15,  1,  5,  0, 10,  9, 12,  0, 14,  1, 16,\r\n        5, 16, 15,  3,  2, 20]))]))\r\n\r\nMore specifically: Substructure \"type=dict str={'action_mask': array([1, 1, 1, 1, 1, 1, 1]), 'obs': array([10., 10., 10., 10., 10., 10.,  0.,  7.,  0.,  7.,  0.,  0.,  0.,\r\n       15.,  0., 15.,  0.,  0.,  3.,  3.,  0.,  5.,  5.])}\" is a sequence, while substructure \"type=ndarray str=[0 1 0 1 1 1 1]\" is not\r\nEntire first structure:\r\n{'group_1': [{'action_mask': ., 'obs': .}, {'action_mask': ., 'obs': .}]}\r\nEntire second structure:\r\n(OrderedDict([('action_mask', .), ('obs', .)]), OrderedDict([('action_mask', .), ('obs', .)]))\r\n\r\nProcess finished with exit code 1\r\n",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 34145,
    "title": "[Prometheus metrics util]  Application level custom metrics aren't getting exported consistently",
    "author": "dhaval0108",
    "state": "open",
    "created_at": "2023-04-06T20:07:16Z",
    "updated_at": "2025-06-17T00:17:29Z",
    "labels": [
      "question",
      "P2",
      "@external-author-action-required",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\n**The problem:**\r\n\r\nUpon using Ray's prometheus metrics utility as described [here](https://docs.ray.io/en/latest/ray-observability/ray-metrics.html), I am not able to get the application level custom metrics exported to the Prometheus server.\r\n\r\nThe code I am using:\r\n```\r\nimport ray\r\nray.init()\r\n\r\nfrom ray.util.metrics import Counter\r\n\r\nclass DummyContext:\r\n    def start_run(self):\r\n        print(\"Incrementing the counter\")\r\n        for i in range(10):\r\n            print(i)\r\n            self.JOB_COUNTER.inc(tags={\"predictor_name\": f\"pred_{self.id}\", \"model_name\": f\"model_{self.id}\"})\r\n        print(\"Counter increment completed\")\r\n        return self\r\n    \r\n    def __enter__(self):\r\n        print(\"Starting Dummy context\")\r\n        self.start_run()\r\n        return self\r\n    \r\n    def __init__(self, id):\r\n        self.JOB_COUNTER = Counter(\r\n            \"num_jobs\",\r\n            description=\"Number of training jobs\",\r\n            tag_keys=(\"predictor_name\", \"model_name\"),\r\n        )\r\n        self.id = id\r\n\r\n    def __exit__(self, exc_type, exc_val, exc_tb):\r\n        print(\"Exiting the Dummy context\")\r\n        if exc_type is not None:\r\n            print(f\"{exc_type}, {exc_val}, {exc_tb}\")\r\n            return False\r\n        return True\r\n        \r\nwith DummyContext(id=i) as dc:\r\n        print(\"Testing prometheus metrics export\")\r\n        print(\"Starting actor.\")\r\n``` \r\n\r\n**Expected Behavior**\r\nI would see the metrics exported to the Prometheus server.\r\n\r\n**Other Information**\r\nThe sample code provided on the [documentation](https://docs.ray.io/en/latest/ray-observability/ray-metrics.html#application-level-metrics) seems to be working fine.\r\n\r\nAlso, I tested using prometheus_client within my Dummy with context above. That seems to be working fine too. I started a new prometheus server on a separate port 9191. \r\n\r\n```\r\nfrom prometheus_client import start_http_server\r\nstart_http_server(port=9191)\r\n\r\nimport prometheus_client as prom\r\n\r\nNUM_JOBS = prom.Counter(\"job_count_metric_9191\",\r\n            documentation=\"Number of training jobs\",\r\n            labelnames=[\"pred_name\", \"model_name\"],)\r\n\r\nclass DummyContext:\r\n    def start_run(self):\r\n        print(\"Incrementing the counter\")\r\n        for i in range(10):\r\n            print(i)\r\n            NUM_JOBS.labels(f\"pred_{self.id}\", f\"model_{self.id}\").inc()\r\n        print(\"Counter increment completed\")\r\n        return self\r\n    \r\n    def __enter__(self):\r\n        print(\"Starting Dummy context\")\r\n        self.start_run()\r\n        return self\r\n    \r\n    def __init__(self, id):\r\n        self.JOB_COUNTER = Counter(\r\n            \"num_jobs\",\r\n            description=\"Number of training jobs\",\r\n            tag_keys=(\"predictor_name\", \"model_name\"),\r\n        )\r\n        self.id = id\r\n\r\n    def __exit__(self, exc_type, exc_val, exc_tb):\r\n        print(\"Exiting the Dummy context\")\r\n        if exc_type is not None:\r\n            print(f\"{exc_type}, {exc_val}, {exc_tb}\")\r\n            return False\r\n        return True\r\n\r\nwith DummyContext(id=i) as dc:\r\n        print(\"Testing prometheus metrics export\")\r\n        print(\"Starting actor.\")\r\n```\r\nAnd then if I visit port 9191, I see the metrics as expected\r\n\r\n<img width=\"608\" alt=\"Screenshot 2023-04-06 at 12 54 21 PM\" src=\"https://user-images.githubusercontent.com/81195454/230482300-d73f2474-a1d0-404d-bc1c-97e63906928f.png\">\r\n\r\n\r\nSo from my experiments it does look like the custom ray Counter metric I have defined in my with context is not getting exported as expected. \r\n\r\n### Versions / Dependencies\r\n\r\nI am using `ray-ml:2.3.1-py38-gpu` image to build the ray cluster. \r\n\r\n### Reproduction script\r\n\r\n```\r\nimport ray\r\nray.init()\r\n\r\nfrom ray.util.metrics import Counter\r\n\r\nclass DummyContext:\r\n    def start_run(self):\r\n        print(\"Incrementing the counter\")\r\n        for i in range(10):\r\n            print(i)\r\n            self.JOB_COUNTER.inc(tags={\"predictor_name\": f\"pred_{self.id}\", \"model_name\": f\"model_{self.id}\"})\r\n        print(\"Counter increment completed\")\r\n        return self\r\n    \r\n    def __enter__(self):\r\n        print(\"Starting Dummy context\")\r\n        self.start_run()\r\n        return self\r\n    \r\n    def __init__(self, id):\r\n        self.JOB_COUNTER = Counter(\r\n            \"num_jobs\",\r\n            description=\"Number of training jobs\",\r\n            tag_keys=(\"predictor_name\", \"model_name\"),\r\n        )\r\n        self.id = id\r\n\r\n    def __exit__(self, exc_type, exc_val, exc_tb):\r\n        print(\"Exiting the Dummy context\")\r\n        if exc_type is not None:\r\n            print(f\"{exc_type}, {exc_val}, {exc_tb}\")\r\n            return False\r\n        return True\r\n        \r\nwith DummyContext(id=i) as dc:\r\n        print(\"Testing prometheus metrics export\")\r\n        print(\"Starting actor.\")\r\n``` \r\n\r\n### Issue Severity\r\n\r\nNone",
    "comments": [
      {
        "user": "alanwguo",
        "body": "For my understanding, you see the metrics in the metrics export URL, but you do not see the metrics if you try to query for them in the prometheus web UI?"
      },
      {
        "user": "alanwguo",
        "body": "@dhaval0108 , one thing about the `ray.util.metrics` package is that it exports the metrics with the `namespace=ray`. What that means is the exported metric will be named something like this: `ray_job_count_metric_9191`"
      },
      {
        "user": "dhaval0108",
        "body": "I understand they will be prefixed with `ray_`, but that's not happening either. \r\nIf I define the metric as I have shown in the reproducible script, they do not show up on the export url. "
      }
    ]
  },
  {
    "issue_number": 34124,
    "title": "[Core] Actors not cleaning up resources correct because `force_kill=true`.",
    "author": "cadedaniel",
    "state": "open",
    "created_at": "2023-04-06T03:25:12Z",
    "updated_at": "2025-06-17T00:17:27Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-worker",
      "core-correctness",
      "stability",
      "pending-cleanup"
    ],
    "body": "Discovered in the investigation of https://github.com/ray-project/ray/issues/31451 and https://github.com/ray-project/ray/pull/33976.\r\n\r\nTL;DR things like actor destructors or atexit handlers are not guaranteed to be executed when we destroy actors. This is because we use `force_kill=true` in [gcs_actor_manager]((https://github.com/ray-project/ray/blob/bb24c16b63578c7ba2cc2f250ef9aad2be2278cc/src/ray/gcs/gcs_server/gcs_actor_manager.cc#L749-L752)).\r\n\r\nIdeally, we should send SIGTERM to worker processes so that they clean up any important state. After some time period, if the process has not already died already, we will then send a SIGKILL.\r\n\r\nMore information in Sang's comment here https://github.com/ray-project/ray/pull/33976#issuecomment-1495949518",
    "comments": [
      {
        "user": "clarng",
        "body": "but if os oom killer kick in these handlers won't get called right? we cannot guarantee os oom killer will never kick in"
      },
      {
        "user": "cadedaniel",
        "body": "Does our OOM killer immediately use SIGKILL? Or is there a SIGTERM first?"
      },
      {
        "user": "clarng",
        "body": "sigkill\r\nif we wait then it might trigger os oom (which will force kill anyway)"
      }
    ]
  },
  {
    "issue_number": 34118,
    "title": "Ray Tune + ray xgboost running out of disk space",
    "author": "amohar2",
    "state": "open",
    "created_at": "2023-04-06T00:34:02Z",
    "updated_at": "2025-06-17T00:17:25Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nI have a script where RayTune is used to tune XGBoost_Ray on a very large distributed dataset and on a distributed 16-node cluster.\r\nSomewhere in the middle of the run I start seeing out of disk errors as follows:\r\n> (raylet) file_system_monitor.cc:105: /dev/shm/ray/spill is over 95% full, available space: 0; capacity: 270374129664. Object creation will fail if spilling is required.\r\n\r\nMeanwhile when I check /dev/shm of some of the nodes, they are actually at 100% of their capacity:\r\n$ df -h /dev/shm/\r\nFilesystem      Size  Used Avail Use% Mounted on\r\ntmpfs           252G  252G     0 100% /dev/shm\r\n\r\nI don't see this issue when I run a single XGboost_ray train job on the same setup.\r\nSo is it possible that RayTune does not properly cleanup the memory from one trial to the next, so that at some point the tmpfs folder runs out of space? If so, is there anyway to avoid this and somehow cleanup the tmpfs in between trials?\r\n\r\nSome information on the setup:\r\n16-node cluster each running 4 Ray jobs\r\nEach XGBoost training jobs will use 16 actors and 3 CPUs per actor\r\nDataset has around 4B samples with approximately 800GB in-memory size. Data is a Ray dataset which is a collection of 16 equal partitions (each partition is a dummy random pandas dataframe of shape 250M x 28).\r\nRay cluster is manually started as follows:\r\nHead node:\r\nray start --head --port=6379 --num-cpus=4 --object-store-memory=270000000000 --disable-usage-stats --temp-dir=/dev/shm/ray --system-config='{\"object_spilling_config\":\"{\\\"type\\\":\\\"filesystem\\\",\\\"params\\\":{\\\"directory_path\\\":\\\"/dev/shm/ray/spill\\\"}}\"}' --include-dashboard True\r\n\r\nWorker nodes:\r\nray start --address='head_node_ip:6379' --num-cpus=4 --object-store-memory=270000000000 --disable-usage-stats --temp-dir=/dev/shm/ray\r\n\r\nSo basically, I set the config such that object_store can use all the space on tmpfs, and spill/ directory is on the same folder (due to some constraints on my system)\r\n\r\n\r\n### Versions / Dependencies\r\n\r\nRay 2.3.0\r\nxgboost_ray 0.1.15\r\nxgboost 1.6.0\r\n\r\n### Reproduction script\r\n\r\n```\r\nimport ray\r\nimport pandas as pd\r\nfrom sklearn.datasets import make_classification\r\nfrom xgboost_ray import RayDMatrix, RayParams, RayParams, train\r\nfrom ray import tune\r\n\r\ndef ray_tune(X, y, n_jobs, cpus_per_actor):\r\n\r\n    ray_params = RayParams(\r\n        num_actors=n_jobs,\r\n        cpus_per_actor=cpus_per_actor,\r\n        )\r\n\r\n    train_set = RayDMatrix(X, y)\r\n    def train_model(config):\r\n\r\n        evals_result = {}\r\n        bst = train(\r\n            params=config,\r\n            dtrain=train_set,\r\n            evals_result=evals_result,\r\n            evals=[(train_set, \"train\")],\r\n            verbose_eval=False,\r\n            ray_params=ray_params)\r\n\r\n    config = {\r\n        \"eval_metric\": [\"logloss\", \"error\"],\r\n        \"learning_rate\": tune.uniform(0.0001, 1.0),\r\n        \"min_child_weight\": tune.randint(0, 21),\r\n        \"max_depth\": tune.randint(2, 11),\r\n        \"reg_alpha\": tune.uniform(0, 4.0),\r\n        \"booster\": tune.grid_search([\"gbtree\", \"dart\"]),\r\n        \"reg_lambda\": tune.uniform(0, 100),\r\n        \"n_estimators\": tune.randint(50, 100),\r\n        }\r\n\r\n    analysis = tune.run(\r\n        train_model,\r\n        config=config,\r\n        metric=\"train-error\",\r\n        mode=\"min\",\r\n        num_samples=4,\r\n        verbose=2,\r\n        resources_per_trial=ray_params.get_tune_resources())\r\n    print(\"Best hyperparameters\", analysis.best_config)\r\n\r\n\r\n@ray.remote\r\ndef create_pandas_df(n_samples, n_features, partition_id):\r\n    X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=28, n_redundant=0, n_classes=2, random_state=partition_id)\r\n    df_X, df_y = pd.DataFrame(X, columns=[f\"col_{i}\" for i in range(n_features)]), pd.DataFrame(y, columns=['label'])\r\n    df_all = pd.concat([df_X, df_y], axis=1)\r\n    return df_all\r\n\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ray.init(_temp_dir=\"/dev/shm/ray\",\r\n             address='head_node_ip:6379')\r\n    all_nodes_info = ray.nodes()\r\n    object_refs = []\r\n    n_samples, n_features = 250_000_000, 28\r\n    for partition_id, node_info in enumerate(all_nodes_info):\r\n        object_ref = create_pandas_df.options(\r\n            scheduling_strategy=ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(\r\n                node_id=node_info['NodeID'],\r\n                soft=False,\r\n            )\r\n        ).remote(n_samples, n_features, partition_id)\r\n        object_refs.append(object_ref)\r\n    print(f\"Submitted All the jobs to create partitions on each node\")\r\n\r\n\r\n    train_data = ray.data.from_pandas_refs(object_refs)\r\n    n_jobs = len(all_nodes_info) # one xgboost actor per node (each actor works on one partition)\r\n    cpus_per_actor = 3\r\n    ray_tot_mem = ray_tune(train_data, 'label', n_jobs, cpus_per_actor)\r\n```\r\n\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "xwjiang2010",
        "body": "Hi,\r\nI think there may be several aspects that contribute to this. \r\nFor starter, could you may be try things like `from_parquet`, instead of doing splitting already by yourself with `create_pandas_df ` on each node. \r\nI am not sure if this will result in the same \"splitting\" effect as needed by xgboost-ray actors.\r\nSplitting should be handled automatically for you in xgboost-ray and you should not need to do any manual work.\r\n\r\nSecond, we seem to be running 4 trials each with 16 actors. I am suspecting that `train_data` amount of data is copied for each trial instance. So we have 4 times the overall data as would be in the case of only 1 trial running. This can contribute to excessive disk spill as well."
      },
      {
        "user": "xwjiang2010",
        "body": "I am not super familiar with the internal impl of ray dataset. cc @amogkam to provide more insights here."
      },
      {
        "user": "amohar2",
        "body": "@xwjiang2010  Thanks for the response\r\n\r\n> For starter, could you may be try things like from_parquet, instead of doing splitting already by yourself with \r\ncreate_pandas_df  on each node.\r\n\r\nThe dataset is larger than a single-node memory so the only way is to create the partitions on different nodes on the cluster. I can see a way of using read_parquet() from different sources, however, due to some other limitations on my setup, using pandas_refs is my only way of creating this large data from in-memory partitions.\r\n\r\n>Second, we seem to be running 4 trials each with 16 actors. I am suspecting that train_data amount of data is copied for each trial instance. So we have 4 times the overall data as would be in the case of only 1 trial running. This can contribute to excessive disk spill as well.\r\n\r\nIf this is the case, is there any way to avoid it? For example, by running some manual cleanup in between trials?\r\nI ran another experiment, by running xgboost trials in a for loop (to somewhat simulate what RayTune does) and my system ran out of memory on the 2nd iteration.\r\n"
      }
    ]
  },
  {
    "issue_number": 34028,
    "title": "[Core][Tune]Trials hang when using Pytorch ",
    "author": "gmt20",
    "state": "open",
    "created_at": "2023-04-03T23:03:40Z",
    "updated_at": "2025-06-17T00:17:23Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "@external-author-action-required",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nI have created a conda environment with Python 3.9, Pytorch 1.12 and ray 2.3 version . When I run the official example of training on cifar dataset with pytorch (https://docs.ray.io/en/latest/tune/examples/tune-pytorch-cifar.html), the trails just hang with no error. I have to forcefully terminate the trails by Ctrl +C. \n\n### Versions / Dependencies\n\nPython 3.9, Pytorch 1.12, Ray 2.3, Ubuntu (16.04)\n\n### Reproduction script\n\nThe official tutorial (https://docs.ray.io/en/latest/tune/examples/tune-pytorch-cifar.html)\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "rkooo567",
        "body": "cc @Yard1 for the visibiilty. The official example should never hang. "
      },
      {
        "user": "Yard1",
        "body": "cc @matthewdeng "
      },
      {
        "user": "matthewdeng",
        "body": "Hi @gmt20 can you share some more details on where it's hanging? "
      }
    ]
  },
  {
    "issue_number": 34007,
    "title": "[Data] `map_batches` hard to use and debug",
    "author": "maxpumperla",
    "state": "open",
    "created_at": "2023-04-03T15:01:10Z",
    "updated_at": "2025-06-17T00:17:20Z",
    "labels": [
      "P2",
      "docs",
      "data",
      "pending-cleanup"
    ],
    "body": "### Description\n\nTo properly define a map function, you have to know the type of your Dataset. In our [object detection CUJ](https://docs.google.com/document/d/1v5Kzp_o3LpgYU1ZFa7-7H3uYDyDLqaDG7epUkjSgwh8/edit#), I was loading images from S3 using\r\n\r\n```\r\nimport ray\r\npath = \"s3://air-example-data-2/movie-image-small-filesize-1GB\"\r\nds = ray.data.read_images(path)\r\nds\r\n```\r\n\r\nThe individual rows here, using `take` are of the form `{\"image\": <numpy array>}`, and the batch format seems to be pandas, according to:\r\n\r\n```\r\nds.default_batch_format()\r\n```\r\n\r\nThe problem is that I need to know the pandas API to define a map function, e.g. using a feature `extractor` and `model` from HF and trying:\r\n\r\n```\r\ndef map_fn(batch: pd.DataFrame) -> pd.DataFrame:\r\n    images = batch[\"image\"]\r\n    \r\n    inputs = extractor(images=images, return_tensors=\"pt\")\r\n    outputs = model(**inputs)\r\n    target_sizes = torch.tensor([image.size[::-1]])\r\n    return extractor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\r\n```\r\n\r\nDoesn't work and the error is difficult to read for beginners. Not everyone knows pandas just like that. The bigger issue is that it's not very clear how to test this function properly, as I'd have to create(!) a pandas dataframe first, somehow. This is a bit chicken-egg, as I might not know pandas. \r\n\r\nTo summarize, I have a Datasets abstraction, which looks and feels like a dictionary on a per-item basis. But it has pandas type, and contains numpy arrays. But using “ds.take(...)”  returns a list of ArrowRows, which is even more confusing. How am I supposed to fix the above map_fn now? All I want to do is apply a model to a Dataset I already have -- this should be easier to handle. I suspect this is where _most_ users would just give up, which isn't good enough for us.\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "jianoaix",
        "body": "I think Dataset is targeting data scientist/ML users, who usually know dataframe/pandas; and pandas is usually considered as having good APIs to manipulate data. These are why we make it the default batch format to write UDFs."
      },
      {
        "user": "ericl",
        "body": "@maxpumperla we recently added `take_batch`, which should unblock this in the short term. Can you try that out and suggest any further improvements along that line?\r\n\r\nSeparately, we are investigating consolidating the map()/take() and map_batches() to use a more similar format, so we don't have this weird \"take returns a different format\" problem, but this change will take more time."
      },
      {
        "user": "maxpumperla",
        "body": "@jianoaix right, I agree with \"pandas as standard API\". the issue is that it's hard to understand why the generic map function you have to define _all of a sudden_ requires you to work with pandas. Nothing in `read_images` seems to indicate that this would be the case. Looks like a dict of numpy arrays.\r\n\r\n@ericl yes!!! `take_batch` is immensely helpful. good step in the right direction."
      }
    ]
  },
  {
    "issue_number": 34001,
    "title": "[Core] improve garbage collection after job go out of scope ",
    "author": "scv119",
    "state": "open",
    "created_at": "2023-04-03T07:04:05Z",
    "updated_at": "2025-06-17T00:17:18Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-gcs",
      "core-correctness",
      "stability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nAs part of https://github.com/ray-project/ray/issues/33984 we noticed that the way we GC job associated resource (Actor, and potentially objects) are a bit brittle and inefficient: it relies on a long polling to keep alive with the owner.\r\n\r\nInstead we should relies on the worker/job information from GCS: if the owner worker or job went out of scope from GCS's metadata, we should clean up all the resources (Actor, object) associate with the job.\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\nn/a\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33954,
    "title": "[Core] Timeout for unschedulable task due to unavailable workers",
    "author": "YQ-Wang",
    "state": "open",
    "created_at": "2023-03-30T23:31:39Z",
    "updated_at": "2025-06-17T00:17:14Z",
    "labels": [
      "enhancement",
      "P2",
      "usability",
      "core",
      "core-scheduler",
      "pending-cleanup"
    ],
    "body": "### Description\n\nFor now, Ray only has support to add timeout to ray.get.\r\n\r\nHowever, a general timeout on ray.get is not that useful because a training job might take very long. It cannot help determine whether it is truly that long or the task just gets blocked by the unavailable workers.\r\n\r\nIt would be good to have a separate timeout for the blocked tasks due to the unavailable workers.\n\n### Use case\n\nWhen there is problem on the deployment of workers, a separate timeout can help early return the unschedulable tasks.",
    "comments": [
      {
        "user": "jjyao",
        "body": "One solution is when you catch the timeout exception, use the state api (https://docs.ray.io/en/latest/ray-observability/api/state/doc/ray.experimental.state.api.get_task.html#ray.experimental.state.api.get_task) to check the state of the task to see if it's running or waiting for workers."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33940,
    "title": "[Observability] Programmatically fetch prometheus metrics",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-03-30T20:14:30Z",
    "updated_at": "2025-06-17T00:17:11Z",
    "labels": [
      "P2",
      "dashboard",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nhttps://ray-distributed.slack.com/archives/G01CT9DBWE8/p1676491605807689\r\nPeople have questions about how to fetch hardware metrics.\r\nCan we add a paragraph somewhere to teach people how to query Prometheus directly to get the metrics?\r\n\r\n`Example code; https://github.com/ray-project/ray/blob/master/release/ray_release/command_runner/_prometheus_metrics.py`\n\n### Link\n\nMetrics page",
    "comments": [
      {
        "user": "hora-anyscale",
        "body": "@rkooo567 - please assign priority and remove triage label"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33932,
    "title": "[Ray AIR] Add more documentation about checkpointing ",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-03-30T18:22:29Z",
    "updated_at": "2025-06-17T00:17:09Z",
    "labels": [
      "P2",
      "docs",
      "train",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### Description\n\nWhen running `xgoobst_benchmark.py`, I didn't understand why only 1 checkpoint is saved for the last iteration.\r\n\r\nI re-read the air doc. I think the issues are:\r\n- after reading the concepts and the beginning of user guide using trainers, it feels like checkpoints are created automatically and work perfectly. See screenshot [1](https://user-images.githubusercontent.com/9677264/228928611-46033d62-0c8d-4228-ada5-2e7e837f51d9.png) , [2](https://user-images.githubusercontent.com/9677264/228928642-f4935c9f-b2a5-4d96-8e4c-00d5a40777f3.png)\r\n\r\n\r\n\r\n- When I continued to read the users guide: using trainers, it’s still not clear to me where the checkpoints come from except the dl trainers which have examples. Other trainers don’t have related instructions or examples\r\n- I didn’t see any links pointing to the checkpoint-related apis\r\n\r\nI feel like we either add a separate user guide about how to create and use checkpoints. Or for each trainer, we should always include instructions and examples about how to checkpoint\n\n### Link\n\nray air doc",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33844,
    "title": "Ray Workflow ",
    "author": "DhavalRepo18",
    "state": "open",
    "created_at": "2023-03-29T03:26:05Z",
    "updated_at": "2025-06-17T00:17:07Z",
    "labels": [
      "P2",
      "workflow",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nI was following the key-concept on Workflow\r\nhttps://docs.ray.io/en/latest/workflows/key-concepts.html\r\n\r\n![image](https://user-images.githubusercontent.com/47201243/228418532-e8d4aa08-7636-4866-bb39-ebde05192ab4.png)\r\n\r\n- [ ] There was no \"output\" variable defined above\r\n- [ ] when I replaced the line in red box by \"assert workflow.run(dag, workflow_id=\"run_1\") == 101\", it work \n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33815,
    "title": "[Train] Intermittent `UnpicklingError` when loading estimator/preprocessor from checkpoint",
    "author": "bdewilde",
    "state": "open",
    "created_at": "2023-03-28T18:55:34Z",
    "updated_at": "2025-06-17T00:17:05Z",
    "labels": [
      "bug",
      "P2",
      "train",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nWhen using a `BatchPredictor` initialized from an existing checkpoint, sometimes the `.predict()` call fails with a `cloudpickle.UnpicklingError`. There's no obvious cause for when/why the task fails. The specific error message — \"pickle data was truncated\" — is potentially misleading, since the checkpoint artifacts' serialized data _is_ fine: I can successfully load them manually via checkpoint methods `.get_estimator()` and `.get_preprocessor()`. The problem seems to be in the batch predictor itself.\r\n\r\nHere's a representative code blob:\r\n\r\n```python\r\ndataset = ray.data.read_parquet(\"[LOCAL_DATA_DIR]\")\r\ncheckpoint = ray.train.sklearn.SklearnCheckpoint(local_path=\"[LOCAL_PATH]\")\r\nbatch_predictor = BatchPredictor.from_checkpoint(\r\n    checkpoint, ray.train.sklearn.SklearnPredictor\r\n)\r\npreds = batch_predictor.predict(dataset)\r\n```\r\n\r\nAnd here's the logging output:\r\n\r\n```\r\n2023-03-28 14:35:44,229\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[read->Featurizer]\r\nread->Featurizer: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:54<00:00,  4.18s/it]\r\n2023-03-28 14:36:38,623\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(ScoringWrapper)]\r\nMapBatches(ScoringWrapper), 3 actors:  62%|█████████████████████████████████████████████████████████████████████████████████████████▊                                                        | 8/13 [00:18<00:06,  1.27s/it]\r\n---------------------------------------------------------------------------\r\nRayTaskError(UnpicklingError)             Traceback (most recent call last)\r\nCell In[12], line 4\r\n      1 batch_predictor = BatchPredictor.from_checkpoint(\r\n      2     checkpoint, ray.train.sklearn.SklearnPredictor\r\n      3 )\r\n----> 4 preds = batch_predictor.predict(dataset)\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/train/batch_predictor.py:334, in BatchPredictor.predict(self, data, feature_columns, keep_columns, batch_size, min_scoring_workers, max_scoring_workers, num_cpus_per_worker, num_gpus_per_worker, separate_gpu_stage, ray_remote_args, **predict_kwargs)\r\n    320 prediction_results = data.map_batches(\r\n    321     ScoringWrapper,\r\n    322     compute=compute,\r\n   (...)\r\n    329     **ray_remote_args,\r\n    330 )\r\n    332 if isinstance(prediction_results, ray.data.Dataset):\r\n    333     # Force execution because Dataset uses lazy execution by default.\r\n--> 334     prediction_results.fully_executed()\r\n    336 return prediction_results\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/dataset.py:3966, in Dataset.fully_executed(self)\r\n   3957 def fully_executed(self) -> \"Dataset[T]\":\r\n   3958     \"\"\"Force full evaluation of the blocks of this dataset.\r\n   3959 \r\n   3960     This can be used to read all blocks into memory. By default, Datasets\r\n   (...)\r\n   3964         A Dataset with all blocks fully materialized in memory.\r\n   3965     \"\"\"\r\n-> 3966     self._plan.execute(force_read=True)\r\n   3967     return self\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/plan.py:539, in ExecutionPlan.execute(self, allow_clear_input_blocks, force_read)\r\n    534 from ray.data._internal.execution.legacy_compat import (\r\n    535     execute_to_legacy_block_list,\r\n    536 )\r\n    538 executor = BulkExecutor(copy.deepcopy(context.execution_options))\r\n--> 539 blocks = execute_to_legacy_block_list(\r\n    540     executor,\r\n    541     self,\r\n    542     allow_clear_input_blocks=allow_clear_input_blocks,\r\n    543     dataset_uuid=self._dataset_uuid,\r\n    544 )\r\n    545 # TODO(ekl) we shouldn't need to set this in the future once we move\r\n    546 # to a fully lazy execution model, unless .cache() is used. The reason\r\n    547 # we need it right now is since the user may iterate over a Dataset\r\n    548 # multiple times after fully executing it once.\r\n    549 if not self._run_by_consumer:\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/legacy_compat.py:84, in execute_to_legacy_block_list(executor, plan, allow_clear_input_blocks, dataset_uuid)\r\n     82 else:\r\n     83     dag, stats = _to_operator_dag(plan, allow_clear_input_blocks)\r\n---> 84 bundles = executor.execute(dag, initial_stats=stats)\r\n     85 _set_stats_uuid_recursive(executor.get_stats(), dataset_uuid)\r\n     86 return _bundles_to_block_list(bundles)\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/bulk_executor.py:82, in BulkExecutor.execute(self, dag, initial_stats)\r\n     77     logger.get_logger(log_to_stdout=context.enable_auto_log_stats).info(\r\n     78         stats_summary_string,\r\n     79     )\r\n     80     return output\r\n---> 82 return execute_recursive(dag)\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/bulk_executor.py:63, in BulkExecutor.execute.<locals>.execute_recursive(op)\r\n     61             op.add_input(r, input_index=i)\r\n     62     op.inputs_done()\r\n---> 63     output = _naive_run_until_complete(op)\r\n     64 finally:\r\n     65     op.shutdown()\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/bulk_executor.py:106, in _naive_run_until_complete(op)\r\n    102 done, _ = ray.wait(\r\n    103     tasks, num_returns=len(tasks), fetch_local=True, timeout=0.1\r\n    104 )\r\n    105 for ready in done:\r\n--> 106     op.notify_work_completed(ready)\r\n    107 tasks = op.get_work_refs()\r\n    108 while op.has_next():\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:160, in ActorPoolMapOperator.notify_work_completed(self, ref)\r\n    157 if ref in self._tasks:\r\n    158     # Get task state and set output.\r\n    159     task, actor = self._tasks.pop(ref)\r\n--> 160     task.output = self._map_ref_to_ref_bundle(ref)\r\n    161     self._handle_task_done(task)\r\n    162     # Return the actor that was running the task to the pool.\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/operators/map_operator.py:296, in MapOperator._map_ref_to_ref_bundle(***failed resolving arguments***)\r\n    294 del ref\r\n    295 block_refs = all_refs[:-1]\r\n--> 296 block_metas = ray.get(all_refs[-1])\r\n    297 assert len(block_metas) == len(block_refs), (block_refs, block_metas)\r\n    298 for ref in block_refs:\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105, in client_mode_hook.<locals>.wrapper(*args, **kwargs)\r\n    103     if func.__name__ != \"init\" or is_client_mode_enabled_by_default:\r\n    104         return getattr(ray, func.__name__)(*args, **kwargs)\r\n--> 105 return func(*args, **kwargs)\r\n\r\nFile ~/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/_private/worker.py:2380, in get(object_refs, timeout)\r\n   2378     worker.core_worker.dump_object_store_memory_usage()\r\n   2379 if isinstance(value, RayTaskError):\r\n-> 2380     raise value.as_instanceof_cause()\r\n   2381 else:\r\n   2382     raise value\r\n\r\nRayTaskError(UnpicklingError): ray::_MapWorker.submit() (pid=47207, ip=127.0.0.1)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py\", line 272, in submit\r\n    yield from _map_task(fn, ctx, *blocks)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/operators/map_operator.py\", line 351, in _map_task\r\n    for b_out in fn(iter(blocks), ctx):\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/legacy_compat.py\", line 219, in do_map\r\n    yield from block_fn(blocks, ctx, *fn_args, **fn_kwargs)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/planner/map_batches.py\", line 102, in fn\r\n    yield from process_next_batch(batch)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/planner/map_batches.py\", line 66, in process_next_batch\r\n    batch = batch_fn(batch, *fn_args, **fn_kwargs)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/legacy_compat.py\", line 199, in fn\r\n    ray.data._cached_fn = fn_(\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/train/batch_predictor.py\", line 210, in __init__\r\n    self._predictor = predictor_cls.from_checkpoint(\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/train/sklearn/sklearn_predictor.py\", line 58, in from_checkpoint\r\n    estimator = checkpoint.get_estimator()\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/train/sklearn/sklearn_checkpoint.py\", line 72, in get_estimator\r\n    return cpickle.load(f)\r\n_pickle.UnpicklingError: pickle data was truncated\r\n```\r\n\r\nMeanwhile, both `checkpoint.get_estimator()` and `checkpoint.get_preprocessor()` return the objects as expected.\n\n### Versions / Dependencies\n\n`ray == 2.3`, `pandas == 1.5`, `pyarrow == 10.0`\r\nPY 3.9.13\r\nmacOS 13.2\n\n### Reproduction script\n\nUnfortunately this is not consistently reproducible.\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "bdewilde",
        "body": "Just following up: I managed to get a bit more info when running the problematic `.predict()` in a debugger. The stack trace is immediately preceded by a warning:\r\n\r\n```\r\n...\r\n2023-03-29 10:01:08,966\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)]\r\nMapBatches(<lambda>): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:02<00:00, 18.62it/s]\r\n2023-03-29 10:01:11,297\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[Featurizer]\r\nPGEFeaturizer: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:16<00:00,  2.62it/s]\r\n2023-03-29 10:01:27,419\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(ScoringWrapper)]\r\nMapBatches(ScoringWrapper):   0%|                                                                                               | 0/42 [00:00<?, ?it/s]2023-03-29 10:01:33,994\tWARNING worker.py:1866 -- WARNING: 24 PYTHON worker processes have been started on node: fb8237642a8d5d85309c32ab37979a8c2f74d5d5d7edc94c3b30d0b7 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\r\nray.exceptions.RayTaskError(UnpicklingError): ray::_MapWorker.submit() (pid=66085, ip=127.0.0.1)\r\n```\r\n\r\nwhere `Featurizer` is the trained `Preprocessor` serialized with the trained model.\r\n\r\nI've checked out the linked issue, and it's given me a couple ideas for fixes/workarounds. Still, as a user, I found this behavior and the error message to be very surprising."
      },
      {
        "user": "bdewilde",
        "body": "Okay, one more follow-up: Despite hacks at mitigating the issue, I'm still running into the original error or something like it. This did not happen in `ray` v2.1 -- it only started once I upgraded to v2.3. I'm also bumping the issue severity to High: I can't use `ray` for batch prediction right now.\r\n\r\nHere's another error pointing at the same problem within the `BatchPredictor`'s `ScoringWrapper`.\r\n\r\n```\r\nRead progress: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  9.62it/s]\r\n*** Temporarily disabling Ray worker logs ***\r\n> /Users/burtondewilde/Desktop/weavegrid/workspace/wg-data-projects/projects/my-project/src/wg/my_pkg/cli/predict.py(82)_predict()\r\n-> preds = batch_predictor.predict(dataset, **predict_kwargs)\r\n(Pdb) dataset.is_fully_executed()\r\nTrue\r\n(Pdb) n\r\n2023-03-29 12:55:20,879\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MyPreprocessor]\r\nMyPreprocessor: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [01:01<00:00,  4.71s/it]\r\n2023-03-29 12:56:22,221\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(ScoringWrapper)]\r\nMapBatches(ScoringWrapper):   0%|                                                                                               | 0/13 [00:00<?, ?it/s]ray.exceptions.RayTaskError(EOFError): ray::_MapWorker.submit() (pid=92004, ip=127.0.0.1)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py\", line 272, in submit\r\n    yield from _map_task(fn, ctx, *blocks)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/operators/map_operator.py\", line 351, in _map_task\r\n    for b_out in fn(iter(blocks), ctx):\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/legacy_compat.py\", line 219, in do_map\r\n    yield from block_fn(blocks, ctx, *fn_args, **fn_kwargs)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/planner/map_batches.py\", line 102, in fn\r\n    yield from process_next_batch(batch)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/planner/map_batches.py\", line 66, in process_next_batch\r\n    batch = batch_fn(batch, *fn_args, **fn_kwargs)\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/data/_internal/execution/legacy_compat.py\", line 199, in fn\r\n    ray.data._cached_fn = fn_(\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/train/batch_predictor.py\", line 210, in __init__\r\n    self._predictor = predictor_cls.from_checkpoint(\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/train/sklearn/sklearn_predictor.py\", line 58, in from_checkpoint\r\n    estimator = checkpoint.get_estimator()\r\n  File \"/Users/burtondewilde/.pyenv/versions/3.9.13/envs/my-env/lib/python3.9/site-packages/ray/train/sklearn/sklearn_checkpoint.py\", line 72, in get_estimator\r\n    return cpickle.load(f)\r\nEOFError: Ran out of input\r\n> /Users/burtondewilde/Desktop/weavegrid/workspace/wg-data-projects/projects/my-project/src/wg/my_pkg/cli/predict.py(82)_predict()\r\n-> preds = batch_predictor.predict(dataset, **predict_kwargs)\r\n```"
      },
      {
        "user": "amogkam",
        "body": "Thanks @bdewilde!\r\n\r\nCalling `checkpoint.get_estimator()` directly on the driver is not the same since it doesn't require the Checkpoint to be serialized and then deserialized.\r\n\r\nIf you try this, do you get the same error?\r\n\r\n```python\r\nimport ray\r\n\r\ncheckpoint = ray.train.sklearn.SklearnCheckpoint(local_path=\"[LOCAL_PATH]\")\r\n\r\n@ray.remote\r\ndef get_estimator(checkpoint):\r\n    return checkpoint.get_estimator()\r\n\r\nray.get(get_estimator.remote(checkpoint))\r\n```"
      }
    ]
  },
  {
    "issue_number": 33810,
    "title": "[AIR output] Warnings for AIR_VERBOSITY is confusing",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-03-28T17:48:29Z",
    "updated_at": "2025-06-17T00:17:03Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nConfusing instructions. Not sure what the second half of it means.\r\n\r\n`2023-03-28 09:36:07,926 WARNING tune.py:526 -- Testing new AIR console output flow with verbosity=1. This will also disable the old flow - setting it to 0 now.`\r\n\r\nCan we change it to something like \r\n`2023-03-28 09:36:07,926 WARNING tune.py:526 -- AIR_VERBOSITY is set to 1 and new AIR console output is enabled. To see the old console output, don't use or unset AIR_VERBOSITY.`\r\n\r\n\r\n### Versions / Dependencies\r\n\r\nNightly\r\n\r\n### Reproduction script\r\n\r\nn/a\r\n\r\n### Issue Severity\r\n\r\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "xwjiang2010",
        "body": "- `time_this_iter` vs `time_total_s`\r\n`time_total_s` means \"Accumulated time in seconds for this entire trial.\"\r\n`time_this_iter` is just for this past iteration.\r\n\r\n- \"running for 00:00:18:09\" captures the entire running time (from the beginning when you say `xx.fit()`).\r\n\r\n- You should see checkpoint information, if it is saved. But whether to save it is a user call. Can you first make sure that checkpoint saving is turned on for every iteration? If it's off, we won't print that line for the iteration.\r\n\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "xwjiang2010",
        "body": "- \"Configurations for TorchTrainer: \"\r\nwhat should we say if there is no specified configuration for starting the trainer (a lot of times the case)?\r\n\r\nRight now, I make it \r\n\"started with configuration {...}\"  v.s. \"started\"."
      },
      {
        "user": "scottsun94",
        "body": "> * `time_this_iter` vs `time_total_s`\r\n>   `time_total_s` means \"Accumulated time in seconds for this entire trial.\"\r\n>   `time_this_iter` is just for this past iteration.\r\n> * \"running for 00:00:18:09\" captures the entire running time (from the beginning when you say `xx.fit()`).\r\n> * You should see checkpoint information, if it is saved. But whether to save it is a user call. Can you first make sure that checkpoint saving is turned on for every iteration? If it's off, we won't print that line for the iteration.\r\n\r\nGot it. Thanks! We can do something like this then?\r\n- Training time of this iteration == `time_this_iter `\r\n- Total running time == `time_total_s` at the end of last iteration\r\n- Total training time == sum of all iterations' `time_this_iter`?\r\n```\r\nIteration 4 finished at 2023-03-28 10:09:42. Training time of this iteration: 8.96s\r\n-------------------------\r\nloss             2.24726\r\n-------------------------\r\nCheckpoint saved at: mnt/ray_result/...\r\n\r\nTraining finished (4 iterations) at 2023-02-24 12:35:39. Total training time: 39.68s. Total running time: 46.97s\r\nLast checkpoint saved at: mnt/ray_result/...\r\n```"
      }
    ]
  },
  {
    "issue_number": 33803,
    "title": "[air output] Aggregation of feedback for air output v2",
    "author": "xwjiang2010",
    "state": "open",
    "created_at": "2023-03-28T16:50:19Z",
    "updated_at": "2025-06-17T00:17:01Z",
    "labels": [
      "P2",
      "train",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "From Kai:\r\n\r\n* in rich table view, the term `total time (s)` in the table is a bit confusing. As the `running for ...` is being updated but `total_time (s)` is staying stale. It actually means \"total time (s) since last time we receive from the trainable.\"\r\n\r\n* we need to shorten significant bits - there is no need of displaying so many of them!\r\n\r\n* non-rich + tune, table refresh rate is every 30 seconds, this is a bit too long.\r\n\r\ncc @scottsun94 ",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33777,
    "title": "[Datasets] `FileBasedDataSource`s do not pass `filesystem` to `_read_stream()` methods' `reader_args`",
    "author": "scottjlee",
    "state": "open",
    "created_at": "2023-03-27T23:58:47Z",
    "updated_at": "2025-06-17T00:16:59Z",
    "labels": [
      "bug",
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nCurrently, all of the `read_api` methods which read from a file-based datasource (e.g. `read_csv`, `read_tfrecords`, etc) all get routed through the central `read_datasource()` method, which calls `FileBasedDatasource.create_reader()` when getting read tasks. In particular, the `filesystem` argument is passed as `kwargs` of `create_reader()`, which is **captured by the `filesystem` parameter** and read into `_FileBasedDatasourceReader._filesystem` [here](https://github.com/ray-project/ray/blob/master/python/ray/data/datasource/file_based_datasource.py#L388). Since it matches the `filesystem` keyword directly, it is **not** stored in `**reader_args`, and as a result, `filesystem` is not accessible from `read_stream()`'s `reader_args` [here](https://github.com/ray-project/ray/blob/master/python/ray/data/datasource/file_based_datasource.py#L490). \r\n\r\nSince we are already passing the filesystem to `read_files` [here](https://github.com/ray-project/ray/blob/master/python/ray/data/datasource/file_based_datasource.py#L446), we could potentially fix this by including `for data in read_stream(f, read_path, filesystem=fs, **reader_args)` [here](https://github.com/ray-project/ray/blob/f19018edb28ac222edd8d57222971625f354d5e2/python/ray/data/datasource/file_based_datasource.py#L490).\n\n### Versions / Dependencies\n\nRay master\n\n### Reproduction script\n\n```\r\nimport ray\r\nimport pyarrow as pa\r\nimport tempfile\r\n\r\ndatasource = ray.data.datasource.FileBasedDatasource()\r\nfs = pa.fs.LocalFileSystem()\r\nkwargs = {\"filesystem\": fs}\r\n\r\nwith tempfile.NamedTemporaryFile() as f:\r\n\tpath = f.name\r\n\treader = datasource.create_reader(paths=path, **kwargs)\r\n\tassert reader._filesystem, \"_filesystem not found in reader\" # succeeds\r\n\tassert \"filesystem\" in reader._reader_args, \"Filesystem not found in _reader_args\" # fails\r\n```\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33746,
    "title": "[Core][Runtime Env] Document how to write custom runtime env plugin",
    "author": "jjyao",
    "state": "open",
    "created_at": "2023-03-27T16:55:39Z",
    "updated_at": "2025-06-17T00:16:57Z",
    "labels": [
      "P2",
      "docs",
      "core",
      "core-runtime-env",
      "pending-cleanup"
    ],
    "body": "### Description\n\nOnce we GA the custom runtime env plugin, we should document how to write your own runtime env plugin.\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33735,
    "title": "Core: Can the ray core's scheduling mechanism support customized extensions?",
    "author": "EnjianGong",
    "state": "open",
    "created_at": "2023-03-27T07:09:54Z",
    "updated_at": "2025-06-17T00:16:55Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "core-scheduler",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nMost of our company's business is some AP offline tasks, so the resource utilization rate of the entire ray cluster is very low, and there are a lot of idle resource fragments in time and space. Therefore, I look forward to using DQN to learn the resource wave model to predict resource occupancy for a period of time in the future, and then share these idle resources to reduce costs. Therefore, we want to customize Ray core's scheduling mechanism to adapt to workload fluctuations\r\n\r\n\r\n\r\n### Use case\r\n\r\nWorkload scenario: There are a lot of idle resources in space and time, and the resource occupation has a strong periodicity",
    "comments": [
      {
        "user": "rkooo567",
        "body": "cc @jjyao "
      },
      {
        "user": "EnjianGong",
        "body": "Is there a solution to this issue @clarng "
      },
      {
        "user": "rkooo567",
        "body": "Currently this is not possible"
      }
    ]
  },
  {
    "issue_number": 33672,
    "title": "[Ray init]  Ray init method does not support pathlib.Path",
    "author": "lcaquot94",
    "state": "open",
    "created_at": "2023-03-24T15:18:02Z",
    "updated_at": "2025-06-17T00:16:53Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "core-api",
      "pending-cleanup"
    ],
    "body": "### Description\n\nRay init method does not support pathlib.Path objects for address and storage parameters. Only string objects are supported. It could be nice to support Path objects to avoid conversion such as: \r\n\r\n```\r\nray.init(storage=Path('/home') / 'ray' / 'workflow_data') # Not working\r\nray.init(storage=str(Path('/home') / 'ray' / 'workflow_data')) # Working\r\n```\n\n### Use case\n\nI want to init ray using Pathlib objects.",
    "comments": [
      {
        "user": "rkooo567",
        "body": "Feel free to make contribution! I believe it is a pretty simple to make it work"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33645,
    "title": "[docs] improve user experience of the API ref",
    "author": "angelinalg",
    "state": "open",
    "created_at": "2023-03-23T21:09:27Z",
    "updated_at": "2025-06-17T00:16:50Z",
    "labels": [
      "P2",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nI wanted to capture this feedback I received on the API ref. \r\n\r\nLooking at the parameters for Ray AIR's BatchMapper https://docs.ray.io/en/latest/ray-air/api/doc/ray.data.preprocessors.BatchMapper.html\r\nI wanted to know what batch_size was because it's pretty important when doing batch processing. The first thing I see is that batch_size will be of type Optional[Union[int, typing_extensions.Literal[default]]] = 'default') which is not very helpful to me.\r\nHowever, I see that it's optional, so that makes me wonder what the default value is. So I scroll down to the bottom of the page, and hidden in a long-ish paragraph is what I'm looking for: \"Defaults to 4096\"\r\n\r\nvisually, our API refs are hard to read\r\n![image](https://user-images.githubusercontent.com/122562471/227361572-5161306e-5c2c-4955-9b2f-d4a2cebf92bd.png)\r\n \r\n![image](https://user-images.githubusercontent.com/122562471/227361617-6b4a0ed5-7a19-4eac-839a-b53c4b72e902.png)\r\n\r\ncc: @maxpumperla @bveeramani @simran-2797 @emmyscode \n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "justinvyu",
        "body": "Some more comments here: https://github.com/ray-project/ray/issues/32824"
      },
      {
        "user": "bveeramani",
        "body": "I think ideally we'd want something simple and readable like \r\n\r\n```\r\nbatch_size: int = 4096\r\n```\r\n\r\nFor context, the type hint used to be\r\n\r\n```\r\nbatch_size: int | None = None\r\n```\r\n\r\nBut with https://github.com/ray-project/ray/pull/29971 and [Decide batch behavior for Ray AIR](https://docs.google.com/document/d/1oJN7DvglL0OVALfzxfW_V8x1qhUS3KR6sj0Y44Qhf2s/edit#), we changed the type hint to\r\n\r\n```\r\nbatch_size: int | \"default\" | None = \"default\"\r\n```\r\n\r\ncc @c21 @amogkam would it be possible to simplify the `batch_size` types? Not sure if we needed the `\"default\"` to avoid breaking changes.\r\n"
      },
      {
        "user": "c21",
        "body": "Hugging Face: https://huggingface.co/docs/datasets/v2.12.0/en/package_reference/main_classes#datasets.Dataset.map\r\nRay: https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches\r\n\r\n<img width=\"844\" alt=\"Screen Shot 2023-05-01 at 10 16 20 AM\" src=\"https://user-images.githubusercontent.com/4629931/235495801-140a068e-4142-40aa-b03a-dcb55f73b81e.png\">\r\n<img width=\"701\" alt=\"Screen Shot 2023-05-01 at 10 17 47 AM\" src=\"https://user-images.githubusercontent.com/4629931/235495803-3137790d-81b3-45ea-9033-d1150c2b27c2.png\">\r\n"
      }
    ]
  },
  {
    "issue_number": 33636,
    "title": "[RLLib] Collecting external experience",
    "author": "ashutosh1906",
    "state": "open",
    "created_at": "2023-03-23T17:16:39Z",
    "updated_at": "2025-06-17T00:16:48Z",
    "labels": [
      "P2",
      "rllib",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nI was trying to collect and store external experience following this [Example: Converting external experiences to batch format](https://docs.ray.io/en/latest/rllib/rllib-offline.html#:~:text=v_behavior_std%27%3A%20...%2C%20%27v_target_std%27%3A%20...%2C%20%27v_delta%27%3A%20...%7D-,Example%3A%20Converting%20external%20experiences%20to%20batch%20format,-%23)\r\n\r\nHowever, while I run it, it shows that the \"SampleBatchBuilder\" will be deprecated soon. Can you please provide an example of how we can collect this experience using \"SampleCollector\"? Thank you.\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "ashutosh1906",
        "body": "Can anyone please help regarding this issue?"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33633,
    "title": "[Workflow] get_metadata(workflow_id)[\"status\"] and get_status(workflow_id) not returning the same status",
    "author": "lcaquot94",
    "state": "open",
    "created_at": "2023-03-23T15:59:55Z",
    "updated_at": "2025-06-17T00:16:46Z",
    "labels": [
      "bug",
      "P2",
      "workflow",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nI got interesting behaviour when trying to know my workflow status:\r\n\r\nOption 1 : ray.workflow.get_status(workflow_id) --> RESUMABLE\r\nOption 2 : ray.workflow.get_metadata(workflow_id)[\"status\"] --> RUNNING\r\n\r\nI call those two options with same workflow_id at the same time, and those two options does return the same status.\r\nNote that bug does not appear for every workflow status, but at least for \"RESUMABLE\" state.\r\n\r\nI think option 2 is wrong because even if I wait for a long time, the workflow does not change its state. So it does not seem RUNNING anymore.\r\n\r\n### Versions / Dependencies\r\n\r\nRay 2.1.0\r\n\r\n### Reproduction script\r\n\r\nimport time\r\nimport ray\r\n\r\n@ray.remote\r\ndef task():\r\n    time.sleep(10)\r\n\r\nworkflow_id = '12345'\r\nray.workflow.run_async(task.bind(), workflow_id=workflow_id)\r\n\r\ntime.sleep(1)\r\nprint(f'\\n{workflow_id}: status from metadata: {ray.workflow.get_metadata(workflow_id)[\"status\"]}')\r\nprint(f'{workflow_id}: status from status: {ray.workflow.get_status(workflow_id)}')\r\n\r\n'''Do something that makes the workflow resumable but I don't know what... '''\r\n\r\ntime.sleep(1)\r\nprint(f'\\n{workflow_id}: status from metadata: {ray.workflow.get_metadata(workflow_id)[\"status\"]}')\r\nprint(f'{workflow_id}: status from status: {ray.workflow.get_status(workflow_id)}')\r\n\r\n### Issue Severity\r\n\r\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33607,
    "title": "[runtime_env] Actors always depend global `pip` field for `runtime_env`",
    "author": "shrekris-anyscale",
    "state": "open",
    "created_at": "2023-03-23T00:02:23Z",
    "updated_at": "2025-06-17T00:16:44Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-runtime-env",
      "core-api",
      "stability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nI tried to launch an actor that had no `runtime_env` dependencies, but it didn't get started until the top level `runtime_env` passed into `ray.init()` finished installing.\r\n\r\nI expect to be able to either:\r\n1. install the `runtime_env` fully before the actor runs, so the actor doesn't need to wait for the installation.\r\n2. specify a per-actor `runtime_env` that fully overrides the parent `runtime_env`, so the actor doesn't need to wait for the installation.\n\n### Versions / Dependencies\n\nRay on the latest master.\n\n### Reproduction script\n\n```python\r\n# repro.py\r\n\r\nimport ray\r\n\r\nruntime_env = {\"pip\": [\"this_is_not_a_real_package_asadf\"]}\r\nray.init(runtime_env=runtime_env, namespace=\"serve\")\r\n\r\n@ray.remote(\r\n    runtime_env={\r\n        \"pip\": [],\r\n    }\r\n)\r\nclass A:\r\n\r\n    def __call__(self, *args, **kwargs):\r\n        return \"hi\"\r\n\r\nprint(\"check2\")\r\n\r\na = A.remote()\r\n\r\nprint(\"check3\")\r\n\r\nprint(ray.get(a.__call__.remote()))\r\n\r\nprint(\"check4\")\r\n```\r\n\r\n```console\r\n$ python repro.py\r\n\r\n2023-03-22 16:48:16,536\tINFO worker.py:1550 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265 \r\ncheck2\r\ncheck3\r\nTraceback (most recent call last):\r\n  File \"repro.py\", line 22, in <module>\r\n    print(ray.get(a.__call__.remote()))\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/_private/client_mode_hook.py\", line 105, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/_private/worker.py\", line 2428, in get\r\n    raise value\r\nray.exceptions.RuntimeEnvSetupError: Failed to set up runtime environment.\r\nCould not create the actor because its associated runtime env failed to be created.\r\nTraceback (most recent call last):\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/dashboard/modules/runtime_env/runtime_env_agent.py\", line 357, in _create_runtime_env_with_retry\r\n    runtime_env_context = await asyncio.wait_for(\r\n  File \"/Users/shrekris/miniforge3/envs/ae/lib/python3.8/asyncio/tasks.py\", line 494, in wait_for\r\n    return fut.result()\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/dashboard/modules/runtime_env/runtime_env_agent.py\", line 312, in _setup_runtime_env\r\n    await create_for_plugin_if_needed(\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/_private/runtime_env/plugin.py\", line 252, in create_for_plugin_if_needed\r\n    size_bytes = await plugin.create(uri, runtime_env, context, logger=logger)\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/_private/runtime_env/pip.py\", line 473, in create\r\n    return await task\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/_private/runtime_env/pip.py\", line 455, in _create_for_hash\r\n    await PipProcessor(\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/_private/runtime_env/pip.py\", line 361, in _run\r\n    await self._install_pip_packages(\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/_private/runtime_env/pip.py\", line 337, in _install_pip_packages\r\n    await check_output_cmd(pip_install_cmd, logger=logger, cwd=cwd, env=pip_env)\r\n  File \"/Users/shrekris/Desktop/ray/python/ray/_private/runtime_env/utils.py\", line 101, in check_output_cmd\r\n    raise SubprocessCalledProcessError(\r\nray._private.runtime_env.utils.SubprocessCalledProcessError: Run cmd[9] failed with the following details.\r\nCommand '['/tmp/ray/session_2023-03-22_16-48-09_709570_42993/runtime_resources/pip/8dfa33a97a688ee7b784034df121eaf4ff642542/virtualenv/bin/python', '-m', 'pip', 'install', '--disable-pip-version-check', '--no-cache-dir', '-r', '/tmp/ray/session_2023-03-22_16-48-09_709570_42993/runtime_resources/pip/8dfa33a97a688ee7b784034df121eaf4ff642542/requirements.txt']' returned non-zero exit status 1.\r\nLast 50 lines of stdout:\r\n    ERROR: Could not find a version that satisfies the requirement this_is_not_a_real_package_asadf (from versions: none)\r\n    ERROR: No matching distribution found for this_is_not_a_real_package_asadf\r\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "rkooo567",
        "body": "Seems to be a API correctness bug"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33554,
    "title": "[Core] Raylet process not respecting `--node-ip-address`",
    "author": "daniel-ziegler",
    "state": "open",
    "created_at": "2023-03-21T22:05:29Z",
    "updated_at": "2025-06-17T00:16:42Z",
    "labels": [
      "P2",
      "docs",
      "core",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nI'm running a Ray cluster inside a VPN. I'm launching the head node and worker nodes with `ray start --node-ip-address 10.8.0.*` so that they know to use VPN IP addresses. Most logs indeed show those addresses, but I still get the following sorts of errors. `10.19.***` is the IP address for the worker on the Ethernet interface, but I want it to use the OpenVPN `tun0` interface.\r\n```\r\n(raylet, ip=10.19.***) [2023-03-21 21:53:44,254 I 76852 76852] global_state_accessor.cc:356: This node has an IP address of 10.19.***, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\r\n(raylet, ip=10.19.***) [2023-03-21 21:53:53,454 E 76852 78530] core_worker_process.cc:216: Failed to get the system config from raylet because it is dead. Worker will terminate. Status: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details:  .Please see `raylet.out` for more details.\r\n(raylet, ip=10.19.***) [2023-03-21 21:53:53,474 E 76597 76853] (raylet) agent_manager.cc:135: The raylet exited immediately because the Ray agent failed. The raylet fate shares with the agent. This can happen because the Ray agent was unexpectedly killed or failed. Agent can fail when\r\n(raylet, ip=10.19.***) - The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.\r\n(raylet, ip=10.19.***) - The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/dashboard_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.\r\n(raylet, ip=10.19.***) - The agent is killed by the OS (e.g., out of memory).\r\n```\r\n\r\nI would have thought that the correct `--node-ip-address` value would make it to this code, but it apparently does not: https://github.com/ray-project/ray/blob/3aa6ede43743a098b5e0eb37ec11505f46100313/cpp/src/ray/util/process_helper.cc#L86-L93\r\n\r\n\r\n### Versions / Dependencies\r\n\r\nPython 3.10.9\r\nRay 2.3.0\r\nUbuntu 20.04\r\n\r\n### Reproduction script\r\n\r\nDepends on my networking setup unfortunately. Happy to provide more details.\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "daniel-ziegler",
        "body": "Possibly this is a dup of #22732 but it seems like the fix might be much simpler"
      },
      {
        "user": "jjyao",
        "body": "@daniel-ziegler,\r\n\r\nCould you try `ray.init(_node_ip_address=\"10.8.0.*\")` in your Ray application. This ip address should match the ip address of the node where you run your application (normally you run on the head node)."
      },
      {
        "user": "jjyao",
        "body": "@daniel-ziegler,\r\n\r\nDoes setting `_node_ip_address` work for you?"
      }
    ]
  },
  {
    "issue_number": 33540,
    "title": "[Tune] Support ExperimentAnalysis.dataframe(mode='mean')",
    "author": "cool-RR",
    "state": "open",
    "created_at": "2023-03-21T19:40:49Z",
    "updated_at": "2025-06-17T00:16:39Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "UX",
      "pending-cleanup"
    ],
    "body": "### Description\n\nIt'll be cool if `ExperimentAnalysis.dataframe` could take an argument of `mode='mean'`. Right now it can only do minimum and maximum.\n\n### Use case\n\nI want to do a comparison between several different settings, and getting the average performance highest is my goal.",
    "comments": [
      {
        "user": "Yard1",
        "body": "Hi @cool-RR, you can retrieve individual trial dataframes (with `ExperimentAnalysis.trial_dataframes`), calculate the mean, and then put them together into the final dataframe. Let me know if that would help you!"
      },
      {
        "user": "cool-RR",
        "body": "Sure, I wrote some code to do that. If you think this use case isn't common\nenough, feel free to close this feature request.\n\nOn Wed, Mar 22, 2023, 20:10 Antoni Baum ***@***.***> wrote:\n\n> Hi @cool-RR <https://github.com/cool-RR>, you can retrieve individual\n> trial dataframes (with ExperimentAnalysis.trial_dataframes), calculate\n> the mean, and then put them together into the final dataframe. Let me know\n> if that would help you!\n>\n> —\n> Reply to this email directly, view it on GitHub\n> <https://github.com/ray-project/ray/issues/33540#issuecomment-1480039131>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAAN3SWGZTS24MAMLOLN5RTW5M6BRANCNFSM6AAAAAAWC43ILY>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n"
      },
      {
        "user": "Yard1",
        "body": "We'll keep it open as a P2 item, thanks!"
      }
    ]
  },
  {
    "issue_number": 33539,
    "title": "[Train] `RunConfig` doesn't get propagated from the Tuner to the Trainer",
    "author": "justinvyu",
    "state": "open",
    "created_at": "2023-03-21T19:38:42Z",
    "updated_at": "2025-06-17T00:16:37Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "train",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nWhen specifying `RunConfig` through the Tuner and not the Trainer, the Trainer does not have access to the run config later on.\r\n\r\nWe should either:\r\n1. Propagate the Tuner's run config down to the trainer.\r\n2. At least initialize the Trainer with a default run config and make it clear in the docs that you can't access these properties in a Trainer.\r\n\r\ncc: @gjoliver \n\n### Versions / Dependencies\n\n2.3.0\n\n### Reproduction script\n\n```python\r\nfrom ray.train.torch import TorchTrainer\r\nfrom ray.air.config import ScalingConfig, RunConfig\r\nfrom ray.tune import Tuner\r\n\r\n\r\nclass MyTrainer(TorchTrainer):\r\n    def training_loop(self) -> None:\r\n        assert self.run_config.local_dir, \"dude ... !\"\r\n\r\ntrainer = MyTrainer(\r\n    train_loop_per_worker=lambda: None,\r\n    scaling_config=ScalingConfig(num_workers=2)\r\n)\r\n\r\ntuner = Tuner(trainer, run_config=RunConfig(local_dir=\"hi/jun\"))\r\n\r\ntuner.fit()\r\n```\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33525,
    "title": "[Core] std::bad_alloc error using ray.init()",
    "author": "Yue-Li-atBain",
    "state": "open",
    "created_at": "2023-03-21T16:34:34Z",
    "updated_at": "2025-06-17T00:16:35Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-correctness",
      "stability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nI was trying to run some code with ray inside a docker image, but ray.init() throws std::bad_alloc error. The error remains even if I set object memory or _memory to less than 1GB. \r\n\r\n\r\n### Versions / Dependencies\r\n\r\nThe docker image was build with the following packages:\r\nfirst\r\ncontinuumio/miniconda3:4.10.3\r\n\r\nchannels: \r\n  - conda-forge\r\ndependencies: \r\n  #core\r\n  - ray-core = 2.3.0\r\n  - tensorflow = 2.9\r\n  - tensorflow-probability\r\n  - pandas=1.3.4\r\n  - u8darts-all=0.19.0\r\n  - jupyterlab=3.2.4\r\n  - ipython=8.3.0\r\n  - numpy=1.22.3\r\n  - pillow=9.1.1\r\n  - ujson=5.4.0\r\n  - jinja2=3.1.2\r\n  - jupyter_server=1.17.1\r\n  - notebook=6.4.12\r\n  - hydra-core=1.2\r\n  - openpyxl=3.0.9\r\n  - mlflow=1.26.0\r\n  - libstdcxx-ng\r\n  - plotly=5.8.2\r\n  #dev\r\n  - pytest=6.2.5\r\n  - pytest-cov=3.0.0\r\n  - sphinx=4.3.0\r\n  - black=22.3.0\r\n  - pytest-helpers-namespace=2021.12.29\r\n  - pip\r\n  - pip:\r\n    - streamlit==1.11.1\r\n    - streamlit-aggrid==0.3.4.post3\r\n    - rsconnect-python==1.15.0\r\n    - statsforecast==0.7.1 # 1.0.0 is does not work with the current version of DARTS\r\n    - pytest-regtest==1.5.0\r\n\r\n### Reproduction script\r\n\r\nThe error occurs with the first call of ray.init()\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "cadedaniel",
        "body": "Hi @Yue-Li-atBain , do you have a Dockerfile that installs these dependencies in a way that reproduces the issue?"
      },
      {
        "user": "Yue-Li-atBain",
        "body": "The docker file content is as follows. The environment.yaml contains the packages list above\r\n```\r\nFROM continuumio/miniconda3:4.10.3 AS main\r\n\r\nRUN apt-get -y --allow-releaseinfo-change update && \\\r\n    apt-get -y install build-essential && \\\r\n    apt-get -y install dos2unix # required to execute ops\\clean_files.sh on windows\r\nRUN conda config --set ssl_verify false\r\nRUN conda update -n base -c defaults conda\r\n\r\nWORKDIR /opt/app\r\n\r\nCOPY --chown=1000:100 environment.yaml .\r\nRUN conda env update -q --name base --file environment.yaml\r\nRUN pip install flask-session==0.4.0\r\n\r\nCOPY src src\r\nCOPY setup.py .\r\nRUN pip install -e .\r\n\r\n```"
      },
      {
        "user": "cadedaniel",
        "body": "Can you share `environment.yaml` as well? Want to make sure we can reproduce exactly what you're seeing :). Also, how is Ray installed? I don't see it in the packages listed above or this dockerfile.\r\n\r\nLastly, what is `pip install -e .` installing?"
      }
    ]
  },
  {
    "issue_number": 33491,
    "title": "[Core] `test_memory_deadlock` times out",
    "author": "cadedaniel",
    "state": "open",
    "created_at": "2023-03-21T00:37:29Z",
    "updated_at": "2025-06-17T00:16:32Z",
    "labels": [
      "P2",
      "core",
      "pending-cleanup"
    ],
    "body": "In https://github.com/ray-project/ray/pull/29622 we discovered that `test_memory_deadlock.py` was not actually running (it was missing an entrypoint). @Yard1 tried enabling it but it times out. We need to diagnose the failure to re-enable these tests.",
    "comments": [
      {
        "user": "Yard1",
        "body": "Please see https://github.com/ray-project/ray/commit/c71d90e6d34c44a7cf9314c659e4000a95308a96#diff-b1f449b3bae27babe6b169d6e7143df65f2e627ebb2af3c5a9444a743052c9d4 for instructions on how to reenable the test in CI"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33465,
    "title": "[Core] Support binding worker processes to NUMA nodes",
    "author": "cadedaniel",
    "state": "open",
    "created_at": "2023-03-20T17:33:06Z",
    "updated_at": "2025-06-17T00:16:30Z",
    "labels": [
      "P2",
      "core",
      "core-worker",
      "pending-cleanup"
    ],
    "body": "Currently users can use [os.sched_setaffinity](https://docs.python.org/3/library/os.html#os.sched_setaffinity) to set scheduling affinity in their actors/tasks. This likely addresses many use-cases, however it allows for the physical pages of process memory to be allocated on other NUMA nodes (particularly, the ones copied before the actor is created). This will be limiting to performance-sensitive workloads, I'm creating this issue for tracking and to gather use-cases.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33456,
    "title": "[Serve] Support for setting `working_dir` to a local directory in `RayService`",
    "author": "smit-kiri",
    "state": "open",
    "created_at": "2023-03-20T13:17:19Z",
    "updated_at": "2025-06-17T00:16:28Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "kuberay",
      "core-runtime-env",
      "pending-cleanup"
    ],
    "body": "### Description\n\nCurrently, the `serveConfig.runtimeEnv.working_dir` in a `RayService` can only be a remote URI to a zip file or a local path to a zip file. You cannot set it to a local directory.\n\n### Use case\n\nI'm trying to deploy a custom docker image built on top of Ray with the deployment code copied over in the image at some path. I cannot set the `working_dir` to that specified path because it is not a zip file. The two options to get around this are\r\n\r\n1. Zip the code and copy it over to the docker file. Set the `working_dir` to that zip file\r\n2. Set the `WORKDIR` in the `Dockerfile` to the path that contains the deployment code.\r\n\r\nIntuitively, you should be able to set the `working_dir` to point to a `dir` instead of a ` zip file`.",
    "comments": [
      {
        "user": "architkulkarni",
        "body": "Thanks for the feature request! We have a [proposed API](https://github.com/ray-project/ray/issues/26784) for this that would work more broadly for any `runtime_env` (not specific to RayService), but it hasn't been implemented yet and we don't have a concrete timeline for it"
      },
      {
        "user": "Repeater9",
        "body": "When I use the command `serve deploy xxx.yml` of ray serve cli to deploy my RayService, I meet the same problem. \r\nThe `working_dir` attribute in the yaml file cannot be a local directory. \r\nI have to manually follow the api `PUT /api/packages/gcs/{package_name}` to upload my local directory to ray gcs and change the `working_dir` attribute to `gcs://{package_name}` in advance, just as `ray job submit --working-dir={local_dir}` does."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33425,
    "title": "RLLIB - RE3 Exploration Algorithm - No GPU support f0r Dynamic TF V2",
    "author": "heinerb",
    "state": "open",
    "created_at": "2023-03-17T20:49:55Z",
    "updated_at": "2025-06-17T00:16:26Z",
    "labels": [
      "bug",
      "P2",
      "rllib",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nIssue RE3 exploration on PPO does not call the `on_learn_on_batch` callback when utilizing for Dynamic TF V2. This limits the ability to utilize GPUs on larger complex environments when exploring algorithm extension. \r\n\r\n- https://docs.ray.io/en/latest/rllib/rllib-algorithms.html\r\n\r\nCurrently there is not path in `/opt/conda/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy_v2.py` which calls the `on_learn_on_batch` call during processing. \r\n\r\n```\r\ndef learn_on_loaded_batch(self, offset: int = 0, buffer_index: int = 0):\r\n        # Shortcut for 1 CPU only: Batch should already be stored in\r\n        # `self._loaded_single_cpu_batch`.\r\n        if len(self.devices) == 1 and self.devices[0] == \"/cpu:0\":\r\n            assert buffer_index == 0\r\n            if self._loaded_single_cpu_batch is None:\r\n                raise ValueError(\r\n                    \"Must call Policy.load_batch_into_buffer() before \"\r\n                    \"Policy.learn_on_loaded_batch()!\"\r\n                )\r\n            # Get the correct slice of the already loaded batch to use,\r\n            # based on offset and batch size.\r\n            batch_size = self.config.get(\r\n                \"sgd_minibatch_size\", self.config[\"train_batch_size\"]\r\n            )\r\n            if batch_size >= len(self._loaded_single_cpu_batch):\r\n                sliced_batch = self._loaded_single_cpu_batch\r\n            else:\r\n                sliced_batch = self._loaded_single_cpu_batch.slice(\r\n                    start=offset, end=offset + batch_size\r\n                )\r\n            return self.learn_on_batch(sliced_batch)\r\n\r\n        tower_stack = self.multi_gpu_tower_stacks[buffer_index]\r\n        results = tower_stack.optimize(self.get_session(), offset)\r\n        self.num_grad_updates += 1\r\n\r\n        results.update(\r\n            {\r\n                NUM_GRAD_UPDATES_LIFETIME: self.num_grad_updates,\r\n                # -1, b/c we have to measure this diff before we do the update above.\r\n                DIFF_NUM_GRAD_UPDATES_VS_SAMPLER_POLICY: (\r\n                    self.num_grad_updates - 1 - (tower_stack.num_grad_updates or 0)\r\n                ),\r\n            }\r\n        )\r\n\r\n        return results\r\n```\r\n\r\nMissing calls for GPU path\r\n```\r\nself.callbacks.on_learn_on_batch(\r\n            policy=self, train_batch=postprocessed_batch, result=learn_stats\r\n        )\r\n```\r\n### Versions / Dependencies\r\n\r\nray --version\r\nray, version 2.2.0\r\n\r\npython --version\r\nPython 3.10.8\r\n\r\nUbuntu\r\n\r\n### Reproduction script\r\n\r\nExample code: \r\n\r\n```Python\r\nfrom functools import partial\r\nimport ray\r\nfrom ray.rllib.algorithms import ppo\r\n\r\nfrom ray.rllib.algorithms import sac\r\nfrom ray.rllib.algorithms.callbacks import MultiCallbacks, RE3UpdateCallbacks\r\nfrom ray.rllib.policy import Policy\r\nfrom ray.rllib.policy.sample_batch import SampleBatch\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ray.init()\r\n\r\n    config = (\r\n        ppo.PPOConfig()\r\n        .environment(\"Pendulum-v1\")\r\n        # Add type as RE3 in the exploration_config parameter\r\n        .exploration(\r\n            exploration_config={\r\n                \"type\": \"RE3\",\r\n                \"sub_exploration\": {\r\n                    \"type\": \"StochasticSampling\",\r\n                },\r\n            }\r\n        )\r\n        # *******************************************\r\n        # **UNCOMMNET LINE TO ENABLE GPU**\r\n        # *******************************************\r\n        # .resources(num_gpus=0.5)\r\n        .debugging(seed=12345)\r\n    )\r\n\r\n    class RE3Callbacks(RE3UpdateCallbacks):\r\n        def __init__(\r\n            self,\r\n            *args,\r\n            embeds_dim: int = 128,\r\n            k_nn: int = 50,\r\n            beta: float = 0.1,\r\n            rho: float = 0.0001,\r\n            beta_schedule: str = \"constant\",\r\n            **kwargs,\r\n        ):\r\n            super().__init__(*args, **kwargs)\r\n\r\n        def on_learn_on_batch(\r\n            self,\r\n            *,\r\n            policy: Policy,\r\n            train_batch: SampleBatch,\r\n            result: dict,\r\n            **kwargs,\r\n        ):\r\n            print(\"Process Works!\")\r\n            super().on_learn_on_batch(policy=policy, train_batch=train_batch, result=result, kwargs=kwargs)\r\n\r\n    # Add a new RE3UpdateCallbacks\r\n    config.callbacks(\r\n        MultiCallbacks(\r\n            [\r\n                config.callbacks_class,\r\n                RE3Callbacks\r\n            ]\r\n        )\r\n    )\r\n    \r\n    config.simple_optimizer = False\r\n\r\n    algo = config.training()\r\n    t = ray.tune.Tuner(\r\n        \"PPO\",\r\n        param_space=algo\r\n    )\r\n\r\n    t.fit()\r\n ```\r\n \r\n- GPU enabled results will not print the \"Process Works!\" string\r\n- CPU enabled results will print the \"Process Works!\" string\r\n\r\n\r\n### Issue Severity\r\n\r\nNone",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33367,
    "title": "[client] kubernetes w ray client",
    "author": "ganu1988",
    "state": "open",
    "created_at": "2023-03-16T09:55:06Z",
    "updated_at": "2025-06-17T00:16:24Z",
    "labels": [
      "enhancement",
      "P2",
      "core-client",
      "core",
      "pending-cleanup"
    ],
    "body": "### Description\n\nis there any we can use domain name instead of head_node_ip_address\r\nUnable to connect from local to AWS head node  due to connection issue.\r\nwe have alb with domain name, we tried using domain name , it is not working. is there a way we can expose via dns. ?\r\n\r\n  ray.init(address='ray://ray.dev.com:10001')\n\n### Use case\n\nwe have installed kuberay on kubernates cluster on AWS. since our connection is private we have to use alb with domain name to expose outside the world. have added alb with IP address and port and given the 10001' to forward the EKS ec2 with public hosted zone.  is there a way we can expose via dns. ?\r\n\r\n  ray.init(address='ray://ray.dev.com:10001')",
    "comments": [
      {
        "user": "richardliaw",
        "body": "@ckw017 could you comment here?\r\n\r\nI'm not sure if this is supported, but if it is not, we currently have no direct eng plans to add much support to the client interface. We are definitely open to contributions!"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33360,
    "title": "[Train] Reporting metrics/checkpoints from multiple workers",
    "author": "matthewdeng",
    "state": "open",
    "created_at": "2023-03-16T05:59:43Z",
    "updated_at": "2025-06-17T00:16:22Z",
    "labels": [
      "P2",
      "train",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### Summary \r\n\r\nThis issue tracks potential improvements for reporting/checkpointing across multiple Ray Train workers for deep learning workloads. \r\n\r\n### Context \r\nCurrently, Ray Train _requires_ each worker to report/checkpoint at the same frequency as a synchronization mechanism. This adheres to the SPMD pattern where each worker runs the same script. Documentation can be found [here](https://docs.ray.io/en/latest/train/dl_guide.html#reporting-intermediate-results-and-handling-checkpoints).\r\n\r\nHowever, this has turned out to be unintuitive, confusing, or even contradictory to user expectations (https://github.com/ray-project/ray/issues/33042).  As a result, we should explore options for improving this experience.\r\n\r\n### Proposal\r\n\r\nOne potential option is to _only_ allow reporting from the rank 0 worker. \r\n\r\n### Related Issues\r\nhttps://github.com/ray-project/ray/issues/31409\r\nhttps://github.com/ray-project/ray/issues/31434\r\n\r\n",
    "comments": [
      {
        "user": "Yard1",
        "body": "Also see https://github.com/ray-project/ray/issues/33360 and https://github.com/ray-project/ray/issues/33073"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33279,
    "title": "[Data] `read_parquet` schema is incorrect (schema is a dict instead of a string)",
    "author": "justinvyu",
    "state": "open",
    "created_at": "2023-03-14T00:30:12Z",
    "updated_at": "2025-06-17T00:16:19Z",
    "labels": [
      "bug",
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\n- I have a parquet file that I'm reading with Ray Data.\r\n- I want to group by a certain key `\"item_id\"`, which is a string.\r\n- Ray Data gives it to me with the wrong schema - the column is a dict instead:\r\n\r\n```\r\nitem_id: dictionary<values=string, indices=int32, ordered=0>\r\ntimestamp: string\r\ndemand: float\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 653\r\n```\r\n\r\n- Group by + aggregate fails due to trying to sort on a dict keyl\n\n### Versions / Dependencies\n\n2.3.0\n\n### Reproduction script\n\n```python\r\nimport ray\r\n\r\nds = ray.data.read_parquet(\r\n    \"s3://anonymous@m5-benchmarks/data/train/target.parquet\",\r\n    columns=[\"item_id\", \"timestamp\", \"demand\"]\r\n)\r\n[sample] = ds.take(1)\r\nprint(ds.take(1))  # `item_id` is clearly a string\r\nprint(\"\\nitem_id type:\", type(sample[\"item_id\"]))\r\n\r\nprint(\"\\n schema:\", ds.schema()) # Schema shows it being a dict with string as a value\r\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "justinvyu",
        "body": "Hmm, looks like the schema is actually correct - `take()` is outputting something different.\r\n\r\n```\r\n>>> ds = pq.read_table(\"s3://anonymous@m5-benchmarks/data/train/target.parquet\")\r\n>>> ds.column(\"item_id\")\r\n<pyarrow.lib.ChunkedArray object at 0x7f1efc5bd4f0>\r\n[\r\n\r\n  -- dictionary:\r\n    [\r\n      \"FOODS_1_001_CA_1\",\r\n      \"FOODS_1_001_CA_2\",\r\n      \"FOODS_1_001_CA_3\",\r\n      \"FOODS_1_001_CA_4\",\r\n      \"FOODS_1_001_TX_1\",\r\n      ...\r\n      \"HOUSEHOLD_2_516_TX_2\",\r\n      \"HOUSEHOLD_2_516_TX_3\",\r\n      \"HOUSEHOLD_2_516_WI_1\",\r\n      \"HOUSEHOLD_2_516_WI_2\",\r\n      \"HOUSEHOLD_2_516_WI_3\"\r\n    ]\r\n  -- indices:\r\n    [\r\n      0,\r\n      0,\r\n      0,\r\n      0,\r\n      0,\r\n      ...\r\n      71,\r\n      71,\r\n      71,\r\n      71,\r\n      71\r\n    ],\r\n```"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33272,
    "title": "[Ray status] confusing output about gpus and accelerators",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2023-03-13T22:37:37Z",
    "updated_at": "2025-06-17T00:16:17Z",
    "labels": [
      "bug",
      "P2",
      "infra",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\n1. it's not clear it means 5 gpus + 5 T4s or 5 gpus which are T4s.\r\n<img width=\"707\" alt=\"Screen Shot 2023-03-13 at 2 57 34 PM\" src=\"https://user-images.githubusercontent.com/9677264/224847068-7931dc7b-e452-4f6a-a90b-3e83906fd351.png\">\r\n\r\n2. The answer from 1 ought to be 5 gpus which are T4s. However, the output is not consistent. When gpu is used, the t4 is still not used.\r\n```\r\nUsage:\r\n40.0/40.0 CPU (21.0 used of 21.0 reserved in placement groups)\r\n5.0/5.0 GPU (5.0 used of 5.0 reserved in placement groups)\r\n0.0/5.0 accelerator_type:T4\r\n0B/101.35GiB memory\r\n215.40MiB/44.70GiB object_store_memory\r\n```\n\n### Versions / Dependencies\n\nray-ml:nightly\r\npy310\r\n\r\n\n\n### Reproduction script\n\nStart a cluster with those nodes and run something to use them.\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "rkooo567",
        "body": "@wuisawesome should we close it? "
      },
      {
        "user": "wuisawesome",
        "body": "I'm fine either way, the rest of this is kinda just the theme that \"ray doesn't distinguish between gpu types well\" "
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33233,
    "title": "[Tune] mlflow logger callback > log_trial_result fail (psycopg2.ProgrammingError) can't adapt type 'numpy.int64'",
    "author": "lifesboy",
    "state": "open",
    "created_at": "2023-03-12T01:55:12Z",
    "updated_at": "2025-06-17T00:16:15Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nUsing MLflowLoggerCallback with postgres DB and calling tune.run(callbacks=[MLflowLoggerCallback(...)]...)\r\nActual: exception in log_trial_result: `(psycopg2.ProgrammingError) can't adapt type 'numpy.int64'`\r\nExpect: logging successfully.\n\n### Versions / Dependencies\n\nhttps://github.com/ray-project/ray/blob/e4ce38d001dbbe09cd21c497fedd03d692b2be3e/python/ray/tune/integration/mlflow.py#L122\r\n\r\n\n\n### Reproduction script\n\n1. Setting mlflow using postgres DB.\r\n2. custom callback: anomalyloggercallback.py\r\n```\r\nclass AnomalyLoggerCallback(MLflowLoggerCallback):\r\n.....\r\n    def log_trial_result(self, iteration: int, trial: \"Trial\", result: Dict):\r\n        try:\r\n            super().log_trial_result(iteration=iteration, trial=trial, result=result)\r\n        except Exception as e:\r\n            log.error('log_trial_result interrupted: %s', e)\r\n```\r\n4. Call `tune.run(callbacks=[AnomalyLoggerCallback(...), ...]`\r\n\r\nActual: exception occur: \r\n```\r\n2023-03-12 01:40:08,835[ERROR][139974694004544-MainThread] anomalyloggercallback.py:log_trial_result(29) log_trial_result interrupted: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\r\n(psycopg2.ProgrammingError) can't adapt type 'numpy.int64'\r\n[SQL: INSERT INTO metrics (key, value, timestamp, step, is_nan, run_uuid) VALUES (%(key)s, %(value)s, %(timestamp)s, %(step)s, %(is_nan)s, %(run_uuid)s)]\r\n[parameters: {'key': 'episode_reward_max', 'value': 62477.455545547586, 'timestamp': 1678585208833, 'step': 4300000, 'is_nan': False, 'run_uuid': '855a5a73d2024cf1a57f707470f7e815'}]\r\n```\r\n\r\n<img width=\"1180\" alt=\"image\" src=\"https://user-images.githubusercontent.com/49264651/224519797-8cdc9e28-d1e8-428f-a1b4-7735ca3d7628.png\">\r\n\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33192,
    "title": "[Serve] Enhance replica upgrade process.",
    "author": "sihanwang41",
    "state": "open",
    "created_at": "2023-03-10T02:38:52Z",
    "updated_at": "2025-06-17T00:16:13Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### Description\n\nIn current ray serve, client will potentially observe latency regression with replica upgrade. \r\n\r\nThe main reasons for this behavior because:\r\n-  Stopping replicas and starting replicas happen in parallel. There is no guarantee that the number of running replicas are enough for handling the traffic during upgrade.\r\n- In the ray serve, it is intentionally not to exceed the target_state.num_replicas ([code](https://github.com/sihanwang41/ray/blob/master/python/ray/serve/_private/deployment_state.py#L1310))\r\n\r\nIn the ideal case, users might be okay to trade off num_replicas during upgrade to maintain the latency.\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33150,
    "title": "[air output] Isolate/refactor/improve rllib related progress reporting logic",
    "author": "xwjiang2010",
    "state": "open",
    "created_at": "2023-03-08T23:03:57Z",
    "updated_at": "2025-06-17T00:16:11Z",
    "labels": [
      "P2",
      "rllib",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nEPISODE_REWARD_MEAN\r\nTIMESTEPS_TOTAL\r\netc\r\n\r\nWe can extend a RLlib specific ProgressReporter callback to host those logic. And move this class to under RLlib.\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\nNA\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "xwjiang2010",
        "body": "A data point:\r\n\r\n```\r\nTrial name                    status      iter    total time (s)     ts    reward    episode_reward_max    episode_reward_min    episode_len_mean    episodes_this_iter\r\n----------------------------  --------  ------  ----------------  -----  --------  --------------------  --------------------  ------------------  --------------------\r\nAPPO_CartPole-v1_29116_00000  RUNNING        3           30.3035  46550  190.75                     440                    33            190.75                      71\r\nAPPO_CartPole-v1_29116_00001  RUNNING        1           10.1083  15850   25.9221                   145                     8             25.9221                   616\r\n```\r\n\r\ncollocate \"episode_reward_xxx\" in one column"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33142,
    "title": "[Tune][wandb] Report tune experiments as a wandb `sweep`",
    "author": "justinvyu",
    "state": "open",
    "created_at": "2023-03-08T20:07:53Z",
    "updated_at": "2025-06-17T00:16:08Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "UX",
      "ray-team-created",
      "wandb",
      "pending-cleanup"
    ],
    "body": "### Description\n\nSee https://docs.wandb.ai/ref/python/sweep\r\n\r\nThis would allow easier access to the Tune results after a run.\r\n\r\nCurrently, we just group runs together, and here's how you would get the best run:\r\n\r\n```python\r\napi = wandb.Api(api_key=\"<YOUR-API-KEY>\")\r\n\r\nruns = api.runs(path=f\"{entity}/{project}\", filters={\"group\": tune_experiment_name})\r\n\r\nfinal_losses = [\r\n    list(run.history()[\"binary_logloss\"])[-1] for run in runs\r\n]\r\nbest_run_idx = np.argmin(final_losses)\r\nbest_run = runs[best_run_idx]\r\n```\r\n\r\nUsing wandb sweep API:\r\n\r\n```python\r\nimport wandb\r\n\r\napi = wandb.Api()\r\nsweep = api.sweep(f\"justin/test/sweeps/my-tune-exp\")\r\n\r\n# Get best run parameters\r\nbest_run = sweep.best_run(order='validation/accuracy')\r\nbest_parameters = best_run.config\r\nprint(best_parameters)\r\n```\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "gjoliver",
        "body": "I feel like the world has largely moved to W&B type of workflow.\r\nhow do we make this actually happen @matthewdeng ?"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33130,
    "title": "[AIR][wandb] Add option to track artifact references in wandb if using cloud storage",
    "author": "justinvyu",
    "state": "open",
    "created_at": "2023-03-08T06:37:11Z",
    "updated_at": "2025-06-17T00:16:06Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "ray-team-created",
      "wandb",
      "pending-cleanup"
    ],
    "body": "### Description\n\nIf we're using Tune w/ S3 for storage + synchronization, then checkpoints shouldn't need to be uploaded to wandb as well. We should be able to track model versions in wandb using **artifact references** rather than needing to upload the checkpoint again as a full artifact.\r\n\r\nhttps://docs.wandb.ai/guides/artifacts/track-external-files\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33084,
    "title": "[AIR][Tune] Add an option in `WandbLoggerCallback` to group wandb runs by config",
    "author": "justinvyu",
    "state": "open",
    "created_at": "2023-03-07T00:32:15Z",
    "updated_at": "2025-06-17T00:16:04Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "ray-team-created",
      "wandb",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nFor cases with multiple samples per unique config, we should allow grouping runs together when reporting to wandb. **This is currently possible if you use `setup_wandb` and report directly in the training function**, but not for the `WandbLoggerCallback`.\r\n\r\nSee \r\nSee https://docs.wandb.ai/guides/runs/grouping\r\n\r\n### Use case\r\n\r\nSee https://discuss.ray.io/t/tune-wandb-group-different-samples-under-job-type/9645 for the feature request.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33059,
    "title": "[Serve] Support external storage for state",
    "author": "liuyang-my",
    "state": "open",
    "created_at": "2023-03-06T15:46:48Z",
    "updated_at": "2025-06-17T00:16:02Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### Description\n\nCan the KV Store in Ray Serve support external storage?\n\n### Use case\n\nAnt Ray Serving plans to complete the integration of Ray Serve in the first half of2023. We have analyzed in detail some of the things needed for this integration, as described in this doc document: [Plan of Ray Serve integrating](https://docs.google.com/document/d/1lvA9nLMs1S2wKZWAxJmWdJglKfe3L-d14JxPh0KQTds/edit#)\r\n\r\nCan the Ray Serve KV Store support external storage such as an HTTP URL? A Cluster may have hundreds of jobs within Ant. Some Deployments’ user_config can be very large. Using the internal store may be risky.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33057,
    "title": "[Serve] Use the namespace of context instead of \"serve\" when the Controller gets all running Actors",
    "author": "liuyang-my",
    "state": "open",
    "created_at": "2023-03-06T15:41:46Z",
    "updated_at": "2025-06-17T00:16:00Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### Description\n\nDuring the start-up phase of the Controller, all running Actors are obtained from the cluster and the namespace is hardcoded as “serve” in this line: https://github.com/ray-project/ray/blob/de2f8da435359bed6c704c1cac288ab06fcaaeca/python/ray/serve/controller.py#L134\r\n\r\nIs it more appropriate to use namespace in the context?\n\n### Use case\n\nAnt Ray Serving plans to complete the integration of Ray Serve in the first half of2023. We have analyzed in detail some of the things needed for this integration, as described in this doc document: [Plan of Ray Serve integrating](https://docs.google.com/document/d/1lvA9nLMs1S2wKZWAxJmWdJglKfe3L-d14JxPh0KQTds/edit#)\r\n\r\nIn Ant Ray Serving, the namespace is not fixed. When the controller is started, it may be a problem to obtain the running actor with namespace 'serve'.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33056,
    "title": "[Serve] Specify replicas when scaling down",
    "author": "liuyang-my",
    "state": "open",
    "created_at": "2023-03-06T15:28:31Z",
    "updated_at": "2025-06-17T00:15:58Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### Description\n\nWhen scaling down the replicas of a Deployment, specify which ones to scale down?\n\n### Use case\n\nAnt Ray Serving plans to complete the integration of Ray Serve in the first half of2023. We have analyzed in detail some of the things needed for this integration, as described in this doc document: [Plan of Ray Serve integrating](https://docs.google.com/document/d/1lvA9nLMs1S2wKZWAxJmWdJglKfe3L-d14JxPh0KQTds/edit#)\r\n\r\nIn the seamless migration scenario of Ant Ray Serving, if a POD fails it is necessary to migrate the Actor on this POD. This process includes two steps:\r\n\r\n- First scale up a new replica.\r\n- Then kill the Actor on the Pod to be migrated through name.\r\n\r\nTherefore it is necessary to support scaling down a batch of replicas by specifying their names.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33055,
    "title": "[Serve] Restart a batch of replicas by Actor names or replica tags",
    "author": "liuyang-my",
    "state": "open",
    "created_at": "2023-03-06T15:09:44Z",
    "updated_at": "2025-06-17T00:15:56Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### Description\n\nSpecify a batch of replicas by Actor names or replica tags to restart.\n\n### Use case\n\nAnt Ray Serving plans to complete the integration of Ray Serve in the first half of2023. We have analyzed in detail some of the things needed for this integration, as described in this doc document: [Plan of Ray Serve integrating](https://docs.google.com/document/d/1lvA9nLMs1S2wKZWAxJmWdJglKfe3L-d14JxPh0KQTds/edit#)\r\n\r\nIn the Ant production environment, some Deployment configurations need to be restarted in order to take effect. Therefore, an interface is needed to specify a batch of replicas to be restarted.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33054,
    "title": "[Serve] Specify a batch of replicas to update their user_config",
    "author": "liuyang-my",
    "state": "open",
    "created_at": "2023-03-06T13:18:59Z",
    "updated_at": "2025-06-17T00:15:54Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### Description\n\nSpecify a group of replicas for the same Deployment by Actor name or replica tag, and update the user_config of these replicas.\n\n### Use case\n\nAnt Ray Serving plans to complete the integration of Ray Serve in the first half of2023. We have analyzed in detail some of the things needed for this integration, as described in this doc document: [Plan of Ray Serve integrating](https://docs.google.com/document/d/1lvA9nLMs1S2wKZWAxJmWdJglKfe3L-d14JxPh0KQTds/edit#)\r\n\r\nInternally at Ant, when making changes to the production environment, such as updating the user_config of a Deployment, we do not update all replicas at once. Instead, we do it in batches. During the process, there is a gray phase where we pause and observe after completing a batch. Therefore, Ant Ray Serving needs to control which replicas to be updated.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33049,
    "title": "Ray Core Runtime Environments with tea.xyz",
    "author": "frosk1",
    "state": "open",
    "created_at": "2023-03-05T20:42:10Z",
    "updated_at": "2025-06-17T00:15:52Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "core-runtime-env",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nLeverage the tea package manager to realise runtime environments that need more than just python dependencies.\r\nWith the functionality of tea's virtual environments it could be quite straight forward to use it in the current Ray runtime environments. \r\n\r\n[tea](https://github.com/teaxyz) \r\n\r\n### Use case\r\n\r\nUser has more than python dependencies to be put in a runtime environment like c++, rust or software dependencies like curl, redis or other software.\r\n\r\nIntegrating the novel tea package manager approach for that could be a great synergy. ",
    "comments": [
      {
        "user": "architkulkarni",
        "body": "This sounds like it could be a good fit for a RuntimeEnv plugin.  I think the existing `conda` field should also work for the above use case of non-Python dependencies."
      },
      {
        "user": "frosk1",
        "body": "I could imagine there are several benefits of having tea.xyz compared to using conda."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33042,
    "title": "[Ray Tune] Support for continuing training when metrics are only reported from some of the workers",
    "author": "wzWang1991",
    "state": "open",
    "created_at": "2023-03-05T00:13:26Z",
    "updated_at": "2025-06-17T00:15:50Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "pending-cleanup"
    ],
    "body": "### Description\n\nCurrently, all workers needs to do session.report() to continue the next iteration. But there are use cases when we use several GPU to train, but not using the train function to directly report the loss. Instead, a separated function is called in one of the worker to calculate the metrics. It's just to save some resource since the calculation on all workers are just the same.\r\n\r\nBut with current requirements in Ray, we are not able to do so since all workers need to report the metric. We've tried to do some \"fake\" reporting but the requirement is to have all workers to report the same format of the metric. There is another possibility that we fake the format, but the actual reporting metric will be used from these fake data, making it hard to use in the end.\r\n\r\nSo it would be great to have the ability to let worker do session.report() but having a way to indicate that this worker has done its job, instead of enforcing the same format of the metric reporting.\n\n### Use case\n\nAllowing one worker to report the metric while multiple workers can still working on training. ",
    "comments": [
      {
        "user": "gjoliver",
        "body": "hi @wzWang1991, thanks for your message.\r\na quick question about your workload, understand that you don't want all the workers to repeat the same metrics calculation, but is the worker that is responsible for reporting per-iteration metrics always the same?\r\nlike if we allow metrics to only be reported from rank 0 worker, would that work for your use case?\r\n\r\nalso I believe that today, you can actually just submit empty metrics from any workers you don't care about.\r\nmaybe you can also give that a try."
      },
      {
        "user": "wzWang1991",
        "body": "Yes, it's always the rank 0 worker. That would be great.\r\n\r\nI've tried the empty metrics and it works. But they are also required to put a checkpoint otherwise ray will complain, because the rank 0 one always submits the metrics along with checkpoint. And I checked the code in ray, it will require other workers to do the same.\r\n\r\nThank you!"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 33004,
    "title": "[Data] Cannot get the length of a tf dataset created from `ray_ds.to_tf`",
    "author": "justinvyu",
    "state": "open",
    "created_at": "2023-03-03T18:29:10Z",
    "updated_at": "2025-06-17T00:15:47Z",
    "labels": [
      "bug",
      "P2",
      "data",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nConverting a Ray dataset to a `tf.data.Dataset` through `ray_ds.to_tf(...)`, then passing it into a Keras `model.fit()` runs into an error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ray/workspace-project-justinvyu-dev/imageclf_raydata.py\", line 172, in <module>\r\n    result = trainer.fit()\r\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 579, in fit\r\n    raise result.error\r\nray.exceptions.RayTaskError(TypeError): ray::_Inner.train() (pid=14334, ip=10.0.57.102, repr=TensorflowTrainer)\r\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 384, in train\r\n    raise skipped from exception_cause(skipped)\r\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\r\n    ray.get(object_ref)\r\nray.exceptions.RayTaskError(TypeError): ray::RayTrainWorker._RayTrainWorker__execute() (pid=14452, ip=10.0.57.102, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7eff24d08a90>)\r\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/worker_group.py\", line 31, in __execute\r\n    raise skipped from exception_cause(skipped)\r\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\r\n    train_func(*args, **kwargs)\r\n  File \"/home/ray/workspace-project-justinvyu-dev/imageclf_raydata.py\", line 121, in train_func\r\n    f\"# training batches per worker = {len(train_ds)} \"\r\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 531, in __len__\r\n    raise TypeError(\"The dataset length is unknown.\")\r\nTypeError: The dataset length is unknown.\r\n```\r\n\r\n## Workaround\r\n\r\n```python\r\ntrain_ray_ds = session.get_dataset_shard(\"train\")\r\ntrain_ds = train_ray_ds.to_tf(\r\n    feature_columns=[\"image\"],\r\n    label_columns=[\"label\"],\r\n    batch_size=batch_size_per_worker,\r\n    local_shuffle_buffer_size=256,\r\n)\r\n# Workaround: set the TF dataset cardinality manually\r\ntrain_ds = train_ds.apply(tf.data.experimental.assert_cardinality(train_ray_ds.count()))\r\n```\r\n\n\n### Versions / Dependencies\n\n2.3.0\n\n### Reproduction script\n\n```python\r\nimport ray\r\nimport tensorflow as tf\r\nfrom typing import Dict\r\n\r\nIMG_SIZE = 224\r\nNUM_CLASSES = 10\r\n\r\nimport numpy as np\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.applications import EfficientNetB0\r\n\r\nfrom ray.data.datasource.partitioning import Partitioning\r\n\r\nray.data.context.DatasetContext.get_current().use_streaming_executor = True\r\n\r\ndef get_dataset_for_split(split: str):\r\n    data_folder = f\"s3://anonymous@air-example-data/food-101-tiny/{split}\"\r\n    partitioning = Partitioning(\r\n        \"dir\", field_names=[\"class\"], base_dir=data_folder\r\n    )\r\n\r\n    def resize(batch: Dict[str, np.ndarray]):\r\n        batch[\"image\"] = tf.convert_to_tensor(batch[\"image\"], dtype=tf.uint8)\r\n        batch[\"image\"] = tf.image.resize(batch[\"image\"], (IMG_SIZE, IMG_SIZE)).numpy()\r\n        return batch\r\n\r\n    return ray.data.read_images(\r\n        data_folder, size=(512, 512), partitioning=partitioning, mode=\"RGB\"\r\n    ).map_batches(resize, batch_format=\"numpy\").random_shuffle()\r\n\r\n\r\ntrain_ds, valid_ds = [get_dataset_for_split(split) for split in (\"train\", \"valid\")]\r\n\r\nlabels = valid_ds.groupby(\"class\").count().to_pandas()\r\nclass_to_idx = {\r\n    class_name: i\r\n    for i, class_name in enumerate(labels[\"class\"])\r\n}\r\n\r\nTRAIN_DS_LENGTH = int(train_ds.count())\r\nVALID_DS_LENGTH = int(valid_ds.count())\r\nNUM_WORKERS = 1\r\n\r\nfrom ray.data.preprocessors import BatchMapper\r\n\r\ndef build_preprocessor():\r\n    # 1. Map the image folder names to label ids\r\n    def map_labels(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\r\n        batch[\"label\"] = np.vectorize(class_to_idx.get)(batch[\"class\"])\r\n        return {\"image\": batch[\"image\"], \"label\": batch[\"label\"]}\r\n\r\n    label_preprocessor = BatchMapper(map_labels, batch_format=\"numpy\")\r\n    return label_preprocessor\r\n\r\n\r\ndef build_model():\r\n    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\r\n    # x = img_augmentation(inputs)\r\n    x = inputs\r\n    model = EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")\r\n\r\n    # Freeze the pretrained weights\r\n    model.trainable = True\r\n\r\n    # Rebuild top\r\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\r\n    x = layers.BatchNormalization()(x)\r\n\r\n    top_dropout_rate = 0.2\r\n    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\r\n    outputs = layers.Dense(NUM_CLASSES, activation=\"linear\", name=\"pred\")(x)\r\n\r\n    # Compile\r\n    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\r\n    return model\r\n\r\n\r\nfrom ray.air import session\r\n\r\ndef train_func(config: dict):\r\n    strategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n    with strategy.scope():  \r\n        model = build_model()\r\n        optimizer = tf.keras.optimizers.Adam()\r\n        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n        model.compile(\r\n            optimizer=optimizer,\r\n            loss=loss_object,\r\n            metrics=[\"accuracy\"],\r\n        )\r\n\r\n    train_ray_ds = session.get_dataset_shard(\"train\")\r\n    train_ds = train_ray_ds.to_tf(\r\n        feature_columns=[\"image\"],\r\n        label_columns=[\"label\"],\r\n        batch_size=64,\r\n        local_shuffle_buffer_size=256,\r\n    )\r\n    # Uncomment this for it to work:\r\n    # train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(train_ray_ds.count()))\r\n    train_ds = train_ds.map(lambda image, label: (tf.image.resize(image[\"image\"], (IMG_SIZE, IMG_SIZE)), label[\"label\"]))\r\n\r\n    model.fit(train_ds, epochs=10)\r\n\r\n\r\nfrom ray import air\r\nfrom ray.train.tensorflow import TensorflowTrainer\r\n\r\nuse_gpu = True\r\n\r\ntrainer = TensorflowTrainer(\r\n    train_loop_per_worker=train_func,\r\n    datasets={\"train\": train_ds},\r\n    preprocessor=build_preprocessor(),\r\n    scaling_config=air.ScalingConfig(\r\n        num_workers=1,\r\n        use_gpu=False,\r\n        trainer_resources={\"CPU\": 0},\r\n        resources_per_worker={\"CPU\": 1.0, \"GPU\": 0},\r\n    ),\r\n)\r\nresult = trainer.fit()\r\n```\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "justinvyu",
        "body": "Actually, this was a result of me calling len(ray_ds.to_tf(...)), not model.fit. Maybe we could set the cardinality of the tf dataset to the ray_ds.count() in to_tf?"
      },
      {
        "user": "amogkam",
        "body": "@justinvyu this is expected since the tensorflow dataset is created from a generator. Calculating the length would require iterating through the entire dataset. See https://discuss.tensorflow.org/t/typeerror-dataset-length-is-unknown-tensorflow/948/2 for more info.\r\n\r\nIt is recommended to use `dataset.count()` beforehand which can return the count from metadata, without executing it."
      },
      {
        "user": "amogkam",
        "body": "What is the use case?"
      }
    ]
  },
  {
    "issue_number": 32989,
    "title": "[Data] Include image class id in the returned datasets of `ray.data.read_images()`.",
    "author": "woshiyyya",
    "state": "open",
    "created_at": "2023-03-03T00:54:48Z",
    "updated_at": "2025-06-17T00:15:45Z",
    "labels": [
      "enhancement",
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nIt is a common use case to read data from a image folders, where the folder names are the class names.\r\n\r\n```python\r\npartitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=data_folder)\r\nds = ray.data.read_images(data_folder, size=(256, 256), partitioning=partitioning, mode=\"RGB\")\r\n```\r\nHowever, the `ray.data.read_images` api returns a datasets with two columns [\"image\", \"class\"]. Actually we need the integer class id more than the class string name. Otherwise we have to write an additional `ds.map_batches()` to map class names to integers.\r\n\r\nWe can do something similar with torchvision `ImageFolder`. For example, when we iterate through a`ImageFolder` dataset, it will generate two tensors, one with image and another with label_id.\r\n\r\n```python\r\ndataset = datasets.ImageFolder(data_folder, data_transforms)\r\ndataloader = DataLoader(dataset, batch_size=3)\r\nprint(next(iter(dataloader)))\r\n>>>       \r\n[tensor([[[[ 0.0912,  0.0398, -0.0287,  ...,  1.0673,  1.0844,  1.0502],\r\n          [ 0.3823,  0.3481,  0.3138,  ...,  1.0331,  1.1187,  1.1015],\r\n          [ 0.6392,  0.6221,  0.5707,  ...,  1.0159,  1.0844,  1.1187],\r\n    ...,\r\n          [-0.3055, -0.3927, -0.4275,  ..., -0.7238, -0.7064, -0.8633],\r\n          [-0.3230, -0.3927, -0.4450,  ..., -0.7761, -0.8981, -0.9330],\r\n          [-0.3753, -0.3753, -0.4275,  ..., -0.8284, -1.0027, -0.9504]]]]), tensor([0, 0])]\r\n```\r\n\r\n\r\n\r\n\r\n### Use case\r\n\r\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32953,
    "title": "[Datasets] Raise descriptive error if `iter_torch_batches` can't convert data",
    "author": "bveeramani",
    "state": "open",
    "created_at": "2023-03-01T22:02:24Z",
    "updated_at": "2025-06-17T00:15:43Z",
    "labels": [
      "enhancement",
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nIf your dataset contains data that can't be converted to Torch tensors:\r\n\r\n```\r\nimport ray\r\n\r\n\r\nds = ray.data.from_items([{\"bytes\": b\"spam\"}, {\"bytes\": b\"ham\"}])\r\nnext(iter(ds.iter_torch_batches()))\r\n```\r\n\r\nThen Torch raises an error:\r\n\r\n```\r\nTypeError: can't convert np.ndarray of type numpy.bytes_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.\r\n```\r\n\r\nIt'd be nice if we provided a more descriptive error that describes the column name and provides a workaround:\r\n\r\n```\r\nValueError: Column 'bytes' of type `numpy.bytes_.` can't be converted to Torch tensors. To fix the issue, drop the column or specify a `collate_fn`.\r\n```\r\n\r\n### Use case\r\n\r\nQOL",
    "comments": [
      {
        "user": "orvindemsy",
        "body": "following, I also in favor to make the error more descriptive.\r\n\r\nCase:\r\ncurrently using the FashionMNIST torch dataset, convert to ray.dataset, call it inside trainer function with `session.get_dataset_shard('train')` then `.iter_torch_batch` it, but failed, but I don't know which part of dataset caused it."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32920,
    "title": "[Serve] Don't start Serve agent if Serve isn't installed",
    "author": "zcin",
    "state": "open",
    "created_at": "2023-02-28T23:15:05Z",
    "updated_at": "2025-06-17T00:15:41Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### Description\n\nCurrently, even if Serve isn't installed, the Serve dashboard agent is started. We delay importing Serve modules to avoid adding Serve as a dependency for the dashboard, but if Serve modules are not installed we shouldn't start the Serve agent at all.\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32913,
    "title": "[Data]: `ds.take()` and `ds.iter_batches()` have unexpected different behavior for pd.Series columns",
    "author": "Rohan138",
    "state": "open",
    "created_at": "2023-02-28T21:39:51Z",
    "updated_at": "2025-06-17T00:15:39Z",
    "labels": [
      "bug",
      "P2",
      "data",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\n`ds.take()` returns underlying pyarrow data as-is, e.g. if the column is a list of lists, you get back the same format.\r\n\r\n`ds.iter_batches` and `ds.map_batches`, instead, return batches where`pd.Series` columns with `dtype=object` are inferred as `np.ndarray`, which results in unexpecged behavior when calling `np.stack` on the values.\r\n\r\ncc: @amogkam\r\n\r\n### Versions / Dependencies\r\n\r\nRay 3.0.0dev (nightly)\r\nPython 3.10.8\r\n\r\n### Reproduction script\r\n\r\n```\r\nimport ray.data\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\npytable = pa.Table.from_pydict({\"foo\": [[[1, 2]], [[3, 4]]]})\r\nds = ray.data.from_arrow(pytable)\r\n\r\ndf = pd.DataFrame(ds.take_all())\r\nprint(df[\"foo\"].values)\r\n# [list([[1, 2]]) list([[3, 4]])]\r\nprint(np.vstack(df[\"foo\"].values))\r\n# [[1 2]\r\n#  [3 4]]\r\n\r\nfor batch in ds.iter_batches(batch_format=\"pandas\"):\r\n    print(batch[\"foo\"].values)\r\n    # [array([array([1, 2])], dtype=object) array([array([3, 4])], dtype=object)]\r\n    print(np.vstack(batch[\"foo\"].values))\r\n    # [[array([1, 2])]\r\n    #  [array([3, 4])]]\r\n```\r\n\r\n### Issue Severity\r\n\r\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "Rohan138",
        "body": "Related issue: https://github.com/numpy/numpy/issues/8052"
      },
      {
        "user": "Rohan138",
        "body": "Tracked it down to https://github.com/ray-project/ray/blob/master/python/ray/data/_internal/arrow_block.py#L231;\r\nthe list is saved in the pyarrow table as `pyarrow.lib.ListScalar`, which is automatically converted to `np.ndarray` when `to_pandas` is called."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32837,
    "title": "[Ray: Serve] Model Composition primitives should be part of Serve Core API docs.",
    "author": "dmatrix",
    "state": "open",
    "created_at": "2023-02-25T00:16:57Z",
    "updated_at": "2025-06-17T00:15:36Z",
    "labels": [
      "P2",
      "serve",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nModel composition APIs are incomplete as part of [ray.serve.* API docs](https://docs.ray.io/en/latest/serve/api/python_api.html#ray-serve-python-api). While we have examples for all those [deployment patterns](https://docs.ray.io/en/latest/serve/tutorials/deployment-graph-patterns.html), there are no API docs or code snippets for any of the primitives: InputNode, Model, DAGDriver, etc. \r\n\r\nWe have spoken at length about model composition at meetups and conferences, yet no complete & comprehensive API docs. Let's make all this part of our efforts to improve Ray Serve developer UX experience.\r\n\r\n### Link\r\n\r\nThis [API page is incomplete](https://docs.ray.io/en/latest/serve/api/python_api.html)\r\nThis Serve API page should be comprehensive and should include all the APIs we use in your [deployment graph examples](https://docs.ray.io/en/latest/serve/tutorials/deployment-graph-patterns.html)",
    "comments": [
      {
        "user": "dmatrix",
        "body": "I suggest this should be taken as a Hackathon project by the serve team .... :-)"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32826,
    "title": "[core][state] Task backend : already submitted cancelled task showing up as finished",
    "author": "rickyyx",
    "state": "open",
    "created_at": "2023-02-24T19:55:29Z",
    "updated_at": "2025-06-17T00:15:34Z",
    "labels": [
      "bug",
      "P2",
      "dashboard",
      "core",
      "observability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nseems we are already declaring a cancelled task that's already running to be finished even though it should be marked as cancelled (or failed)\r\n\r\n\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\n```\r\nimport ray\r\nimport time\r\n\r\n\r\n@ray.remote\r\ndef fail():\r\n    raise ValueError(\"failed\")\r\n\r\n\r\n@ray.remote\r\ndef sleep(x):\r\n    start = time.perf_counter()\r\n    while True:\r\n        if time.perf_counter() - start < x:\r\n            time.sleep(0.1)\r\n        else:\r\n            break\r\n\r\n\r\ntry:\r\n\r\n    t = sleep.remote(999)\r\n   # Make sure t is submitted and running\r\n    ray.get(sleep.remote(3))\r\n  \r\n    ray.cancel(t)\r\n    ray.get(t)\r\n```\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32823,
    "title": "[AIR][Tune] Make trial checkpoint + artifact upload happen atomically",
    "author": "justinvyu",
    "state": "open",
    "created_at": "2023-02-24T19:09:09Z",
    "updated_at": "2025-06-17T00:15:32Z",
    "labels": [
      "tune",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "https://github.com/ray-project/ray/pull/32334 added artifact syncing to cloud.\r\n\r\nThis happens on every checkpoint so that artifact state is consistent with checkpoint state. However, it's possible for the experiment to crash in between checkpoint upload and artifact upload, which would lead to inconsistency.\r\n\r\nIn general, uploading to cloud should happen atomically. Right now, if `upload_to_uri(exclude)` is provided, we'll write individual files one at a time. If the upload operation fails somewhere, then it'll retain only a partial checkpoint which may fail on restore/usage later.\r\n\r\n**Question**: Should these two happen atomically?\r\n\r\nPros:\r\n- Consistent state\r\n\r\nCons:\r\n- We care more about checkpoints for downstream tasks + training resume, so we may want to upload the checkpoint fully first, and perfect artifact consistency may not matter that much.\r\n\r\nRemove this TODO after this is resolved:\r\nhttps://github.com/ray-project/ray/blob/2fb1bcca792afd81795484aead4c925e261233c2/release/tune_tests/cloud_tests/workloads/run_cloud_test.py#L849",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32810,
    "title": "[Tune] During multi-GPU training (using mp.spawn), ray.tune.report does not take effect.",
    "author": "1757525671",
    "state": "open",
    "created_at": "2023-02-24T07:34:39Z",
    "updated_at": "2025-06-17T00:15:30Z",
    "labels": [
      "bug",
      "needs-repro-script",
      "tune",
      "P2",
      "@external-author-action-required",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nraise ValueError(\r\nValueError: Trial returned a result which did not include the specified metric(s) `false_wakeup_num` that `AsyncHyperBandScheduler` expects. Make sure your calls to `tune.report()` include the metric, or set the TUNE_DISABLE_STRICT_METRIC_CHECKING environment variable to 1. Result: {'trial_id': '3e919_00000', 'experiment_id': '23541f8d4f3645c1ad479e26be4ea4e9', 'date': '2023-02-24_09-40-44', 'timestamp': 1677202844, 'pid': 76041, 'done': True, 'config/hidden_state': 196, 'config/feature_length': 37}\r\n\r\n\r\nIn mp.spawn serialization mode, ray.tune.report is invalid, the result cannot be updated, and the AsyncHyperBandScheduler early stop cannot be used.\r\n\r\nDoes ray.tune support only the single-GPU or multi-GPU DP mode but not the DDP mode during multi-GPU training?\n\n### Versions / Dependencies\n\nray=2.2.0\r\npython=3.8.5\r\n\n\n### Reproduction script\n\nThe code is sensitive and cannot be shared.\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "justinvyu",
        "body": "Hi @1757525671, could you provide a minimal reproduction script? How are you using torch multiprocessing with Ray? Have you considered using a Ray `TorchTrainer` to do DDP on multiple GPUs instead? See the [API reference here](https://docs.ray.io/en/latest/train/api/api.html#pytorch) and a [starter guide here](https://docs.ray.io/en/latest/ray-air/examples/convert_existing_pytorch_code_to_ray_air.html)."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32760,
    "title": "[Tune] failure when using more than one GPU",
    "author": "fehtemam",
    "state": "open",
    "created_at": "2023-02-22T23:57:36Z",
    "updated_at": "2025-06-17T00:15:28Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\n1. The bug when switching from one GPU core to anything with >1 cores:\r\n\r\n```python\r\nValueError                                Traceback (most recent call last)\r\nFile ~/Code/Gits/demand-forecasting/venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:948, in TrialRunner._wait_and_handle_event(self, next_trial)\r\n    947 if event.type == _ExecutorEventType.TRAINING_RESULT:\r\n--> 948     self._on_training_result(\r\n    949         trial, result[_ExecutorEvent.KEY_FUTURE_RESULT]\r\n    950     )\r\n    951 else:\r\n\r\nFile ~/Code/Gits/demand-forecasting/venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:1073, in TrialRunner._on_training_result(self, trial, result)\r\n   1072 with warn_if_slow(\"process_trial_result\"):\r\n-> 1073     self._process_trial_results(trial, result)\r\n\r\nFile ~/Code/Gits/demand-forecasting/venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:1156, in TrialRunner._process_trial_results(self, trial, results)\r\n   1155 with warn_if_slow(\"process_trial_result\"):\r\n-> 1156     decision = self._process_trial_result(trial, result)\r\n   1157 if decision is None:\r\n   1158     # If we didn't get a decision, this means a\r\n   1159     # non-training future (e.g. a save) was scheduled.\r\n   1160     # We do not allow processing more results then.\r\n\r\nFile ~/Code/Gits/demand-forecasting/venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:1193, in TrialRunner._process_trial_result(self, trial, result)\r\n   1192 flat_result = flatten_dict(result)\r\n-> 1193 self._validate_result_metrics(flat_result)\r\n   1195 if self._stopper(trial.trial_id, result) or trial.should_stop(flat_result):\r\n...\r\n    280     experiment_checkpoint_dir = ray.get(\r\n    281         self._remote_tuner.get_experiment_checkpoint_dir.remote()\r\n    282     )\r\n\r\nTuneError: The Ray Tune run failed. Please inspect the previous error messages for a cause. After fixing the issue, you can restart the run from scratch or continue this run. To continue this run, you can use `tuner = Tuner.restore(\"/home/ubuntu/ray_results/train_tune_2023-02-22_23-38-08\")`.\r\n```\r\nThis is happening when using Nixtla library neuralforecast which calls Tune for its hyperparameter tuning. \r\nThis is done on an EC2 instance g3.8xlarge on Ubuntu 22.04 but it is reproducible using other EC2 instances with GPU cores >1.\r\n\r\n2. Expected behavior is no error as it is the case with only one GPU core.\n\n### Versions / Dependencies\n\nOS: Ubuntu 22.04\r\nPython: 3.10.10\r\nneuralforecast==1.4.0\r\nnumpy==1.23.5\r\npandas==1.5.2\r\nray==2.2.0\n\n### Reproduction script\n\nHere is a reproducible example:\r\n\r\n```python\r\nfrom neuralforecast import NeuralForecast\r\nfrom neuralforecast.auto import AutoNHITS\r\nimport numpy as np\r\nimport pandas as pd\r\nimport ray\r\n\r\ndf_t = pd.DataFrame(columns=['unique_id', 'ds', 'y'])\r\ndf_t['ds'] = pd.date_range('2018-01-01', '2022-01-02', freq='D')\r\ndf_t['unique_id'] = 'series_1'\r\nrng = np.random.default_rng(seed=6)\r\ndf_t['y'] = rng.uniform(low=0, high=1, size=1463)\r\n\r\nhrz = 365\r\nmodels = [\r\n    AutoNHITS(h=hrz, gpus=2)\r\n]\r\n\r\nnforecast = NeuralForecast(models=models, freq='D')\r\nnforecast.fit(df=df_t)\r\n```\r\n\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "justinvyu",
        "body": "Hi @fehtemam,\r\n\r\nSorry for the delay in getting back to you! `neuralforecast` seems to use Tune and its lightning integration. For multi-GPU training, lighting creates a process for each GPU with `torch.multiprocessing`, which may not be playing well with Ray. Take a look at this section for more info: https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html#training-with-gpus"
      },
      {
        "user": "fehtemam",
        "body": "@justinvyu Thanks for looking into this. My background is in TF so I am not very familiar with torch but I want to see if I can help with this issue. So does this mean `neuralforecast` needs to change the way they are using Ray Tune or does it mean they should instead use Ray Lightning? When you say \r\n>lighting creates a process for each GPU with torch.multiprocessing, which may not be playing well with Ray\r\n\r\nDo you mean torch lightning creates a process or do you mean Ray lightning creates a process?\r\n"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32704,
    "title": "[Runtime Env] Add docstring for public class methods and attributes",
    "author": "jjyao",
    "state": "open",
    "created_at": "2023-02-21T19:04:45Z",
    "updated_at": "2025-06-17T00:15:26Z",
    "labels": [
      "P2",
      "docs",
      "core-runtime-env",
      "pending-cleanup"
    ],
    "body": "### Description\n\nFor runtime env apis https://docs.ray.io/en/master/ray-core/api/doc/ray.runtime_env.RuntimeEnv.html, some public methods are missing docstrings so they don’t show up in the generated page\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32698,
    "title": "[tune] Add suggestions on when `reuse_actor` should be set to false.",
    "author": "xwjiang2010",
    "state": "open",
    "created_at": "2023-02-21T16:45:50Z",
    "updated_at": "2025-06-17T00:15:24Z",
    "labels": [
      "tune",
      "P2",
      "docs",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### Description\n\nI have seen various issues related to `reuse_actor` set to `True` by default in Tune.\r\nWhile it's debatable whether that's the workload we really want to target. Let's first make sure that it's well surfaced in the documentation so that people are aware of the implications of it.\r\n\r\nExamples:\r\n- https://anyscaleteam.slack.com/archives/C04KQU8S38W/p1676415345475009\r\n- https://discuss.ray.io/t/gpu-memory-not-being-freed-every-other-trial-in-ray-tune/9413\r\n\r\n@Yard1 I think there is another one that either you or Kai have looked at? Could you also help me link it here?\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32633,
    "title": "[serve] serve run doesn't restart app successfully in some environments",
    "author": "ericl",
    "state": "open",
    "created_at": "2023-02-16T21:23:11Z",
    "updated_at": "2025-06-17T00:15:22Z",
    "labels": [
      "bug",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nSteps to reproduce:\r\n1. serve run dummy:transformer\r\n2. curl localhost:8000 ----> succeeds\r\n3. Ctrl-C (1) and run it again\r\n4. curl localhost:8000 ----> Connection refused\r\n\r\nThere is no way to recover now except to run \"serve shutdown\".\r\n\r\nYou can reproduce this on this workspace: https://console.anyscale.com/o/anyscale-internal/workspaces/expwrk_648fzyx25qt5ih54ylfjfybl2d/ses_hg15be2l74pppi8f42nzflpwi9?command-history-section=command_history\r\n\r\nIt is not reproducible on my laptop, only that workspace.\n\n### Versions / Dependencies\n\nRay 2.2\n\n### Reproduction script\n\n```\r\nfrom starlette.requests import Request\r\n\r\nimport ray\r\nfrom ray import serve\r\n\r\nimport argparse\r\nfrom os import path\r\n\r\n@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 2})\r\nclass Transformer:\r\n    def __init__(self, model_dir):\r\n        print(\"Loading\", model_dir)\r\n        self.model_dir = model_dir\r\n\r\n    def complete(self, text):\r\n        return self.model_dir + \" \" + text\r\n\r\n    async def __call__(self, http_request: Request) -> str:\r\n        prompt: str = {\"text\": \"foo\"}\r\n        return self.complete(prompt[\"text\"])\r\n\r\ntransformer = Transformer.bind(\"dummy\")\r\n```\r\n\r\nYou can use this or any other dummy script.\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "edoakes",
        "body": "@sihanwang41 assigning to you for triage"
      },
      {
        "user": "sihanwang41",
        "body": "I am not able to reproduce the issue with the nightly cluster env and 2.3.0rc in the workspace right now (keep typing ctrl+c :) ). It is probably something messed up in that specific workspace.\r\n\r\npropose to downgrade to p2. If we can reproduce the issue in the workspace in a reliable way, we can bump it back."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32509,
    "title": "[train] Big performance hit when TensorFlow trainer is not scheduled on head node",
    "author": "krfricke",
    "state": "open",
    "created_at": "2023-02-14T02:21:14Z",
    "updated_at": "2025-06-17T00:15:19Z",
    "labels": [
      "bug",
      "P2",
      "train",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nI'm running the Quick start example script on a cluster with 3 nodes a 8 CPUs.\r\n\r\nThe training time can be vastly different:\r\n\r\n```\r\n2023-02-13 18:17:26,474 INFO tune.py:762 -- Total run time: 34.41 seconds (33.59 seconds for the tuning loop).\r\n```\r\n\r\nvs.\r\n\r\n```\r\n2023-02-13 18:18:58,509 INFO tune.py:763 -- Total run time: 120.22 seconds (119.19 seconds for the tuning loop).\r\n```\r\n\r\nIt seems like this depends mostly on where the trainable actor is scheduled. If it's scheduled on the head node, it runs fast. If it is scheduled on a worker node, it's slow.\r\n\r\nWhen it's slow, I see a lot of these:\r\n\r\n```\r\n2023-02-13 18:18:44,975 WARNING util.py:244 -- The `process_trial_save` operation took 1.714 s, which may be a performance bottleneck.`\r\n```\r\n\r\npresumably because checkpoints are synced to the head node.\r\n\r\nIdeally we would co-locate the trainable with the driver, especially when not using cloud storage.\n\n### Versions / Dependencies\n\n2.2.0\n\n### Reproduction script\n\nFull script:\r\n\r\n```\r\nimport ray\r\n\r\n# Load data.\r\ndataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\r\n\r\n# Split data into train and validation.\r\ntrain_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\r\n\r\n# Create a test dataset by dropping the target column.\r\ntest_dataset = valid_dataset.drop_columns(cols=[\"target\"])\r\n\r\nimport numpy as np\r\n\r\nfrom ray.data.preprocessors import Concatenator, Chain, StandardScaler\r\n\r\n# Create a preprocessor to scale some columns and concatenate the result.\r\npreprocessor = Chain(\r\n    StandardScaler(columns=[\"mean radius\", \"mean texture\"]),\r\n    Concatenator(exclude=[\"target\"], dtype=np.float32),\r\n)\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nfrom ray.air import session\r\nfrom ray.air.config import ScalingConfig\r\nfrom ray.air.integrations.keras import Callback as KerasCallback\r\nfrom ray.train.tensorflow import TensorflowTrainer\r\n\r\n\r\ndef create_keras_model(input_features):\r\n    return keras.Sequential(\r\n        [\r\n            keras.Input(shape=(input_features,)),\r\n            layers.Dense(16, activation=\"relu\"),\r\n            layers.Dense(16, activation=\"relu\"),\r\n            layers.Dense(1),\r\n        ]\r\n    )\r\n\r\n\r\ndef train_loop_per_worker(config):\r\n    batch_size = config[\"batch_size\"]\r\n    lr = config[\"lr\"]\r\n    epochs = config[\"num_epochs\"]\r\n    num_features = config[\"num_features\"]\r\n\r\n    # Get the Ray Dataset shard for this data parallel worker,\r\n    # and convert it to a Tensorflow Dataset.\r\n    train_data = session.get_dataset_shard(\"train\")\r\n\r\n    strategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n    with strategy.scope():\r\n        # Model building/compiling need to be within `strategy.scope()`.\r\n        multi_worker_model = create_keras_model(num_features)\r\n        multi_worker_model.compile(\r\n            optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\r\n            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n            metrics=[\r\n                tf.keras.metrics.BinaryCrossentropy(\r\n                    name=\"loss\",\r\n                )\r\n            ],\r\n        )\r\n\r\n    for _ in range(epochs):\r\n        tf_dataset = train_data.to_tf(\r\n            feature_columns=\"concat_out\", label_columns=\"target\", batch_size=batch_size\r\n        )\r\n        multi_worker_model.fit(\r\n            tf_dataset,\r\n            callbacks=[KerasCallback()],\r\n            verbose=0,\r\n        )\r\n\r\n\r\nnum_features = len(train_dataset.schema().names) - 1\r\n\r\ntrainer = TensorflowTrainer(\r\n    train_loop_per_worker=train_loop_per_worker,\r\n    train_loop_config={\r\n        \"batch_size\": 128,\r\n        \"num_epochs\": 50,\r\n        \"num_features\": num_features,\r\n        \"lr\": 0.0001,\r\n    },\r\n    scaling_config=ScalingConfig(\r\n        num_workers=2,  # Number of data parallel training workers\r\n        use_gpu=False,\r\n        trainer_resources={\"CPU\": 6},  # so that the example works on Colab.\r\n        resources_per_worker={\"CPU\": 8},\r\n    ),\r\n    datasets={\"train\": train_dataset},\r\n    preprocessor=preprocessor,\r\n)\r\n\r\nresult = trainer.fit()\r\nprint(f\"Last result: {result.metrics}\")\r\n# Last result: {'loss': 8.997025489807129, ...}\r\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32497,
    "title": "[doc][tune] clarify `Stopper`, what is `training_iteration`",
    "author": "xwjiang2010",
    "state": "open",
    "created_at": "2023-02-13T22:00:14Z",
    "updated_at": "2025-06-17T00:15:18Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "docs",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nhttps://docs.ray.io/en/latest/tune/tutorials/tune-stopping.html#stopping-with-a-dictionary\r\n\r\nSee https://discuss.ray.io/t/concept-of-training-iteration/9357\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\nNA\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32491,
    "title": "[release] update our xgboost release test to catch issues like (see discription)",
    "author": "xwjiang2010",
    "state": "open",
    "created_at": "2023-02-13T20:46:35Z",
    "updated_at": "2025-06-17T00:15:16Z",
    "labels": [
      "bug",
      "P2",
      "train",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nIdeally our release test should capture this, instead of waiting till the last minute of release to uncover it. \r\n\r\nhttps://anyscaleteam.slack.com/archives/CLJQR6420/p1675711316713589\r\n\r\n\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\nNA\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "xwjiang2010",
        "body": "Actually the issue is the reported cluster uses python 3.10 that has a higher pandas version than our release test clusters (python 3.7 with lower pandas version).\r\nwe should run our release test also with python 3.10 to catch issues like this.\r\n\r\ncc @matthewdeng to comment on priority.\r\n"
      },
      {
        "user": "amogkam",
        "body": "As mentioned in the Slack thread, I don't think the python version is the issue. \r\n\r\nThe issue comes from the fact that we explicitly install `modin==0.12.1` in the test, so that the compatible pandas version is installed.\r\n\r\nIn the default `ray-ml` docker image, however, we use the deprecated pip legacy resolver which makes no guarantees on compatibility for transient dependencies."
      },
      {
        "user": "xwjiang2010",
        "body": "yeah that makes sense. \r\nI wonder if there should be better measurements in place to make sure that the two are consistent. Which one should be the one to be fixed? Less twigs on each test's app_config? Or better procedure of building ray-ml docker image?"
      }
    ]
  },
  {
    "issue_number": 32454,
    "title": "[Core]  The remote function in the worker no longer runs after the head crashes",
    "author": "YQ-Wang",
    "state": "open",
    "created_at": "2023-02-11T01:28:38Z",
    "updated_at": "2025-06-17T00:15:13Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nIn the [Ray v2 whitepaper](https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.kr3vvxhmpdmu), it mentions that\r\n\r\n> Note that any running Ray tasks and actors will remain alive since these components do not require reading or writing the GCS. Similarly, any existing objects will continue to be available.\r\n\r\nHowever, it seems that the remote function in the worker is no longer running after the head crashes.\r\n\r\n### Versions / Dependencies\r\n\r\nPython 3.7\r\nRay 2.2.0\r\n\r\n1 Head: 1 cpu\r\n1 Worker: 1 cpu\r\n1 Redis\r\n\r\n### Reproduction script\r\n\r\n1. Use the following commands to reproduce the env on local machine using kind:\r\n```\r\nkubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml\r\n```\r\n\r\n```\r\nkubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster-networkpolicy.yaml\r\n```\r\n\r\n2. Copy the following script to the head node:\r\n```\r\nimport ray\r\nimport time\r\n\r\n@ray.remote(num_cpus=1, max_calls=1)\r\ndef write_redis():\r\n    import redis\r\n    r = redis.Redis(host='redis', port=6379, decode_responses=True)  \r\n\r\n    for i in range(120):\r\n        time.sleep(1)\r\n        r.set('counter', i)\r\n        print(r.get('counter'))\r\n    return \"hello world\"\r\n\r\nray.init()\r\nprint(ray.get(write_redis.remote()))\r\n```\r\n\r\n3. Port forward to the head node:\r\n```\r\nkubectl port-forward service/service-ray-cluster 8265:8265\r\n```\r\n\r\n4. Run the following code on local machine:\r\n```\r\nfrom ray.job_submission import JobSubmissionClient\r\n\r\nclient = JobSubmissionClient(\"http://127.0.0.1:8265\")\r\n\r\njob_id = client.submit_job(\r\n    entrypoint=\"python script.py\",\r\n    runtime_env = {\"pip\": [\"redis\"]},\r\n)\r\nprint(job_id)\r\n```\r\n\r\nWhat I observed:\r\nAfter the job gets submitted, the remote function is able to call Redis to record the counter value every 1s.\r\n \r\n![r1](https://user-images.githubusercontent.com/20148872/218231037-5cea5171-b997-4c1e-b886-6fd66a482a7d.png)\r\n\r\nThen I set the head replica from 1 to 0. The value of counter stops at 53 as shown below:\r\n![r2](https://user-images.githubusercontent.com/20148872/218231157-806b6a3e-7229-43dd-82ae-0c1588850737.png)\r\n\r\nAfter that, I set the head replica back to 1, copy the script to the head node and port forward. I can see the old job is still running, but the Redis value cannot be updated.\r\n![r3](https://user-images.githubusercontent.com/20148872/218231246-46c11d1b-3bd8-4f47-9d9b-b68931e61865.png)\r\n\r\nThen I submitted a new job, the previous job immediately failed. And the Redis is written with the new value.\r\n![r4](https://user-images.githubusercontent.com/20148872/218231280-432e096b-c3fa-4885-9db5-aa9d16a167d2.png)\r\n![r5](https://user-images.githubusercontent.com/20148872/218231284-1c450346-ba0c-430e-8935-a0b3f12cdf88.png)\r\n\r\nI think the above experiment shows that after the head crashes, the function in the worker also gets terminated (but the status shows as running in the dashboard for some reason). \r\n\r\n### Issue Severity\r\n\r\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cadedaniel",
        "body": "cc @architkulkarni what's the expected behavior for jobs when head node dies?"
      },
      {
        "user": "architkulkarni",
        "body": "The head node is supposed to recover the state of all running jobs, so this looks like some issue with that. @YQ-Wang if you happen to have logs for when this happens, it could be useful if you zipped up all the logs and shared them."
      },
      {
        "user": "YQ-Wang",
        "body": "> The head node is supposed to recover the state of all running jobs, so this looks like some issue with that. @YQ-Wang if you happen to have logs for when this happens, it could be useful if you zipped up all the logs and shared them.\r\n\r\nSure thing. Also, the above steps on this page can easily help you reproduce the issue.\r\n"
      }
    ]
  },
  {
    "issue_number": 32399,
    "title": "[RLlib] Special __common__ key in MultiAgent batches is not documented",
    "author": "ArturNiederfahrenhorst",
    "state": "open",
    "created_at": "2023-02-10T00:56:51Z",
    "updated_at": "2025-06-17T00:15:11Z",
    "labels": [
      "P2",
      "rllib",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nWe ignore the __common__ key when processing MultiAgent batches so that users can put arbitrary stuff there.\r\nThis is not documented.\r\n\r\nRelated PR/issue: https://github.com/ray-project/ray/pull/32362\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32380,
    "title": "[tune] update how trainable reports result/checkpoint to driver",
    "author": "xwjiang2010",
    "state": "open",
    "created_at": "2023-02-09T21:34:11Z",
    "updated_at": "2025-06-17T00:15:09Z",
    "labels": [
      "bug",
      "tune",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nUpdate how trainable reports result/checkpoint to driver.\r\n\r\nThe current flow is `def train()` returns result dict, which will then trigger `save()`, which will then come back for driver to handle it (mostly syncing files to head node if applicable). We also do stuff like `cached_decision` so that we can have non-blocking save. All these add to complexity to our tune loop.\r\n\r\nProposed:\r\n\r\n- when `train()` returns, trainable has already \"pushed\" checkpoint (if needed) to relevant destination (let it be head node or cloud storage). `train()` returns to head node both result dict as well as the checkpoint path for driver bookkeeping. \r\n- there will be no need of magic key word like \"should_checkpoint\", there will be no need of syncer stuff to \"pull\".\r\n- this also helps later if we want to have a unified storage file-system like API (because cloud storage and head node use the same code path and flow) --> easier to abstract.\n\n### Versions / Dependencies\n\nNA\n\n### Reproduction script\n\nNA\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "xwjiang2010",
        "body": "@krfricke @gjoliver some tech debt stuff?"
      },
      {
        "user": "matthewdeng",
        "body": "@krfricke is this captured as part of the control loop refactor work?"
      },
      {
        "user": "matthewdeng",
        "body": "BTW is this only if people return results or also if they use the `session.report` API?"
      }
    ]
  },
  {
    "issue_number": 32301,
    "title": "[Datasets] The projection pushdown cannot work with hive style partitioning file path",
    "author": "jianoaix",
    "state": "open",
    "created_at": "2023-02-08T01:04:58Z",
    "updated_at": "2025-06-17T00:15:07Z",
    "labels": [
      "P2",
      "pending-cleanup"
    ],
    "body": "Version: Ray master\r\n\r\nProjection pushdown cannot work with file path with hive style partitioning (e.g. \"some/file/path/X=A/shard\"). It will treat the \"X\" in the path as a field and expect the field to exist in the file.\r\n\r\n```\r\nimport ray\r\n\r\nds = ray.data.read_parquet(\r\n    [\"s3://anonymous@air-example-data/ursa-labs-taxi-data/epoch=10001/downsampled_2009_01_data.parquet\", \"s3://anonymous@air-example-data/ursa-labs-taxi-data/epoch=10002/downsampled_2009_01_data.parquet\"],\r\n    columns=[\"vendor_id\", \"trip_distance\"]\r\n)\r\n\r\nprint(ds.take(10))\r\n```\r\nIt failed with:\r\n\r\n```\r\n(_execute_read_task_split pid=1903508) Traceback (most recent call last):\r\n(_execute_read_task_split pid=1903508)   File \"python/ray/_raylet.pyx\", line 641, in ray._raylet.execute_dynamic_generator_and_store_task_outputs\r\n(_execute_read_task_split pid=1903508)     core_worker.store_task_outputs(\r\n(_execute_read_task_split pid=1903508)   File \"python/ray/_raylet.pyx\", line 2498, in ray._raylet.CoreWorker.store_task_outputs\r\n(_execute_read_task_split pid=1903508)     for i, output in enumerate(outputs):\r\n(_execute_read_task_split pid=1903508)   File \"/home/ubuntu/ray/python/ray/data/_internal/lazy_block_list.py\", line 685, in _execute_read_task_split\r\n(_execute_read_task_split pid=1903508)     for block in blocks:\r\n(_execute_read_task_split pid=1903508)   File \"/home/ubuntu/ray/python/ray/data/datasource/datasource.py\", line 185, in __call__\r\n(_execute_read_task_split pid=1903508)     for block in result:\r\n(_execute_read_task_split pid=1903508)   File \"/home/ubuntu/ray/python/ray/data/datasource/parquet_datasource.py\", line 393, in _read_pieces\r\n(_execute_read_task_split pid=1903508)     table = table.set_column(\r\n(_execute_read_task_split pid=1903508)   File \"pyarrow/table.pxi\", line 4503, in pyarrow.lib.Table.set_column\r\n(_execute_read_task_split pid=1903508)   File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n(_execute_read_task_split pid=1903508)   File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\n(_execute_read_task_split pid=1903508) pyarrow.lib.ArrowInvalid: Invalid column index to set field.\r\n(raylet) /usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\r\n(raylet)   warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\n(_execute_read_task_split pid=1903508) 2023-02-08 00:57:28,778\tINFO worker.py:772 -- Task failed with retryable exception: TaskID(32d950ec0ccf9d2affffffffffffffffffffffff01000000).\r\n(_execute_read_task_split pid=1903508) Traceback (most recent call last):\r\n(_execute_read_task_split pid=1903508)   File \"python/ray/_raylet.pyx\", line 641, in ray._raylet.execute_dynamic_generator_and_store_task_outputs\r\n(_execute_read_task_split pid=1903508)     core_worker.store_task_outputs(\r\n(_execute_read_task_split pid=1903508)   File \"python/ray/_raylet.pyx\", line 2498, in ray._raylet.CoreWorker.store_task_outputs\r\n(_execute_read_task_split pid=1903508)     for i, output in enumerate(outputs):\r\n(_execute_read_task_split pid=1903508)   File \"/home/ubuntu/ray/python/ray/data/_internal/lazy_block_list.py\", line 685, in _execute_read_task_split\r\n(_execute_read_task_split pid=1903508)     for block in blocks:\r\n(_execute_read_task_split pid=1903508)   File \"/home/ubuntu/ray/python/ray/data/datasource/datasource.py\", line 185, in __call__\r\n(_execute_read_task_split pid=1903508)     for block in result:\r\n(_execute_read_task_split pid=1903508)   File \"/home/ubuntu/ray/python/ray/data/datasource/parquet_datasource.py\", line 393, in _read_pieces\r\n(_execute_read_task_split pid=1903508)     table = table.set_column(\r\n(_execute_read_task_split pid=1903508)   File \"pyarrow/table.pxi\", line 4503, in pyarrow.lib.Table.set_column\r\n(_execute_read_task_split pid=1903508)   File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n(_execute_read_task_split pid=1903508)   File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\n(_execute_read_task_split pid=1903508) pyarrow.lib.ArrowInvalid: Invalid column index to set field.\r\n(_execute_read_task_split pid=1903508) 2023-02-08 00:57:29,704\tINFO worker.py:772 -- Task failed with retryable exception: TaskID(32d950ec0ccf9d2affffffffffffffffffffffff01000000).\r\n```\r\n",
    "comments": [
      {
        "user": "jianoaix",
        "body": "As a workaround, user can add \"epoch\" into the column list to push down. The following will work:\r\n```\r\nimport ray\r\n\r\nds = ray.data.read_parquet(\r\n    [\"s3://anonymous@air-example-data/ursa-labs-taxi-data/epoch=10001/downsampled_2009_01_data.parquet\", \"s3://anonymous@air-example-data/ursa-labs-taxi-data/epoch=10002/downsampled_2009_01_data.parquet\"],\r\n    columns=[\"vendor_id\", \"trip_distance\", \"epoch\"]\r\n)\r\n\r\nprint(ds.take(10))\r\n```\r\n\r\n"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32297,
    "title": "[Core][utilization] some anti-pattern that not well supported by Ray core.",
    "author": "scv119",
    "state": "open",
    "created_at": "2023-02-08T00:44:06Z",
    "updated_at": "2025-06-17T00:15:04Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\none of our user want to use Ray object store as a cache to be consumed by multiple down stream tasks, and tolerate node failures.  He does so by storing it in an actor, and fetch the data from cloud storage on actor reconstruction. \r\n\r\nHowever, the prefetch data is multi-threaded so we'd like to be allocated on a beefy machine. In this case, the actor is scheduled with 16 cpus. \r\n\r\nThis leads to a lot of resource wasted: the actor is scheduled on a large machine and uses all the logical resources, but in reality the resource are barely used, unless the actor restarted and loading the data.\r\n\r\n\r\n### Use case\r\n\r\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32232,
    "title": "[tune/train] Provide actionable error messages for common thirdparty errors",
    "author": "krfricke",
    "state": "open",
    "created_at": "2023-02-06T17:35:46Z",
    "updated_at": "2025-06-17T00:15:02Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### Description\n\nUsers run into common errors when using thirdparty libraries with Ray. For instance:\r\n\r\n```\r\nRuntimeError: No CUDA GPUs are available\r\n```\r\n\r\nis commonly encountered when GPU usage is not configured correctly.\r\n\r\nSee also https://discuss.ray.io/t/runtimeerror-no-cuda-gpus-are-available/1787/14\r\n\r\nWe may want to catch some of these common exceptions and re-raise them with an actionable message. In the case above for instance, we could inform the user that they should specify `GPU` resources.\r\n\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32113,
    "title": "[ci] Mirror external dependenies in CI",
    "author": "krfricke",
    "state": "open",
    "created_at": "2023-01-31T18:12:10Z",
    "updated_at": "2025-06-17T00:15:01Z",
    "labels": [
      "P2",
      "testing",
      "pending-cleanup"
    ],
    "body": "Our CI sometimes breaks due to unavailability of external resources. In a [recent example](https://buildkite.com/ray-project/oss-ci-build-branch/builds/2093#018608f5-a6d3-4889-8536-c5bc68982afb) the SSL certificate of a vendor expired, leading all our build jobs to fail.\r\n\r\nIn this particular case we're referring to a pinned dependency. Instead of relying on third party hosts, we should mirror these dependencies and host them ourselves. This requires a process to add new dependencies and remove stale ones.\r\n",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32049,
    "title": "[Serve] ValueError: Message ray.serve.ReplicaConfig exceeds maximum protobuf size of 2GB",
    "author": "jamm1985",
    "state": "open",
    "created_at": "2023-01-30T13:42:49Z",
    "updated_at": "2025-06-17T00:14:59Z",
    "labels": [
      "bug",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nDeployment fails with large objects, especially large numpy arrays.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nInput In [14], in <module>\r\n----> 1 serve.run(MyModelDeployment.bind(\"test\", ray.get(weights_ref)))\r\n\r\nFile /root/conda/envs/recommender/lib/python3.9/site-packages/ray/serve/api.py:536, in run(target, _blocking, host, port)\r\n    523     deployment_parameters = {\r\n    524         \"name\": deployment._name,\r\n    525         \"func_or_class\": deployment._func_or_class,\r\n   (...)\r\n    533         \"is_driver_deployment\": deployment._is_driver_deployment,\r\n    534     }\r\n    535     parameter_group.append(deployment_parameters)\r\n--> 536 client.deploy_group(\r\n    537     parameter_group, _blocking=_blocking, remove_past_deployments=True\r\n    538 )\r\n    540 if ingress is not None:\r\n    541     return ingress._get_handle()\r\n\r\nFile /root/conda/envs/recommender/lib/python3.9/site-packages/ray/serve/_private/client.py:37, in _ensure_connected.<locals>.check(self, *args, **kwargs)\r\n     35 if self._shutdown:\r\n     36     raise RayServeException(\"Client has already been shut down.\")\r\n---> 37 return f(self, *args, **kwargs)\r\n\r\nFile /root/conda/envs/recommender/lib/python3.9/site-packages/ray/serve/_private/client.py:251, in ServeControllerClient.deploy_group(self, deployments, _blocking, remove_past_deployments)\r\n    248 deployment_args_list = []\r\n    249 for deployment in deployments:\r\n    250     deployment_args_list.append(\r\n--> 251         self.get_deploy_args(\r\n    252             deployment[\"name\"],\r\n    253             deployment[\"func_or_class\"],\r\n    254             deployment[\"init_args\"],\r\n    255             deployment[\"init_kwargs\"],\r\n    256             ray_actor_options=deployment[\"ray_actor_options\"],\r\n    257             config=deployment[\"config\"],\r\n    258             version=deployment[\"version\"],\r\n    259             route_prefix=deployment[\"route_prefix\"],\r\n    260             is_driver_deployment=deployment[\"is_driver_deployment\"],\r\n    261         )\r\n    262     )\r\n    264 updating_list = ray.get(\r\n    265     self._controller.deploy_group.remote(deployment_args_list)\r\n    266 )\r\n    268 tags = []\r\n\r\nFile /root/conda/envs/recommender/lib/python3.9/site-packages/ray/serve/_private/client.py:37, in _ensure_connected.<locals>.check(self, *args, **kwargs)\r\n     35 if self._shutdown:\r\n     36     raise RayServeException(\"Client has already been shut down.\")\r\n---> 37 return f(self, *args, **kwargs)\r\n\r\nFile /root/conda/envs/recommender/lib/python3.9/site-packages/ray/serve/_private/client.py:485, in ServeControllerClient.get_deploy_args(self, name, deployment_def, init_args, init_kwargs, ray_actor_options, config, version, route_prefix, is_driver_deployment)\r\n    471 if (\r\n    472     deployment_config.autoscaling_config is not None\r\n    473     and deployment_config.max_concurrent_queries\r\n    474     < deployment_config.autoscaling_config.target_num_ongoing_requests_per_replica  # noqa: E501\r\n    475 ):\r\n    476     logger.warning(\r\n    477         \"Autoscaling will never happen, \"\r\n    478         \"because 'max_concurrent_queries' is less than \"\r\n    479         \"'target_num_ongoing_requests_per_replica' now.\"\r\n    480     )\r\n    482 controller_deploy_args = {\r\n    483     \"name\": name,\r\n    484     \"deployment_config_proto_bytes\": deployment_config.to_proto_bytes(),\r\n--> 485     \"replica_config_proto_bytes\": replica_config.to_proto_bytes(),\r\n    486     \"route_prefix\": route_prefix,\r\n    487     \"deployer_job_id\": ray.get_runtime_context().job_id,\r\n    488     \"is_driver_deployment\": is_driver_deployment,\r\n    489 }\r\n    491 return controller_deploy_args\r\n\r\nFile /root/conda/envs/recommender/lib/python3.9/site-packages/ray/serve/config.py:501, in ReplicaConfig.to_proto_bytes(self)\r\n    500 def to_proto_bytes(self):\r\n--> 501     return self.to_proto().SerializeToString()\r\n\r\nValueError: Message ray.serve.ReplicaConfig exceeds maximum protobuf size of 2GB: 3200001162\r\n```\r\n\r\n### Versions / Dependencies\r\n\r\n```\r\nLinux 4cb379e01e77 3.10.0-1160.15.2.el7.x86_64 #1 SMP Wed Feb 3 15:06:38 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n```\r\nPython 3.9.10\r\n```\r\n\r\n```\r\nray-air                   2.2.0            py39hf3d152e_2    conda-forge\r\nray-all                   2.2.0            py39hf3d152e_2    conda-forge\r\nray-core                  2.2.0            py39h4d85f9a_2    conda-forge\r\nray-dashboard             2.2.0            py39h9f3bf79_2    conda-forge\r\nray-data                  2.2.0            py39hf3d152e_2    conda-forge\r\nray-default               2.2.0            py39hf3d152e_2    conda-forge\r\nray-k8s                   2.2.0            py39hf3d152e_2    conda-forge\r\nray-rllib                 2.2.0            py39hf3d152e_2    conda-forge\r\nray-serve                 2.2.0            py39hf3d152e_2    conda-forge\r\nray-train                 2.2.0            py39hf3d152e_2    conda-forge\r\nray-tune                  2.2.0            py39hf3d152e_2    conda-forge\r\n```\r\n\r\n```\r\ngrpc-cpp                  1.43.2               h9e046d8_3    conda-forge\r\ngrpcio                    1.46.3           py39h0f497a6_0    conda-forge\r\n```\r\n\r\n\r\n### Reproduction script\r\n\r\n```\r\nimport ray\r\nfrom ray import serve\r\n\r\nray.init()\r\n\r\nweights = np.ones((20000, 20000))\r\nweights_ref = ray.put(weights)\r\n\r\n@serve.deployment(route_prefix=\"/\")\r\nclass MyModelDeployment:\r\n    def __init__(self, msg: str, weights: np.ndarray):\r\n        # Initialize model state: could be very large neural net weights.\r\n        self._msg = msg\r\n        self._weights = weights\r\n\r\n    def __call__(self, request: Request) -> Dict:\r\n        return {\"result\": self._msg}\r\n\r\nserve.run(MyModelDeployment.bind(\"test\", ray.get(weights_ref)))\r\n```\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "sharlec",
        "body": "I am experiencing the same problem, did you solve it?\r\n"
      },
      {
        "user": "jamm1985",
        "body": "Right now, I'm not pushing big weights from object storage through server deployments. Instead, the weights are obtained from the internal deployment process after the deployment graph is run in the cluster."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 32037,
    "title": "[CLI] make `ray get-head-ip` and `ray get-worker-ips` work for kuberay clusters when run outside the cluster",
    "author": "davidxia",
    "state": "open",
    "created_at": "2023-01-29T22:58:19Z",
    "updated_at": "2025-06-17T00:14:56Z",
    "labels": [
      "enhancement",
      "P2",
      "kuberay",
      "core-autoscaler",
      "infra",
      "core",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nI want to be able to run `ray get-head-ip cluster-config.yaml` and `ray get-worker-ips cluster-config.yaml` for a RayCluster created with kuberay.\r\n\r\nCurrently these commands for kuberay fail because they assume they're running inside the GKE cluster that the RayCluster is on with correct permissions. It'd be great if these commands also worked when run outside the cluster.\r\n\r\n\r\n## example cluster config\r\n\r\n```\r\ncat cluster-config.yaml\r\n\r\ncluster_name: dxia-test\r\nprovider:\r\n    type: kuberay\r\n    namespace: hyperkube\r\n    worker_liveness_check: False\r\n    worker_rpc_drain: True\r\n    disable_node_updaters: True\r\n    disable_launch_config_check: True\r\n    foreground_node_launch: True\r\n    use_internal_ips: True\r\n```\r\n\r\n## current behavior\r\n\r\n```\r\n❯ ray get-head-ip cluster-config.yaml\r\n2023-01-29 12:32:59,109\tINFO node_provider.py:211 -- Creating KuberayNodeProvider.\r\nTraceback (most recent call last):\r\n  File \"/Users/dxia/.pyenv/versions/hray/bin/ray\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2386, in main\r\n    return cli()\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1665, in get_head_ip\r\n    click.echo(get_head_node_ip(cluster_config_file, cluster_name))\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/autoscaler/_private/commands.py\", line 1295, in get_head_node_ip\r\n    provider = _get_node_provider(config[\"provider\"], config[\"cluster_name\"])\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/autoscaler/_private/providers.py\", line 229, in _get_node_provider\r\n    new_provider = provider_cls(provider_config, cluster_name)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 215, in __init__\r\n    self.headers, self.verify = load_k8s_secrets()\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 144, in load_k8s_secrets\r\n    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/token\") as secret:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/var/run/secrets/kubernetes.io/serviceaccount/token'\r\n```\r\n\r\n```\r\n❯ ray get-head-ip cluster-config.yaml\r\n2023-01-29 12:35:43,105\tINFO node_provider.py:211 -- Creating KuberayNodeProvider.\r\nTraceback (most recent call last):\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/urllib3/connection.py\", line 174, in _new_conn\r\n    conn = connection.create_connection(\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/urllib3/util/connection.py\", line 72, in create_connection\r\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\r\n  File \"/Users/dxia/.pyenv/versions/3.8.12/lib/python3.8/socket.py\", line 918, in getaddrinfo\r\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\nsocket.gaierror: [Errno 8] nodename nor servname provided, or not known\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\r\n    self._validate_conn(conn)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 1042, in _validate_conn\r\n    conn.connect()\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/urllib3/connection.py\", line 358, in connect\r\n    self.sock = conn = self._new_conn()\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/urllib3/connection.py\", line 186, in _new_conn\r\n    raise NewConnectionError(\r\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x125a111f0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/requests/adapters.py\", line 489, in send\r\n    resp = conn.urlopen(\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\r\n    retries = retries.increment(\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/urllib3/util/retry.py\", line 592, in increment\r\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='kubernetes.default', port=443): Max retries exceeded with url: /apis/ray.io/v1alpha1/namespaces/hyperkube/rayclusters/dxia-test (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x125a111f0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/dxia/.pyenv/versions/hray/bin/ray\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2386, in main\r\n    return cli()\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1665, in get_head_ip\r\n    click.echo(get_head_node_ip(cluster_config_file, cluster_name))\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/autoscaler/_private/commands.py\", line 1296, in get_head_node_ip\r\n    head_node = _get_running_head_node(config, config_file, override_cluster_name)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/autoscaler/_private/commands.py\", line 1362, in _get_running_head_node\r\n    nodes = provider.non_terminated_nodes(head_node_tags)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/autoscaler/batching_node_provider.py\", line 155, in non_terminated_nodes\r\n    self.node_data_dict = self.get_node_data()\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 232, in get_node_data\r\n    self._raycluster = self._get(f\"rayclusters/{self.cluster_name}\")\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 427, in _get\r\n    result = requests.get(url, headers=self.headers, verify=self.verify)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/requests/api.py\", line 73, in get\r\n    return request(\"get\", url, params=params, **kwargs)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/requests/api.py\", line 59, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/requests/sessions.py\", line 587, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/requests/sessions.py\", line 701, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/Users/dxia/.pyenv/versions/hray/lib/python3.8/site-packages/requests/adapters.py\", line 565, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='kubernetes.default', port=443): Max retries exceeded with url: /apis/ray.io/v1alpha1/namespaces/hyperkube/rayclusters/dxia-test (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x125a111f0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\r\n```\r\n\r\nOnce these lines are changed it works\r\n\r\nhttps://github.com/ray-project/ray/blob/ray-2.2.0/python/ray/autoscaler/_private/kuberay/node_provider.py#L144 to file path of token\r\n\r\nhttps://github.com/ray-project/ray/blob/ray-2.2.0/python/ray/autoscaler/_private/kuberay/node_provider.py#L150 to file path of cert\r\n\r\nhttps://github.com/ray-project/ray/blob/ray-2.2.0/python/ray/autoscaler/_private/kuberay/node_provider.py#L170 to `\"https://K8S_API_HOSTNAME_OR_IP\"`\r\n\r\n\r\n### Use case\r\n\r\nIt's convenient to be able to run these `ray` CLI commands outside the kuberay RayCluster. This will close the feature parity gap between kuberay clusters and other types of clusters.",
    "comments": [
      {
        "user": "davidxia",
        "body": "cc @DmitriGekhtman if you have any thoughts"
      },
      {
        "user": "DmitriGekhtman",
        "body": "KubeRay node provider is meant as an internal object -- basically a shim to get autoscaling to work.\n\nMaybe the implementation could involve a separate node provider with nothing but the ray cluster name and namespace in the config (because that's all you need assuming you've configured access to your k8 cluster)"
      },
      {
        "user": "gvspraveen",
        "body": "cc @architkulkarni @kevin85421\r\n"
      }
    ]
  },
  {
    "issue_number": 32001,
    "title": "Serve build usage of click CLI library conflicts python argparse",
    "author": "ashahab",
    "state": "open",
    "created_at": "2023-01-27T19:13:36Z",
    "updated_at": "2025-06-17T00:14:54Z",
    "labels": [
      "bug",
      "P2",
      "serve",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nIf the serve deployment code uses  python argparse, serve build will fail with an arg-parse related error.\r\n\r\nFor now the workaround is to remove argparse from the deployment code.\n\n### Versions / Dependencies\n\nray 2.2.0. python 3.7\n\n### Reproduction script\n\n```python\r\n@serve.deployment(num_replicas=1, ray_actor_options={\"num_cpus\": 1})\r\nclass ImagePostIntentInference:\r\n  def __init__(self, args):\r\n    pass\r\n  async def __call__(self, http_request: Request):\r\n    pass\r\nargparse.ArgumentParser().parse_args()\r\ninference_model_driver = ImagePostIntentInference.bind(args)\r\n```\r\n\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31928,
    "title": "[Serve] Version Support in 2.X API",
    "author": "sihanwang41",
    "state": "open",
    "created_at": "2023-01-25T17:50:16Z",
    "updated_at": "2025-06-17T00:14:52Z",
    "labels": [
      "enhancement",
      "P2",
      "serve",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### Description\r\n\r\nAdd a version support in the serve.run() API to make sure the deployment will not be redeployed with same version.\r\n\r\n```\r\nserver.run(xxxx, version: str = None)\r\n```\r\n\r\nWhen the version is provided, serve will set all deployments to the version, and the same version of the deployment won't be redeployed.\r\n\r\n(Also need to be supported in the dashboard serve agent for REST API deploy)\r\n\r\n### Use case\r\n\r\n_No response_",
    "comments": [
      {
        "user": "zoltan-fedor",
        "body": "Just to clarify, this is to restore the functionality of the `version` tag in deployments when using API v1.x.\r\n\r\nWith API v1.x when you deploy a Serve deployment with a version specified (by calling the `deploy()` method), if such a deployment with such name and version is already running, then it will just return the ref for that. \r\nIf such a deployment (name and version matching) does not exist, then it will create (or redeploy) the deployment and then return its ref. \r\n\r\nThis is basically the well-known `get_or_create` pattern of `classmethods` of OOP.\r\n\r\nAgain, this existed in API v1.x - and being used by people. We are trying to restore existing functionality into API v2.x."
      },
      {
        "user": "sihanwang41",
        "body": "Yes, @zoltan-fedor. We are not going to deprecate the version functionality in v1.x API until v2.x API have supports with it.\r\n\r\n> If such a deployment (name and version matching) does not exist, then it will create (or redeploy) the deployment and then return its ref.\r\n\r\nThat is the correct expected behavior for v2.x API with this support."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31913,
    "title": "[Train] User exceptions not propagated from remote cluster",
    "author": "mlappelbaum",
    "state": "open",
    "created_at": "2023-01-25T00:11:53Z",
    "updated_at": "2025-06-17T00:14:49Z",
    "labels": [
      "bug",
      "P2",
      "train",
      "core-client",
      "observability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nRay Trainer does not propagate exceptions from `train_loop_per_worker` when workers run on a remote cluster.  Specifically, when training on a shared cluster and submitting the training job using the ray client in a driver script running on a separate machine.  The workers will print the exception and stack trace to ray worker logs but the driver will not raise an exception, and the `Result` returned from the call to `Trainer.fit()` has `error=None`.\r\n\r\nThe expected behavior is that user exceptions are re-raised in the driver script.  This occurs when the trainer is run in a local ray cluster (i.e. when calling `ray.init()` or `ray.init('auto')`), and occurs in all cases in ray versions < 2.0.0.\n\n### Versions / Dependencies\n\nRay: >= 2.0.0 (tested with 2.0.0, 2.1.0, and 2.2.0) \r\nPython: 3.7.11\r\nOS: Ubuntu 18.04.6 LTS (Bionic Beaver)\n\n### Reproduction script\n\n1. On one host run `ray start --head --port 6379`.\r\n\r\n2. On a different host set environment variable `RAY_CLUSTER_IP` to the ip address of the first host and run:\r\n\r\n```python\r\nimport ray\r\nfrom ray.train.data_parallel_trainer import DataParallelTrainer\r\nfrom ray.air.config import ScalingConfig, RunConfig, FailureConfig\r\nimport os\r\n\r\n# ip address of remote cluster head node\r\ncluster_ip = os.getenv(\"RAY_CLUSTER_IP\")\r\n\r\ndef train_func():\r\n  raise RuntimeError(\"User error\")\r\n\r\nray.init(address=f\"ray://{cluster_ip}:10001\")\r\n\r\ntrainer = DataParallelTrainer(\r\n  train_loop_per_worker=train_func,\r\n  scaling_config=ScalingConfig(num_workers=1),\r\n  run_config=RunConfig(\r\n    # Setting fail_fast=True does not work.\r\n    # Setting fail_fast='raise' works but is discouraged \r\n    failure_config=FailureConfig(fail_fast=True),\r\n  ),\r\n)\r\n\r\ntry:\r\n  result = trainer.fit()\r\n  print(\"No exception raised\")\r\n  if result.error is None:\r\n    print(\"Result indicates no error\")\r\n  else:\r\n    print(f\"Result contains error {result.error}\")\r\nexcept Exception as e:\r\n  print(f\"Trainer.fit() raised exception: {e}\")\r\n```\r\n\r\n\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31783,
    "title": "[RLlib] AlgorithmConfig() defaults not used by build_sac_model when implementing custom model",
    "author": "gresavage",
    "state": "open",
    "created_at": "2023-01-19T21:08:28Z",
    "updated_at": "2025-06-17T00:14:48Z",
    "labels": [
      "enhancement",
      "P2",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\n- [x] I searched for related issues and did not find anything matching. The closest issue(s) are: https://github.com/ray-project/ray/issues/22747 and https://github.com/ray-project/ray/issues/13218 but do not exactly match my problem\r\n\r\nI am using a custom model with SAC. I have recently updated my ray version from 1.13.0 to 2.2.0 and am in the process of updating my code to comply with the major release changes.\r\n\r\nI have created a custom `AlgorithmConfig()` class which subclasses from `SACConfig()` and sets the `self.q_model_config['custom_model']` to use the custom model class.\r\n\r\nI have created my own `build_sac_model` function which line-for-line mimics the behavior of the stock `build_sac_model` function except it sets `default_model_cls` to use my custom `ModelV2` classes\r\n\r\n### The Bug\r\n\r\nWhen calling `build_sac_model` only `MODEL_DEFAULTS` + the explicit `q_model_config` / `policy_model_config` values are used rather than `MODEL_DEFAULTS` + custom `AlgorithmConfig()` defaults + explicit `q_model_config` / `policy_model_config` values. Thus, since I specify the `custom_model` class in the default values for my custom `AlgorithmConfig()` class the model(s) are not constructed using this custom model.\r\n\r\n### Expected Behavior\r\n\r\nWhen calling `build_sac_model` the `policy_model_config` and `q_model_config` should merge (in order):\r\n\r\n1. The `MODEL_DEFAULTS` from the model catalog\r\n2. Any defaults in the `AlgorithmConfig()`\r\n3. Any explicitly passed values in the `config`\r\n\r\nWhen using a custom `DEFAULT_CONFIG` in Ray 1.13.0 the default value for custom model would correctly be passed to the config going to `build_sac_model`\r\n\r\n### Useful Information\r\n\r\n#### Source of Issue\r\n\r\nI have found that the call to `build_policy_class` which merges the default config with the passed config fully overwrites the dict because it's a simple call: `config = dict(get_default_config(), **config)`\r\n\r\nAfter even more investigation I found this basic `config = dict(default_config, **config)` in numerous places including `PPO.__init__` as well.\r\n\r\nIt would be better to do <del>`merge_trainer_configs`</del> `ray.tune.utils.merge_dicts` and traverse the whole dict structure\r\n\r\n#### My Workaround\r\n\r\nIn my `CustomPolicy.__init__` function I have the following workaround:\r\n\r\n```\r\nfrom ray.tune.utils import merge_dicts\r\n\r\nclass CustomPolicy(Policy):\r\n    def __init__(\r\n        self,\r\n        observation_space: gym.spaces.Space,\r\n        action_space: gym.spaces.Space,\r\n        config: AlgorithmConfigDict,\r\n        *,\r\n        max_seq_len: int = 20,\r\n    ):\r\n        config = merge_dicts(<path>.<to>.<custom config class>.CustomConfig().to_dict(), config)\r\n        super().__init__(observation_space=observation_space, action_space=action_space, config=config, max_seq_len=max_seq_len)\r\n````",
    "comments": [
      {
        "user": "ArturNiederfahrenhorst",
        "body": "Hi @gresavage , thanks for filing this issue.\r\nSAC is a bit quirky here with these two model configs and I agree that this is not straightforward the way we are doing it right now. The modelconfig contained in AlgorithmConfig is a deepcopy of MODEL_DEFAULTS. So I think the way to go here is it leave out your step (1) and only do (2) and (3):\r\n\r\n> 2. Any defaults in the AlgorithmConfig()\r\n> 3. Any explicitly passed values in the config\r\n\r\nI'm setting up a PR with that and see if we have to consider other stuff."
      },
      {
        "user": "gresavage",
        "body": "Hi, @ArturNiederfahrenhorst Thanks for the quick reply and I appreciate you taking up the matter!\r\n\r\nI don't know if you saw some of the edits I made to the original issue but I found that the same behavior happens even in PPO which does not use `build_policy_class`. I was able to trace the behavior to the call to `__init__` which sets up the config as follows:\r\n```\r\nconfig = dict(ray.rllib.algorithms.ppo.ppo.PPOConfig().to_dict(), **config)\r\n```\r\n\r\nMay I suggest an even *better* solution than my original which is to use `ray.tune.utils.merge_dicts` and do a full recursive merge:\r\n\r\n```\r\nconfig = merge_dicts(ray.rllib.algorithms.ppo.ppo.PPOConfig().to_dict(), config)\r\n```\r\n\r\nI haven't checked over all the algorithms yet but I imagine many more than just SAC and PPO use this same `config = dict(default_config, **config)` logic."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31623,
    "title": "[kubernetes/cluster] More guides on deployment",
    "author": "richardliaw",
    "state": "open",
    "created_at": "2023-01-12T01:56:06Z",
    "updated_at": "2025-06-17T00:14:45Z",
    "labels": [
      "P2",
      "docs",
      "kuberay",
      "core",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "### Description\n\nOften see the following two questions:\r\n\r\n* Can you use ray up and ray submit if you deploy with kubernetes?\r\n* How do we do ephemeral cluster deployments?\r\n\r\nIn particular, there seems to be some missing user guides for \"what's the right way to deploy ray for multiple users\" on Kubernetes.\r\n\r\nAlso, the user guide index page doesn't provide a lot of detail as to what each guide is for.\n\n### Link\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31549,
    "title": "[core][state] ray log supporting regex searching ",
    "author": "rickyyx",
    "state": "open",
    "created_at": "2023-01-09T23:50:45Z",
    "updated_at": "2025-06-17T00:14:43Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "observability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nAdd support for \r\n`ray logs <glob> --search[-s] <regex>` should return search results of keywords from logs files filtered by <glob> \r\n\r\n\r\n\r\n### Versions / Dependencies\r\n\r\nmaster\r\n\r\n### Reproduction script\r\n\r\nNA\r\n\r\n### Issue Severity\r\n\r\nNone",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31492,
    "title": "[Tune] Support NLopt search algorithms",
    "author": "nikosavola",
    "state": "open",
    "created_at": "2023-01-06T09:39:55Z",
    "updated_at": "2025-06-17T00:14:41Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "pending-cleanup"
    ],
    "body": "### Description\n\n[NLopt](https://nlopt.readthedocs.io/en/latest/) support could be added to the [search algorithms](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#custom-search-algorithms-tune-search-searcher) to increase the amount of available optimisation algorithms and suitability for general black-bok optimisation.\r\n\r\nThere exists a [Python API with a common interface through `nl.opt`](https://nlopt.readthedocs.io/en/latest/NLopt_Python_Reference/#the-nloptopt-class) to which support could be added.\n\n### Use case\n\nEnabling using Ray Tune as a manager for black-box optimisation with NLopt.",
    "comments": [
      {
        "user": "richardliaw",
        "body": "Seems cool! Would you be interested in making an initial PR? We'd be happy to shepherd it after :)"
      },
      {
        "user": "jon-chuang",
        "body": "> general black-box optimisation.\r\n\r\nThis would probably have to be the global and local derivative-free algorithms. The ones that require gradients probably won't work without them, and it's hard to supply those gradients to Tune (you'd have to have a setup similar to meta learning to collect e.g. `d loss / d hyperarameter`).\r\n\r\nIt's also unclear to what degree algorithms like NLOpt would be for Tune. Algorithms like BayesOpt are designed to be sample efficient, by contrast, I am a little afraid that NLOpt algorithms may assume evaluation of the objective function f is cheap.\r\n\r\nIt is also a little unclear to me to what degree the constraints will be useful, especially non-linear constraints. Perhaps I am intending to set a non-linear bound on the complexity of my models?\r\n\r\ne.g. `channels^3 + layers^2 < 500`\r\n\r\nAnw, I think I can give this a shot."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31463,
    "title": "[Rllib] Possible Redudant Code",
    "author": "tudorjnu",
    "state": "open",
    "created_at": "2023-01-05T11:24:20Z",
    "updated_at": "2025-06-17T00:14:39Z",
    "labels": [
      "question",
      "P2",
      "rllib",
      "rllib-env",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nI am using Rllib in combination with `dm_control`. However, I am using a custom environment that has been created with `dm_control.composer` and I managed to get it working by changing the file from [here](https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/dm_control_wrapper.py). \r\n\r\nWhat would be the reason for having the following: \r\n- https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/dm_control_wrapper.py#L148\r\n- https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/dm_control_wrapper.py#L181-L183\r\n\r\nLooking at the rest of the code it seems not to have any reference to it. Thank you! \n\n### Versions / Dependencies\n\nRay 2.3.0\n\n### Reproduction script\n\nThe code is referenced above. \n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "sven1977",
        "body": "Hey @tudorjnu , thanks for your question. I'm not sure I understand correctly, though.\r\nIn my opinion, we are simply defining the private `self._state_space` property in the constructor, then make this property available to users via the\r\n```\r\n@property\r\ndef state_space(self):\r\n    return self._state_space\r\n```\r\n\r\n\"getter\". You are right, it might be debatable, whether this detour is needed here at all (I'm usually not in favor of getter/setter design in python), but it does protect the value of the property against unwanted, accidental changes by the user, so it might be the right thing to do here. Either way, it's not duplicate code."
      },
      {
        "user": "tudorjnu",
        "body": "Hello @sven1977, thanks for the response!\r\n\r\nThe question is why the user might want to access that property in the first space as it only makes reference to the `observation_space` which the user already has access to through `self.observation_space`. \r\n\r\nI encountered some errors when I tried to incorporate a mix of observations (e.g. camera, joint_position). Also, by looking at the `gymnasium` repo I cannot find anything related to that property at all. This brings the question, was this used in previous versions of `rllib` as a way to reference the observation space and it is not needed anymore? "
      },
      {
        "user": "stale[bot]",
        "body": "Hi, I'm a bot from the Ray team :)\n\nTo help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.\n\nIf there is no further activity in the 14 days, the issue will be closed!\n\n- If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!\n- If you'd like to get more attention to the issue, please tag one of Ray's contributors.\n\nYou can always ask for help on our [discussion forum](https://discuss.ray.io/) or [Ray's public slack channel](https://github.com/ray-project/ray#getting-involved).\n"
      }
    ]
  },
  {
    "issue_number": 31380,
    "title": "[aws] ray submit --stop fails on aws",
    "author": "vladfi1",
    "state": "open",
    "created_at": "2022-12-31T00:58:58Z",
    "updated_at": "2025-06-17T00:14:37Z",
    "labels": [
      "bug",
      "P2",
      "infra",
      "core",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nI would like to stop my remote computations by setting the `--stop` flag when using `ray submit`. However, this flag seems to be broken, at least with aws, as it results in the following error:\r\n\r\n```\r\nFailed to fetch IAM instance profile data for ray-autoscaler-v1 from AWS.\r\nError code: AccessDenied\r\n\r\n!!! Boto3 error:\r\nAn error occurred (AccessDenied) when calling the GetInstanceProfile operation: User: arn:aws:sts::961518004113:assumed-role/ray-autoscaler-v1/i-003a939b25fb7e013 is not authorized to perform: iam:GetInstanceProfile on resource: instance profile ray-autoscaler-v1 because no identity-based policy allows the iam:GetInstanceProfile action\r\n!!!\r\n```\r\n\r\nIt works fine without `--stop` and manually shutting down with `ray down`, but that requires me to be around when the computation finishes.\n\n### Versions / Dependencies\n\nI've tried this with ray 2.2 on both ubuntu with python 3.9 and 3.10.\n\n### Reproduction script\n\nHere is a simple reproduction, although it takes a while due to ray's docker image being very large.\r\n\r\n```bash\r\nray exec example-full.yaml --start --stop 'echo \"hello world\"'\r\n```\n\n### Issue Severity\n\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "richardliaw",
        "body": "That's very odd... `--stop` should just be an appended statement at the very end of the command. Thanks for reporting. The team will take a look after we're back from the holidays!"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31292,
    "title": "[Tune] Avoid insufficient resources warning if cluster is autoscaling",
    "author": "bveeramani",
    "state": "open",
    "created_at": "2022-12-22T03:56:00Z",
    "updated_at": "2025-06-17T00:14:34Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "UX",
      "pending-cleanup"
    ],
    "body": "### Description\n\nDon't raise warnings like:\r\n\r\n```\r\n2022-12-15 21:06:11,922 WARNING insufficient_resources_manager.py:133 -- Ignore this\r\nmessage if the cluster is autoscaling. No trial is running and no new trial has been\r\nstarted within at least the last 60.0 seconds. This could be due to the cluster not\r\nhaving enough resources available to start the next trial. Stop the tuning job and\r\nadjust the resources requested per trial (possibly via `resources_per_trial` or via\r\n`num_workers` for rllib) and/or add more resources to your Ray runtime.\r\n```\r\n\r\nIf the cluster is autoscaling. \n\n### Use case\n\nBetter UX.\r\n\r\nEvery time I train my model, I get bombarded with warning messages while I wait for my worker nodes to finish setting up. It's mildly annoying.",
    "comments": [
      {
        "user": "iojc",
        "body": "you can set log level to avoid warning\r\n\r\nimport ray.tune.execution.insufficient_resources_manager\r\nimport logging\r\n\r\nray.tune.execution.insufficient_resources_manager.logger.warning(\"warning before setLevel\")\r\nray.tune.execution.insufficient_resources_manager.logger.setLevel(logging.CRITICAL)\r\nray.tune.execution.insufficient_resources_manager.logger.warning(\"warning after setLevel\")\r\n\r\nterminal:\r\nwarning before setLevel"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31288,
    "title": "No worker logs in the dashboard after recreating the K8S Ray pods",
    "author": "YQ-Wang",
    "state": "open",
    "created_at": "2022-12-22T01:34:46Z",
    "updated_at": "2025-06-17T00:14:32Z",
    "labels": [
      "bug",
      "P2",
      "core",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nAfter deleted the pods and restarted the pods using the preserved data from Redis, the Ray job worker logs no longer get exported to the dashboard. \r\nIt returns the following error when using the get_job_logs API:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 64, in <module>\r\n    logs = client.get_job_logs(job_id)\r\n  File \"/Users/yiqingwang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ray/dashboard/modules/job/sdk.py\", line 341, in get_job_logs\r\n    self._raise_error(r)\r\n  File \"/Users/yiqingwang/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ray/dashboard/modules/dashboard_sdk.py\", line 260, in _raise_error\r\n    f\"Request failed with status code {r.status_code}: {r.text}.\"\r\nRuntimeError: Request failed with status code 500: Traceback (most recent call last):\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/aiohttp/connector.py\", line 986, in _wrap_create_connection\r\n    return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa\r\n  File \"/home/ray/anaconda3/lib/python3.7/asyncio/base_events.py\", line 962, in create_connection\r\n    raise exceptions[0]\r\n  File \"/home/ray/anaconda3/lib/python3.7/asyncio/base_events.py\", line 949, in create_connection\r\n    await self.sock_connect(sock, address)\r\n  File \"/home/ray/anaconda3/lib/python3.7/asyncio/selector_events.py\", line 473, in sock_connect\r\n    return await fut\r\n  File \"/home/ray/anaconda3/lib/python3.7/asyncio/selector_events.py\", line 503, in _sock_connect_cb\r\n    raise OSError(err, f'Connect call failed {address}')\r\nConnectionRefusedError: [Errno 111] Connect call failed ('10.0.35.85', 52365)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/job_head.py\", line 464, in get_job_logs\r\n    resp = await job_agent_client.get_job_logs_internal(job.submission_id)\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/job_head.py\", line 113, in get_job_logs_internal\r\n    f\"{self._agent_address}/api/job_agent/jobs/{job_id}/logs\"\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/aiohttp/client.py\", line 1138, in __aenter__\r\n    self._resp = await self._coro\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/aiohttp/client.py\", line 536, in _request\r\n    req, traces=traces, timeout=real_timeout\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/aiohttp/connector.py\", line 542, in connect\r\n    proto = await self._create_connection(req, traces, timeout)\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/aiohttp/connector.py\", line 907, in _create_connection\r\n    _, proto = await self._create_direct_connection(req, traces, timeout)\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/aiohttp/connector.py\", line 1206, in _create_direct_connection\r\n    raise last_exc\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/aiohttp/connector.py\", line 1187, in _create_direct_connection\r\n    client_error=client_error,\r\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/aiohttp/connector.py\", line 992, in _wrap_create_connection\r\n    raise client_error(req.connection_key, exc) from exc\r\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 10.0.35.85:52365 ssl:default [Connect call failed ('10.0.35.85', 52365)]\r\n```\r\nThe worker dashboard agent logs contain this error\r\n```\r\n182022-12-21 13:22:26,046\tERROR event_agent.py:85 -- Report event failed, reconnect to the dashboard.1119Traceback (most recent call last):1120  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/event/event_agent.py\", line 81, in report_events1121    await self._stub.ReportEvents(request)1122  File \"/home/ray/anaconda3/lib/python3.7/site-packages/grpc/aio/_call.py\", line 286, in __await__1123    self._cython_call._status)1124grpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:1125\tstatus = StatusCode.UNAVAILABLE1126\tdetails = \"failed to connect to all addresses\"1127\tdebug_error_string = \"{\"created\":\"@1671657746.044145704\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":5390,\"referenced_errors\":[{\"created\":\"@1671657746.044142142\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":397,\"grpc_status\":14}]}\"1128>\r\n\r\n```\n\n### Versions / Dependencies\n\nWe set up the static Ray cluster with an external Redis in K8S.\r\nRay 2.2.0\r\nPython 3.7\n\n### Reproduction script\n\nIf you delete the Ray pods (both head and worker) and recreate Ray pods using the preserved metadata from Redis in the K8S, you will find the newly submitted jobs have no logs from the worker.\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31264,
    "title": "[core] Please improve warning message for ip mismatch",
    "author": "richardliaw",
    "state": "open",
    "created_at": "2022-12-21T08:48:13Z",
    "updated_at": "2025-06-17T00:14:30Z",
    "labels": [
      "bug",
      "P2",
      "fix-error-msg",
      "core",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nI'm getting this error, but this error is not easy to understand and it is easy to miss. \r\n\r\n```\r\n[2022-12-20 03:13:08,669 I 25449 25449] global_state_accessor.cc:357: This node has an IP address of 11.22.33.44, while we can not find the matched Raylet address. This maybe come from when you connect the Ray cluster with a different IP address or connect a container.\r\n```\r\n\r\nCan we improve it in the following way:\r\n\r\n1. What are the detected IP addresses on the node?\r\n2. What are the detected Raylet addresses, if there exists any?\r\n3. What can the user do to reconfigure Ray to start properly?\r\n4. [optional] Should we fail to start Ray if this happens?\r\n\r\nAn example idea to improve this error message.\r\n\r\n```\r\n[2022-12-20 03:13:08,669 I 25449 25449] global_state_accessor.cc:357: WARNING: Ray is configured to use the ip address 11.22.33.44 for this node but cannot find a corresponding Raylet address (detected raylet addresses of {11.22.33.55}), which may cause undefined behavior. This node has the following ip addresses: {11.22.33.44, 11.22.33.45, 11.22.33.55}. Try setting the Ray IP address by using `ray start --node-ip-address=CHOSEN_IP_ADDRESS ...`.\r\n```\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\nrun `ray start --address=...` on a node with multiple ip addresses\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31259,
    "title": "[Ray Collective]   Ray Collective AllGather is Completely Broken",
    "author": "functionstackx",
    "state": "open",
    "created_at": "2022-12-21T02:40:50Z",
    "updated_at": "2025-06-17T00:14:28Z",
    "labels": [
      "bug",
      "P2",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nRay Collective AllGather is Broken :( @jiaodong @zhisbug\r\n\r\nOutput:\r\n```\r\n(Worker pid=2428236) [1. 1. 1. 1.]\r\n(Worker pid=2428240) [5. 5. 5. 5.]\r\n(Worker pid=2428236) [array([5., 5., 5., 5.], dtype=float32), array([5., 5., 5., 5.], dtype=float32)]\r\n(Worker pid=2428240) [array([5., 5., 5., 5.], dtype=float32), array([5., 5., 5., 5.], dtype=float32)]\r\n```\r\n\r\nExpected Behaviour\r\n```\r\n(Worker pid=2428236) [1. 1. 1. 1.]\r\n(Worker pid=2428240) [5. 5. 5. 5.]\r\n(Worker pid=2428236) [array([1., 1., 1., 1.], dtype=float32), array([5., 5., 5., 5.], dtype=float32)]\r\n(Worker pid=2428240) [array([1., 1., 1., 1.], dtype=float32), array([5., 5., 5., 5.], dtype=float32)]\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/47992694/208807214-ed078647-6332-4504-86c4-9f7f9813e1eb.png)\r\n\n\n### Versions / Dependencies\n\npython=3.8.10\r\nlinux ubuntu\r\nnvidia gpus\r\ncupy-cuda11x                 11.4.0\r\nray=2.2.0\n\n### Reproduction script\n\n```python\r\nimport ray\r\nimport ray.util.collective as collective\r\n\r\nimport cupy as cp\r\n\r\n\r\n@ray.remote(num_gpus=1)\r\nclass Worker:\r\n   def __init__(self):\r\n       self.buffer = [cp.zeros((4,), dtype=cp.float32)] * 2\r\n\r\n   def setup(self, world_size, rank):\r\n        collective.init_collective_group(world_size, rank, \"nccl\", \"default\")\r\n        \r\n        if rank == 0:\r\n            self.current_shard = cp.ones((4, ), dtype=cp.float32)\r\n        else:\r\n            self.current_shard = cp.ones((4,), dtype=cp.float32) * 5\r\n\r\n        \r\n        print(self.current_shard)\r\n   def compute(self):\r\n        collective.allgather(self.buffer, self.current_shard)\r\n        print(self.buffer)\r\n\r\n   def destroy(self):\r\n       collective.destroy_group()\r\n\r\n# imperative\r\nnum_workers = 2\r\nworkers = []\r\ninit_rets = []\r\nfor i in range(num_workers):\r\n   w = Worker.remote()\r\n   workers.append(w)\r\n   init_rets.append(w.setup.remote(num_workers, i))\r\ns = ray.get(init_rets)\r\nresults = ray.get([w.compute.remote() for w in workers])\r\n\r\n```\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "zhe-thoughts",
        "body": "Thanks for reporting @OrenLeung . Marking this as P2 as the Collective feature is an Alpha feature. Actually we should clarify that on the docs page. I will file a PR for that"
      },
      {
        "user": "richardliaw",
        "body": "@zhe-thoughts did you file a PR for this?"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31158,
    "title": "[core][state] Refactor use of bounded LRU/FIFO buffer/map used in task backend",
    "author": "rickyyx",
    "state": "open",
    "created_at": "2022-12-16T18:30:16Z",
    "updated_at": "2025-06-17T00:14:26Z",
    "labels": [
      "enhancement",
      "P2",
      "observability",
      "Ray-2.4",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nWe have seen usage of data structures while working on the task backend where:\r\n1. Has a bounded number of max allowed elements stored \r\n2. Eviction with a LRU behavior when max number exceeds (namely, the least recently inserted element should be evicted). \r\n\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\nNA\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31087,
    "title": "[core] Ray resources should be case-insensitive",
    "author": "krfricke",
    "state": "open",
    "created_at": "2022-12-14T00:57:08Z",
    "updated_at": "2025-06-17T00:14:24Z",
    "labels": [
      "enhancement",
      "P2",
      "core-placement-group",
      "core",
      "pending-cleanup"
    ],
    "body": "### Description\n\nCurrently a resource `CPU` is different from a resource `cpu`.\r\n\r\nThis can be potentially confusing. I can't think of use cases where we actually would want to have separate resources with the same name but in different capitalization. \r\n\r\nI'd suggest we don't distinguish between lower and upper case in resource requests (e.g. placement groups or custom resources).\r\n\r\n\r\n```\r\nimport ray\r\n\r\n\r\nray.init()\r\n\r\npg = ray.util.placement_group([{\"cpu\": 1}])\r\n\r\nray.get(pg.ready())\r\n\r\n# (scheduler +16s) Error: No available node types can fulfill resource request {'cpu': 1.0}. Add suitable node types to this cluster to resolve this issue.\r\n```\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 31041,
    "title": "[RayCluster] ",
    "author": "PeterPirog",
    "state": "open",
    "created_at": "2022-12-12T22:15:22Z",
    "updated_at": "2025-06-17T00:14:22Z",
    "labels": [
      "question",
      "P2",
      "docs",
      "pending-cleanup"
    ],
    "body": "### Description\n\nI wonder that some post (topic, examples, maybe step by step video tutorials) about usage ray in docker can be useful. Some people like me, have came to the ray framework not from devops area but from computational science area  (MATLAB, MATHCAD, numpy, scipy etc.), so docker useage, cluster configuration and all these stuff is not obvious, sometimes very unclear and mysterious :). By the other hand for the people with scientific bacground it’s much easier to understand algorithms configurations, feature engineering, probability issues etc.\r\n\r\n1. Some step by step tutorials how tu use ray (rllib) in docker may be very useful. Of course I don’t mean to place docker course here, I suppose some bascic knowledge of using docker.\r\n2.  I suggest to add line\"COPY Dockerfile /\" in Dockerfiles to better understan how the images is created and how to modify its.\r\n3. Ray-ml tags aren’t precise enouht https://hub.docker.com/r/rayproject/ray-ml/tags , many times I need to pull image and run command “pip list” to show what excatly is inside th image.\r\n4. Maybe docker issues need separate forum section  in https://discuss.ray.io/?\r\n\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 30891,
    "title": "[Serve] gRPCis should not allow route_prefix set ",
    "author": "sihanwang41",
    "state": "open",
    "created_at": "2022-12-05T17:58:32Z",
    "updated_at": "2025-06-17T00:14:20Z",
    "labels": [
      "bug",
      "P2",
      "serve",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\n\nWhen user set route_prefix for gRPC service, it should explicitly give error.\n\n### Versions / Dependencies\n\nmaster\n\n### Reproduction script\n\nhttps://discuss.ray.io/t/high-ray-serve-run-grpc-services-in-one-cluster/8531\n\n### Issue Severity\n\nNone",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 30852,
    "title": "[General] Setup a \"code walkthrough\" meetup or tutorial",
    "author": "zhe-thoughts",
    "state": "open",
    "created_at": "2022-12-02T05:23:53Z",
    "updated_at": "2025-06-17T00:14:18Z",
    "labels": [
      "enhancement",
      "P2",
      "pending-cleanup"
    ],
    "body": "### Description\n\nFor someone to start contributing to Ray, one major obstacle is the high complexity of Ray's code base. This is true for most components, and especially for Ray Core. \r\n\r\nOne solution to help with this situation is to setup \"code walkthrough\" meetups (or other formats such as recorded videos). Creating this issue to track the work.\r\n\r\nThis is a concrete case to illustrate the need: https://discuss.ray.io/t/what-use-for-data-transferring/7678/7\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 30843,
    "title": "[RFC][core] Option to avoid scheduling tasks to nodes with disk full",
    "author": "stephanie-wang",
    "state": "open",
    "created_at": "2022-12-02T00:11:23Z",
    "updated_at": "2025-06-17T00:14:16Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "core-scheduler",
      "pending-cleanup"
    ],
    "body": "### Description\n\nFor load-balancing purposes, it is often desirable to schedule a task onto a node with less disk space. A user might also require a certain amount of disk space to run a task, and ideally if it fails on one node, have it be automatically retried on another node that does have enough disk space.\r\n\r\nWe can make two possible enhancements:\r\n\r\n- [ ] Consider disk utilization in the scheduling policy. We could alternatively consider just Ray spilled objects.\r\n- [ ] Support arbitrary user-defined scheduling constraints, like \"only schedule this task on a node with X disk space\"\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "stephanie-wang",
        "body": "cc @jjyao"
      },
      {
        "user": "pedropgusmao",
        "body": "I'm very interested in this. Spillage is becoming a problem when it fills the disks.\r\nMaybe a wrapper that returns information, whether it is spilling, would be great. "
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 30832,
    "title": "[core] Enable greater control over log verbosity",
    "author": "stephanie-wang",
    "state": "open",
    "created_at": "2022-12-01T19:58:27Z",
    "updated_at": "2025-06-17T00:14:14Z",
    "labels": [
      "enhancement",
      "P2",
      "core",
      "observability",
      "pending-cleanup"
    ],
    "body": "### Description\n\nRay tries to suppress error logs by only logging periodically. However, this sometimes still leads to too much logging. It would be great to add more options for logging verbosity. Some options:\r\n- Adding a global config for how often to log errors\r\n- Adding a flag to suppress all but the first log to the driver output. Rest could go only to log files\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 30805,
    "title": "[Core] ux issues of ray state cli for tasks",
    "author": "scottsun94",
    "state": "open",
    "created_at": "2022-11-30T22:55:00Z",
    "updated_at": "2025-06-17T00:14:12Z",
    "labels": [
      "P2",
      "dashboard",
      "observability",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nSeveral issues:\r\n- [x] `ray list tasks` doesn't show all the tasks (discrepancy between it and the dashboard). Finished tasks get gc'ed\r\n- [ ] the ordering of the results of `ray list tasks --detail` is weird. Task Id should be the first.\r\n\r\n![Screen Shot 2022-11-30 at 2 47 03 PM](https://user-images.githubusercontent.com/9677264/204925168-00403b8f-de76-442e-ad67-9a7b4be43594.png)\r\n\r\n\r\n### Versions / Dependencies\r\n\r\nray nightly Nov 30\r\n\r\n### Reproduction script\r\n\r\nSee above\r\n\r\n### Issue Severity\r\n\r\nLow: It annoys or frustrates me.",
    "comments": [
      {
        "user": "rkooo567",
        "body": "1 is P1, and it will be fixed by the end of the year. \r\n\r\n2 is P2 for now, but fix should be easy"
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 30802,
    "title": "[Tune] ability to specify search algorithm when using tune.run_experiments()",
    "author": "xblaauw",
    "state": "open",
    "created_at": "2022-11-30T22:29:22Z",
    "updated_at": "2025-06-17T00:14:10Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "pending-cleanup"
    ],
    "body": "I have 200 jobs that use the same objective function and parameter space but a different target variable. I can search the space using any of the tuning algorithms for any one job just fine:\r\n\r\n```\r\nimport ray\r\nfrom ray import tune\r\nfrom ray.tune import Tuner, TuneConfig\r\nfrom ray.tune.search.optuna import OptunaSearch\r\n\r\n\r\nray.init()\r\n\r\ndef objective(config):\r\n    ground_truth = [1,2,3,4]\r\n    yhat = [i*config['factor'] + config['constant'] for i in range(4)]\r\n    abs_err = [abs(gt - yh) for gt, yh in zip(ground_truth, yhat)]\r\n    mae = sum(abs_err)/len(abs_err)\r\n\r\n    tune.report(mean_accuracy = mae)\r\n\r\nconfig = {\r\n    'factor': tune.quniform(0,3,1),\r\n    'constant': tune.quniform(0,3,1)\r\n}\r\n\r\nalgo = OptunaSearch()\r\n\r\ntuner = tune.Tuner(\r\n    objective,\r\n    tune_config=TuneConfig(\r\n        metric=\"mean_accuracy\",\r\n        mode=\"min\",\r\n        search_alg=algo,\r\n        num_samples=100\r\n    ),\r\n    param_space=config\r\n)\r\nresults = tuner.fit()\r\n```\r\n\r\nI would expect to be able to use something like this for queuing multiple similar jobs:\r\n\r\n```\r\nimport ray\r\nfrom ray import tune\r\n\r\n\r\nray.init()\r\n\r\ndef objective(config):\r\n    ground_truth = [1,2,3,4]\r\n    yhat = [i*config['factor'] + config['constant'] for i in range(4)]\r\n    abs_err = [abs(gt - yh) for gt, yh in zip(ground_truth, yhat)]\r\n    mae = sum(abs_err)/len(abs_err)\r\n\r\n    tune.report(mean_accuracy = mae)\r\n\r\nconfig = {\r\n    'factor': tune.quniform(0,3,1),\r\n    'constant': tune.quniform(0,3,1)\r\n}\r\n\r\nexperiments = []\r\nfor i in range(3):\r\n    experiment_spec = tune.Experiment(\r\n        name=f'{i}',\r\n        run=objective,\r\n        stop={\"mean_accuracy\": 0},\r\n        config=config,\r\n        num_samples=10\r\n    )\r\n    experiments.append(experiment_spec)\r\n\r\nout = tune.run_experiments(experiments)\r\n```\r\n\r\nWhen I run this I even get the notice: \"Running with multiple concurrent experiments. All experiments will be using the same SearchAlgorithm.\". But to my knowledge there is no way to specify _which_ search algorithm is used.\r\n\r\nAdditionally, these experiments appear to be part of one large optimization out is a list of 30 objective objects. The parameter values chosen are from a uniform distribution, without the q. However all 30 values fall in the specified range.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 30800,
    "title": "[RLlib] Deprecate the RLlib spaces that are duplications of gym spaces.",
    "author": "avnishn",
    "state": "open",
    "created_at": "2022-11-30T20:11:25Z",
    "updated_at": "2025-06-17T00:14:08Z",
    "labels": [
      "enhancement",
      "P2",
      "rllib",
      "pending-cleanup"
    ],
    "body": "### Description\n\nRLlib supports repeated which is a duplication of gym sequences. We should deprecate this API in favor of gym sequences.\r\n\r\nThere are likely other spaces that we should dedup in favor of the gym counterparts.\r\n\r\n\n\n### Use case\n\n_No response_",
    "comments": [
      {
        "user": "sven1977",
        "body": "Great point @avnishn , there are also the Simplex and FlexDict spaces (utils.spaces/...)."
      },
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 30795,
    "title": "[Tune] Guard against users overriding internal `Trainable` methods",
    "author": "Yard1",
    "state": "open",
    "created_at": "2022-11-30T18:20:25Z",
    "updated_at": "2025-06-17T00:14:05Z",
    "labels": [
      "enhancement",
      "tune",
      "P2",
      "ray-team-created",
      "pending-cleanup"
    ],
    "body": "### Description\n\nWe should detect and warn user if internal Trainable methods such as `train`, `save` and `load` are overriden.\n\n### Use case\n\nSee https://github.com/ray-project/ray/issues/30786 for an example of confusion.",
    "comments": [
      {
        "user": "cszhu",
        "body": "This P2 issue has seen no activity in the past 2 years. It will be closed in 2 weeks as part of ongoing cleanup efforts.\n\nPlease comment and remove the `pending-cleanup` label if you believe this issue should remain open.\n\nThanks for contributing to Ray!"
      }
    ]
  },
  {
    "issue_number": 30780,
    "title": "Ray Cluster Resources Issue",
    "author": "CansuCandan",
    "state": "open",
    "created_at": "2022-11-30T06:43:47Z",
    "updated_at": "2025-06-17T00:14:03Z",
    "labels": [
      "bug",
      "P2",
      "@author-action-required",
      "core",
      "core-clusters",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nHi,\r\n\r\nI have some issues.\r\nI don’t know this is a bug or not. Please notify me about this issue.\r\nI am setting up cluster. Firstly, I set Centos machine as head node, worker node1 Ubuntu, worker node2 also ubuntu. But when deployed code which simple consume resource, Centos machine not consume of resource. Also when I checked as ray status, Centos Cpu resource not adding to Head Node.\r\n\r\nThis scenario like also same between centos machines. (But Centos CPU resource not adding to cluster)\r\n\r\nI could just connected CPU resources between ubuntu 18.04 Linux distribution.\r\n\r\nRay version = 2.1.0\r\nPython version=3.8.8\r\n\r\nRay and Python versions same in the all machines.\r\n\r\nWhy this happening?\r\n\r\nPlease let me know.\r\n\r\nMachines:\r\n\r\n1.Cluster structure ( This worked)\r\nLinux Machines:\r\nx.x.x.x => Head Node => Ubuntu 18.04.4 LTS  \r\nx.x.x.x => Worker Node => Ubuntu 18.04.4 LTS\r\nx.x.x.x => Worker Node => Ubuntu 18.04.4 LTS\r\n\r\n2.Cluster structure(On this scenario, Ubuntu CPU resource adding to cluster. But Centos CPU resource not adding to cluster)\r\nLinux Machines:\r\nx.x.x.x => Head Node => Centos\r\nx.x.x.x => Worker Node => Ubuntu\r\nx.x.x.x => Worker Node => Ubuntu\r\n\r\n3.Cluster structure (On this scenario, only the head node is added to the cluster. CPU of other machines are not adding to the cluster )\r\nLinux Machines:\r\nx.x.x.x => Head Node => CentOS-7\r\nx.x.x.x => Worker Node => CentOS-7\r\nx.x.x.x => Worker Node => CentOS-7\r\n\r\n### Versions / Dependencies\r\n\r\n1.Cluster structure ( This worked)\r\nLinux Machines:\r\nx.x.x.x => Head Node => Ubuntu 18.04.4 LTS\r\nx.x.x.x => Worker Node => Ubuntu 18.04.4 LTS\r\nx.x.x.x => Worker Node => Ubuntu 18.04.4 LTS\r\nPython Version = 3.8.8\r\nRay version = 2.1.0\r\n\r\n2.Cluster structure(On this scenario, Ubuntu CPU resource adding to cluster. But Centos CPU resource not adding to cluster)\r\nLinux Machines:\r\nx.x.x.x => Head Node => Centos\r\nx.x.x.x => Worker Node => Ubuntu\r\nx.x.x.x => Worker Node => Ubuntu\r\nPython Version = 3.8.8\r\nRay version = 2.1.0\r\n\r\n3.Cluster structure (On this scenario, only the head node is added to the cluster. CPU of other machines are not adding to the cluster )\r\nLinux Machines:\r\nx.x.x.x => Head Node => CentOS-7\r\nx.x.x.x => Worker Node => CentOS-7\r\nx.x.x.x => Worker Node => CentOS-7\r\nPython Version = 3.8.8\r\nRay version = 2.1.0\r\n\r\n### Reproduction script\r\n\r\nPython code which simple consume resource:\r\n\r\nimport ray\r\nimport time\r\nimport numpy as np\r\n\r\n\r\nray.init(address='ray://xxxx:10001', runtime_env={'pip':['numpy']}) # xxxx IP is Head Node IP\r\n\r\nprint(ray.cluster_resources())\r\n\r\n@ray.remote\r\ndef ray_task(id):\r\n    np.random.randn(10_000, 10_000) * np.random.randn(10_000, 10_000)\r\n    print(f\"done: {id}\")\r\n\r\nstart = time.time()\r\nray.get([ray_task.remote(i+1) for i in range(164)])\r\nprint(f\"elapsed time: {time.time()-start}\")\r\nray.shutdown()\r\n\r\n### Scripts\r\nFor head node: ray start --head  --port=6379 --object-manager-port=8076 --include-dashboard=true --dashboard-host=0.0.0.0 --dashboard-port=9090\r\nFor Worker Nodes:  ray start --address='x.x.x.x:6379'   # xxxx IP is Head Node IP. I copied from \"To connect to this Ray runtime from another node, run\" step.\r\n\r\nAfter run \"python ray_test.py\" I run the \"top\" command in the Centos machines, I can't see ray:task. But I can see in the Ubuntu machines.\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "rkooo567",
        "body": "Did you make sure all the ports are open based on https://docs.ray.io/en/master/ray-core/configure.html#ports-configurations? "
      },
      {
        "user": "rkooo567",
        "body": "Normally this happens because of connectivity issues."
      },
      {
        "user": "hora-anyscale",
        "body": "@CansuCandan - please confirm if your system has the appropriate connectivity?"
      }
    ]
  },
  {
    "issue_number": 30731,
    "title": "[Core] Worker leak ",
    "author": "tianyicui-tsy",
    "state": "open",
    "created_at": "2022-11-29T08:50:20Z",
    "updated_at": "2025-06-17T00:14:01Z",
    "labels": [
      "bug",
      "P2",
      "usability",
      "core",
      "core-worker",
      "core-correctness",
      "pending-cleanup"
    ],
    "body": "### What happened + What you expected to happen\r\n\r\nWhen task `work` depends on `fail` (which immediately raise an exception) and `inf_loop` (which runs infinite loop), canceling `work` doesn't recursively cancel `inf_loop`. Result is a worker with no way to cancel or kill. What's more, **shutdown the ray job doesn't seem to stop the `inf_loop` worker**, as a result, there's no way to stop the `inf_loop` worker (in practice it should be an arbitrary long-running task)  and reclaim its resources. So I believe it's accurate to describe its state as leaked.\r\n\r\n### Versions / Dependencies\r\n\r\nI tried both Ray 2.1.0 and nightly\r\nPython 3.10.6\r\nUbuntu 22.04\r\n\r\n### Reproduction script\r\n\r\nIf I run a ray cluster locally with `ray start --head`, and run the script below with `RAY_ADDRESS=auto python test.py`, I see that even after the script finishes (i.e. after the job shutdown), the `inf_loop` worker is still running and there is one less available CPU according to `ray.available_resources` from another job.\r\n\r\n```\r\nimport ray\r\nimport time\r\n\r\nray.init()\r\n\r\ndef cpu():\r\n    return ray.available_resources()['CPU']\r\n\r\n@ray.remote\r\ndef inf_loop():\r\n    while True:\r\n        time.sleep(0.1)\r\n\r\n@ray.remote\r\ndef fail():\r\n    raise ValueError(\"fail\")\r\n\r\n@ray.remote\r\ndef work(*, include_fail):\r\n    if include_fail:\r\n        ray.get([inf_loop.remote(), fail.remote()])\r\n    else:\r\n        ray.get([inf_loop.remote()])\r\n\r\nn_cpu = cpu()\r\nassert n_cpu > 1\r\n\r\n# when include_fail=False, cancel the work task will recursively also cancel inf_loop as expected\r\no = work.remote(include_fail=False)\r\ntime.sleep(1)\r\nassert cpu() == n_cpu - 1\r\nray.cancel(o, force=True)\r\ntime.sleep(1)\r\nassert cpu() == n_cpu\r\n\r\n# when include_fail=True, cancel the work task doesn't cancel inf_loop\r\no = work.remote(include_fail=True)\r\ntime.sleep(1)\r\nray.cancel(o, force=True)\r\ntime.sleep(1)\r\nassert cpu() == n_cpu # this fails\r\n\r\n# even if we shutdown this job, it appears the worker that runs inf_loop is still running. So looks\r\n# like a worker is leaked.\r\n```\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments": [
      {
        "user": "tianyicui-tsy",
        "body": "Just saw this was pushed back to Ray 2.3. Is it possible to shed some light on the rationale behind the decision?\r\n\r\nI'm not sure how often people run into this issue but I ran into it the first time I tried to cancel some tasks in Ray. And if my description of the bug is accurate, I'd argue the severity of the issue's result can be quite high: worker that aren't released even after the ray job stops.\r\n\r\nThanks and appreciate the quick triage of my issue!"
      },
      {
        "user": "rkooo567",
        "body": "@jjyao will do some initial investigation "
      },
      {
        "user": "vitsai",
        "body": "Hi @tianyicui-tsy, we are working on several fixes for this. In the meantime, you can work around this by adding a try-except block in the `fail()` task that returns the exception instead of raising it."
      }
    ]
  }
]