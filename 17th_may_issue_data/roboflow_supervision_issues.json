[
  {
    "issue_number": 1855,
    "title": "Error on polygon zones",
    "author": "yyakupoglu",
    "state": "open",
    "created_at": "2025-06-02T17:46:19Z",
    "updated_at": "2025-06-02T17:46:19Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nAs seen in the images. When the track bounding box (person) is in the zone and the next frame person is moved behind the wall (disappear) it keeps the show number 1. I guess a reset function should be implemented?\n![Image](https://github.com/user-attachments/assets/12560028-76fe-43ab-a151-35e93a5d096d)\n![Image](https://github.com/user-attachments/assets/4466d0b9-18be-4226-97fb-8c07c73703b3)\n\n### Environment\n\n-Supervision 0.25.1\n-Ubuntu 22.04\n-Python 3.10\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": []
  },
  {
    "issue_number": 1619,
    "title": "Bug found in ConfusionMatrix.from_detections",
    "author": "chiggins2024",
    "state": "open",
    "created_at": "2024-10-24T15:02:23Z",
    "updated_at": "2025-05-27T14:36:33Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nIssue found in code when producing a confusion matrix for object detection. It seems like the FN was being added incorrectly to the matrix. Here is the code that was problematic for me. When removing the else condition, I was getting the correct TP value. It seems that num_classes, ends up being at the same position detection_classes[matched_detection_idx[j]]\r\n```\r\n        ```\r\nfor i, true_class_value in enumerate(true_classes):\r\n            j = matched_true_idx == i\r\n            print('sum(j)', sum(j))\r\n            if matches.shape[0] > 0 and sum(j) == 1:\r\n                result_matrix[\r\n                    true_class_value, detection_classes[matched_detection_idx[j]]\r\n                ] += 1  # TP\r\n            else:\r\n                result_matrix[true_class_value, num_classes] += 1  # FN\r\n```\r\n```\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @chiggins2024 üëã \r\n\r\nThank you for the report. We'll check it as soon as we can!\r\nAs we're transitioning away from the legacy MeanAveragePrecision and ConfusionMatrix, most likely the fix will come as a new ConfusionMatrix version."
      },
      {
        "user": "chiggins2024",
        "body": "Hi thanks! do you know when we can anticipate this new ConfusionMatrix\r\nversion to the ready ? \r\n\r\nOn Fri, Nov 1, 2024 at 9:20‚ÄØAM LinasKo ***@***.***> wrote:\r\n\r\n> Hi @chiggins2024 <https://github.com/chiggins2024> üëã\r\n>\r\n> Thank you for the report. We'll check it as soon as we can!\r\n> As we're transitioning away from the legacy MeanAveragePrecision and\r\n> ConfusionMatrix, most likely the fix will come as a new ConfusionMatrix\r\n> version.\r\n>\r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/roboflow/supervision/issues/1619#issuecomment-2451785592>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BMMRA527N4OACGT7KVTXYW3Z6NWZVAVCNFSM6AAAAABQRJYZESVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDINJRG44DKNJZGI>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "Buckler89",
        "body": "Any update on that? When new Confusion matrix will be ready?"
      }
    ]
  },
  {
    "issue_number": 1851,
    "title": "Markdown doc rendering",
    "author": "alexespencer",
    "state": "open",
    "created_at": "2025-05-25T08:35:03Z",
    "updated_at": "2025-05-25T08:35:03Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nWhen viewing the docs in Github the syntax for the examples/how to guides is not valid markdown and code blocks do not render properly. When running mkdocs, it appears they do render properly. Is this the desired behaviour? It would be a nice experience for people viewing the project externally to not have to create an environment to view docs? \n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1850,
    "title": "GStreamer integration",
    "author": "VincentDuf",
    "state": "open",
    "created_at": "2025-05-23T15:48:06Z",
    "updated_at": "2025-05-23T15:48:06Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi everyone ! \n\nI'm currently working on a product developed by my current company which include the use of gstreamer pipeline to capture video flow, apply AI model (person detection for instance) and get the flow out with incrusted bounding box. \n\nI find supervision library super interesting, but it seems that it uses only OpenCV capture feature, do you think it would be possible to integrate with gstreamer or maybe is there something already available ?\n\nThanks a lot !\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1849,
    "title": "Opcatiy in Color Class",
    "author": "Oilgrim",
    "state": "open",
    "created_at": "2025-05-21T12:34:24Z",
    "updated_at": "2025-05-21T12:34:24Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nHello,\n\ni'd like to have the rgba format supported in the sv.Color Class to adjust the opacity for every Annotator ( Box and LabelAnnotator in my Case. \n\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": []
  },
  {
    "issue_number": 1847,
    "title": "The CCTV system with multiple cameras is tracking .",
    "author": "fti-sbargule",
    "state": "open",
    "created_at": "2025-05-16T09:05:24Z",
    "updated_at": "2025-05-16T09:05:24Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI provided CCTV footage from multiple cameras, and the system generates a tracking ID for each individual automatically.\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1838,
    "title": "line_zone does not update in_count and out_count?",
    "author": "KennyTC",
    "state": "open",
    "created_at": "2025-05-08T21:32:14Z",
    "updated_at": "2025-05-13T08:14:40Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI follow the example  [https://supervision.roboflow.com/latest/notebooks/count-objects-crossing-the-line/](https://supervision.roboflow.com/latest/notebooks/count-objects-crossing-the-line/) to  want to count number of car. My version is `0.24.0`\nHere is my code.\n\n```python\nfrom pathlib import Path\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nimport logging\nfrom datetime import datetime\nimport argparse\nimport os\nfrom typing import Union, Dict\nimport supervision as sv\n\nclass YOLOCounter:\n    \"\"\"Counts objects using YOLOv8 as they cross a line.\"\"\"\n    \n    def __init__(self, model_path: str, output_dir: str):\n        \"\"\"\n        Initialize counter with model and output paths.\n        \n        Args:\n            model_path: Path to model file or name of pretrained model (e.g., 'yolov8n')\n            output_dir: Directory to save counting results\n        \"\"\"\n        try:\n            # self.model = YOLO(model_path)\n            self.model = YOLO(\"yolov8n.pt\")\n            logging.info(f\"Successfully loaded model: {model_path}\")\n        except Exception as e:\n            raise ValueError(f\"Failed to load model from {model_path}: {str(e)}\")\n            \n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        self.bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=1)\n        self.label_annotator = sv.LabelAnnotator(text_thickness=1, text_scale=0.5)\n        self.trace_annotator = sv.TraceAnnotator()\n        self.line_zone = sv.LineZone(start=sv.Point(0, 10), end=sv.Point(1296, 10))\n        self.line_zone_annotator = sv.LineZoneAnnotator(\n            thickness=2, \n            text_thickness=1, \n            text_scale=1,\n            text_orient_to_line=True,\n            text_offset=0,\n            display_out_count = False,\n            display_in_count = True,\n            # custom_in_text=\"Number of objects: \",\n\n\n        )\n\n        self.byte_tracker = sv.ByteTrack()\n        \n        # Setup logging\n        self._setup_logging()\n    \n    def _setup_logging(self):\n        \"\"\"Setup logging configuration.\"\"\"\n        log_dir = self.output_dir / 'logs'\n        log_dir.mkdir(exist_ok=True)\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        \n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(message)s',\n            handlers=[\n                logging.FileHandler(log_dir / f'counting_log_{timestamp}.txt'),\n                logging.StreamHandler()\n            ]\n        )\n    \n    def count_objects_in_video(self, \n                   video_path: Union[str, Path], \n                   conf: float = 0.25,\n                   line_position: float = 0.5) -> Dict:\n        \"\"\"\n        Count objects crossing a line in video using YOLO.\n        \n        Args:\n            video_path: Path to video file\n            conf: Confidence threshold\n            line_position: Position of the counting line (0-1, relative to frame height)\n        \"\"\"\n        video_path = Path(video_path)\n        if not video_path.exists():\n            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n        \n        video_info = sv.VideoInfo.from_video_path(video_path)\n        print(\"video_info\", video_info, video_info.resolution_wh)\n        sv.process_video(\n            source_path = video_path,\n            target_path = self.output_dir / 'tracking_count.mp4',\n            callback=self.callback,\n            # show_progress=True,\n\n\n        )\n\n        \n    def callback(self, frame: np.ndarray, index:int) -> np.ndarray:\n    \n        results = self.model(frame, verbose=False, conf=0.7)[0]\n        detections = sv.Detections.from_ultralytics(results)\n        detections = self.byte_tracker.update_with_detections(detections)\n\n        labels = [\n            f\"#{tracker_id} {self.model.names[class_id]} {confidence:0.2f}\"\n            for confidence, class_id, tracker_id\n            in zip(detections.confidence, detections.class_id, detections.tracker_id)\n        ]\n        print(\"labels\", labels)\n\n        annotated_frame = frame.copy()\n        annotated_frame = self.trace_annotator.annotate(\n            scene=annotated_frame,\n            detections=detections)\n        annotated_frame = self.bounding_box_annotator.annotate(\n            scene=annotated_frame,\n            detections=detections)\n        annotated_frame = self.label_annotator.annotate(\n            scene=annotated_frame,\n            detections=detections,\n            labels=labels)\n\n        self.line_zone.trigger(detections)\n        print(\"self.line_zone\", self.line_zone)\n        print(\"self.line_zone.out_count\", self.line_zone.out_count)\n        print(\"self.line_zone.in_count\", self.line_zone.in_count)\n\n        return  self.line_zone_annotator.annotate(annotated_frame, line_counter=self.line_zone)        \n\ndef is_valid_model_path(model_path):\n    \"\"\"Check if the provided model path is valid.\"\"\"\n    # List of known pretrained model names\n    pretrained_models = ['yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x']\n    \n    # Check if it's a pretrained model name\n    if any(model_path.startswith(name) for name in pretrained_models):\n        return True\n        \n    # Check if it's a file that exists\n    if os.path.isfile(model_path) and model_path.endswith('.pt'):\n        return True\n        \n    return False\n\ndef main():\n    parser = argparse.ArgumentParser(description='Run YOLO object counting')\n    parser.add_argument('--model_path', type=str, \n                      default=\"yolov8n\",\n                      help='Path to model weights (.pt) or pretrained model name (e.g., yolov8n)')\n    parser.add_argument('--output_dir', type=str, \n                      default=\"outputs/counting\",\n                      help='Directory to save counting results')\n    parser.add_argument('--source', type=str, required=True,\n                      help='Path to video file')\n    parser.add_argument('--conf', type=float, default=0.25,\n                      help='Confidence threshold (0-1)')\n    parser.add_argument('--line_position', type=float, default=0.5,\n                      help='Position of the counting line (0-1, relative to frame height)')\n    \n    args = parser.parse_args()\n    \n    # Validate model path\n    if not is_valid_model_path(args.model_path):\n        if not args.model_path.endswith('.pt'):\n            print(f\"Warning: Model path '{args.model_path}' doesn't end with .pt\")\n            print(\"If this is a pretrained model name, it will be downloaded\")\n        else:\n            print(f\"Warning: Model file '{args.model_path}' doesn't exist\")\n    \n    # Initialize counter\n    print(f\"\\nInitializing counter with model: {args.model_path}\")\n    counter = YOLOCounter(args.model_path, args.output_dir)\n    \n    # Process source based on type\n    source_path = Path(args.source)\n    print(f\"Processing video: {source_path}\")\n    \n    if source_path.is_file():\n        if source_path.suffix in ['.mp4', '.avi', '.mov']:\n            # Count objects in video\n            stats = counter.count_objects_in_video(\n                source_path,\n                conf=args.conf,\n                line_position=args.line_position\n            )\n            # print(\"\\nCounting Statistics:\")\n            # for key, value in stats.items():\n            #     print(f\"{key}: {value}\")\n        else:\n            raise ValueError(\"Source must be a video file (.mp4, .avi, .mov)\")\n    else:\n        raise ValueError(f\"Invalid source path: {source_path}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nTo run the above code \n`python src/models/06.tracking_count.py --source outputs/videos/view35_sequence2.avi\n`\nHere is the log\n```\n\nInitializing counter with model: yolov8n\nSupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nProcessing video: outputs/videos/view35_sequence2.avi\nvideo_info VideoInfo(width=1296, height=972, fps=2, total_frames=48) (1296, 972)\nlabels ['#1 car 0.76']\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels []\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels []\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels ['#2 car 0.94']\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels []\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels ['#1 car 0.75']\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels ['#1 car 0.90']\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels ['#1 car 0.90']\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels ['#1 car 0.91']\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels ['#2 car 0.91']\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels ['#2 car 0.87']\nself.line_zone.out_count 0\nself.line_zone.in_count 0\nlabels []\n..\n\n```\nAs you can see, it seems that the tracking works well but the `line_zone` does not update `in_count` and `out_count`. How can I fix this?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "KennyTC",
        "body": "It seems that the project is not maintained anymore?"
      }
    ]
  },
  {
    "issue_number": 1832,
    "title": "Problem with target_path parameter in process_video function.",
    "author": "hoang-nam-2004",
    "state": "closed",
    "created_at": "2025-04-28T17:10:41Z",
    "updated_at": "2025-05-12T17:26:01Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi, I am trying to follow the documentation: https://supervision.roboflow.com/utils/video/#process_video\n\nI ran the pipeline perfectly in here: https://www.kaggle.com/code/namlh2004/yolov8-video-inference\n\nBut I could not find the result video in the target_path\n\nCan anyone have a look at this for me please ?\n\nThank you!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "hoang-nam-2004",
        "body": "Update: It just showed me a result.mp4 file, but it's an empty file."
      },
      {
        "user": "onuralpszr",
        "body": "Hello @hoang-nam-2004  sorry late answer, \n\n<img width=\"482\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f359158d-ac71-447a-9792-df784262a0ad\" />\n\nI checked your kaggle and I found that you were missing \"return\" I also did some small changes like you don't need to \"re-init\" box_annotator or add labels since you also added. \n\nHere is my kaggle version from your original kaggle notebook : https://www.kaggle.com/code/onuralpsezer/yolov8-video-inference\n\nI hope that helps."
      },
      {
        "user": "onuralpszr",
        "body": "Converting to Q&A "
      }
    ]
  },
  {
    "issue_number": 183,
    "title": "Show Progress in time consuming tasks",
    "author": "hardikdava",
    "state": "open",
    "created_at": "2023-07-06T17:56:50Z",
    "updated_at": "2025-05-12T15:40:12Z",
    "labels": [
      "enhancement",
      "good first issue",
      "version: 0.13.0",
      "version 0.14.0"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nIt is highly suggestable that users have information on how process is going on especially on time consuming tasks such as dataset operations.\r\n\r\n\r\nExample:\r\nLoading a large dataset can be time consuming. It can take a while to load whole dataset. Meanwhile users might be wondering if it is working properly or not. In worst situaiton, the process might even fails due to circumstances like memory issue or dataset issue.\r\n\n\n### Use case\n\n- Dataset ops\r\n- Image saving ops\r\n- Video processing ops\n\n### Additional\n\nSolution:\r\n\r\nIntroducing Progressbar from `tqdm` can be very useful in such scenario. \n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "hardikdava",
        "body": "@SkalskiP This can be wait for later. "
      },
      {
        "user": "SkalskiP",
        "body": "Hi @hardikdava üëãüèª I was also thinking about it. There are two things we need to consider:\r\n- The progress bar needs to work in the terminal and notebook environment.\r\n- The progress bar needs to be optional. You should be able to turn that off.\r\n\r\nLet's keep that issue open. We will pick it up in the future 100%."
      },
      {
        "user": "onuralpszr",
        "body": "@SkalskiP  \r\n\r\nTqdm could be a solution for this \r\n\r\nhttps://github.com/tqdm/tqdm (works with terminal/notebooks) you can see in tqdm docs.\r\n"
      }
    ]
  },
  {
    "issue_number": 1836,
    "title": "Incorrect mAP calculation",
    "author": "shataxiDubey",
    "state": "closed",
    "created_at": "2025-05-05T08:31:12Z",
    "updated_at": "2025-05-05T10:45:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nIn [L190](https://github.com/roboflow/supervision/blob/ed3186c506cf99ceef617b379c12a0702d72c60e/supervision/metrics/mean_average_precision.py#L190),\nwhile calculating mAP, list stats is not updated for all the cases.\n\nstats gets updated in two cases only:\nCase 1. prediction does not exist but target exists (False Negative case)\nCase 2. prediction exists and target also exists (True positive case)\n\nstats is not updated when:\n3. prediction exists but target does not exist ( False positive case)\n\nBecause of missing Case 3, the mAP values are very high even when the precision is very low.\n\n### Environment\n\n- Supervision 0.26.0rc4\n- numpy 1.26.4\n\n### Minimal Reproducible Example\n\n```\nimport supervision as sv\nimport numpy as np\n\n# no ground truth exists but object is detected\ntarget1 = sv.Detections.empty()\n\nprediction1 = sv.Detections.empty()\nprediction1.xyxy = np.array([[10, 20, 30, 40]])\nprediction1.mask = None\nprediction1.class_id = np.array([0]) \nprediction1.confidence = np.array([1])\n\n# ground truth exists and object is detected correctly\ntarget2 = sv.Detections.empty()\ntarget2.xyxy = np.array([[10, 20, 30, 40]])\ntarget2.mask = None\ntarget2.class_id = np.array([0])\ntarget2.confidence = None\n\nprediction2 = target2\nprediction2.confidence = np.array([1])\n\n# 4 False Positives and 1 true positive\npredictions_list = [prediction1, prediction1, prediction1, prediction1, prediction2]\ntargets_list = [target1, target1, target1, target1, target2]\n\n\nmap = sv.metrics.MeanAveragePrecision()\nmap = map.update(predictions_list, targets_list)\nmap = map.compute()\nmap.map50\n```\n\nThe mAP@50 is 0.99999 but precision is 0.2.\n\nThis shows that mAP is calculated very high even when there are so many false positives. \n\ncc @patel-zeel \n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @shataxiDubey üëãüèª, thanks a lot for your interest in `supervision`, and for reporting this issue!\n\nYes, I'm aware that the current mAP implementation is incomplete. We've asked @rafaelpadilla to take a look, and he has already opened a PR: https://github.com/roboflow/supervision/pull/1834. Once merged, the results provided by `supervision` should align with those from pycocotools. I plan to merge this change by the end of this week at the latest."
      },
      {
        "user": "SkalskiP",
        "body": "I'm closing the issue for now. Feel free to reopen it if you have any further questions!"
      }
    ]
  },
  {
    "issue_number": 1830,
    "title": "Getting started docs suggestion how_to/detect_and_annotate",
    "author": "cdeil",
    "state": "open",
    "created_at": "2025-04-27T20:39:26Z",
    "updated_at": "2025-04-27T20:39:26Z",
    "labels": [],
    "body": "Thanks for making `supervision` and releasing it open source and with nice docs! :-)\n\nI'm using supervision for the first time and as suggested in the docs started here: https://supervision.roboflow.com/latest/how_to/detect_and_annotate/\n\nTwo suggestions that would have made it a bit quicker to try it out:\n\n1. Add link to the image used in the notebook at the top. (I was struggling to get any mask detected on the first images I tried, IMO nice to start with a known result)\n2. Add this call to display the images and see the results `sv.plot_image(annotated_image)` (not mentioned in this first tutorial that sv has this utility function)\n\n",
    "comments": []
  },
  {
    "issue_number": 1827,
    "title": "Track Objects on Video (inference) example not retrieving class name correctly",
    "author": "fmousinho",
    "state": "open",
    "created_at": "2025-04-23T15:00:36Z",
    "updated_at": "2025-04-23T20:07:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nIn the example in # https://supervision.roboflow.com/develop/how_to/track_objects/#annotate-video-with-tracking-ids, we see label array assignment below. However, \"results\" does not have the attribute \"names\" which yields an error when running the code.\n\n```\nlabels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n```\nI was able to solve this by using the data attribute instead:\n\n```\n  labels = [\n        f\"#{tracker_id} {class_name}\"\n        for class_name, tracker_id\n        in zip(detections.data['class_name'], detections.tracker_id)\n    ]\n```\n\nTraceback:\n```\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[23], line 26\n     21     annotated_frame = box_annotator.annotate(\n     22         frame.copy(), detections=detections)\n     23     return label_annotator.annotate(\n     24         annotated_frame, detections=detections, labels=labels)\n---> 26 sv.process_video(\n     27     source_path=VIDEO_URL,\n     28     target_path=\"./result.mp4\",\n     29     callback=callback\n     30 )\n\nFile ~/Documents/Learning_to_Code/roboflow/roboenv/lib/python3.12/site-packages/supervision/utils/video.py:230, in process_video(source_path, target_path, callback)\n    226 with VideoSink(target_path=target_path, video_info=source_video_info) as sink:\n    227     for index, frame in enumerate(\n    228         get_video_frames_generator(source_path=source_path)\n    229     ):\n--> 230         result_frame = callback(frame, index)\n    231         sink.write_frame(frame=result_frame)\n\nCell In[23], line 16, in callback(frame, _)\n     12 detections = sv.Detections.from_inference(results)\n     13 detections = tracker.update_with_detections(detections)\n     15 labels = [\n---> 16     f\"#{tracker_id} {results.names[class_id]}\"\n     17     for class_id, tracker_id\n     18     in zip(detections.class_id, detections.tracker_id)\n     19 ]\n     21 annotated_frame = box_annotator.annotate(\n     22     frame.copy(), detections=detections)\n     23 return label_annotator.annotate(\n     24     annotated_frame, detections=detections, labels=labels)\n\nFile ~/Documents/Learning_to_Code/roboflow/roboenv/lib/python3.12/site-packages/pydantic/main.py:994, in BaseModel.__getattr__(self, item)\n    991     return super().__getattribute__(item)  # Raises AttributeError if appropriate\n    992 else:\n    993     # this is the current error\n--> 994     raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n\nAttributeError: 'ObjectDetectionInferenceResponse' object has no attribute 'names'\n```\n\n### Environment\n\n- Supervision: 0.25.1\n- OS: MacOS 15.4.1\n- Python 3.12.10\n\n### Minimal Reproducible Example\n\nUse code in doc example at # https://supervision.roboflow.com/develop/how_to/track_objects/#annotate-video-with-tracking-ids\n\n### Additional\n\nFirst time I file a bug.. Constructive criticism welcome!\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": []
  },
  {
    "issue_number": 1814,
    "title": "Repeat detections with InferenceSlicer",
    "author": "robmarkcole",
    "state": "open",
    "created_at": "2025-04-02T23:18:06Z",
    "updated_at": "2025-04-16T09:15:10Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nThe detections show a clear repeat pattern:\n\n![Image](https://github.com/user-attachments/assets/48bc76f9-ccee-458f-a505-95282100d401)\n\nThere are 720 detections, but when I check the `confidence` values, there are only 6 unique confidence values - i.e. appears detections are somehow mirrored to other locations on the image\n\n![Image](https://github.com/user-attachments/assets/3d03c353-ac21-4da7-83b1-6452f945fa41)\n\n### Environment\n\n- sv = 0.25.1\n\n### Minimal Reproducible Example\n\n```python\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom dotenv import load_dotenv\nimport supervision as sv\nfrom inference import get_model\n\n# Load environment variables (e.g., from .env file)\nload_dotenv()\napi_key = os.getenv(\"ROBOFLOW_API_KEY\")\n\n# Load the Roboflow model\nmodel = get_model(model_id=\"your-model-id\", api_key=api_key)  # anonymized model ID\n\n# Constants\nPATCH_SIZE = 640\nimage_dir = \"your-image-dir\"  # replace with actual directory path\nimages = glob(f\"{image_dir}/*.png\")\n\n# Read an image\nimage_file = images[1]  # change index as needed\nimage = cv2.imread(image_file)\n\n# Define inference callback\ndef callback(image_slice: np.ndarray) -> sv.Detections:\n    result = model.infer(image)[0]\n    return sv.Detections.from_inference(result)\n\n# Set up slicer\nslicer = sv.InferenceSlicer(\n    callback=callback,\n    overlap_filter=sv.OverlapFilter.NON_MAX_SUPPRESSION,\n    slice_wh=(PATCH_SIZE, PATCH_SIZE),\n    thread_workers=1,\n)\n\n# Run inference with slicing\ndetections = slicer(image)\nprint(f\"Number of detections: {len(detections)}\")\n\n# Annotate results\nbounding_box_annotator = sv.BoxAnnotator(\n    color=sv.ColorPalette.DEFAULT.colors[8],\n    thickness=2\n)\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{confidence:.2f}\"\n    for class_id, confidence in zip(detections.class_id, detections.confidence)\n]\n\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    annotated_image, detections=detections, labels=labels\n)\n\n# Save results to CSV\ncsv_sink = sv.CSVSink(\"out.csv\")\nwith csv_sink as sink:\n    sink.append(detections, {})\n\n# Load CSV into DataFrame for inspection\ndf = pd.read_csv(\"out.csv\")\nprint(f\"Detections saved: {len(df)}\")\nprint(df.sort_values(by=\"confidence\", ascending=False).head())\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @robmarkcole üëãüèª have you tried to manipulate `iou_threshold` value?"
      }
    ]
  },
  {
    "issue_number": 1818,
    "title": "Labels new edge can disappear",
    "author": "hidara2000",
    "state": "open",
    "created_at": "2025-04-09T10:17:24Z",
    "updated_at": "2025-04-16T08:23:33Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nWhen using label annotator if the box is close to an edge and the text is on the edge side disappears off frame.\n\nAdd an option to ensure it stays fully in the frame\n\n### Use case\n\nSuggested approach\n\nin the .annotate function labels near the top can be moved inside the bounding box using something similar to\n\n``` python      \nfor i in range(label_properties.shape[0]):\n     if label_properties[i, 1] < 0:\n         spacing = 3\n         offset = spacing + self.text_padding + self.font.size\n         label_properties[i, :] = label_properties[i, :] + [0,offset,0,offset,0,0]\n```\n\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @hidara2000 üëãüèª! Thanks so much for your interest in `supervision`. Would you be interested in helping us implement this feature?"
      },
      {
        "user": "hidara2000",
        "body": "https://github.com/roboflow/supervision/pull/1820"
      },
      {
        "user": "SkalskiP",
        "body": "@hidara2000 responded ;)"
      }
    ]
  },
  {
    "issue_number": 1085,
    "title": "Bug during `as_coco()` conversion for segmentation (+ solution)",
    "author": "Youho99",
    "state": "open",
    "created_at": "2024-04-02T12:38:15Z",
    "updated_at": "2025-04-06T18:08:29Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nCe probl√®me fait suite au travail fait sur mon pr√©c√©dent issue #1052 \r\n\r\n`results.as_coco(annotations_path=\"test_coco/\", min_image_area_percentage=0.01,)`\r\n>IndexError: list index out of range\r\n\r\nThe problem is due to the fact that in certain cases, the mask returned by GroundedSam is completely inconsistent (less than 3 pixels for example). In this specific case, the function approximate_mask_with_polygons() (as_coco() > detections_to_coco_annotations() > approximate_mask_with_polygons()) returns an empty array.\r\n\r\nSo, the line \r\n```py\r\nlist(approximate_mask_with_polygons(\r\n             mask=mask,\r\n             min_image_area_percentage=min_image_area_percentage,\r\n             max_image_area_percentage=max_image_area_percentage,\r\n             approximation_percentage=approximation_percentage,\r\n         )[0].flatten()\r\n``` \r\ncauses my error, because there is no item '0' in an empty list.\r\n\r\nI will propose a fix following the writing of this issue.\r\n\r\nI changed the architecture of the as_coco() function to be the same as the as_yolo() and as_pascal_voc() architecture.\n\n### Environment\n\nSupervision 0.19.0\n\n### Minimal Reproducible Example\n\n```py\r\nfrom autodistill_grounded_sam import GroundedSAM\r\nfrom autodistill.detection import CaptionOntology\r\nfrom pathlib import Path\r\nimport supervision as sv\r\nimport os\r\nimport shutil\r\n```\r\n```py\r\n# Put the cat image on your input directory\r\n# Put your input directory path\r\ninput_dir = \"/home/ggiret/T√©l√©chargements/chat\"\r\noutput_dir = \"test/\"\r\n```\r\n```py\r\nif os.path.exists(output_dir):\r\n    shutil.rmtree(output_dir)\r\n\r\nclasses = {\"cat\": \"a cat\",}\r\n\r\nbase_model = GroundedSAM(\r\n    ontology=CaptionOntology(\r\n        classes\r\n    ),\r\n    box_threshold = 0.005\r\n)\r\n```\r\n```py\r\nresults = base_model.label(\r\n        input_folder=input_dir,\r\n        extension=\".png\",\r\n        output_folder=output_dir, \r\n        record_confidence=True)\r\n```\r\n```py\r\nresults.as_coco(annotations_path=\"test_coco/\", min_image_area_percentage=0.01,)\r\n```\r\n\r\nImage to use : \r\n![t√©l√©chargement (4)](https://github.com/roboflow/supervision/assets/44434482/9a417299-2d41-47ac-8378-61762dc3108e)\r\n\n\n### Additional\n\nIn the code that I am going to propose, as_coco() will record either the BoundingBoxes coordinates or the Segmentation coordinates, but not both at the same time.\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "0xD4rky",
        "body": "hey @LinasKo, if this issue's still open then can I take this up as well as I think I may have a solution to it!"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @0xD4rky üëãüèª Honestly, I don't know if this bug still exists. Additionally, it seems that the bug is rather related to the `autodistill` project and not `supervision`. "
      },
      {
        "user": "Youho99",
        "body": "Hi @SkalskiP @0xD4rky \r\n\r\nThe Supervision library has evolved a lot since this bug was discovered.\r\nIt is indeed possible that this bug no longer exists. To test this, it would be necessary to run the bug reproduction case that I had shared in the issue ticket.\r\n\r\nAt the time, I had already proposed a fix solution for this one (in the associated PR). Since then, I have moved the PR to draft, because I do not know if the bug still exists.\r\n\r\nHowever, the bug mentioned came from the Supervision library and not from Autodistill."
      }
    ]
  },
  {
    "issue_number": 1101,
    "title": "[PolygonZone] - drop `frame_resolution_wh` requirement",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-04-08T10:57:53Z",
    "updated_at": "2025-03-28T17:07:17Z",
    "labels": [
      "enhancement",
      "good first issue",
      "help wanted",
      "api:polygonzone",
      "Q2.2024"
    ],
    "body": "### Description\r\n\r\nCurrently, the initialization of [sv.PolygonZone](https://supervision.roboflow.com/0.19.0/detection/tools/polygon_zone/) requires passing `frame_resolution_wh` in the constructor. Upon further reflection, this seems totally unnecessary.\r\n\r\n- Change the [`self.mask`](https://github.com/roboflow/supervision/blob/4729e20a9408fbe08d342cc4dbae835d808686a5/supervision/detection/tools/polygon_zone.py#L54) initialization located in the constructor. Instead of basing the shape of the mask on `frame_resolution_wh`, calculate the `x_max` and `y_max` of our `polygon` and create a mask of a size sufficient for the `polygon` to fit on it.\r\n\r\n  ```python\r\n  x_max, y_max = np.max(polygon, axis=0)\r\n  self.mask = polygon_to_mask(\r\n      polygon=polygon, resolution_wh=(x_max + 1, y_max + 1)\r\n  )\r\n  ```\r\n\r\n- Print an appropriate deprecation warning whenever someone sets the value of `frame_resolution_wh`, notifying that `frame_resolution_wh` is no longer required and will be dropped in version `supervision-0.24.0`.\r\n- Drop `frame_resolution_wh` from docs.\r\n\r\n### API\r\n\r\nExample usage after making the changes.\r\n\r\n```python\r\nimport numpy as np\r\nimport supervision as sv\r\n\r\nfrom ultralytics import YOLO\r\nfrom supervision.assets import download_assets, VideoAssets\r\n\r\ndownload_assets(VideoAssets.VEHICLES)\r\n\r\npolygon = np.array([[1252, 787], [2298, 803], [5039, 2159], [-550, 2159]])\r\nzone = sv.PolygonZone(polygon=polygon)\r\n\r\nmodel = YOLO(\"yolov8x.pt\")\r\n\r\ncolor = sv.Color.ROBOFLOW\r\ncolor_annotator = sv.ColorAnnotator(color=color)\r\n\r\ndef callback(scene: np.ndarray, index: int) -> np.ndarray:\r\n    result = model(scene)[0]\r\n    detections = sv.Detections.from_ultralytics(result)\r\n    detections_in_zone = detections[zone.trigger(detections=detections)]\r\n\r\n    annotated_frame = scene.copy()\r\n    annotated_frame = sv.draw_polygon(scene, polygon=polygon, color=color)\r\n    annotated_frame = color_annotator.annotate(scene, detections=detections_in_zone)\r\n    return annotated_frame\r\n\r\nsv.process_video(\r\n    source_path=\"vehicles.mp4\",\r\n    target_path=\"vehicles-result.mp4\",\r\n    callback=callback\r\n)\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "jeslinpjames",
        "body": "Hey, I'd like to take this one."
      },
      {
        "user": "onuralpszr",
        "body": "> Hey, I'd like to take this one.\n\nTask is yours, good luck  :) "
      },
      {
        "user": "jeslinpjames",
        "body": "Hi @onuralpszr,\r\n\r\nI have a doubt regarding handling the deprecation warning for `frame_resolution_wh`. Should I use the `deprecated()` function like this:\r\n\r\n```python\r\nclass PolygonZone:\r\n  \r\n    @deprecated(\"The `frame_resolution_wh` parameter is deprecated and will be removed in supervision-0.24.0. The mask resolution is now calculated automatically based on the polygon coordinates.\")\r\n    def __init__(\r\n        self,\r\n        polygon: npt.NDArray[np.int64],\r\n        triggering_anchors: Iterable[Position] = (Position.BOTTOM_CENTER,),\r\n        frame_resolution_wh: Optional[Tuple[int, int]] = None,\r\n    ):\r\n```\r\nThis will issue warnings whenever frame_resolution_wh is passed or not as a parameter. However, if I have to use the `deprecated_parameter()` function, it expects `new_parameter` as a string, so I can't set it to None. Could you advise me on what to do?"
      }
    ]
  },
  {
    "issue_number": 1810,
    "title": "get_video_frames_generator too slow",
    "author": "westlinkin",
    "state": "open",
    "created_at": "2025-03-25T05:50:50Z",
    "updated_at": "2025-03-25T05:50:50Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nWhen loop every frame ( from a video, size 2-3Gb), `get_video_frames_generator` is very low. \n\nI use L40S to do the classification inference, often times, (I think) the CPU is too slow to get each frame, then send it to GPU to inference. \n\nIs there any way to boost the performance?\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1805,
    "title": "Meaning of `lost_track_buffer` in `ByteTrack`",
    "author": "amaletzk",
    "state": "open",
    "created_at": "2025-03-21T10:34:31Z",
    "updated_at": "2025-03-21T10:34:31Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi,\n\nI do not understand the precise meaning of `lost_track_buffer` in the `ByteTrack` class. It is only used to initialize `max_time_lost`, which, however, seems to refer to a number of frames rather than a time (and so is a misnomer, IMHO). Furthermore, that variable is initialized as\n\n`self.max_time_lost = int(frame_rate / 30.0 * lost_track_buffer)`\n\nwhich does not make sense to me: why is there a division by 30?\n\nInstead, I would understand either of the following possibilities:\n* Remove the division by 30. In that case, `lost_frame_buffer` refers to the *time* (in seconds) a track is retained.\n* Initialize `max_time_lost` with `int(lost_track_buffer)`. In that case, `lost_track_buffer` refers to the *number of frames* directly, in agreement with the current docstring.\n\nThanks for clarifying this, and thanks for your great work!\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1800,
    "title": "Install supervision with numpy<2",
    "author": "pirnerjonas",
    "state": "closed",
    "created_at": "2025-03-06T13:36:44Z",
    "updated_at": "2025-03-06T14:08:20Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi,\n\nI wanted to add supervision to a project of mine that depends on `numpy<2`. \n\nIf I try this via poetry I get:\n\n```\n(.venv) C:\\Users\\z0039hdz\\Projects\\chip-segmentation [feat/export-model ‚â° +3 ~3 -0 !]> poetry env info\n\nVirtualenv\nPython:         3.10.11\nImplementation: CPython\nPath:           C:\\Users\\z0039hdz\\Projects\\chip-segmentation\\.venv\nExecutable:     C:\\Users\\z0039hdz\\Projects\\chip-segmentation\\.venv\\Scripts\\python.exe\nValid:          True\n\nBase\nPlatform:   win32\nOS:         nt\nPython:     3.10.11\nPath:       C:\\Users\\z0039hdz\\.pyenv\\pyenv-win\\versions\\3.10.11\nExecutable: C:\\Users\\z0039hdz\\.pyenv\\pyenv-win\\versions\\3.10.11\\python.exe\n\n(.venv) C:\\Users\\z0039hdz\\Projects\\chip-segmentation [feat/export-model ‚â° +3 ~3 -0 !]> poetry add supervision\nUsing version ^0.25.1 for supervision\n\nUpdating dependencies\nResolving dependencies... (5.3s)\n\nBecause no versions of supervision match >0.25.1,<0.26.0\n and supervision (0.25.1) depends on numpy (>=2.1.0), supervision (>=0.25.1,<0.26.0) requires numpy (>=2.1.0).\nSo, because chip-segmentation depends on both numpy (<2) and supervision (^0.25.1), version solving failed.\n```\n\nit seems like supervision depends on numpy>=2.1.0 for python 3.10 but if I check the pyproject.toml of supervision I see   `\"numpy>=1.21.2\"`.\n\nIs it possible to use supervision with `numpy<2`? Am I doing something wrong?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "@pirnerjonas  hello in current \"released\" version of supervision If I create a new python project with toml and add numpy<2 and latest supervision like this ;\n\n```toml\n[project]\nname = \"test-env\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\ndependencies = [\n    \"numpy<2\",\n    \"supervision>=0.25.1\",\n]\n```\n\nI was able to install without problem. You might want to check your other packages or conflicts.\n\n<img width=\"735\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/913f38b4-4b4a-466b-b912-68f96ee51feb\" />"
      },
      {
        "user": "onuralpszr",
        "body": "Converting to Q&A Discussion"
      }
    ]
  },
  {
    "issue_number": 1687,
    "title": "Issue with sv.VideoInfo FPS Handling for Precise Video Metadata Retrieval",
    "author": "joesu-angible",
    "state": "open",
    "created_at": "2024-11-25T08:17:09Z",
    "updated_at": "2025-02-24T13:26:23Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Description\r\n\r\nHello,\r\n\r\nI'm using ```sv.VideoInfo.from_video_path``` to retrieve video metadata and perform video manipulations. However, I've noticed an issue with how fps is calculated. Specifically, ```sv.VideoInfo.from_video_path``` uses the following method:\r\n```fps = int(video.get(cv2.CAP_PROP_FPS))```\r\n\r\nThis approach can lead to inaccuracies in certain scenarios. For example, if the actual FPS of a video is 24.999, the method will round this down to 24. Over a long video, this discrepancy can cause significant shifts and synchronization problems.\r\n\r\nWould it be possible to modify the implementation to return the FPS as a float rather than truncating it to an integer? This would improve accuracy for edge cases like this.\r\n\r\nThank you for your attention!\r\n\r\n\r\n\r\n### Use case\r\n\r\nChange the method of fps from ```fps = int(video.get(cv2.CAP_PROP_FPS))``` to ```fps = float(video.get(cv2.CAP_PROP_FPS))```\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @joesu-angible üëã \r\n\r\nThank you for reporting the issue! Indeed, half of our own asset videos don't have a round number for the fps.\r\n\r\n<details>\r\n<summary>Test code</summary>\r\n\r\n```python\r\nimport supervision as sv\r\nfrom supervision.assets import VideoAssets, download_assets\r\nimport cv2\r\n\r\n# int fps:   People walking, Market square, Skiing, Milk bottling plant, Vehicles\r\n# float fps: Subway, Basketball, Grocery Store, Beach, Vehicles 2\r\n\r\nasset = VideoAssets.VEHICLES_2\r\ndownload_assets(asset)\r\n\r\nvideo = cv2.VideoCapture(asset.value)\r\n\r\nfps = video.get(cv2.CAP_PROP_FPS)\r\nprint(fps, type(fps))\r\n\r\nvideo_info = sv.VideoInfo.from_video_path(asset.value)\r\nprint(video_info)\r\n```\r\n\r\n</details>\r\n\r\nThis would be a superb PR submission! However, since we've had the `int(fps)` code for a while, this issue requires looking at the wider impacts. There's multiple things to check, but I believe each of these is pretty simple:\r\n\r\n- [ ] Can we still use `VideoSink` to store a video?\r\n- [ ] Does `ByteTrack` still work if a non-int FPS is passed in? Does setting `track_seconds * fps` for the lost track buffer work?\r\n- [ ] Does speed estimation in `inference_example.py` work?\r\n- [ ] It's also used in coordinate calculation in `{ultralytics,inference,yolo_nas}_example.py`\r\n- [ ] `timers.py` for time in zone calculation. Used in `ultralytics{_naive}_stream_example.py`\r\n- [ ] Is FPS monitor anyhow affected? I don't think so, but it'd be worth checking.\r\n\r\nDoes that sound interesting, @joesu-angible?"
      },
      {
        "user": "miteshashar",
        "body": "@LinasKo I have been testing this.\n\nI ran the sample code for `VideoSink` for both the cases, `fps` being `int` and `float`.\n\nWhile the output videos are visually the same, there are minor differences in some parameters.\n\n<img width=\"603\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ad6da97b-6744-4a56-b892-55549d48cbc9\" />\n\n<img width=\"881\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/61aace7f-d9d5-460f-b8e4-4aaf95950412\" />\n\n<img width=\"571\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ca15e5bb-05bd-4d8a-90c1-8e45239feca0\" />"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @miteshashar thanks for running the tests. I think the most noticeable difference shouldn't be in the file size but in the video length. Did you take such measurements as well?"
      }
    ]
  },
  {
    "issue_number": 1438,
    "title": "[KeypointsDataset] - create a new dataset format to load keypoints",
    "author": "SkalskiP",
    "state": "open",
    "created_at": "2024-08-08T12:03:47Z",
    "updated_at": "2025-02-22T14:52:33Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Description\r\n\r\nSupervision can currently load object detection, instance segmentation, and classification datasets. Let's add the ability to handle keypoint datasets. The sv.KeypointDataset should offer the same functionalities as [sv.DetectionDataset](https://supervision.roboflow.com/latest/datasets/core/) and [sv.ClassificationDataset](https://supervision.roboflow.com/latest/datasets/core/#supervision.dataset.core.ClassificationDataset), including `merge`, `split`, `__getitem__`, `__iter__`, `from_yolo`, and `as_yolo`.\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "shaddu",
        "body": "Hi @SkalskiP üëã \r\n\r\nI was analyzing the issue and understood that KeypointsDataset will be similar to DetectionDataset, with the difference being the type of annotations.\r\n\r\n```\r\nclasses: List[str],\r\nimages: Union[List[str], Dict[str, np.ndarray]],\r\nannotations: Dict[str, KeyPoints]\r\n```\r\n\r\nIf this is a correct understanding then I will like to work on this issue.\r\n"
      },
      {
        "user": "LinasKo",
        "body": "Let's only implement the new, lazy-loaded, non-deprecated API, with `images: List[str]`. This will save work in the future."
      },
      {
        "user": "LinasKo",
        "body": "@shaddu, I also made this template, which should help with starting your colab: [Colab Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89?usp=sharing).\r\nFeel free to use as much or as little of it as you wish üòâ \r\n"
      }
    ]
  },
  {
    "issue_number": 1785,
    "title": "What is supervision's coordinate system?",
    "author": "mattico",
    "state": "closed",
    "created_at": "2025-02-05T15:45:15Z",
    "updated_at": "2025-02-19T14:25:10Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nWhat is supervision's coordinate system? I'm pretty sure that it uses top-left origin (y increases down, x increases right), but I couldn't find it mentioned anywhere in the docs. I'm about to go test what it is, but I think the docs could be improved by mentioning this somewhere. Perhaps on a foundational class's docs like `Detections`.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @mattico üëãüèª Yes, the coordinate system starts at the top-left corner. Indeed, we don't mention it in the documentation. Probably because it's a common practice in almost every computer vision library. But I think it would be worth adding such information to the documentation. `Detections` is a good place. Would you like to do it?"
      }
    ]
  },
  {
    "issue_number": 1787,
    "title": "how to use yolov11s-seg supervision onnx runtime?",
    "author": "pranta-barua007",
    "state": "closed",
    "created_at": "2025-02-15T14:26:55Z",
    "updated_at": "2025-02-17T17:32:33Z",
    "labels": [
      "question"
    ],
    "body": "dear @onuralpszr i saw similar case on #1626 and tried some customization with my own usecase for segmentation but doesn't seem to properly working\n\n  here is how I am exporting my model with ultralytics\n\n```python\nft_loaded_best_model.export(\nformat=\"onnx\",\nnms=True,\ndata=\"/content/disease__instance_segmented/data.yaml\",\n) # creates 'best.onnx'\n```\n\nwhich outputs in console\n\n```console\nUltralytics 8.3.75 üöÄ Python-3.11.11 torch-2.5.1+cu124 CPU (Intel Xeon 2.00GHz)\nYOLO11s-seg summary (fused): 265 layers, 10,068,364 parameters, 0 gradients, 35.3 GFLOPs\n\nPyTorch: starting from '/content/drive/MyDrive/ML/DENTAL_THESIS/fine_tuned/segment/train/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 300, 38), (1, 32, 160, 160)) (19.6 MB)\n\nONNX: starting export with onnx 1.17.0 opset 19...\nONNX: slimming with onnxslim 0.1.48...\nONNX: export success ‚úÖ 4.2s, saved as '/content/drive/MyDrive/ML/DENTAL_THESIS/fine_tuned/segment/train/weights/best.onnx' (38.7 MB)\n\nExport complete (5.5s)\nResults saved to /content/drive/MyDrive/ML/DENTAL_THESIS/fine_tuned/segment/train/weights\nPredict:         yolo predict task=segment model=/content/drive/MyDrive/ML/DENTAL_THESIS/fine_tuned/segment/train/weights/best.onnx imgsz=640  \nValidate:        yolo val task=segment model=/content/drive/MyDrive/ML/DENTAL_THESIS/fine_tuned/segment/train/weights/best.onnx imgsz=640 data=/content/dental_disease__instance_segmented-7/data.yaml  \nVisualize:       https://netron.app/\n/content/drive/MyDrive/ML/DENTAL_THESIS/fine_tuned/segment/train/weights/best.onnx\n```\n\nI have 4 classes in my model\nas I applied nms my output0 is already transposed I think\nwhere first 4 indices are bbox. 5 is prob, 6 is class id 7 and rest 32 are mask and the 300 is for the model will detect up to 300 results, educate if my interpretation is wrong ?\n\nhere is my implementation\n\n```python\ndef xywh2xyxy(x):\n    y = np.copy(x)\n    y[..., 0] = x[..., 0] - x[..., 2] / 2\n    y[..., 1] = x[..., 1] - x[..., 3] / 2\n    y[..., 2] = x[..., 0] + x[..., 2] / 2\n    y[..., 3] = x[..., 1] + x[..., 3] / 2\n    return y\n\n\nclass YOLOv11:\n    def __init__(self, path, conf_thres=0.7, iou_thres=0.5):\n        self.conf_threshold = conf_thres\n        self.iou_threshold = iou_thres\n        # Initialize the ONNX model\n        self.initialize_model(path)\n\n    def __call__(self, image):\n        return self.detect_objects(image)\n\n    def initialize_model(self, path):\n        self.session = onnxruntime.InferenceSession(\n            path, providers=onnxruntime.get_available_providers()\n        )\n        self.get_input_details()\n        self.get_output_details()\n\n    def detect_objects(self, image):\n        input_tensor = self.prepare_input(image)\n        outputs = self.inference(input_tensor)\n        self.boxes, self.scores, self.class_ids, self.masks = self.process_output(outputs)\n        return self.boxes, self.scores, self.class_ids, self.masks\n\n    def prepare_input(self, image):\n        self.img_height, self.img_width = image.shape[:2]\n        input_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        input_img = cv2.resize(input_img, (self.input_width, self.input_height))\n        input_img = input_img / 255.0\n        input_img = input_img.transpose(2, 0, 1)\n        input_tensor = input_img[np.newaxis, :, :, :].astype(np.float32)\n        return input_tensor\n\n    def inference(self, input_tensor):\n        outputs = self.session.run(self.output_names, {self.input_names[0]: input_tensor})\n        return outputs\n\n    def process_output(self, outputs):\n        \"\"\"\n        Process model outputs:\n          - outputs[0]: shape (1, 300, 38)\n            * 0-3: bounding box (xywh)\n            * 4: confidence score\n            * 5: class id\n            * 6-37: segmentation coefficients (32 values)\n          - outputs[1]: shape (1, 38, 160, 160) mask prototypes\n        \"\"\"\n        # Remove batch dimension from detections\n        predictions = np.squeeze(outputs[0])  # shape (300, 38)\n        mask_protos = outputs[1]               # shape (1, 38, 160, 160)\n\n        # Filter predictions using the confidence score (index 4)\n        conf_scores = predictions[:, 4]\n        valid = conf_scores > self.conf_threshold\n        predictions = predictions[valid]\n        scores = conf_scores[valid]\n\n        if len(scores) == 0:\n            return [], [], [], []\n\n        # Extract bounding boxes (indices 0-3)\n        boxes = self.extract_boxes(predictions)\n\n        # Extract class ids (index 5) and cast them to int\n        class_ids = predictions[:, 5].astype(np.int32)\n\n        # Extract segmentation masks using segmentation coefficients (indices 6-37)\n        masks = self.extract_masks(predictions, mask_protos)\n\n        return boxes, scores, class_ids, masks\n\n    def extract_boxes(self, predictions):\n        boxes = predictions[:, :4]  # xywh format\n        boxes = self.rescale_boxes(boxes)\n        boxes = xywh2xyxy(boxes)\n        return boxes\n\n    def rescale_boxes(self, boxes):\n        # Scale boxes from network input dimensions to original image dimensions\n        input_shape = np.array([self.input_width, self.input_height, self.input_width, self.input_height])\n        boxes = np.divide(boxes, input_shape, dtype=np.float32)\n        boxes *= np.array([self.img_width, self.img_height, self.img_width, self.img_height])\n        return boxes\n\n    def extract_masks(self, predictions, mask_protos):\n        \"\"\"\n        Compute segmentation masks:\n          - predictions: (num_detections, 38) with segmentation coefficients at indices 6-37\n          - mask_protos: (1, 38, 160, 160); we use the first 32 channels to match coefficients.\n        \"\"\"\n        # Get segmentation coefficients from predictions (32 coefficients)\n        seg_coeffs = predictions[:, 6:38]  # shape: (num_detections, 32)\n\n        # Use the first 32 channels from mask prototypes\n        mask_protos = mask_protos[0, :32, :, :]  # shape: (32, 160, 160)\n\n        # Compute per-detection masks as a weighted sum over mask prototypes\n        masks = np.einsum('nc,chw->nhw', seg_coeffs, mask_protos)\n\n        # Apply sigmoid to get values between 0 and 1\n        masks = 1 / (1 + np.exp(-masks))\n\n        # Threshold masks to produce binary masks\n        masks = masks > 0.5\n\n        # Resize each mask to the original image dimensions\n        final_masks = []\n        for mask in masks:\n            mask_uint8 = (mask.astype(np.uint8)) * 255\n            mask_resized = cv2.resize(mask_uint8, (self.img_width, self.img_height), interpolation=cv2.INTER_NEAREST)\n            final_masks.append(mask_resized)\n        final_masks = np.array(final_masks)\n\n        return final_masks\n\n    def get_input_details(self):\n        model_inputs = self.session.get_inputs()\n        self.input_names = [inp.name for inp in model_inputs]\n        self.input_shape = model_inputs[0].shape\n        self.input_height = self.input_shape[2]\n        self.input_width = self.input_shape[3]\n\n    def get_output_details(self):\n        model_outputs = self.session.get_outputs()\n        self.output_names = [out.name for out in model_outputs]\n\n```",
    "comments": [
      {
        "user": "pranta-barua007",
        "body": "here is how i am making predictions\n\n# Load the model and create InferenceSession\nbest_weights_path = f\"{saved_model_results_path}/train/weights/best.onnx\"\n\ndetector = YOLOv11(best_weights_path, conf_thres=0.2, iou_thres=0.3)\n\nimg = cv2.imread(\"/content/download (1).jpeg\")\n# Detect Objects (now returns bounding boxes, scores, class_ids, and segmentation masks)\nboxes, scores, class_ids, masks = detector(img)\n\nboxes\n```console\narray([[     274.24,      185.68,      958.67,       689.4],\n       [     244.84,      252.61,      830.42,      883.34]], dtype=float32)\n```\n\nscores\n```console\narray([     0.8895,     0.86876], dtype=float32)\n```\n\nclass_Ids\n```console\narray([2, 2], dtype=int32)\n```\n\nmasks\n```console\narray([[[255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255],\n        ...,\n        [255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255]],\n\n       [[255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255],\n        ...,\n        [255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255]]], dtype=uint8)\n```\n\n![Image](https://github.com/user-attachments/assets/90cbd899-95d0-499e-b9ab-095ca42fd253)"
      },
      {
        "user": "pranta-barua007",
        "body": "i have checked predicted the class id and prob it is right\n\nthings that are working fine \n‚úÖ scores \n‚úÖclass_Ids\nnot working\n‚ùåboxes\n‚ùåmasks\n\nhere is how it should be, below is direct prediction results  with ultralytics\n\n![Image](https://github.com/user-attachments/assets/ee2d8c82-3575-4747-a79c-ffa58051e655)"
      },
      {
        "user": "onuralpszr",
        "body": "@pranta-barua007  hello, let me check mask case quickly in my collab "
      }
    ]
  },
  {
    "issue_number": 1676,
    "title": "Add way to \"filter out\" low confidence keypoints",
    "author": "Dref360",
    "state": "open",
    "created_at": "2024-11-19T15:20:59Z",
    "updated_at": "2025-02-15T14:49:44Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nIn pose estimation, low confidence keypoints should be set to 0.\r\nIt would be great to include this in `supervision` instead of asking the user to do it.\r\n\r\n\r\nProposal:\r\n\r\n```python\r\n\r\nkpts = KeyPoints(xy=...,\r\n                 confidence=...,)\r\n\r\nkpts = kpts.with_threshold(threshold=0.5)\r\n```\r\n\r\nAll keypoints with confidence lower than 0.5 would be set to 0.\n\n### Use case\n\nUseful when visualizing skeletons with low confidence.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "miteshashar",
        "body": "@Dref360 I am trying to better understand the utility of this.\n\nSetting the key points below the threshold to 0(if that is what you mean) could in some cases result in a low %age of key points remaining in place ‚Äì the ones satisfying the threshold.\nThe key points determine the shape of the pose. And hence, this would effectively result in just a less sparse pose structure. Which is why I am unable to grasp what you are trying to visualize.\n\nTo visually understand what is the impact on the skeletons when I mute key points below a certain threshold, [I ran a small experiment to visualize it](https://github.com/miteshashar/supervision-keypoint-muting).\n\nBelow is a sample of these illustrations, there are more on the repo.\n\nhttps://github.com/user-attachments/assets/62c5a34b-c522-446a-a102-ca2d8b82625c\n\nWhat would you do with the `KeyPoints` object having key points below the confidence threshold muted? It would be great if you can point to some plausible use cases you are trying to achieve to clarify whether I am misunderstanding what your requirement is.\n\nAlso, now with slightly better initial understanding of Supervision's code, I feel the method you have proposed is a very thin abstraction and can be achieved simply with [one line of code](https://github.com/miteshashar/supervision-keypoint-muting/blob/main/main.py#L95), since `kpts.xy` and `kpts.confidence` are just NumPy arrays:\n\n```\nTHRESHOLD = 0.5\nREPLACEMENT = 0\nkpts.xy[kpts.confidence < THRESHOLD] = REPLACEMENT\n```"
      }
    ]
  },
  {
    "issue_number": 1729,
    "title": "Maximum of 20 tabs in annotator docs",
    "author": "LinasKo",
    "state": "open",
    "created_at": "2024-12-11T15:18:55Z",
    "updated_at": "2025-02-13T02:56:12Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nI recently discovered that when you have more than 20 tabs in our docs, it doesn't render the content beyond the 20th. The tab icon is visible, yet the content is not.\r\n\r\nAt present this is only relevant in the [annotators page](https://supervision.roboflow.com/latest/detection/annotators/) where the code and preview image is displayed at the top.\r\n\r\nThe bug is currently visible on the [latest docs](https://supervision.roboflow.com/latest/detection/annotators/) page - the `BackgroundColor` tab shows no contents. I have thoroughly tested it while adding the `ComparisonAnnotator` (#001f85ad), and in that case, the last 2 annotator content blocks don't show up. Lastly, if I reorder the tabs, once again - the final two don't show up.\r\n\r\nI skimmed our code and the code of https://facelessuser.github.io/pymdown-extensions/extensions/tabbed/#syntax-tab-1, but couldn't find an issue. At this point, I'm not sure where it could be.\n\n### Environment\n\n`pip freeze`:\r\n\r\n```bash\r\nanyio==4.5.2\r\nappnope==0.1.4\r\nargon2-cffi==23.1.0\r\nargon2-cffi-bindings==21.2.0\r\narrow==1.3.0\r\nasttokens==3.0.0\r\nasync-lru==2.0.4\r\nattrs==24.2.0\r\nbabel==2.16.0\r\nbackcall==0.2.0\r\nbeautifulsoup4==4.12.3\r\nbleach==6.1.0\r\nbuild==1.2.2.post1\r\ncachetools==5.5.0\r\ncairocffi==1.7.1\r\nCairoSVG==2.7.1\r\ncertifi==2024.8.30\r\ncffi==1.17.1\r\ncfgv==3.4.0\r\nchardet==5.2.0\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\ncolorama==0.4.6\r\ncomm==0.2.2\r\ncontourpy==1.3.1\r\ncssselect2==0.7.0\r\ncycler==0.12.1\r\ndebugpy==1.8.9\r\ndecorator==5.1.1\r\ndefusedxml==0.7.1\r\ndistlib==0.3.9\r\ndocutils==0.21.2\r\nexecuting==2.1.0\r\nfastjsonschema==2.21.1\r\nfilelock==3.16.1\r\nfiletype==1.2.0\r\nfonttools==4.55.2\r\nfqdn==1.5.1\r\nghp-import==2.1.0\r\ngitdb==4.0.11\r\nGitPython==3.1.43\r\ngriffe==1.4.0\r\nh11==0.14.0\r\nhttpcore==1.0.7\r\nhttpx==0.28.1\r\nidentify==2.6.1\r\nidna==3.10\r\nimportlib_metadata==8.5.0\r\nimportlib_resources==6.4.5\r\niniconfig==2.0.0\r\nipykernel==6.29.5\r\nipython==8.12.3\r\nipywidgets==8.1.5\r\nisoduration==20.11.0\r\njaraco.classes==3.4.0\r\njaraco.context==6.0.1\r\njaraco.functools==4.1.0\r\njedi==0.19.2\r\nJinja2==3.1.4\r\njson5==0.10.0\r\njsonpointer==3.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2023.12.1\r\njupyter-events==0.10.0\r\njupyter-lsp==2.2.5\r\njupyter_client==8.6.3\r\njupyter_core==5.7.2\r\njupyter_server==2.14.2\r\njupyter_server_terminals==0.5.3\r\njupyterlab==4.3.2\r\njupyterlab_pygments==0.3.0\r\njupyterlab_server==2.27.3\r\njupyterlab_widgets==3.0.13\r\njupytext==1.16.4\r\nkeyring==25.5.0\r\nkiwisolver==1.4.7\r\nMarkdown==3.7\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmatplotlib==3.9.3\r\nmatplotlib-inline==0.1.7\r\nmdit-py-plugins==0.4.2\r\nmdurl==0.1.2\r\nmergedeep==1.3.4\r\nmike==2.1.3\r\nmistune==3.0.2\r\nmkdocs==1.6.1\r\nmkdocs-autorefs==1.2.0\r\nmkdocs-get-deps==0.2.0\r\nmkdocs-git-committers-plugin-2==2.4.1\r\nmkdocs-git-revision-date-localized-plugin==1.3.0\r\nmkdocs-jupyter==0.24.8\r\nmkdocs-material==9.5.48\r\nmkdocs-material-extensions==1.3.1\r\nmkdocstrings==0.26.1\r\nmkdocstrings-python==1.11.1\r\nmore-itertools==10.5.0\r\nmypy==1.13.0\r\nmypy-extensions==1.0.0\r\nnbclient==0.10.1\r\nnbconvert==7.16.4\r\nnbformat==5.10.4\r\nnest-asyncio==1.6.0\r\nnh3==0.2.19\r\nnodeenv==1.9.1\r\nnotebook==7.3.1\r\nnotebook_shim==0.2.4\r\nnumpy==2.2.0\r\nopencv-python==4.10.0.84\r\nopencv-python-headless==4.10.0.84\r\noverrides==7.7.0\r\npackaging==24.2\r\npaginate==0.5.7\r\npandocfilters==1.5.1\r\nparso==0.8.4\r\npathspec==0.12.1\r\npexpect==4.9.0\r\npickleshare==0.7.5\r\npillow==10.4.0\r\npkginfo==1.12.0\r\nplatformdirs==4.3.6\r\npluggy==1.5.0\r\npre-commit==3.5.0\r\nprometheus_client==0.21.1\r\nprompt_toolkit==3.0.48\r\npsutil==6.1.0\r\nptyprocess==0.7.0\r\npure_eval==0.2.3\r\npycparser==2.22\r\nPygments==2.18.0\r\npymdown-extensions==10.12\r\npyparsing==3.1.4\r\npyproject-api==1.8.0\r\npyproject_hooks==1.2.0\r\npytest==8.3.4\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\npython-json-logger==2.0.7\r\npytz==2024.2\r\nPyYAML==6.0.2\r\npyyaml_env_tag==0.1\r\npyzmq==26.2.0\r\nreadme_renderer==43.0\r\nreferencing==0.35.1\r\nregex==2024.11.6\r\nrequests==2.32.3\r\nrequests-toolbelt==1.0.0\r\nrfc3339-validator==0.1.4\r\nrfc3986==2.0.0\r\nrfc3986-validator==0.1.1\r\nrich==13.9.4\r\nroboflow==1.1.49\r\nrpds-py==0.20.1\r\nruff==0.8.2\r\nscipy==1.14.1\r\nSend2Trash==1.8.3\r\nsetuptools==75.3.0\r\nsix==1.17.0\r\nsmmap==5.0.1\r\nsniffio==1.3.1\r\nsoupsieve==2.6\r\nstack-data==0.6.3\r\n-e git+ssh://git@github.com/roboflow/supervision.git@001f85ad0f79a9ebd38094437adfc88d4fc1f7dc#egg=supervision\r\nterminado==0.18.1\r\ntinycss2==1.4.0\r\ntornado==6.4.2\r\ntox==4.23.2\r\ntqdm==4.67.1\r\ntraitlets==5.14.3\r\ntwine==6.0.1\r\ntypes-python-dateutil==2.9.0.20241206\r\ntyping_extensions==4.12.2\r\nuri-template==1.3.0\r\nurllib3==2.2.3\r\nverspec==0.1.0\r\nvirtualenv==20.28.0\r\nwatchdog==4.0.2\r\nwcwidth==0.2.13\r\nwebcolors==24.8.0\r\nwebencodings==0.5.1\r\nwebsocket-client==1.8.0\r\nwheel==0.45.1\r\nwidgetsnbextension==4.0.13\r\nzipp==3.20.2\r\n```\n\n### Minimal Reproducible Example\n\n![Screenshot 2024-12-11 at 5 17 27‚ÄØPM](https://github.com/user-attachments/assets/90e2ded3-b3a5-4247-a226-43fb34ba4966)\r\n\n\n### Additional\n\nDocs can be run after installing the repo with`poetry install --with-dev` and running `mkdocs serve`.\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "@SkalskiP  I also investigate this issue, in mkdocs-material and it is indeed 20 tab is hard limit \r\n\r\nRelated issue: https://github.com/squidfunk/mkdocs-material/issues/6665\r\nAnswer from maintainer : https://github.com/squidfunk/mkdocs-material/issues/6665#issuecomment-1899815035\r\n\r\nHe clearly indicate that 20 is limit and show codes too. And for passing this limit it require custom build and of course patch those numbers and build it. \r\n\r\nFor fixing this issue, we can split annotators tab (\"Content Tab\" is name of the tab in mkdocs-material) or we can think something else as well. \r\n\r\nReference Document: https://squidfunk.github.io/mkdocs-material/reference/content-tabs/#content-tabs \r\n"
      },
      {
        "user": "SkalskiP",
        "body": "@onuralpszr, it looks like we won't be able to do anything about it?"
      },
      {
        "user": "onuralpszr",
        "body": "> @onuralpszr, it looks like we won't be able to do anything about it?\r\n\r\n- We can split those tabs and make it 2 rows (10/11) to show them all\r\n- We can remove tabs (they are technically duplicates because we already in the page but selection wise it is easier to notice)\r\n- We can create a separate page dedicated to example usage of annotators (Because they are too many so) \r\n\r\nI don't know else we can do, that's all come to my mind. "
      }
    ]
  },
  {
    "issue_number": 480,
    "title": "Instance Segmentation Mean Average Precision",
    "author": "cgrtrifork",
    "state": "open",
    "created_at": "2023-10-14T21:38:54Z",
    "updated_at": "2025-02-06T23:24:48Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nCurrently the metrics have a `MeanAveragePrecision` class that works for object detection. It would be great to have an implementation for mAP for masks of an instance segmentation model.\r\n\r\nI'd expect only `from_detections` static method would be used, instead of the `from_tensors`, without any API change, as the former uses Detections, and these have a `mask` attribute already.\r\n\r\nThis goes probably hand in hand with #238.\r\n\r\n\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "Bhavay-2001",
        "body": "Hi @LinasKo, this is the issue I am talking about. Is this available and open to work? "
      },
      {
        "user": "LinasKo",
        "body": "Hi @Bhavay-2001,\r\n\r\nI've had a look, and we have something more urgent.\r\nHow does #1293 sound? (you may reply there)"
      }
    ]
  },
  {
    "issue_number": 1778,
    "title": "Improving object detection benchmarking process for unrelated datasets",
    "author": "patel-zeel",
    "state": "open",
    "created_at": "2025-01-20T17:11:42Z",
    "updated_at": "2025-02-04T19:08:42Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\n##### Context\nIf a model is trained on a dataset, benchmarking with `supervision`'s evaluation API is straightforward.\n\nHowever, if the dataset was not used to train a model (unrelated dataset), one should follow [the \"how-to\" guide on Benchmarking a model](https://supervision.roboflow.com/develop/how_to/benchmark_a_model/). \n\nWhat if we can improve this with a simpler API?\n\n### Use case\n\nIn cases where the dataset was not used to train the model, people would need to: 1) remap the class names to match with the dataset; 2) transform the class IDs to match with the dataset. Here is what a boilerplate code would look like:\n\n```py\nclass_mapping = {\"dog\": \"animal\", \"cat\": \"animal\", \"eagle\": \"bird\"}\nfor _, image, target in dataset:\n    result = model.infer(image)[0]\n    detection = sv.Detections.from_inference(result)\n    \n    # check if all mapped values are within the dataset classes\n    if not all([value in dataset.classes for value in class_mapping.values()]):\n        raise ValueError(\"All mapped values must be in dataset classes\")\n    \n    # remap class names\n    detection['class_name'] = list(map(lambda name: class_mapping[name] if name in class_maping else name, detection['class_name']))\n\n    # remove predicted classes not in the dataset\n    detection = detection[np.isin(detection['class_name'], dataset.classes)]\n\n    # remap Class IDs based on Class names\n    detection.class_id = np.array([dataset.classes.index(name) for name in detection['class_name']])\n```\n\nProposed API for the above transformation:\n\n```py\nclass_mapping = {\"dog\": \"animal\", \"cat\": \"animal\", \"eagle\": \"bird\"}\nfor _, image, target in dataset:\n    result = model.infer(image)[0]\n    detection = sv.Detections.from_inference(result)\n    detection = detection.transform(dataset, class_mapping=class_mapping)\n```\n\nWhere the `transform` method implements the same logic described in the boilerplate. Note that `class_mapping` is an optional argument for the cases where class names are the same between a model and a dataset.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": []
  },
  {
    "issue_number": 1777,
    "title": "Deprecation warning for `sv.MeanAveragePrecision` and other outdated metrics",
    "author": "patel-zeel",
    "state": "open",
    "created_at": "2025-01-19T08:15:46Z",
    "updated_at": "2025-01-19T18:05:54Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\n##### What's the problem?\nI found two ways of computing mAP metric: `sv.MeanAveragePrecision` and `sv.metrics.MeanAveragePrecision`. However, I see [here](https://supervision.roboflow.com/develop/detection/metrics/) that `sv.MeanAveragePrecision` and a few other metrics will be deprecated.\n\n##### Why is it important?\nBoth `sv.MeanAveragePrecision` and `sv.metrics.MeanAveragePrecision` give different results as shown in the following example.\n```py\nimport os\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\n# Download dataset\nif not os.path.exists(\"/tmp/rf_animals\"):\n    !wget https://universe.roboflow.com/ds/1LLwpXz2td?key=8JnJML5YF6 -O /tmp/rf_animals.zip\n    !unzip /tmp/dataset.zip -d /tmp/rf_animals\n\n# Load dataset\ndataset = sv.DetectionDataset.from_yolo(\"/tmp/rf_animals/train/images\", \"/tmp/rf_animals/train/labels\", \"/tmp/rf_animals/data.yaml\")\n\n# Inference\nmodel = YOLO(\"yolov8s\")\ntargets, detections = [], []\nfor image_path, image, target in dataset:\n    targets.append(target)\n    \n    prediction = model(image, verbose=False)[0]\n    detection = sv.Detections.from_ultralytics(prediction)\n    detection = detection[np.isin(detection['class_name'], dataset.classes)]\n    detection.class_id = np.array([dataset.classes.index(class_name) for class_name in detection['class_name']])\n    detections.append(detection)\n    \n# Method #1\nmAP = sv.metrics.MeanAveragePrecision().update(detections, targets).compute()\nprint(f\"mAP50: {mAP.map50:.4f}\")\n\n# Method #2\nmAP = sv.MeanAveragePrecision.from_detections(detections, targets)\nprint(f\"mAP50: {mAP.map50:.4f}\")\n```\n\nOutput\n```\nmAP50: 0.1553\nmAP50: 0.2100\n```\n\nAs per the docstrings, Method 1 computes the average precision, given the recall and precision curves following https://github.com/rafaelpadilla/Object-Detection-Metrics and Method 2 uses 101-point interpolation (COCO) method. People may end up using a mixture of these methods and may derive wrong conclusions.\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": []
  },
  {
    "issue_number": 1554,
    "title": "Allow TIFF (and more) image formats in `load_yolo_annotations`",
    "author": "patel-zeel",
    "state": "closed",
    "created_at": "2024-09-28T15:04:57Z",
    "updated_at": "2025-01-19T13:02:38Z",
    "labels": [
      "enhancement",
      "hacktoberfest"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Description\r\n\r\n* Currently, `load_yolo_annotations` only allows `png,` `jpg`, and `jpeg` file formats. `load_yolo_annotations` is internally called by `sv.DetectionDataset.from_yolo` functionality.\r\n\r\nhttps://github.com/roboflow/supervision/blob/1860fdb0a4e21edc5fa03d973e9f31c055bdcf4f/supervision/dataset/formats/yolo.py#L156\r\n\r\n* Ultralytics supports a wide variety of image formats. Copied the following table from [their website](https://docs.ultralytics.com/modes/predict/#image-and-video-formats:~:text=The%20below%20table%20contains%20valid%20Ultralytics%20image%20formats.).\r\n\r\n| Image Suffix  | Example Predict Command           | Reference                       |\r\n|---------------|-----------------------------------|---------------------------------|\r\n| .bmp          | yolo predict source=image.bmp     | Microsoft BMP File Format       |\r\n| .dng          | yolo predict source=image.dng     | Adobe DNG                       |\r\n| .jpeg         | yolo predict source=image.jpeg    | JPEG                            |\r\n| .jpg          | yolo predict source=image.jpg     | JPEG                            |\r\n| .mpo          | yolo predict source=image.mpo     | Multi Picture Object            |\r\n| .png          | yolo predict source=image.png     | Portable Network Graphics       |\r\n| .tif          | yolo predict source=image.tif     | Tag Image File Format           |\r\n| .tiff         | yolo predict source=image.tiff    | Tag Image File Format           |\r\n| .webp         | yolo predict source=image.webp    | WebP                            |\r\n| .pfm          | yolo predict source=image.pfm     | Portable FloatMap               |\r\n\r\n* Use of TIFF files is common in satellite imagery such as Sentinel-2. One may prefer to preserve the TIFF format over convert it to PNG/JPG because TIFF allows the storage of georeferencing information. \r\n* I see that the `load_yolo_annotations` uses `cv2.imread` to read the image files. [OpenCV seems to support](https://docs.opencv.org/4.x/d4/da8/group__imgcodecs.html#gacbaa02cffc4ec2422dfa2e24412a99e2) many of the Ultralytics-supported formats.\r\n\r\nhttps://github.com/roboflow/supervision/blob/1860fdb0a4e21edc5fa03d973e9f31c055bdcf4f/supervision/dataset/formats/yolo.py#L170\r\n\r\n### Proposals\r\n* P1: We can expand the hardcoded list of allowed formats.\r\n* P2: We may choose not to restrict the image format and let it fail later.\r\n\r\n### Use case\r\n\r\n* Everyone who'd like to use formats other than `png,` `jpg`, and `jpeg` will be able to use this extension.\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @patel-zeel :wave:\n\nThanks for giving a very specific example. That's invaluable to us.\n\nGenerally - sounds good!\n\nWhat we need to be careful about is silent failures. For example, `cv2.imread` does not raise an error when an image is not loaded - it simply returns `None`. Reading videos was similarly problematic, as it wouldn't tell of a missing codex. I bet the current code doesn't handle it all that well.\n\nNext, formats like TIFF may have image layers. We probably want the merged image in those cases. I don't know the peculiaritues of other models.\n\nGenerally, If you're aware of what's restricting us from having more image formats, I'm happy for us to limit those restrictions."
      },
      {
        "user": "patel-zeel",
        "body": "Hi @LinasKo,\r\n\r\n>cv2.imread does not raise an error when an image is not loaded - it simply returns None. Reading videos was similarly problematic, as it wouldn't tell of a missing codex. I bet the current code doesn't handle it all that well.\r\n\r\nI see. Is there any reason we are sticking to `cv2`? Isn't `from PIL import Image` more Pythonic? At least for the scope of this issue, we are loading the image in memory to know its dimensions. I can test `from PIL import Image` for some known issues that you might have come across already.\r\n\r\nhttps://github.com/roboflow/supervision/blob/1860fdb0a4e21edc5fa03d973e9f31c055bdcf4f/supervision/dataset/formats/yolo.py#L170-L173\r\n\r\n> Next, formats like TIFF may have image layers. We probably want the merged image in those cases. I don't know the peculiarities of other models.\r\n\r\nThat's true. We may also have to `assert image.shape[2] == 3`. Does `supervision`/`ultralytics` work with images with more or less than three channels? I checked that in the case of a multi-image TIFF file, `from PIL import Image` only loads the first image, which, in some way, takes away the burden of handling it at our end.\r\n\r\n```py\r\nfrom PIL import Image\r\n\r\n# Create and save a multi-image TIFF\r\nimg1 = Image.new('RGB', (111, 222), color='red')    # 100x100\r\nimg2 = Image.new('RGB', (333, 444), color='green')  # 200x150\r\nimg3 = Image.new('RGB', (555, 666), color='blue')   # 300x100\r\nimg1.save('/tmp/multi_image.tif', save_all=True, append_images=[img2, img3])\r\n\r\n# Read the multi-image TIFF\r\nimg = Image.open('/tmp/multi_image.tif')\r\nprint(img.size)\r\n# Output: (111, 222)\r\n```\r\n\r\nAlso, if an image is corrupt, the current code will generate the following error which is uninformative.\r\n```py\r\n---> h, w, _ = image.shape\r\n\r\nAttributeError: 'NoneType' object has no attribute 'shape'\r\n```\r\n\r\nOverall, if we can replace `cv2` with something else that raises an error on failure, we may lift the image format checker and let the image loader raise the error. If we need to use `cv2`, it seems challenging to raise a meaningful error with the user about why the image loading was unsuccessful. "
      },
      {
        "user": "LinasKo",
        "body": "`cv2` loads the image as a `numpy` array, which makes all subsequent computation easier. I'd strongly prefer to stick with `cv2` and avoid conversions from PIL every time.\r\n\r\nSupervision uses transparent images very infrequently, e.g. when loading images for the `IconAnnotator`. If possible, it would be great to keep the channel size flexible. However, if you spot `cv2.imread` with no extra parameters, it means the image is 2d or 3d. `cv2.IMREAD_UNCHANGED` param is required to read transparent images.\r\n\r\nI've tested it - calling an object detection model in ultralytics on an image with transparency raises a runtime error.\r\n```\r\nRuntimeError: Given groups=1, weight of size [16, 3, 3, 3], expected input[1, 4, 640, 640] to have 3 channels, but got 4 channels instead\r\n```\r\n\r\n> Also, if an image is corrupt, the current code will generate the following error which is uninformative.\r\n\r\nAgreed. After most imreads, we should check if the image is None and raise a `ValueError`. Because of opencv's popularity and convenience, I'd still prefer to stick to it, but we can raise ValueError(\"Failed to read image\") for an earlier warning\r\n\r\nThat's my line of thinking - hopefully it clears things up a little."
      }
    ]
  },
  {
    "issue_number": 1632,
    "title": "Getting segmentation fault when running InferenceSlicer with OBB model with thread_workers > 1",
    "author": "zxsk1974",
    "state": "open",
    "created_at": "2024-10-30T07:20:14Z",
    "updated_at": "2025-01-12T21:40:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n### Bug\r\n\r\nInferenceSlicer throws Segmentation fault with thread_workers = 4:\r\n\r\nSegmentation fault (core dumped)\r\n\r\n### Environment\r\n\r\nSupervision 0.24.0\r\nPython 3.11\r\n\r\n### Minimal Reproducible Example\r\n\r\n```python\r\nfrom ultralytics import YOLO\r\nimport cv2\r\nimport sys\r\nimport torch\r\nimport numpy as np\r\nfrom pathlib import Path\r\nimport math\r\nimport supervision as sv\r\n\r\ndef callback(image_slice: np.ndarray) -> sv.Detections:\r\n    result = model(image_slice, conf=0.6)\r\n    print(\"Results:\", result)\r\n    return sv.Detections.from_ultralytics(result[0])\r\n\r\nmodel = YOLO('yolo11x-obb.pt')\r\n\r\nprint(\"GPU:\", torch.cuda.is_available())\r\nif torch.cuda.is_available():\r\n    device = torch.device(\"cuda\")\r\n    print(\"Using GPU:\", torch.cuda.get_device_name())\r\n\r\n# load image\r\nfile_name = Path(sys.argv[1])\r\nimage = cv2.imread(sys.argv[1]) #Image.open(file_name, mode='r')\r\nprint(\"Loaded image:\", file_name)\r\n\r\nimage_wh = (image.shape[1], image.shape[0])\r\nslice_wh = (1024, 1024)\r\noverlap_ratio_wh = (0.2, 0.2)\r\noverlap_ratio_w, overlap_ratio_h = overlap_ratio_wh\r\nslice_w, slice_h = slice_wh\r\noverlap_wh = (math.ceil(slice_w * overlap_ratio_w), math.ceil(slice_h * overlap_ratio_h))\r\n\r\nslicer = sv.InferenceSlicer(\r\n  callback=callback,\r\n  overlap_filter=sv.OverlapFilter.NON_MAX_MERGE,\r\n  iou_threshold=0.15,\r\n  slice_wh=slice_wh,\r\n  overlap_ratio_wh=None,\r\n  overlap_wh=overlap_wh,\r\n  thread_workers=2\r\n)\r\n\r\ndetections = slicer(image)\r\n\r\nlabels = [\r\n    f\"{class_name} {confidence:.1f}\"\r\n    for class_name, confidence\r\n    in zip(detections['class_name'], detections.confidence)\r\n]\r\n\r\nlabel_annotator = sv.LabelAnnotator(text_scale=0.2, text_thickness=1, text_padding=0, text_position=sv.Position.TOP_LEFT)\r\nbbox_annotator = sv.BoxAnnotator(color=sv.ColorPalette.DEFAULT.colors[6], thickness=2)\r\nobb_annotator = sv.OrientedBoxAnnotator(color=sv.ColorPalette.DEFAULT.colors[6], thickness=2)\r\n\r\nprint(f\"Image shape: {image_wh[0]}w x {image_wh[1]}h\")\r\nprint(f\"Tile size: {slice_wh[0]}w x {slice_wh[1]}h\")\r\nprint(f\"Overlap: {overlap_wh[0]}w x {overlap_wh[1]}h. Ratio {overlap_ratio_wh}\")\r\nprint(f\"Overlap Filter: {sv.OverlapFilter.NON_MAX_MERGE}\")\r\nprint(f\"Found {len(detections)} objects\")\r\n\r\nannotated_image = obb_annotator.annotate(scene=image.copy(), detections=detections)\r\nannotated_image = label_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\r\n\r\ncv2.imwrite(file_name.stem + \"-output.jpg\", annotated_image)\r\n![airplane-graveyard-zoomed](https://github.com/user-attachments/assets/0bf79b23-4775-43f3-8573-bfe0e96cbf91)\r\n```\r\n\r\n### Additional\r\n\r\nTo reproduce:\r\n\r\n>python detect-image-slicer-bug.py airplane-graveyard-zoomed.jpg\r\n\r\n![airplane-graveyard-zoomed](https://github.com/user-attachments/assets/e4b34fdc-a03f-4fb0-8c2a-08553d97ecea)\r\n.jpg\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @zxsk1974 üëã \r\n\r\nThank you for reporting it. Unfortunately, many models aren't made to be run from multiple threads. We plan to shift to running in batches (linked PR), but it is unfortunately relatively low on the priority list."
      },
      {
        "user": "zxsk1974",
        "body": "@LinasKo batching will work too, can I try it on pre-release branch?"
      },
      {
        "user": "LinasKo",
        "body": "Not as a pre-release branch. I don't expect to merge this any time soon.\r\n\r\nHowever, I brought the branch up-to-date with the latest supervision version.\r\nFeel free to install via \r\n`pip install git+https://github.com/LinasKo/supervision.git@feature/batched-inference-slicer`.\r\n\r\nAlternatively, you may fork it from my [repo](https://github.com/LinasKo/supervision/tree/feature/batched-inference-slicer). Install it in the same way, but from your own namespace."
      }
    ]
  },
  {
    "issue_number": 1772,
    "title": "Detections loaded with `sv.DetectionDataset.from_yolo` annotate class IDs instead of class names with `sv.LabelAnnotator`",
    "author": "patel-zeel",
    "state": "open",
    "created_at": "2025-01-09T05:08:26Z",
    "updated_at": "2025-01-10T08:38:55Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\n##### What's the problem?\r\n1. We usually convert the inference results into `sv.Detections` with something like this:\r\n```py\r\nresults = model.infer(image)[0]\r\ndetections = sv.Detections.from_inference(results)\r\n```\r\n\r\nWhen we use `sv.LabelAnnotator` with these `detections`, we get the following output:\r\n![image](https://github.com/user-attachments/assets/84418073-a950-4d60-891a-efed97fa2e23)\r\n\r\n2. `sv.DetectionDataset.from_yolo` loads the dataset and stores the labels as `sv.Detections`\r\n```py\r\ndataset = sv.DetectionDataset.from_yolo(...)\r\n_, _, detections = dataset[0]\r\n```\r\n\r\nWhen we use `sv.LabelAnnotator` with these `detections`, we get the following output:\r\n![image](https://github.com/user-attachments/assets/d0212eae-44d6-4940-95f9-102f69b6bb47)\r\n\r\n##### What's the expected result?\r\n![image](https://github.com/user-attachments/assets/f6cf188c-a6db-4582-bbed-b20adcde55ce)\r\n\r\n##### How to fix the problem?\r\nPlease see the Minimal Reproducible Example.\r\n\r\n### Environment\r\n\r\n- Supervision 0.26.0rc3\r\n- Ubuntu 20.04.6 LTS\r\n- Python 3.10.15\r\n\r\n### Minimal Reproducible Example\r\n\r\nNote: Please run the code in a notebook to use functions such as `display`.\r\n\r\n##### Common code\r\n```py\r\nimport requests\r\nfrom io import BytesIO\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport supervision as sv\r\nfrom inference.models.utils import get_roboflow_model\r\n\r\n# Create a dummy dataset\r\ndata = requests.get(\"https://raw.githubusercontent.com/jigsawpieces/dog-api-images/main/pitbull/dog-3981033_1280.jpg\")\r\nimage = Image.open(BytesIO(data.content)).reduce(5)\r\nlabel = np.random.rand(1, 5) / 10 + 0.5\r\nlabel[:, 0] = 0\r\n!mkdir -p /tmp/dummy_dataset/images\r\n!mkdir -p /tmp/dummy_dataset/labels\r\nimage.save(\"/tmp/dummy_dataset/images/0.jpg\")\r\nnp.savetxt(\"/tmp/dummy_dataset/labels/0.txt\", label, fmt=\"%d %f %f %f %f\")\r\nwith open(\"/tmp/dummy_dataset/dataset.yml\", \"w\") as f:\r\n    f.write(\"\"\"train: _\r\nval: _\r\ntest: _\r\nnc: 1\r\nnames: [\"dummy\"]\"\"\")\r\n```\r\n\r\n##### Annotate `detections` from `sv.Detections.from_inference`\r\n```py\r\nmodel = get_roboflow_model(\"yolov8s-640\")\r\n_, image, _ = dataset[0]\r\nprediction = model.infer(image)[0]\r\ndetection = sv.Detections.from_inference(prediction)\r\nannotated_image = box_annotator.annotate(image.copy(), detection)\r\nannotated_image = label_annotator.annotate(annotated_image, detection)\r\ndisplay(Image.fromarray(annotated_image))\r\n```\r\n![image](https://github.com/user-attachments/assets/bbe69916-9f02-4637-bbf0-f95854052ea5)\r\n\r\n\r\n##### Annotate `detections` from `sv.DetectionDataset.from_yolo`\r\n```py\r\n# Load as supervision dataset\r\ndataset = sv.DetectionDataset.from_yolo(\"/tmp/dummy_dataset/images\", \"/tmp/dummy_dataset/labels\", \"/tmp/dummy_dataset/dataset.yml\")\r\n\r\n_, image, detection = dataset[0]\r\nbox_annotator = sv.BoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator()\r\nannotated_image = box_annotator.annotate(image.copy(), detection)\r\nannotated_image = label_annotator.annotate(annotated_image, detection)\r\ndisplay(Image.fromarray(annotated_image))\r\n```\r\n![image](https://github.com/user-attachments/assets/09f0cbec-97d8-4c03-b1a6-c0bbfc08ea5e)\r\n\r\n##### How to Fix?\r\nWe need to add the `data` in the detections.\r\n\r\n```py\r\n_, image, detection = dataset[0]\r\ndetection.data = {\"class_name\": np.array(['dummy'])}\r\nbox_annotator = sv.BoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator()\r\nannotated_image = box_annotator.annotate(image.copy(), detection)\r\nannotated_image = label_annotator.annotate(annotated_image, detection)\r\ndisplay(Image.fromarray(annotated_image))\r\n```\r\n![image](https://github.com/user-attachments/assets/a7e6e333-215c-42af-94e6-d4e5d6b6c6f2)\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @patel-zeel üëãüèª Another great find! This is indeed a bug. It looks like when loading labels using `sv.DetectionDataset.from_yolo`, we are not saving the `class_names` key in `sv.Detections.data`.\r\n\r\nThis is how `sv.LabelAnnotator` works. First, it checks if `sv.Detections` has the key `class_names`. If so, we print the label found in it; if not, we use `sv.Detections.class_id` as a fallback."
      },
      {
        "user": "patel-zeel",
        "body": "Thank you for the confirmation, @SkalskiP! I checked that `sv.DetectionDataset.from_coco` and `sv.DetectionDataset.from_pascal_voc` also have the same issue. In this case, would the following location be optimal to fix this bug?\r\n\r\nhttps://github.com/roboflow/supervision/blob/1b5c592abb7ef1a74d68dfd747efb32a102988e5/supervision/dataset/core.py#L84-L87\r\n\r\nProposed changes:\r\n```py\r\nself.annotations = annotations\r\nnp_classes = np.array(self.classes)\r\nfor annotation in self.annotations:\r\n    annotation.data[\"class_name\"] = np_classes[annotation.class_id]\r\n\r\n# Eliminate duplicates while preserving order\r\nself.image_paths = list(dict.fromkeys(images))\r\n```"
      }
    ]
  },
  {
    "issue_number": 1670,
    "title": "Problem with minimum matching threshold parameter of ByteTracker",
    "author": "rsnk96",
    "state": "open",
    "created_at": "2024-11-15T01:54:36Z",
    "updated_at": "2025-01-09T15:57:27Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nHi folks. Amazing project, but I'm getting a peculiar behaviour in ByteTracker. \r\n\r\nMy assumption for the `minimum_matching_threshold` parameter of ByteTracker is that it acts similar to an IoU threshold. A smaller threshold should make boxes match more easily, and a larger threshold should make boxes match only if they have a really good match score (ex: really high IoU). However, I observe the inverse behaviour. Not sure if this is expected, but thought I'll highlight it here\r\n\r\n### Environment\r\n\r\n- Supervision: 0.25.0\r\n- Ubuntu: 22.04\r\n- Python: 3.10\r\n\r\n### Minimal Reproducible Example\r\n\r\n\r\nCode block to reproduce:\r\n```python\r\nimport supervision as sv\r\nimport numpy as np\r\n\r\ndetections = [sv.Detections(xyxy=np.array([[10, 10, 20, 20]]),class_id=np.array([1]),confidence=np.array([1]))]*2\r\ndetections+= [sv.Detections(xyxy=np.array([[11, 11, 21, 21]]), class_id=np.array([1]), confidence=np.array([1]))]*2   # 90% overlap\r\n\r\nbyte_tracker_low_threshold = sv.ByteTrack(minimum_matching_threshold=0.1)\r\ntracked_detections = [byte_tracker_low_threshold.update_with_detections(d) for d in detections]\r\nprint(\"Track IDs associated with detections in 10\\% overlap: \", list(t_det.tracker_id for t_det in tracked_detections))\r\nprint(\"Internally tracked states in 10\\% overlap: \", byte_tracker_low_threshold.tracked_tracks)\r\n\r\nprint()\r\nprint()\r\n\r\nbyte_tracker_high_threshold = sv.ByteTrack(minimum_matching_threshold=0.9)\r\ntracked_detections = [byte_tracker_high_threshold.update_with_detections(d) for d in detections]\r\nprint(\"Track IDs associated with detections in 90\\% overlap: \", list(t_det.tracker_id for t_det in tracked_detections))\r\nprint(\"Internally tracked states in 90\\% overlap: \", byte_tracker_high_threshold.tracked_tracks)\r\n```\r\n\r\nGives the output:\r\n```\r\nTrack IDs associated with detections in 10\\% overlap:  [array([1]), array([1]), array([], dtype=int64), array([2])]\r\nInternally tracked states in 10\\% overlap:  [OT_1_(3-4)]\r\n\r\n\r\nTrack IDs associated with detections in 90\\% overlap:  [array([1]), array([1]), array([1]), array([1])]\r\nInternally tracked states in 90\\% overlap:  [OT_0_(1-4)]\r\n```\r\n\r\nI would expect the opposite to be true, i.e. when we set a low `minimum_matching_threshold`, it should assign the same track ID to detections more easily (with less IoU overlap). However, that doesn't seem to be the case.\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "rolson24",
        "body": "Hi there @rsnk96!\r\n\r\nThis is the expected behavior of this threshold, although you are correct that it is poorly named. What it really represents in the code is the MAXIMUM allowed COST of matching a track to a new box, or you can think of it as the MINIMUM allowed SCORE (1 - cost) of matching a track to a new box. "
      }
    ]
  },
  {
    "issue_number": 1449,
    "title": "PolygonZone: estimate how much of the zone is occupied",
    "author": "stevensagaert",
    "state": "open",
    "created_at": "2024-08-14T14:58:04Z",
    "updated_at": "2025-01-08T14:35:07Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Description\r\n\r\nNext to the count of the objects in the zone, it would be useful to know how much of the zone area is occupied by the objects.\r\n\r\n### Use case\r\n\r\ne.g. for loading zones in warehouses\r\n\r\n### Additional\r\n\r\nBasically I've implemented it myself outside of the PolygonZone class. The core of what is needed is these two functions:\r\n\r\n```python\r\ndef polygon_to_binary_mask(img,contour):\r\n    \"\"\"convert bounding polygon coord to binary mask\"\"\"\r\n    # Create binary mask\r\n    b_mask = np.zeros(img.shape[:2], np.uint8)\r\n\r\n    #  Extract contour result\r\n    #contour = c.masks.xy.pop()\r\n    #  Changing the type\r\n    contour = contour.astype(np.int32) #Note: This must be int32 and not unit8!!!\r\n    #  Reshaping\r\n    contour = contour.reshape(-1, 1, 2)\r\n\r\n\r\n    # Draw contour onto mask\r\n    mask = cv2.drawContours(b_mask, [contour], -1, (1, 1, 1), cv2.FILLED)\r\n    return mask\r\n```\r\n\r\n#### strategy: \r\n1) make union of binary masks of detected objects (only the classes for consideration in zone)\r\n2) make binary mask of the zone\r\n3) do and of zone mask & object masks\r\n4) sum the 1 pixels\r\n\r\n```python\r\ndef calculate_overlap_area(zone_mask, masks,show_plot=True):\r\n    \"\"\"calculate how much % of the zone is occupied\"\"\"\r\n    #create one mask from the object masks as the union\r\n    union_mask=np.bitwise_or.reduce(np.stack(masks),axis=0)\r\n    #for debugging\r\n    if show_plot:\r\n        plt.title(\"union object mask\")\r\n        plt.imshow(union_mask,cmap='gray')\r\n        plt.show()\r\n    \r\n\r\n    #do the bitwise and between union_mask & zone_mask\r\n    overlap_mask=np.bitwise_and(union_mask,zone_mask)\r\n    print(f\"overlap_mask shape {overlap_mask.shape}\")\r\n    if show_plot:\r\n        plt.title(\"overlap mask\")\r\n        plt.imshow(union_mask,cmap='gray')\r\n        plt.show()\r\n    \r\n    overlap_size= np.sum(overlap_mask)\r\n    zone_size=np.sum(zone_mask)\r\n    return 100*overlap_size/zone_size\r\n```\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "Sapienscoding",
        "body": "Hi @stevensagaert, I'd like to help by submitting a PR"
      },
      {
        "user": "LinasKo",
        "body": "Hi @Sapienscoding,\r\n\r\nI've assigned another issue to you. Let's go with one at a time.\r\n"
      },
      {
        "user": "LinasKo",
        "body": "Hi @stevensagaert üëã \r\n\r\nApologies for the delay. If you're still keen, we'd be happy to include area occupation stats in our PolygonZone."
      }
    ]
  },
  {
    "issue_number": 1769,
    "title": "Include additional file format in load_yolo_annotations",
    "author": "pirnerjonas",
    "state": "closed",
    "created_at": "2025-01-07T14:31:50Z",
    "updated_at": "2025-01-08T09:29:23Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi, \r\n\r\nI wanted to load my yolo dataset which uses `.bmp` images via `sv.DetectionDataset.from_yolo()`. Currently only `[\"jpg\", \"jpeg\", \"png\"]` are allowed in [load_yolo_annotations](https://github.com/roboflow/supervision/blob/develop/supervision/dataset/formats/yolo.py#L156). I just tested to include `.bmp` in the list and it seemed to work. Would it be possible to extend the available file formats?\r\n\r\nAlso I think yolo allows [even more image formats](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/utils.py#L38)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "patel-zeel",
        "body": "Hi, @pirnerjonas, thanks for raising this issue. I am happy to see that my PR is likely to help people beyond myself :) This issue is discussed in #1554 and will be resolved by #1636. In summary, once the PR is merged, `supervision` will allow all image formats supported by Pillow (restricted to RGB or grayscale)."
      },
      {
        "user": "pirnerjonas",
        "body": "Hi @patel-zeel, sorry I missed that issue. Sounds great, thanks for the response. Looking forward to having this in supervision."
      },
      {
        "user": "SkalskiP",
        "body": "Hi @pirnerjonas üëãüèª thanks for bringing this issue to our attention. Given that @patel-zeel is already working on a fix, let's close this issue."
      }
    ]
  },
  {
    "issue_number": 369,
    "title": "Possible issue in precision converting annotations with \"force_mask=True\"",
    "author": "josephofiowa",
    "state": "open",
    "created_at": "2023-09-20T06:37:02Z",
    "updated_at": "2025-01-03T17:46:58Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nI'm cross posting an issue reported on Roboflow forum with regards to supervision `force_mask` condition: https://discuss.roboflow.com/t/masks-become-crooked-and-noisy-after-passing-through-supervision-library/3269\r\n\r\n-------------------------\r\nI am working on Yolov8 and SAM. I am trying to use the same annotations for both the models.\r\n\r\nWe wanted to convert the annotations that we got done on your platform, to masks to train SAM. However, when I use the code that uses supervision library for(pasting it below). My masks are crooked and not like how they are annotated.\r\n\r\n##  treat all labels as masks/polygons (vs how ultralytics converts some to bboxes)\r\n\r\n# import supervision (open source set of cvutils)\r\nimport supervision as sv\r\n# grab our data\r\nproject = rf.workspace(\"\").project(\"\")\r\ndataset = project.version(3).download(\"yolov8\")\r\n\r\n# for each image, load YOLO annotations and require mask format for each\r\nfor subset in [\"train\", \"test\", \"valid\"]:\r\n    ds = sv.DetectionDataset.from_yolo(\r\n        images_directory_path=f\"{dataset.location}/{subset}/images\",\r\n        annotations_directory_path=f\"{dataset.location}/{subset}/labels\",\r\n        data_yaml_path=f\"{dataset.location}/data.yaml\",\r\n        force_masks=True\r\n    )\r\n    ds.as_yolo(annotations_directory_path=f\"{dataset.location}/{subset}/labels\")\r\nAfter doing this, I took a look at the txt file before and after.\r\nThis is before:\r\n0 0.24367703857421874 0.5947080708007813 0.32577574609375 0.5946408608398438 0.3256521215820313 0.584001533203125 0.38616494580078126 0.5838778208007812 0.38616494580078126 0.44245848974609375 0.24366789990234375 0.4424227666015625 0.24367703857421874 0.5947080708007813\r\n0 0.4681246420898437 0.576236357421875 0.5384933212890625 0.5761529790039063 0.5382433774414063 0.4423616875 0.3912994501953125 0.4423616875 0.39114250390625 0.594635390625 0.4681246420898437 0.5945798715820313 0.4681246420898437 0.576236357421875\r\n0 0.19015809912109374 0.532453453125 0.179838498046875 0.5323361000976562 0.17960396533203124 0.49474487890625 0.049611380859375 0.49474487890625 0.04761782568359375 0.4960357807617187 0.02815130517578125 0.49615313818359374 0.028172552734375 0.6419591596679688 0.19027537451171875 0.6423729399414062 0.19015809912109374 0.532453453125\r\n0 0.20617587744140625 0.6494069682617187 0.11866647216796875 0.649729455078125 0.11880823681640625 0.6924327993164062 0.09655086279296875 0.6925746689453125 0.0968343916015625 0.65015507421875 0.0251399677734375 0.6496627353515625 0.024852654296875 0.8023033012695312 0.24352058984375 0.8023752939453125 0.24407509423828125 0.6602119057617187 0.20595053466796875 0.6604566748046875 0.20617587744140625 0.6494069682617187\r\n0 0.11568 0.6509393481445312 0.100116763671875 0.6509393481445312 0.10020867333984375 0.6883252021484375 0.1157992568359375 0.6883736821289063 0.11568 0.6509393481445312\r\nThis is after:\r\n0 0.24365 0.44238 0.24365 0.59424 0.32568 0.59424 0.32568 0.58936 0.32520 0.58887 0.32520 0.58447 0.32568 0.58398 0.35547 0.58398 0.35596 0.58350 0.38574 0.58350 0.38574 0.44238\r\n0 0.39111 0.44189 0.39111 0.59424 0.46777 0.59424 0.46777 0.57666 0.46826 0.57617 0.50293 0.57617 0.50342 0.57568 0.53809 0.57568 0.53809 0.44189\r\n0 0.04932 0.49463 0.04883 0.49512 0.04834 0.49512 0.04785 0.49561 0.03809 0.49561 0.03760 0.49609 0.02783 0.49609 0.02783 0.64160 0.10889 0.64160 0.10938 0.64209 0.18994 0.64209 0.18994 0.53223 0.18018 0.53223 0.17969 0.53174 0.17969 0.51367 0.17920 0.51318 0.17920 0.49463\r\n0 0.16260 0.64893 0.16211 0.64941 0.11865 0.64941 0.11865 0.69189 0.11816 0.69238 0.09668 0.69238 0.09619 0.69189 0.09619 0.67139 0.09668 0.67090 0.09668 0.64990 0.06104 0.64990 0.06055 0.64941 0.02490 0.64941 0.02490 0.72559 0.02441 0.72607 0.02441 0.80225 0.24316 0.80225 0.24316 0.73145 0.24365 0.73096 0.24365 0.66016 0.20605 0.66016 0.20557 0.65967 0.20557 0.65479 0.20605 0.65430 0.20605 0.64893\r\n0 0.10010 0.65088 0.10010 0.68799 0.11572 0.68799 0.11572 0.66992 0.11523 0.66943 0.11523 0.65088\r\n0 0.24854 0.64941 0.24854 0.80176 0.39014 0.80176 0.39014 0.73145 0.39062 0.73096 0.39062 0.66016 0.33008 0.66016 0.32959 0.65967 0.32959 0.64941\r\n\r\nYou can see that the values change quite a lot. However the problem is when I convert these into masks.\r\n\r\nright side is the mask visualized before passing to sv library. left side is after passing through library. You can see my mask on the left is crooked, and not straight. This is due to int rounding, I guess. I am not sure.\r\nI wanted some clarity on why the annotation values in txt change so much after using supervision. Also, I can generate the masks without passing to sv and get done for the use case of SAM. However, now that we are going to feed these to YOLOv8(I need to use sv as there is an ultralytics mism\r\n![image (48)](https://github.com/roboflow/supervision/assets/13970811/f0dcff8b-4d13-4364-9acf-d7e28fb18ede)\r\natch problem), the model could suffer due to the 1-2 pixel error that is happening.\n\n### Environment\n\nsupervision 0.14.0\n\n### Minimal Reproducible Example\n\n```##  treat all labels as masks/polygons (vs how ultralytics converts some to bboxes)\r\n\r\n# import supervision (open source set of cvutils)\r\nimport supervision as sv\r\n# grab our data\r\nproject = rf.workspace(\"\").project(\"\")\r\ndataset = project.version(3).download(\"yolov8\")\r\n\r\n# for each image, load YOLO annotations and require mask format for each\r\nfor subset in [\"train\", \"test\", \"valid\"]:\r\n    ds = sv.DetectionDataset.from_yolo(\r\n        images_directory_path=f\"{dataset.location}/{subset}/images\",\r\n        annotations_directory_path=f\"{dataset.location}/{subset}/labels\",\r\n        data_yaml_path=f\"{dataset.location}/data.yaml\",\r\n        force_masks=True\r\n    )\r\n    ds.as_yolo(annotations_directory_path=f\"{dataset.location}/{subset}/labels\")\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "0xD4rky",
        "body": "hey, I would like to take this issue up and solve the bug. I would raise a PR soon, please allow me to do so team!"
      },
      {
        "user": "LinasKo",
        "body": "Hi @0xD4rky :wave:\n\nI'm happy for you to take a look at it. Let me know how it goes!"
      }
    ]
  },
  {
    "issue_number": 1762,
    "title": "Extreme memory usage loading OBB dataset",
    "author": "patel-zeel",
    "state": "open",
    "created_at": "2025-01-01T17:41:18Z",
    "updated_at": "2025-01-01T17:42:52Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nI tried loading the DOTAv1 dataset processed by Ultralytics from this URL: https://github.com/ultralytics/assets/releases/download/v0.0.0/DOTAv1.zip with `sv.DetectionDataset.from_yolo` but it was unsuccessful on Google colab. When I loaded a subset of 100 instances on our compute server, it consumed nearly 36 GB of memory. Free Google colab instance comes with ~12 GB memory only so it was bound to fail there.\r\n\r\nI investigated further and found the following reason for the extreme memory usage:\r\n1. `_with_mask` function is called at the following position and it sets `with_masks` argument to `True` for OBB.\r\nhttps://github.com/roboflow/supervision/blob/77ae6821207eaea4322ed91b8fee9ca55ac8ff45/supervision/dataset/formats/yolo.py#L175\r\n2. `with_masks` argument is then passed to the following function with other arguments:\r\nhttps://github.com/roboflow/supervision/blob/77ae6821207eaea4322ed91b8fee9ca55ac8ff45/supervision/dataset/formats/yolo.py#L76-L81\r\n3. Due to the effect of the `with_masks` argument, the following lines are executed:\r\nhttps://github.com/roboflow/supervision/blob/77ae6821207eaea4322ed91b8fee9ca55ac8ff45/supervision/dataset/formats/yolo.py#L120-L121\r\n4. The 2D shape of the `mask` is the same as the (w, h) resolution of the image. If an image has N detections, the `mask` shape is (N, w, h), which is not space-friendly. \r\n\r\nThe size of the mask is the main reason for extreme storage usage.\r\n\r\n### Environment\r\n\r\nGoogle colab:\r\n- Supervision: 0.25.1\r\n- Python: 3.10.12\r\n- Ubuntu: 22.04\r\n\r\n### Minimal Reproducible Example\r\n\r\n[Colab](https://colab.research.google.com/drive/1wG4GEOYSX9uRzMhwUS3vMqxuVufwivYX?usp=sharing) to reproduce the results.\r\n\r\n### Additional\r\n\r\nI have not taken a deeper look, but a naive solution could be to generate masks only when needed and then discard them for efficient memory usage.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": []
  },
  {
    "issue_number": 1760,
    "title": "OBB support for ConfusionMatrix",
    "author": "patel-zeel",
    "state": "open",
    "created_at": "2024-12-30T15:07:17Z",
    "updated_at": "2024-12-30T15:07:17Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\n#1562 discusses the OBB support for Metrics in detail, and #1593 added OBB support for mAP, Precision, Recall, and F1 Score. I see that `ConfusionMatrix` still lacks this support.\r\n\r\n##### Why is it important?\r\nCurrently, anyone using `ConfusionMatrix` with OBB may assume that IoU is computed for the OBBs, but actually, IoU is being computed for the derived axis-aligned bounding boxes.\r\n\r\nhttps://github.com/roboflow/supervision/blob/77ae6821207eaea4322ed91b8fee9ca55ac8ff45/supervision/metrics/detection.py#L299\n\n### Use case\n\nIt will help anyone working with OBBs to compute ConfusionMatrix correctly.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": []
  },
  {
    "issue_number": 1758,
    "title": "Supervision support format",
    "author": "ssvicnent",
    "state": "closed",
    "created_at": "2024-12-27T08:39:21Z",
    "updated_at": "2024-12-27T12:42:55Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nDoes supervision only support three model format(inference\\ultralytics\\transformers)?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "Hello @ssvicnent üëã  No, we support more than 3 of them \r\n\r\nFor Detections : https://supervision.roboflow.com/latest/detection/core/\r\nFor Keypoints: https://supervision.roboflow.com/latest/keypoint/core/\r\n\r\nI hope that helps"
      }
    ]
  },
  {
    "issue_number": 1715,
    "title": "`move_masks` only supports movement in positive direction",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-12-06T09:17:48Z",
    "updated_at": "2024-12-16T14:08:27Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "body": "If you compare the code of `move_masks`, `move_detections` and `move_oriented_boxes`, you'll find that only mask code restricts offset direction:\r\n\r\n```python\r\n    if offset[0] < 0 or offset[1] < 0:\r\n        raise ValueError(f\"Offset values must be non-negative integers. Got: {offset}\")\r\n```\r\n\r\nIt should be possible to move masks in either direction, even if it results in cropping.\r\n\r\nTo complete this:\r\n- [ ] Change the code so masks can be moved with negative offset\r\n- [ ] Create a unit test suite for `move_masks`\r\n\r\n---\r\n\r\nIt would help us immensely and speed up the review process if you could create a [Colab](https://colab.research.google.com/) showcasing the changes, but for this task it is optional. You may use the [Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89?usp=sharing).",
    "comments": [
      {
        "user": "Anirudh2112",
        "body": "Hi @LinasKo, I would like to contribute to this. Can you assign it to me?"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @Anirudh2112! I just reviewed your code. I made a few changes to take care of an edge case, and I think we‚Äôre ready to merge."
      },
      {
        "user": "Anirudh2112",
        "body": "@SkalskiP Thank you for reviewing and merging the changes! I really enjoyed contributing to Supervision."
      }
    ]
  },
  {
    "issue_number": 844,
    "title": "[PolygonZone] - allow `triggering_position` to be `Iterable[Position]`",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-02-02T17:51:07Z",
    "updated_at": "2024-12-12T09:37:48Z",
    "labels": [
      "enhancement",
      "Q1.2024",
      "api:polygonzone"
    ],
    "body": "### Description\r\n\r\nUpdate [`PolygonZone`](https://github.com/roboflow/supervision/blob/87a4927d03b6d8ec57208e8e3d01094135f9c829/supervision/detection/tools/polygon_zone.py#L15) logic, triggering the zone when not one anchor but multiple anchors are inside the zone. This type of logic is already implemented in [`LineZone`](https://github.com/roboflow/supervision/blob/87a4927d03b6d8ec57208e8e3d01094135f9c829/supervision/detection/line_counter.py#L12).\r\n\r\n- Rename `triggering_position` to `triggering_anchors` to be consistent with the `LineZone` naming convention.\r\n- `Update type of argument from `Position` to `Iterable[Position]\r\n- Maintain the default behavior. The zone should, by default, be triggered by `Position.BOTTOM_CENTER`.\r\n\r\n### API\r\n\r\n```python\r\nclass PolygonZone:\r\n\r\n    def __init__(\r\n        self,\r\n        polygon: np.ndarray,\r\n        frame_resolution_wh: Tuple[int, int],\r\n        triggering_anchors: Iterable[Position] = (Position.BOTTOM_CENTER, )\r\n    ):\r\n        pass\r\n\r\n    def trigger(self, detections: Detections) -> np.ndarray:\r\n        pass\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "LeviVasconcelos",
        "body": "Hi, I would like to work on this."
      },
      {
        "user": "SkalskiP",
        "body": "Hi @LeviVasconcelos üëãüèª Go for it! Let us know if you need help."
      },
      {
        "user": "LeviVasconcelos",
        "body": "Hi @SkalskiP,\r\n\r\nPR is ready for review, let me know what you think when you find time :).\r\n\r\nBest,"
      }
    ]
  },
  {
    "issue_number": 1728,
    "title": "LineZone does not match the actual situation",
    "author": "DreamerYinYu",
    "state": "open",
    "created_at": "2024-12-11T02:28:44Z",
    "updated_at": "2024-12-11T02:28:44Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHello, I am using LineZone and testing \"vehicles. mp4\". The document shows \"line_zone. in_comunt\", Line_zone. out_comunt is 7, 2, but I used the same code to display 3 and 4. May I ask if my usage is incorrect? thank you\r\n\r\n![20241211102824](https://github.com/user-attachments/assets/9254f8c4-f3ac-44d9-82f5-cd13335f4e09)\r\n\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1719,
    "title": "How to call YOLO's built-in API?",
    "author": "DreamerYinYu",
    "state": "closed",
    "created_at": "2024-12-09T03:09:02Z",
    "updated_at": "2024-12-10T08:44:33Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI want to call yolo's model.track so that I can use GPU or other yolo interfaces through yolo. Is this possible? thank you\r\n![20241209110527](https://github.com/user-attachments/assets/372d6bcb-5b40-416d-b9c8-fd319cd946d8)\r\n\r\n\r\nimport cv2\r\nimport supervision\r\nfrom ultralytics import YOLO\r\n\r\nmodel = YOLO(\"yolov8n.pt\")\r\nimage = cv2.imread(\"videos/img.png\")\r\n\r\n#results = model(image)[0]\r\nresults = model.track(image, persist=True, classes=[0], verbose=False, device='0')[0]\r\n\r\ndetections = supervision.Detections.from_ultralytics(results)\r\n\r\n\r\n\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @DreamerYinYu üëã \r\n\r\nThis is part of Ultralytics, so you should ask them in their forums.\r\n\r\nHowever, we have the same tracker (`ByteTrack`) that you can use on any `Detections` object. \r\n\r\nHere's a step-by-step implementation:\r\n* https://supervision.roboflow.com/latest/how_to/track_objects/\r\n"
      }
    ]
  },
  {
    "issue_number": 1707,
    "title": "Implement metrics comparison table & plotting",
    "author": "LinasKo",
    "state": "open",
    "created_at": "2024-12-03T10:05:11Z",
    "updated_at": "2024-12-09T21:44:33Z",
    "labels": [
      "enhancement"
    ],
    "body": "The metrics system allows the users to compute a metrics result - a class with values for a specific metrics run.\r\n\r\nWhen comparing multiple models, a natural next step is to aggregate the results into a single table and/or plot them on a single chart.\r\n\r\nLet's make this step easy!\r\n\r\nI propose two new functions:\r\n```python\r\ndef aggregate_metric_results(metrics_results: List[MetricResult], *,  include_object_sizes=False) -> pd.DataFrame:\r\n   \"\"\"\r\n   Raises when different types of metrics results are passed in\r\n   \"\"\"\r\n\r\ndef plot_aggregate_metric_results(metrics_results: List[MetricResult], *, include_object_sizes=False) -> None:\r\n    ...\r\n\r\n\r\nclass MetricResult(ABC):\r\n   @abstractmethod\r\n   def to_pandas():\r\n      raise NotImplementedError()\r\n\r\n   def plot():\r\n      raise NotImplementedError()\r\n```\r\n\r\n---\r\n\r\nSeveral questions we need to address:\r\n1. Would it be better for `plot_aggregate_metric_results` to take in a `pd.DataFrame`? This way, the user can apply their own sorting or preprocessing, but we're not guaranteed to have the fields we need.\r\n2. Could the naming be improved?\r\n\r\n---\r\n\r\nSuggested by @David-rn, [discussion](https://github.com/roboflow/supervision/discussions/1706#discussioncomment-11445217)\r\n",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. You may use the [Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89). The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! :pray:"
      },
      {
        "user": "David-rn",
        "body": "Regarding the first question and considering [the guide](https://supervision.roboflow.com/develop/how_to/benchmark_a_model/#f1-score) that led to this discussion, I think that maybe it is a better option using a `List[MetricResult]` as input as first suggested, since the user would already have used supervision classes (`Detections`, `MetricResult`). \r\n\r\nIf you think it's a good idea, I can give it a try these days and share a Colab to evaluate whether it is feasible or there are more things to consider."
      },
      {
        "user": "LinasKo",
        "body": "That'd be most helpful @David-rn ! I'm assigning it to you."
      }
    ]
  },
  {
    "issue_number": 1720,
    "title": "The character's ID changed after a brief loss",
    "author": "DreamerYinYu",
    "state": "closed",
    "created_at": "2024-12-09T11:06:03Z",
    "updated_at": "2024-12-09T11:12:54Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHello, I am using supervision ByteTrack, in the video, the male ID is 2 and the female ID is 3. However, when the female continues to move forward and the ID is lost, when it is detected again, the female ID is no longer 3, and the male and female IDs alternate. What should I do?Thank you\r\n\r\nMale ID: 2\r\nGirl ID: 3\r\n![1](https://github.com/user-attachments/assets/a6143eff-381d-4aaf-82e4-db49cd0c4af4)\r\n\r\n![2](https://github.com/user-attachments/assets/dacf22d4-a364-4d46-8c65-9ab47c8af72e)\r\n\r\nMale ID: 3\r\nFemale ID: 2\r\n![3](https://github.com/user-attachments/assets/475f6e81-9568-49fc-bc15-fb278a6014e0)\r\n\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "DreamerYinYu",
        "body": "codeÔºö\r\n\r\n\r\nimport numpy\r\nimport supervision\r\nfrom ultralytics import YOLO\r\n\r\nmodel = YOLO(\"yolo11n.pt\")\r\ntracker = supervision.ByteTrack(track_activation_threshold=0.8,minimum_matching_threshold=0.9)\r\nbox_annotator = supervision.BoxAnnotator()\r\nlabel_annotator = supervision.LabelAnnotator(text_scale=1.5)\r\ntrace_annotator = supervision.TraceAnnotator()\r\n\r\ndef callback(frame: numpy.ndarray, _: int) -> numpy.ndarray:\r\n    results = model(frame)[0]\r\n    detections = supervision.Detections.from_ultralytics(results)\r\n    detections = tracker.update_with_detections(detections)\r\n\r\n    labels = [\r\n        f\"#{tracker_id} {results.names[class_id]}\"\r\n        for class_id, tracker_id\r\n        in zip(detections.class_id, detections.tracker_id)\r\n    ]\r\n\r\n    annotated_frame = box_annotator.annotate(\r\n        frame.copy(), detections=detections)\r\n    annotated_frame = label_annotator.annotate(\r\n        annotated_frame, detections=detections, labels=labels)\r\n    return trace_annotator.annotate(\r\n        annotated_frame, detections=detections)\r\n\r\nsupervision.process_video(\r\n    source_path=\"videos/07.mp4\",\r\n    target_path=\"result.mp4\",\r\n    callback=callback\r\n)"
      }
    ]
  },
  {
    "issue_number": 1718,
    "title": "May I ask how to use the botsort tracker?",
    "author": "DreamerYinYu",
    "state": "closed",
    "created_at": "2024-12-09T03:03:06Z",
    "updated_at": "2024-12-09T09:30:58Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nMay I ask how to use the botsort tracker?Thank you!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @DreamerYinYu üëã \r\n\r\nWe don't have `BotSort` implemented at the moment.\r\n\r\nHowever `sv.Detections` is a straightforward object and you should be able to get it working with some fiddling around. Each `Detections` basically has N-sized arrays `xyxy`, `mask`, `confidence`, `class_id`, `tracker_id`.\r\n\r\nYou need to create a method that takes `Detections` runs `BotSort` and updates the `tracker_id` value, assigning an `int` ID to each detected object.\r\n\r\nBest of luck!\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1695,
    "title": "InferenceSlicer threaded implementation slower than obss/sahi",
    "author": "iokarkan",
    "state": "closed",
    "created_at": "2024-11-28T11:57:42Z",
    "updated_at": "2024-12-04T10:17:44Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nI was trying to establish the performance boost of supervision's SAHI implementation in InferenceSlicer with many worker threads against obss/sahi original implementation, and I cooked up this script below to compare the two.\r\n\r\nIn my test, It appears that supervision is slower doing one iteration for 256x256 slices of a 1024x527 sample image and using various values for the worker threads. I am skipping some warmup runs, and I am avoiding overlap in both cases.\r\n\r\nI believe obss/sahi is single threaded, therefore having worker threads should help.\r\n\r\nIndicatively, for 4 worker threads, I get:\r\n```\r\n{'Implementation': ['obss/sahi', 'supervision'], 'Inference Time (s)': [0.4129594915053424, 1.240290361292222]}\r\n```\r\n\r\nAs an aside, I'm also getting verbose SuperVision inference output that I can't find how to disable, but it shouldn't play too big of a role):\r\n```\r\n0: 416x640 1 tie, 1 vase, 56.9ms\r\nSpeed: 1.7ms preprocess, 56.9ms inference, 22.2ms postprocess per image at shape (1, 3, 416, 640)\r\n```\r\n\r\n### Environment\r\n\r\n- OS Ubuntu 22.04\r\n- python 3.10.12\r\n- requirements\r\n```\r\ncertifi==2024.8.30\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\ncontourpy==1.3.1\r\ncycler==0.12.1\r\ndefusedxml==0.7.1\r\nfilelock==3.16.1\r\nfire==0.7.0\r\nfonttools==4.55.0\r\nfsspec==2024.10.0\r\nidna==3.10\r\nimagecodecs==2024.9.22\r\nimageio==2.36.0\r\nJinja2==3.1.4\r\nkiwisolver==1.4.7\r\nlazy_loader==0.4\r\nMarkupSafe==3.0.2\r\nmatplotlib==3.9.2\r\nmpmath==1.3.0\r\nnetworkx==3.4.2\r\nnumpy==1.26.4\r\nnvidia-cublas-cu12==12.4.5.8\r\nnvidia-cuda-cupti-cu12==12.4.127\r\nnvidia-cuda-nvrtc-cu12==12.4.127\r\nnvidia-cuda-runtime-cu12==12.4.127\r\nnvidia-cudnn-cu12==9.1.0.70\r\nnvidia-cufft-cu12==11.2.1.3\r\nnvidia-curand-cu12==10.3.5.147\r\nnvidia-cusolver-cu12==11.6.1.9\r\nnvidia-cusparse-cu12==12.3.1.170\r\nnvidia-nccl-cu12==2.21.5\r\nnvidia-nvjitlink-cu12==12.4.127\r\nnvidia-nvtx-cu12==12.4.127\r\nopencv-python==4.9.0.80\r\npackaging==24.2\r\npandas==2.2.3\r\npillow==11.0.0\r\npsutil==6.1.0\r\npy-cpuinfo==9.0.0\r\npybboxes==0.1.6\r\npyparsing==3.2.0\r\npython-dateutil==2.9.0.post0\r\npytz==2024.2\r\nPyYAML==6.0.2\r\nrequests==2.32.3\r\nsahi==0.11.18\r\nscikit-image==0.24.0\r\nscipy==1.14.1\r\nseaborn==0.13.2\r\nshapely==2.0.6\r\nsix==1.16.0\r\nsupervision==0.25.0\r\nsympy==1.13.1\r\ntermcolor==2.5.0\r\nterminaltables==3.1.10\r\nthop==0.1.1.post2209072238\r\ntifffile==2024.9.20\r\ntorch==2.5.1\r\ntorchvision==0.20.1\r\ntqdm==4.67.1\r\ntriton==3.1.0\r\ntyping_extensions==4.12.2\r\ntzdata==2024.2\r\nultralytics==8.1.27\r\nurllib3==2.2.3\r\n```\r\n\r\n### Minimal Reproducible Example\r\n\r\n```\r\nfrom ultralytics import YOLO\r\nfrom sahi.auto_model import AutoDetectionModel\r\nfrom sahi.predict import get_sliced_prediction\r\nfrom supervision import InferenceSlicer, Detections\r\nimport time\r\nfrom PIL import Image\r\nimport cv2\r\nimport numpy as np\r\n\r\nsample_image_path = \"image.jpg\"\r\nmodel_path = \"yolov8n.pt\"\r\n\r\nyolo_model = YOLO(model_path, verbose=False)\r\n\r\n\r\n# Function to measure obss/sahi inference\r\ndef run_obss_sahi_inference(image_path, model):\r\n    detection_model = AutoDetectionModel.from_pretrained(\r\n        model_type=\"yolov8\", model_path=model_path, confidence_threshold=0.25\r\n    )\r\n    times = []\r\n    for _ in range(20):\r\n        start_time = time.time()\r\n        result = get_sliced_prediction(\r\n            image_path,\r\n            detection_model=detection_model,\r\n            postprocess_match_metric=\"IOU\",\r\n            postprocess_match_threshold=0.1,\r\n            slice_height=256,\r\n            slice_width=256,\r\n            overlap_height_ratio=0.0,\r\n            overlap_width_ratio=0.0,\r\n        )\r\n        end_time = time.time()\r\n        times.append(end_time - start_time)\r\n    return result, sum(times[3:]) / len(times[3:])\r\n\r\n\r\n# Function to measure supervision inference\r\ndef run_supervision_inference(image, model):\r\n    def callback(image_slice: np.ndarray) -> Detections:\r\n        result = model(image_slice, conf=0.25, iou=0.1, device=0)[0]\r\n        return Detections.from_ultralytics(result)\r\n\r\n    inference_slicer = InferenceSlicer(\r\n        callback=callback,\r\n        slice_wh=(256, 256),\r\n        overlap_wh=None,\r\n        thread_workers=4,\r\n    )\r\n    times = []\r\n    for _ in range(20):\r\n        start_time = time.time()\r\n        detections = inference_slicer(image)\r\n        end_time = time.time()\r\n        times.append(end_time - start_time)\r\n    return detections, sum(times[3:]) / len(times[3:])\r\n\r\n\r\ndef main():\r\n    img_array = cv2.imread(sample_image_path)\r\n    obss_result, obss_time = run_obss_sahi_inference(img_array, yolo_model)\r\n    supervision_result, supervision_time = run_supervision_inference(\r\n        img_array, yolo_model\r\n    )\r\n\r\n    comparison_results = {\r\n        \"Implementation\": [\"obss/sahi\", \"supervision\"],\r\n        \"Inference Time (s)\": [obss_time, supervision_time],\r\n    }\r\n\r\n    print(comparison_results)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Additional\r\n\r\n![image](https://github.com/user-attachments/assets/6b7f5d33-19ca-46e8-a853-38b4392c584b)\r\n\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @iokarkan üëã \r\n\r\nThank you for a thorough report. It does not surprise me, as threads work weirdly when involving vision models / access to GPU. We'll look into it, but it might take some time.\r\n\r\nMeanwhile, if you're keen on speed, we have an implementation that runs inference in bulk, on a GPU. It's not up-to-date, but if urgent, you might be able to hack it with a custom `InferenceSlicer` class or some monkeypatching.\r\n\r\nhttps://github.com/roboflow/supervision/pull/1239\r\n\r\nAgain, thank you. It's wonderful to receive such a thorough report, with full reproduction steps."
      },
      {
        "user": "iokarkan",
        "body": "Hi @LinasKo, thanks for the answer.\r\n\r\nI'm interested in real-time scenarios, so single image per-iteration. However if batching refers to sending the all component slice (and/or original image, like SAHI does) as a batch to the GPU, it should be more performant. I'll take a look!"
      },
      {
        "user": "LinasKo",
        "body": "I'm closing this, as there's not much we can do besides batching, which is already covered by #1239 and related issues."
      }
    ]
  },
  {
    "issue_number": 1709,
    "title": "Object detection precision-ap",
    "author": "GiannisApost",
    "state": "open",
    "created_at": "2024-12-03T12:55:58Z",
    "updated_at": "2024-12-04T10:16:47Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Question\r\n\r\nSuppose we have a single class object detection problem. Shouldn't the average precision and the precision metrics across the different iou thresholds be the same? This does not seem to be the case here.\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "I'll have a look. Thank you for the report, @GiannisApost"
      }
    ]
  },
  {
    "issue_number": 1694,
    "title": "Crash when filtering empty detections: xyxy shape (0, 0, 4).",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-11-28T11:31:18Z",
    "updated_at": "2024-12-04T10:15:33Z",
    "labels": [
      "bug"
    ],
    "body": "Reproduction code:\r\n\r\n```python\r\nimport supervision as sv\r\nimport numpy as np\r\n\r\nCLASSES = [0, 1, 2]\r\n\r\nprediction = sv.Detections.empty()\r\nprediction = prediction[np.isin(prediction[\"class_name\"], CLASSES)]\r\n```\r\n\r\nError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/run_detections.py\", line 7, in <module>\r\n    prediction = prediction[np.isin(prediction[\"class_name\"], CLASSES)]\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/supervision/detection/core.py\", line 1206, in __getitem__\r\n    return Detections(\r\n  File \"<string>\", line 10, in __init__\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/supervision/detection/core.py\", line 144, in __post_init__\r\n    validate_detections_fields(\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/supervision/validators/__init__.py\", line 120, in validate_detections_fields\r\n    validate_xyxy(xyxy)\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/supervision/validators/__init__.py\", line 11, in validate_xyxy\r\n    raise ValueError(\r\nValueError: xyxy must be a 2D np.ndarray with shape (_, 4), but got shape (0, 0, 4)\r\n```\r\n",
    "comments": []
  },
  {
    "issue_number": 505,
    "title": "Letterbox of images",
    "author": "hardikdava",
    "state": "closed",
    "created_at": "2023-10-19T10:15:20Z",
    "updated_at": "2024-12-03T12:15:43Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nIt would be great to have  a feature which resize images as per letterbox concept i.e. resize based on maximum size and adding padding to an image. This is very helpful resize method to increase result quality. \r\n\r\n\n\n### Use case\n\n- Best method for preprocessing image before detector, segmentation models.\n\n### Additional\n\n\r\n```python\r\ndef letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32) -> np.ndarray:\r\n```\r\n\r\nuse case:\r\n```python\r\nimport supervision as sv\r\nimport cv2\r\n\r\nimage = cv2.imread(...)\r\nimage = sv.letterbox(image)\r\n\r\ndetection_result = detector(image)\r\n\r\n```\r\n\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @hardikdava üëãüèª Yup! That's an excellent idea. But then your `detection_result` is in the context of `letterbox` so we would need to have a utility capable of reversing this process. "
      },
      {
        "user": "hardikdava",
        "body": "Hi @SkalskiP üëã , you are absolutely correct. "
      },
      {
        "user": "SkalskiP",
        "body": "@hardikdava, what is the meaning of all of those arguments? Is that function definition copied from YOLOv8?"
      }
    ]
  },
  {
    "issue_number": 1688,
    "title": "Numpy incompatibility error, locally installed repo, Python 3.13",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-11-25T11:41:33Z",
    "updated_at": "2024-12-02T12:57:43Z",
    "labels": [
      "bug"
    ],
    "body": "# Reproduction steps:\r\n\r\n```bash\r\ngit clone git@github.com:roboflow/supervision.git supervision-fresh\r\ncd supervision-fresh\r\npoetry install --with dev\r\npoetry shell\r\necho \"import supervision\" > simple_script.py\r\npython simple_script.py\r\n```\r\n\r\nThis produces:\r\n\r\n```\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/simple_script.py\", line 1, in <module>\r\n    import supervision\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/supervision/__init__.py\", line 9, in <module>\r\n    from supervision.annotators.core import (\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/supervision/annotators/core.py\", line 5, in <module>\r\n    import cv2\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/.venv/lib/python3.13/site-packages/cv2/__init__.py\", line 181, in <module>\r\n    bootstrap()\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/.venv/lib/python3.13/site-packages/cv2/__init__.py\", line 153, in bootstrap\r\n    native_module = importlib.import_module(\"cv2\")\r\n  File \"/opt/homebrew/Cellar/python@3.13/3.13.0_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/importlib/__init__.py\", line 88, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nAttributeError: _ARRAY_API not found\r\nTraceback (most recent call last):\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/simple_script.py\", line 1, in <module>\r\n    import supervision\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/supervision/__init__.py\", line 9, in <module>\r\n    from supervision.annotators.core import (\r\n    ...<23 lines>...\r\n    )\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/supervision/annotators/core.py\", line 5, in <module>\r\n    import cv2\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/.venv/lib/python3.13/site-packages/cv2/__init__.py\", line 181, in <module>\r\n    bootstrap()\r\n    ~~~~~~~~~^^\r\n  File \"/Users/linasko/.settler_workspace/pr/supervision-fresh/.venv/lib/python3.13/site-packages/cv2/__init__.py\", line 153, in bootstrap\r\n    native_module = importlib.import_module(\"cv2\")\r\n  File \"/opt/homebrew/Cellar/python@3.13/3.13.0_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/importlib/__init__.py\", line 88, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nImportError: numpy.core.multiarray failed to import\r\n```\r\n\r\n# Environment:\r\n\r\nNote:\r\n* I use locally created `.venv` environments\r\n* I use 3.13 as the base, on MacOS.\r\n\r\n<details>\r\n<summary>Poetry env</summary>\r\n\r\n```\r\nFound existing alias for \"poetry env info\". You should use: \"pvinf\"\r\n\r\nVirtualenv\r\nPython:         3.13.0\r\nImplementation: CPython\r\nPath:           /Users/linasko/.settler_workspace/pr/supervision-fresh/.venv\r\nExecutable:     /Users/linasko/.settler_workspace/pr/supervision-fresh/.venv/bin/python\r\nValid:          True\r\n\r\nBase\r\nPlatform:   darwin\r\nOS:         posix\r\nPython:     3.13.0\r\nPath:       /opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13\r\nExecutable: /opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/bin/python3.13\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>Poetry config</summary>\r\n\r\ncache-dir = \"/Users/linasko/Library/Caches/pypoetry\"\r\nexperimental.system-git-client = false\r\ninstaller.max-workers = null\r\ninstaller.modern-installation = true\r\ninstaller.no-binary = null\r\ninstaller.parallel = true\r\nkeyring.enabled = true\r\nsolver.lazy-wheel = true\r\nvirtualenvs.create = true\r\nvirtualenvs.in-project = true\r\nvirtualenvs.options.always-copy = false\r\nvirtualenvs.options.no-pip = false\r\nvirtualenvs.options.no-setuptools = false\r\nvirtualenvs.options.system-site-packages = false\r\nvirtualenvs.path = \"{cache-dir}/virtualenvs\"  # /Users/linasko/Library/Caches/pypoetry/virtualenvs\r\nvirtualenvs.prefer-active-python = false\r\nvirtualenvs.prompt = \"{project_name}-py{python_version}\"\r\nwarnings.export = true\r\n\r\n</details>",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hey @onuralpszr üëã \r\n\r\nCould you check if it also happens on Linux?"
      },
      {
        "user": "onuralpszr",
        "body": "> Hey @onuralpszr üëã\r\n> \r\n> Could you check if it also happens on Linux?\r\n\r\nTesting with py3.13 linux üëç - PASSED (no error)"
      },
      {
        "user": "LinasKo",
        "body": "Thanks a lot!\r\n\r\nConfirming this also happens with `--depth 1 -b develop`.\r\n\r\n<details>\r\n<summary>`pip freeze`</summary>\r\n\r\n```\r\nanyio==4.5.2\r\nappnope==0.1.4\r\nargon2-cffi==23.1.0\r\nargon2-cffi-bindings==21.2.0\r\narrow==1.3.0\r\nasttokens==2.4.1\r\nasync-lru==2.0.4\r\nattrs==24.2.0\r\nbabel==2.16.0\r\nbackcall==0.2.0\r\nbeautifulsoup4==4.12.3\r\nbleach==6.1.0\r\nbuild==1.2.2.post1\r\ncachetools==5.5.0\r\ncairocffi==1.7.1\r\nCairoSVG==2.7.1\r\ncertifi==2024.8.30\r\ncffi==1.17.1\r\ncfgv==3.4.0\r\nchardet==5.2.0\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\ncolorama==0.4.6\r\ncomm==0.2.2\r\ncontourpy==1.3.1\r\ncssselect2==0.7.0\r\ncycler==0.12.1\r\ndebugpy==1.8.8\r\ndecorator==5.1.1\r\ndefusedxml==0.7.1\r\ndistlib==0.3.9\r\ndocutils==0.21.2\r\nexecuting==2.1.0\r\nfastjsonschema==2.20.0\r\nfilelock==3.16.1\r\nfonttools==4.55.0\r\nfqdn==1.5.1\r\nghp-import==2.1.0\r\ngitdb==4.0.11\r\nGitPython==3.1.43\r\ngriffe==1.4.0\r\nh11==0.14.0\r\nhttpcore==1.0.7\r\nhttpx==0.27.2\r\nidentify==2.6.1\r\nidna==3.10\r\nimportlib_metadata==8.5.0\r\nimportlib_resources==6.4.5\r\niniconfig==2.0.0\r\nipykernel==6.29.5\r\nipython==8.12.3\r\nipywidgets==8.1.5\r\nisoduration==20.11.0\r\njaraco.classes==3.4.0\r\njaraco.context==6.0.1\r\njaraco.functools==4.1.0\r\njedi==0.19.2\r\nJinja2==3.1.4\r\njson5==0.9.28\r\njsonpointer==3.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2023.12.1\r\njupyter-events==0.10.0\r\njupyter-lsp==2.2.5\r\njupyter_client==8.6.3\r\njupyter_core==5.7.2\r\njupyter_server==2.14.2\r\njupyter_server_terminals==0.5.3\r\njupyterlab==4.2.6\r\njupyterlab_pygments==0.3.0\r\njupyterlab_server==2.27.3\r\njupyterlab_widgets==3.0.13\r\njupytext==1.16.4\r\nkeyring==25.5.0\r\nkiwisolver==1.4.7\r\nMarkdown==3.7\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmatplotlib==3.7.2\r\nmatplotlib-inline==0.1.7\r\nmdit-py-plugins==0.4.2\r\nmdurl==0.1.2\r\nmergedeep==1.3.4\r\nmike==2.1.3\r\nmistune==3.0.2\r\nmkdocs==1.6.1\r\nmkdocs-autorefs==1.2.0\r\nmkdocs-get-deps==0.2.0\r\nmkdocs-git-committers-plugin-2==2.4.1\r\nmkdocs-git-revision-date-localized-plugin==1.3.0\r\nmkdocs-jupyter==0.24.8\r\nmkdocs-material==9.5.45\r\nmkdocs-material-extensions==1.3.1\r\nmkdocstrings==0.26.1\r\nmkdocstrings-python==1.11.1\r\nmore-itertools==10.5.0\r\nmypy==1.13.0\r\nmypy-extensions==1.0.0\r\nnbclient==0.10.0\r\nnbconvert==7.16.4\r\nnbformat==5.10.4\r\nnest-asyncio==1.6.0\r\nnh3==0.2.18\r\nnodeenv==1.9.1\r\nnotebook==7.2.2\r\nnotebook_shim==0.2.4\r\nnumpy==2.1.3\r\nopencv-python==4.8.1.78\r\noverrides==7.7.0\r\npackaging==24.2\r\npaginate==0.5.7\r\npandocfilters==1.5.1\r\nparso==0.8.4\r\npathspec==0.12.1\r\npexpect==4.9.0\r\npickleshare==0.7.5\r\npillow==10.4.0\r\npkginfo==1.10.0\r\nplatformdirs==4.3.6\r\npluggy==1.5.0\r\npre-commit==3.5.0\r\nprometheus_client==0.21.0\r\nprompt_toolkit==3.0.48\r\npsutil==6.1.0\r\nptyprocess==0.7.0\r\npure_eval==0.2.3\r\npycparser==2.22\r\nPygments==2.18.0\r\npymdown-extensions==10.12\r\npyparsing==3.0.9\r\npyproject-api==1.8.0\r\npyproject_hooks==1.2.0\r\npytest==8.3.3\r\npython-dateutil==2.9.0.post0\r\npython-json-logger==2.0.7\r\npytz==2024.2\r\nPyYAML==6.0.2\r\npyyaml_env_tag==0.1\r\npyzmq==26.2.0\r\nreadme_renderer==43.0\r\nreferencing==0.35.1\r\nregex==2024.11.6\r\nrequests==2.32.3\r\nrequests-toolbelt==1.0.0\r\nrfc3339-validator==0.1.4\r\nrfc3986==2.0.0\r\nrfc3986-validator==0.1.1\r\nrich==13.9.4\r\nrpds-py==0.20.1\r\nruff==0.8.0\r\nscipy==1.14.1\r\nSend2Trash==1.8.3\r\nsetuptools==75.3.0\r\nsix==1.16.0\r\nsmmap==5.0.1\r\nsniffio==1.3.1\r\nsoupsieve==2.6\r\nstack-data==0.6.3\r\n-e git+ssh://git@github.com/roboflow/supervision.git@bbbb37434c4f7dbc113228c634e17459bb01c4f7#egg=supervision\r\nterminado==0.18.1\r\ntinycss2==1.4.0\r\ntornado==6.4.2\r\ntox==4.23.2\r\ntqdm==4.67.1\r\ntraitlets==5.14.3\r\ntwine==5.1.1\r\ntypes-python-dateutil==2.9.0.20241003\r\ntyping_extensions==4.12.2\r\nuri-template==1.3.0\r\nurllib3==2.2.3\r\nverspec==0.1.0\r\nvirtualenv==20.27.1\r\nwatchdog==4.0.2\r\nwcwidth==0.2.13\r\nwebcolors==24.8.0\r\nwebencodings==0.5.1\r\nwebsocket-client==1.8.0\r\nwheel==0.45.1\r\nwidgetsnbextension==4.0.13\r\nzipp==3.20.2\r\n```\r\n\r\n</details>"
      }
    ]
  },
  {
    "issue_number": 1681,
    "title": "Pyright reportPrivateImportUsage error on supervision.Detections",
    "author": "anriha",
    "state": "closed",
    "created_at": "2024-11-24T04:51:38Z",
    "updated_at": "2024-11-26T10:49:06Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nAfter supervision added `py.typed` support, Pyright is reporting a `reportPrivateImportUsage` error when trying to use `Detections` type from the package's root import, even though it's explicitly re-exported in the package's `__init__.py`.\r\n\r\n\r\n### Environment\r\n\r\nSupervision: 0.25.0\r\nPython: 3.12.7\r\nPyright: 1.1.382\r\nOS: NixOS\r\n\r\n### Minimal Reproducible Example\r\n\r\n```python\r\nimport supervision as sv\r\ndef process_detections(detections: sv.Detections) -> None:\r\n    pass\r\n```\r\n    \r\n# This triggers the error\r\n\r\n```python\r\ndef process_detections(detections: sv.Detections) -> None:\r\n    pass\r\n```\r\n\r\n### Additional\r\n\r\nThe error can be temporarily worked around by using direct import: `from supervision.detection.core import Detections`\r\n\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "Hello üëã @anriha,\r\n\r\nCould you share your pyright CLI command or configuration file? I was unable to reproduce the error you mentioned. Since pyright is used by Pylance in VS Code, I didn't see the error there. To double-check, I also installed pyright via npm and ran it on a file I created using the same code you provided. The result was:\r\n\r\n> 0 errors, 0 warnings, 0 informations.\r\n\r\nAdditionally, I added a simple pyright configuration to a toml file for testing purposes, and I got the same results.\r\n\r\n```toml\r\n[tool.pyright]\r\ninclude = [\"supervision\"]\r\nexclude = [\"**/node_modules\",\r\n    \"**/__pycache__\",\r\n    \"**/.mypy_cache\",\r\n]\r\ndefineConstant = { DEBUG = true }\r\n\r\nreportMissingImports = \"error\"\r\nreportMissingTypeStubs = false\r\nreportPrivateImportUsage = true\r\n\r\n```\r\n\r\nUnless you have an alternative approach, I will proceed to close this issue. We are also planning to fully integrate mypy to ensure the project is compatible with static typing. I should mention that pyright and mypy sometimes differ in how they interpret typing rules. Once we finalize the mypy configuration, we might need to make adjustments or add specific settings for pyright or Pylance to ensure compatibility. However, our primary guideline will be the mypy configuration.\r\n\r\n\r\nFor information here is mypy vs pyright: https://github.com/microsoft/pyright/blob/main/docs/mypy-comparison.md\r\n\r\ncc @LinasKo"
      },
      {
        "user": "anriha",
        "body": "Thank you for looking into this. I'm still getting the error with just a basic `pyright file.py` command:\r\n```python\r\n# file.py\r\nimport supervision as sv\r\n\r\ndef process_detections(detections: sv.Detections) -> None:\r\n    pass\r\n```\r\n\r\nI don't have any specific pyright config, everything should be default. \r\nDid you test with supervision 0.25? "
      },
      {
        "user": "onuralpszr",
        "body": "> Thank you for looking into this. I'm still getting the error with just a basic `pyright file.py` command:\r\n> \r\n> ```python\r\n> # file.py\r\n> import supervision as sv\r\n> \r\n> def process_detections(detections: sv.Detections) -> None:\r\n>     pass\r\n> ```\r\n> \r\n> I don't have any specific pyright config, everything should be default. Did you test with supervision 0.25?\r\n\r\nYes I tested with latest version of  course"
      }
    ]
  },
  {
    "issue_number": 1683,
    "title": "How to convert the PyTorch Data Loader to sv.Detections Object",
    "author": "shanalikhan",
    "state": "open",
    "created_at": "2024-11-24T21:01:36Z",
    "updated_at": "2024-11-24T21:03:36Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Question\r\n\r\nI've Dataloader object and I am able to convert the predicted model output using `from_transformer` to Detection Object. Since i want to generate the MAP and confusion matrix, I need to convert the actual object to Detection object as well. \r\n\r\nHow to do that?\r\n\r\n\r\n\r\n```python\r\nfor idx,batch in enumerate(tqdm(TEST_DATALOADER)):\r\n    \r\n    pixel_values = batch[\"pixel_values\"].to(DEVICE)\r\n    pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\r\n    #print(batch[\"labels\"])\r\n    labels = [{k: v.to(DEVICE) for k, v in t.items()} for t in batch[\"labels\"]]\r\n    #print(labels)\r\n    with torch.no_grad(): \r\n      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\r\n      #outputs = model(**batch)\r\n\r\n    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\r\n    #print(outputs)\r\n    results = image_processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes,threshold=0.5)\r\n    print(\"LABELS\")\r\n    print(labels[0].keys())\r\n    \r\n    print(labels[0]['class_labels'])\r\n    print(\"RESULTS\")\r\n    print(results[0].keys())\r\n    print(results[0]['labels'])\r\n    for result in results:\r\n      detections = sv.Detections.from_transformers(result)#.with_nms(threshold=IOU_TRESHOLD)\r\n      #sv.Detections.from_\r\n      targets.append(annotations)\r\n      predictions.append(detections)\r\n    break\r\n```\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1383,
    "title": "[LabelAnnotator, RichLabelAnnotator, VertexLabelAnnotator] - add smart label positioning",
    "author": "SkalskiP",
    "state": "open",
    "created_at": "2024-07-19T12:25:58Z",
    "updated_at": "2024-11-22T18:32:31Z",
    "labels": [
      "enhancement",
      "api:annotator",
      "hacktoberfest"
    ],
    "body": "### Description\r\n\r\nOverlapping labels are a common issue, especially in crowded scenes. Let's add an optional smart label positioning feature to the [`LabelAnnotator`](https://supervision.roboflow.com/develop/detection/annotators/#supervision.annotators.core.LabelAnnotator), [`RichLabelAnnotator`](https://supervision.roboflow.com/develop/detection/annotators/#supervision.annotators.core.RichLabelAnnotator), and [`VertexLabelAnnotator`](https://supervision.roboflow.com/develop/keypoint/annotators/#supervision.keypoint.annotators.EdgeAnnotator.annotate) that:\r\n\r\n- Ensures that the label box does not extend beyond the image.\r\n- Automatically adjust the position of overlapping labels to prevent them from overlapping.\r\n\r\n![IMG_0062FC89843B-1](https://github.com/user-attachments/assets/a73872e2-a805-48bc-a78b-018c9c7846bf)\r\n\r\nThe algorithm boils down to locating overlapping label boxes and then calculating the direction of vectors to push the labels apart. This process may require an iterative approach, as moving label boxes can lead to new overlaps with other label boxes.\r\n\r\n![IMG_8FB8BD194AE1-1](https://github.com/user-attachments/assets/b0105ef3-b68d-4989-a5e5-793d7cfd1b45)\r\n\r\nImportantly, the bounding box remains in the same place, only the label boxes are moved. It would be great if, after the shift, the label and its original position were connected by a line.\r\n\r\n### Examples of incorrect behavior\r\n\r\n![image (85)](https://github.com/user-attachments/assets/6b865fd6-0cd9-49c8-9d5c-91f84e269644)\r\n\r\n![download - 2024-04-25T171410 778](https://github.com/user-attachments/assets/43ea6053-31bd-4b05-80ad-0a3b10be42e0)\r\n\r\n### Examples of expected behavior \r\n\r\nhttps://github.com/user-attachments/assets/acc70301-7459-47c5-882c-720cd84b3ae0\r\n\r\nHere's the [Google Colab](https://colab.research.google.com/drive/1VQ_uGjfYXPMeVe8NvpIOSRN4qKMMxPju?usp=sharing) I used to experiment with this feature.\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "jeslinpjames",
        "body": "Hey, @SkalskiP \r\n\r\nI'd like to try working on this. Could you provide any specific guidelines or tips for implementing this feature?"
      },
      {
        "user": "LinasKo",
        "body": "We're opening this up to the community! @jeslinpjames, it's been a long time - are you still interested? I'll leave this open for a few days on the off-chance you're still around.\r\n\r\nEdit: Assigning to you temporarily until I hear back or a few days pass."
      },
      {
        "user": "LinasKo",
        "body": "With respect to the implementation details, I'm glad to see Piotr's plan as I had the exact same idea, down to the connector line.\r\n\r\n1. The mentioned annotators would have a new argument `use_smart_positioning`, activating this feature if set to `True`.\r\n2. Let's treat the annotators as independent. If two different LabelAnnotators are used at once, let's allow their labels to overlap, even if this feature is on.\r\n3. Unmentioned annotators that use labels can be ignored (LineZoneAnnotator, PolygonZoneAnnotator).\r\n4. There are alternate approaches to this.\r\n\r\nLet's start with the simplest one:\r\n* If two boxes intersect with another box, move it either along x or y, based on where the overlap is SMALLEST (this minimizes the distance we need to move).\r\n* Move proportionally to overlap size size, but with a cap. When everything moves at once, large movements may cause even greater overlaps. (similar to gradient descent!)\r\n* region boundaries should not allow labels to escape. They should reset the labels to the nearest possible position, at least along one axis. Even if the label moved or started out-of-bounds.\r\n\r\nAn upgraded version of this would take into account both x and y axes of the overlap and allow arbitrary motion direction. Implement this if you wish, but take care to minimize motion - overemphasize the motion along the SMALLER overlap direction.\r\n\r\nAn even more robust system uses some random noise to avoid stable states. We don't need this much detail üòâ \r\n\r\n```python\r\nfind_smart_rectangle_positions(\r\n    xyxy: npt.NDarray[float], shape (H, W, 4).\r\n    region_boundary_wh: (float, float)  # This is a hard boundary on the edges.\r\n    max_iterations=10:  # return if there are no overlaps or this many iterations have passed.\r\n    force_multiplier=1.0  # Make the movements larger.\r\n)\r\n```\r\n\r\n5. You may find the function `cv2.getTextSize` helpful.\r\n6. Before drawing the label boxes, each annotator should draw a line between the center of the old and new label locations.\r\n\r\nWhoever ends up working on this, I hope it gives you some ideas of how this could work!"
      }
    ]
  },
  {
    "issue_number": 1679,
    "title": "OBB detection failed with SAHI for small object detection",
    "author": "ERYAGNIK003",
    "state": "open",
    "created_at": "2024-11-22T08:42:09Z",
    "updated_at": "2024-11-22T08:42:09Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI am working on vehicle detection in traffic scene. I have used YOLOv11x-obb (pre-trained on DOTAv1) for vehicle detection. I tried SAHI so I can detect small vehicles successfully but it failed. I used codes from https://github.com/roboflow/supervision/issues/1394. I have attached codes and results. Please guide me How to improve detection with SAHI. \r\n  \r\nPlatform details: \r\n- Ultralytics 8.3.35 üöÄ Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\r\n- Supervision version: 0.25.0\r\n\r\n# YOLO prediction without SAHI:\r\n    from ultralytics import YOLO\r\n    import cv2\r\n    from PIL import Image\r\n    \r\n    model = YOLO(\"yolo11x-obb.pt\")  \r\n    model.hide_labels = True\r\n    model.hide_conf = True\r\n    results=model.predict(\"img.png\",show_labels=False)\r\n    img = results[0].plot(labels=False, conf=False)\r\n    cv2_imshow(img)\r\n![yolo](https://github.com/user-attachments/assets/c821eb66-ce28-47cc-ae66-cc7cf8e9e112)\r\n\r\n# YOLO prediction with SAHI\r\n    import cv2\r\n    import supervision as sv\r\n    from ultralytics import YOLO\r\n    import numpy as np\r\n    \r\n    image = cv2.imread(\"img.png\")\r\n    model = YOLO(\"yolo11x-obb.pt\")\r\n    \r\n    def callback(image_slice: np.ndarray) -> sv.Detections:\r\n        result = model(image_slice)[0]\r\n        return sv.Detections.from_ultralytics(result)\r\n    \r\n    slicer = sv.InferenceSlicer(callback = callback,overlap_filter=\"NON_MAX_SUPPRESSION\")\r\n    detections = slicer(image)\r\n     \r\n    oriented_box_annotator = sv.OrientedBoxAnnotator()\r\n    annotated_frame = oriented_box_annotator.annotate(\r\n        scene=image.copy(),\r\n        detections=detections\r\n    )\r\n    sv.plot_image(annotated_frame)\r\n![sahi](https://github.com/user-attachments/assets/7eb77558-9442-4b5a-9d12-602c6a56b517)\r\n\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1583,
    "title": "Metrics: Mean Average Recall (mAR)",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-10-09T12:03:22Z",
    "updated_at": "2024-11-11T12:17:47Z",
    "labels": [
      "enhancement",
      "hacktoberfest"
    ],
    "body": "# Metrics: Mean Average Recall (mAR)\r\n\r\n> [!TIP]\r\n> [Hacktoberfest](https://hacktoberfest.com/) is calling! Whether it's your first PR or your 50th, you‚Äôre helping shape the future of open source. Help us build the most reliable and user-friendly computer vision library out there! üå±\r\n\r\nWe'd like to expand our suite of metrics with a new one - Mean Average Recall (mAR). This would involve creating it, its accompanying results class, and briefly testing it.\r\n\r\nNote it is different from `mAP` in that it don't use the precision-recall curve, but recall-IoU. This affects `MeanAverageRecallResult`:\r\n\r\n* The will not be `mAR@50`, `mAR@75`, etc - only the global main `mAR` value.\r\n* Other frameworks threshold by max detections. I suggest we add this later if required.\r\n* mAP currently implements a single averaging method, and F1 has 3 different ones (micro, macro, weighted). The choice between those is left to PR author - we can always add more later.\r\n* The metric can report mAR values per-class. Do tell if that is difficult.\r\n* I think we should have `1.0` as the default if no value are provided. `-1` is a consideration too, which should bring the same change to `mAP`.\r\n\r\nFeel free to change the above if it feels better.\r\n\r\n---\r\n\r\nHelpful links:\r\n* [Contribution guide](https://supervision.roboflow.com/develop/contributing/#how-to-contribute-changes)\r\n* Metrics:\r\n  * mAP metric: [docs](https://supervision.roboflow.com/develop/metrics/mean_average_precision/), [code](https://github.com/roboflow/supervision/blob/d6aa72c0f2b158b838145a81ed5995db6a1e9015/supervision/metrics/mean_average_precision.py#L25)\r\n  * F1 Score: [docs](https://supervision.roboflow.com/develop/metrics/f1_score/), [code](https://github.com/roboflow/supervision/blob/d6aa72c0f2b158b838145a81ed5995db6a1e9015/supervision/metrics/f1_score.py#L25)\r\n* [Supervision Cheatsheet](https://roboflow.github.io/cheatsheet-supervision/)\r\n* [Colab Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89#scrollTo=pjmCrNre2g58)\r\n* [Prior metrics test Colab](https://colab.research.google.com/drive/1qSMDDpImc9arTgQv-qvxlTA87KRRegYN)",
    "comments": [
      {
        "user": "LinasKo",
        "body": "### Contribution guidelines\r\nIf you would like to make a contribution, please check that no one else is assigned already. Then leave a comment such as \"Hi, I would like to work on this issue\". We're happy to answer any questions about the task even if you choose not to contribute.\r\n\r\n### Testing\r\nPlease share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. You may use the [Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89). The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! :pray:"
      },
      {
        "user": "LinasKo",
        "body": "@onuralpszr, I've heard you already put some work towards this one. Shall I assign it to you?"
      },
      {
        "user": "onuralpszr",
        "body": "> @onuralpszr, I've heard you already put some work towards this one. Shall I assign it to you?\n\nThank you and yes "
      }
    ]
  },
  {
    "issue_number": 1367,
    "title": "Add tracking for KeyPoints",
    "author": "rolson24",
    "state": "open",
    "created_at": "2024-07-16T20:07:44Z",
    "updated_at": "2024-11-06T20:03:49Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nRight now there is no way to track objects that have keypoints associated with them, because the tracker does not have a way to track keypoints. This feature would be able to track objects that keypoints are associated with even if there are multiple options. Ideally it would simply use the existing ByteTrack module to track the objects' bounding boxes and then keep the keypoints associated with that tracked object. Note this is different than tracking each individual keypoint, which would require an entirely different tracker.\n\n### Use case\n\nThis is important for many different applications where tracking keypoints through a video can provide some important information. For example for sports science if two players a playing basketball and you want to analyze the movement of players, you would need to track the keypoints of the two players separately.\n\n### Additional\n\nI see several ways this could be implemented.\r\n\r\n**Option 1: Add keypoints to Detections and change all the `from_model()` functions**\r\nAdd keypoints as another possible attribute to the `Detections` object, similar to the `mask` attribute. This would most likely involve adding keypoints to `Detections` and adding `from_mediapipe()` to the `Detections` class and modifying all of the other `from_model()` functions to support KeyPoints. Then the tracker could be used as normal on these detections objects.\r\n\r\n**Option 2: Add keypoints to Detections after a detections object has been created**\r\nThe same as option 1, but instead of modifying all of the `from_model()` functions, make it so that the keypoints attribute is None unless the keypoints object were added to the existing Detections object. This would require the indices of the keypoints to exactly match the associated detection boxes. This could work with models that don't output bounding boxes by creating the boxes from the keypoints. Then the tracker could be used as normal.\r\n\r\n**Option 3: Add bounding boxes and object confidence scores to the `KeyPoints` class**\r\nWe could add bounding boxes and object confidence scores to the `KeyPoints` class in the same way as `Detections`. For the ultralytics pose models this would be easy as they are included as outputs. For the other models this could be implemented by creating a bounding box from the keypoints of each object, and confidence scores as an average of the keypoints confidence values. Then the `KeyPoints` object could simply be sent into the object tracker. It would require a small amount of modification to the tracker, but would be relatively simple on the whole. It would be redundant to have `KeyPoints` and `Detections` have some of the same information.\r\n\r\n**Option 4: Do this hacky thing**\r\nI don't like this option because it is ugly and inefficient and is slightly confusing, but it works right now without any changes.\r\n```\r\nresults = model(frame, imgsz = 1280,verbose=False)[0]\r\npre_track_detections = sv.Detections.from_ultralytics(results)\r\nkeypoints = sv.KeyPoints.from_ultralytics(results)\r\npost_track_detections = byte_tracker.update_with_detections(pre_track_detections)\r\n\r\npre_track_bounding_boxes = pre_track_detections.xyxy\r\npost_track_bounding_boxes = post_track_detections.xyxy\r\n\r\nious = sv.tracker.byte_tracker.matching.box_iou_batch(pre_track_bounding_boxes, post_track_bounding_boxes)\r\niou_costs = 1 - ious\r\nmatches, _, _ = sv.tracker.byte_tracker.matching.linear_assignment(iou_costs, 0.5)\r\n\r\npost_track_keypoints = sv.KeyPoints.empty()\r\n\r\npost_track_keypoints.xy = np.empty((len(post_track_detections), keypoints.xy.shape[1], 2), dtype=np.float32)\r\npost_track_keypoints.class_id = np.empty((len(post_track_detections), keypoints.xy.shape[1]), dtype=np.float32)\r\npost_track_keypoints.confidence = np.empty((len(post_track_detections), keypoints.xy.shape[1]), dtype=np.float32)\r\npost_track_keypoints.data = keypoints.data\r\n\r\nfor i_detection, i_track in matches:\r\n    post_track_keypoints.xy[i_track] = keypoints.xy[i_detection]\r\n    post_track_keypoints.class_id[i_track] = keypoints.class_id[i_detection]\r\n    post_track_keypoints.confidence[i_track] = keypoints.confidence[i_detection]\r\n```\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "AyuK03",
        "body": "Hi, it is my first time contributing to the project and I would like to work on this issue :)"
      },
      {
        "user": "onuralpszr",
        "body": "> Hi, it is my first time contributing to the project and I would like to work on this issue :)\r\n\r\nPlease don't send message to every single issues and since it is your first time to contribute please read the issue and try to understand and try to do in fork and show us google collab and we can proceed. So a little bit slow down please :) \r\n\r\nThank you."
      },
      {
        "user": "AHB102",
        "body": "@onuralpszr @rolson24 Any other thoughts? I'm leaning towards option 1. It's tedious, but I think it'll be better in the long run."
      }
    ]
  },
  {
    "issue_number": 1021,
    "title": "enhance numpy arrays static type checking",
    "author": "onuralpszr",
    "state": "open",
    "created_at": "2024-03-19T02:43:12Z",
    "updated_at": "2024-11-06T11:49:48Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Description\r\n\r\n- This issue aims to improve the static type checking in our codebase and expand the functionality of the Supervision library. The enhancements will make the static type checking and improve code readilibty and help developers to understand which type they need to use\r\n- Ensure type checking for all possible the functions and classes in the Supervision library.\r\n\r\n### Old Style code snippet\r\n\r\n```python\r\n\r\nimport numpy as np\r\n\r\n@dataclass\r\nclass Detection:\r\n    xyxy: np.ndarray\r\n    mask: Optional[np.ndarray] = None\r\n    confidence: Optional[np.ndarray] = None\r\n    class_id: Optional[np.ndarray] = None\r\n    tracker_id: Optional[np.ndarray] = None\r\n    data: Dict[str, Union[np.ndarray, List]] = field(default_factory=dict)\r\n\r\n    def __len__(self):\r\n        return len(self.xyxy)\r\n\r\n    def __eq__(self, other: Detections):\r\n        return all(\r\n            [\r\n                np.array_equal(self.xyxy, other.xyxy),\r\n                np.array_equal(self.mask, other.mask),\r\n                np.array_equal(self.class_id, other.class_id),\r\n                np.array_equal(self.confidence, other.confidence),\r\n                np.array_equal(self.tracker_id, other.tracker_id),\r\n                is_data_equal(self.data, other.data),\r\n            ]\r\n        )\r\n```\r\n\r\n### New Style code snippet\r\n\r\n```python\r\n\r\nimport numpy as np\r\nimport numpy.typing as npt\r\n\r\n@dataclass\r\nclass Detection:\r\n    xyxy: npt.NDArray[np.float32]\r\n    mask: Optional[npt.NDArray[np.float32]] = None\r\n    confidence: Optional[npt.NDArray[np.float32]] = None\r\n    class_id: Optional[npt.NDArray[np.float32]] = None\r\n    tracker_id: Optional[npt.NDArray[np.float32]] = None\r\n    data: Dict[str, Union[npt.NDArray[Any], List[Any]]] = field(default_factory=dict)\r\n\r\n    def __len__(self) -> int:\r\n        return len(self.xyxy)\r\n\r\n    def __eq__(self, other: Detections) -> bool:\r\n        return all(\r\n            [\r\n                np.array_equal(self.xyxy, other.xyxy),\r\n                np.array_equal(self.mask, other.mask),\r\n                np.array_equal(self.class_id, other.class_id),\r\n                np.array_equal(self.confidence, other.confidence),\r\n                np.array_equal(self.tracker_id, other.tracker_id),\r\n                is_data_equal(self.data, other.data),\r\n            ]\r\n        )\r\n```\r\n\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will definitely speed up the review process. Each change must be tested by the reviewer. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "@onuralpszr, thanks a lot for creating this issue üôèüèª I really appreciate that you're so helpful."
      },
      {
        "user": "onuralpszr",
        "body": "> @onuralpszr, thanks a lot for creating this issue üôèüèª I really appreciate that you're so helpful.\n\nYou are welcome  :)"
      },
      {
        "user": "jeslinpjames",
        "body": "Hello, can i try to do this? "
      }
    ]
  },
  {
    "issue_number": 1275,
    "title": "FPSMonitor.__call__ warning ",
    "author": "MubashirWaheed",
    "state": "closed",
    "created_at": "2024-06-11T13:01:20Z",
    "updated_at": "2024-11-04T11:24:06Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Question\r\n\r\nI keep getting the following warning in my console when I run the code \r\n`SupervisionWarnings: __call__ is deprecated: `FPSMonitor.__call__` is deprecated and will be removed in `supervision-0.22.0`. Use `FPSMonitor.fps` instead`\r\nHere is the code \r\n```js\r\n// inside the custom sink class\r\n`       def __init__(self, weights_path: str, zone_configuration_path: str, classes: List[int]):\r\n        self._model = YOLO(weights_path, task=\"segment\")\r\n        self.fps_monitor = sv.FPSMonitor()\r\n\r\n    def on_prediction(self, result: dict, frame: VideoFrame) -> None:\r\n        self.fps_monitor.tick()\r\n        fps = self.fps_monitor\r\n```\r\nI tried this one as well in the on_prediction function `fps = self.fps_monitor.fps()` but still gettinig the warning.\r\nWhat can I do to fix this?\r\nMy console is filled with this warning and I can't see any other log \r\nsupervision version :  0.21.0\r\nPython: 3.11.8\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "I'll look into it, thanks for letting us know.\r\n\r\nMeanwhile, if it gets annoying, setting the environmental variable `SUPERVISON_DEPRECATION_WARNING=0` should suppress those."
      },
      {
        "user": "MubashirWaheed",
        "body": "Yes setting the environment variable fixed it for me\r\n```js\r\nimport os\r\nos.environ['SUPERVISON_DEPRECATION_WARNING'] = '0'\r\n```"
      },
      {
        "user": "LinasKo",
        "body": "Hi @MubashirWaheed,\r\n\r\nBy any chance, could you confirm the issue still persists?\r\nIt was [fixed](https://github.com/roboflow/inference/pull/461) in inference, and I can't find deprecated usage in `supervision.`"
      }
    ]
  },
  {
    "issue_number": 1582,
    "title": "Allow ByteTrack to track detections without IDs",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-10-09T11:47:05Z",
    "updated_at": "2024-11-04T11:22:49Z",
    "labels": [
      "enhancement",
      "help wanted",
      "hacktoberfest"
    ],
    "body": "# Allow ByteTrack to track detections without IDs\r\n\r\n> [!TIP]\r\n> [Hacktoberfest](https://hacktoberfest.com/) is calling! Whether it's your first PR or your 50th, you‚Äôre helping shape the future of open source. Help us build the most reliable and user-friendly computer vision library out there! üå±\r\n\r\nFor a long time, all of models we support would return a `class_id` which we set inside Detections (`class_id` is an array of N integers). However, we've seen more and more models where that isn't provided.\r\n\r\n[ByteTrack](https://supervision.roboflow.com/latest/trackers/#bytetrack) allows you to track objects in the scene. It uses the class and location of the detection to figure out where it is. It can track objects where all class IDs are the same. However, if you try passing something without an ID, it will fail. See [this Colab](https://colab.research.google.com/drive/1oAi30puYa7ZgJIAYCe2E_-7Q_dCap4Sr?usp=sharing).\r\n\r\nLet's change ByteTrack so it can track detections, even if they didn't have a `class_id`. I see two ways of doing this:\r\n1) Change the algorithm such that class_ids are not required\r\n2) Set an internal class ID such as `-1`.\r\n\r\nAs a result, I expect the videos produced in the [Colab](https://colab.research.google.com/drive/1oAi30puYa7ZgJIAYCe2E_-7Q_dCap4Sr?usp=sharing) to be visually indistinguishable.\r\n\r\nIt would also be useful to test on a video where some frames feature no detections (you can simulate with `detections = sv.Detections.empty()`)\r\n\r\nAlso, check what happens when some detections have `class_id` and some do not.\r\n\r\n---\r\n\r\nHelpful links:\r\n* [Contribution guide](https://supervision.roboflow.com/develop/contributing/#how-to-contribute-changes)\r\n* ByteTrack: [docs](https://supervision.roboflow.com/latest/trackers/#bytetrack), [code](https://github.com/roboflow/supervision/blob/70869c73379026614c651828123f35e771f65e67/supervision/tracker/byte_tracker/core.py#L190)\r\n* Detections: [docs](https://supervision.roboflow.com/latest/detection/core/), [code](https://github.com/roboflow/supervision/blob/develop/supervision/detection/core.py)\r\n* [Supervision Cheatsheet](https://roboflow.github.io/cheatsheet-supervision/)\r\n* [Colab Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89#scrollTo=pjmCrNre2g58)\r\n* [Prior metrics test Colab](https://colab.research.google.com/drive/1qSMDDpImc9arTgQv-qvxlTA87KRRegYN)",
    "comments": [
      {
        "user": "LinasKo",
        "body": "### Contribution guidelines\r\nIf you would like to make a contribution, please check that no one else is assigned already. Then leave a comment such as \"Hi, I would like to work on this issue\". We're happy to answer any questions about the task even if you choose not to contribute.\r\n\r\n### Testing\r\nPlease share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. You may use the [Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89). The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! :pray:"
      },
      {
        "user": "Kadermiyanyedi",
        "body": "Hi, I would like to work on this issue. :) "
      },
      {
        "user": "onuralpszr",
        "body": "@Kadermiyanyedi  task is yours, good luck ;) "
      }
    ]
  },
  {
    "issue_number": 1226,
    "title": "Prediction-level metadata in `sv.Detections`",
    "author": "PawelPeczek-Roboflow",
    "state": "closed",
    "created_at": "2024-05-24T09:49:58Z",
    "updated_at": "2024-11-04T11:22:39Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Description\r\n\r\n`sv.Detections` supports data aligned detection-major (one element for one detection), guarded by\r\n\r\n```python\r\ndef validate_data(data: Dict[str, Any], n: int) -> None:\r\n    for key, value in data.items():\r\n        if isinstance(value, list):\r\n            if len(value) != n:\r\n                raise ValueError(f\"Length of list for key '{key}' must be {n}\")\r\n        elif isinstance(value, np.ndarray):\r\n            if value.ndim == 1 and value.shape[0] != n:\r\n                raise ValueError(f\"Shape of np.ndarray for key '{key}' must be ({n},)\")\r\n            elif value.ndim > 1 and value.shape[0] != n:\r\n                raise ValueError(\r\n                    f\"First dimension of np.ndarray for key '{key}' must have size {n}\"\r\n                )\r\n        else:\r\n            raise ValueError(f\"Value for key '{key}' must be a list or np.ndarray\")\r\n```\r\n\r\nPutting prediction in broader context sometimes requires information about the whole prediction. Instances when those would be needed:\r\n* one makes prediction on image previously cropped and wants to retain coordinates system conversion to transform bboxes and masks. To do it one only needs parent dimensions and shift in parent coordinates system. Currently if we wanted to put that into `sv.Detections`, we would need to broadcast that information into all detections - and (in absence of detections) this may not be possible (yet, would let more graceful `sv.Detections` processing downstream)\r\n\r\nThe solution that probably does not introduce breaking change would be:\r\n```python\r\n@dataclass\r\nclass Detections:\r\n    xyxy: np.ndarray\r\n    mask: Optional[np.ndarray] = None\r\n    confidence: Optional[np.ndarray] = None\r\n    class_id: Optional[np.ndarray] = None\r\n    tracker_id: Optional[np.ndarray] = None\r\n    data: Dict[str, Union[np.ndarray, List]] = field(default_factory=dict)\r\n    prediction_data: Dict[str, Any] = field(default_factory=dict)\r\n\r\n# not including `prediction_data` into `__getitem__(...)` and iterations,\r\n# but assuming direct access `detections.prediction_data[...]`\r\n``` \r\n\r\n\r\n### Use case\r\n\r\n_No response_\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "What we'd need to figure out too, is how two `Detections` with different `prediction_data` should interact.\r\n\r\nRight now the interactions happens in:\r\n1. `Detections.merge` - aimed to stack contents of 2x `Detections` and produce 1x `Detections`.\r\n2. `Detections._merge_inner_detections_objects` in #500, aimed to merge everything inside - e.g. boxes of N `Detections` into one `Detections` with one large box.\r\n\r\nFor each respective case, we can follow similar implementations we have for other variables:\r\n1.  `raise` when different keys are defined, merge successfully when some are missing.\r\n    * How to do the merge itself? I don't know.\r\n2. Keep the `prediction_data` of the 'winning' detection (highest confidence detection, or  if both have None - first detection)"
      },
      {
        "user": "PawelPeczek-Roboflow",
        "body": "üëç  yeah, haven't thought about that \r\nI guess that keys collisions may occur, in that particular case I would just create list of values, that should be quite predictable and easy to explain default behaviour. We may also expose additional parameter - `on_prediction_data_merge` which would be callable given list of metadata supposed to return merged dict. That way we let people decide\r\n"
      },
      {
        "user": "LinasKo",
        "body": "Suppose it's metadata about source image. Would merging and keeping a list of `[image_1.png, image_2.png]` be useful? Also, do you have some specifics in mind on what you'd use it for?"
      }
    ]
  },
  {
    "issue_number": 1585,
    "title": "Detections Metadata",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-10-09T13:49:04Z",
    "updated_at": "2024-11-04T11:22:17Z",
    "labels": [
      "help wanted",
      "hacktoberfest"
    ],
    "body": "# Detections Metadata\r\n\r\n> [!TIP]\r\n> [Hacktoberfest](https://hacktoberfest.com/) is calling! Whether it's your first PR or your 50th, you‚Äôre helping shape the future of open source. Help us build the most reliable and user-friendly computer vision library out there! üå±\r\n\r\nThis is a summarized version of #1226.\r\nSee the original discussion for more context.\r\n\r\nIn brief: `Detections` object stores arrays of values and a dict of arrays, each of length `N`. We'd like to add a global dict of values to store data on a collection-level.\r\n\r\nI expect this to be a hard issue, as it involves many elements within the library.\r\n\r\n---\r\n\r\n`Detections` is a class for encoding the results of any model - detection, segmentation, etc. Here's how it looks:\r\n\r\n```python\r\n@dataclass\r\nclass Detections:\r\n    xyxy: np.ndarray\r\n    mask: Optional[np.ndarray] = None\r\n    confidence: Optional[np.ndarray] = None\r\n    class_id: Optional[np.ndarray] = None\r\n    tracker_id: Optional[np.ndarray] = None\r\n    data: Dict[str, Union[np.ndarray, List]] = field(default_factory=dict)\r\n```\r\n\r\nAll of these, as well as `data` contents are either:\r\n1. `None`: Model will never detect anything of that field\r\n2. `empty array`: Model detected no elements this time.\r\n3. Array of N elements: N objects were detected in your image. \r\n\r\nWhat if we want to store 1 value per-list? E.g. a video name for what every detection was extracted from? Or camera parameters?\r\n\r\nLet's introduce a new field: `metadata`. Detections will now look as follows:\r\n\r\n```python\r\n@dataclass\r\nclass Detections:\r\n    xyxy: np.ndarray\r\n    mask: Optional[np.ndarray] = None\r\n    confidence: Optional[np.ndarray] = None\r\n    class_id: Optional[np.ndarray] = None\r\n    tracker_id: Optional[np.ndarray] = None\r\n    data: Dict[str, Union[np.ndarray, List]] = field(default_factory=dict)\r\n    metadata: Dict[str, Any] = field(default_factory=dict)\r\n```\r\n\r\nThe users can set it directly by doing `detections.metadata[\"key\"] = \"val\"`.\r\n\r\nThe primary complexity is caused by functions that merge, slice, split and index into detections.\r\n\r\nRelevant methods to be updated:\r\n* `__eq__` should use metadata for comparison\r\n* `is_empty` should borrow the data for comparison, just like it does with `data`.\r\n* `__iter__`, **should NOT** return metadata.\r\n* None of the `from_...` methods should be affected\r\n* `merge` should:\r\n  * retain metadata even when all detections passed were empty.\r\n  * call a new `merge_metadata` function.\r\n  * `merge_data` is rather aggressive. It merges if the keys are identical. Let's mimick that here as well - merge if the keys are the same, and the values are identical, while assuming that `merge` took care of empty detetions.\r\n* `validate_detection_fields` should not - there's nothing to check so far.\r\n* `__getitem__` can return either a `data` element or a sliced detections object. Let's update so it sets the metadata as well.\r\n* __setitem__ is unaffected.\r\n* `merge_inner_detection_object_pair` should merge metadata similarly to how `merge` does it.\r\n* JsonSink and CsvSink handle writing detections to files. Let's not touch it yet.\r\n\r\nI believe I've covered everything, but we'll check later on. When testing, make sure to test these changes, as well as `ByteTrack.update_with_detections` and `sv.DetectionsSmoother.update_with_detection`.\r\n\r\n---\r\n\r\nHelpful links:\r\n* [Contribution guide](https://supervision.roboflow.com/develop/contributing/#how-to-contribute-changes)\r\n* Detections: [docs](https://supervision.roboflow.com/latest/detection/core/#detections), [code](https://github.com/roboflow/supervision/blob/develop/supervision/detection/core.py)\r\n* [Supervision Cheatsheet](https://roboflow.github.io/cheatsheet-supervision/)\r\n* [Colab Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89#scrollTo=pjmCrNre2g58)\r\n* [Prior metrics test Colab](https://colab.research.google.com/drive/1qSMDDpImc9arTgQv-qvxlTA87KRRegYN)",
    "comments": [
      {
        "user": "LinasKo",
        "body": "### Contribution guidelines\r\nIf you would like to make a contribution, please check that no one else is assigned already. Then leave a comment such as \"Hi, I would like to work on this issue\". We're happy to answer any questions about the task even if you choose not to contribute.\r\n\r\n### Testing\r\nPlease share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. You may use the [Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89). The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! :pray:"
      },
      {
        "user": "LinasKo",
        "body": "Note: For historical reasons, `Detections.empty()` and `Detections.is_empty()` are an exception. When returning frames from a video, for example, the same model may return some defined as `array` / `None` when successfully detecting objects, but different fields defined as `array` / `None` when nothing is detected. Therefore, empty detections are mostly checked with `Detections.is_empty()` and treated as a special case."
      },
      {
        "user": "souhhmm",
        "body": "Hey, I'd like to give this issue a try."
      }
    ]
  },
  {
    "issue_number": 1647,
    "title": "Remove Images for which there are no annotations",
    "author": "shanalikhan",
    "state": "closed",
    "created_at": "2024-11-02T13:36:04Z",
    "updated_at": "2024-11-04T09:19:01Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nHi, \r\nAny idea how to remove the images where there are no annotations or we need to do it manually in JSON in COCO Format?\n\n### Use case\n\nCleaning COCO\n\n### Additional\n\nN/a\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @shanalikhan,\r\n\r\nI'm converting this to a discussion."
      }
    ]
  },
  {
    "issue_number": 1218,
    "title": "tflite with supervison",
    "author": "abichoi",
    "state": "closed",
    "created_at": "2024-05-21T15:38:18Z",
    "updated_at": "2024-11-02T17:22:19Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi, does supervsion support tflite detection results? If not, is there a way to convert tflite outputs to the Detections class format?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @abichoi üëãüèª Thanks for your interest in `supervision`. As far as I know, it does not support `tflite`. What's the structure of your `tflite` inference result?"
      },
      {
        "user": "abichoi",
        "body": "Hi, thank you for your reply! I have figured out how the class Detection is formated and solved the problem by writing a function to convert the tflite output to the class Detection format."
      },
      {
        "user": "SkalskiP",
        "body": "That's awesome! üî• Good luck"
      }
    ]
  },
  {
    "issue_number": 1646,
    "title": "call from_ultralytics except: 'Results' object has no attribute 'names'",
    "author": "Mao-Sky",
    "state": "closed",
    "created_at": "2024-11-02T07:58:53Z",
    "updated_at": "2024-11-02T08:14:58Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\n+ windows 11\r\n+ python 3.8\r\n\r\n## Console output\r\n\r\nultralytics version:  8.0.20\r\nsupervision version:  0.20.0\r\nUltralytics YOLOv8.0.20  Python-3.8.20 torch-2.4.1+cu121 CPU\r\nYOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\r\nresults:  [Ultralytics YOLO <class 'ultralytics.yolo.engine.results.Boxes'> masks\r\ntype: <class 'torch.Tensor'>\r\nshape: torch.Size([15, 6])\r\ndtype: torch.float32\r\n + tensor([[2.81000e+02, 3.83000e+02, 3.89000e+02, 4.52000e+02, 7.25721e-01, 2.00000e+00],\r\n        [6.51000e+02, 2.82000e+02, 6.82000e+02, 3.08000e+02, 7.25578e-01, 2.00000e+00],\r\n\r\nresults[0]:  tensor([[2.81000e+02, 3.83000e+02, 3.89000e+02, 4.52000e+02, 7.25721e-01, 2.00000e+00],\r\n        [6.51000e+02, 2.82000e+02, 6.82000e+02, 3.08000e+02, 7.25578e-01, 2.00000e+00],\r\n\r\nTraceback (most recent call last):\r\n  File \"d:/moyy/work/yao/yolov/hello_yolov/01.supervision_hello.py\", line 18, in <module>\r\n    detections = sv.Detections.from_ultralytics(results)\r\n  File \"E:\\Users\\ProgramData\\miniconda3\\envs\\yolo8\\lib\\site-packages\\supervision\\detection\\core.py\", line 261, in from_ultralytics\r\n    class_names = np.array([ultralytics_results.names[i] for i in class_id])\r\n  File \"E:\\Users\\ProgramData\\miniconda3\\envs\\yolo8\\lib\\site-packages\\supervision\\detection\\core.py\", line 261, in <listcomp>\r\n    class_names = np.array([ultralytics_results.names[i] for i in class_id])\r\n  File \"E:\\Users\\ProgramData\\miniconda3\\envs\\yolo8\\lib\\site-packages\\ultralytics\\yolo\\engine\\results.py\", line 103, in __getattr__\r\n    raise AttributeError(f\"\"\"\r\nAttributeError:\r\n            'Results' object has no attribute 'names'. Valid 'Results' object attributes and properties are:\r\n\r\n            Attributes:\r\n                boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\r\n                masks (Masks, optional): A Masks object containing the detection masks.\r\n                probs (torch.Tensor, optional): A tensor containing the detection class probabilities.\r\n                orig_shape (tuple, optional): Original image size.\r\n\r\n## Python Code\r\n\r\n``` python\r\nimport cv2\r\nimport supervision as sv\r\n\r\nimport ultralytics\r\nfrom ultralytics import YOLO\r\n\r\nprint(\"ultralytics version: \", ultralytics.__version__)\r\nprint(\"supervision version: \", sv.__version__)\r\n\r\nimage = cv2.imread(\"car.jpg\")\r\n\r\nmodel = YOLO(\"yolov8n.pt\")\r\nresults = model(image, device=\"cpu\")\r\nprint(\"results: \", results)\r\n\r\nresults = results[0]\r\nprint(\"results[0]: \", results)\r\ndetections = sv.Detections.from_ultralytics(results)\r\n```\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1641,
    "title": "DetectionDataset merge fails when class name contains capital letter",
    "author": "Suhas-G",
    "state": "closed",
    "created_at": "2024-11-01T08:49:54Z",
    "updated_at": "2024-11-02T06:16:37Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nHello, thanks for this great library! I'm facing an issue while trying to merge 2 datasets when any of the class names contain a capital letter.\r\n\r\nError: \r\n```\r\nValueError: Class Animal not found in target classes. source_classes must be a subset of target_classes.\r\n```\r\n\r\nThe issue stems from the ```merge_class_lists function``` at https://github.com/roboflow/supervision/blob/37cacec70443a2c28ea6642f6bc54e6c5151c111/supervision/dataset/utils.py#L53\r\nwhere the class names are converted to lower-case, but ```build_class_index_mapping``` keeps the class names as it is. For my use case, I was able to get around by removing the lower-case conversion.\n\n### Environment\n\n- Supervision 0.24.0\r\n- OS: Windows 10\r\n- Python 3.10.14\n\n### Minimal Reproducible Example\n\nExample: I downloaded 2 roboflow datasets - https://universe.roboflow.com/cvlab-6un5p/cv-lab-kpdek and https://universe.roboflow.com/padidala-indhu-e1dhl/animals-gzsxr and tried to merge them\r\n\r\n```python\r\nimport supervision as sv\r\n\r\n\r\ndef main():\r\n    ds1 = sv.DetectionDataset.from_coco(\"data/CV-LAB.v1i.coco/train\", \"data/CV-LAB.v1i.coco/train/_annotations.coco.json\")\r\n    ds2 = sv.DetectionDataset.from_coco(\"data/Animals.v1i.coco/train\", \"data/Animals.v1i.coco/train/_annotations.coco.json\")\r\n\r\n    sv.DetectionDataset.merge([ds1, ds2])\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @Suhas-G üëã\r\n\r\nWe'd like to treat classes with different capitalization as different classes. That is, \"Animal\" and \"animal\" are different.\r\n\r\nRight now I expect people to rename these manually inside Detections. However, adding a `Detections.rename_class(from: str, to: str)` and `DetectionsDataset.rename_class(from: str, to: str)` would be very useful!\r\n\r\n"
      },
      {
        "user": "Suhas-G",
        "body": "Hi @LinasKo, to treat classes with different capitalization as different classes, isn't it better to not call ```.lower()``` in the merge function? That way the merged list will have \"Animal\" as a different class. Note that in this example, there is no \"animal\" class but only \"Animal\" which is converted to lower case in merged list, and so \"Animal\" is not found while building the index mapping. "
      },
      {
        "user": "LinasKo",
        "body": "Are we calling that? If so, I believe we can remove it."
      }
    ]
  },
  {
    "issue_number": 1562,
    "title": "Connect Oriented Bounding Box to Metrics",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-10-03T11:50:31Z",
    "updated_at": "2024-11-01T09:45:43Z",
    "labels": [
      "good first issue",
      "hacktoberfest"
    ],
    "body": "# Connect Oriented Bounding Box to Metrics\r\n\r\n> [!TIP]\r\n> [Hacktoberfest](https://hacktoberfest.com/) is calling! Whether it's your first PR or your 50th, you‚Äôre helping shape the future of open source. Help us build the most reliable and user-friendly computer vision library out there! üå±\r\n\r\n---\r\n\r\nSeveral new features were recently added to supervision:\r\n* Mean Average Precision (mAP)\r\n* F1 Score\r\n* IoU calculation for Oriented Bounding Boxes\r\n\r\nIntersection Over Union (IoU) is the starting point when computing these metrics. It determines which detections are considered true positives. However, [take a look](https://github.com/roboflow/supervision/blob/d6aa72c0f2b158b838145a81ed5995db6a1e9015/supervision/metrics/mean_average_precision.py#L176)! The Oriented Box IoU is not supported yet! Help us add support by using `oriented_box_iou_batch`.\r\n\r\nHelpful links:\r\n* [Contribution guide](https://supervision.roboflow.com/develop/contributing/#how-to-contribute-changes)\r\n* Metrics:\r\n  * mAP metric: [docs](https://supervision.roboflow.com/develop/metrics/mean_average_precision/), [code](https://github.com/roboflow/supervision/blob/d6aa72c0f2b158b838145a81ed5995db6a1e9015/supervision/metrics/mean_average_precision.py#L25)\r\n  * F1 Score: [docs](https://supervision.roboflow.com/develop/metrics/f1_score/), [code](https://github.com/roboflow/supervision/blob/d6aa72c0f2b158b838145a81ed5995db6a1e9015/supervision/metrics/f1_score.py#L25)\r\n* Oriented box IoU calculation function: [docs](https://supervision.roboflow.com/develop/detection/utils/#supervision.detection.utils.oriented_box_iou_batch), [code](https://github.com/roboflow/supervision/blob/d6aa72c0f2b158b838145a81ed5995db6a1e9015/supervision/detection/utils.py#L143)\r\n* [Supervision Cheatsheet](https://roboflow.github.io/cheatsheet-supervision/)\r\n* [Colab Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89#scrollTo=pjmCrNre2g58)\r\n* [Prior metrics test Colab](https://colab.research.google.com/drive/1qSMDDpImc9arTgQv-qvxlTA87KRRegYN)",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @patel-zeel üëã \r\n\r\nI'm moving the IoU integration into metrics here. I believe we spoke about it briefly. Have you done substantial work on it already? If not, I will open it to the community."
      },
      {
        "user": "LinasKo",
        "body": "### Contribution guidelines\r\nIf you would like to make a contribution, please check that no one else is assigned already. Then leave a comment such as \"Hi, I would like to work on this issue\". We're happy to answer any questions about the task even if you choose not to contribute.\r\n\r\nPlease share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. You may use the [Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89). The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! :pray:"
      },
      {
        "user": "Prasadayus",
        "body": "Hi ,I would like to work on this issue. Please @LinasKo  assign me this issue"
      }
    ]
  },
  {
    "issue_number": 1633,
    "title": "labels or label isn't an existing parameter for box_annotate()",
    "author": "Jarradmorden",
    "state": "closed",
    "created_at": "2024-10-30T11:27:25Z",
    "updated_at": "2024-10-30T14:59:27Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\n  labels or label isn't an existing parameter for box_annotate(), what do we replace this with? is it needed I just removed it?\r\n\r\nNotebook = https://github.com/roboflow/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb\r\n\r\n\r\nbox_annotator = sv.BoxAnnotator()\r\nimage = cv2.imread(f\"C:/Users/jmorde02/DATA/2024-10-18_15-13-59\\sample\\left_3070_2024-10-18 15-17-30.481540.png\")\r\n\r\nmask_annotator = sv.MaskAnnotator()\r\nresults = base_model.predict(image)\r\n\r\nannotated_image = mask_annotator.annotate(\r\n\timage.copy(), detections=results\r\n)\r\n\r\nimages = []\r\nfor image_name in image_names:\r\n    image  = dataset.images[image_name]\r\n    annotations= dataset.annotations[image_name]\r\n    labels = [\r\n        dataset.classes[class_id]\r\n        for class_id\r\n        in annotations.class_id]\r\n    annotates_image = mask_annotator.annotate(\r\n        scene=image.copy(),\r\n        detections=annotations)\r\n    annotates_image = box_annotator.annotate(\r\n        scene=annotates_image,\r\n        detections=annotations\r\n        labels=labels)\r\n    images.append(annotates_image)\r\n\r\n\r\nAs you can see here the options\r\n\r\n(method) def annotate(\r\n    scene: ImageType@annotate,\r\n    detections: Detections,\r\n    custom_color_lookup: ndarray | None = None\r\n) -> ImageType@annotate\r\n\r\n\r\n### Environment\r\n\r\nsupervision==0.24.0\r\nWindows 11\r\nPython3.11\r\nCuda 11.8\r\n\r\n### Minimal Reproducible Example\r\n\r\n_No response_\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @Jarradmorden üëã\r\n\r\nThe supervision version there is indeed very old. Here's how you'd use the annotators right now:\r\n\r\n```\r\nmask_annotator = sv.MaskAnnotator()\r\nbox_annotator = sv.BoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator()\r\n\r\nimages = []\r\nfor image_name, image, annotations in dataset:\r\n    annotated_image = image.copy()\r\n    annotated_image = mask_annotator.annotate(\r\n        scene=annotated_image,\r\n        detections=annotations)\r\n    annotated_image = box_annotator.annotate(\r\n        scene=annotated_image,\r\n        detections=annotations)\r\n    annotated_image = label_annotator.annotate(\r\n        scene=annotated_image,\r\n        detections=annotations,\r\n    )\r\n    images.append(annotated_image)\r\n```\r\n\r\nThe `LabelAnnotator.annotate()` does accept a `labels` parameter, but you probably won't need it."
      },
      {
        "user": "onuralpszr",
        "body": "Converting to Q&A since this is not a bug"
      }
    ]
  },
  {
    "issue_number": 1313,
    "title": "Colormap for visualizing depth, normal and gradient images",
    "author": "hardikdava",
    "state": "open",
    "created_at": "2024-06-27T13:41:04Z",
    "updated_at": "2024-10-30T09:28:37Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nhello :wave: @SkalskiP @LinasKo ,\r\n\r\nwhat do you think about adding a feature to visualize depth images, normal image and graident images across various tasks?\n\n### Use case\n\n_No response_\n\n### Additional\n\n[here](https://learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/) is a full reference of implementation in opencv\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "Bhavay-2001",
        "body": "Hi, if this issue is something that the team is considering to add, I would like to be a part of it.\r\nThanks"
      },
      {
        "user": "Bhavay-2001",
        "body": "Hi @LinasKo, any updates on this issue?\r\nThanks"
      },
      {
        "user": "hardikdava",
        "body": "hello :wave:  @LinasKo @SkalskiP, any response?"
      }
    ]
  },
  {
    "issue_number": 1630,
    "title": "Support for Korean Characters in Annotator Function Fonts",
    "author": "YoungjaeDev",
    "state": "closed",
    "created_at": "2024-10-30T02:35:13Z",
    "updated_at": "2024-10-30T03:28:12Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI'm using the Annotator function in Roboflow Supervision and would like to add annotations in Korean. However, it seems that the default font doesn't support Korean characters. Are there any workarounds or recommended methods to enable Korean text in annotations?\r\n\r\nThanks in advance for your help!\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1623,
    "title": "Using supervision with onnxruntime",
    "author": "acode-x",
    "state": "closed",
    "created_at": "2024-10-27T19:04:45Z",
    "updated_at": "2024-10-28T14:17:49Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi,\r\nI am having yolo11.onnx model. How to use supervision along with onnxruntime?\r\n\r\n```python\r\n def callback(frame: np.ndarray, _: int) -> np.ndarray:\r\n    onnx_model_path = \"./yolo11n.onnx\"\r\n    session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\r\n  \r\n    input_tensor = preprocess(frame) # do something here?\r\n    outputs = session.run(None, {\"images\": input_tensor})\r\n    detections = postprocess(outputs) # do something here?\r\n    detections = tracker.update_with_detections(detections)\r\n    \r\nsv.process_video(source_path=\"people-walking.mp4\", target_path=\"result.mp4\", callback=callback)\r\n```\r\n\r\nI couldn't use torch / ultralytics because I had to run inference on edge device and it was too heavy.\r\nAppreciate if any help on same.\r\n\r\nThanks!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "acode-x",
        "body": "I have the same query with tflite-runtime as well."
      },
      {
        "user": "onuralpszr",
        "body": "@acode-x üëã  Hello,  We don't have onnxruntime yet to be supported but for edge cases I understand you wanted to be minimal, I made a rather small collab for you to run onnxruntime with supervision I hope you enjoy\r\n\r\nhttps://colab.research.google.com/drive/1eDM_MFuMgvb3znAXq31GtwQHYHPcNaL6?usp=sharing\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1545,
    "title": "[Tracking] Update ByteTrack to include ReID",
    "author": "tteresi7",
    "state": "open",
    "created_at": "2024-09-26T01:23:14Z",
    "updated_at": "2024-10-27T23:36:23Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nAccording to the original paper ReID was vital in ensuring a good IDF1 score, which for most practical applications is the most crucial metric.  Currently only IOU matching method is enabled.\n\n### Use case\n\n_No response_\n\n### Additional\n\nI recognize that this is a difficult task, but I think if you're going to stick strictly to IOU based trackers there isn't even a point, because IOU trackers lag behind state of the art.\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "atonalfreerider",
        "body": "I see that ReID from few-shot learning is a feature that works through the API\r\nhttps://x.com/skalskip92/status/1848858750335209686\r\n\r\nI think you need to sign up for early access here:\r\nhttps://docs.google.com/forms/d/e/1FAIpQLSco5oAhhiFGUYXqQOrecefqUHLjtdhgG1MIfVJ5kk5M-76few/viewform\r\n\r\nApparently this button appears when you have access to the feature:\r\n![GakItYLWoAAjF8f](https://github.com/user-attachments/assets/39710a09-06d0-4d7e-9d02-32c335a58767)\r\n\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 351,
    "title": "opencv-headless requirement breaks existing installation",
    "author": "adrianboguszewski",
    "state": "closed",
    "created_at": "2023-09-04T15:43:04Z",
    "updated_at": "2024-10-22T10:51:50Z",
    "labels": [
      "bug",
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nSupervision >= 0.12.0 requires opencv-headless. If I use opencv-python in my application and then install supervision, my opencv is broken (overwritten by the headless package).\r\nIt would be nice to configure it somehow not to install opencv-headless if any opencv package already installed.\n\n### Environment\n\nThis happens for supervision >= 0.12.0\r\nUbuntu 22.04\r\nPython 3.10\n\n### Minimal Reproducible Example\n\npip install opencv-python\r\npip install supervision==0.14.0\r\n\r\nimport cv2\r\nimport numpy as np\r\narr = np.zeros((10, 10), dtype=np.uint8)\r\ncv2.imshow(\"img\", arr)\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "onuralpszr",
        "body": "@adrianboguszewski  for workaround you can delete headless and reinstall full and use with full version and that would work.  I can make them explicitly separated and ask install one of them but if you just install supervision it won't have any opencv so we can change our install to force either let's say \"headless\" and \"full\"  like \"pip install supervision[full]\" but If you also try to install headless it will skip supervision but install headless in the of the day. So in the end of the day user also has to choose one of them and make sure not have both or delete one them. "
      },
      {
        "user": "adrianboguszewski",
        "body": "Yeah, that's what I did as a workaround. But also, I'm thinking about the mature solution. I don't know if there is any way to check opencv has been already installed so more research is needed.\r\n\r\nI think your proposition isn't bad. Let's say the default version is headless and if the user wants to have full opencv they must go with pip install supervision[full]"
      }
    ]
  },
  {
    "issue_number": 1610,
    "title": "How to define image size (dimensions) for detection inference while using sv.Detections.from_ultralytics() for images/videos?",
    "author": "tonmoy-TS",
    "state": "closed",
    "created_at": "2024-10-20T21:04:17Z",
    "updated_at": "2024-10-21T02:29:57Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHow can I define specific image sizes while running inference with supervision ?\r\n\r\nWhenever I am using `sv.Detections.from_ultralytics()` method for running inference with COCO-pretrained YOLO models on videos (1280 x 720 resolution), the inference output always shows a fixed dimension (384 x 640) for the frames.\r\n\r\nHere is the sample code I am running :\r\n```\r\nimport ultralytics\r\nfrom ultralytics import YOLO\r\nimport supervision as sv\r\n\r\nmodel = YOLO(\"yolov9e.pt\")\r\nmodel.fuse()\r\n\r\nselected_classes = [1, 2, 3, 5, 7] # from COCO\r\nselected_class_info = [(class_id, CLASS_NAMES_DICT.get(class_id, f\"No class name found for Class ID: {class_id}\")) \r\n                       for class_id in selected_classes]\r\n\r\nvideo_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\r\nprint(video_info)\r\n\r\nstride =15\r\ngenerator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH,stride=stride) \r\niterator = iter(generator)\r\nframe = next(iterator)\r\nfps = video_info.fps\r\n\r\nbounding_box_annotator = sv.BoxAnnotator(thickness=1)\r\nlabel_annotator = sv.LabelAnnotator(text_thickness=1, text_scale=0.4, text_padding=1)\r\n\r\n# Define callback function for video processing\r\ndef callback(frame: np.ndarray, index: int, fps: int) -> np.ndarray:\r\n    \r\n    # Resize frames using cv2\r\n    #frame = cv2.resize(frame, (video_info.width, video_info.height))\r\n    #print(\"Current frame dimensions: {}x{}\".format(frame.shape[1], frame.shape[0]))      \r\n\r\n    results = model(frame, verbose=True)[0]\r\n    detections = sv.Detections.from_ultralytics(results)\r\n    detections = detections[np.isin(detections.class_id, selected_classes)]\r\n    detections = detections[detections.confidence > 0.1]\r\n\r\n    # annotate\r\n    labels = [\r\n        f\"{model.model.names[class_id]} {confidence:0.2f}\" for confidence, class_id\r\n        in zip(detections.confidence, detections.class_id)\r\n    ]\r\n\r\n    annotated_frame = bounding_box_annotator.annotate(scene=frame, detections=detections)\r\n    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\r\n\r\n    return annotated_frame\r\n\r\n# Process video\r\nwith sv.VideoSink(target_path=TARGET_VIDEO_PATH, video_info=video_info) as sink:\r\n    for index, frame in enumerate(tqdm(generator, desc=\"Processing Video\", unit=\"frame\", total=video_info.total_frames // stride)):\r\n        annotated_frame = callback(frame, index, fps)\r\n        sink.write_frame(annotated_frame)\r\n```\r\n\r\nHere's what the output looks like:\r\n\r\n```\r\nProcessing Video:   1%|                     | 6/1024 [00:02<03:54,  4.35frame/s]\r\n\r\n0: 384x640 16 cars, 1 truck, 5 traffic lights, 20.3ms\r\nSpeed: 3.5ms preprocess, 20.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\r\n\r\n0: 384x640 1 person, 13 cars, 1 truck, 5 traffic lights, 22.7ms\r\nSpeed: 1.7ms preprocess, 22.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\r\n\r\n0: 384x640 16 cars, 1 truck, 5 traffic lights, 22.3ms\r\nSpeed: 2.0ms preprocess, 22.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\r\n```\r\n\r\nEven if I resize the frames inside the callback function:\r\n\r\n```\r\ndef callback(frame: np.ndarray, index: int, fps: int) -> np.ndarray:\r\n    ...\r\n    frame = cv2.resize(frame, (video_info.width, video_info.height))\r\n    print(\"Current frame dimensions: {}x{}\".format(frame.shape[1], frame.shape[0]))\r\n\r\n    results = model(frame, verbose=True)[0]\r\n    detections = sv.Detections.from_ultralytics(results)\r\n    ...\r\n```\r\nI still have the same inference outputs:\r\n```\r\nProcessing Video:   0%|                     | 1/1024 [00:01<30:54,  1.81s/frame]\r\nCurrent frame dimensions: 1280x720\r\n\r\n0: 384x640 14 cars, 5 traffic lights, 18.3ms\r\nSpeed: 1.7ms preprocess, 18.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\r\nCurrent frame dimensions: 1280x720\r\n\r\n0: 384x640 1 person, 17 cars, 6 traffic lights, 39.9ms\r\nSpeed: 2.5ms preprocess, 39.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\r\n```\r\n\r\nHowever, if I am running the models separately (without supervision), this isn't a problem.  Because I can define the image size. For example:\r\nRunning from Ultralytics (using `imgsz=1280`):\r\n```\r\nfrom ultralytics import YOLO\r\nmodel = YOLO('yolov9e.pt')\r\nresult = model.predict( src=SOURCE_VIDEO_PATH, save=True, imgsz=1280, conf=0.1, classes=[1, 2, 3, 5, 7])\r\n```\r\n\r\nor, from the Github package (using `img=1280`):\r\n```\r\n%cd yolov9\r\n!python detect.py  --img 1280 --conf 0.1 --weights /yolov9/weights/yolov9-e.pt --source SOURCE_VIDEO_PATH --verbose 1 --classes 1 2 3 5 7 --line-thickness 1\r\n```\r\n\r\nAnd using the YOLO models from Ultralytics or the Github package is showing better detection results than using the `sv.Detections.from_ultralytics()` method. I tried them for YOLOv8, YOLOv9, YOLOv11, and YOLOv8+SAHI and all showed the same trends. So, am I missing something ?\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "tonmoy-TS",
        "body": "Found the solution. \r\n\r\nThe frame dimension can be fixed by simply using `imgsz`  as an argument of `model()`:\r\n\r\n`results = model(frame, imgsz=1280, verbose=True)[0]`\r\n\r\nAnd the output would be something like this:\r\n\r\n```\r\nProcessing Video:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1014/1024 [00:58<00:00, 18.21frame/s]\r\n\r\n0: 736x1280 20 cars, 1 bus, 5 traffic lights, 34.4ms\r\nSpeed: 2.2ms preprocess, 34.4ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\r\n\r\n0: 736x1280 22 cars, 1 bus, 5 traffic lights, 35.8ms\r\nSpeed: 2.3ms preprocess, 35.8ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\r\n```"
      }
    ]
  },
  {
    "issue_number": 1411,
    "title": "Increasing Video FPS running on CPU Using Threading",
    "author": "dsaha21",
    "state": "open",
    "created_at": "2024-07-27T21:45:43Z",
    "updated_at": "2024-10-19T01:27:38Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nI want to increase FPS of a video running on my CPU system. I tested with few annotated and object tracking videos. When I am running the frames without passing through the model the fps is still low thus resulting lesser while passing them through YOLO or any model. \r\n\r\nThe code snippet I am using is \r\n-----\r\n<img width=\"410\" alt=\"VideoSpeed1\" src=\"https://github.com/user-attachments/assets/f36f708c-b9aa-477f-af1d-031a90d6fa01\">\r\n\r\n\r\nSo, with the following method and running the normal frames I am getting something like the following :\r\n\r\n<img width=\"469\" alt=\"VideoSpeed2\" src=\"https://github.com/user-attachments/assets/438503da-2746-4bc4-8bd4-be12099cef15\">\r\n\r\nWith normal supervision's frame generator - fps is around 1-10 max \r\nWith threading its increasing to a greater value\n\n### Use case\n\nIf we notice there is a significant change with threading. I was wondering if we could add a MainThread Class in the supervision utils in sv.VideoInfo or add a total new class so that frames running on CPU can have such fps. Let me know if we can handle such case. I can share the python file on drive if necesssary. \r\n\r\nThanks \n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "yeldarby",
        "body": "Have you tried [InferencePipeline](https://inference.roboflow.com/using_inference/inference_pipeline/) from our other open source repo? It handles multithreading for video and [can even handle processing multiple streams concurrently](https://blog.roboflow.com/vision-models-multiple-streams/)."
      },
      {
        "user": "dsaha21",
        "body": "Hi @yeldarby, Let me give it a try with the Inference Pipeline. If its successful, I will close the issue. \r\n\r\nThanks for the help üëç "
      },
      {
        "user": "SkalskiP",
        "body": "Hi @dsaha21, `get_video_frames_generator` was meant to be a very simple utility. I agree with @yeldarby. If you want a higher fps throughput [`InferencePipeline`](https://inference.roboflow.com/using_inference/inference_pipeline/) is for you. Also are you sure you got `0.17` fps? It seems super low. "
      }
    ]
  },
  {
    "issue_number": 1594,
    "title": "Why does traffic analysis output differ from the video in readme file?",
    "author": "INF800",
    "state": "open",
    "created_at": "2024-10-13T21:43:18Z",
    "updated_at": "2024-10-15T07:09:19Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI tried running the same code in readme to generate video outputs but accuracy is not upto mark.\r\n\r\nHere are videos with different thresholds: \r\n\r\nhttps://drive.google.com/drive/folders/1TFcEJcSvVSQXaMEYQTnhw2s-QNXP-LOz?usp=sharing\r\n\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "INF800",
        "body": "FYI, I used roboflow API instead of running it locally.\r\n\r\ncc: @onuralpszr "
      },
      {
        "user": "INF800",
        "body": "I see that default model id is `vehicle-count-in-drone-video/6` was it supposed to be `8` instead of `6`\r\n\r\ncc: @onuralpszr "
      },
      {
        "user": "INF800",
        "body": "8 is giving resource error\r\n"
      }
    ]
  },
  {
    "issue_number": 1587,
    "title": "Add default color: PolygonZoneAnnotator, drawing functions",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-10-10T16:31:48Z",
    "updated_at": "2024-10-12T16:07:34Z",
    "labels": [
      "hacktoberfest"
    ],
    "body": "# Add default color: PolygonZoneAnnotator, drawing functions\r\n\r\n> [!TIP]\r\n> [Hacktoberfest](https://hacktoberfest.com/) is calling! Whether it's your first PR or your 50th, you‚Äôre helping shape the future of open source. Help us build the most reliable and user-friendly computer vision library out there! üå±\r\n\r\nIn a [recent stream](https://www.youtube.com/live/RvVLqXdhdVQ), we noticed that some drawing functions and annotators require you to specify a color. Please give these a default color!\r\n\r\nOf course, we wish to have `sv.Color.ROBOFLOW`.\r\n\r\nCheck [polygon annotator](https://supervision.roboflow.com/latest/detection/tools/polygon_zone/#supervision.detection.tools.polygon_zone.PolygonZone.trigger) and [draw functions](https://supervision.roboflow.com/latest/utils/draw/)!\r\n\r\n---\r\n\r\nHelpful links:\r\n* [Contribution guide](https://supervision.roboflow.com/develop/contributing/#how-to-contribute-changes)\r\n* [Supervision Cheatsheet](https://roboflow.github.io/cheatsheet-supervision/)\r\n* [Colab Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89#scrollTo=pjmCrNre2g58)\r\n* [Prior metrics test Colab](https://colab.research.google.com/drive/1qSMDDpImc9arTgQv-qvxlTA87KRRegYN)",
    "comments": [
      {
        "user": "souhhmm",
        "body": "Hey, I am already working on the issue #1585, would it be possible to assign this one to me as well?"
      },
      {
        "user": "DivyaVijay1234",
        "body": "Hello, I would like to work on this issue. Please assign it to me."
      },
      {
        "user": "LinasKo",
        "body": "@souhhmm, let's see how yours goes first, before taking another üòâ \r\n\r\nAssigning it to you, @DivyaVijay1234!"
      }
    ]
  },
  {
    "issue_number": 1566,
    "title": "Bugfix: Class-agnostic mAP",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-10-03T19:21:08Z",
    "updated_at": "2024-10-11T15:51:28Z",
    "labels": [
      "bug",
      "help wanted",
      "hacktoberfest"
    ],
    "body": "# Bugfix: Class-agnostic metrics.\r\n\r\n> [!TIP]\r\n> [Hacktoberfest](https://hacktoberfest.com/) is calling! Whether it's your first PR or your 50th, you‚Äôre helping shape the future of open source. Help us build the most reliable and user-friendly computer vision library out there! üå±\r\n\r\nWe recently realized that the `class_agnostic` argument in `MeanAveragePrecision` does nothing. Oops! Fix the algorithm, so if this argument is set, the metric treats all predictions and targets as if they belong to the same class.\r\n\r\n---\r\n\r\nHelpful links:\r\n* [Contribution guide](https://supervision.roboflow.com/develop/contributing/#how-to-contribute-changes)\r\n* Metrics:\r\n  * mAP metric: [docs](https://supervision.roboflow.com/develop/metrics/mean_average_precision/), [code](https://github.com/roboflow/supervision/blob/d6aa72c0f2b158b838145a81ed5995db6a1e9015/supervision/metrics/mean_average_precision.py#L25)\r\n  * F1 Score: [docs](https://supervision.roboflow.com/develop/metrics/f1_score/), [code](https://github.com/roboflow/supervision/blob/d6aa72c0f2b158b838145a81ed5995db6a1e9015/supervision/metrics/f1_score.py#L25)\r\n* [Supervision Cheatsheet](https://roboflow.github.io/cheatsheet-supervision/)\r\n* [Colab Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89#scrollTo=pjmCrNre2g58)\r\n* [Prior metrics test Colab](https://colab.research.google.com/drive/1qSMDDpImc9arTgQv-qvxlTA87KRRegYN)",
    "comments": [
      {
        "user": "LinasKo",
        "body": "### Contribution guidelines\r\nIf you would like to make a contribution, please check that no one else is assigned already. Then leave a comment such as \"Hi, I would like to work on this issue\". We're happy to answer any questions about the task even if you choose not to contribute.\r\n\r\n### Testing\r\nPlease share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. You may use the [Starter Template](https://colab.research.google.com/drive/1rin7WrS-UvVIe-_Gfxmu-yVslGphOq89). The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! :pray:"
      },
      {
        "user": "AHuzail",
        "body": "Hi, I would like to work on this issue. I would be happy to contribute. Let me know if there are any specific guidelines or additional information I should be aware of. Looking forward to your response."
      },
      {
        "user": "LinasKo",
        "body": "Hi @AHuzail, the issue is yours! I believe you will find the info you need in the documentation. Don't forget to make a Colab that tests your implementation!\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1576,
    "title": "Exported YOLOv8 Segmentation Data (OBB) Not Matching Annotations",
    "author": "Pavankunchala",
    "state": "closed",
    "created_at": "2024-10-05T00:37:44Z",
    "updated_at": "2024-10-06T01:27:54Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nDescription:\r\nI am using Roboflow as the annotation tool for labeling images for YOLOv8-segmentation. However, the issue I am encountering is that the exported annotations (OBB format) do not match the annotations I created in the Roboflow interface. The segmentation points and bounding boxes seem misaligned or improperly exported, which is causing issues with model training and leading to inaccurate results.\r\n\r\nSteps to Reproduce:\r\nAnnotate images for YOLOv8-segmentation using Roboflow.\r\nExport the dataset in OBB (Oriented Bounding Box) format.\r\nCompare the exported annotations with the annotations as seen in the Roboflow tool.\r\nExpected Behavior:\r\nThe exported annotations should exactly match the annotations created in the Roboflow app, ensuring consistency between what was labeled and the exported data.\r\n\r\nActual Behavior:\r\nThe exported annotations are misaligned or inaccurate, and they do not reflect the original annotations created within the Roboflow tool. This is impacting the quality of the dataset and, consequently, my model training.\r\n\r\nImpact:\r\nThe mismatched annotations lead to poor model performance as the training data is inconsistent with the actual labeled objects.\r\nThe segmentation results during inference are inaccurate due to improper training data.\r\n\r\n![Figure_1](https://github.com/user-attachments/assets/aed8666b-2908-4180-8334-83f2e0cb0dc1)\r\n\r\n\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "@Pavankunchala hello I understand that you have problem `roboflow platform` itself not the supervision ? Based on what I understand I am closing this issue because it is not a bug for `supervision` but you can ask in roboflow support or discord channel to find better answer related to this case I hope that will help you out, If I am wrong please re-open the issue and explain and sharing minimal google collab would be nice as well.\r\n\r\n\r\nThank you. "
      }
    ]
  },
  {
    "issue_number": 1567,
    "title": "Bug in git-committers-plugin-2, v2.4.0",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-10-03T21:29:35Z",
    "updated_at": "2024-10-04T23:41:07Z",
    "labels": [
      "bug",
      "documentation",
      "github_actions"
    ],
    "body": "At the moment, an error is observed when running the `mkdocs build` action from develop.\r\n\r\n```\r\nFile \"/opt/hostedtoolcache/Python/3.10.15/x64/lib/python3.10/site-packages/mkdocs_git_committers_plugin_2/plugin.py\", line 121, in get_contributors_to_file\r\n    'avatar': commit['author']['avatar_url'] if user['avatar_url'] is not None else ''\r\nUnboundLocalError: local variable 'user' referenced before assignment\r\n```\r\n\r\nThis is due to: https://github.com/ojacques/mkdocs-git-committers-plugin-2/issues/72\r\n\r\n",
    "comments": [
      {
        "user": "LinasKo",
        "body": "fyi @onuralpszr. We could drop the version, but let's wait till tomorrow's evening. Might be fixed by then."
      },
      {
        "user": "onuralpszr",
        "body": "> fyi @onuralpszr. We could drop the version, but let's wait till tomorrow's evening. Might be fixed by then.\r\n\r\nI will do what needs to be done I will keep pr  ready in case we need"
      },
      {
        "user": "onuralpszr",
        "body": "Fix attempt PR : https://github.com/ojacques/mkdocs-git-committers-plugin-2/pull/73"
      }
    ]
  },
  {
    "issue_number": 1384,
    "title": "[LabelAnnotator, RichLabelAnnotator, VertexLabelAnnotator] - allow `text_color` to be `Union[Color, ColorPalette]`",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-07-19T13:06:44Z",
    "updated_at": "2024-10-03T10:44:28Z",
    "labels": [
      "enhancement",
      "help wanted",
      "api:annotator"
    ],
    "body": "### Description\r\n\r\n[`LabelAnnotator`](https://supervision.roboflow.com/develop/detection/annotators/#supervision.annotators.core.LabelAnnotator), [`RichLabelAnnotator`](https://supervision.roboflow.com/develop/detection/annotators/#supervision.annotators.core.RichLabelAnnotator), and [`VertexLabelAnnotator`](https://supervision.roboflow.com/develop/keypoint/annotators/#supervision.keypoint.annotators.EdgeAnnotator.annotate) currently allow defining the background color as `Union[Color, ColorPalette]`. When the background color is a single `Color`, all boxes are drawn with the same color. If the background color is a `ColorPalette`, the detection index, class_id, or tracker_id is mapped to the corresponding color from the palette. Unfortunately, this functionality is not currently available for `text_color`.\r\n\r\nUpdate `LabelAnnotator`, `RichLabelAnnotator`, and `VertexLabelAnnotator` to enable defining `text_color` as `Union[Color, ColorPalette]` and implement the same mechanism as for the background color.\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "Bhavay-2001",
        "body": "Hi @SkalskiP, you can assign this to me. I'll take a look over thr weekend."
      },
      {
        "user": "SkalskiP",
        "body": "@Bhavay-2001 Done ‚úÖ "
      },
      {
        "user": "SkalskiP",
        "body": "The expected change was added to `LabelAnnotator` and `RichLabelAnnotator` via https://github.com/roboflow/supervision/pull/1387. @Bhavay-2001 would you be interested in adding this change to `VertexLabelAnnotator` as well?"
      }
    ]
  },
  {
    "issue_number": 1558,
    "title": "too many values to unpack (expected 5)",
    "author": "0xD4rky",
    "state": "closed",
    "created_at": "2024-09-30T18:40:27Z",
    "updated_at": "2024-10-01T16:58:16Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nI was using Supervision for an inference of yolov8 model on a sample video data. When I am trying to run the program using the code below, its showing me this error: \r\n```\r\nfor _, _, confidence, class_id, _\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: too many values to unpack (expected 5)\r\n```\r\n\r\nI have went through the updated code in the `Detections/core.py` but haven't found out exactly why this error is coming up.\r\n\r\n![Screenshot from 2024-10-01 00-03-56](https://github.com/user-attachments/assets/c475f80d-df40-44b7-9faa-15e33b92fabf)\r\n\r\n\r\nI have also referred to downgrading the `supervision` version to `0.3.0` but that has created more clashes as python versions and pytorch versions also clash. \r\n\r\n\r\n\n\n### Environment\n\n- supervision==0.23.0\r\n- opencv-python==4.10.0.84\r\n- ultralytics==8.3.1\r\n- python==3.12.0\n\n### Minimal Reproducible Example\n\n```import supervision as sv\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\n\r\nvideo_path = r'det/data/football.mp4'\r\n\r\nmodel = YOLO(\"yolov8s.pt\")\r\nvideo_info = sv.VideoInfo.from_video_path(video_path)\r\n\r\ndef process_frame(frame: np.ndarray,_) -> np.ndarray:\r\n\r\n    results = model(frame,imgsz = 1280)[0]\r\n    detections = sv.Detections.from_ultralytics(results)\r\n    box_annotator = sv.BoundingBoxAnnotator()\r\n\r\n    labels = [\r\n\tf\"{model.names[class_id]} {confidence:0.2f}\"\r\n\tfor _, _, confidence, class_id, _\r\n\tin detections\r\n    ]\r\n    frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\r\n\r\n    return frame\r\n\r\nsv.process_video(source_path=video_path, target_path=f\"result.mp4\", callback=process_frame)\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @0xD4rky üëã \r\n\r\nThis happens because the detections `__iter__` iterators used to produce less values in the past. In recent supervision versions you will have 6 values: \r\n\r\n```python\r\nlabels = [\r\n\tf\"{model.names[class_id]} {confidence:0.2f}\"\r\n\tfor xyxy, mask, confidence, class_id, tracker_id, data_slice\r\n\tin detections\r\n    ]\r\n```"
      },
      {
        "user": "LinasKo",
        "body": "Closing as I'm certain it's the solution üòâ "
      },
      {
        "user": "0xD4rky",
        "body": "Thanks a lot @LinasKo for clearing out, was stuck in this problem for some time!"
      }
    ]
  },
  {
    "issue_number": 1295,
    "title": "Does evaluation API support OBB",
    "author": "elifhelfer",
    "state": "closed",
    "created_at": "2024-06-20T03:39:18Z",
    "updated_at": "2024-10-01T16:57:35Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHello! I am working with YOLOv8 for an object detection issue. I need to evaluate my model on a testset with a custom filtering method, and would like to be able to use supervision to do things like calculate mAP50 and create a confusion matrix, with the MeanAveragePrecision and ConfusionMatrix classes. I saw in issue #1227 that support for Yolo OBB dataset format is getting added. \r\n\r\nIf i do this:\r\n```\r\ndataset = sv.DetectionDataset.from_yolo(... is_obb = True)\r\n\r\nmodel = YOLO(...)\r\n    def callback(image: np.ndarray) -> sv.Detections:\r\n    result = model(image)[0]\r\n    return sv.Detections.from_ultralytics(result)\r\n\r\nmean_average_precision = sv.MeanAveragePrecision.benchmark(\r\n    dataset = dataset,\r\n    callback = callback\r\n)\r\n```\r\n\r\nWill it work properly, considering the dataset and detections are OBB's? Or is it still something to be added?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @lfurtadoh üëã \r\n\r\nLooking at the code, `MeanAveragePrecision` does not support OBB - only standard object detection boxes.\r\nGiven we're expanding our OBB support, I'll open a feature request for the community.\r\n\r\nIf you're willing, feel free to help us out!"
      },
      {
        "user": "LinasKo",
        "body": "I looked into it deeper. In my opinion, the cleanest solution uses `shapely` which we don't currently have as a dependency. \r\n\r\n@SkalskiP, let's speak about that next time we meet."
      },
      {
        "user": "Bhavay-2001",
        "body": "Hi @SkalskiP, is the team considering this issue? I have started to think about where can the changes possibly be."
      }
    ]
  },
  {
    "issue_number": 1415,
    "title": "[InferenceSlicer] - it is hard to set specific tile dimensions ",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-07-29T21:01:08Z",
    "updated_at": "2024-10-01T12:48:36Z",
    "labels": [
      "bug"
    ],
    "body": "### Description\r\n\r\nI tried to use `InferenceSlicer` to divide the frame in four equally sized tiles and it turned out to be hard to do. \r\n\r\n```python\r\nimport numpy as np\r\nimport supervision as sv\r\nfrom inference import get_model\r\n\r\nmodel = get_model(model_id=\"football-ball-detection-rejhg/3\", api_key=ROBOFLOW_API_KEY)\r\n\r\nframe_generator = sv.get_video_frames_generator(source_path='/content/2e57b9_0.mp4')\r\nframe = next(frame_generator)\r\n\r\ndef callback(patch: np.ndarray) -> sv.Detections:\r\n    print(patch.shape)\r\n    result = model.infer(patch, confidence=0.3)[0]\r\n    return sv.Detections.from_inference(result)\r\n\r\nslicer = sv.InferenceSlicer(\r\n    callback=callback,\r\n    overlap_filter_strategy=sv.OverlapFilter.NONE,\r\n    slice_wh=(\r\n        (1920 // 2) * 1.1, \r\n        (1080 // 2) * 1.1\r\n    ),\r\n    overlap_ratio_wh=(0.1, 0.1)\r\n)\r\n\r\ndetections = slicer(frame).with_nms(threshold=0.1)\r\n```\r\n\r\nI was expecting the code above to produce 4 tiles with 10% overlap, but it created 9. This ended up being very wasteful as `InferenceSlicer` is expensive to run.\r\n\r\n```\r\n(594, 1056, 3)\r\n(594, 969, 3)\r\n(594, 18, 3)\r\n(545, 1056, 3)\r\n(545, 969, 3)\r\n(545, 18, 3)\r\n(10, 1056, 3)\r\n(10, 969, 3)\r\n(10, 18, 3)\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "eric220",
        "body": "@SkalskiP I've created a generate_grid_offset function that takes a grid shape argument (i.e. (2,2) in your case) and returns the required offsets. I can throw together a colab if you're interested. "
      },
      {
        "user": "SkalskiP",
        "body": "I'd love to see it! I already started to work on somehow related topics in this PR: https://github.com/roboflow/supervision/pull/1434"
      },
      {
        "user": "eric220",
        "body": "@SkalskiP Here's the colab:\r\nhttps://colab.research.google.com/drive/1syvRehgUFfu4jt7M8C7KcbSITakI8olt?usp=sharing\r\n\r\nIt's a rough draft, but you can get the idea. Just set the frame size and grid argument to whatever you want and it slices and dices into even sized tiles."
      }
    ]
  },
  {
    "issue_number": 1543,
    "title": "[InferenceSlicer] Contradictory documentation regarding overlap_ratio_wh",
    "author": "tibeoh",
    "state": "closed",
    "created_at": "2024-09-25T12:32:13Z",
    "updated_at": "2024-10-01T12:18:14Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\n`InferenceSlicer` object in the latest update. Specifically, the `overlap_ratio_wh` parameter is mentioned as both a new feature and deprecated in different sections of the release notes.\r\n\r\nIn the \"Changed\" section, it states:\r\n> InferenceSlicer now features an `overlap_ratio_wh` parameter, making it easier to compute slice sizes when handling overlapping slices. [#1434](https://github.com/supervisely/supervisely/issues/1434)\r\n\r\nHowever, in the \"Deprecated\" section, it mentions:\r\n> `overlap_ratio_wh` in InferenceSlicer.__init__ is deprecated and will be removed in supervision-0.27.0. Use `overlap_wh` instead.\r\n\r\nThis seems contradictory: on one hand, `overlap_ratio_wh` is presented as a new feature, while on the other hand, it is marked for deprecation and removal in favor of `overlap_wh`.\r\n\r\nCould you clarify whether `overlap_ratio_wh` should be used, or if we should transition directly to `overlap_wh`? Additionally, updating the documentation to make this distinction clear would help avoid confusion for future users.\r\n\r\nThank you!\n\n### Environment\n\n- Supervision 0.23.0\r\n- Python 3.11\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @tibeoh üëã \r\n\r\nThanks for spotting it! That was my mistake ‚Äî the new recommended parameter is `overlap_wh`, as computing the slices using the ratio was more complicated."
      },
      {
        "user": "LinasKo",
        "body": "I see you're willing to submit a PR, so I'll assign it to you. Let me know if you have any questions!"
      },
      {
        "user": "tibeoh",
        "body": "Hi @LinasKo \r\n\r\nMy PR is ready for review. üëç \r\n\r\nPlease note that this is my first contribution to this project, so I hope everything is done correctly. I‚Äôm open to any feedback or suggestions for improvement."
      }
    ]
  },
  {
    "issue_number": 790,
    "title": "[LineZone] - allow per class counting  ",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-01-26T13:20:45Z",
    "updated_at": "2024-10-01T09:45:59Z",
    "labels": [
      "enhancement",
      "good first issue",
      "api: linezone",
      "Q2.2024"
    ],
    "body": "### Description\n\nCurrently, [sv.LineZone](https://github.com/roboflow/supervision/blob/3024ddca83ad837651e59d040e2a5ac5b2b4f00f/supervision/detection/line_counter.py#L11) provides only aggregated counts - all classes are thrown into one bucket. In the past, many users have asked us to provide more granular - per class count. This can be achieved by adding `class_in_count` and `class_out_count` dictionaries that will store per-class counts.\n\n### API\n\n```python\nclass LineZone:\n\n    def __init__(\n        self,\n        start: Point,\n        end: Point,\n        triggering_anchors: Iterable[Position] = (\n            Position.TOP_LEFT,\n            Position.TOP_RIGHT,\n            Position.BOTTOM_LEFT,\n            Position.BOTTOM_RIGHT,\n        ),\n    ):\n        # Existing initialization code...\n\n        self.class_in_count: Dict[int, int] = {}\n        self.class_out_count: Dict[int, int] = {}\n\n    def trigger(self, detections: Detections) -> Tuple[np.ndarray, np.ndarray]:\n        crossed_in = np.full(len(detections), False)\n        crossed_out = np.full(len(detections), False)\n\n        # Required logic changes...\n```\n\n### Additional\n\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will definitely speed up the review process. Each change must be tested by the reviewer. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "RaghavvGupta",
        "body": "Hey @SkalskiP, I was wondering to work on this. I have this logic in mind, pls confirm if this approach is what you guys want?\r\n\r\nDictionaries to keep track of counts for each class\r\n\r\n`self.class_in_count: Dict[str, int] = {} self.class_out_count: Dict[str, int] = {} `\r\n\r\nUpdating per class-in and class-out count\r\n\r\n```\r\nclass_label = detections.class_labels[i]\r\n                if class_label not in self.class_in_count:\r\n                    self.class_in_count[class_label] = 0\r\n                self.class_in_count[class_label] += 1\r\n            else:\r\n                self.out_count += 1\r\n                crossed_out[i] = True\r\n\r\n                # Updating per-class out count\r\n                class_label = detections.class_labels[i]\r\n                if class_label not in self.class_out_count:\r\n                    self.class_out_count[class_label] = 0\r\n                self.class_out_count[class_label] += 1\r\n```"
      },
      {
        "user": "RaghavvGupta",
        "body": "Hey @SkalskiP üòÄüëã\r\nhttps://colab.research.google.com/drive/1HxRcfzqnS6axAlYPiPpe8WLblfdn6fC_?usp=sharing\r\n\r\nThis is what I was talking about. Please tell me whether this is what you want."
      },
      {
        "user": "SkalskiP",
        "body": "Hi, @RaghavvGupta, Would that be the implementation of a current count or the total count? "
      }
    ]
  },
  {
    "issue_number": 1381,
    "title": "[Detections] - enable `from_mmdetection` to extract `tracker_id` from MMDetection result ",
    "author": "rolson24",
    "state": "open",
    "created_at": "2024-07-18T17:42:06Z",
    "updated_at": "2024-09-26T06:02:27Z",
    "labels": [
      "enhancement",
      "api:detection"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nThis is somewhat similar to #1308 in that it would be modifying `from_mmdetection()` to support `TrackDataSample`s. This would allow for easy integration with the MMTracking library, which has several trackers implemented in it, as well as integration with the Match and Segment Anything (MASA) model.\n\n### Use case\n\n_No response_\n\n### Additional\n\nI imagine it could be pretty simple and would only require checking if the input object has the `pred_track_instances` attribute, and then it could simply add the tracks with their track ID's into a `Detections` object.\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @rolson24 I just managed to plug in your MASA code into my football demo. I'd never be able to do it without your help. As for MASA: not bad, but I was hoping for a slightly better result. Would that allow us to plug in all [MMTracking](https://github.com/open-mmlab/mmtracking) models?\r\n\r\nhttps://github.com/user-attachments/assets/2c3bd164-82a4-40db-b12d-04442f534da0\r\n\r\n"
      },
      {
        "user": "rolson24",
        "body": "I don't think so. It seems like MASA is built on the trackers in the MMDet library, which are set up slightly different than the trackers in MMTracking. The trackers in MMTracking seem to output dicts of numpy arrays, while the trackers in MMDet output objects called `TrackDataSample`. Also I tried to test MMTracking and it seems that it does not support the newer versions of their own libraries. (Max MMCV version is 2.1.0 and newest is 2.2.0, and max MMDet version is 3.0.0 and newest is 3.3.0) So in order to use MMTracking with any recent version of CUDA and pytorch, a user would have to compile their own binaries. MMTracking has not had a commit since last April, so it seems like OpenMMLab doesn't have the bandwidth to support it right now.\r\n\r\nSo maybe not worth trying to support it if its a dead library anyway."
      },
      {
        "user": "rolson24",
        "body": "As for the MASA performance, I also noticed it was pretty bad at handling occlusion and creating new track IDs. I looked a bit into the code and it seems like the actual tracking is a very simple tracking algorithm with no motion modeling. I think MASA has some pretty good potential if motion modeling was added, and a more strict matching pipeline was incorporated. I think the real benefit of MASA is its ability to plug into foundational open world detectors, and doesn't hold up as well in specific applications. I might try to play around and see if I can integrate it with a better tracker like BoTSORT with MASA as the ReID model."
      }
    ]
  },
  {
    "issue_number": 1320,
    "title": "YOLOv8 + ByteTrack integration issues",
    "author": "ddrisco11",
    "state": "open",
    "created_at": "2024-07-01T20:47:56Z",
    "updated_at": "2024-09-26T01:19:58Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHello! I'm currently building a program to detect deep sea creatures in submarine video. I am using YOLOv8 to make detections and ByteTrack to assign object IDs to these detections. My output includes both an annotated video (based exclusively on YOLO output) and a csv file of all distinct detections (determined as distinct by ByteTrack). I am having an issue where certain creatures are annotated in the video output, ie. detected by YOLO, but then they are omitted from the csv output ie. not assigned a tracking ID by ByteTrack. Please help! Thanks!\n\n### Additional\n\ndef process_video(video_path: str, output_path: str, model_path: str, location_path: str, start_time: str, time_col: int, lat_col: int, lon_col: int, depth_col: int, salinity_col: int, oxygen_col: int, altitude_col: int,\r\n                  confidence_threshold: float, iou_threshold: float, track_activation_threshold: float, minimum_matching_threshold: float, lost_track_buffer: int,\r\n                  frame_rate: int, min_box_area: int, aspect_ratio_thresh: float):\r\n    \"\"\"Process the video to track objects and save tracking data.\"\"\"\r\n    model = YOLO(model_path)\r\n    tracker = ByteTrack(\r\n        track_activation_threshold=track_activation_threshold,\r\n        minimum_matching_threshold=minimum_matching_threshold,\r\n        lost_track_buffer=lost_track_buffer\r\n    )\r\n    location_data = get_location_data(location_path, time_col, lat_col, lon_col, depth_col, salinity_col, oxygen_col, altitude_col)\r\n    start_time_seconds = time_to_seconds(start_time)\r\n\r\n    cap = cv2.VideoCapture(video_path)\r\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n    fps = cap.get(cv2.CAP_PROP_FPS)\r\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n\r\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\r\n    out = cv2.VideoWriter(output_path.replace('.csv', '.mp4'), fourcc, fps, (width, height))\r\n\r\n    tracking_info = {}\r\n    pbar = tqdm(total=frame_count, desc='Processing frames', leave=True, mininterval=10)\r\n\r\n    frame_index = 0\r\n    cached_boxes = None\r\n    cached_labels = None\r\n\r\n    try:\r\n        while cap.isOpened():\r\n            ret, frame = cap.read()\r\n            if not ret:\r\n                break\r\n\r\n            current_time = start_time_seconds + (frame_index / fps)\r\n            lat, lon, depth, salinity, oxygen, altitude = get_location_at_time(location_data, current_time)\r\n\r\n            if frame_index % 5 == 0:  # Process frame every 5 frames\r\n                results = process_frame(frame, model, confidence_threshold, iou_threshold)\r\n                cached_boxes = results.boxes.xyxy.numpy()  # Convert to numpy array\r\n                names = model.names  # Class names\r\n                labels = results.boxes.cls.numpy().astype(int)  # Convert to integer labels\r\n\r\n                cached_labels = [\r\n                    f\"{names[label]} {round(confidence, 2)}\"\r\n                    for label, confidence in zip(labels, results.boxes.conf.numpy())\r\n                ]\r\n\r\n            # Draw bounding boxes using cached detections and labels\r\n            annotated_frame = frame.copy()\r\n            if cached_boxes is not None and cached_labels is not None:\r\n                drawn_boxes = set()  # Track drawn boxes\r\n                for box, label in zip(cached_boxes, cached_labels):\r\n                    x1, y1, x2, y2 = map(int, box)  # Get box coordinates\r\n                    class_name = label.split()[0]  # Get class name from label\r\n\r\n                    # Check if the box is already drawn\r\n                    if (x1, y1, x2, y2) not in drawn_boxes:\r\n                        # Draw rectangle with red color (BGR: (0, 0, 255)) and thicker lines (thickness=3)\r\n                        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 3)\r\n                        # Put label text with red color\r\n                        cv2.putText(annotated_frame, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\r\n                        drawn_boxes.add((x1, y1, x2, y2))\r\n\r\n            # Write the frame to the output video\r\n            out.write(annotated_frame)\r\n\r\n            if frame_index % 5 == 0:\r\n                detections = sv.Detections.from_ultralytics(results)\r\n                detections = tracker.update_with_detections(detections)\r\n\r\n                for index in range(len(detections.class_id)):\r\n                    object_id = detections.tracker_id[index]\r\n                    class_name = model.names[int(detections.class_id[index])]\r\n                    confidence = detections.confidence[index]\r\n                    \r\n                    if object_id not in tracking_info:\r\n                        image_path = save_detection_image(frame, detections[index], object_id, current_time, SOURCE_VIDEO_PATH)\r\n                        tracking_info[object_id] = {\r\n                            'Class': class_name,\r\n                            'Confidence': confidence,\r\n                            'Start Time': seconds_to_time_str(int(current_time)),\r\n                            'End Time': seconds_to_time_str(int(current_time)),\r\n                            'Latitude': lat,\r\n                            'Longitude': lon,\r\n                            'Depth': depth,\r\n                            'Salinity': salinity,\r\n                            'Oxygen': oxygen,\r\n                            'Altitude': altitude,\r\n                            'Image Path': image_path,\r\n                            'All Classes': [class_name]\r\n                        }\r\n                    else:\r\n                        tracking_info[object_id]['End Time'] = seconds_to_time_str(int(current_time))\r\n                        tracking_info[object_id]['Latitude'] = lat\r\n                        tracking_info[object_id]['Longitude'] = lon\r\n                        tracking_info[object_id]['Depth'] = depth\r\n                        tracking_info[object_id]['Salinity'] = salinity\r\n                        tracking_info[object_id]['Oxygen'] = oxygen\r\n                        tracking_info[object_id]['Altitude'] = altitude\r\n                        tracking_info[object_id]['All Classes'].append(class_name)\r\n\r\n            pbar.update(1)\r\n            frame_index += 1",
    "comments": [
      {
        "user": "rolson24",
        "body": "Hi @ddrisco11!\r\n\r\nThis could be caused by several things, could you upload a test video and the model weights to google drive and share the link so we can reproduce this?\r\nMy initial thought is that the creatures are being detected inconsistently, and the object tracker is struggling to refind the tracks if they have been lost for several frames, but there is no way to verify without using your model on your test video.\r\n\r\nThanks"
      },
      {
        "user": "ddrisco11",
        "body": "Hi @rolson24 , thanks for the response! Here is a link to a short training video, the full code, and the model weights I am using. Please let me know if you need anything else. https://drive.google.com/drive/folders/11u0m7Koew1D7lPEZngvfSR762Rvz0BNr?usp=drive_link"
      },
      {
        "user": "rolson24",
        "body": "Thanks for the code, model weights, and video. I have done a few tests and it looks like there is a few things going on.\r\n\r\nFirst off, it looks like there is a small bug in supervision that makes the tracker_id's skip several numbers. This can be solved by using `minimum_consecutive_frames=2` for now. This may be part of your confusion.\r\n\r\nThe other part is that the tracker relies on high confidence detections to determine if it should create a new track. Most of the detections from your model have confidence values of less than 0.3. Generally a good performing model will have confidence values of around 0.8. The confidence values of the detections greatly affect how the tracker performs because the tracker uses it as a metric of how likely the detection will be detected again in the next frame, and thus if it should be an object to track. To increase the confidence values of the detections you will need more training data. I would recommend adding image augmentations to your existing training data and considering using a more powerful foundation model like DETIC with Roboflow autodistill to automatically label images and then train your smaller yolov8 model on those labeled images.\r\n\r\nThe final thing you can try is to reduce the `minimum_matching_threshold`. This parameter determines the minimum threshold of an existing track being matched to a new detection. It essentially combines both the confidence of the detection and how much the track and the detection overlap into one number. By reducing the threshold, you allow the tracker to track objects with lower confidence, but also track detections that happen to overlap with an existing track that corresponds to a different object. By reducing the `minimum_matching_threshold` you risk tracks switching between different objects, but you may be able to track lower confidence detections. This is unlikely though, and I would first recommend improving the performance of your object detector.\r\n"
      }
    ]
  },
  {
    "issue_number": 594,
    "title": "Include DEEP SORT TRACKING",
    "author": "yeongnamtan",
    "state": "open",
    "created_at": "2023-11-14T06:15:05Z",
    "updated_at": "2024-09-26T01:09:19Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nWould you consider including DEEP SORT Tracker in Supervision ?\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi, @yeongnamtan! üëãüèª Correct me if I'm wrong, but DeepSort requires loading the model in PyTorch or TensorFlow. We are trying to make Supervision not require installation of such heavy dependencies."
      },
      {
        "user": "yeongnamtan",
        "body": "@SkalskiP I did a comparison between ByteTrack (using LINE COUNTER),  and DeepSort for the same source video. DeepSort performed much better in terms of counting.\r\n![Picture1](https://github.com/roboflow/supervision/assets/105199717/61bb789f-9f65-4c13-b0bf-83c50c445976)\r\n\r\n\r\nFor ByteTrack, my settings as follow:\r\nbyte_tracker = sv.ByteTrack(track_thresh=0.3, track_buffer=60, match_thresh=0.9, frame_rate=30)\r\n\r\nFor DeepSort,  confidence level same at 30%\r\nDEEPSORT:\r\n  MODEL_TYPE: \"osnet_x_25\"\r\n  MAX_DIST: 0.1 # The matching threshold. Samples with larger distance are considered an invalid match\r\n  MAX_IOU_DISTANCE: 0.7 # Gating threshold. Associations with cost larger than this value are disregarded.\r\n  MAX_AGE: 30 # Maximum number of missed misses before a track is deleted\r\n  N_INIT: 3 # Number of frames that a track remains in initialization phase\r\n  NN_BUDGET: 100 # Maximum size of the appearance descriptors gallery\r\n  \r\n\r\n"
      },
      {
        "user": "tteresi7",
        "body": "> Hi, @yeongnamtan! üëãüèª Correct me if I'm wrong, but DeepSort requires loading the model in PyTorch or TensorFlow. We are trying to make Supervision not require installation of such heavy dependencies.\r\n\r\nDeepSORT has no such requirements.  It only requires the bounding box detection and a feature vector.  The feature vector can be generated via a secondary model or extracting a specific layer of another model.  It doesn't matter if it's TF, PyTorch or whatever."
      }
    ]
  },
  {
    "issue_number": 1522,
    "title": "Problem to use supervision with docker image's nvcr.io/nvidia/pytorch",
    "author": "MattBlue92",
    "state": "closed",
    "created_at": "2024-09-18T12:39:02Z",
    "updated_at": "2024-09-18T14:38:21Z",
    "labels": [
      "bug",
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nIn [1]: import supervision as sv\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[1], line 1\r\n----> 1 import supervision as sv\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/supervision/__init__.py:9\r\n      6 except importlib_metadata.PackageNotFoundError:\r\n      7     __version__ = \"development\"\r\n----> 9 from supervision.annotators.core import (\r\n     10     BackgroundOverlayAnnotator,\r\n     11     BlurAnnotator,\r\n     12     BoundingBoxAnnotator,\r\n     13     BoxAnnotator,\r\n     14     BoxCornerAnnotator,\r\n     15     CircleAnnotator,\r\n     16     ColorAnnotator,\r\n     17     CropAnnotator,\r\n     18     DotAnnotator,\r\n     19     EllipseAnnotator,\r\n     20     HaloAnnotator,\r\n     21     HeatMapAnnotator,\r\n     22     IconAnnotator,\r\n     23     LabelAnnotator,\r\n     24     MaskAnnotator,\r\n     25     OrientedBoxAnnotator,\r\n     26     PercentageBarAnnotator,\r\n     27     PixelateAnnotator,\r\n     28     PolygonAnnotator,\r\n     29     RichLabelAnnotator,\r\n     30     RoundBoxAnnotator,\r\n     31     TraceAnnotator,\r\n     32     TriangleAnnotator,\r\n     33 )\r\n     34 from supervision.annotators.utils import ColorLookup\r\n     35 from supervision.classification.core import Classifications\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/supervision/annotators/core.py:5\r\n      2 from math import sqrt\r\n      3 from typing import List, Optional, Tuple, Union\r\n----> 5 import cv2\r\n      6 import numpy as np\r\n      7 import numpy.typing as npt\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/cv2/__init__.py:181\r\n    176             if DEBUG: print(\"Extra Python code for\", submodule, \"is loaded\")\r\n    178     if DEBUG: print('OpenCV loader: DONE')\r\n--> 181 bootstrap()\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/cv2/__init__.py:175, in bootstrap()\r\n    172 if DEBUG: print('OpenCV loader: binary extension... OK')\r\n    174 for submodule in __collect_extra_submodules(DEBUG):\r\n--> 175     if __load_extra_py_code_for_module(\"cv2\", submodule, DEBUG):\r\n    176         if DEBUG: print(\"Extra Python code for\", submodule, \"is loaded\")\r\n    178 if DEBUG: print('OpenCV loader: DONE')\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/cv2/__init__.py:28, in __load_extra_py_code_for_module(base, name, enable_debug_print)\r\n     26 native_module = sys.modules.pop(module_name, None)\r\n     27 try:\r\n---> 28     py_module = importlib.import_module(module_name)\r\n     29 except ImportError as err:\r\n     30     if enable_debug_print:\r\n\r\nFile /usr/lib/python3.10/importlib/__init__.py:126, in import_module(name, package)\r\n    124             break\r\n    125         level += 1\r\n--> 126 return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/cv2/typing/__init__.py:171\r\n    169 ExtractArgsCallback = _typing.Callable[[_typing.Sequence[GTypeInfo]], _typing.Sequence[GRunArg]]\r\n    170 ExtractMetaCallback = _typing.Callable[[_typing.Sequence[GTypeInfo]], _typing.Sequence[GMetaArg]]\r\n--> 171 LayerId = cv2.dnn.DictValue\r\n    172 IndexParams = _typing.Dict[str, _typing.Union[bool, int, float, str]]\r\n    173 SearchParams = _typing.Dict[str, _typing.Union[bool, int, float, str]]\r\n\r\nAttributeError: module 'cv2.dnn' has no attribute 'DictValue'\r\n\n\n### Environment\n\n-Python 3.10.12\r\n-supervision 0.23.0\r\n-ubuntu 24.04.1 LTS\r\n-opencv 4.7.0\r\n-opencv-contrib-python 4.10.0.84\r\n-opencv-python 4.10.0.84\r\n-opencv-python-headless 4.10.0.84\r\n- NVIDIA TITAN RTX\r\n- Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz\r\n\n\n### Minimal Reproducible Example\n\ndocker pull nvcr.io/nvidia/pytorch:24.08-py3\r\n\r\ndocker run --gpus all -it --rm nvcr.io/nvidia/pytorch:24.08-py3\r\n\r\npip install supervision\r\n\r\nipython\r\n\r\nimport supervision as sv\n\n### Additional\n\nHi there,\r\n\r\nI'm going to be mad to solve this bug, there are problem to install flash-attention, so the docker images nvcr.io/nvidia/pytorch is only way to get flash-attention but supervision don't work.\r\n\r\nI have some data that cannot be upload in colab, the only way is use local machine and for research goals I need to use florence-2 that working only with flash-attention.\r\nAny suggestion?\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @MattBlue92 üëã \r\n\r\nIt's unlikely we can help fix it, but here's something you can try:\r\nCan you run `pip install opencv-python` after installing supervision?\r\n\r\nThere's a very tiny chance this is a clash between `opencv-python-headless` and `opencv-python`, as one you install last overwrites a couple of folders, causing odd errors.\r\n\r\nAlso, @onuralpszr, I can't quite remember - were you the one who mentioned flash attention recently? Something relating to the new `from_` connector?"
      },
      {
        "user": "onuralpszr",
        "body": "Hello @LinasKo  and @MattBlue92  I decided to install and show you via this; https://asciinema.org/a/676564\r\nI hope that will explain everything :) all works and you can see in terminal record, I just ran container and start import and show all"
      },
      {
        "user": "MattBlue92",
        "body": "Hi @LinasKo  \r\n\r\nOpen-cv in nvcr.io/nvidia/pytorch:24.08-py3 is allready installed when the container is created. I'll try to install opencv and then install supervision first and opencv after.\r\n\r\nFor now I solved installing conda inside  the container and manually I installed the packages that I need and it worked."
      }
    ]
  },
  {
    "issue_number": 1513,
    "title": "Looking for a Model or Dataset for Detecting Objects Held in Hand",
    "author": "YoungjaeDev",
    "state": "closed",
    "created_at": "2024-09-13T23:56:54Z",
    "updated_at": "2024-09-14T19:59:05Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi,\r\n\r\nI‚Äôm trying to detect objects held in hand. Do you know of any models or datasets that are well-suited for this task?\r\n\r\nIf labeling is required, would it be better to use `YOLO-world` for bbox grounding?\r\n\r\nAdditionally, there are a large number of product classes involved. I‚Äôm wondering if it would be better to only detect objects and handle class recognition through retrieval methods.\r\n\r\nThank you!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "Converting to discussion "
      }
    ]
  },
  {
    "issue_number": 1437,
    "title": "[KeyPoints] - `KeyPoints` doesn't work like `Detections` filtering",
    "author": "SkalskiP",
    "state": "open",
    "created_at": "2024-08-08T11:08:55Z",
    "updated_at": "2024-09-11T12:34:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug\r\n\r\nWhen trying to filter `KeyPoints` by `confidence` like this:\r\n\r\n```python\r\nfrom inference import get_model\r\n\r\npitch_detection_model = get_model(model_id=\"football-field-detection-f07vi/14\", api_key=ROBOFLOW_API_KEY)\r\n\r\nresult = pitch_detection_model.infer(frame, confidence=0.3)[0]\r\nkeypoints = sv.KeyPoints.from_inference(result)\r\n\r\nkeypoints[keypoints.confidence > 0.5]\r\n```\r\n\r\nI get this:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n[<ipython-input-21-6a7425a97626>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 keypoints[keypoints.confidence > 0.5]\r\n\r\n[/usr/local/lib/python3.10/dist-packages/supervision/keypoint/core.py](https://localhost:8080/#) in __getitem__(self, index)\r\n    556             xy=self.xy[index],\r\n    557             confidence=self.confidence[index] if self.confidence is not None else None,\r\n--> 558             class_id=self.class_id[index] if self.class_id is not None else None,\r\n    559             data=get_data_item(self.data, index),\r\n    560         )\r\n\r\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\r\n```\r\n\r\nThis is a symptom of a much broader problem - the `KeyPoints` object does not allow the skeleton of different objects to have different lengths. \r\n\r\nI managed to work around this problem like this, but it's a very hacky solution.\r\n\r\n```python\r\nkeypoints = sv.KeyPoints.from_inference(result)\r\nfilter = keypoints.confidence > 0.5\r\nkeypoints.xy = keypoints.xy[filter][np.newaxis]\r\nkeypoints.confidence = keypoints.confidence[filter][np.newaxis]\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "@SkalskiP  do we want to filter \"keypoint\" group (ex: I have 7 detected humans I want to filter last 4 human keypoints and left other 3  humans based on filter) or just points itself because filtering points itself visually can break. Also in case of \"shortages\" should we fill values in something like negative values for prevents np.newaxis change requirement"
      },
      {
        "user": "LinasKo",
        "body": "@onuralpszr At which point would the newaxis cause issues? Would one of the dimensions collapse?"
      },
      {
        "user": "onuralpszr",
        "body": "> @onuralpszr At which point would the newaxis cause issues? Would one of the dimensions collapse?\r\n\r\nMy thinking was, Not using \"newaxis\" at all case and also curious about solution because we also talked about this in earlier how to manage because \"filtering points\" cause issue also key points can be incomplete (half body detection or semi visible cases) "
      }
    ]
  },
  {
    "issue_number": 1492,
    "title": "plotting single bounding prompt with SAM2 inference ",
    "author": "nadeem414",
    "state": "closed",
    "created_at": "2024-08-29T21:23:21Z",
    "updated_at": "2024-09-03T17:44:34Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nboxes = np.array([[5.94719971e+02, 1.98900009e+02, 6.28320007e+02, 2.64900024e+02]])\r\n\r\n\r\nbox_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\r\nmask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\r\n\r\ndetections = sv.Detections(\r\n    xyxy=sv.mask_to_xyxy(masks=masks),\r\n    mask=masks.astype(bool)\r\n)\r\n\r\nsource_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\r\nsegmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\r\n\r\nsv.plot_images_grid(\r\n    images=[source_image, segmented_image],\r\n    grid_size=(1, 2),\r\n    titles=['source image', 'segmented image']\r\n)\n\n### Environment\n\nhow-to-segment-images-with-sam-2.ipynb on google collab notebook\n\n### Minimal Reproducible Example\n\npass single bbox to sv.mask_to_xyxy function in sam2 notebook. \n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "venkatram-dev",
        "body": "Created PR for notebook code fix\r\n\r\nhttps://github.com/roboflow/notebooks/pull/305"
      },
      {
        "user": "LinasKo",
        "body": "Hi @nadeem414,\r\n\r\nThank you for reporting the issue. The solution by @venkatram-dev is correct - you can solve this by changing\r\n\r\n```python\r\nmasks = np.squeeze(masks)\r\n```\r\n\r\ninto\r\n\r\n```python\r\nif boxes.shape[0] != 1:\r\n    masks = np.squeeze(masks)\r\n```\r\n\r\nWe'll update the notebook. Feel free to let me know if there are any more issues!"
      },
      {
        "user": "LinasKo",
        "body": "Closing as solved. Thanks @venkatram-dev!"
      }
    ]
  },
  {
    "issue_number": 1496,
    "title": "Support for Zero-Shot Object Detection Models (e.g., YOLO-World)",
    "author": "YoungjaeDev",
    "state": "closed",
    "created_at": "2024-08-31T00:31:03Z",
    "updated_at": "2024-08-31T00:33:32Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI'm curious about the current support for zero-shot object detection models in Roboflow Supervision. Specifically:\r\n\r\n1. Does Supervision currently support zero-shot object detection models like YOLO-World?\r\n\r\n2. If not, are there any plans to implement support for such models in the future?\r\n\r\n3. Are there any specific challenges or considerations for integrating zero-shot models into the Supervision framework?\r\n\r\nThank you for your time and consideration.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "YoungjaeDev",
        "body": "I found link for https://supervision.roboflow.com/develop/notebooks/zero-shot-object-detection-with-yolo-world/\r\n"
      }
    ]
  },
  {
    "issue_number": 1440,
    "title": "Invalid validations on KeyPoints class?",
    "author": "Chappie74",
    "state": "closed",
    "created_at": "2024-08-10T23:46:39Z",
    "updated_at": "2024-08-27T11:07:05Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nI'm trying to use the KeyPoints class, with my xy data as [ [x1,y1], [x2,y1], ....  ] \r\nBased on the information in the docs string this seems to be input needed. However, I run into an error.\r\n```\r\nValueError: xy must be a 2D np.ndarray with shape ((68, 2),), but got shape (68, 2)\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/6a7c65e1-78c0-4fdc-8c04-e152a20a3f3d)\r\n\r\nThe validation for the shape check does not seems to be consistent. Either that or I may be doing something wrong\r\n![image](https://github.com/user-attachments/assets/6e3f2b97-9510-4203-ae2c-feb0c4c9c5ef)\r\n\n\n### Environment\n\nSupervision: 0.22.0\r\nPython: 3.10\n\n### Minimal Reproducible Example\n\n```python\r\nsv.KeyPoints(xy=np.array([[1,3], [1,3], [1,3]]))\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Indeed, the docs are not correct. KeyPoints objects expects to hold N skeletons, with dimensions of `(number_of_skeletons x number_of_keypoints x keypoint_dimensions)`."
      },
      {
        "user": "SkalskiP",
        "body": "@LinasKo are you working on \"fix\" for it (doc update), or should I do it?"
      },
      {
        "user": "LinasKo",
        "body": "@SkalskiP, I wasn't. Feel free!"
      }
    ]
  },
  {
    "issue_number": 1450,
    "title": "Custom Symbol Annotators",
    "author": "mhsmathew",
    "state": "closed",
    "created_at": "2024-08-15T02:16:08Z",
    "updated_at": "2024-08-27T10:54:43Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nA way to add custom symbols of annotators onto the image. Or possibly even other images.\n\n### Use case\n\nI want to be able to annotator multiple classes of objects with different symbols, like Xs and Os.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @mhsmathew üëã \r\n\r\nThis is a duplicate of #460, so I'll close this.\r\n\r\nGood reminder that we still don't have this functionality - I'll ping you when we deploy it."
      },
      {
        "user": "LinasKo",
        "body": "Hi @mhsmathew, the new supervision release tomorrow shall include `IconAnnotator`. Feel free to try it out when you have the time :)"
      }
    ]
  },
  {
    "issue_number": 460,
    "title": "[IconAnnotator] - mark objects with custom icons / images",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2023-10-12T12:26:54Z",
    "updated_at": "2024-08-27T10:51:37Z",
    "labels": [
      "enhancement",
      "good first issue",
      "api:annotator",
      "Q1.2024"
    ],
    "body": "### Description\r\n\r\nCreate `IconAnnotator` - whose task is to mark objects related to [`sv.Detections`](https://supervision.roboflow.com/detection/core/#detections) with custom icons.\r\n\r\n![273726799-6b0ea345-bb22-4c3d-9e67-6a91371f852e](https://github.com/roboflow/supervision/assets/26109316/2f184480-c424-4724-a494-189d59592d13)\r\n\r\nIt should also allow you to control the icon position in relation to the box using [`sv.Position`](https://supervision.roboflow.com/geometry/core/#position).\r\n\r\n### API\r\n\r\n```python\r\nclass IconAnnotator(BaseAnnotator):\r\n\r\n    def __init__(\r\n        self,\r\n        icon_path: str,\r\n        position: Position = Position.TOP_CENTER\r\n    ):\r\n        pass\r\n\r\n    def annotate(\r\n        self,\r\n        scene: np.ndarray,\r\n        detections: Detections,\r\n    ) -> np.ndarray:\r\n        pass\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will definitely speed up the review process. Each change must be tested by the reviewer. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª \r\n",
    "comments": [
      {
        "user": "Rajarshi-Misra",
        "body": "@SkalskiP I'm interested in working on this one."
      },
      {
        "user": "SkalskiP",
        "body": "@Rajarshi-Misra you are assigned! "
      },
      {
        "user": "Shishuii",
        "body": "@SkalskiP Is this still available ? I would be glad to contribute to this. :)"
      }
    ]
  },
  {
    "issue_number": 1444,
    "title": "Incomplete docs",
    "author": "Bhavay-2001",
    "state": "closed",
    "created_at": "2024-08-13T07:12:02Z",
    "updated_at": "2024-08-26T10:16:23Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nThe docs to the `from_sam` method [here](https://github.com/roboflow/supervision/blob/d08d22dec6f932d273d3d217c64343a47d5972a1/supervision/detection/core.py#L624) feels like incomplete. In my opinion, the user should be provided with more details like what are the different `MODEL_TYPE` and the checkpoint for weights etc. \n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": []
  },
  {
    "issue_number": 1463,
    "title": "Notebook not found: Serialise Detections to a CSV File",
    "author": "ediardo",
    "state": "closed",
    "created_at": "2024-08-19T20:22:59Z",
    "updated_at": "2024-08-20T14:08:37Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nThe Colab in this [cookbook](https://supervision.roboflow.com/develop/notebooks/serialise-detections-to-csv/) is not found. \r\n\r\n<img width=\"1118\" alt=\"Screenshot 2024-08-19 at 1 19 21‚ÄØPM\" src=\"https://github.com/user-attachments/assets/07b23e28-0ccc-456d-a496-631e3600bb57\">\r\n\r\n```\r\nNotebook not found\r\nThere was an error loading this notebook. Ensure that the file is accessible and try again.\r\nEnsure that you have permission to view this notebook in GitHub and authorize Colab to use the GitHub API.\r\n\r\nhttps://github.com/roboflow/supervision/blob/develop/docs/notebooks/detections-to-jsonsink.ipynb\r\nCould not find detections-to-jsonsink.ipynb in https://api.github.com/repos/roboflow/supervision/contents/docs/no\r\n```\r\n\r\n### Environment\r\n\r\nBrowser only error: https://supervision.roboflow.com/develop/notebooks/serialise-detections-to-csv/\r\n\r\n### Minimal Reproducible Example\r\n\r\nSteps:\r\n\r\n1. Open the cookbook https://supervision.roboflow.com/develop/notebooks/serialise-detections-to-csv/\r\n2. Click on \"Open in Colab\"\r\n3. Get the 404 error\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "venkatram-dev",
        "body": "created PR https://github.com/roboflow/supervision/pull/1466"
      },
      {
        "user": "LinasKo",
        "body": "Thank you @ediardo and @venkatram-dev!\r\n\r\nClosing as complete.\r\n"
      }
    ]
  },
  {
    "issue_number": 1468,
    "title": "Research Paper ",
    "author": "olawalejuwonm",
    "state": "closed",
    "created_at": "2024-08-20T12:29:46Z",
    "updated_at": "2024-08-20T13:33:07Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nThis is a great library. I've used it for a research project, but I'm curious to know if there is any paper on it that i can cite?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @olawalejuwonm üëã ,\r\n\r\nGlad to hear it's useful! If you go to the [Repository page](https://github.com/roboflow/supervision), on the right hand side you'll see a \"Cite this repository\" button. That should provide a copyable snippet for you.\r\n\r\n<img width=\"365\" alt=\"Screenshot 2024-08-20 at 4 31 19‚ÄØPM\" src=\"https://github.com/user-attachments/assets/346a4087-84fe-4958-a9e9-f513976da736\">\r\n\r\nThanks again for the kind words!"
      }
    ]
  },
  {
    "issue_number": 1403,
    "title": "[TriangleAnnotator, DotAnnotator] - allow customization of the contour color",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-07-25T14:35:33Z",
    "updated_at": "2024-08-18T20:31:42Z",
    "labels": [
      "enhancement",
      "help wanted",
      "api:annotator"
    ],
    "body": "### Description\r\n\r\n`TriangleAnnotator` and `DotAnnotator` allow rendering an outline around a triangle and a dot, respectively. This feature is activated by setting `outline_thickness` to a value greater than 0. However, there is no way to customize the outline color.\r\n\r\nAdd a new argument `outline_color` to both of these annotators. Set the type of `outline_color` as `Union[Color, ColorPalette]`. `outline_color` should integrate with `color_lookup` like other colors in supervision annotators.\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "Bhavay-2001",
        "body": "Hi, I can give it a try"
      },
      {
        "user": "SkalskiP",
        "body": "@Bhavay-2001, maybe it would be better if you finish https://github.com/roboflow/supervision/issues/1384 first?"
      },
      {
        "user": "Bhavay-2001",
        "body": "Yeah definitely, only after that. "
      }
    ]
  },
  {
    "issue_number": 1453,
    "title": "`sv.Detections.from_ultralytics()` can't handle segmentation results `ultralytics.engine.results.Results` from ultralytics `SAM()` when using `bboxes` or `points` arguments",
    "author": "xaristeidou",
    "state": "closed",
    "created_at": "2024-08-15T15:34:16Z",
    "updated_at": "2024-08-15T20:00:19Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/christoforos/Documents/supervision/supervision/detection/core.py\", line 277, in from_ultralytics\r\n    class_id = ultralytics_results.boxes.cls.cpu().numpy().astype(int)\r\nAttributeError: 'NoneType' object has no attribute 'cls'\r\n```\n\n### Environment\n\n- Supervision: 0.22.0\n\n### Minimal Reproducible Example\n\n```\r\nimport cv2\r\nimport supervision as sv\r\nfrom ultralytics import SAM\r\n\r\nimage = cv2.imread(\"image.jpg\")\r\n\r\nmodel = SAM(\"mobile_sam.pt\")\r\nresults = model(\r\n    image,\r\n    bboxes=[[100, 100, 200, 200]],\r\n)\r\n\r\ndetections = sv.Detections.from_ultralytics(results[0])\r\n```\n\n### Additional\n\nWhen predicting with any model using `SAM()` class from `ultralytics`, if we don't specify `bboxes` or `points`  arguments in the `predict()` function of `SAM()` model, it has expected behaviour.\r\n\r\nThe problem is when we pass `bboxes` or `points` arguments, which will results in `ultralytics.engine.results.Results` to contain `None` type for `boxes`, `probs` attributes (not sure if `probs` attribute affects the process but `boxes` does for sure).\r\n\r\nAs a results, mask annotators (polygon, halo, mask) also can't be used.\r\n\r\nWe could slightly modify `sv.Detection.from_ultralytics()` to handle this problem, by making an extra check and fill in required attributes of the class. I have implemented a fix of this issue, and I could submit a PR for review.\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "@xaristeidou  may I see branch + collab to test ? "
      },
      {
        "user": "onuralpszr",
        "body": "Let's fix that out quickly "
      },
      {
        "user": "xaristeidou",
        "body": "@onuralpszr PR #1454 with colab"
      }
    ]
  },
  {
    "issue_number": 1451,
    "title": "How does line zone get triggered?",
    "author": "abichoi",
    "state": "closed",
    "created_at": "2024-08-15T11:17:47Z",
    "updated_at": "2024-08-15T11:25:06Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi, I am trying to under how the line zone get triggered. \r\nLet's say I set the triggering_anchors as BOTTOM_LEFT.  Does xy of the bottom_left corner of the detection box of the object has to cross the line exactly to trigger the line zone?  For example, does the bottom left corner needs to be one pixel below the line in frame 1,  then the corner is on the line in frame 2, and then the corner is one pixel above the line in frame 3, to trigger the line?\r\n\r\nOr is there a buffer zone? For example, if the bottom_left corner is a few pixels BELOW the line in frame 1 and it is a few pixels ABOVE the line in frame 2, will it trigger the line?\r\n\r\nSorry I am aware how confusing my question sounds... \n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1397,
    "title": "Save detection area with CSVSink",
    "author": "robmarkcole",
    "state": "open",
    "created_at": "2024-07-23T19:04:51Z",
    "updated_at": "2024-08-09T10:07:51Z",
    "labels": [
      "bug",
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nI would like to save all the Detections data to csv. Currently the `area` is not saved, and my approach below isn't successful. This FR is to save the area (and potentially other detection attributes that are currently not saved)\r\n```python\r\nwith sv.CSVSink(csv_path) as sink:\r\n    for detection in detections:\r\n        sink.append(detection, {\"area\": detection.area})\r\nAttributeError: 'tuple' object has no attribute 'area'\r\n```\n\n### Use case\n\nI will perform filtering in a separate application \n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "robmarkcole",
        "body": "I've also got some functions which will convert the output of JSONSink to coco_json, for importing as pre-annotations. This currently has to calc the area and parse the xyxyxyxy with regex, so if these were exposed more simply this process would be streamlined"
      },
      {
        "user": "SkalskiP",
        "body": "@robmarkcole, you should pass the whole `sv.Detections` object as a `sink.append` argument. No loop is needed.\r\n\r\n```python\r\nwith sv.CSVSink(csv_path) as sink:\r\n    sink.append(detections, {\"area\": detections.area})     \r\n```"
      },
      {
        "user": "SkalskiP",
        "body": "@robmarkcole, I see you reacted. Did that solve your problem? If so, I'm closing the issue. ;)"
      }
    ]
  },
  {
    "issue_number": 1373,
    "title": "[InferenceSlicer] - instance segmentation inference results are not post-processed properly ",
    "author": "SaiJeevanPuchakayala",
    "state": "open",
    "created_at": "2024-07-17T09:23:56Z",
    "updated_at": "2024-08-09T08:04:08Z",
    "labels": [
      "bug"
    ],
    "body": "I am experiencing an issue where the model is performing inference on the full image resolution of 2048x2048 instead of the sliced resolution of 512x512 as intended. Below are the details of the function and the problem encountered.\r\n\r\n```python\r\ndef callback(image: np.ndarray) -> sv.Detections:\r\n    print(f\"Image shape in callback: {image.shape}\")  # Debugging line\r\n    result = model(image, device='cuda')[0]\r\n    return sv.Detections.fromultralytics(result)\r\n\r\n# Function to process an image and display the result\r\ndef process_image(image_path: str):\r\n    try:\r\n        image = cv2.imread(image_path)\r\n        print(\"Input image resolution\", image.shape)\r\n        if image is None:\r\n            raise FileNotFoundError(f\"Image at path {image_path} could not be loaded.\")\r\n\r\n        # Perform inference\r\n        slicer = sv.InferenceSlicer(\r\n            slice_wh=(512, 512),\r\n            overlap_ratio_wh=(0.2, 0.2),\r\n            thread_workers=10,\r\n            iou_threshold=0.5,\r\n            overlap_filter_strategy=sv.OverlapFilter.NON_MAX_MERGE,\r\n            callback=callback\r\n        )\r\n\r\n        detections = slicer(image)\r\n\r\n        # Filter detections based on confidence\r\n        detections = detections[detections.confidence > 0.3]\r\n\r\n        mask_annotator = sv.MaskAnnotator()\r\n        annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\r\n\r\n        # Draw contours around the masks and fit ellipses\r\n        for i in range(detections.mask.shape[0]):\r\n            mask = detections.mask[i].astype(np.uint8)\r\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n            cv2.drawContours(annotated_image, contours, -1, (0, 255, 0), 2)\r\n\r\n        plt.figure(figsize=(20, 20))\r\n        plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\r\n        plt.axis('off')\r\n        plt.show()\r\n\r\n        # Print the number of detections\r\n        print(f\"Number of detections: {len(detections.confidence)}\")\r\n\r\n        # Clear memory\r\n        del slicer, detections, annotated_image\r\n        torch.cuda.empty_cache()\r\n        gc.collect()\r\n\r\n    except Exception as e:\r\n        print(f\"An error occurred: {str(e)}\")\r\n        raise\r\n\r\n# Example usage\r\nprocess_image(image_path)\r\n```\r\n\r\n\r\n### Output\r\n```\r\nInput image resolution (2048, 1448, 3)\r\nImage shape in callback: (512, 512, 3)\r\n...\r\n0: 2048x2048 4 WholeRedRices, 8126.6ms\r\nSpeed: 34.0ms preprocess, 8126.6ms inference, 4.0ms postprocess per image at shape (1, 3, 2048, 2048)\r\nImage shape in callback: (512, 512, 3)\r\n...\r\n0: 2048x2048 1 BrokenRedRice, 12 WholeRedRices, 7223.1ms\r\nSpeed: 120.0ms preprocess, 7223.1ms inference, 118.0ms postprocess per image at shape (1, 3, 2048, 2048)\r\nImage shape in callback: (408, 218, 3)\r\n\r\n```\r\n\r\n\r\n### Issue\r\nThe model is performing inference on the full image resolution (2048x2048) instead of the sliced resolution (512x512) as specified in the InferenceSlicer. This results in longer inference times and processing on larger image chunks than intended.\r\n\r\n![image](https://github.com/user-attachments/assets/5d5a9f2e-b9e0-47a1-b937-b7948636b33f)\r\n\r\n### Steps to Reproduce\r\n1. Use the provided function code.\r\n2. Process an image with a resolution of 2048x2048.\r\n3. Observe the output logs indicating the inference is happening on 2048x2048 instead of 512x512.\r\n\r\n### Expected Behavior\r\nThe model should perform inference on 512x512 slices of the image, as specified in the InferenceSlicer.\r\n\r\n### Actual Behavior\r\nInference is performed on the full 2048x2048 image resolution.\r\n\r\n### Environment\r\n1. Model: Custom-trained on images with resolution 512 X 512 which are sliced from 2048 X 2048.\r\n2. Device: CUDA-enabled GPU\r\n\r\n### Additional Context\r\nAny insights or suggestions to ensure the model performs inference on the specified 512x512 slices would be greatly appreciated.",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @SaiJeevanPuchakayala üëãüèª Looks like `ultralytics` is resizing images before inference. This is probably because during training you passed `imgsz = 2048` as argument. Try to update `result = model(image, device='cuda')[0]` to `result = model(image, device='cuda', imgsz=640)[0]`, rerun the script."
      },
      {
        "user": "SaiJeevanPuchakayala",
        "body": "> Hi @SaiJeevanPuchakayala üëãüèª Looks like `ultralytics` is resizing images before inference. This is probably because during training you passed `imgsz = 2048` as argument. Try to update `result = model(image, device='cuda')[0]` to `result = model(image, device='cuda', imgsz=640)[0]`, rerun the script.\r\n\r\nThanks for the fix @SkalskiP.\r\n\r\nThe line of code below is working for me.\r\n```\r\nresult = model(image, device='cuda', imgsz=512)[0]\r\n```\r\n\r\nBut while stitching back the image the 512 images to 2048, I'm not able to get a few detections and annotations properly near the sliced region as shown in the image below.\r\n\r\n![image](https://github.com/user-attachments/assets/11d690a5-a89b-4de2-aed1-d6b1afab18b7)"
      },
      {
        "user": "SkalskiP",
        "body": "@SaiJeevanPuchakayala, what `overlap_ratio_wh` did you use? `(0.2, 0.2)`? It looks like there is no overlap at all."
      }
    ]
  },
  {
    "issue_number": 1419,
    "title": "How to access tracker id of objects that crossed the line zone/ objects that are inside polygon zone",
    "author": "abichoi",
    "state": "closed",
    "created_at": "2024-07-31T18:00:27Z",
    "updated_at": "2024-08-06T09:45:15Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nIs it possible to access the tracker id of objects that crossed the line zone or the id of objects that are inside the polygon zone?\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "rolson24",
        "body": "Hi @abichoi,\r\nYes you can access the tracker id of the objects that crossed the line zone like this:\r\n\r\n```\r\nzone = LineZone()\r\nfor frame in frames:\r\n  # get the detections as normal\r\n  detections_in_zone = detections[zone.trigger(detections=detections)]\r\n  tracker_ids_in_zone = detections_in_zone.tracker_id\r\n```\r\nThis will give you the detections in that zone at that frame. If you want to keep track of all of the detections that cross a line or enter a zone, you can just make a `set` with all of the tracker ids that gets added to every frame."
      },
      {
        "user": "SkalskiP",
        "body": "Thanks for the help, @rolson24 üôèüèª @abichoi. I'm converting this issue into a discussion if you have more questions."
      }
    ]
  },
  {
    "issue_number": 1429,
    "title": "Autodistill or Reparameterize?",
    "author": "adrielkuek",
    "state": "closed",
    "created_at": "2024-08-05T11:32:36Z",
    "updated_at": "2024-08-06T09:37:31Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nSo Robovision provides a framework with Autodistill to transfer knowledge from larger foundational models into smaller models on custom data that runs faster. https://roboflow.com/train/yolo-world-and-yolov8. I'm just curious on the differences between this framework, and that of Reparameterization of Yolo-World with the same custom dataset to improve efficiency on custom datasets (https://github.com/AILab-CVC/YOLO-World/blob/master/docs/reparameterize.md). From the Yolo-World paper, it does seem that reparameterization, at least for coco dataset's vocabulary, does seem to perform slightly better with Yolov8-fine-tuned.\r\n![Screenshot from 2024-08-05 19-29-10](https://github.com/user-attachments/assets/1e368ba4-f527-4e7e-bdd3-2990a9451332)\r\n\r\nJust wondering are there any merits to both of the methods? Have anybody evaluated either of the approach and which would be the recommended approach? Thanks!\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi, @adrielkuek. Let me first convert this issue into a discussion."
      }
    ]
  },
  {
    "issue_number": 1284,
    "title": "yolov5 instance segmentation inference",
    "author": "DrawingProcess",
    "state": "open",
    "created_at": "2024-06-16T13:56:14Z",
    "updated_at": "2024-08-06T07:50:25Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nyolov5 is a detection and 'instance segmentation' model.\r\nso, I think If there is a mask area, I think it should be return.\r\n\r\nlike below\r\n\r\n```\r\nreturn cls(\r\n    xyxy=yolov5_detections_predictions[:, :4],\r\n    confidence=yolov5_detections_predictions[:, 4],\r\n    class_id=yolov5_detections_predictions[:, 5].astype(int),\r\n)\r\n```\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @DrawingProcess üëã \r\n\r\nWell spotted. I thought we had it. Still, let me check with the core team and check our stance on this.\r\n\r\nMeanwhile, there are things we need to find out first:\r\n1. Are `yolov5` weights for segmentation available in Pytorch Hub. It would make our testing easier.\r\n2. Can our `from_ultralytics` method handle yolov5 response? My guess is no.\r\n  * If yolov5 provides a list as a response, we should check both `from_ultralytics(results)` and `from_ultralytics(results[0])`\r\n3. Is segmentation with yolov5 something you need for your project? In 99% of the cases, we'd recommend using yolov8 instead."
      },
      {
        "user": "Bhavay-2001",
        "body": "Hi @DrawingProcess @LinasKo, can I start working on this if no one is working on this already?"
      },
      {
        "user": "DrawingProcess",
        "body": "@Bhavay-2001 Sure. I'll contribute to another part."
      }
    ]
  },
  {
    "issue_number": 1394,
    "title": "Clarification of obb behaviour with InferenceSlicer",
    "author": "robmarkcole",
    "state": "closed",
    "created_at": "2024-07-22T22:46:47Z",
    "updated_at": "2024-08-06T07:40:32Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nBased on [reviewing source code](https://github.com/roboflow/supervision/blob/a8d91b0ae5a17ccf93aa83d5866cf5daa511d7a3/supervision/detection/core.py#L258), I understand that with an obb model, the regular box enclosing the obb box is used with SAHI. As a result, merged regular boxes, and not obb boxes are returned. I wanted to confirm this, and suggest a note is added in the docs. \r\n\r\nI also want to know if this could be responsible for the odd predictions below - perhaps the enclosing boxes are not merged correctly:\r\n\r\n![image](https://github.com/user-attachments/assets/7890dfe8-b8c1-42be-b98e-6b1c54bdb075)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi, @robmarkcole üëãüèª Lots to unpack here.\r\n\r\n- OBB support in `InferenceSlicer` has not been officially released yet, but we're working on it. (We should perhaps explicitly state that only object detection and instance segmentation are currently supported.)\r\n- A test version of OBB in `InferenceSlicer` is available in the `develop` branch. You can read more about it [here](https://github.com/roboflow/supervision/issues/1339) and [here](https://github.com/roboflow/supervision/pull/1350). To use it, you would need to install supervision like this: `pip install git+https://github.com/roboflow/supervision.git`.\r\n- If you want to plot OBB, use [`OrientedBoxAnnotator`](https://supervision.roboflow.com/develop/detection/annotators/#supervision.annotators.core.OrientedBoxAnnotator) instead of `BoundingBoxAnnotator` (deprecated) or [`BoxAnnotator`](https://supervision.roboflow.com/develop/detection/annotators/#supervision.annotators.core.BoxAnnotator).\r\n- Regarding the quality of the result you received, it might be related to the second [issue](https://github.com/roboflow/supervision/issues/1395) you opened - incorrect channel order. What `slice_wh` value are you using? The default is `(320, 320)`, I'd try to increase it to `(640, 640)`."
      },
      {
        "user": "robmarkcole",
        "body": "Thanks @SkalskiP these results are much better!\r\n\r\n<img width=\"461\" alt=\"image\" src=\"https://github.com/user-attachments/assets/d639717c-6342-4d92-8ba7-b635ef9aa858\">\r\n\r\n```python\r\nimage_path = \"my.png\"\r\nimage = np.array(Image.open(image_path))\r\n\r\ndef callback(image_slice: np.ndarray) -> sv.Detections:\r\n    result = model(image_slice)[0]\r\n    return sv.Detections.from_ultralytics(result)\r\n\r\nslicer = sv.InferenceSlicer(callback = callback, slice_wh=(768, 768))\r\ndetections = slicer(image)\r\ndetections = detections[detections.class_id == 1] #¬†ships id in DOTA\r\n\r\noriented_box_annotator = sv.OrientedBoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator()\r\n\r\nannotated_image = oriented_box_annotator.annotate(scene=image, detections=detections)\r\nannotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\r\n```\r\n"
      },
      {
        "user": "SkalskiP",
        "body": "@robmarkcole that is true; this looks a lot better! There is still room for improvement. Especially where it comes to post-processing and merging double detections. We don't have that component yet for OBB. \r\n\r\nCan I close the issue? "
      }
    ]
  },
  {
    "issue_number": 1395,
    "title": "Results differ when using cv2 vs pillow",
    "author": "robmarkcole",
    "state": "closed",
    "created_at": "2024-07-22T23:13:29Z",
    "updated_at": "2024-08-06T07:24:25Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nFrom [this comment](https://github.com/roboflow/supervision/issues/1038#issuecomment-2018147877), I understand supervision doesn't change channel order, and the issue I highlight here is likely addressed by documentation. I observe that if I open an image with cv2 or with pillow the predictions are different. The model was trained using ultraltics which I believe also uses cv2, so when I use pillow the channels order is changed. I suggest adding a note to the docs to check which library was used in training, then use that with supervision. Comparisons below:\r\n\r\ncv2:\r\n![image](https://github.com/user-attachments/assets/fd2692aa-d869-4a9a-abfa-d304c342f38a)\r\n\r\npillow:\r\n![Pasted Graphic 5](https://github.com/user-attachments/assets/73a0e0e7-096f-46e3-805f-15798d7c7bd9)\r\n\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n```python\r\nimage_path = \"my.png\"\r\n\r\n# change\r\nimage = cv2.imread(image_path)\r\n#¬†image = np.array(Image.open(image_path))\r\n\r\ndef callback(image_slice: np.ndarray) -> sv.Detections:\r\n    result = model(image_slice)[0]\r\n    return sv.Detections.from_ultralytics(result)\r\n\r\nslicer = sv.InferenceSlicer(callback = callback)\r\ndetections = slicer(image)\r\ndetections = detections[detections.class_id == 1]\r\n\r\nbox_annotator = sv.BoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator()\r\n\r\nannotated_image = box_annotator.annotate(scene=image, detections=detections)\r\nannotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "robmarkcole",
        "body": "Reading https://github.com/ultralytics/ultralytics/issues/9912 I suspect the issue is that the docs should include the conversion `im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)`"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @robmarkcole üëãüèª Long time no see. Correct me if I'm wrong, but I'm not sure this is a problem with Supervision. Constructing a correct callback is the responsibility of the user.\r\n\r\nIn our [documentation](https://supervision.roboflow.com/develop/detection/tools/inference_slicer/#supervision.detection.tools.inference_slicer.InferenceSlicer.__call__), we showcase an example using the ultralytics library, demonstrating image loading with OpenCV. Have you found an example using Pillow in our materials?\r\n\r\n![Screenshot 2024-07-23 at 09 18 35](https://github.com/user-attachments/assets/b5c0ba08-1ec9-4ce2-bc41-527a9afa1788)\r\n\r\nI recommend using `sv.cv2_to_pillow` and `sv.pillow_to_cv2`. These two methods take care of channel order conversion."
      },
      {
        "user": "robmarkcole",
        "body": "Right that is the example I followed. I agree there's a limit in what to cover in examples, and you don't want to become a cv2 tutorial. But I think updating this example to correctly handle the channel order could save other people from doing this investigation. I only use cv2 occasionally and every time have to re remember this channels issue üòÄ"
      }
    ]
  },
  {
    "issue_number": 1113,
    "title": "Update to support new `from_transformers` methods",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-04-12T14:33:46Z",
    "updated_at": "2024-08-05T14:00:26Z",
    "labels": [],
    "body": "### Description\r\n\r\nSupervision contains the function `from_transformers` that includes the results of a Hugging Face transformer and converts it into `Detections.`\r\n\r\nUp until now, we were recommending users to call one of two `transformers` functions:\r\n1. `post_process_segmentation` for any segmentation task\r\n2. `post_process` for detection.\r\n\r\nReading through the [code of transformers](https://github.com/huggingface/transformers/blob/main/src/transformers/models/detr/image_processing_detr.py), it appears both are being deprecated and will be removed in version 5. At the time of writing, the latest is v4.39.3, with the first v4 release in late 2020.\r\n\r\nLet's make sure we support the new version! `Detections` object is universal - it can contain both masks and segmentation. Let's change the `from_transformers` method so it checks its inputs, determines which function was called formats the output, and builds the Detections object like we're doing now.\r\n\r\n### Additional\r\n\r\n* Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª",
    "comments": [
      {
        "user": "LinasKo",
        "body": "You can find examples of the new and old function calls in [this Colab](https://colab.research.google.com/drive/1mYNfB__p6CjmJw2wDv0ruG7smiGAvMbY?usp=sharing)"
      },
      {
        "user": "shaddu",
        "body": "@LinasKo I was checking on this issue and below are my observations :\r\n\r\n- in transformer code `image_processing_detr.py` there are four functions that are getting depreciated in v5\r\n\r\n| Old (v4) | New (v5) |\r\n|----------|----------|\r\n| post_process   | post_process_object_detection   | \r\n| post_process_panoptic    | post_process_panoptic_segmentation  |\r\n| post_process_segmentation    | post_process_semantic_segmentation   |\r\n| post_process_instance    | post_process_instance_segmentation   |\r\n\r\n- I checked locally and in official documentation also and found that `post_process_object_detection` is working fine\r\n- For `post_process_panoptic` method as shown in shared collab , I can see that the method returns `List[Dict]` each dictionary containing a _PNG string_ and _segments_info_ which is not supported by method `from_transformers` in current version also.\r\n- For `post_process_segmentation` in v5, the new method returns totally different return type and we should change `from_transformers` but you have mentioned in this issue that we need to change `from_tensors`. If you could add some more info then I would like to work on this issue \r\n- Same for `post_process_instance`\r\n\r\n\r\n  "
      },
      {
        "user": "LinasKo",
        "body": "Hi @shaddu üëã \r\n\r\nThat's an analysis we vitally needed. Thank you very much!\r\n\r\n`from_tensors` was a typo and I've now fixed it.\r\nI very much appreciate you clarifying it before jumping in.\r\n\r\nIf you still have the time, I'd gladly assign the issue to you. Is there any more information you'd like?"
      }
    ]
  },
  {
    "issue_number": 1388,
    "title": "Create new cookbook for utilizing supervision methods to easily create YOLO datasets for training",
    "author": "xaristeidou",
    "state": "open",
    "created_at": "2024-07-21T16:37:17Z",
    "updated_at": "2024-08-05T07:52:54Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nI find myself creating dataset structures and split in train, valid, test and images, labels folders multiple times. The whole process could easily be automated.\n\n### Use case\n\nThere are methods that currently exist to load a dataset `sv.DetectionDataset.from_yolo()`, split in selected ratio `sv.DetectionDataset.split()` and export to YOLO format `sv.DetectionDataset.to_yolo()`.\r\n\r\nNevertheless, in creation of YOLO model training dataset structure, someone must write a custom split in train/valid/test (because `split()` is able to split only in two parts), and also create manually the train/valid/test folders needed for the preparation of the dataset. (As of my knowledge ultralytics YOLO models require by default to have train/valid folders that contain valid and not empty annotations, test folder can be empty).\r\n\r\n For that reason I propose a new method to be added in `sv.DetectionDataset` which will combine the arguments of `from_yolo(), split(), to_yolo()` and will run the whole backend for creating train/valid/test folder and images/labels subfolders along with data.yaml file.\r\n\r\nAt this current point I have developed an implementation of such method which provides the ability to the user to create a YOLO dataset structure with a single line of code. An example of executing such a process can be seen in the following example:\r\n\r\n```\r\nimport supervision as sv\r\n\r\ndataset_directory = \"/path/to/directory\"\r\n\r\nsv.DetectionDataset.create_yolo_dataset(\r\n    images_directory_path=f\"{dataset_directory}/images\",\r\n    annotations_directory_path=f\"{dataset_directory}/labels\",\r\n    data_yaml_path=f\"{dataset_directory}/data.yaml\",\r\n    train_ratio=0.7,\r\n    valid_ratio=0.15,\r\n    folders_export_path=f\"{dataset_directory}\",\r\n    data_yaml_export_path = f\"{dataset_directory}/data.yaml\",\r\n)\r\n```\n\n### Additional\n\n- The `test_ratio` is automatically calculated based on the `train_ratio, valid_ratio`.\r\n- The arguments provided in the example are all mandatory arguments to be passed.\r\n- The user can provide any additional optional argument incorporated in `from_yolo(), split(), to_yolo()`.\r\n\r\nLet me know if you like this idea, and if you want to submit a PR with the initial implementation.\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Cze≈õƒá @xaristeidou üëãüèª \r\n\r\nTo be honest, I would really prefer not to treat YOLO differently than other data formats. The Supervision API aims to provide reusable building blocks like `sv.DetectionDataset.split` or `sv.DetectionDataset.as_yolo` that you can compose together. To be honest, that sounds like the expected usage of supervision."
      },
      {
        "user": "xaristeidou",
        "body": "@SkalskiP That is a fact, I was thinking about that it is \"too much\" automation. Maybe I could create a cookbook similar to 'Serialise Detections to a CSV File' and 'Serialise Detections to a JSON File', guiding and combining the aforementioned methods to construct a YOLO dataset easily."
      },
      {
        "user": "SkalskiP",
        "body": "I think the cookbook makes a lot more sense. We also released [this](https://supervision.roboflow.com/develop/how_to/process_datasets/) how-to guide last week. MAybe you could reuse some of those code snippet in your cookbook?\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1417,
    "title": "Recieving -> IndexError: arrays used as indices must be of integer (or boolean) type. When trying to annotate detections.",
    "author": "fernanh98",
    "state": "closed",
    "created_at": "2024-07-31T11:30:33Z",
    "updated_at": "2024-08-01T07:11:26Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nI have extracted masks from an image using SAM 2 model and then I am trying to annotate those masks using sv.detections and MaskAnnotator like this:\r\n\r\nimage_path = \"car.jpg\"\r\nimage_bgr = cv2.imread(image_path)\r\nimage_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\r\nimage_array = np.array(image_rgb)\r\nbox = np.array([1, 1, image_array.shape[1], image_array.shape[0]])\r\n\r\n# SAM 2 prediction\r\nmask_predictor.set_image(image_rgb)\r\nmasks, scores, logits = mask_predictor.predict(\r\n    box=box,\r\n    multimask_output=True\r\n)\r\n\r\nbox_annotator = sv.BoxAnnotator(color=sv.Color.RED)\r\nmask_annotator = sv.MaskAnnotator(color=sv.Color.RED)\r\ndetections = sv.Detections(\r\n    xyxy=sv.mask_to_xyxy(masks=masks),\r\n    mask=masks\r\n)\r\ndetections.class_id = np.zeros(len(detections), dtype=int)\r\nsource_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\r\nsegmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\r\n\r\nIn this code everything executes correctly until it reaches last line (segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)) in which I receive this error:\r\n\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n[<ipython-input-135-0ff839f33faa>](https://localhost:8080/#) in <cell line: 10>()\r\n      8 #detections = detections[detections.area == np.max(detections.area)]\r\n      9 source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\r\n---> 10 segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\r\n     11 sv.plot_images_grid(\r\n     12     images=[source_image, segmented_image],\r\n\r\n1 frames\r\n[/usr/local/lib/python3.10/dist-packages/supervision/annotators/core.py](https://localhost:8080/#) in annotate(self, scene, detections, custom_color_lookup)\r\n    358             )\r\n    359             mask = detections.mask[detection_idx]\r\n--> 360             colored_mask[mask] = color.as_bgr()\r\n    361 \r\n    362         cv2.addWeighted(\r\n\r\nIndexError: arrays used as indices must be of integer (or boolean) type\n\n### Environment\n\n- Supervision: 0.22.0\r\n- Python: 3.10.12\r\n- Working on Google Colab\n\n### Minimal Reproducible Example\n\nimage_path = \"car.jpg\"\r\nimage_bgr = cv2.imread(image_path)\r\nimage_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\r\nimage_array = np.array(image_rgb)\r\nbox = np.array([1, 1, image_array.shape[1], image_array.shape[0]])\r\n\r\n# SAM 2 prediction\r\nmask_predictor.set_image(image_rgb)\r\nmasks, scores, logits = mask_predictor.predict(\r\n    box=box,\r\n    multimask_output=True\r\n)\r\n\r\nbox_annotator = sv.BoxAnnotator(color=sv.Color.RED)\r\nmask_annotator = sv.MaskAnnotator(color=sv.Color.RED)\r\ndetections = sv.Detections(\r\n    xyxy=sv.mask_to_xyxy(masks=masks),\r\n    mask=masks\r\n)\r\ndetections.class_id = np.zeros(len(detections), dtype=int)\r\nsource_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\r\nsegmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "rolson24",
        "body": "Hmm I think you need to make the masks have the datatype of bool. I have attached the code below\r\n\r\n\r\n### SAM 2 prediction\r\n```\r\nmask_predictor.set_image(image_rgb)\r\nmasks, scores, logits = mask_predictor.predict(\r\nbox=box,\r\nmultimask_output=True\r\n)\r\n\r\nbox_annotator = sv.BoxAnnotator(color=sv.Color.RED)\r\nmask_annotator = sv.MaskAnnotator(color=sv.Color.RED)\r\ndetections = sv.Detections(\r\nxyxy=sv.mask_to_xyxy(masks=masks),\r\n# change this line to make dtype of masks bool\r\nmask=masks.astype(bool)\r\n)\r\ndetections.class_id = np.zeros(len(detections), dtype=int)\r\nsource_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\r\nsegmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\r\n```"
      },
      {
        "user": "fernanh98",
        "body": "Yep, that worked. Thank you @rolson24 :)"
      }
    ]
  },
  {
    "issue_number": 1418,
    "title": "Add method for REDETR model ",
    "author": "Bhavay-2001",
    "state": "closed",
    "created_at": "2024-07-31T14:58:36Z",
    "updated_at": "2024-07-31T16:24:16Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nAdd a method to take predictions from REDETR model and convert them to detections just like for other models for instance - yolo5, ultralytics etc.\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @Bhavay-2001,\r\n\r\nUnfortunately we're rather preoccupied at the moment, and would prefer to not assign more than 1 issue per contributor. We know we wouldn't be able to review anyway!\r\n\r\nI shall close this for now."
      },
      {
        "user": "Bhavay-2001",
        "body": "Hi @LinasKo, actually I have worked on this on the inference repository and I think I can add this method. It won't be a problem much. "
      },
      {
        "user": "LinasKo",
        "body": "Feel free to open a PR! We'll have a look when we have time."
      }
    ]
  },
  {
    "issue_number": 1412,
    "title": "About color Settings",
    "author": "dearMOMO",
    "state": "closed",
    "created_at": "2024-07-29T10:14:07Z",
    "updated_at": "2024-07-29T11:15:55Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI don't want to default these colors, I want to dynamically generate colors instead of manually specifying fixed colors below, what should I do. Thank you! üôèüèª\r\n\r\nultralytics_file_example.py\r\n\r\nCOLORS = sv.ColorPalette.from_hex([\"#E6194B\", \"#3CB44B\", \"#FFE119\", \"#3C76D1\"])\r\nCOLOR_ANNOTATOR = sv.ColorAnnotator(color=COLORS)\r\nLABEL_ANNOTATOR = sv.LabelAnnotator(\r\n    color=COLORS, text_color=sv.Color.from_hex(\"#000000\")\r\n)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi, @dearMOMO üëãüèª\r\n\r\nLet me confirm this issue into discussion first."
      }
    ]
  },
  {
    "issue_number": 1022,
    "title": "How to register detection in `PolygonZone` for any overlap",
    "author": "marcospgp",
    "state": "open",
    "created_at": "2024-03-19T11:22:32Z",
    "updated_at": "2024-07-26T22:02:03Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHow can I register a detection inside a `PolygonZone` when there is any overlap? Without requiring the entire bounding box to be contained inside the zone.\r\n\r\n![image](https://github.com/roboflow/supervision/assets/3464445/65e4e5c5-928c-4bbc-9115-626e34e60706)\r\n\r\nI tried using `triggering_anchors` but it didn't work:\r\n\r\n```python\r\n    polygon_zone = sv.PolygonZone(\r\n        polygon=zone_ndarray,\r\n        frame_resolution_wh=(img_width, img_height),\r\n        # Make a detection be considered inside the zone if there is any\r\n        # overlap.\r\n        triggering_anchors=[\r\n            Position.TOP_LEFT,\r\n            Position.TOP_RIGHT,\r\n            Position.BOTTOM_RIGHT,\r\n            Position.BOTTOM_LEFT,\r\n        ],\r\n    )\r\n\r\n```\r\n\r\nThanks!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "marcospgp",
        "body": "I believe my code would work if this line had an `np.any` instead of `np.all`: https://github.com/roboflow/supervision/blob/5f9560afb8d63027f261871c7b5d5de178aaff6f/supervision/detection/tools/polygon_zone.py#L88\r\n\r\nMaybe there could be a way to specify whether one wants to register a detection if all anchors are contained, or if any is?\r\n\r\nMaybe a `require_all_anchors=True` parameter to the `sv.PolygonZone()` constructor, which by defaulting to True would not cause any breaking changes.\r\n\r\nThis list of anchors way of specifying how to determine presence is not ideal maybe, I believe most use cases would be either \"full overlap\" or \"any overlap\". Currently, only full overlap is possible, and the default of `(Position.BOTTOM_CENTER)` does not seem like a common use case at all ü§î"
      },
      {
        "user": "SkalskiP",
        "body": "Hi once again, @marcospgp üëãüèª Yup, it looks like changing this one line could unlock you. Would you like to implement such a feature for us?\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "zaymuel",
        "body": "Hi, @SkalskiP and @marcospgp! How are you? I would like to contribute in this. May I do what you suggested? This would be useful for a project I'm developing."
      }
    ]
  },
  {
    "issue_number": 1408,
    "title": "No detections when using ByteTracker",
    "author": "mariosconsta",
    "state": "closed",
    "created_at": "2024-07-26T08:19:32Z",
    "updated_at": "2024-07-26T19:37:10Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nI am trying to use supervision on a yolo V7. Because the outputs of YOLO V7 are different from what's expect, I created 2 classes to convert the outputs to what's expected. Here's a screenshot of the detections from before and after going through the tracker:\r\n\r\n![image](https://github.com/user-attachments/assets/cae27aef-4228-465d-91e0-c8fb20eb2474)\r\n\r\nYou can see in most cases it will never track the 'car' object and sometimes none of the objects.\r\n\r\n\r\n\r\nHere's the code:\r\n\r\n```\r\nclass DetectionResults:\r\n    def __init__(self, boxes, scores, cls, names, masks=None, ids=None):\r\n        self.boxes = DetectionBoxes(boxes, cls, scores, ids)\r\n        self.names = names\r\n        self.masks = masks\r\n\r\nclass DetectionBoxes:\r\n    def __init__(self, boxes, cls, scores, ids=None):\r\n        self.xyxy = boxes\r\n        self.cls = cls\r\n        self.conf = scores\r\n        self.id = ids\r\n        \r\n        \r\nclass ObjectDetector:\r\n    def __init__(self, droneId, operationId, userId, sessionId, droneName) -> None:\r\n        // some other parameters\r\n        self.model = os.path.join(\r\n            \"/app/waldo\", \"yolov7-W25_rect_1280_736_newDefaults-bs96-best-topk-200.onnx\"\r\n        )\r\n        self.sess, self.input_name = self.init_session()\r\n        self.tracker = sv.ByteTrack()\r\n        self.annotator = sv.BoxAnnotator()\r\n        self.label_annotator = sv.LabelAnnotator()\r\n\r\n    ######## Some helper functions ######## \r\n\r\n    def process_frame(self, frame, sess, max_outputs):\r\n        names = [\r\n            \"car_og\",\r\n            \"van\",\r\n            \"truck\",\r\n            \"building\",\r\n            \"person\",\r\n            \"gastank\",\r\n            \"digger\",\r\n            \"container\",\r\n            \"bus\",\r\n            \"u_pole\",\r\n            \"car\",\r\n            \"bike\",\r\n            \"smoke\",\r\n            \"solarpanels\",\r\n            \"arm\",\r\n            \"plane\",\r\n        ]\r\n\r\n\r\n        image = frame.copy()\r\n        image = image.transpose((2, 0, 1))\r\n        image = np.expand_dims(image, 0)\r\n        image = np.ascontiguousarray(image)\r\n\r\n        im = image.astype(np.float32)\r\n        im /= 255\r\n\r\n        inp = {self.input_name: im}\r\n        outputs = sess.run(None, inp)[0]\r\n\r\n        # Convert model's output to be compatible with supervisions required input\r\n        boxes = np.stack([outputs[:, 1], outputs[:, 2], outputs[:, 3], outputs[:, 4]], axis=1)\r\n        scores = outputs[:, 6]\r\n        cls_ids = outputs[:, 5].astype(int)\r\n\r\n        detection_results = DetectionResults(boxes=boxes, scores=scores, cls=cls_ids, names=names)\r\n        \r\n        detection_results.boxes.xyxy = torch.tensor(detection_results.boxes.xyxy)\r\n        detection_results.boxes.cls = torch.tensor(detection_results.boxes.cls)\r\n        detection_results.boxes.conf = torch.tensor(detection_results.boxes.conf)\r\n        \r\n        # Update tracker with detections\r\n        detections = sv.Detections.from_ultralytics(detection_results)\r\n        # When I print the detection classes: detections[\"class_name\"] I can see that the model detects every object on the frame\r\n        detections = self.tracker.update_with_detections(detections)\r\n        # When I print the detection classes here, most of the detections from before are gone\r\n        \r\n        thickness = 1\r\n        category_counts = {}\r\n        \r\n        # Initialize lists for storing results\r\n        box_center_points = []\r\n        labels_info = []\r\n        detection_info = []\r\n\r\n        # Calculate center points and create labels_info and detection_info in a single loop\r\n        for i in range(len(detections.xyxy)):\r\n            x0, y0, x1, y1 = detections.xyxy[i]\r\n            cls_id = detections.class_id[i]\r\n            tracker_id = detections.tracker_id[i]\r\n            score = detections.confidence[i]\r\n\r\n            center_x = (x0 + x1) / 2\r\n            center_y = (y0 + y1) / 2\r\n            box_center_points.append([center_x, center_y])\r\n\r\n            class_name = detection_results.names[cls_id]\r\n            print(class_name)\r\n            labels_info.append(f\"#{tracker_id} {class_name}\")\r\n\r\n            detection_info.append({\r\n                \"tracking_id\": tracker_id,\r\n                \"class_name\": class_name,\r\n                \"center_point\": [center_x, center_y]\r\n            })\r\n\r\n            name = names[int(cls_id)]\r\n            name += \" \" + str(score)\r\n\r\n            if max_outputs is not None:\r\n                cv2.putText(\r\n                    frame,\r\n                    f\"ONNX network max Outputs: {max_outputs}\",\r\n                    (frame.shape[1] - 250, 20),\r\n                    cv2.FONT_HERSHEY_SIMPLEX,\r\n                    0.4,\r\n                    (0, 0, 255),\r\n                    1,\r\n                )\r\n\r\n            # Get the name of the class without the score\r\n            class_name = name.split()[0]\r\n\r\n            # Increment the count for this class in the dictionary\r\n            if class_name in category_counts:\r\n                category_counts[class_name] += 1\r\n            else:\r\n                category_counts[class_name] = 1\r\n\r\n        annotated_frame = self.annotator.annotate(frame, detections=detections)\r\n        annotated_frame = self.label_annotator.annotate(annotated_frame, detections=detections, labels=labels_info)\r\n\r\n        # Write the category counts on the frame\r\n        y_position = 20  # Initial y position\r\n        for category, count in category_counts.items():\r\n            cv2.putText(\r\n                frame,\r\n                f\"{category}: {count}\",\r\n                (10, y_position),\r\n                cv2.FONT_HERSHEY_SIMPLEX,\r\n                0.4,\r\n                (255, 255, 255),\r\n                1,\r\n            )\r\n            y_position += 20  # Increment the y position for the next text\r\n\r\n        # Return the frame with detection boxes\r\n        return annotated_frame, detection_info\r\n```\r\n\r\n### Environment\r\n\r\nThis is the relevant part of the dockerfile:\r\n\r\n```\r\nFROM python:3.9\r\n\r\n# Install required dependencies\r\nRUN apt-get update && apt-get install -y \\ \r\n    netcat-traditional \\\r\n    libgl1-mesa-glx && \\\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\n# Copy the python dependencies\r\nCOPY cvn/requirements.txt /app/requirements.txt\r\n# Install Python dependencies\r\nRUN if [ \"$(uname)\" = \"Darwin\" ] && [ \"$(uname -m)\" = \"arm64\" ]; then \\\r\n    pip install onnxruntime-silicon==1.16.0; \\\r\n    else \\\r\n    pip install onnxruntime-gpu==1.15.1; \\\r\n    fi\r\n\r\nRUN pip install -r /app/requirements.txt\r\n```\r\n\r\nThis is the requirements.txt file:\r\n\r\n```\r\nmysql-connector-python\r\nflask\r\npytz\r\nrequests==2.24.0\r\n\r\n# computer vision\r\nopencv-python\r\ntorch==2.0.0 # ~4 mins to install\r\ntorchvision\r\n\r\n# waldo\r\nnumpy==1.23.5\r\n\r\n# Library with various utilities for computer vision tasks\r\nsupervision\r\n\r\n# disaster classification\r\nPillow\r\n\r\n# crowd localization\r\nyacs\r\neasydict\r\n```\r\n\r\n\r\n### Minimal Reproducible Example\r\n\r\nThe code I shared above should be more than enough for a test drive. Basically after the process frame function returns the annotated frame and the detection info, they get passed into a database and displayed on a platform for the user. Anyway, that's irrelevant to this issue I suppose.\r\n\r\n### Additional\r\n\r\nI tried changing the various parameters of ByteTracker such as:\r\n\r\n1) track_activation_threshold\r\n2) lost_track_buffer\r\n3) minimum_matching_threshold\r\n4) frame_rate\r\n\r\nBut it did not solve the problem. One final thing is, because I am running on CPU right now, the frame rate is really low. Lower than 1 FPS. Does FPS matter for the tracker?\r\n\r\nAlso I tried grabbing almost all the detections regardless of confidence by doing detections = detections[detections.confidence > 0.05] but it also yielded no improvements. The screenshot below is with the following bytetrack settings:\r\n\r\ntrack_activation_threshold=0.05\r\nlost_track_buffer=60\r\nminimum_matching_threshold = 0.1 (I tried with 0.9) and it was the same\r\nframe_rate = 1 (tried with the default as well)\r\n\r\n![image (1)](https://github.com/user-attachments/assets/e40e411a-2588-43f5-b67c-83619c0b53f0)\r\n\r\nOn this link https://drive.google.com/file/d/1YbyuUXfnr3N6Ukyc2ODk3t0FVNTk4X2w/view?usp=sharing you will find the weights we used and the video. We are trying to track the boat (I know the label says 'car', ignore that).\r\n\r\nFor the code, we basically modified the existing code of this repo https://github.com/stephansturges/WALDO/blob/master/playground/run_local_network_on_videos_onnxruntime.py\r\n\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @mariosconsta üëãüèª \r\n\r\nCould you reproduce this experiment in Google Colab? Given that you are using a model that is not one of our standard supported models, I would like to simplify the environment setup as much as possible to help you as quickly as possible."
      },
      {
        "user": "mariosconsta",
        "body": "> Hi @mariosconsta üëãüèª\r\n> \r\n> Could you reproduce this experiment in Google Colab? Given that you are using a model that is not one of our standard supported models, I would like to simplify the environment setup as much as possible to help you as quickly as possible.\r\n\r\nHey @SkalskiP ! Sorry for taking a bit longer to reply, I am a bit baffled by the results. First, let me share the [colab notebook](https://drive.google.com/drive/folders/1pVzTnzYyoeFbla4yNrlJDFs3sqe0YuVC?usp=sharing). The tracker in the notebook **works**.\r\n\r\nThe only difference between the colab and our codebase is the I/O. On the notebook I am reading the video directly using *cap.read(frame)*, on the other hand, our main code, we read an RTMP stream, save each frame in a database, and send the path for the frame to the detector. Here's a code snippet:\r\n\r\n\r\n```\r\ndef process_frame(self, frame, sess, max_outputs):\r\n        names = [ #### list with class names]\r\n\r\n\r\n        image = frame.copy()\r\n        image = image.transpose((2, 0, 1))\r\n        image = np.expand_dims(image, 0)\r\n        image = np.ascontiguousarray(image)\r\n\r\n        im = image.astype(np.float32)\r\n        im /= 255\r\n\r\n        inp = {self.input_name: im}\r\n        outputs = sess.run(None, inp)[0]\r\n\r\n        # Convert model's output to be compatible with supervisions required input\r\n        boxes = np.stack([outputs[:, 1], outputs[:, 2], outputs[:, 3], outputs[:, 4]], axis=1)\r\n        scores = outputs[:, 6]\r\n        cls_ids = outputs[:, 5].astype(int)\r\n\r\n        detection_results = DetectionResults(boxes=boxes, scores=scores, cls=cls_ids, names=names)\r\n\r\n        detection_results.boxes.xyxy = torch.tensor(detection_results.boxes.xyxy)\r\n        detection_results.boxes.cls = torch.tensor(detection_results.boxes.cls)\r\n        detection_results.boxes.conf = torch.tensor(detection_results.boxes.conf)\r\n\r\n        # Update tracker with detections\r\n        detections = sv.Detections.from_ultralytics(detection_results)\r\n        detections = detections[detections.confidence > 0.05]\r\n        print(f'BEFORE TRACKER: {detections[\"class_name\"]}')\r\n        detections = self.tracker.update_with_detections(detections)\r\n        print(f'AFTER TRACKER: {detections[\"class_name\"]}\\n\\n')\r\n        \r\n        thickness = 1\r\n        category_counts = {}\r\n        \r\n        # Initialize lists for storing results\r\n        box_center_points = []\r\n        labels_info = []\r\n        detection_info = []\r\n\r\n        # Calculate center points and create labels_info and detection_info in a single loop\r\n        for i in range(len(detections.xyxy)):\r\n            x0, y0, x1, y1 = detections.xyxy[i]\r\n            cls_id = detections.class_id[i]\r\n            tracker_id = detections.tracker_id[i]\r\n            score = detections.confidence[i]\r\n\r\n            center_x = (x0 + x1) / 2\r\n            center_y = (y0 + y1) / 2\r\n            box_center_points.append([center_x, center_y])\r\n\r\n            class_name = detection_results.names[cls_id]\r\n            labels_info.append(f\"#{tracker_id} {class_name}\")\r\n\r\n            detection_info.append({\r\n                \"tracking_id\": tracker_id,\r\n                \"class_name\": class_name,\r\n                \"center_point\": [center_x, center_y]\r\n            })\r\n\r\n            name = names[int(cls_id)]\r\n            name += \" \" + str(score)\r\n\r\n            if max_outputs is not None:\r\n                cv2.putText(\r\n                    frame,\r\n                    f\"ONNX network max Outputs: {max_outputs}\",\r\n                    (frame.shape[1] - 250, 20),\r\n                    cv2.FONT_HERSHEY_SIMPLEX,\r\n                    0.4,\r\n                    (0, 0, 255),\r\n                    1,\r\n                )\r\n\r\n            # Get the name of the class without the score\r\n            class_name = name.split()[0]\r\n\r\n            # Increment the count for this class in the dictionary\r\n            if class_name in category_counts:\r\n                category_counts[class_name] += 1\r\n            else:\r\n                category_counts[class_name] = 1\r\n\r\n        annotated_frame = self.annotator.annotate(frame, detections=detections)\r\n        annotated_frame = self.label_annotator.annotate(annotated_frame, detections=detections, labels=labels_info)\r\n\r\n        # Write the category counts on the frame\r\n        y_position = 20  # Initial y position\r\n        for category, count in category_counts.items():\r\n            cv2.putText(\r\n                frame,\r\n                f\"{category}: {count}\",\r\n                (10, y_position),\r\n                cv2.FONT_HERSHEY_SIMPLEX,\r\n                0.4,\r\n                (255, 255, 255),\r\n                1,\r\n            )\r\n            y_position += 20  # Increment the y position for the next text\r\n\r\n        # Return the frame with detection boxes\r\n        return annotated_frame, detection_info\r\n```\r\n\r\n```\r\n    def run_inference(self, img_path, output_img_path):\r\n        input_frame_raw = cv2.imread(img_path)\r\n        height, width, channels = input_frame_raw.shape\r\n        expected_width, expected_height = self.get_resolution_from_model_path(\r\n            self.model\r\n        )  # Get expected resolution\r\n\r\n        max_outputs = self.get_max_outputs()\r\n        frame = self.resize_and_pad(input_frame_raw, expected_width, expected_height)\r\n        out_frame, detection_info = self.process_frame(\r\n            frame=frame, sess=self.sess, max_outputs=max_outputs\r\n        )\r\n\r\n        cv2.imwrite(output_img_path, out_frame)\r\n        frameId = database.queries.saveFrame(self.sessionId, output_img_path)\r\n\r\n        # loop through all the detected points and calculate their coordinates\r\n        telemetry = database.queries.getDroneLatestTelemetry(\r\n            self.droneId\r\n        )  # get latest drone telemetry\r\n\r\n        # TODO: get fov from database and adjust for zoom level(?)\r\n        fov_horizontal = 68  # FOR MAVIC\r\n        fov_vertical = 40  # mavic\r\n\r\n        gimbal_angle = telemetry[4] + 90\r\n\r\n        detectedCoords = []\r\n        for info in detection_info:\r\n            lat, lon = self.pixel_to_gps(\r\n                info['center_point'],\r\n                (width, height),\r\n                (fov_horizontal, fov_vertical),\r\n                (telemetry[0], telemetry[1], telemetry[2], telemetry[3], gimbal_angle),\r\n            )\r\n            detectedCoords.append([lat, lon])\r\n            database.queries.saveDetectedObject(lat, lon, info['class_name'], int(info['tracking_id']), 1.2, self.operationId, self.droneId, self.sessionId, frameId)\r\n\r\n```\r\n\r\n```\r\ndef start_loop(self):\r\n        output_folder = os.path.join(\r\n            \"/media\", f\"detector_session{self.sessionId}_{self.droneName}\"\r\n        )\r\n        os.makedirs(output_folder, exist_ok=True)\r\n        startTime = time.time()\r\n        detectionInterval = 1 / int(\r\n            os.environ.get(\"COMPUTER_VISION_FPS\")\r\n        )*2  # run detection X frames per second\r\n        nextDetectionTime = startTime + detectionInterval\r\n        frameCounter = 0\r\n\r\n        while not self.stop:\r\n            currentTime = time.time()\r\n            if currentTime >= nextDetectionTime:\r\n                frameCounter += 1\r\n                nextDetectionTime += detectionInterval\r\n\r\n                latestLiveStreamFrame = database.queries.getLatestLiveStreamFrame(\r\n                    self.droneId\r\n                )\r\n                image_path = latestLiveStreamFrame[0]\r\n\r\n                # prepare frame filename\r\n                currentDateTime = datetime.now(timezone).time()\r\n                formattedDateTime = currentDateTime.strftime(\"%H-%M-%S\")\r\n                frame_name = f\"detector-frame{frameCounter:05d}_{formattedDateTime}.jpg\"\r\n                self.run_inference(\r\n                    img_path=image_path,\r\n                    output_img_path=os.path.join(output_folder, frame_name),\r\n                )\r\n\r\n        print(f\"{self.droneName}: Waldo detector stopped.\")\r\n\r\n    def stopDetector(self):\r\n        self.stop = True\r\n```\r\n\r\nSorry for the code dump, I just want to make our workflow \"clear\". As you can see the only difference between this and the colab notebook is how the video/stream is been processed.\r\n\r\nIs this behavior normal? The tracker works with the yolo V7 as you can see from the colab, so the issue must be on how we read and process each frame."
      },
      {
        "user": "rolson24",
        "body": "Hi @mariosconsta,\r\n\r\nThanks for sharing the colab notebook!\r\nI ran it and like you said it seems to be doing the correct thing. The tracker drops detections that it can't track, which is why a few detections get cut removed after the tracker. But it sounds like the tracker was performing much worse with your original stream.\r\nIt looks like you have your inference function inside your capture loop. This is most likely the problem because if your system can only process at 1 FPS, then you can only capture video at 1 FPS. The tracker is not designed to handle such low framerates because it relies on the assumption that objects don't move very far in-between two frames. If the source framerate (in this case the 1 FPS of capturing the stream) is too low, the tracker won't be able to track very well.\r\n\r\nIs the processing need to happen in real-time? If not you could simply capture in real-time and run detection and tracking offline. The other option is just making the processing faster. The tracker needs at least 15 FPS to perform correctly."
      }
    ]
  },
  {
    "issue_number": 1401,
    "title": "\"Failed to create CUDAExecutionProvider\" onnxruntime error on Google Colab",
    "author": "hkhare42",
    "state": "closed",
    "created_at": "2024-07-25T04:36:07Z",
    "updated_at": "2024-07-25T09:27:42Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nReally appreciate this library and ecosystem!\r\n\r\nI'm trying to run the \"time_in_zone\" example on Google Colab (CUDAExecutionProvider). I pip installed the requirements.txt and in the next step, also pip installed inference-gpu library. \r\n\r\n!nvidia-smi gives me the following showing that CUDA is available. \r\n![image](https://github.com/user-attachments/assets/6f4aed57-6888-4ccf-a378-02053a3ab7cd)\r\n\r\nOn running the inference_file_example.py, I get the following error:\r\n`UserWarning: Specified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\r\n2024-07-22 18:17:36.983840877 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:640 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.`\r\n\r\nI tried going through the referenced link in the error and going over forums but wasn't able to resolve this. What additional step is needed to get CUDAExecutionProvider to work?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "hkhare42",
        "body": "Also sharing links to external threads that discuss similar issues: \r\nhttps://github.com/roboflow/inference/issues/324\r\nhttps://github.com/microsoft/onnxruntime/issues/11092\r\nhttps://stackoverflow.com/questions/75267445/why-does-onnxruntime-fail-to-create-cudaexecutionprovider-in-linuxubuntu-20\r\n"
      },
      {
        "user": "onuralpszr",
        "body": "@hkhare42  hello o/ let me this to discussion and also could you try this codes I made small collab for you test and see are you in GPU \r\n\r\nhttps://colab.research.google.com/drive/1Ltv7tKae7SOkM7bOjwPQ0x3kYMik5XkE?usp=sharing"
      }
    ]
  },
  {
    "issue_number": 1392,
    "title": "How do I get the current frame count",
    "author": "dearMOMO",
    "state": "closed",
    "created_at": "2024-07-22T02:19:16Z",
    "updated_at": "2024-07-22T10:58:45Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nvideo.py Ôºö how to obtain the current video frame numberÔºåthanks\r\n\r\n\r\ndef get_video_frames_generator(\r\n    source_path: str, stride: int = 1, start: int = 0, end: Optional[int] = None\r\n) -> Generator[np.ndarray, None, None]:\r\n    \"\"\"\r\n    Get a generator that yields the frames of the video.\r\n\r\n    Args:\r\n        source_path (str): The path of the video file.\r\n        stride (int): Indicates the interval at which frames are returned,\r\n            skipping stride - 1 frames between each.\r\n        start (int): Indicates the starting position from which\r\n            video should generate frames\r\n        end (Optional[int]): Indicates the ending position at which video\r\n            should stop generating frames. If None, video will be read to the end.\r\n\r\n    Returns:\r\n        (Generator[np.ndarray, None, None]): A generator that yields the\r\n            frames of the video.\r\n\r\n    Examples:\r\n        ```python\r\n        import supervision as sv\r\n\r\n        for frame in sv.get_video_frames_generator(source_path=<SOURCE_VIDEO_PATH>):\r\n            ...\r\n        ```\r\n    \"\"\"\r\n    video, start, end = _validate_and_setup_video(source_path, start, end)\r\n    frame_position = start\r\n    while True:\r\n        success, frame = video.read()\r\n        if not success or frame_position >= end:\r\n            break\r\n        yield frame\r\n        for _ in range(stride - 1):\r\n            success = video.grab()\r\n            if not success:\r\n                break\r\n        frame_position += stride\r\n    video.release()\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "dearMOMO",
        "body": "version  0.22.0\r\nvideo.py  line=132"
      },
      {
        "user": "xaristeidou",
        "body": "@dearMOMO The generator as it's developed yields only `frame`. I don't think there is a way to extract the count from the generator itself unless it is modified. There is an easy solution that could solve your problem using `enumerate`.\r\n\r\n```\r\nimport supervision as sv\r\n\r\nvideo_source = \"/path/to/video.mp4\"\r\n\r\nfor count, frame in enumerate(sv.get_video_frames_generator(source_path=video_source), start = 0):\r\n    cv2.imshow(\"Image\", frame)\r\n    key = cv2.waitKey(1)\r\n    if key == 113: # press 'q' to break loop\r\n        break\r\n    print(count)\r\ncv2.destroyAllWindows()\r\n```"
      },
      {
        "user": "dearMOMO",
        "body": "very thankful"
      }
    ]
  },
  {
    "issue_number": 1378,
    "title": "setting up dev environment fails in two places",
    "author": "sharukiln",
    "state": "closed",
    "created_at": "2024-07-17T23:45:29Z",
    "updated_at": "2024-07-18T18:02:03Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\n![Screenshot from 2024-07-18 05-06-08](https://github.com/user-attachments/assets/a82b3e2f-d413-47d2-b43b-d2859d53ef68)\r\n![Screenshot from 2024-07-18 05-13-04](https://github.com/user-attachments/assets/954cc3eb-a86e-4e6c-838a-e9c55c247812)\r\n\n\n### Environment\n\nubuntu 24.04\r\npython 3.12.3\n\n### Minimal Reproducible Example\n\n`poetry install`\r\n`poetry run pytest`\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "@sharukiln hello, first error could cause because of old poetry so make sure it is latest version, second error basically happen because of First one, it just opencv not installed. So first try update your poetry and re run installation. "
      },
      {
        "user": "onuralpszr",
        "body": "@sharukiln I talked little bit early, so I made a PR for fix the issue, sorry for the trouble "
      }
    ]
  },
  {
    "issue_number": 1339,
    "title": "draw wrong OrientedBoxAnnotator with InferenceSlicer",
    "author": "DawnMagnet",
    "state": "closed",
    "created_at": "2024-07-10T05:52:36Z",
    "updated_at": "2024-07-18T13:46:39Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nI'm using official yolov8m_obb.pt and if I write code like below\r\n\r\n```python\r\ndef callback(image_slice: np.ndarray) -> sv.Detections:\r\n    result = model(image_slice)[0]\r\n    return sv.Detections.from_ultralytics(result)\r\nslicer = sv.InferenceSlicer(callback=callback, thread_workers=1)\r\ndetections = slicer(image)\r\nbounding_box_annotator = sv.OrientedBoxAnnotator()\r\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\r\n```\r\n\r\nThe OrientedBoxAnnotator in annotated_image was in the wrong place, and if i check variables inside detections\r\nit shows like below\r\n```text\r\nDetections(xyxy=array([[     759.02,      8.0095,      770.99,      12.812]]), mask=None, confidence=array([    0.31441], dtype=float32), class_id=array([1]), tracker_id=None, data={'xyxyxyxy': array([[[     247.02,      8.3203],\r\n        [     247.13,      12.812],\r\n        [     258.99,      12.501],\r\n        [     258.87,      8.0095]]], dtype=float32), 'class_name': array(['Van'], dtype='<U13')})\r\n```\r\nwhich xyxy can't match xyxyxyxy and boxes inside picture are wrong too\r\n![image](https://github.com/roboflow/supervision/assets/61567130/bca011ee-8d0b-48c5-8092-3747e2356f17)\r\n\r\nbut if i don't use the InferenceSlicer, then the answer is correct\n\n### Environment\n\n```toml\r\npython = 3.9\r\ndependencies = [\r\n    \"ultralytics~=8.2.45\",\r\n    \"opencv-contrib-python~=4.10.0.84\",\r\n    \"onnx~=1.16.1\",\r\n    \"supervision~=0.21.0\",\r\n]\r\n```\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @DawnMagnet üëã \r\n\r\nInferenceSlicer does not support OBB boxes at the moment. But it definitely should!\r\n\r\n---\r\n\r\n@Bhavay-2001, would you have some time to work on this?\r\n\r\nInside `_run_callback` of `InferenceSlicer`, there's a call to `move_detections`. Inside you'll find calls to `move_boxes` and `move_masks`. Notice that we don't mention `data` at all.\r\n\r\nWe need to create a function `move_obb_boxes` and place it inside `move_detections`, calling it if `ORIENTED_BOX_COORDINATES` key is in `data`."
      },
      {
        "user": "Bhavay-2001",
        "body": "Sure @LinasKo, give me some time and I will have a look at this.\r\nThanks"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @DawnMagnet üëãüèª \r\n\r\n@eric220 just submitted a PR aiming to fix this issue. It would greatly help us if you could test it on your example. All you need to do is:\r\n\r\n1. uninstall supervision\r\n\r\n```\r\npip uninstall -y supervision\r\n```\r\n\r\n2. install updated version\r\n\r\n```\r\npip install git+https://github.com/eric220/supervision@develop\r\n```\r\n\r\n3. run your code\r\n\r\nLike I said it would help us a lot to run this new version on your example. Thanks in advance üôèüèª "
      }
    ]
  },
  {
    "issue_number": 1376,
    "title": "mAP for small, medium and large objects",
    "author": "Bhavay-2001",
    "state": "closed",
    "created_at": "2024-07-17T15:15:41Z",
    "updated_at": "2024-07-17T16:16:32Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nI saw an issue online where the user demanded calculation of MeanAveragePrecision for small, medium and large objects for HBB and OBB detection models. I thought that maybe this will be a good idea to expand this in supervision library. \r\n\r\nWe can discuss about this issue and maybe potential approach on how to achieve this. \n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @Bhavay-2001 üëãüèª, We plan to add this feature. But for now, we need to decide on the API internally. Let me close this issue for now. I'll open the new one once we make the final design decisions. "
      },
      {
        "user": "Bhavay-2001",
        "body": "Alright @SkalskiP, pls let me know. Thanks"
      }
    ]
  },
  {
    "issue_number": 1371,
    "title": "LabelAnnotator mixes up RGB and BGR",
    "author": "yeldarby",
    "state": "closed",
    "created_at": "2024-07-17T03:23:27Z",
    "updated_at": "2024-07-17T15:59:29Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nLabelAnnotator is using `to_rgb` instead of `to_bgr` for the text color so the channels get mixed up. Using `text_color = sv.Color.RED` produces blue labels.\r\n\r\n![image](https://github.com/user-attachments/assets/21f1fd77-0342-43b1-bd84-11a777b196e9)\r\n\n\n### Environment\n\n- Supervision: 0.22.0\r\n- Colab\n\n### Minimal Reproducible Example\n\nColab: https://colab.research.google.com/drive/1DVjQ4ETQK2o4CUqMGD8rBaLSxOu7KE4o#scrollTo=_Czi4iAkPt01\r\n\r\n```\r\nbox_annotator = sv.BoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator(\r\n    text_color = sv.Color.RED\r\n)\r\n\r\nannotated_image = box_annotator.annotate(image.copy(), detections=detections)\r\nannotated_image = label_annotator.annotate(annotated_image, detections=detections)\r\n\r\nsv.plot_image(image=annotated_image, size=(8, 8))\r\n```\n\n### Additional\n\nPretty sure this is the culprit: https://github.com/roboflow/supervision/blob/e76dec7e1518cd286ea7391caf3a5766bbda1890/supervision/annotators/core.py#L1155\r\n\r\nLooking at the file it also seems like it might affect RichLabelAnnotator (though that uses `to_rgb` both for the background and the text).\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "Bhavay-2001",
        "body": "Hi @yeldarby, if you are okay with it, can I work on this issue?"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @yeldarby üëãüèª Thanks for pointing this out. It's funny how this bug flew under the radar for so long. As for `RichLabelAnnotator,` it uses Pillow for text rendering, and in that case, `as_rgb` is the expected behavior (I just tested this).\r\n\r\n@Bhavay-2001, feel free to open PR to fix this issue. I assign this issue to you. Let me know if you need any help."
      },
      {
        "user": "SkalskiP",
        "body": "@yeldarby fix is merged into `develop`. I'm closing the issue. Feel free to re-open it if you'll face more problems."
      }
    ]
  },
  {
    "issue_number": 1366,
    "title": "Metrics API",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-07-16T12:52:52Z",
    "updated_at": "2024-07-17T12:06:20Z",
    "labels": [],
    "body": "This issue aggregates the discussion and near-future plans to introduce metrics to supervision.\r\n\r\nThe first steps shall be enacted by the core Roboflow team, and then we'll open submissions for specific metrics for the community.\r\n\r\n### I propose the following:\r\n* Aim for ease of usage, compact API, sacrificing completeness if required.\r\n* Provide public classes with aggregation by default (metrics.py), keep implementation in impl.py or equivalent, to be used internally.\r\n* Expose not in global scope, but in supervision.metrics.\r\n* I don't think we need to split into metrics.detection, metrics.segmentation, metrics.classification, but I'm on the fence. \r\n* Focus only on what we can apply to Detections object.\r\n* This means, only implement metrics if they use some of: class_id, confidence, xyxy, mask, xyxyxyxy (in Detections.data).\r\n\r\n### :warning:  I don't know:\r\n* How metrics are computed when targets and predictions have different numbers of detections or they are mismatched.\r\n* I don't think metrics should fail in that case, but perhaps there's a standard way of addressing this.\r\n\r\n### I believe we could start with:\r\n* Importing current metrics into the new system:\r\n  * IoU\r\n  * mAP\r\n  * Confusion Matrix\r\n* Detections\r\n  * Accuracy\r\n  * Precision\r\n  * Recall\r\n* General\r\n  * Mean confidence\r\n  * Median confidence\r\n  * Min confidence\r\n  * Max confidence\r\n  * (not typical, but I'd find useful) - number of unique classes detected & aggregate count of how many objects of which class were detected - N defects / hour).\r\n\r\nI believe the param Metrics needs to provide during construction is queue_size.\r\n* 1 - don't keep history, only ever give metrics of current batch\r\n* N - keep up to N metric results in history for computation.\r\n\r\n### Other thoughts:\r\n* I don't think metrics should know about datasets. Instead of benchmark as it is in current API, let's have def benchmark_dataset(dataset, metric) in metrics/utils.py.\r\n\r\n### API:\r\n\r\n```python\r\nclass Accuracy(Metric):\r\n    def __init__(self, queue_size=1) -> None\r\n    \r\n    @override\r\n    def update(predictions: Detections, targets: Detections) -> None\r\n    \r\n    @override\r\n    def compute() -> NotSureYet\r\n\r\n    # Metric also provides  `def detect_and_compute(args*, kwargs**)`.\r\n\r\naccuracy_metric = Accuracy()\r\naccuracy_metric.add(detections, detections_ground_truth)\r\naccuracy = accuracy_metric.compute()\r\n```\r\n\r\nRelated features:\r\n* https://github.com/roboflow/supervision/issues/140\r\n* https://github.com/roboflow/supervision/pull/177\r\n* https://github.com/roboflow/supervision/issues/232\r\n* https://github.com/roboflow/supervision/pull/236\r\n* https://github.com/roboflow/supervision/issues/292\r\n* https://github.com/roboflow/supervision/issues/480\r\n* https://github.com/roboflow/supervision/issues/632\r\n",
    "comments": [
      {
        "user": "Bhavay-2001",
        "body": "Hi @LinasKo, I know that these changes are for Metrics API, what I am thinking was maybe we could add some object detection models into Roboflow library? Like the RTDETR and others?"
      },
      {
        "user": "SkalskiP",
        "body": "Hi, @Bhavay-2001 üëãüèª Lines is out on vacation. I'll be covering for him. Do you mean add models like RT-DETR to [Inference](https://github.com/roboflow/inference)?"
      },
      {
        "user": "SkalskiP",
        "body": "I'm closing this issue for now. "
      }
    ]
  },
  {
    "issue_number": 1345,
    "title": "Bug: `get_video_frames_generator` does not produce any frames.",
    "author": "likith1908",
    "state": "closed",
    "created_at": "2024-07-11T23:53:44Z",
    "updated_at": "2024-07-17T10:20:02Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "body": "Foreword by @LinasKo:\r\nWe'd appreciate some help from the community! There's some code to run a super-simple test on your videos. If you have the time, please check out [my comment in the PR](https://github.com/roboflow/supervision/pull/1348#issuecomment-2225192477)\r\n\r\n---\r\n\r\n@likith1908:\r\nSo as of now my feed was at a resolution of 2868 x 1104 for which it works good enough but i have changed my feed to 3072 x 1080 now it doesn't process the frames. I have debugged by using a bunch of print statements, the program exits after initialising the ```byte_track```  \r\n\r\nmy source_video : https://drive.google.com/file/d/1FLE-YKUBQa70XBM3e2wsecp0cT0nwzur/view?usp=drive_link\r\ntoken.pt : https://drive.google.com/file/d/1vkj-Wax7PzfHsNCnmcqmEWXxKkstoAUp/view?usp=drive_link\r\n# MyCode\r\n\r\n```python\r\nimport argparse\r\nimport supervision as sv\r\nimport cv2\r\nfrom ultralytics import YOLO\r\nimport numpy as np\r\nfrom collections import defaultdict, deque\r\n\r\nSOURCE = np.array([[0, 0], [3070, 0], [3070, 1080], [0, 1080]])\r\n\r\nTARGET_WIDTH = 0.81227083231\r\nTARGET_HEIGHT = 0.28574999964\r\n\r\nTARGET = np.array([\r\n    [0, 0],\r\n    [TARGET_WIDTH, 0],\r\n    [TARGET_WIDTH, TARGET_HEIGHT],\r\n    [0, TARGET_HEIGHT]\r\n])\r\n\r\nclass ViewTransformer:\r\n    def __init__(self, source=np.ndarray, target=np.ndarray):\r\n        source = source.astype(np.float32)\r\n        target = target.astype(np.float32)\r\n        self.m = cv2.getPerspectiveTransform(source, target)\r\n\r\n    def transform_points(self, points: np.ndarray) -> np.ndarray:\r\n        if points.size == 0:\r\n            print(\"Warning: No points to transform.\")\r\n            return np.array([])\r\n        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\r\n        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\r\n        return transformed_points.reshape(-1, 2)\r\n\r\ndef parse_arguments() -> argparse.Namespace:\r\n    parser = argparse.ArgumentParser(\r\n        description=\"Speed Estimation using Ultralytics and Supervision\"\r\n    )\r\n    parser.add_argument(\r\n        \"--source_video_path\",\r\n        required=False,\r\n        default=\"/home/harvestedlabs/Desktop/Codes/39.mp4\",\r\n        help=\"Path to the source video file\",\r\n        type=str\r\n    )\r\n    return parser.parse_args()\r\n\r\ndef main():\r\n    args = parse_arguments()\r\n    print(f\"Source video path: {args.source_video_path}\")\r\n    \r\n    video_info = sv.VideoInfo.from_video_path(args.source_video_path)\r\n    print(f\"Video info: {video_info}\")\r\n    \r\n    model = YOLO(\"/home/harvestedlabs/Desktop/Codes/Likith/token.pt\")\r\n    print(\"YOLO model loaded.\")\r\n    \r\n    byte_track = sv.ByteTrack(frame_rate=video_info.fps)\r\n    print(\"ByteTrack initialized.\")\r\n    \r\n    thickness = sv.calculate_optimal_line_thickness(resolution_wh=video_info.resolution_wh)\r\n    text_scale = sv.calculate_optimal_text_scale(resolution_wh=video_info.resolution_wh)\r\n    bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=thickness)\r\n    label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=thickness)\r\n    \r\n    frame_generator = sv.get_video_frames_generator(args.source_video_path)\r\n    polygon_zone = sv.PolygonZone(SOURCE)\r\n    view_transformer = ViewTransformer(SOURCE, TARGET)\r\n    coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\r\n\r\n    frame_count = 0\r\n    for frame in frame_generator:\r\n        try:\r\n            frame_count += 1\r\n            print(f\"Processing frame {frame_count}/{video_info.total_frames}\")\r\n            \r\n            # Ensure the frame is valid\r\n            if frame is None:\r\n                print(f\"Frame {frame_count} is None, skipping.\")\r\n                continue\r\n            \r\n            result = model(frame)\r\n            print(\"Frame processed by model.\")\r\n            \r\n            if not result:\r\n                print(f\"No result for frame {frame_count}, skipping.\")\r\n                continue\r\n            \r\n            detections = sv.Detections.from_ultralytics(result[0])\r\n            print(f\"Detections: {detections}\")\r\n            \r\n            detections = detections[polygon_zone.trigger(detections)]\r\n            detections = byte_track.update_with_detections(detections=detections)\r\n            \r\n            points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\r\n            if points.size > 0:\r\n                points = view_transformer.transform_points(points=points)\r\n            else:\r\n                print(\"No points detected in the frame.\")\r\n            \r\n            labels = []\r\n            for tracker_id, [_, y] in zip(detections.tracker_id, points):\r\n                coordinates[tracker_id].append(y)\r\n                if len(coordinates[tracker_id]) < video_info.fps / 2:\r\n                    labels.append(f\"#{tracker_id}\")\r\n                else:\r\n                    coordinates_start = coordinates[tracker_id][-1]\r\n                    coordinates_stop = coordinates[tracker_id][0]\r\n                    distance = abs(coordinates_start - coordinates_stop)\r\n                    time = len(coordinates[tracker_id]) / video_info.fps\r\n                    speed = (distance / time) * 3.6\r\n\r\n                    print(f\"Tracker ID: {tracker_id}\")\r\n                    print(f\"Coordinates Start: {coordinates_start}\")\r\n                    print(f\"Coordinates Stop: {coordinates_stop}\")\r\n                    print(f\"Distance: {distance}\")\r\n                    print(f\"Time: {time}\")\r\n                    print(f\"Speed: {speed} km/h\")\r\n\r\n                    labels.append(f\"#{tracker_id}, {float(speed)} kmph\")\r\n\r\n            annotated_frame = frame.copy()\r\n            annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\r\n            annotated_frame = sv.draw_polygon(annotated_frame, polygon=SOURCE, color=sv.Color.RED)\r\n            annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\r\n\r\n            cv2.namedWindow(\"Annotated Frame\", cv2.WINDOW_NORMAL)\r\n            cv2.imshow(\"Annotated Frame\", annotated_frame)\r\n\r\n            if cv2.waitKey(1) == ord(\"q\"):\r\n                break\r\n        except Exception as e:\r\n            print(f\"Error processing frame {frame_count}: {e}\")\r\n    \r\n    cv2.destroyAllWindows()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n\r\n# Output\r\n```\r\nSource video path: /home/harvestedlabs/Desktop/Codes/39.mp4\r\nVideo info: VideoInfo(width=3072, height=1080, fps=30, total_frames=261)\r\nYOLO model loaded.\r\nByteTrack initialized.\r\n```\r\n\r\nCan you help me with this issue? @LinasKo @skylargivens @iurisilvio @sberan Can you suggest a way?\r\n\r\nThanks\r\nLikith\r\n\r\n_Originally posted by @likith1908 in https://github.com/roboflow/supervision/discussions/1344#discussioncomment-10025628_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Confirming the bug where no frames are produced by `sv. get_video_frames_generator`.\r\n\r\nI'll look into it, I have a hunch it's something I've dealt with on a camera I had."
      },
      {
        "user": "likith1908",
        "body": "@LinasKo Did you find any solution for this issue? "
      },
      {
        "user": "LinasKo",
        "body": "Yes, give me 10 min."
      }
    ]
  },
  {
    "issue_number": 1368,
    "title": "color_lookup=\"INDEX\",  TypeError: '<' not supported between instances of 'NoneType' and 'int'",
    "author": "yeldarby",
    "state": "closed",
    "created_at": "2024-07-16T20:56:59Z",
    "updated_at": "2024-07-17T09:54:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nUsing `color_lookup = \"INDEX\"` throws: `TypeError: '<' not supported between instances of 'NoneType' and 'int'`.\n\n### Environment\n\n- Supervision: 0.22.0\r\n- Colab\n\n### Minimal Reproducible Example\n\nColab: https://colab.research.google.com/drive/1SpxXrpDJKadaJLMNxx5ukXygl2JA9dxH#scrollTo=1tnbpMYBQAVF\r\n\r\n```\r\nbox_annotator = sv.BoxAnnotator(\r\n    color_lookup = \"INDEX\"\r\n)\r\nlabel_annotator = sv.LabelAnnotator()\r\n\r\nannotated_image = box_annotator.annotate(image.copy(), detections=detections)\r\nannotated_image = label_annotator.annotate(annotated_image, detections=detections)\r\n\r\nsv.plot_image(image=annotated_image, size=(8, 8))\r\n```\n\n### Additional\n\n[The docs](https://supervision.roboflow.com/latest/detection/annotators/) say this should be a string (`Type: str`, `Options are INDEX, CLASS, TRACK.`) but it's actually an enum. This works:\r\n\r\n```\r\nbox_annotator = sv.BoxAnnotator(\r\n    color_lookup = sv.annotators.utils.ColorLookup.INDEX\r\n)\r\nlabel_annotator = sv.LabelAnnotator()\r\n\r\nannotated_image = box_annotator.annotate(image.copy(), detections=detections)\r\nannotated_image = label_annotator.annotate(annotated_image, detections=detections)\r\n\r\nsv.plot_image(image=annotated_image, size=(8, 8))\r\n```\r\n\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi, @yeldarby üëãüèª good point. It is interesting to see how wrong documentation can result in wrong usage. The `ColorLookup` enum is the intended usage."
      },
      {
        "user": "SkalskiP",
        "body": "I updated the docstrings. You can take a look here: https://supervision.roboflow.com/develop/detection/annotators. It will become official along with the next release. "
      }
    ]
  },
  {
    "issue_number": 1369,
    "title": "from_matplotlib: 'LinearSegmentedColormap' object has no attribute 'colors'",
    "author": "yeldarby",
    "state": "closed",
    "created_at": "2024-07-16T23:46:03Z",
    "updated_at": "2024-07-17T08:04:25Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nSome matplotlib color palettes fail with `'LinearSegmentedColormap' object has no attribute 'colors'`.\n\n### Environment\n\n- Supervision: 0.22.0\r\n- Colab\n\n### Minimal Reproducible Example\n\nColab: https://colab.research.google.com/drive/1t8n2nnGGHqbm6idj8FmI7HYFe78ZhYxa#scrollTo=aGRu4uWp6wrB\r\n\r\n```\r\nbox_annotator = sv.BoxAnnotator(\r\n    color = sv.ColorPalette.from_matplotlib(\"hsv\", 10)\r\n)\r\nlabel_annotator = sv.LabelAnnotator()\r\n\r\nannotated_image = box_annotator.annotate(image.copy(), detections=detections)\r\nannotated_image = label_annotator.annotate(annotated_image, detections=detections)\r\n\r\nsv.plot_image(image=annotated_image, size=(8, 8))\r\n```\n\n### Additional\n\nA list of a bunch of matplotlib palettes that fail:\r\n\r\n```\r\ntwilight\r\ntwilight_shifted\r\nhsv\r\njet\r\nturbo\r\nrainbow\r\ngist_rainbow\r\nnipy_spectral\r\ngist_ncar\r\nocean\r\ngist_earth\r\nterrain\r\ngist_stern\r\ngnuplot\r\ngnuplot2\r\nspring\r\nsummer\r\nautumn\r\nwinter\r\ncool\r\nhot\r\ncopper\r\nbone\r\ngreys_r\r\npurples_r\r\nblues_r\r\ngreens_r\r\noranges_r\r\nreds_r\r\n```\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @yeldarby üëãüèª Thanks a lot for reporting this. I've already identified the issue, and I'm working on a fix. "
      },
      {
        "user": "SkalskiP",
        "body": "@yeldarby Fix is available on `develop`. You can test it here: https://colab.research.google.com/drive/1cRxvuuJe8jrQYt8kepwUGRcAU80T9N-y?usp=sharing\r\n\r\nIf you'd face any more problems feel free to reopen the issue."
      }
    ]
  },
  {
    "issue_number": 1362,
    "title": "sv changes bbox shape for object detection with YOLOv8??",
    "author": "abelBEDOYA",
    "state": "open",
    "created_at": "2024-07-15T10:20:40Z",
    "updated_at": "2024-07-16T18:01:17Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI've been using supervision, its tracker, annotators, ... Nice work!! However I've noticed that, doing object detection with yolov8, bboxe shape from ultralytics are changed by supervision even though it refers to the same detection. The following screenshot shows a detected object provided by YOLO, ultralytics.Result (before doing `supervision_tracker.update(results[0])` and after parsing it to `supervision_tracker`.\r\n\r\n![Screenshot from 2024-07-15 12-16-53](https://github.com/user-attachments/assets/1acbed42-6ed1-4184-9de2-7a2059912bb0)\r\nThe bboxes are diferent. I expect they shouldn't...\r\n\r\nCan this bbox shape change be removed? I would like to keep original bbox shape.\r\n\r\nThanks!!\r\n\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @abelBEDOYA üëã \r\n\r\nCould you share a short snippet of the code, with the print statements?\r\n\r\nAlso, to clarify, which of these are you measuring the difference between?\r\n1. ultralytics result (`result.boxes.xyxy`)\r\n2. Detections, as created by `from_ultralytics`\r\n3. Detections, as updated by `tracker.update_with_detections`"
      },
      {
        "user": "abelBEDOYA",
        "body": "Here is the code. It just open webcam with cv2 and runs callback() parsing last frame, which infers and tracks:\r\n\r\n```python\r\nimport numpy as np\r\nimport supervision as sv\r\nfrom ultralytics import YOLO\r\nimport torch\r\n\r\nmodel = YOLO(\"yolov8n.pt\")\r\ntracker = sv.ByteTrack()\r\nbox_annotator = sv.BoundingBoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator()\r\n\r\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\r\n    results = model(frame)[0]\r\n    print('YOLO bbox: ', results.boxes.cpu().xyxy[0] if len(results.boxes.cpu().xyxy)>0 else [])\r\n    detections = sv.Detections.from_ultralytics(results)\r\n    detections = tracker.update_with_detections(detections)\r\n    print('bbox from tracker sv: ', torch.tensor(tracker.tracked_tracks[0].tlbr).cpu())\r\n    print('\\n \\n ')\r\n    labels = [\r\n        f\"#{tracker_id} {results.names[class_id]}\"\r\n        for class_id, tracker_id\r\n        in zip(detections.class_id, detections.tracker_id)\r\n    ]\r\n\r\n    annotated_frame = box_annotator.annotate(\r\n        frame.copy(), detections=detections)\r\n    return label_annotator.annotate(\r\n        annotated_frame, detections=detections, labels=labels)\r\n\r\nimport cv2\r\n\r\n# Abre la webcam (0 es el √≠ndice por defecto de la c√°mara)\r\ncap = cv2.VideoCapture(0)\r\n\r\n# Verifica si la c√°mara se abri√≥ correctamente\r\nif not cap.isOpened():\r\n    print(\"Error: No se puede abrir la c√°mara\")\r\n    exit()\r\n\r\nwhile True:\r\n    # Captura frame por frame\r\n    ret, frame = cap.read()\r\n\r\n    # Si no se recibi√≥ el frame correctamente, sal del loop\r\n    if not ret:\r\n        print(\"Error: No se puede recibir frame (stream end?). Saliendo ...\")\r\n        break\r\n\r\n    img = callback(frame, 0)\r\n    # # Muestra el frame resultante\r\n    cv2.imshow('Webcam', img)\r\n\r\n    # Presiona 'q' para salir del loop\r\n    if cv2.waitKey(1) == ord('q'):\r\n        break\r\n\r\n# Cuando todo est√© listo, libera el capture\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n```\r\n\r\nThese are the \"key\" lines:\r\n![Screenshot from 2024-07-15 13-12-25](https://github.com/user-attachments/assets/2d824c59-82f8-4ba0-a382-e466e730da7d)\r\n\r\nThe output bbox have change (YOLO vs SV):\r\n![Screenshot from 2024-07-15 13-11-54](https://github.com/user-attachments/assets/f9090b7d-479c-4414-9251-0bc2f845e8be)\r\n"
      },
      {
        "user": "LinasKo",
        "body": "Curious. Thanks for letting us know - we'll test it."
      }
    ]
  },
  {
    "issue_number": 1365,
    "title": "Greetings ",
    "author": "Ab222-c",
    "state": "closed",
    "created_at": "2024-07-15T22:29:58Z",
    "updated_at": "2024-07-16T09:03:08Z",
    "labels": [],
    "body": "Abood ",
    "comments": []
  },
  {
    "issue_number": 1349,
    "title": "Issue with NMM Merging Mechanism for Dense Rice Grain Images",
    "author": "SaiJeevanPuchakayala",
    "state": "closed",
    "created_at": "2024-07-12T11:31:12Z",
    "updated_at": "2024-07-15T10:19:13Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Question\r\n\r\nIssue with the stitching mechanism of detected images using Non-Maximum Merging (NMM) in the supervision library. The merging mechanism does not seem to be working correctly on dense rice grain images. The code slices a high-resolution image into smaller 512x512 images for detection and attempts to merge them back to the original 2048x2048 image. However, the merging results are not accurate for densely packed objects.\r\n\r\n\r\n### Steps to Reproduce:\r\n```\r\nimport torch\r\nfrom sahi.predict import get_sliced_prediction\r\nfrom sahi import AutoDetectionModel\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# Callback function for inference\r\ndef callback(image: np.ndarray) -> sv.Detections:\r\n    with torch.no_grad():  # Disable gradient calculation\r\n        result = model(image, device='cuda')[0]  # Specify GPU if available\r\n    return sv.Detections.from_ultralytics(result)\r\n\r\n# Function to process an image and display the result\r\ndef process_image(image_path: str):\r\n    try:\r\n        image = cv2.imread(image_path)\r\n        image = cv2.resize(image, (2048, 2048))\r\n        if image is None:\r\n            raise FileNotFoundError(f\"Image at path {image_path} could not be loaded.\")\r\n\r\n        # Perform inference\r\n        slicer = sv.InferenceSlicer(\r\n            slice_wh=(512, 512),\r\n            overlap_ratio_wh=(0.1, 0.1),\r\n            thread_workers=10,\r\n            iou_threshold=0.1,\r\n            overlap_filter_strategy=sv.OverlapFilter.NON_MAX_MERGE,\r\n            callback=callback)\r\n\r\n        detections = slicer(image)\r\n\r\n        # Filter detections based on confidence\r\n        detections = detections[detections.confidence > 0.3]\r\n\r\n        mask_annotator = sv.MaskAnnotator()\r\n        annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\r\n\r\n        # Draw contours around the masks and fit ellipses\r\n        for i in range(detections.mask.shape[0]):\r\n            mask = detections.mask[i].astype(np.uint8)\r\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n            cv2.drawContours(annotated_image, contours, -1, (0, 255, 0), 2)\r\n\r\n        plt.figure(figsize=(20, 20))\r\n        plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\r\n        plt.axis('off')\r\n        plt.show()\r\n\r\n        # Print the number of detections\r\n        print(f\"Number of detections: {len(detections.confidence)}\")\r\n\r\n    except Exception as e:\r\n        print(f\"An error occurred: {str(e)}\")\r\n        raise\r\n\r\n# Example usage\r\nimage_path = '/content/drive/MyDrive/Mixed Grains/test image .jpg'\r\nprocess_image(image_path)\r\n\r\n```\r\n\r\nAnd here‚Äôs what I‚Äôm seeing: instead of nicely merged detections, I get overlapping and misaligned masks. The NMM just doesn't seem to handle these dense objects well.\r\n\r\n![download](https://github.com/user-attachments/assets/f3b5a630-8c27-460c-b667-73e19851e3d1)\r\n\r\n\r\nSample Image: The image used is a high-resolution dense rice grain image.\r\n\r\nExpected Behavior: The sliced 512x512 images should be correctly merged back to the original 2048x2048 image with accurate detections.\r\n\r\nObserved Behavior: The merging mechanism of NMM does not seem to work as expected on dense rice grain images. The detections are not accurately merged, leading to overlapping and misaligned masks.\r\n\r\n\r\nWhat I‚Äôve Tried:\r\n- Image Size: Adjusted the image size and slice size.\r\n- Overlap Ratio: Played around with different overlap ratios.\r\n- Confidence Threshold: Tweaked the confidence threshold for detections.\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1355,
    "title": "Tracker IDs Skipping",
    "author": "likith1908",
    "state": "open",
    "created_at": "2024-07-13T18:10:05Z",
    "updated_at": "2024-07-15T10:08:58Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Question\r\n\r\n- While using the tracker, I noticed that the tracker IDs are not assigned sequentially or some numbers are being skipped.\r\n\r\n- Videos Used: https://drive.google.com/drive/folders/14o1MStr357HPr0zl1uauMP6k4swOh63D?usp=sharing\r\nThis folder has input and output video files\r\n\r\n- Below is my code\r\n\r\n```\r\nimport argparse\r\nimport supervision as sv\r\nimport cv2\r\nfrom ultralytics import YOLO\r\nimport numpy as np\r\nfrom collections import defaultdict, deque\r\n\r\nSOURCE = np.array([[0, 0], [3072, 0], [3072, 1080], [0, 1080]])\r\n\r\nTARGET_WIDTH = 0.7388\r\nTARGET_HEIGHT = 0.2594\r\n\r\nTARGET = np.array([\r\n    [0, 0],\r\n    [TARGET_WIDTH, 0],\r\n    [TARGET_WIDTH, TARGET_HEIGHT],\r\n    [0, TARGET_HEIGHT]\r\n])\r\n\r\nclass ViewTransformer:\r\n    def __init__(self, source=np.ndarray, target=np.ndarray):\r\n        source = source.astype(np.float32)\r\n        target = target.astype(np.float32)\r\n        self.m = cv2.getPerspectiveTransform(source, target)\r\n\r\n    def transform_points(self, points: np.ndarray) -> np.ndarray:\r\n        if points.size == 0:\r\n            print(\"Warning: No points to transform.\")\r\n            return np.array([])\r\n        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\r\n        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\r\n        return transformed_points.reshape(-1, 2)\r\n\r\ndef parse_arguments() -> argparse.Namespace:\r\n    parser = argparse.ArgumentParser(\r\n        description=\"Speed Estimation using Ultralytics and Supervision\"\r\n    )\r\n    parser.add_argument(\r\n        \"--source_video_path\",\r\n        required=False,\r\n        default=\"/input.mp4\",\r\n        help=\"Path to the source video file\",\r\n        type=str``\r\n    )\r\n    return parser.parse_args()\r\n\r\ndef main():\r\n    args = parse_arguments()\r\n    print(f\"Source video path: {args.source_video_path}\")\r\n    \r\n    video_info = sv.VideoInfo.from_video_path(args.source_video_path)\r\n    print(f\"Video info: {video_info}\")\r\n    \r\n    model = YOLO(\"model.pt\")\r\n    print(\"YOLO model loaded.\")\r\n    \r\n    byte_track = sv.ByteTrack(frame_rate=video_info.fps)\r\n    print(\"ByteTrack initialized.\")\r\n    \r\n    thickness = sv.calculate_optimal_line_thickness(resolution_wh=video_info.resolution_wh)\r\n    text_scale = sv.calculate_optimal_text_scale(resolution_wh=video_info.resolution_wh)\r\n    bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=thickness, color_lookup=sv.ColorLookup.TRACK)\r\n    label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=thickness, text_position= sv.Position.BOTTOM_CENTER, color_lookup=sv.ColorLookup.TRACK)\r\n    trace_annotator = sv.TraceAnnotator(thickness=thickness, trace_length= video_info.fps * 2 , position= sv.Position.BOTTOM_CENTER, color_lookup= sv.ColorLookup.TRACK)\r\n    frame_generator = sv.get_video_frames_generator(args.source_video_path)\r\n    polygon_zone = sv.PolygonZone(SOURCE)\r\n\r\n    # box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\r\n    # box_annotator = sv.BoundingBoxAnnotator(thickness=thickness)\r\n    zone_annotator = sv.PolygonZoneAnnotator(zone=polygon_zone, color=sv.Color.WHITE, thickness=6, text_thickness=6, text_scale=4)\r\n\r\n    view_transformer = ViewTransformer(SOURCE, TARGET)\r\n    coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\r\n\r\n    frame_count = 0\r\n    with sv.VideoSink(target_path='output.mp4', video_info=video_info) as sink:\r\n\r\n        for frame in frame_generator:\r\n            \r\n            try:\r\n                frame_count += 1\r\n                print(f\"Processing frame {frame_count}/{video_info.total_frames}\")\r\n                \r\n                # Ensure the frame is valid\r\n                if frame is None:\r\n                    print(f\"Frame {frame_count} is None, skipping.\")\r\n                    continue\r\n                \r\n                result = model(frame)\r\n                print(\"Frame processed by model.\")\r\n                \r\n                if not result:\r\n                    print(f\"No result for frame {frame_count}, skipping.\")\r\n                    continue\r\n                \r\n                detections = sv.Detections.from_ultralytics(result[0])\r\n                print(f\"Detections: {detections}\")\r\n                \r\n                detections = detections[polygon_zone.trigger(detections)]\r\n                detections = byte_track.update_with_detections(detections=detections)\r\n                \r\n                points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\r\n                if points.size > 0:\r\n                    points = view_transformer.transform_points(points=points)\r\n                else:\r\n                    print(\"No points detected in the frame.\")\r\n                \r\n                labels = []\r\n                for tracker_id, [_, y] in zip(detections.tracker_id, points):\r\n                    coordinates[tracker_id].append(y)\r\n                    if len(coordinates[tracker_id]) < video_info.fps / 2:\r\n                        labels.append(f\"#{tracker_id}\")\r\n                    else:\r\n                        coordinates_start = coordinates[tracker_id][-1]\r\n                        coordinates_stop = coordinates[tracker_id][0]\r\n                        distance = abs(coordinates_start - coordinates_stop)\r\n                        time = len(coordinates[tracker_id]) / video_info.fps\r\n                        speed = (distance / time) * 3.6\r\n\r\n                        print(f\"Tracker ID: {tracker_id}\")\r\n                        print(f\"Coordinates Start: {coordinates_start}\")\r\n                        print(f\"Coordinates Stop: {coordinates_stop}\")\r\n                        print(f\"Distance: {distance}\")\r\n                        print(f\"Time: {time}\")\r\n                        print(f\"Speed: {speed} km/h\")\r\n\r\n                        labels.append(f\"#{tracker_id}, {float(speed)} kmph\")\r\n\r\n                annotated_frame = frame.copy()\r\n                annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)\r\n                annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\r\n                annotated_frame = sv.draw_polygon(annotated_frame, polygon=SOURCE, color=sv.Color.RED)\r\n                annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\r\n                annotated_frame = zone_annotator.annotate(scene=annotated_frame)            \r\n                # cv2.namedWindow(\"Annotated Frame\", cv2.WINDOW_NORMAL)\r\n                # cv2.imshow(\"Annotated Frame\", annotated_frame)\r\n                sink.write_frame(frame=annotated_frame)\r\n                if cv2.waitKey(1) == ord(\"q\"):\r\n                    break\r\n            except Exception as e:\r\n                print(f\"Error processing frame {frame_count}: {e}\")\r\n        \r\n        cv2.destroyAllWindows()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Can provide you ```model.pt``` if you like to reproduce.\r\n\r\n\r\n- I have also observed the same in a YouTube video as well attaching the link and the timestamp for reference\r\n\r\n While using the tracker, I noticed that the tracker IDs are not assigned sequentially. Specifically, after tracker ID \"#6\", the next assigned tracker ID was \"#8\". Tracker ID \"#7\" was skipped. This issue can be observed starting at timestamp [22:45](https://www.youtube.com/watch?v=uWP6UjDeZvY&t=1365s) in the video.\r\n\r\nlink to video : https://www.youtube.com/watch?v=uWP6UjDeZvY OR click on timestamp\r\n\r\n\r\nCan someone explain this, is there any issue with TrackerID or is there any mistake in my code??@LinasKo @skylargivens @iurisilvio @sberan\r\n\r\n\r\nThanks\r\nLikith G\r\n",
    "comments": [
      {
        "user": "likith1908",
        "body": "Hi @LinasKo,\r\n\r\nI see similar type of issue was closed and have tried  using ```supervision==0.21.0rc5``` as mentioned in #1196 to overcome the issue of tracker_id's , but it leads me to the bug we were facing earlier #1345 , so I had to go back to ```supervision==0.22.0rc1```\r\n\r\nThanks\r\nLikith"
      },
      {
        "user": "LinasKo",
        "body": "Hi again @likith1908 üëã \r\n\r\nYou don't have to tag any members of the team - we see every issue coming in üòâ \r\n(Also, the only core members working on supervision are me and @SkalskiP)\r\n\r\nRegarding tracking, it will skip IDs if they're uncertain. We have this on our sights and may fix it in `supervision 0.24.0` or `supervision 0.25.0`, which is estimated to be in September-October."
      },
      {
        "user": "likith1908",
        "body": "Thanks and Sorry for tagging everyone. Would like to know what is causing to skip the ID's?\r\n\r\nLikith"
      }
    ]
  },
  {
    "issue_number": 1359,
    "title": "Help with using Supervision on real time feed",
    "author": "likith1908",
    "state": "closed",
    "created_at": "2024-07-15T04:48:35Z",
    "updated_at": "2024-07-15T09:43:58Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI am using a Lucid Machine Vision Camera and I want to use supervision for detection, speed estimation of objects from real time feed.\r\n\r\nSince some of the functions/annotators are having the arguments as the ```video_info``` which has ```fps``` and ```total_frames``` which cannot be calculated on a real time feed and also I am using a machine vision camera which are connected with ethernet cable so getting feed from the cameras is also very different compared to web cam's since ```cv2.VideoCapture()``` doesn't work. \r\n\r\n\r\n# I am attaching the code how I get the camera feed below.  \r\n```\r\nimport time\r\nimport cv2\r\nimport numpy as np\r\nfrom arena_api import enums\r\nfrom arena_api.buffer import BufferFactory\r\nfrom arena_api.system import system\r\n\r\nwindow_width = 800\r\nwindow_height = 600\r\n\r\ndef select_device_from_user_input():\r\n\r\n    device_infos = system.device_infos\r\n    if len(device_infos) == 0:\r\n        print(\"No camera connected\\nPress enter to search again\")\r\n        input()\r\n    print(\"Devices found:\")\r\n    selected_index = 0\r\n    for i in range(len(device_infos)):\r\n        if device_infos[i]['serial'] == \"camera_serial_number\":\r\n            selected_index = i\r\n\r\n    selected_model = device_infos[selected_index]['model']\r\n    print(f\"\\nCreate device: {selected_model}...\")\r\n    device = system.create_device(device_infos=device_infos[selected_index])[0]\r\n\r\n    return device\r\n\r\n\r\ndef apply_gamma_correction(frame, gamma):\r\n    corrected_frame = frame.astype(np.float32) / 255.0\r\n    corrected_frame = np.power(corrected_frame, gamma)\r\n    corrected_frame = (corrected_frame * 255.0).astype(np.uint8)\r\n    return corrected_frame\r\n\r\ndef is_moving(frame1, frame2):\r\n    diff = np.sqrt(np.mean(np.square(frame1 - frame2)))\r\n    print(diff)\r\n    return diff\r\n\r\ndef get_image_buffers(is_color_camera=False):\r\n    device = select_device_from_user_input()\r\n\r\n    device.tl_stream_nodemap.get_node(\r\n        'StreamBufferHandlingMode').value = 'NewestOnly'\r\n    device.tl_stream_nodemap.get_node('StreamPacketResendEnable').value = True\r\n    device.tl_stream_nodemap.get_node(\r\n        'StreamAutoNegotiatePacketSize').value = True\r\n\r\n    isp_bayer_pattern = device.nodemap.get_node('IspBayerPattern').value\r\n    is_color_camera = False\r\n\r\n    device.nodemap.get_node('Width').value = 3072\r\n    device.nodemap.get_node('Height').value = 1080\r\n\r\n    if isp_bayer_pattern != 'NONE':\r\n        is_color_camera = True\r\n\r\n    if is_color_camera == True:\r\n        device.nodemap.get_node('PixelFormat').value = \"BayerRG8\"\r\n    else:\r\n        device.nodemap.get_node('PixelFormat').value = \"Mono8\"\r\n\r\n    device.nodemap.get_node('DeviceStreamChannelPacketSize').value = 1500\r\n    device.nodemap.get_node('AcquisitionMode').value = \"Continuous\"\r\n    device.nodemap.get_node('AcquisitionFrameRateEnable').value = True\r\n    device.nodemap.get_node('AcquisitionFrameRate').value = 30.0\r\n    device.nodemap.get_node('AcquisitionFrameRateEnable').value = True\r\n    device.nodemap.get_node('GainAuto').value = \"Off\"\r\n    device.nodemap.get_node('Gain').value = 0.0\r\n\r\n    device.nodemap.get_node('BalanceWhiteEnable').value = True\r\n    device.nodemap.get_node('BalanceWhiteAuto').value = \"Continuous\"\r\n\r\n    device.nodemap['GammaEnable'].value = True\r\n    device.nodemap.get_node('ExposureAuto').value = \"Off\"\r\n    device.nodemap.get_node('ExposureTime').value = 4000.00\r\n    device.nodemap['Gamma'].value = 0.350\r\n    device.nodemap['ColorTransformationEnable'].value = True\r\n\r\n    key = -1\r\n    cv2.namedWindow(\"Image-1\", cv2.WINDOW_NORMAL)\r\n\r\n    device.start_stream()\r\n    \r\n    # Initialize FPS calculation\r\n    fps_start_time = time.time()\r\n    fps_counter = 0\r\n\r\n    while True:\r\n        image_buffer = device.get_buffer()  # optional args\r\n        nparray = np.ctypeslib.as_array(image_buffer.pdata, shape=(image_buffer.height, image_buffer.width, int(\r\n            image_buffer.bits_per_pixel / 8))).reshape(image_buffer.height, image_buffer.width, int(image_buffer.bits_per_pixel / 8))\r\n\r\n        if is_color_camera == True:\r\n            display_img = cv2.cvtColor(nparray, cv2.COLOR_BayerBG2BGR)\r\n            nparray = cv2.cvtColor(display_img, cv2.COLOR_BGR2GRAY)\r\n        else:\r\n            display_img = cv2.cvtColor(nparray, cv2.COLOR_GRAY2BGR)\r\n\r\n        decoded_img = display_img\r\n        # decoded_img = display_img[700:2000, :]\r\n        \r\n        print(decoded_img.shape)\r\n        # decoded_img = apply_gamma_correction(decoded_img, gamma=0.5)\r\n\r\n        # Calculate and display FPS\r\n        fps_counter += 1\r\n        if time.time() - fps_start_time >= 1:\r\n            fps = fps_counter / (time.time() - fps_start_time)\r\n            fps_start_time = time.time()\r\n            fps_counter = 0\r\n            cv2.putText(decoded_img, f'FPS: {fps:.2f}', (50, 150),\r\n                        cv2.FONT_HERSHEY_SIMPLEX, 6, (0, 255, 0), 2)\r\n            print(fps)\r\n\r\n        cv2.imshow(\"Image-1\", decoded_img)\r\n\r\n        key = cv2.waitKey(1) & 0xFF\r\n        if key == ord(\"q\"):\r\n            break\r\n        device.requeue_buffer(image_buffer)\r\n\r\nif __name__ == \"__main__\":\r\n    get_image_buffers(is_color_camera=True)\r\n\r\n```\r\nDon't worry about ```device.nodemap``` or ```device.nodemap.get_node``` these are the settings to get a clear image from the camera.\r\n\r\nüîç Seeking Expert Help üîç\r\nDear @LinasKo, @skylargivens, @iurisilvio, @sberan,\r\n\r\nI hope you can assist with a challenging issue I've encountered. \r\n\r\n\r\nThanks \r\nLikith \n\n### Additional\n\n### I've been deeply engaged in refining the following code snippet to achieve a dual goal: detecting objects and calculating speed estimation. But sadly unable to achieve what I am looking for, The core of our computations revolves around the ```decoded_image```(have a look at the code below). Your expertise in this area could provide crucial insights into overcoming the current challenges.\r\n\r\nI hope this context gives you a clear understanding of the work at hand. Your input on this matter would be greatly appreciated.\r\n\r\n```\r\nimport argparse\r\nfrom collections import defaultdict, deque\r\nfrom pathlib import Path\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom shapely.geometry import Polygon, Point\r\n\r\nfrom ultralytics import YOLO\r\nfrom ultralytics.utils.plotting import Annotator, colors\r\n\r\nimport time\r\nfrom arena_api import enums\r\nfrom arena_api.buffer import BufferFactory\r\nfrom arena_api.system import system\r\nimport torch\r\n\r\nimport supervision as sv\r\n\r\nSOURCE = np.array([[0, 0], [3072, 0], [3072, 1080], [0, 1080]])\r\n\r\nTARGET_WIDTH = 0.7388 \r\nTARGET_HEIGHT = 0.2594\r\n\r\nTARGET = np.array([\r\n    [0, 0],\r\n    [TARGET_WIDTH, 0],\r\n    [TARGET_WIDTH, TARGET_HEIGHT],\r\n    [0, TARGET_HEIGHT]\r\n])\r\n\r\n# Global variables\r\nwindow_width = 800\r\nwindow_height = 600\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\nprint(\"Current Device:\", device)\r\nis_color_camera = False\r\ngamma = 0.1\r\ngamma_table = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in np.arange(0, 256)]).astype(np.uint8)\r\n\r\ndef parse_arguments() -> argparse.Namespace:\r\n    parser = argparse.ArgumentParser(\r\n        description=\"Speed Estimation using Ultralytics and Supervision\"\r\n    )\r\n    parser.add_argument(\r\n        \"--source_video_path\",\r\n        required=False,\r\n        default=\"/home/harvestedlabs/Desktop/Codes/39l.mp4\",\r\n        help=\"Path to the source video file\",\r\n        type=str\r\n    )\r\n    return parser.parse_args()\r\n\r\nclass ViewTransformer:\r\n    def __init__(self, source=np.ndarray, target=np.ndarray):\r\n        source = source.astype(np.float32)\r\n        target = target.astype(np.float32)\r\n        self.m = cv2.getPerspectiveTransform(source, target)\r\n\r\n    def transform_points(self, points: np.ndarray) -> np.ndarray:\r\n        if points.size == 0:\r\n            print(\"Warning: No points to transform.\")\r\n            return np.array([])\r\n        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\r\n        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\r\n        return transformed_points.reshape(-1, 2)\r\n\r\ndef select_device_from_user_input():\r\n    device_infos = system.device_infos\r\n    if len(device_infos) == 0:\r\n        print(\"No camera connected\\nPress enter to search again\")\r\n        input()\r\n    print(\"Devices found:\")\r\n    selected_index = 0\r\n    for i in range(len(device_infos)):\r\n        if device_infos[i]['serial'] == \"222600043\":\r\n            selected_index = i\r\n        # 222600043  223200992\r\n\r\n    selected_model = device_infos[selected_index]['model']\r\n    print(f\"\\nCreate device: {selected_model}...\")\r\n    device = system.create_device(device_infos=device_infos[selected_index])[0]\r\n\r\n    return device\r\n\r\ndef get_image_buffers(is_color_camera=False):\r\n    \"\"\"Captures and processes image buffers from the camera.\"\"\"\r\n    device = select_device_from_user_input()\r\n\r\n    # Camera configuration\r\n    device.tl_stream_nodemap.get_node('StreamBufferHandlingMode').value = 'NewestOnly'\r\n    device.tl_stream_nodemap.get_node('StreamPacketResendEnable').value = True\r\n    device.tl_stream_nodemap.get_node('StreamAutoNegotiatePacketSize').value = True\r\n\r\n    isp_bayer_pattern = device.nodemap.get_node('IspBayerPattern').value\r\n    is_color_camera = isp_bayer_pattern != 'NONE'\r\n\r\n    device.nodemap.get_node('Width').value = 3072\r\n    device.nodemap.get_node('Height').value = 2048\r\n\r\n    if is_color_camera:\r\n        device.nodemap.get_node('PixelFormat').value = \"BayerRG8\"\r\n    else:\r\n        device.nodemap.get_node('PixelFormat').value = \"Mono8\"\r\n\r\n    # Features\r\n    device.nodemap.get_node('BalanceWhiteAuto').value = \"Continuous\"\r\n    device.nodemap.get_node('DeviceStreamChannelPacketSize').value = 1500\r\n    device.nodemap.get_node('AcquisitionMode').value = \"Continuous\"\r\n    device.nodemap.get_node('AcquisitionFrameRateEnable').value = True\r\n    device.nodemap['ColorTransformationEnable'].value = True\r\n    device.nodemap['BalanceWhiteEnable'].value = True\r\n    device.nodemap['GammaEnable'].value = True\r\n    device.nodemap['Gamma'].value = 0.350\r\n    device.nodemap.get_node('ExposureAuto').value = \"Off\"\r\n    device.nodemap.get_node('ExposureTime').value = 2000.00\r\n    device.nodemap.get_node('GainAuto').value = \"Off\"\r\n    device.nodemap.get_node('Gain').value = 0.0\r\n\r\n    device.start_stream()\r\n\r\n    fps_start_time = time.time()  # Initialize start_time\r\n    fps_counter = 0  # Initialize fps_counter\r\n\r\n    return device, fps_start_time, fps_counter\r\n\r\ndef process_frames(device, fps_start_time, fps_counter):\r\n    \"\"\"Process frames from the device and calculate FPS.\"\"\"\r\n    model = YOLO(\"/home/harvestedlabs/Desktop/Codes/Likith/token.pt\")\r\n    print(\"YOLO model loaded.\")\r\n\r\n    byte_track = sv.ByteTrack(frame_rate=0)  # Placeholder, will be updated later\r\n    print(\"ByteTrack initialized.\")\r\n\r\n    # Obtain the resolution of the camera feed\r\n    width = device.nodemap.get_node('Width').value\r\n    height = device.nodemap.get_node('Height').value\r\n    resolution_wh = (width, height)\r\n\r\n    thickness = sv.calculate_optimal_line_thickness(resolution_wh=resolution_wh)\r\n    text_scale = sv.calculate_optimal_text_scale(resolution_wh=resolution_wh)\r\n    bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=thickness, color_lookup=sv.ColorLookup.TRACK)\r\n    label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=thickness, text_position=sv.Position.BOTTOM_CENTER, color_lookup=sv.ColorLookup.TRACK)\r\n    trace_annotator = sv.TraceAnnotator(thickness=thickness, trace_length=0, position=sv.Position.BOTTOM_CENTER, color_lookup=sv.ColorLookup.TRACK)  # Placeholder, will be updated later\r\n    polygon_zone = sv.PolygonZone(SOURCE)\r\n\r\n    zone_annotator = sv.PolygonZoneAnnotator(zone=polygon_zone, color=sv.Color.WHITE, thickness=6, text_thickness=6, text_scale=4)\r\n\r\n    view_transformer = ViewTransformer(SOURCE, TARGET)\r\n    coordinates = defaultdict(lambda: deque(maxlen=0))  # Placeholder, will be updated later\r\n\r\n    # Define video_info\r\n    video_info = sv.VideoInfo(width=width, height=height, fps=30, total_frames=None)  # Change fps if necessary\r\n\r\n    with sv.VideoSink(target_path='target.mp4', video_info=video_info) as sink:\r\n        while True:\r\n            image_buffer = device.get_buffer()\r\n            nparray = np.ctypeslib.as_array(image_buffer.pdata, shape=(image_buffer.height, image_buffer.width, int(\r\n                image_buffer.bits_per_pixel / 8))).reshape(image_buffer.height, image_buffer.width, int(image_buffer.bits_per_pixel / 8))\r\n\r\n            if is_color_camera:\r\n                display_img = cv2.cvtColor(nparray, cv2.COLOR_BayerBG2BGR)\r\n                nparray = cv2.cvtColor(display_img, cv2.COLOR_BGR2GRAY)\r\n            else:\r\n                display_img = cv2.cvtColor(nparray, cv2.COLOR_GRAY2BGR)\r\n\r\n            decoded_img = display_img\r\n            image = cv2.resize(decoded_img, (960, 540))\r\n\r\n            # Calculate and display FPS\r\n            fps_counter += 1\r\n            if time.time() - fps_start_time >= 1:\r\n                fps = fps_counter / (time.time() - fps_start_time)\r\n                fps_start_time = time.time()\r\n                fps_counter = 0\r\n                cv2.putText(decoded_img, f'FPS: {fps:.2f}', (50, 150),\r\n                            cv2.FONT_HERSHEY_SIMPLEX, 6, (0, 255, 0), 2)\r\n                print(fps)\r\n\r\n            byte_track.frame_rate = fps\r\n            trace_annotator.trace_length = fps * 2\r\n            coordinates.default_factory = lambda: deque(maxlen=fps)\r\n\r\n            try:\r\n                result = model(image)\r\n                print(\"Frame processed by model.\")\r\n\r\n                if not result:\r\n                    print(\"No result for the frame, skipping.\")\r\n                    continue\r\n\r\n                detections = sv.Detections.from_ultralytics(result[0])\r\n                detections = detections[polygon_zone.trigger(detections)]\r\n                detections = byte_track.update_with_detections(detections=detections)\r\n\r\n                points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\r\n                if points.size > 0:\r\n                    points = view_transformer.transform_points(points=points)\r\n                else:\r\n                    print(\"No points detected in the frame.\")\r\n\r\n                labels = []\r\n                for tracker_id, [_, point] in zip(detections.tracker_id, points):\r\n                    coordinates[tracker_id].append(point)\r\n                    points = np.array(coordinates[tracker_id], np.int32)\r\n                    if points.size > 0:\r\n                        speeds = sv.calculate_speed(\r\n                            points=points, fps=byte_track.frame_rate, scaler=3600 / 1000)\r\n                        label = f'{speeds[-1]:.2f} km/h'\r\n                        labels.append(label)\r\n\r\n                zone_annotator.annotate(frame=image)\r\n                bounding_box_annotator.annotate(frame=image, detections=detections)\r\n                label_annotator.annotate(frame=image, detections=detections, labels=labels)\r\n                trace_annotator.annotate(frame=image, detections=detections, tracker_coordinates=coordinates)\r\n\r\n            except Exception as e:\r\n                print(f\"Error processing frame: {e}\")\r\n\r\n            sink.write_frame(image)\r\n\r\n            cv2.imshow(\"Processed Frame\", image)\r\n            if cv2.waitKey(1) & 0xFF == ord('q'):\r\n                break\r\n\r\n    device.stop_stream()\r\n    system.destroy_device(device)\r\n\r\ndef main() -> None:\r\n    args = parse_arguments()\r\n    device, fps_start_time, fps_counter = get_image_buffers()\r\n    process_frames(device, fps_start_time, fps_counter)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```",
    "comments": []
  },
  {
    "issue_number": 1346,
    "title": "text cannot be painted in valid area in `RichLabelAnnotator`",
    "author": "Ying-Kang",
    "state": "closed",
    "created_at": "2024-07-12T07:20:35Z",
    "updated_at": "2024-07-12T09:44:40Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nhere is my code:\r\n```python\r\n...\r\nLABEL_ANNOTATOR = sv.RichLabelAnnotator(font_path=\"utils/resource/xxxfont.ttf\", color=sv.Color.ROBOFLOW)\r\n...\r\nimg = BOUNDING_BOX_ANNOTATOR.annotate(img, detections)\r\nimg = LABEL_ANNOTATOR.annotate(img, detections, labels=labels)\r\n```\r\nmost cases, it works well\r\nhowever, when the bbox with xmin=0, ymin=0, which is the edge case\r\npainted text area can not be put in a visible area just like below\r\n![image](https://github.com/user-attachments/assets/f383c3c4-0ce0-4b5a-98c4-43a98ee38f0c)\r\n\n\n### Environment\n\n- python3.9\r\n- supervision 0.21\r\n- ubuntu\n\n### Minimal Reproducible Example\n\nsame as illurstrated in bug\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @Ying-Kang üëã \r\n\r\nThis is expected behavior. By default, label boxes are meant to be shown **above** the top-left corner of the bounding box.\r\n\r\nIf that doesn't work for you, I suggest playing around with the `text_position`.\r\n\r\nIf none of the values for it work for your use case, do tell."
      },
      {
        "user": "Ying-Kang",
        "body": "undestand...\r\nI mean it can be more intelligient if you could offer more edge judgement instead of align with fixed behavior like 'top left'"
      },
      {
        "user": "LinasKo",
        "body": "Yeah, I've been thinking similarly myself. I'll add it to our backlog"
      }
    ]
  },
  {
    "issue_number": 1343,
    "title": "Speed Estimation",
    "author": "likith1908",
    "state": "closed",
    "created_at": "2024-07-11T21:01:25Z",
    "updated_at": "2024-07-11T21:10:07Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\n## Resource used : I have followed the steps mentioned in the youtube video : https://www.youtube.com/watch?v=uWP6UjDeZvY\r\n\r\nI am using tokens on a treadmill and i want to do speed estimation, i am getting speed as 0.0 kmph since it's moving at very less speed and my source and target are mentioned below\r\n\r\n```\r\nSOURCE = np.array([[0, 0], [2868, 0], [2868, 1104], [0, 1104]])\r\n\r\nTARGET_WIDTH = 0.758825\r\nTARGET_HEIGHT = 0.29210\r\n\r\nTARGET = np.array([\r\n    [0, 0],\r\n    [TARGET_WIDTH, 0],\r\n    [TARGET_WIDTH, TARGET_HEIGHT],\r\n    [0, TARGET_HEIGHT]\r\n])\r\n```\r\n\r\n# Video \r\nlink : https://drive.google.com/file/d/1z5QKJ9g7mCbz14N9NDoeso5YVskJwlig/view?usp=sharing\r\n\r\nSince my video is of dimensions or resolution : 2868‚Ää√ó‚Ää1104\r\nI have got my real life values as \r\n# My code\r\n\r\n```\r\nimport argparse\r\nimport supervision as sv\r\nimport cv2\r\nfrom ultralytics import YOLO\r\nimport numpy as np\r\nfrom collections import defaultdict, deque\r\n\r\nSOURCE = np.array([[0, 0], [2868, 0], [2868, 1104], [0, 1104]])\r\n\r\nTARGET_WIDTH = 0.758825\r\nTARGET_HEIGHT = 0.29210\r\n\r\nTARGET = np.array([\r\n    [0, 0],\r\n    [TARGET_WIDTH, 0],\r\n    [TARGET_WIDTH, TARGET_HEIGHT],\r\n    [0, TARGET_HEIGHT]\r\n])\r\n\r\nclass ViewTransformer:\r\n    def __init__(self, source=np.ndarray, target=np.ndarray):\r\n        source = source.astype(np.float32)\r\n        target = target.astype(np.float32)\r\n        self.m = cv2.getPerspectiveTransform(source, target)\r\n\r\n    def transform_points(self, points: np.ndarray) -> np.ndarray:\r\n        if points.size == 0:\r\n            print(\"Warning: No points to transform.\")\r\n            return np.array([])\r\n        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\r\n        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\r\n        return transformed_points.reshape(-1, 2)\r\n\r\ndef parse_arguments() -> argparse.Namespace:\r\n    parser = argparse.ArgumentParser(\r\n        description=\"Speed Estimation using Ultralytics and Supervision\"\r\n    )\r\n    parser.add_argument(\r\n        \"--source_video_path\",\r\n        required=False,\r\n        default=\"/home/harvestedlabs/Desktop/Codes/test.mp4\",\r\n        help=\"Path to the source video file\",\r\n        type=str\r\n    )\r\n    return parser.parse_args()\r\n\r\ndef main():\r\n    args = parse_arguments()\r\n    video_info = sv.VideoInfo.from_video_path(args.source_video_path)\r\n    model = YOLO(\"/home/harvestedlabs/Desktop/Codes/Likith/token.pt\")\r\n    byte_track = sv.ByteTrack(frame_rate=video_info.fps)\r\n    thickness = sv.calculate_optimal_line_thickness(resolution_wh=video_info.resolution_wh)\r\n    text_scale = sv.calculate_optimal_text_scale(resolution_wh=video_info.resolution_wh)\r\n    bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=thickness)\r\n    label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=thickness)\r\n    frame_generator = sv.get_video_frames_generator(args.source_video_path)\r\n    polygon_zone = sv.PolygonZone(SOURCE, frame_resolution_wh=video_info.resolution_wh)\r\n    view_transformer = ViewTransformer(SOURCE, TARGET)\r\n    coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\r\n\r\n    for frame in frame_generator:\r\n        result = model(frame)[0]  # Ensure using the correct method for prediction\r\n        detections = sv.Detections.from_ultralytics(result)\r\n        detections = detections[polygon_zone.trigger(detections)]\r\n        detections = byte_track.update_with_detections(detections=detections)\r\n\r\n        points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\r\n        if points.size > 0:\r\n            points = view_transformer.transform_points(points=points).astype(int)\r\n        else:\r\n            print(\"No points detected in the frame.\")\r\n\r\n        labels = []\r\n        for tracker_id, [_, y] in zip(detections.tracker_id, points):\r\n            coordinates[tracker_id].append(y)\r\n            if len(coordinates[tracker_id]) < video_info.fps / 2:\r\n                labels.append(f\"#{tracker_id}\")\r\n            else:\r\n                coordinates_start = coordinates[tracker_id][-1]\r\n                coordinates_stop = coordinates[tracker_id][0]\r\n                distance = abs(coordinates_start - coordinates_stop)\r\n                time = len(coordinates[tracker_id]) / video_info.fps\r\n                speed = (distance / time) * 3.6\r\n                labels.append(f\"#{tracker_id}, {float(speed)} kmph\")\r\n\r\n        annotated_frame = frame.copy()\r\n        annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\r\n        annotated_frame = sv.draw_polygon(annotated_frame, polygon=SOURCE, color=sv.Color.RED)\r\n        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\r\n\r\n        cv2.namedWindow(\"Annotated Frame\", cv2.WINDOW_NORMAL)\r\n        cv2.imshow(\"Annotated Frame\", annotated_frame)\r\n\r\n        if cv2.waitKey(1) == ord(\"q\"):\r\n            break\r\n\r\n    cv2.destroyAllWindows()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n### Can someone explain how to resolve this issue?\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 357,
    "title": "Real time video streaming",
    "author": "Alex-Wenner-FHR",
    "state": "closed",
    "created_at": "2023-09-05T17:31:19Z",
    "updated_at": "2024-07-10T09:01:57Z",
    "labels": [
      "question",
      "api:video"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI have a question around the `VideoSink`. I know that support for a static video file is there, but is there support for real time streaming? Maybe by using something like an RTSP address? \r\n\r\nI was digging through the docs and did not find anything that insinuated there was. \r\n\r\nThanks!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "hardikdava",
        "body": "@Alex-Wenner-FHR Thanks for raising an issue. Currently, it is not supported. If `cv2.VideoCapture` supports direct RTSP stream then you should be able to use it directly. Otherwise, I am afraid it is not supported yet."
      },
      {
        "user": "SkalskiP",
        "body": "Hi @Alex-Wenner-FHR üëãüèª Like @hardikdava said, it is not supported yet. But I like the idea a lot, I'd love to add this to our roadmap. "
      }
    ]
  },
  {
    "issue_number": 1338,
    "title": "Add Gpu support to time in zone solutions ! ",
    "author": "Rasantis",
    "state": "closed",
    "created_at": "2024-07-10T04:52:50Z",
    "updated_at": "2024-07-10T08:47:07Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nit not support executes the code in the gpu/cuda\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @Rasantis üëã \r\n\r\nApart from running models, we strive to have our code run efficiently on the CPU. Wide GPU operation support is beyond the scope of this library for the time being."
      }
    ]
  },
  {
    "issue_number": 1337,
    "title": "What models or data sets are used for object detection",
    "author": "dearMOMO",
    "state": "closed",
    "created_at": "2024-07-10T03:50:18Z",
    "updated_at": "2024-07-10T07:03:01Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nThe following picture is an example, but no documentation was found\r\n\r\n![image](https://github.com/roboflow/supervision/assets/34231503/a5c9d548-a641-43ca-b9e0-9aff26aee6b9)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "Example code: https://github.com/roboflow/supervision/blob/develop/examples%2Ftraffic_analysis%2FREADME.md"
      }
    ]
  },
  {
    "issue_number": 1333,
    "title": "How to determine the source coordinates better in computer vision speed estimation",
    "author": "dearMOMO",
    "state": "closed",
    "created_at": "2024-07-08T14:47:59Z",
    "updated_at": "2024-07-08T20:53:10Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nblog    https://blog.roboflow.com/estimate-speed-computer-vision/\r\n\r\n![image](https://github.com/roboflow/supervision/assets/34231503/62335a5a-5133-4c22-be2a-4ee76ce5f4d6)\r\n\r\nIs there A good tool to calculate the coordinates of A, B, C, D? Thank you very much\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1330,
    "title": "Estimate Speed with Computer Vision",
    "author": "dearMOMO",
    "state": "closed",
    "created_at": "2024-07-07T09:35:13Z",
    "updated_at": "2024-07-08T14:35:28Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nspeed = distance / time * 3.6\r\nExcuse me, what does 3.6 do here\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "I don't know the context, but:\n\nThat's an hour (3600 seconds) divided by 1000m - it's used to convert between `km/h` and `m/s`."
      },
      {
        "user": "dearMOMO",
        "body": "Wow, thank you so much"
      }
    ]
  },
  {
    "issue_number": 1308,
    "title": "Panoptic Segmentation Annotator Request",
    "author": "bhyun-kim",
    "state": "open",
    "created_at": "2024-06-25T18:06:10Z",
    "updated_at": "2024-07-06T04:01:56Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nIt seems that supervision doesn't currently support Panoptic segmentation annotator. Is this feature included in your roadmap? If so, I would like to contribute.\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @bhyun-kim üëã \r\n\r\nSure, you can go ahead! Which model were you thinking of? I know `transfomers` has panoptic that we don't support yet, for example.\r\n\r\nIf I understand correctly, we'll need 2 types of `id` values - one for class and one for instance. If so, you may use `Detections.class_id` and create `PANOPTIC_INSTANCE_ID_DATA_FIELD` in `supervision/config.py` (it can be used in the `Detections.data` field)."
      },
      {
        "user": "bhyun-kim",
        "body": "Hi @LinasKo,\r\n\r\n> Sure, you can go ahead! Which model were you thinking of? I know `transfomers` has panoptic that we don't support yet, for example.\r\n\r\nHow about starting with 'from_mmdetection'? The panoptic segmentation prediction results in MMDetection include `pred_panoptic_seg` along with `pred_instances`, which is already supported. Huggingface's Transformers have various output format, so it might be difficult to handle them at once. \r\n\r\n> If I understand correctly, we'll need 2 types of `id` values - one for class and one for instance. If so, you may use `Detections.class_id` and create `PANOPTIC_INSTANCE_ID_DATA_FIELD` in `supervision/config.py` (it can be used in the `Detections.data` field).\r\n\r\nPanoptic segmentation models return both semantic and instance segmentation results. Therefore, we need to store a numpy array that contains class information for each pixel in an image.\r\n\r\nMy concern is that `Detections.data` needs to be iterable so that it has the same length as `Detections.xyxy` or `Detections.mask`, as far as I understand the code. If so, this could be problematic because we only need to store one numpy array for the entire image. If not, we can implement it by storing the semantic segmentation in `Detections.data` as shown below.\r\n\r\n```Python\r\n@classmethod\r\ndef from_mmdetection(cls, mmdet_results) -> Detections:\r\n    if hasattr(mmdet_results, 'pred_panoptic_seg'):\r\n        return cls(\r\n            xyxy=mmdet_results.pred_instances.bboxes.cpu().numpy(),\r\n            confidence=mmdet_results.pred_instances.scores.cpu().numpy(),\r\n            class_id=mmdet_results.pred_instances.labels.cpu().numpy().astype(int),\r\n            mask=mmdet_results.pred_instances.masks.cpu().numpy()\r\n            if \"masks\" in mmdet_results.pred_instances\r\n            else None,\r\n            data={'segmentation_map': mmdet_results.pred_panoptic_seg.sem_seg.cpu().numpy()}\r\n        )\r\n    else:\r\n        return cls(\r\n            xyxy=mmdet_results.pred_instances.bboxes.cpu().numpy(),\r\n            confidence=mmdet_results.pred_instances.scores.cpu().numpy(),\r\n            class_id=mmdet_results.pred_instances.labels.cpu().numpy().astype(int),\r\n            mask=mmdet_results.pred_instances.masks.cpu().numpy()\r\n            if \"masks\" in mmdet_results.pred_instances\r\n            else None,\r\n        )\r\n```\r\n\r\n"
      },
      {
        "user": "LinasKo",
        "body": "Assigning transformers panoptic implementation to @onuralpszr."
      }
    ]
  },
  {
    "issue_number": 1325,
    "title": "comment utilser les resultas de ce model hors connectiom",
    "author": "Tkbg237",
    "state": "closed",
    "created_at": "2024-07-04T03:04:03Z",
    "updated_at": "2024-07-05T11:41:13Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nj'ai projet sur le quel je travail qui consiste a calculer les dimensions reelles en mm d'un oeuf a partir de son image et je voulais savoir comment integrer ce model a mon projet et comment utiliser ses resultats pour avoir les dimensions en pixels de l'oeuf sur l'image\r\n\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1274,
    "title": "[KeyPoints] - extend `from_mediapipe` with Google MediaPipe FaceMesh",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-06-11T11:27:49Z",
    "updated_at": "2024-07-05T09:56:51Z",
    "labels": [
      "enhancement",
      "api:keypoints"
    ],
    "body": "# Description\r\n\r\nMuch like #1174 and #1232 adding pose landmark support, we'd also like to add face detection support to the `from_mediapipe` method.\r\n\r\n* Add `Skeleton.FACEMESH_TESSELETION` of size `468` to the [Skeleton](https://github.com/roboflow/supervision/blob/447ef41fc45353130ec4dccdc7eeaf68b622fb7e/supervision/keypoint/skeletons.py#L7) enum.\r\n  * The nodes can be found here: https://github.com/google-ai-edge/mediapipe/blob/8cb99f934073572ce73912bb402a94f1875e420a/mediapipe/python/solutions/face_mesh_connections.py#L74\r\n  * Docs can be found here: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md\r\n* Add the code to the `from_mediapipe` function in [`KeyPoints`](https://github.com/roboflow/supervision/blob/447ef41fc45353130ec4dccdc7eeaf68b622fb7e/supervision/keypoint/core.py#L16) object that is introduced in #1232.\r\n* We'd like to support responses from both legacy and modern way to call the face mesher - see links below.\r\n\r\n![facemesh](https://github.com/roboflow/supervision/assets/6500785/59433e66-74e0-448c-b902-4f19947d379e)\r\n\r\n# Links:\r\n- Google Mediapipe repository: https://github.com/google/mediapipe\r\n- Google Mediapipe face landmarker: https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker\r\n  - Python Guide (Modern): https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/python\r\n  - Legacy: https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb\r\n- Skeletons: https://github.com/google-ai-edge/mediapipe/blob/8cb99f934073572ce73912bb402a94f1875e420a/mediapipe/python/solutions/face_mesh_connections.py#L74\r\n\r\n# Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hey @David-rn,\r\n\r\nI saw your comment regarding the lazy loader / high memory usage issue. I need to scope out that one first, but if you're keen to contribute and enjoyed the work on MediaPipe, this one is up for grabs üòâ "
      },
      {
        "user": "onuralpszr",
        "body": "@LinasKo  if not I worked face mesh before  so I really want to be involved too "
      },
      {
        "user": "LinasKo",
        "body": "Hi @onuralpszr,\r\n\r\nSure, we can reserve it for you - I recall you mentioning it before üôÇ \r\n\r\nHowever, we can afford to a wait a little bit, so I'd prefer to close the sinks and maybe the water meter cookboks first (I have some feedback to send about the latter)."
      }
    ]
  },
  {
    "issue_number": 990,
    "title": "[RichLabelAnnotator] - add support for unicode labels ",
    "author": "Ying-Kang",
    "state": "closed",
    "created_at": "2024-03-12T06:38:31Z",
    "updated_at": "2024-07-03T11:39:11Z",
    "labels": [
      "enhancement",
      "api:annotator",
      "Q2.2024"
    ],
    "body": "### Description\r\n\r\n[`LabelAnnotator`](https://github.com/roboflow/supervision/blob/b68e7c2059f1da9eee8c3cdc66f50f3898fcb6ba/supervision/annotators/core.py#L902) uses OpenCV as a rendering engine. Unfortunately, `cv2.putText`, which we use underneath, only supports ASCII characters. A solution to this problem would be the implementation of a new annotator that adds support for Unicode characters based on Pillow. \r\n\r\nWe previously considered a similar idea, but we decided not to implement it due to time constraints. https://github.com/roboflow/supervision/pull/159\r\n\r\n### API\r\n\r\n```python\r\nclass RichLabelAnnotator:\r\n\r\n    def __init__(\r\n        self,\r\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\r\n        text_color: Color = Color.WHITE,\r\n        font_path: str = \"path/to/default/font.ttf\",\r\n        font_size: int = 10,\r\n        text_padding: int = 10,\r\n        text_position: Position = Position.TOP_LEFT,\r\n        color_lookup: ColorLookup = ColorLookup.CLASS,\r\n    ):\r\n        pass\r\n\r\n    def annotate(\r\n        self,\r\n        scene: ImageType,\r\n        detections: Detections,\r\n        labels: List[str] = None,\r\n        custom_color_lookup: Optional[np.ndarray] = None,\r\n    ) -> ImageType:\r\n       pass\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @Ying-Kang üëãüèª, thanks for your interest in supervision. \r\n\r\nWe initially wanted to add support for Unicode labels, but we ultimately became overwhelmed by the work involved in working around other annotators and decided to limit the scope.\r\n\r\nI think it would be a good idea to revisit this idea now. We already have the framework ready we would just have to add this new functionality. \r\n\r\n"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @Ying-Kang üëãüèª I'll keep it open if that's okey with you. "
      },
      {
        "user": "jeslinpjames",
        "body": "Hi üëã, I'd like to try working on this.\r\n"
      }
    ]
  },
  {
    "issue_number": 1318,
    "title": "Dwell Time Analysis with Computer Vision",
    "author": "Geniuspond",
    "state": "open",
    "created_at": "2024-07-01T12:00:31Z",
    "updated_at": "2024-07-01T12:00:31Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi;\r\n\r\nI installed[Dwell Time Analysis with Computer Vision | Real-Time Stream Processing](https://youtu.be/hAWpsIuem10) ( on my raspberry pi 5 64 bit system and it works. However, when I analyze the video, I always see photos in jpeg format. Where can I view the video after it has been fully analyzed?\r\n\r\nthank you\r\n\n\n### Additional\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1301,
    "title": "overlay_image() function",
    "author": "Bhavay-2001",
    "state": "closed",
    "created_at": "2024-06-21T17:32:20Z",
    "updated_at": "2024-07-01T11:32:32Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nThe `overlay_image` function used in the `CropAnnotator` class [here](https://github.com/roboflow/supervision/blob/f00dcede1a2e0e3bca0f403ed4a0d23e16cde937/supervision/annotators/core.py#L2105) has different arguments as compared to the parameters in the `overlay_image` function defined over [here](https://github.com/roboflow/supervision/blob/f00dcede1a2e0e3bca0f403ed4a0d23e16cde937/supervision/utils/image.py#L293)\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n```python\r\nimport supervision as sv\r\nfrom inference import get_model\r\nimport cv2\r\n\r\nmodel = get_model(model_id=\"yolov8n-seg-640\")\r\nimage = cv2.imread('/content/highway.jpg')\r\nresults = model.infer(image)[0]\r\ndetections = sv.Detections.from_inference(results)\r\n\r\ncrop_annotator = sv.CropAnnotator()\r\nannotated_frame = crop_annotator.annotate(\r\n    scene=image.copy(),\r\n    detections=detections\r\n)\r\ncv2.imshow(annotated_frame)\r\n``` \n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Confirming the bug: https://colab.research.google.com/drive/1liqwH6p_rPbNrV2D1jPPe8pOmoA0DCME?usp=sharing\r\n\r\n@Bhavay-2001, if you're still keen, feel free to open a PR with the fix!"
      }
    ]
  },
  {
    "issue_number": 291,
    "title": "[DetectionDataset] - migrate field from `List[str]` to `Dict[int, str]`",
    "author": "SkalskiP",
    "state": "open",
    "created_at": "2023-08-16T13:23:46Z",
    "updated_at": "2024-06-30T10:57:12Z",
    "labels": [
      "enhancement",
      "api:datasets",
      "Q1.2024"
    ],
    "body": "### Description\r\n\r\nMigrate datasets [`sv.ClassificationDataset.classes`](https://supervision.roboflow.com/datasets/#classificationdataset) and [`sv.DetectionDataset.classes`](https://supervision.roboflow.com/datasets/#detectiondataset) field from `List[str]` to `Dict[int, str]`. Necessary changes should be made in all `.from_*` and `.as_*` methods in both `sv.ClassificationDataset` and `sv.DetectionDataset`.\r\n\r\n### Use case\r\n\r\nUsing `list` to store `classes` forces us to use consecutive and starting from 0 integers. In fact, formats such as COCO or PASCAL VOC and, more recently, YOLO use `int: string` relationships to store classes. Our dependence on `classes` in the form of `list` forces us to unnecessarily map `class_id` while loading the dataset.\r\n\r\n### Testing\r\n\r\nMake sure to create Google Colab where you install your version of Supervision and showcase a demo of the changes you made. ",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "@capjamesg, take a look at this issue. I'm not sure if this API change will somehow affect `autodistill`."
      },
      {
        "user": "hardikdava",
        "body": "Related issues to keep in mind:\r\n\r\n#150 \r\n#285 "
      },
      {
        "user": "hardikdava",
        "body": "@SkalskiP , can you handle this one? Some other things came up which needs my attention. "
      }
    ]
  },
  {
    "issue_number": 269,
    "title": "map plots and prints",
    "author": "hardikdava",
    "state": "open",
    "created_at": "2023-08-07T08:56:47Z",
    "updated_at": "2024-06-28T18:31:02Z",
    "labels": [
      "enhancement",
      "api:metrics"
    ],
    "body": "### Description\r\n\r\n`sv.MeanAveragePrecision` does not support for plots and pretty printing. This could be improved by extending exisiting method which allows to save plots and API should be similar to `sv.ConfusionMatrix`.",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "@hardikdava, awesome! üî• I initially have it marked as `supervision-0.14.0` release."
      },
      {
        "user": "AHB102",
        "body": "@hardikdava Are you still working on this ?"
      },
      {
        "user": "hardikdava",
        "body": "@AHB102 I am not working on it. Feel free to go ahead. "
      }
    ]
  },
  {
    "issue_number": 1309,
    "title": "Some slices take longer to infer with Slicer",
    "author": "guelermus",
    "state": "closed",
    "created_at": "2024-06-26T12:28:10Z",
    "updated_at": "2024-06-27T12:13:19Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi supervisioners,\r\n\r\nThis my first use of SAHI, and I started with Supervision Slicer.\r\nI use an image on the standard callback example of Slicer with default parameters. \r\nFrom around 100 sclices on this image, some %5-6 of the slices take longer to complete:\r\n...\r\n0: 544x640 1 person, 1 boat, 3.8ms\r\nSpeed: 1.1ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 544, 640)\r\n0: 640x320 (no detections), 51.8ms\r\nSpeed: 0.5ms preprocess, 51.8ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 320)\r\n0: 32x640 (no detections), 53.5ms\r\nSpeed: 0.2ms preprocess, 53.5ms inference, 0.1ms postprocess per image at shape (1, 3, 32, 640)\r\n0: 32x640 (no detections), 2.9ms\r\nSpeed: 0.2ms preprocess, 2.9ms inference, 0.4ms postprocess per image at shape (1, 3, 32, 640)\r\n...\r\nThose are from Yolov10n. If I change to Yolov10m,\r\nsome slices took again around +50ms longer than others.\r\n\r\nI have an NVIDIA RTX A2000 12GB, so it is not a fast one.\r\nIs these sileces with longer times about the GPU capabilities? \r\nDo you recommend a different parametrization for Slicer?\r\n\r\nBest,\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @guelermus üëã \r\n\r\nTo be frank, we haven't analysed it at this level. Looking at your output, this may be happening in response to changing slice sizes. For example, the last two responses send slices of the same size, and the latter one completes much sooner.\r\n\r\nIf that's true, computing the exact pixel values for a perfect slice and setting the `slice_wh` and `overlap_ratio_wh` very precisely would solve this. However, it's more effort than what we'd expect users to need.\r\n\r\nWe may need to revisit the slicing algorithm in the next few weeks."
      },
      {
        "user": "guelermus",
        "body": "Thank you for the answer.\r\n\r\nThe image size is 3200 x 1800 here.\r\nI attach the full slicer log for archiving purposes:\r\n[slicer.log](https://github.com/user-attachments/files/16009975/slicer.log)\r\n"
      },
      {
        "user": "LinasKo",
        "body": "Thank you!\r\n\r\nI'll try to leave a comment here if we end up speeding it up."
      }
    ]
  },
  {
    "issue_number": 1293,
    "title": "Help in gathering commonly used supervision functions.",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-06-19T08:02:12Z",
    "updated_at": "2024-06-26T10:39:04Z",
    "labels": [
      "help wanted"
    ],
    "body": "We will be releasing a supervision cheatsheet soon - something of a similar style as https://github.com/a-anjos/python-opencv/blob/master/cv2cheatsheet.pdf\r\n\r\nFirst, we need to gather a list of code snippets in a Colab. We'll then go over it and copy out a subset of the functions into the PDF.\r\n\r\nPartial submissions are fine - every little bit helps!\r\n---\r\n\r\nThe sections in the Colab I anticipate are as follows:\r\n\r\n1. `# Supervision Basics` - pip install supervision and inference, load an imagerun the model (`\"yolov8s-640\"`) and `from_inference`, visualize result. This section also has `Detections.empty`, `is_empty` and `Detections.merge`.\r\n\r\nMore importantly this should have frequently searched behaviour such as selecting detections with one class. I'll add this myself later.\r\n\r\nLet's include `with_nmm` and `with_nms` too. \r\n\r\n> https://supervision.roboflow.com/develop/#hello\r\n> https://supervision.roboflow.com/develop/how_to/detect_and_annotate/\r\n> https://supervision.roboflow.com/develop/detection/core/\r\n\r\n2. `# supervision assets` - an example of loading from https://supervision.roboflow.com/assets/\r\n\r\n3. `# Loading model results` - examples of installing dependencies and running every model mentioned [here](https://supervision.roboflow.com/latest/detection/core/), showing every `from_X` method at work.\r\n\r\n> https://supervision.roboflow.com/develop/detection/core/\r\n\r\n3. `Annotators` - Create and run every annotator on a detections object, show the result.\r\n\r\n> https://supervision.roboflow.com/develop/detection/annotators/\r\n\r\n4. `# KeyPoints` - similar sections of the Basics, from_X, annotators\r\n\r\n> https://supervision.roboflow.com/latest/keypoint/core/\r\n> https://supervision.roboflow.com/latest/keypoint/annotators/\r\n\r\n---\r\n\r\nIf you start creating a Colab, please post a link on this issue.\r\n\r\nAfter you're done, please set it to \"anyone can edit\" - it will be useful in the next few days.",
    "comments": [
      {
        "user": "LinasKo",
        "body": "@Bhavay-2001, does this sound interesting? I believe it's a good opportunity to learn more about the library, and we need this urgently. We expect to release the cheatsheet by the end of this week.\r\n\r\nRealistically, this needs help with today, but the contribution doesn't have to be extensive - every little bit helps üôÇ "
      },
      {
        "user": "Bhavay-2001",
        "body": "Hi @LinasKo, I have created a [colab](https://colab.research.google.com/drive/1YNPqO5aE5iAu9Q9KtmP2NyzYdgTb43m8?usp=sharing) notebook. I have started with some examples and will add more in the coming days. Please have a look and let me know about that changes."
      },
      {
        "user": "LinasKo",
        "body": "Hi @Bhavay-2001,\r\n\r\nThank you; that's already very useful!\r\n\r\nFeel free to just copy things from our docs - in many cases there are examples.\r\nhttps://supervision.roboflow.com/latest/detection/core/\r\n\r\nBonus points for including many different use cases - a bit from every section"
      }
    ]
  },
  {
    "issue_number": 781,
    "title": "[InferenceSlicer] - allow batch size inference",
    "author": "inakierregueab",
    "state": "open",
    "created_at": "2024-01-25T21:09:02Z",
    "updated_at": "2024-06-20T19:43:54Z",
    "labels": [
      "enhancement",
      "Q2.2024"
    ],
    "body": "### Description\n\nCurrently, [`sv.InferenceSlicer`](https://supervision.roboflow.com/detection/tools/inference_slicer/) processes each slice in a separate callback call - hindering inference with a batch size larger than 1. We can change this by:\n\n- **Batching Slices:** Instead of submitting individual tasks for each slice, group slices into batches. `batch_size` can be a new parameter for the `InferenceSlicer` class.\n- **Modifying the Callback:** Ensure the callback function can handle a batch of slices instead of a single slice. Changing the [callback](https://github.com/roboflow/supervision/blob/3024ddca83ad837651e59d040e2a5ac5b2b4f00f/supervision/detection/tools/inference_slicer.py#L52) signature from `callback: Callable[[np.ndarray], Detections]` to callback: `Callable[[List[np.ndarray]], List[Detections]]`.\n- **Collecting and Merging Results:** After processing, you must appropriately collect and merge the results from the batches.\n\n### Additional\n\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will definitely speed up the review process. Each change must be tested by the reviewer. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi, @inakierregueab üëãüèª That is something we were considering but didn't implement due to time restrictions. Let me add some details to this issue. Maybe someone will pick it up."
      },
      {
        "user": "Bhavay-2001",
        "body": "Hi @SkalskiP, can I work on this issue if it is for beginners? \r\nThanks"
      },
      {
        "user": "SkalskiP",
        "body": "Hi, @Bhavay-2001 üëãüèª Do you already have experience with running model inference at different batch sizes? "
      }
    ]
  },
  {
    "issue_number": 1285,
    "title": "Modified Triangle Annotator",
    "author": "dsaha21",
    "state": "closed",
    "created_at": "2024-06-16T16:49:00Z",
    "updated_at": "2024-06-20T18:49:24Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nUpgradation of the triangle above head annotators more prominently\r\n\r\n![Modified__traiangle_2](https://github.com/roboflow/supervision/assets/73748916/cb7e9cb7-a540-4f43-bb67-1e38d89a5175)\r\n![Modified_Triangle_1](https://github.com/roboflow/supervision/assets/73748916/c6bcf82f-1edc-4c0b-94a8-bc560095b438)\r\n\r\n\n\n### Use case\n\nThe original Triangle Annotator is attached below :-\r\n![Original](https://github.com/roboflow/supervision/assets/73748916/e4c1f08d-7bba-49c0-93a4-4d69596d5200)\r\n\r\n\r\nThe modified one might help us to view for a more clear detection.\n\n### Additional\n\nMy request is to please analyse the modified annotator. If its pleasing and if possible please accept for a PR. Thank You\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @dsaha21 üëã \r\n\r\nI like how this looks! Feel free to open a PR for it. Please also include a Colab we can test it in.\r\n\r\nLet's make sure the old behaviour is the default, (e.g. if `outline_thickness=0` then old triangles are shown).\r\n\r\nAre the any [other annotators](https://supervision.roboflow.com/annotators/) we should add this to? "
      },
      {
        "user": "dsaha21",
        "body": "Hi @LinasKo, \r\nThanks for the reply. So I have made changes which are as follows :-\r\n\r\n* If user wants the old triangle, then by default I have kept the outer thickness = 0\r\n* Otherwise, according to the user's problem use case, they can enter any integer \r\n\r\nIncluding my colab [notebook](https://colab.research.google.com/drive/1vHUSwxYtMz_wbp7IH18kFlZBD1UZdeYV?usp=sharing) .\r\nPlease check and let me know if I can proceed forward with a PR. Thanks"
      },
      {
        "user": "LinasKo",
        "body": "This looks great. Let's make a PR :)\r\n\r\nOne change I'd like to request is to check if this can also be added to sv.DotAnnotator, as it can be just as hard to see: [Colab](https://colab.research.google.com/drive/1NvVs28MnSb-_W-zkFa6-FXiY-0G4nx7k#scrollTo=VjdCk7cMuhh2)."
      }
    ]
  },
  {
    "issue_number": 1096,
    "title": "[DetectionDataset] - expand `from_yolo` to include support for OBB (Oriented Bounding Boxes)",
    "author": "pedbrgs",
    "state": "closed",
    "created_at": "2024-04-05T13:02:11Z",
    "updated_at": "2024-06-19T09:10:24Z",
    "labels": [
      "enhancement",
      "help wanted",
      "api:datasets",
      "Q2.2024"
    ],
    "body": "### Description\r\n\r\nIn [supervision-0.18.0](https://github.com/roboflow/supervision/releases/tag/0.18.0), we added initial support for OBB; it's time to extend it to include dataset loading.\r\n\r\nMake the necessary changes in [sv.DetectionDataset.from_yolo](https://supervision.roboflow.com/0.19.0/datasets/#supervision.dataset.core.DetectionDataset.from_yolo) to enable loading OBB datasets from disk in YOLO format. [Here](https://docs.ultralytics.com/datasets/obb/) you can read more about the YOLO OBB Format. In short, each line of the `.txt` file should have the following format.\r\n\r\n```\r\nclass_index, x1, y1, x2, y2, x3, y3, x4, y4\r\n```\r\n\r\nThe [sv.OrientedBoxAnnotator](https://supervision.roboflow.com/0.19.0/annotators/#supervision.annotators.core.OrientedBoxAnnotator.annotate) expects information about oriented bounding boxes to be stored in the `xyxyxyxy` field of [`sv.Detections.data`](https://github.com/roboflow/supervision/blob/4729e20a9408fbe08d342cc4dbae835d808686a5/supervision/detection/core.py#L88). Ensure that the information loaded from the dataset is stored there.\r\n\r\n### API\r\n\r\nHere's an example of how to use the new API. Roboflow allows for the export of segmentation datasets as OBB. Let's ensure that our support for OBB definitely works with datasets exported from Roboflow.\r\n\r\n```python\r\nimport random\r\nimport roboflow\r\nfrom roboflow import Roboflow\r\nimport supervision as sv\r\n\r\nroboflow.login()\r\nrf = Roboflow()\r\n\r\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"fashion-assistant\")\r\nversion = project.version(3)\r\ndataset = version.download(\"yolov8-obb\")\r\n\r\ntrain_ds = sv.DetectionDataset.from_yolo(\r\n    images_directory_path=f\"{dataset.location}/train/images\",\r\n    annotations_directory_path=f\"{dataset.location}/train/labels\",\r\n    data_yaml_path=f\"{dataset.location}/data.yaml\"\r\n)\r\n\r\nimage_name = random.choice(list(train_ds.images))\r\nimage = train_data.images[image_name]\r\ndetections = train_data.annotations[image_name]\r\n\r\noriented_box_annotator = sv.OrientedBoxAnnotator()\r\nannotated_frame = oriented_box_annotator.annotate(\r\n    scene=image.copy(),\r\n    detections=detections\r\n)\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi, @pedbrgs üëãüèª Thanks a lot for your interest in Supervision.\r\n\r\nThat's because, at the moment, [`DetectionDataset.from_yolo`](https://supervision.roboflow.com/latest/datasets/#supervision.dataset.core.DetectionDataset.from_yolo) does not support OBB (Oriented Bounding Boxes). \r\n\r\n\r\nIt would be a good idea to convert your question into a feature request and add support for OBB. LEt's do it! "
      },
      {
        "user": "pedbrgs",
        "body": "@SkalskiP Thanks for considering this! It will be great to have this feature."
      },
      {
        "user": "SkalskiP",
        "body": "Hi, @pedbrgs üëãüèª Fingers crossed, someone from the community. will pick it up. "
      }
    ]
  },
  {
    "issue_number": 958,
    "title": "Load OBB Detection, ideally from a Dictionary/Json",
    "author": "DaleBestOrbexLabs",
    "state": "closed",
    "created_at": "2024-02-29T03:08:03Z",
    "updated_at": "2024-06-19T07:41:21Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nTo be able to create a Detection object to then run through Byte Track from the OBB information that is stored in a Dictionary/Json.\n\n### Use case\n\nIdeally this would allow to run the OBB prediction separately and save the information to a JSON file and then later parse the JSON file to get the prediction results and run it frame by frame through Byte Track.\r\n\r\nThis way would allow much faster revision and tuning on the tracker variables as it wouldn't require re-running the costly inference over and over again to find good fits.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @DaleBestOrbexLabs üëãüèª Thanks for your interest in supervision. `supervision-0.19.0` will add support for saving detections to [CSV](https://supervision.roboflow.com/develop/detection/tools/save_detections/) and [JSON](https://github.com/roboflow/supervision/pull/819) file. \r\n\r\nIf I understood you correctly, you would like us to also add a function for loading these detections?"
      },
      {
        "user": "DaleBestOrbexLabs",
        "body": "Hi @SkalskiP, yup pretty much a Detections.from_json()/Detections.from_dict() function."
      },
      {
        "user": "SkalskiP",
        "body": "Hi @DaleBestOrbexLabs, I think it's a good idea, but we'll proceed with it only if it integrates seamlessly with the `CSVSink` and `JSONSink` API."
      }
    ]
  },
  {
    "issue_number": 1283,
    "title": "DotAnnotator support radius proportional to object size",
    "author": "zhaoruibing",
    "state": "closed",
    "created_at": "2024-06-16T00:17:37Z",
    "updated_at": "2024-06-17T09:17:10Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nCurrently DotAnnotator takes a fixed radius. It would be great if the radii can be relative to the object sizes.\r\n\n\n### Use case\n\nFor rebar counting, the size of the rebar in the photo depends on the proximity of the camera, meaning the dots' radii should vary for each image.\r\n\r\nFor airplanes or insects, objects closer to the camera will appear larger, while those further away will look smaller within the same image. Therefore, the radii should adjust for each object.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @zhaoruibing üëã \r\n\r\nIf someone wishes to see the size, they can use `sv.CircleAnnotator`, with custom thickness.\r\n![image](https://github.com/roboflow/supervision/assets/6500785/cac9c81a-4039-48eb-aad7-a096ab9a0043)\r\n\r\nI'm closing this for now but adding it to a list of less urgent ideas to be investigated in the future."
      }
    ]
  },
  {
    "issue_number": 1215,
    "title": "Problems with tracker.update_with_detections(detections)",
    "author": "CodingMechineer",
    "state": "open",
    "created_at": "2024-05-21T12:45:53Z",
    "updated_at": "2024-06-16T15:12:54Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nSomehow, I loose predicted bounding boxes in this line:\r\n\r\n`tracker.update_with_detections(detections)`\r\n\r\nIn the plot from Ultralytics, everything is fine. Though, after the line above gets executed, I loose some bounding boxes. In this example, I loose two.\r\n\r\nThat's the plot from Ultralytics, how it should be:\r\n![image](https://github.com/roboflow/supervision/assets/149016088/b715953b-d6af-4d0a-85ef-a18a5479b85e)\r\n\r\nThat's the plot after the Roboflow labling, some predictions are missing:\r\n![image](https://github.com/roboflow/supervision/assets/149016088/e025214a-b3b4-46bf-ab80-e05d9ea1cbe1)\r\n\r\nCan somebody help me with this issue?\r\n\r\n\r\n### Environment\r\n\r\n- Supervision 0.20.0\r\n- Python 3.12.3\r\n- Ultralytics 8.2.18\r\n\r\n### Minimal Reproducible Example\r\n\r\n\r\n### Code:\r\n```\r\nimport cv2\r\nimport supervision as sv\r\nfrom ultralytics import YOLO\r\n\r\nmodel_path = \"path/to/your/model.pt\"\r\nvideo_path = \"path/to/your/video.mp4\"\r\n\r\ncap = cv2.VideoCapture(video_path)\r\nmodel = YOLO(model_path)\r\nbox_annotator = sv.BoundingBoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator()\r\ntracker = sv.ByteTrack()\r\n\r\nwhile True:\r\n    ret, frame = cap.read()\r\n    \r\n    results = model(frame, verbose=False)[0]\r\n    print(f\"CLS_YOLO-model: {results.boxes.cls}\")\r\n    \r\n    results_2 = model.predict(frame,     \r\n                        show=True, # The plot from the Ultralytics library\r\n                        conf = 0.5,\r\n                        save = False,\r\n                        )\r\n    \r\n    detections = sv.Detections.from_ultralytics(results)\r\n    print(f\"ClassID_Supervision_1: {detections.class_id}\") # Between this and the next print, predictions are lost\r\n    \r\n    detections = tracker.update_with_detections(detections) # The detections get lost here\r\n    \r\n    labels = [\r\n        f\"{results.names[class_id]} {confidence:0.2f}\"\r\n        for confidence, class_id\r\n        in zip(detections.confidence, detections.class_id)\r\n    ]\r\n    \r\n    print(f\"ClassID_Supervision_2: {detections.class_id}\") # Here two predictions from the Ultralytics model are lost\r\n    \r\n    annotated_frame = frame.copy()\r\n    \r\n    annotated_frame = box_annotator.annotate(\r\n        annotated_frame,\r\n        detections\r\n        ) \r\n    \r\n    labeled_frame = label_annotator.annotate(\r\n        annotated_frame,\r\n        detections,\r\n        labels\r\n        )\r\n    \r\n    print(f\"ClassID_Supervision_3: {detections.class_id}\")\r\n    print(f\"{len(detections)} detections, Labels: {labels}\", )\r\n        \r\n    cv2.imshow('Predictions', labeled_frame) # The with Roboflow generated frame\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n```\r\n\r\n### Prints in console:\r\nCLS_YOLO-model: tensor([1., 1., 1., 1.], device='cuda:0') **--> Class ID's from the predicted bounding boxes**\r\nClassID_Supervision_1: [1 1 1 1] **--> Converted into Supervision**\r\nClassID_Supervision_2: [1 1] **--> After the tracker method class ID's are lost**\r\nClassID_Supervision_3: [1 1]\r\n2 detections, Labels: ['Spot 0.87', 'Spot 0.86']\r\n\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @CodingMechineer üëã \r\n\r\nLet's do one quick test - does installing `supervision==0.21.0.rc5` change anything?"
      },
      {
        "user": "SkalskiP",
        "body": "@CodingMechineer, we accidentally shipped a tracking bug in `supervision==0.20.0`. Try using `supervision==0.19.0` or `supervision==0.21.0.rc5` pre-release. "
      },
      {
        "user": "CodingMechineer",
        "body": "@LinasKo @SkalskiP I installed version `supervision==0.21.0.rc5` and `supervision==0.19.0`. Though with both versions I have the same problem.\r\n\r\nTop: YOLO predictions\r\nBottom: Supervision tracker\r\n![Screenshot 2024-05-21 153420](https://github.com/roboflow/supervision/assets/149016088/ea7f4e72-6c1f-42da-9b49-5dac872b80a3)\r\n"
      }
    ]
  },
  {
    "issue_number": 458,
    "title": "Cost Matrix contains NaN in ByteTrack() code ",
    "author": "tobieabel",
    "state": "closed",
    "created_at": "2023-10-12T10:34:31Z",
    "updated_at": "2024-06-14T13:13:12Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nI got the following error message when using supervision ByteTrack().  Something in the image is causing a NaN value to appear in the cost matrix in matching.py.  I have got around the issue for now by inserting the below line of code in bold to remove the NaN value from the matrix in line 32 of matching.py:\r\n\r\n    cost_matrix[cost_matrix > thresh] = thresh +  1e-4\r\n    **cost_matrix = np.nan_to_num(cost_matrix)**\r\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\r\n    indices = np.column_stack((row_ind, col_ind))\r\n\r\nconsole output:\r\n\r\n/Users/tobieabel/PycharmProjects/Demo_4_Fight_and_Fall/venv/lib/python3.11/site-packages/supervision/detection/utils.py:56: RuntimeWarning: invalid value encountered in divide\r\n  return area_inter / (area_true[:, None] + area_detection - area_inter)\r\nTraceback (most recent call last):\r\n  File \"/Users/tobieabel/PycharmProjects/Demo_4_Fight_and_Fall/Main.py\", line 192, in <module>\r\n    start()\r\n  File \"/Users/tobieabel/PycharmProjects/Demo_4_Fight_and_Fall/Main.py\", line 171, in start\r\n    obj_detections, class_results, Incident = start.Run_Models(frame)\r\n                                              ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tobieabel/PycharmProjects/Demo_4_Fight_and_Fall/Main.py\", line 76, in Run_Models\r\n    obj_detections = self.tracker.update_with_detections(obj_detections)  # pass the detections through the tracker to add tracker ID as additional field to detections object\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tobieabel/PycharmProjects/Demo_4_Fight_and_Fall/venv/lib/python3.11/site-packages/supervision/tracker/byte_tracker/core.py\", line 235, in update_with_detections\r\n    tracks = self.update_with_tensors(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tobieabel/PycharmProjects/Demo_4_Fight_and_Fall/venv/lib/python3.11/site-packages/supervision/tracker/byte_tracker/core.py\", line 367, in update_with_tensors\r\n    matches, u_unconfirmed, u_detection = matching.linear_assignment(\r\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tobieabel/PycharmProjects/Demo_4_Fight_and_Fall/venv/lib/python3.11/site-packages/supervision/tracker/byte_tracker/matching.py\", line 33, in linear_assignment\r\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: matrix contains invalid numeric entries\r\n\n\n### Environment\n\n - Supervision v0.15.0\r\n - Python 3.11\n\n### Minimal Reproducible Example\n\nI'm using the following code to run YOLO8 object detector through supervision to plot the tracking on the annotated frame\r\n\r\ndef Run_Models(self,frame:np.ndarray) ->sv.detection:\r\n        obj_det_result = self.object_det_model.predict(frame, device = \"mps\", verbose = False, conf=0.4,iou=0.7)[0]#send every frame to YOLO model\r\n        obj_detections = sv.Detections.from_ultralytics(obj_det_result)  # pass the results from the model to the sv libary as they are easier to manipulate\r\n        obj_detections= obj_detections[obj_detections.class_id == 0]  # filter the list of detections so it only shows category '0' which is people\r\n        obj_detections = self.tracker.update_with_detections(obj_detections)  # pass the detections through the tracker to add tracker ID as additional field to detections object\r\n\r\n\r\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "SkalskiP",
        "body": "Hi @tobieabel üëãüèª Thanks for reporting the issue. Is it possible for you to provide us with a video example that we could use for debugging? I mean the video that is causing that issue for you. "
      },
      {
        "user": "tobieabel",
        "body": "Sorry for the late reply, the video is attached, but I think the issue may have been caused by the object detector I was using at the time, which was the standard YOLOv8n model.  Once I retrained it to get better detection results the issue seemed to go away.\r\n\r\nhttps://github.com/roboflow/supervision/assets/34671814/8d7cf557-f70c-4f5d-9a0d-867bad690b59\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1281,
    "title": "Machine vision",
    "author": "Romu10",
    "state": "closed",
    "created_at": "2024-06-13T19:07:39Z",
    "updated_at": "2024-06-14T13:12:04Z",
    "labels": [],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 1174,
    "title": "[KeyPoints] - add `from_mediapipe` connector allowing Supervision to be used with Google MediaPipe models ",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-05-06T13:51:45Z",
    "updated_at": "2024-06-13T18:09:57Z",
    "labels": [
      "enhancement",
      "help wanted",
      "api:keypoints"
    ],
    "body": "### Description\r\n\r\n- Add `Skeleton.GHUM` to the [`Skeleton`](https://github.com/roboflow/supervision/blob/447ef41fc45353130ec4dccdc7eeaf68b622fb7e/supervision/keypoint/skeletons.py#L7) enum. This is the skeleton format used by Mediapipe.\r\n- Extend the [`KeyPoints`](https://github.com/roboflow/supervision/blob/447ef41fc45353130ec4dccdc7eeaf68b622fb7e/supervision/keypoint/core.py#L16) object and add the `from_keypoints` connector.\r\n\r\n<img width=\"723\" alt=\"pose_landmarks_index\" src=\"https://github.com/roboflow/supervision/assets/26109316/9a3181d3-8b5a-4dec-b6d2-368ee2a8f14f\">\r\n\r\n### Links\r\n\r\n- Google Mediapipe repository: https://github.com/google/mediapipe\r\n- Google Mediapipe example notebook: https://colab.research.google.com/github/kevinash/awesome-ai/blob/main/notebooks/6_PosesAndAction/Pose_MediaPipe.ipynb\r\n- Google Medipipe documentation: https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Last time I worked on it, Mediapipe provided `visiblity` and not `confidence`. Should the implementer treat this as `confidence` for now?\r\nDocs on: [https://developers.google.com/mediapipe](https://developers.google.com/mediapipe/api/solutions/python/mp/tasks/components/containers/Landmark)\r\n\r\nThere's cases when we have high confidence of invisible points (e.g. left or right shoulder when person is facing sideways).\r\n\r\n(I'm not claiming the issue - just asking for details)"
      },
      {
        "user": "LinasKo",
        "body": "Let's not get bogged down by details. I propose we implement `visibility` as confidence for now and adjust later if required. Other than that, this is one of the easier features - get frames+keypoints from mediapipe, and implement something similar to `from_ultralytics` :slightly_smiling_face:"
      },
      {
        "user": "LinasKo",
        "body": "Hey @David-rn,\r\n\r\nI saw your comment on the RLE ticket. If you wish, this one is up for grabs. Should be pretty straightforward too.\r\n\r\nIs this something you'd be interest in helping out with?"
      }
    ]
  },
  {
    "issue_number": 1269,
    "title": "Trigger when detection lost",
    "author": "wong-kene",
    "state": "closed",
    "created_at": "2024-06-07T05:59:41Z",
    "updated_at": "2024-06-11T12:38:46Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI use supervision for visitor counting inside specific area.\r\nThe whole system was running well except for one area only (let's call it area X).\r\nThe problem is the distance between camera and area X is too far, So that the person is so smaill. Engine still can detect the people but sometimes detection is lost for several second although the person is not move. and suddenly it be detected again\r\n\r\nMy question is.\r\nIs any trigger when the people/object is : \r\n1. lost from detection\r\n2. move out from area\r\n\r\nif both trigger are exist, i will make a different treatment like this :\r\n1. if object lost from detection, i will not decrease the total of people\r\n2. if the person move out from are, i will descrease the total of people\r\n\r\n\r\nOr any solution to cover my problem?\r\n\r\nthanks...\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @wong-kene üëãüèª At present, there is no distinction between the situation when an object crosses the zone boundaries and simply disappears. We plan to add `cross_in` and `cross_our` counts for the polygon zone."
      },
      {
        "user": "wong-kene",
        "body": "thanks for your response @SkalskiP \r\ni have tried to use Dataset for 'small people' over internet. but nothing matched with requirement. \r\nare you or anyone have recommendation dataset for 'small people'?"
      },
      {
        "user": "SkalskiP",
        "body": "@wong-kene, I guess the dataset is not the issue; the people are simply too small for the model to detect. As mentioned, you could detect them by increasing the model input resolution, but then your model would become slower."
      }
    ]
  },
  {
    "issue_number": 1270,
    "title": "Is DetectionDataset intended behavior to store all images?",
    "author": "David-rn",
    "state": "closed",
    "created_at": "2024-06-07T06:43:15Z",
    "updated_at": "2024-06-07T18:23:11Z",
    "labels": [
      "duplicate",
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi! :wave: \r\n\r\nI found that supervision was using >40GB of RAM in my system when using `DectectionDataset` to save a relatively large dataset as_yolo. I realized that the `DectectionDataset` uses a [dictionary to map](https://github.com/roboflow/supervision/blob/7f78f93407a13a9f73ef547af85dbda29f857b08/supervision/dataset/core.py#L62) the image name to the image, which is stored in memory. \r\n\r\nI wanted to ask if this is the expected behavior since it could lead to take all the available RAM when dealing with some datasets. \r\nIf it is not, maybe it could store a reference path to the original image that is only opened when exporting the dataset.\r\n\r\nAs an example of this behavior I used this skeleton script:\r\n```python\r\nfor image_name in list_images:\r\n    image = cv2.imread(join(images_path, image_name))\r\n    detection = get_detection_from_file(gt_file_path)\r\n\r\n    images[image_name] = image\r\n    annotations[image_name] = detection\r\n\r\ndataset = sv.DetectionDataset(\r\n        classes=classes,\r\n        images=images,\r\n        annotations=annotations,\r\n    )\r\ndataset.as_yolo(annotations_directory_path=output_gt_path, images_directory_path=output_images_path)\r\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @David-rn üëã \r\n\r\nYou've hit on something very high in our priorities list.\r\nIn the next `supervision` release we'd like to start implementing a more efficient data loader.\r\n\r\nYou can find some context here if it's something that catches your interest: #316 \r\n\r\n"
      },
      {
        "user": "SkalskiP",
        "body": "@LinasKo and @David-rn, I'm closing this issue and marking it as duplicate."
      },
      {
        "user": "David-rn",
        "body": "Perfect! I didn't find that issue, thanks @LinasKo for the clarification. If you need anything, glad to help!"
      }
    ]
  },
  {
    "issue_number": 678,
    "title": "[InferenceSlicer] - add segmentation models support",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2023-12-15T17:27:54Z",
    "updated_at": "2024-06-06T10:32:29Z",
    "labels": [
      "enhancement",
      "Q2.2024"
    ],
    "body": "### Description\r\n\r\nCurrently, [`sv.InferenceSlicer`](https://supervision.roboflow.com/detection/tools/inference_slicer/) supports only object detection models. Adding support for instance segmentation would require the following changes:\r\n\r\n- The `sv.InferenceSlicer` uses Non-Max Suppression (NMS) to sift out duplicate detections at the tile intersection. At the moment, Supervision only has a box-based [NMS](https://github.com/roboflow/supervision/blob/5b7e0bac5afa1c4aaf47d75fa58d60bf92d032a2/supervision/detection/utils.py#L59). A segmentation-based NMS would be almost ideal, the only change would be to replace the [`box_iou_batch`](https://github.com/roboflow/supervision/blob/5b7e0bac5afa1c4aaf47d75fa58d60bf92d032a2/supervision/detection/utils.py#L28) with a new `mask_iou_batch`.\r\n- A segmentation-based NMS must be plugged into `sv.InferenceSlicer`. At [this](https://github.com/roboflow/supervision/blob/5b7e0bac5afa1c4aaf47d75fa58d60bf92d032a2/supervision/detection/tools/inference_slicer.py#L112) point, we would have to check whether the detections have masks. And if so, use the new NMS.\r\n\r\n### API\r\n\r\n```python\r\n# create\r\ndef mask_iou_batch(boxes_true: np.ndarray, boxes_detection: np.ndarray) -> np.ndarray:\r\n    pass\r\n\r\n# rename non_max_suppression -> box_non_max_suppression\r\n\r\n# create\r\ndef mask_non_max_suppression(predictions: np.ndarray, iou_threshold: float = 0.5) -> np.ndarray:\r\n    pass\r\n\r\n# change InferenceSlicer\r\n```\r\n\r\n### Usage example\r\n\r\n```python\r\nimport cv2\r\nimport supervision as sv\r\nfrom ultralytics import YOLO\r\n\r\nimage = cv2.image = cv2.imread(<SOURCE_IMAGEPATH>)\r\nmodel = YOLO(\"yolov8x-seg.pt\")\r\n\r\ndef callback(image_slice: np.ndarray) -> sv.Detections:\r\n    result = model(image_slice)[0]\r\n    return sv.Detections.from_ultralytics(result)\r\n\r\nslicer = sv.InferenceSlicer(\r\n    callback=callback,\r\n    slice_wh=(512, 512),\r\n    iou_threshold=0.5,\r\n)\r\n\r\ndetections = slicer(image)\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will definitely speed up the review process. Each change must be tested by the reviewer. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "AdonaiVera",
        "body": "Hi @SkalskiP ,\r\n\r\nI've been working on integrating advanced mask handling capabilities into the InferenceSlicer class, \r\nincluding mask_non_max_suppression and mask_iou_batch. As part of these enhancements, I've also \r\nrenamed non_max_suppression to box_non_max_suppression for clarity.\r\n\r\nI've hit a snag while trying to merge detection objects with variable-sized masks in the Detections.merge \r\nfunction. The issue arises because numpy arrays require uniform dimensions for stacking, but our masks, \r\nbeing tied to detected objects, vary in size. This discrepancy causes the stacking operation to fail.\r\n\r\nI'm considering a few approaches to address this, such as resizing, padding, or storing masks individually, \r\nbut each has its trade-offs regarding efficiency, complexity, and fidelity to the original mask shapes.\r\n\r\nI'd appreciate your thoughts on the best path forward. Should we prioritize memory efficiency, ease of \r\nimplementation, or mask integrity? Or is there an alternative solution you'd recommend?\r\n\r\n\r\nThank you for your guidance. üëã \r\nAdo, "
      },
      {
        "user": "SkalskiP",
        "body": "Hi @AdonaiVera üëãüèª We expect the exact dimensions of masks (width and height) to be the same as the source image. This is our approach for now. We are, of course, aware of potential memory and speed optimizations.\r\n\r\nIf your masks are variable-sized, you should pad them and make them all equally sized. \r\n\r\n"
      },
      {
        "user": "AdonaiVera",
        "body": "Yes @SkalskiP, Its true masks match the dimensions of the slice's image. However, the Inference Slicer creates slices of variable sizes, particularly at image boundaries, depending on the slice_wh and the actual dimension of the image, leading to masks that don't all share the same dimensions. This variability creates a challenge when we try to merge these masks using np.vstack, as it requires uniform dimensions.\r\n\r\n```python\r\ndef stack_or_none(name: str):\r\n    if all(d.__getattribute__(name) is None for d in detections_list):\r\n        return None\r\n    if any(d.__getattribute__(name) is None for d in detections_list):\r\n        raise ValueError(\"All or none of the '{}' fields must be None\".format(name))\r\n    return np.vstack([d.__getattribute__(name) for d in detections_list]) if name == \"mask\" else np.hstack([d.__getattribute__(name) for d in detections_list])\r\n\r\nmask = stack_or_none(\"mask\")\r\n``` \r\n\r\nI am considering two potential solutions to address the issue at hand. The first approach is to scale masks to match the largest dimensions and then resize them as needed (As you suggest, but we need to add two resizing steps). The second approach is to store the masks in a different format that can accommodate variable sizes. \r\n\r\nI would appreciate your thoughts on these approaches or any other suggestions you might have. I want to make sure that I fully understand the problem before creating the PR.\r\n\r\nThank you üöÄ "
      }
    ]
  },
  {
    "issue_number": 1219,
    "title": "Loading OBB detection from ultralytics fails",
    "author": "dbestPacksol",
    "state": "closed",
    "created_at": "2024-05-21T16:13:33Z",
    "updated_at": "2024-06-03T07:49:56Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nWhen passing in the results from ultralytics yolov8 OBB model to from_ultralytics it will fail and give the following: \r\n\r\nAttributeError: 'NoneType' object has no attribute 'cls'\r\n\r\ndue to:\r\n\r\nclass_id = ultralytics_results.boxes.cls.cpu().numpy().astype(int)\r\n\r\n \r\n\r\n### Environment\r\n\r\n- Supervision 0.20.0\r\n- Ultralytics 8.2.18\r\n\r\n### Minimal Reproducible Example\r\n\r\n\r\n```python\r\nfrom typing import List\r\n\r\nimport cv2\r\nfrom ultralytics import YOLO\r\nfrom supervision import Detections\r\nfrom ultralytics.engine.results import Results\r\nfrom supervision.tracker.byte_tracker.core import ByteTrack\r\n\r\nyolo = YOLO(\"yolov8n-obb.pt\", task=\"obb\")\r\n\r\ntracker = ByteTrack(track_activation_threshold=0.8,\r\n                    minimum_matching_threshold=0.8,\r\n                    lost_track_buffer=30,\r\n                    frame_rate=30)\r\n\r\nvideo_feed = cv2.VideoCapture(\"test_rgb.mp4\")\r\n\r\nstatus, frame = video_feed.read()\r\n\r\nwhile status:\r\n    results: List[Results] = yolo(frame,\r\n                                  show=False,\r\n                                  verbose=False,\r\n                                  imgsz=[736, 1280])\r\n    result: Results = results[0]\r\n\r\n    inference_detections: Detections = Detections.from_ultralytics(result)\r\n    detections = tracker.update_with_detections(inference_detections)\r\n\r\n    for (xyxy, mask, conf, class_id, tracker_id, data) in detections:\r\n        print(\"xyxy:\", xyxy)\r\n\r\n    status, frame = video_feed.read()\r\n```\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @dbestPacksol üëãüèª Thanks for your interest in `supervision`. Could you install `ultralytics==8.1` and check if the code works or fails? "
      },
      {
        "user": "dbestPacksol",
        "body": "@SkalskiP Even when running on ultralytics==8.1 it still has the same failure"
      },
      {
        "user": "kavanagh-cw",
        "body": "Same problem here and I have tried both version."
      }
    ]
  },
  {
    "issue_number": 1250,
    "title": "Low FPS on video stream",
    "author": "gmijo47",
    "state": "closed",
    "created_at": "2024-06-01T13:20:46Z",
    "updated_at": "2024-06-01T14:37:58Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nHi there,\r\nI'm working on some realtime object detecion and my fps is always 0.3 FPS (both on GPU T4, and CPU), i started form time_in_zone example. On start it spikes, for single second GPU-40FPS, CPU 10 FPS, and as time goes it goes down to 0.3FPS\r\n\r\nrequirments.txt\r\n`supervision>=0.20.0\r\nultralytics==8.2.22\r\nopencv-python==4.8.0.76\r\ninference==0.9.17`\r\n\r\nmy app.py\r\n```\r\nfrom utils.configHandler import ConfigHandler\r\nfrom inference import InferencePipeline\r\nfrom inference.core.interfaces.camera.entities import VideoFrame\r\nfrom ultralytics import YOLO\r\nfrom utils.functions import find_in_list\r\n\r\nimport cv2\r\nimport supervision as sv\r\nfrom typing import List\r\n\r\nCOLORS = sv.ColorPalette.from_hex([\"#E6194B\", \"#3CB44B\", \"#FFE119\", \"#3C76D1\"])\r\nCOLOR_ANNOTATOR = sv.ColorAnnotator(color=COLORS)\r\nLABEL_ANNOTATOR = sv.LabelAnnotator(\r\n    color=COLORS, text_color=sv.Color.from_hex(\"#000000\")\r\n)\r\n\r\nclass DataProcessor:\r\n    def __init__(self, classes: List[int], config):\r\n        self.classes = classes\r\n        self.config = config\r\n        self.tracker = sv.ByteTrack(minimum_matching_threshold=0.8)\r\n        self.fps_monitor = sv.FPSMonitor()\r\n\r\n    def on_prediction(self, detections: sv.Detections, frame: VideoFrame) -> None:\r\n        self.fps_monitor.tick()\r\n        fps = self.fps_monitor.fps\r\n\r\n        detections = detections[find_in_list(detections.class_id, self.classes)]\r\n        detections = self.tracker.update_with_detections(detections)\r\n\r\n        annotated_frame = frame.image.copy()\r\n        match fps:\r\n            case fps if fps <= 5.0:\r\n                bg_color = \"#D20103\"\r\n            case fps if fps <= 12.5:\r\n                bg_color = \"#FFDE59\"\r\n            case _:\r\n                bg_color = \"#7DDA58\"\r\n        annotated_frame = sv.draw_text(\r\n            scene=annotated_frame,\r\n            text=f\"{fps:.2f}\",\r\n            text_anchor=sv.Point(40, 30),\r\n            background_color=sv.Color.from_hex(bg_color),\r\n            text_color=sv.Color.from_hex(\"#000000\"),\r\n        )\r\n\r\n        annotated_frame = COLOR_ANNOTATOR.annotate(\r\n            scene=annotated_frame,\r\n            detections=detections,\r\n        )\r\n        labels = [\r\n            f\"#{tracker_id}\"\r\n            for tracker_id in detections.tracker_id\r\n        ]\r\n        annotated_frame = LABEL_ANNOTATOR.annotate(\r\n            scene=annotated_frame,\r\n            detections=detections,\r\n            labels=labels,\r\n        )              \r\n        print(\"fps\" + str(fps))\r\n        if self.config.debug:\r\n            cv2.imshow(\"Processed Video\", annotated_frame)\r\n            cv2.waitKey(1)\r\n\r\ndef main() -> None:\r\n    config = ConfigHandler()\r\n\r\n    model = YOLO(config.weights)\r\n\r\n    def inference_callback(frame: VideoFrame) -> sv.Detections:\r\n        results = model(frame.image, verbose=config.model_verbose, conf=config.confidence, device=config.device)[0]\r\n        return sv.Detections.from_ultralytics(results).with_nms(threshold=config.iou)\r\n\r\n    processor = DataProcessor(classes=config.classes, config=config)\r\n\r\n    pipeline = InferencePipeline.init_with_custom_logic(\r\n        video_reference=config.rtsp_url,\r\n        on_video_frame=inference_callback,\r\n        on_prediction=processor.on_prediction,\r\n    )\r\n\r\n    pipeline.start()\r\n\r\n    try:\r\n        pipeline.join()\r\n    except KeyboardInterrupt:\r\n        pipeline.terminate()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\nThanks in advance.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @gmijo47 üëãüèª Let me convert this question into a discussion first."
      }
    ]
  },
  {
    "issue_number": 1247,
    "title": "Speed Estimator for Vehicle Tracking",
    "author": "bthoma48",
    "state": "closed",
    "created_at": "2024-05-30T11:10:04Z",
    "updated_at": "2024-05-30T11:38:05Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nWhere and how do I specifically add the configurations for both \"vehicles.mp4\" and \"vehicles-result.mp4\" in the ultralytics script, \"ultralytics_example.py\"?\r\n\r\nDoes it simply replace the code-line: \"--source_video_path\" and \"--target_video-path\"?\r\n\r\nCan you specifically send the 146-line ultralytics script to incorporate \"vehicles.mp4\" and \"vehicles-result.mp4\"?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @bthoma48 :wave: \r\n\r\nI see you've already opened a discussion. Let's chat there :wink: \r\n\r\nhttps://github.com/roboflow/supervision/discussions/1248\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1243,
    "title": "Request: PolygonZone determination using object recognition",
    "author": "pasionline",
    "state": "closed",
    "created_at": "2024-05-29T12:20:02Z",
    "updated_at": "2024-05-29T12:44:24Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nI am currently using supervision in my thesis to analyse driving behavior in different videos and its super useful. But the PolygonZone array must be determined manually for each video.\r\n\r\nWould it be possible to (semi-) automate this process with object recognition? By specifying an object that can be found in several places in a frame, the feature would then return the coordinates of the object from the frame and append them to an array.\n\n### Use case\n\nIt would be very useful, for example, when determining the polygon zone, which is created on the basis of delineators. In this way, a road section can be recognized directly without having to enter an array manually.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @pasionline üëãüèª let me convert this issue into discussion "
      }
    ]
  },
  {
    "issue_number": 1209,
    "title": "ValueError when loading COCO dataset with multiple segmentation masks for one class ",
    "author": "DancinParrot",
    "state": "open",
    "created_at": "2024-05-20T09:47:11Z",
    "updated_at": "2024-05-24T13:39:32Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nMy current COCO dataset includes annotations with more than 1 segmentation masks of the same class. A rough analogy is as follows whereby one eye of a cat is segmented  as a whole but when exported from Fiftyone two polygons are produced (turned into segmentation masks):\r\n\r\n![cat](https://github.com/roboflow/supervision/assets/63692048/d3d35914-a484-40e3-9a51-880d601b8551)\r\n\r\nAs a result, when the COCO dataset is loaded into my program using supervision, the program crashes with the following error:\r\n```\r\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (7,) + inhomogeneous part.\r\n```\r\n\r\nAfter some research, I discovered Ultralytics' [JSON2YOLO](https://github.com/ultralytics/JSON2YOLO) repository on GitHub, and adapted the library's `merge_multi_segment()` (seen [here](https://github.com/ultralytics/JSON2YOLO/blob/e6dd7784b77db4f1d8c3aa95ed93de2890b3ac23/general_json2yolo.py#L330)) function in supervision's `coco.py` file which then allows the COCO dataset to be loaded.\r\n\r\n### Environment\r\n\r\n- Supervision: 0.20.0\r\n- OS: OpenSUSE Tumbleweed 20240423\r\n- Python: 3.12.2\r\n\r\n### Minimal Reproducible Example\r\n\r\nThe following code is used to load the COCO dataset with the annotations_path being the path to a .json file containing the paths and annotations for all images in the dataset:\r\n```Python\r\nds = sv.DetectionDataset.from_coco(\r\n        images_directory_path=images_directory_path,\r\n        annotations_path=annotations_path,\r\n        force_masks=True,\r\n    )\r\n```\r\n\r\nThe following is an example of a class/category containing multiple segmentation masks:\r\n```JSON\r\n{\r\n ...\r\n    {\r\n      \"id\": 41,\r\n      \"image_id\": 6,\r\n      \"category_id\": 0,\r\n      \"bbox\": [\r\n        694.801517364719,\r\n        278.90263698033465,\r\n        161.52883212628387,\r\n        282.881946369456\r\n      ],\r\n      \"segmentation\": [\r\n        [\r\n          694,\r\n          560.5,\r\n          764.5,\r\n          407,\r\n          759.5,\r\n          400,\r\n          765.5,\r\n          397,\r\n          765.5,\r\n          393,\r\n          760.5,\r\n          391,\r\n          759.5,\r\n          384,\r\n          754.5,\r\n          381,\r\n          763,\r\n          376.5,\r\n          767.5,\r\n          370,\r\n          764.5,\r\n          363,\r\n          768.5,\r\n          354,\r\n          735,\r\n          278.5,\r\n          741.5,\r\n          284,\r\n          776,\r\n          356.5,\r\n          782,\r\n          359.5,\r\n          794.5,\r\n          348,\r\n          806.5,\r\n          321,\r\n          809.5,\r\n          321,\r\n          799.5,\r\n          346,\r\n          800,\r\n          348.5,\r\n          806.5,\r\n          349,\r\n          798.5,\r\n          364,\r\n          800.5,\r\n          387,\r\n          808.5,\r\n          400,\r\n          818.5,\r\n          408,\r\n          811,\r\n          413.5,\r\n          802,\r\n          405.5,\r\n          802.5,\r\n          416,\r\n          855.5,\r\n          530,\r\n          851.5,\r\n          529,\r\n          787.5,\r\n          395,\r\n          779,\r\n          394.5,\r\n          714.5,\r\n          531,\r\n          715.5,\r\n          522,\r\n          729,\r\n          490.5,\r\n          694,\r\n          560.5\r\n        ],\r\n        [\r\n          713,\r\n          534.5,\r\n          713,\r\n          531.5,\r\n          713,\r\n          534.5\r\n        ]\r\n      ],\r\n      \"area\": 45693.59042666829,\r\n      \"iscrowd\": 0,\r\n      \"ignore\": 0\r\n    }\r\n}\r\n```\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @DancinParrot üëãüèª Sorry for the late response, but I traveled a lot at the end of last week, and my access to GitHub was limited.\r\n\r\nYou're correct, we currently do not support loading multi-segment masks. I assume the change you want to make would be in the `coco_annotations_to_masks` function?"
      },
      {
        "user": "DancinParrot",
        "body": "Hi @SkalskiP! Thanks for your response, I wasn't expecting a response this quick actually so no worries!\r\n\r\nI see, that would explain the error. However, the error was actually raised from this [line](https://github.com/roboflow/supervision/blob/3fbce7b6bbdccfbbd68a525cfa3993e16c0637c7/supervision/dataset/formats/coco.py#L88) which is within the `coco_annotations_to_detections` function. I could not find any reference of the `coco_annotations_to_masks` function within this repo, did I miss out anything?\r\n\r\nI modified the `coco_annotations_to_detections` function based on JSON2YOLO's [implementation](https://github.com/ultralytics/JSON2YOLO/blob/e6dd7784b77db4f1d8c3aa95ed93de2890b3ac23/general_json2yolo.py#L298):\r\n```Python\r\ndef coco_annotations_to_detections(\r\n    image_annotations: List[dict], resolution_wh: Tuple[int, int], with_masks: bool\r\n) -> Detections:\r\n    #...\r\n\r\n    if with_masks:\r\n        polygons = []\r\n\r\n        for image_annotation in image_annotations:\r\n            segmentations = image_annotation[\"segmentation\"]\r\n            if len(segmentations) > 1:\r\n                s = merge_multi_segment(segmentations)\r\n                s = (\r\n                    (np.concatenate(s, axis=0) / np.array(resolution_wh))\r\n                    .reshape(-1)\r\n                    .tolist()\r\n                )\r\n                reshaped = np.reshape(np.asarray(s, dtype=np.int32), (-1, 2))\r\n            else:\r\n                reshaped = np.reshape(\r\n                    np.asarray(segmentations, dtype=np.int32), (-1, 2)\r\n                )\r\n            polygons.append(reshaped)\r\n\r\n        #...\r\n\r\n    return Detections(xyxy=xyxy, class_id=np.asarray(class_ids, dtype=int))\r\n```\r\n\r\nThe aforementioned modification allows the dataset to be loaded. Though, I'm not sure if the outcome really fits my use case since the `merge_multi_segment()` function included in JSON2YOLO seems to connect all segmentation masks into one with a thin line, which I presume would form one whole mask as opposed to the intended seperate masks. Any thoughts on this? "
      },
      {
        "user": "SkalskiP",
        "body": "> However, the error was actually raised from this [line](https://github.com/roboflow/supervision/blob/3fbce7b6bbdccfbbd68a525cfa3993e16c0637c7/supervision/dataset/formats/coco.py#L88) [...]\r\n\r\nI'm very sorry. You're right, of course. I mentioned `coco_annotations_to_masks` because there is ongoing work on COCO loading and saving in [#1163](https://github.com/roboflow/supervision/pull/1163), and this method will appear in that PR. Don't worry about it.\r\n\r\nAs for `merge_multi_segment`, we can't use that implementation since JSON2YOLO is under an AGPL license, which would conflict with our MIT license. Therefore, we need to implement our own version of that function.\r\n\r\n> I'm not sure if the outcome really fits my use case since the `merge_multi_segment()` function included in JSON2YOLO seems to connect all segmentation masks into one with a thin line, which I presume would form one whole mask as opposed to the intended separate masks.\r\n\r\nIf you want to load this as two separate masks, your COCO JSON is incorrectly constructed. You should not have multiple lists under the `segmentation` key. These should be separate annotations. If you have multiple lists there, they should be loaded as a single mask."
      }
    ]
  },
  {
    "issue_number": 1092,
    "title": "sv.Detections has exactly N of each member, except for `tracker_id`.",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-04-04T11:14:42Z",
    "updated_at": "2024-05-24T07:17:48Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nFound in: #1044, #1076.\r\n\r\nThis is a sub-issue of #943 and should be solved before starting that one.\r\n\r\nsv.Detections has a list of numpy arrays as its members: `xyxy`, `mask`, `confidence`, `class_id`, `trackier_id` When `N` detections are made, all of these except `tracker_id` will have a shape `(N, ...)`.\r\n\r\nFor example: When there are no tracks, `update_with_detections` will return detections, with `detections.tracker_id` set to `[]`.\r\n\r\nBUT, a few lines above, it only returns those detections where `tracker_id` was not `-1`. Many places in the codebase, e.g. the `TraceAnnotator` expect len(tracker_id) to match len(detections). There's many zips in examples, zipping with `tracker_id`.\r\n\r\nSo there's 2 paradigms:\r\n1) Make the tracker ID length always match the length of other members by having a special value such as `-1`, already used in some places.\r\n2) Make `tracker_id` different than other members of `sv.Detections`, keep the current implementation, but root out incorrect usage.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "I'm heavily in favour of 1. The steps for this would be:\r\n\r\n- [x] Find, learn of all places where `tracker_id` is used.\r\n- [ ] Modify the core of `ByteTrack` - if there is a detection but there's no tracker - instead of returning nothing, return `-1`. Make sure the length of `tracker_id` matches the length of `detections` being returned.\r\n- [x] Think a bit how this affects case where we deliberately hide uncertain trackers - this would be introduced as part of #1076.\r\n- [x] Propagate the changes in the core of `ByteTrack`, to the uses by various components - annotators, examples\r\n- [ ] Some tests would be good to have.\r\n\r\nThis would be a draft PR until #1076 is merged. This PR new should remove the `Detections.empty` call from it."
      },
      {
        "user": "LinasKo",
        "body": "Example: line_zone.py:trigger() checks if iterating over `tracker_id` array produces `None`\r\n\r\nRelated: Test case in #1206 checks it too."
      },
      {
        "user": "LinasKo",
        "body": "Example: smoother.py:update_with_detections() checks if iterating over `tracker_id` array produces `None`"
      }
    ]
  },
  {
    "issue_number": 1103,
    "title": "Interpolation error on computing average precision",
    "author": "illian01",
    "state": "closed",
    "created_at": "2024-04-09T10:11:55Z",
    "updated_at": "2024-05-23T16:19:41Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nI have precision and recall curve as below.\r\n```\r\nprecision = np.array([1.0, 1.0, 1.0, 1.0, 0.99875156, 0.998002, 0.99666944, 0.9971449, 0.99625234, 0.99555802, 0.994003, 0.9918219, 0.99083715, 0.99000384, 0.98786148, 0.98600467, 0.98344267, 0.98265216, 0.98083866, 0.97947908, 0.97525619, 0.97048322, 0.96682572, 0.96305151, 0.95875859, 0.955009, 0.95154778, 0.94815775, 0.94215319, 0.93880365, 0.93284453, 0.92710853, 0.9206374, 0.91410392, 0.90604323, 0.89815741, 0.89070962, 0.88096203, 0.87304302, 0.86565825, 0.85939258, 0.84989635, 0.84192358, 0.833624, 0.82422452, 0.81490945, 0.80480383, 0.79629827, 0.78731382, 0.77798184, 0.76882312, 0.76002353, 0.75040861, 0.74181681, 0.7328951, 0.72293428, 0.71431122, 0.70528901, 0.69700888, 0.68824676, 0.67994334, 0.67183018, 0.66446254, 0.65629712, 0.64869932, 0.64125836, 0.63434588, 0.62696814, 0.61995442, 0.61256431, 0.60617099, 0.59918316, 0.59245886, 0.58591877, 0.57982569, 0.57362842, 0.56759424, 0.56145705, 0.55528492, 0.54958547, 0.5440285, 0.53811493, 0.5325285, 0.52695621, 0.52151658, 0.51626375, 0.51136562, 0.50652261, 0.50133515, 0.49632043, 0.4914727, 0.48689632, 0.48220205, 0.47777001, 0.47305994, 0.46897532, 0.46487162, 0.46038864, 0.45635427, 0.45230039, 0.44817759, 0.44438394, 0.44027254, 0.43658075, 0.43271958, 0.42912242, 0.42531013, 0.42184945, 0.41822138, 0.41461401, 0.41120858, 0.40781947, 0.40458015, 0.40130968, 0.39827201, 0.39511326, 0.39226757, 0.38925687, 0.38646668, 0.38355531, 0.38073414, 0.37771166, 0.3748207, 0.37197675, 0.36909802, 0.36642534, 0.36387445, 0.36136373, 0.35873599, 0.3562265, 0.3536787, 0.3511698, 0.34869891, 0.34626518, 0.34390508, 0.3414318, 0.33906842, 0.33681253, 0.33469802, 0.33250602, 0.33023821, 0.32810893, 0.32615049, 0.32397469, 0.32203743, 0.31998897, 0.31824253, 0.31621373, 0.31421236, 0.31217073, 0.31032299, 0.30846661, 0.30663465, 0.30479396, 0.30304211, 0.30131286, 0.29947758, 0.29772937, 0.29587671, 0.29420458, 0.29245961, 0.29076737, 0.28900343, 0.2874145, 0.28566202, 0.2841126, 0.28243125, 0.28085985, 0.27930716, 0.27783202, 0.27625658, 0.27469957, 0.27324787, 0.27181295, 0.27033706, 0.26882089, 0.26735036, 0.26595294, 0.26459931, 0.2632608, 0.26190939, 0.26071103, 0.25941595, 0.25816235, 0.25675933, 0.25550661, 0.25413295, 0.25280073, 0.25150927, 0.25023148, 0.24904608, 0.24774221, 0.246504, 0.24530453, 0.24419474, 0.24301941, 0.24185608, 0.24062841, 0.2394889, 0.23833572, 0.23724407, 0.23611353, 0.23499418, 0.23386123, 0.23278841, 0.23167728, 0.23064974, 0.22958383, 0.2285522, 0.2274826, 0.2264708, 0.2253975, 0.22440508, 0.22342199, 0.22244807, 0.22145997, 0.22050415, 0.21951107, 0.21857297, 0.21757494, 0.21663144, 0.2157191, 0.2148375, 0.21396381, 0.21309792, 0.21219528, 0.21132276, 0.21041387, 0.20951295, 0.20866357, 0.20777809, 0.20687864])\r\nrecall = np.array([9.088e-05, 0.01826609, 0.03644129, 0.0546165, 0.07270084, 0.09078517, 0.10877863, 0.12695383, 0.14494729, 0.16294075, 0.18075245, 0.19838241, 0.21619411, 0.23400582, 0.25145402, 0.26890222, 0.28607779, 0.30370774, 0.32097419, 0.33833152, 0.35459833, 0.37050164, 0.38667757, 0.40267176, 0.41830244, 0.43402399, 0.44974555, 0.46537623, 0.47955289, 0.49491094, 0.5087241, 0.52244638, 0.53553253, 0.54834606, 0.55997819, 0.57142857, 0.58287895, 0.59251181, 0.60305344, 0.61368593, 0.62486369, 0.63340603, 0.64276627, 0.65158124, 0.65921483, 0.66657579, 0.67293711, 0.68029807, 0.68693202, 0.69292984, 0.69874591, 0.70456198, 0.70928753, 0.71464922, 0.71937477, 0.72273719, 0.72709924, 0.73073428, 0.7348237, 0.73809524, 0.74154853, 0.74491094, 0.74881861, 0.75154489, 0.75463468, 0.75763359, 0.760996, 0.76354053, 0.76626681, 0.76826609, 0.77126499, 0.77326427, 0.77535442, 0.77744457, 0.77989822, 0.78198837, 0.78407852, 0.78580516, 0.78725918, 0.78916758, 0.79107597, 0.79225736, 0.79371138, 0.79498364, 0.79625591, 0.79761905, 0.79934569, 0.80098146, 0.80189022, 0.80288986, 0.80398037, 0.80534351, 0.80634315, 0.80761541, 0.80825154, 0.80979644, 0.81115958, 0.81170483, 0.81288622, 0.81388586, 0.81461287, 0.81579426, 0.81624864, 0.81733915, 0.81797528, 0.81897492, 0.8194293, 0.82042893, 0.82097419, 0.82142857, 0.82215558, 0.82279171, 0.8236096, 0.82424573, 0.82524537, 0.8258815, 0.82706289, 0.82778989, 0.82888041, 0.82960742, 0.8304253, 0.83069793, 0.83115231, 0.83160669, 0.83187932, 0.83251545, 0.83333333, 0.83415122, 0.8346056, 0.83524173, 0.83569611, 0.83615049, 0.83660487, 0.83705925, 0.83760451, 0.83778626, 0.83814976, 0.83869502, 0.8395129, 0.84005816, 0.84033079, 0.84087605, 0.84178481, 0.84205743, 0.84287532, 0.8433297, 0.84451109, 0.84487459, 0.8452381, 0.84541985, 0.84605598, 0.84660124, 0.84714649, 0.84760087, 0.848237, 0.84887314, 0.84914577, 0.84960015, 0.84969102, 0.85023628, 0.85050891, 0.85087241, 0.85096329, 0.85150854, 0.85150854, 0.8520538, 0.85214467, 0.85250818, 0.85287168, 0.85341694, 0.85359869, 0.85378044, 0.85423482, 0.8546892, 0.85496183, 0.85505271, 0.85523446, 0.85559796, 0.85605234, 0.85650672, 0.85687023, 0.85768811, 0.85814249, 0.85868775, 0.85868775, 0.85914213, 0.85914213, 0.85923301, 0.85941476, 0.85959651, 0.86005089, 0.86005089, 0.86023264, 0.86050527, 0.86105053, 0.86132316, 0.86159578, 0.86159578, 0.86186841, 0.86205016, 0.86241367, 0.86259542, 0.86277717, 0.86286805, 0.86314068, 0.86323155, 0.86359506, 0.86377681, 0.86404944, 0.86414031, 0.86441294, 0.86441294, 0.86468557, 0.8649582, 0.86523083, 0.86541258, 0.86568521, 0.86577608, 0.86604871, 0.86604871, 0.86623046, 0.86650309, 0.86686659, 0.8672301, 0.8675936, 0.86777535, 0.86804798, 0.86813886, 0.86822973, 0.86850236, 0.86859324, 0.86859324])\r\n```\r\n\r\nSo, I can plot PR curve as below.\r\n```\r\nplt.clf()\r\nplt.plot(recall, precision)\r\nplt.show()\r\n```\r\n\r\n![test](https://github.com/roboflow/supervision/assets/29498170/76ee0630-dc9e-446f-a87f-418b44a5a59c)\r\n\r\nHowever, according to `compute_average_precision` method, always pad `0.0` and `1.0` to precision and recall vectors.\r\n\r\nhttps://github.com/roboflow/supervision/blob/4729e20a9408fbe08d342cc4dbae835d808686a5/supervision/metrics/detection.py#L727-L749\r\n\r\nI think these extensions are dangerous and makes pr curve inaccurate. Below is curve of `interpolated_precision` and `interpolated_recall_levels`.\r\n\r\n```\r\nplt.clf()\r\nplt.plot(interpolated_recall_levels, interpolated_precision)\r\nplt.show()\r\n```\r\n\r\n![test](https://github.com/roboflow/supervision/assets/29498170/d77aa454-86e2-4d91-b080-355baafcc0e9)\r\n\r\nThe bent part of right bottom in the plot seems unnatural and effects on the accuracy of the average precision calculation.\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n```\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nprecision = np.array([1.0, 1.0, 1.0, 1.0, 0.99875156, 0.998002, 0.99666944, 0.9971449, 0.99625234, 0.99555802, 0.994003, 0.9918219, 0.99083715, 0.99000384, 0.98786148, 0.98600467, 0.98344267, 0.98265216, 0.98083866, 0.97947908, 0.97525619, 0.97048322, 0.96682572, 0.96305151, 0.95875859, 0.955009, 0.95154778, 0.94815775, 0.94215319, 0.93880365, 0.93284453, 0.92710853, 0.9206374, 0.91410392, 0.90604323, 0.89815741, 0.89070962, 0.88096203, 0.87304302, 0.86565825, 0.85939258, 0.84989635, 0.84192358, 0.833624, 0.82422452, 0.81490945, 0.80480383, 0.79629827, 0.78731382, 0.77798184, 0.76882312, 0.76002353, 0.75040861, 0.74181681, 0.7328951, 0.72293428, 0.71431122, 0.70528901, 0.69700888, 0.68824676, 0.67994334, 0.67183018, 0.66446254, 0.65629712, 0.64869932, 0.64125836, 0.63434588, 0.62696814, 0.61995442, 0.61256431, 0.60617099, 0.59918316, 0.59245886, 0.58591877, 0.57982569, 0.57362842, 0.56759424, 0.56145705, 0.55528492, 0.54958547, 0.5440285, 0.53811493, 0.5325285, 0.52695621, 0.52151658, 0.51626375, 0.51136562, 0.50652261, 0.50133515, 0.49632043, 0.4914727, 0.48689632, 0.48220205, 0.47777001, 0.47305994, 0.46897532, 0.46487162, 0.46038864, 0.45635427, 0.45230039, 0.44817759, 0.44438394, 0.44027254, 0.43658075, 0.43271958, 0.42912242, 0.42531013, 0.42184945, 0.41822138, 0.41461401, 0.41120858, 0.40781947, 0.40458015, 0.40130968, 0.39827201, 0.39511326, 0.39226757, 0.38925687, 0.38646668, 0.38355531, 0.38073414, 0.37771166, 0.3748207, 0.37197675, 0.36909802, 0.36642534, 0.36387445, 0.36136373, 0.35873599, 0.3562265, 0.3536787, 0.3511698, 0.34869891, 0.34626518, 0.34390508, 0.3414318, 0.33906842, 0.33681253, 0.33469802, 0.33250602, 0.33023821, 0.32810893, 0.32615049, 0.32397469, 0.32203743, 0.31998897, 0.31824253, 0.31621373, 0.31421236, 0.31217073, 0.31032299, 0.30846661, 0.30663465, 0.30479396, 0.30304211, 0.30131286, 0.29947758, 0.29772937, 0.29587671, 0.29420458, 0.29245961, 0.29076737, 0.28900343, 0.2874145, 0.28566202, 0.2841126, 0.28243125, 0.28085985, 0.27930716, 0.27783202, 0.27625658, 0.27469957, 0.27324787, 0.27181295, 0.27033706, 0.26882089, 0.26735036, 0.26595294, 0.26459931, 0.2632608, 0.26190939, 0.26071103, 0.25941595, 0.25816235, 0.25675933, 0.25550661, 0.25413295, 0.25280073, 0.25150927, 0.25023148, 0.24904608, 0.24774221, 0.246504, 0.24530453, 0.24419474, 0.24301941, 0.24185608, 0.24062841, 0.2394889, 0.23833572, 0.23724407, 0.23611353, 0.23499418, 0.23386123, 0.23278841, 0.23167728, 0.23064974, 0.22958383, 0.2285522, 0.2274826, 0.2264708, 0.2253975, 0.22440508, 0.22342199, 0.22244807, 0.22145997, 0.22050415, 0.21951107, 0.21857297, 0.21757494, 0.21663144, 0.2157191, 0.2148375, 0.21396381, 0.21309792, 0.21219528, 0.21132276, 0.21041387, 0.20951295, 0.20866357, 0.20777809, 0.20687864])\r\nrecall = np.array([9.088e-05, 0.01826609, 0.03644129, 0.0546165, 0.07270084, 0.09078517, 0.10877863, 0.12695383, 0.14494729, 0.16294075, 0.18075245, 0.19838241, 0.21619411, 0.23400582, 0.25145402, 0.26890222, 0.28607779, 0.30370774, 0.32097419, 0.33833152, 0.35459833, 0.37050164, 0.38667757, 0.40267176, 0.41830244, 0.43402399, 0.44974555, 0.46537623, 0.47955289, 0.49491094, 0.5087241, 0.52244638, 0.53553253, 0.54834606, 0.55997819, 0.57142857, 0.58287895, 0.59251181, 0.60305344, 0.61368593, 0.62486369, 0.63340603, 0.64276627, 0.65158124, 0.65921483, 0.66657579, 0.67293711, 0.68029807, 0.68693202, 0.69292984, 0.69874591, 0.70456198, 0.70928753, 0.71464922, 0.71937477, 0.72273719, 0.72709924, 0.73073428, 0.7348237, 0.73809524, 0.74154853, 0.74491094, 0.74881861, 0.75154489, 0.75463468, 0.75763359, 0.760996, 0.76354053, 0.76626681, 0.76826609, 0.77126499, 0.77326427, 0.77535442, 0.77744457, 0.77989822, 0.78198837, 0.78407852, 0.78580516, 0.78725918, 0.78916758, 0.79107597, 0.79225736, 0.79371138, 0.79498364, 0.79625591, 0.79761905, 0.79934569, 0.80098146, 0.80189022, 0.80288986, 0.80398037, 0.80534351, 0.80634315, 0.80761541, 0.80825154, 0.80979644, 0.81115958, 0.81170483, 0.81288622, 0.81388586, 0.81461287, 0.81579426, 0.81624864, 0.81733915, 0.81797528, 0.81897492, 0.8194293, 0.82042893, 0.82097419, 0.82142857, 0.82215558, 0.82279171, 0.8236096, 0.82424573, 0.82524537, 0.8258815, 0.82706289, 0.82778989, 0.82888041, 0.82960742, 0.8304253, 0.83069793, 0.83115231, 0.83160669, 0.83187932, 0.83251545, 0.83333333, 0.83415122, 0.8346056, 0.83524173, 0.83569611, 0.83615049, 0.83660487, 0.83705925, 0.83760451, 0.83778626, 0.83814976, 0.83869502, 0.8395129, 0.84005816, 0.84033079, 0.84087605, 0.84178481, 0.84205743, 0.84287532, 0.8433297, 0.84451109, 0.84487459, 0.8452381, 0.84541985, 0.84605598, 0.84660124, 0.84714649, 0.84760087, 0.848237, 0.84887314, 0.84914577, 0.84960015, 0.84969102, 0.85023628, 0.85050891, 0.85087241, 0.85096329, 0.85150854, 0.85150854, 0.8520538, 0.85214467, 0.85250818, 0.85287168, 0.85341694, 0.85359869, 0.85378044, 0.85423482, 0.8546892, 0.85496183, 0.85505271, 0.85523446, 0.85559796, 0.85605234, 0.85650672, 0.85687023, 0.85768811, 0.85814249, 0.85868775, 0.85868775, 0.85914213, 0.85914213, 0.85923301, 0.85941476, 0.85959651, 0.86005089, 0.86005089, 0.86023264, 0.86050527, 0.86105053, 0.86132316, 0.86159578, 0.86159578, 0.86186841, 0.86205016, 0.86241367, 0.86259542, 0.86277717, 0.86286805, 0.86314068, 0.86323155, 0.86359506, 0.86377681, 0.86404944, 0.86414031, 0.86441294, 0.86441294, 0.86468557, 0.8649582, 0.86523083, 0.86541258, 0.86568521, 0.86577608, 0.86604871, 0.86604871, 0.86623046, 0.86650309, 0.86686659, 0.8672301, 0.8675936, 0.86777535, 0.86804798, 0.86813886, 0.86822973, 0.86850236, 0.86859324, 0.86859324])\r\n\r\nplt.clf()\r\nplt.plot(recall, precision)\r\nplt.savefig('original_pr_curve.png')\r\n\r\ndef compute_average_precision(recall: np.ndarray, precision: np.ndarray) -> float:\r\n    \"\"\"\r\n    Compute the average precision using 101-point interpolation (COCO), given\r\n        the recall and precision curves.\r\n\r\n    Args:\r\n        recall (np.ndarray): The recall curve.\r\n        precision (np.ndarray): The precision curve.\r\n\r\n    Returns:\r\n        float: Average precision.\r\n    \"\"\"\r\n    extended_recall = np.concatenate(([0.0], recall, [1.0]))\r\n    extended_precision = np.concatenate(([1.0], precision, [0.0]))\r\n    max_accumulated_precision = np.flip(\r\n        np.maximum.accumulate(np.flip(extended_precision))\r\n    )\r\n    interpolated_recall_levels = np.linspace(0, 1, 101)\r\n    interpolated_precision = np.interp(\r\n        interpolated_recall_levels, extended_recall, max_accumulated_precision\r\n    )\r\n    average_precision = np.trapz(interpolated_precision, interpolated_recall_levels)\r\n    #return average_precision\r\n    return interpolated_precision, interpolated_recall_levels\r\n\r\ninterpolated_precision, interpolated_recall_levels = compute_average_precision(recall, precision)\r\n\r\nplt.clf()\r\nplt.plot(interpolated_recall_levels, interpolated_precision)\r\nplt.savefig('interpolated_pr_curve.png')\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hey @illian01 :wave:\r\n\r\nThank you for reporting the issue!\r\nYou're right - it definitely looks wrong. We'll need to rethink how we do it a bit.\r\n\r\nIf anyone in the community stumbles upon this - would you like to help us out? :slightly_smiling_face: \r\n\r\n"
      },
      {
        "user": "Griffin-Sullivan",
        "body": "Hey I was just playing around with this and had a couple questions:\r\n\r\n1. Why do we extend the recall and precision between 0.0 and 1.0? Is there an assumption you will always have a value very close to these numbers and that's the issue here?\r\n2. Could we not just use the minimum and maximum recall for `np.linspace(0, 1, 101)`? "
      },
      {
        "user": "Griffin-Sullivan",
        "body": "For reference here's what I was doing: https://colab.research.google.com/drive/1vT_9Q2RwQnCFqlcz-ptpmeZaQomGY-WB?usp=sharing"
      }
    ]
  },
  {
    "issue_number": 615,
    "title": " Can use as_yolo convert coco2yolo to segmentation data  format ?",
    "author": "lonngxiang",
    "state": "open",
    "created_at": "2023-11-22T05:38:59Z",
    "updated_at": "2024-05-22T19:56:46Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\n\r\nuse this script the conversion format is incorrect\r\n[https://docs.ultralytics.com/datasets/segment/coco/#dataset-yaml](url)\r\n```\r\nimport supervision as sv\r\n\r\nsv.DetectionDataset.from_coco(\r\n    images_directory_path= r\"C:\\Users\\loong\\Downloads\\Car\\valid\",\r\n    annotations_path=r\"C:\\Users\\loong\\Downloads\\Car\\valid\\_annotations.coco.json\",\r\n    force_masks=True\r\n).as_yolo(\r\n    images_directory_path=r\"C:\\Users\\loong\\Downloads\\Car_yolo\\val\\images\",\r\n    annotations_directory_path=r\"C:\\Users\\loong\\Downloads\\Car_yolo\\val\\labels\",\r\n    data_yaml_path=r\"C:\\Users\\loong\\Downloads\\Car_yolo\\data.yaml\"\r\n)\r\n```\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @lonngxiang üëãüèª Thanks for your interest in Supervision. Could you be a bit more specific? What do you mean by incorrect? "
      },
      {
        "user": "lonngxiang",
        "body": "> Hi @lonngxiang üëãüèª Thanks for your interest in Supervision. Could you be a bit more specific? What do you mean by incorrect?\r\n\r\nyeap,I want to use Ultralytics' YOLOv8 to train a segmentation algorithm, but the provided function seems to convert COCO dataset into YOLO format. Could you please help me how to convert it into the required format for segmentation dataset?\r\n\r\nPlease refer to for details:\r\n[https://github.com/roboflow/supervision/issues/267#issuecomment-1821974837](url)\r\n\r\n"
      },
      {
        "user": "lonngxiang",
        "body": "> Hi @lonngxiang üëãüèª Thanks for your interest in Supervision. Could you be a bit more specific? What do you mean by incorrect?\r\n\r\nUsing this script looks just converted to yolo object detection format, not segmentation data set format requirements\r\n```\r\nimport supervision as sv\r\n\r\nsv.DetectionDataset.from_coco(\r\n    images_directory_path= r\"C:\\Users\\loong\\Downloads\\Car\\valid\",\r\n    annotations_path=r\"C:\\Users\\loong\\Downloads\\Car\\valid\\_annotations.coco.json\",\r\n    force_masks=True\r\n).as_yolo(\r\n    images_directory_path=r\"C:\\Users\\loong\\Downloads\\Car_yolo\\val\\images\",\r\n    annotations_directory_path=r\"C:\\Users\\loong\\Downloads\\Car_yolo\\val\\labels\",\r\n    data_yaml_path=r\"C:\\Users\\loong\\Downloads\\Car_yolo\\data.yaml\"\r\n)\r\n```"
      }
    ]
  },
  {
    "issue_number": 1114,
    "title": "[DetectionDataset] - extend `from_coco` and `as_coco` with support for masks in RLE format",
    "author": "LinasKo",
    "state": "closed",
    "created_at": "2024-04-12T15:06:27Z",
    "updated_at": "2024-05-21T11:30:15Z",
    "labels": [
      "enhancement",
      "help wanted",
      "api:datasets",
      "Q2.2024"
    ],
    "body": "### Description\r\n\r\nThe COCO dataset format allows for the storage of segmentation masks in two ways:\r\n\r\n- Polygon Masks: These masks use a series of vertices on an x-y plane to represent segmented object areas. The vertices are connected by straight lines to form polygons that approximate the shapes of objects.\r\nRun-Length Encoding (RLE): RLE compresses segments of pixels into counts of consecutive pixels (runs). This method efficiently sequences pixels by reporting the number of pixels that are either foreground or background. For instance, starting from the top left of an image, the encoding might record '5 white pixels, 3 black pixels, 6 white pixels', and so on.\r\n\r\nSupervision currently only supports Polygon Masks, but we want to expand support for masks in RLE format. To do this, you will need to make changes in [`coco_annotations_to_detections`](https://github.com/roboflow/supervision/blob/9d9acd7e587d117a2faa395580664aeb83be5efb/supervision/dataset/formats/coco.py#L72) and [`detections_to_coco_annotations`](https://github.com/roboflow/supervision/blob/9d9acd7e587d117a2faa395580664aeb83be5efb/supervision/dataset/formats/coco.py#L100).\r\n\r\n### Links\r\n\r\n- an official [explanation](https://github.com/cocodataset/cocoapi/issues/184) from the COCO dataset repository\r\n- old supervision [issue](https://github.com/roboflow/supervision/issues/373) providing more context\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "David-rn",
        "body": "Hi, great job with supervision! I would like to try contributing, but first I have some doubts regarding this issue. According to  [coco data format](https://cocodataset.org/#format-data) the RLE format is used only when `iscrowd=1` but I don't see this attribute in `Detections` class so, does this mean that the idea is to select one type of segmentation storage when loading or saving annotations? Thanks in advance!"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @David-rn üëãüèª I'm really glad you want to contribute to supervision. It's true that detections don't have an `iscrowd` field, and we don't need it but don't worry about that for now. Let's start with implementing two functions:\r\n\r\n- `mask_to_rle`, which would convert a 2D boolean `np.array` into a valid RLE (Run-Length Encoding) representation.\r\n- `rle_to_mask`, which converts an RLE representation back into a 2D boolean `np.array`.\r\n\r\n\r\nIn the second phase, we'll discuss how to integrate them into methods `as_coco` and `from_coco`. Does that sound okay?"
      },
      {
        "user": "David-rn",
        "body": "Hi @SkalskiP, sure! It does sound okay!"
      }
    ]
  },
  {
    "issue_number": 1207,
    "title": "Integrate OCSORT to supervision",
    "author": "tgbaoo",
    "state": "closed",
    "created_at": "2024-05-18T10:18:43Z",
    "updated_at": "2024-05-20T14:18:20Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Description\r\n\r\nMy project require another tracking algorithm is OCSORT (https://github.com/noahcao/OC_SORT) beside bytetracker into supervision lib?\r\n\r\nI tend to migrate th·ªÉ algorithm and write some custom code for update_with_detection method to tracking the Detection objects of supervision lib.\r\n\r\nLike this boxmot repo already integrate ocsort into their repo:\r\nhttps://github.com/mikel-brostrom/yolo_tracking/blob/master/boxmot%2Ftrackers%2Focsort%2Focsort.py\r\n\r\nIs there anything that I have to pay attention when plug some custom tracker into supervision lib beside the update_with_detection method?\r\n\r\nI want to custom tracker to use in this file of code replace bytetracker to ocsorttracker:\r\nhttps://github.com/roboflow/supervision/blob/develop/examples%2Ftime_in_zone%2Fultralytics_stream_example.py\r\n\r\n### Use case\r\n\r\n_No response_\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @tgbaoo üëãüèª Sorry for the late response, but I traveled a lot at the end of last week, and my access to GitHub was limited.\r\n\r\nI want to make sure I understand correctly. Do you want to add OCSORT to the Supervision codebase or just use it in combination with Supervision?"
      },
      {
        "user": "tgbaoo",
        "body": "Yes, I want to integrate ocsort to use with supervision library by sync the code and setup update_with_detectuon method"
      },
      {
        "user": "SkalskiP",
        "body": "In that case, let me start by converting this issue into a discussion and placing it in the Q&A section."
      }
    ]
  },
  {
    "issue_number": 943,
    "title": "`Detections.empty()` invalidates detections, causes crashes when `Detections.merge()` is called.",
    "author": "LinasKo",
    "state": "open",
    "created_at": "2024-02-25T09:02:56Z",
    "updated_at": "2024-05-20T07:30:42Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [ ] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\nThis is the underlying reason for #928. I believe it is important enough to deserve a separate issue.\r\n\r\n### Bug\r\n\r\n`Detectons.empty()` creates a very specific type of `Detections`, which is guaranteed to be incompatible with some models when merging results.\r\n\r\n### Example\r\nSuppose we run an instance segmentation model on a video. It finds one detection in frame 1, but no detections in frame 2. When we try calling `Detections.merge` on these detections, it raises an error.\r\n\r\n```python\r\nIMAGE_PATH = \"cat.png\"\r\nimage = cv2.imread(IMAGE_PATH)\r\nblack_image = np.zeros_like(image, dtype=np.uint8)\r\n\r\nmodel = get_roboflow_model(model_id=\"yolov8n-640\")        # Error type 1\r\n# model = get_roboflow_model(model_id=\"yolov8n-seg-640\")  # Error type 2\r\n\r\nresult = model.infer(image)[0]\r\ndetections_1 = sv.Detections.from_inference(result)\r\n\r\nresult = model.infer(black_image)[0]\r\ndetections_2 = sv.Detections.from_inference(result)\r\n\r\nsv.Detections.merge([detections_1, detections_2])\r\n```\r\n\r\n<details>\r\n  <summary>Error type 1</summary>\r\n  \r\n     ValueError                                Traceback (most recent call last)\r\n     <ipython-input-8-173450170fe7> in <cell line: 18>()\r\n         16 detections_2 = sv.Detections.from_inference(result)\r\n         17 \r\n    ---> 18 sv.Detections.merge([detections_1, detections_2])\r\n    \r\n    1 frames\r\n    /usr/local/lib/python3.10/dist-packages/supervision/detection/core.py in merge(cls, detections_list)\r\n        768         tracker_id = stack_or_none(\"tracker_id\")\r\n        769 \r\n    --> 770         data = merge_data([d.data for d in detections_list])\r\n        771 \r\n        772         return cls(\r\n    \r\n    /usr/local/lib/python3.10/dist-packages/supervision/detection/utils.py in merge_data(data_list)\r\n        629     all_keys_sets = [set(data.keys()) for data in data_list]\r\n        630     if not all(keys_set == all_keys_sets[0] for keys_set in all_keys_sets):\r\n    --> 631         raise ValueError(\"All data dictionaries must have the same keys to merge.\")\r\n        632 \r\n        633     for data in data_list:\r\n    \r\n    ValueError: All data dictionaries must have the same keys to merge.\r\n</details>\r\n\r\n<details>\r\n  <summary>Error type 2</summary>\r\n  \r\n    ValueError                                Traceback (most recent call last)\r\n    [<ipython-input-10-1ac0380e7de6>](https://localhost:8080/#) in <cell line: 20>()\r\n         18 \r\n         19 \r\n    ---> 20 sv.Detections.merge([detections_1, detections_2])\r\n    \r\n    1 frames\r\n    [/usr/local/lib/python3.10/dist-packages/supervision/detection/core.py](https://localhost:8080/#) in stack_or_none(name)\r\n        756                 return None\r\n        757             if any(d.__getattribute__(name) is None for d in detections_list):\r\n    --> 758                 raise ValueError(f\"All or none of the '{name}' fields must be None\")\r\n        759             return (\r\n        760                 np.vstack([d.__getattribute__(name) for d in detections_list])\r\n    \r\n    ValueError: All or none of the 'mask' fields must be None\r\n</details>\r\n\r\n### Deeper explanation\r\n\r\n`Detections` contains these variables:\r\n```python\r\nxyxy: np.ndarray\r\nmask: Optional[np.ndarray] = None\r\nconfidence: Optional[np.ndarray] = None\r\nclass_id: Optional[np.ndarray] = None\r\ntracker_id: Optional[np.ndarray] = None\r\ndata: Dict[str, Union[np.ndarray, List]] = field(default_factory=dict)\r\n```\r\n\r\nSuppose we call `Detections.merge([detections_1, detections_2])`. An error is rightfully raised when the same variable is defined differently in the detections. For example - when `mask` is `None` in `detections_1` and `np.array([])` in `detections_2`. It makes sense as we don't want to merge incompatible detections.\r\n\r\nHowever, when calling `Detections.empty()`, frequently used to initialize the no-detections case, it sets a specific subset of the fields - only the `xyxy`, `condifence` and `class_id`.\r\nMany models use other fields as well. Because of this, the set of defined fields in an empty detection might be different to what the model returns. When trying to merge these, an error is raised.\r\n\r\n### Environment\r\n\r\n_No response_\r\n\r\n### Minimal Reproducible Example\r\n\r\nSee aforementioned example.\r\n\r\n\r\n\r\n[this Colab](https://colab.research.google.com/drive/1ktj_CIlM9mcmboo8LJUCsYgKlaPN8bJM?usp=sharing) contains examples, list of functions affected.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!\r\n\r\nI could help submit a PR when I figure out a solution, but this is a non-trivial bug and to my best estimates, solving it 'correctly' would require a redesign of `Detections`. Quick remediation may mean band-aid code and hope that future features are written defensively when `merge` is involved.",
    "comments": [
      {
        "user": "LinasKo",
        "body": "# Affected code\r\nThe good news is - the error mostly appears when `Detections.merge` is called on results from _unusual models_ - where `confidence` is missing, where `data` is set, where `mask` exists (and there's no detections at least once).\r\n\r\nAffected functions:\r\n* `supervision/detection/core.py -> from_inference()`\r\n* `supervision/detection/core.py -> from_roboflow()`\r\n* `dataset/formats/yolo.py -> yolo_annotations_to_detections()`\r\n* `dataset/formats/yolo.py -> load_yolo_annotations()`\r\n* `dataset/formats/pascal_voc.py -> detections_from_xml_obj()`\r\n* `dataset/formats/coco.py -> coco_annotations_to_detections()`\r\n* `supervision/detection/core.py -> merge()`\r\n* `tracker/byte_tracker/core.py -> update_with_detections()`\r\n* `supervision/detection/tools/inference_slicer.py -> __call__()`\r\n* (maybe) `detection/tools/smoother.py -> DetectionsSmoother.get_smoothed_detections()`\r\n\r\nSee [Colab](https://colab.research.google.com/drive/1ktj_CIlM9mcmboo8LJUCsYgKlaPN8bJM?usp=sharing) for more details."
      },
      {
        "user": "LinasKo",
        "body": "# Mitigation Options\r\n**TL;DR: I think Option 3 is the least-bad solution. Adds complexity in exactly one location, keeps design and architecture intact, stops errors, prevents future worries. I also advocate for two eventual changes regardless of option taken (final section)**\r\n\r\nHere's how I see various mitigation options and their downsides. I can only speculate about the design goals and future plans of Roboflow, and I do not have access to the internal message board. So my goal now is to bring other devs up to speed and help think through the solutions.\r\n\r\n### My Assumptions\r\n1. `Detections` is meant to be universal and easily-understood, fitting both detection and segmentation models.\r\n2. Therefore, it must hold undefined variables, ready to take values from different model types.\r\n3. There's a need for `Detections.empty`, which represents empty detections of ANY type.\r\n\r\n### Caveats\r\n1. I have done little research on the `data` field, but found an example where it fails for the same reason.\r\n2. I may make generalizations for which models are used less frequently than others. This comes from observing their presence in the code, and how Roboflow prioritizes them on the site.\r\n\r\n### Option 1: Do nothing\r\nThe error has two stages:\r\n1. Calling `Detections.empty` from specific model loaders sets it up - this happens frequently.\r\n2. Calling `Detections.merge` triggers it - this happens rarely.\r\n\r\n`merge` is:\r\n* Used in `InferenceSlicer`\r\n* Used in smoother (only partially patched in #928.)\r\n* Exposed externally for users\r\n\r\nMaybe it's not worth fixing? There haven't been many complaints.\r\n\r\nThe devs just need to be extra careful of using `merge` in anything new, largely relating to instance segmentation. Who knows - maybe there's not that many features left unimplemented.\r\n\r\n### Option 2: Forbid everything affected.\r\n(The worst approach, in my opinion)\r\nVery radical - comment out merge and inference slicer, disallow smoothing. Who cares if there's incompatible objects around - they're not interacting with each other anyway.\r\n(I care).\r\n\r\n### Option 3: Change merge.\r\nI like how `merge` works, given the assumptions. But maybe it's harmful, even if correct?\r\n\r\nIf (`xyxy`, `confidence`, `class_id`) merges with (`xyxy`, `mask`, `class_id`), maybe find a way to let it happen? Form (`xyxy`, `mask`, `confidence`, `class_id`) somehow.\r\n\r\n(Brainstorming) When we need new values for:\r\n* `xyxy` - make a bounding box around `mask`, 0-sized box otherwise.\r\n* `confidence` - default to `1`.\r\n* `class_id` - set to `-1` (raaaarely seen - e.g. `from_sam`)\r\n* `tracker_id` - won't be an issue after #928 - in other cases `update_with_detections` ensures the ID is set.\r\n* `mask` - expand `xyxy` into a rectangular mask.\r\n* `data` - I don't know enough to say.\r\nIt seems to avoid both errors and architectural shifts, redefining what \"compatible Detections\" mean, at least until the next feature comparing multiple detections is added. Is this the least-bad solution?\r\n\r\n### Option 4: Explicitly define what's being initialized.\r\n`empty` sets (`xyxy`, `confidence`, `class_id`). What is that? A detection model.\r\n\r\nLet's make it initialize (`xyxy`, `mask`, `class_id`) for segmentation models. Some won't conform - SAM returns no `class_id`. So force it, make a mock id! This way you'd have two distinct model types.\r\n\r\nThen, either pass an egument to `Detections.empty` to tell which one you're initializing, or define two different `empty` functions. For the outliers - full field set detections and those models with `data` - either set the fields explicitly, or add a third, more flexible `Detections.empty`. Same thing.\r\n\r\nDid you spot an issue?\r\n\r\nSuppose you have an intermediary function. A call chain of:\r\n`Smoother` -> `some_func` -> `specific_empty`.\r\n\r\nWhich type of `empty` should it call? Somehow, with an argument or context of some sort, `some_func` needs to know which type of variables to initialize / which `empty` to call. At present, if there's no detections such an intermediary would only receive an empty list - `[]`. No type information!\r\n\r\nThat, or you need to forbid intermediaries from accepting `[]`, return a specific `empty()` from the top-level caller instead.\r\n\r\nHere's a thought experiment: try inlining `Detections.empty`. I mean, replace it with the `Detections` constructor and then explicitly set variables inside. In each case, can you tell which ones should be set? Try this inside `Detections.merge` to see the issue.\r\n\r\n### Option 4b: Frickin' Templates\r\nWe're passing the type information with function calls, elsewhere that would be done via templates or generics. `Detections.empty<Det>()`, `Detections.empty<Seg>()` might be completely normal in other languages.\r\n\r\n[Type parameter syntax](https://docs.python.org/3.12/library/typing.html#generics)is only introduced in 3.12. Perhaps [TypeVar](https://docs.python.org/3.8/library/typing.html), could help, but generally this would make the code less accessible to new devs.\r\n### Option 5: Multiple Classes\r\nDid you notice that we're approaching a multi-class solution? My claim is that `Detections` can be at least two distinct objects - the detection and the segmentation result.\r\n\r\nAn option is to change the core design, split them in two, then define strict non-nullable members, adjust everything else to match that.\r\n\r\n`Detections` becomes a base class passed to users and annotators, those have to frequently check contents. Notice that with nullable members similar checks might already be happening (I haven't checked).\r\n\r\nDownside:\r\n* Lots of changes\r\n* Less flexibility - it defeats the purpose, if we made the types in separate classes nullable.\r\n* Edge cases - `data` and non-standard parameter sets are an issue - this solution is an issue.\r\n\r\n### Option 6: Get rid of nullable types\r\nThis is my least thought-out option. Null values serve a purpose, representing invalid members - E.g. \"This `Detections` result will never have `confidence`\". This requires extra validation, disallows safely having a strong `Detections.empty` function such as the one we have now.\r\n\r\nPerhaps `Optional` status can be revoked. There needs to be logic such that whether there's an empty `mask` or never any mask would look the same. Is there harm to that?\r\n\r\nI'm think so. Null checks can be removed in places like `__iter__()`, but in a `for` loop, is there a way for users to easily tell which fields might suddenly produce values?\r\n\r\n### Essential Changes\r\nI advocate for these two changes to be made _eventually._\r\n1. `Detections.empty` is meant to construct empty detections for ANY model. Therefore it HAS to be weaker-defined than any model. The code now sets `(xyxy, confidence, class_id)`. Does every model have at least all of these? No. Trim it down until it can be used as the constructor for `Detections`. Otherwise you'll be setting fields that normal model results may never have and constructing invalid objects.\r\n   _After this is done - can `Detections` constructor be used instead of `Detections.empty()`?_\r\n2. Visually separate `tracker_id` from other members. Is is addressed differently, has a special set of methods ensuring its existence and is not produced by the models. A single empty line would give a hint to maintainers of its special status - miniscule thing, but it would help learn the code faster."
      },
      {
        "user": "LinasKo",
        "body": "We've now implemented option 3 in https://github.com/roboflow/supervision/pull/1177\r\n\r\nIdeally, I'd like to see **an attempt at** trimming down `Detections.empty` to just `xyxy` before closing this.\r\n\r\nWould that cause issues? Would that be easy fine? Would that entail too much work to be worth it? I'd like to know that."
      }
    ]
  },
  {
    "issue_number": 1196,
    "title": "Smoother Fails Due to Missing tracker_id in Detections",
    "author": "YoungjaeDev",
    "state": "closed",
    "created_at": "2024-05-14T15:48:32Z",
    "updated_at": "2024-05-20T07:02:40Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nI'm experiencing an issue with the supervision library when using the `smoother` for better visualization. While my model is successfully detecting objects, it seems that tracker_id is not being assigned to the detections (maybe due to threshold). This causes the smoother to fail with the following error:\r\n\r\n```bash\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n```\r\nHere are the details of the error:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"app.py\", line 110, in <module>\r\n    main()\r\n  File \"app.py\", line 104, in main\r\n    process_video((video_path, xml_dir_path, save_video_dir_path, use_cut_video, parse_gt))\r\n  File \"app.py\", line 51, in process_video\r\n    results = KisaModel.run(frame, frame_number)\r\n  File \"/home/aicads/workspace/kisa_cctv_cert/models/model.py\", line 67, in run\r\n    results[key] = tracker.process(frame_number, detections)\r\n  File \"/home/aicads/workspace/kisa_cctv_cert/models/intrusion.py\", line 41, in process\r\n    detections_filtered = self.smoother.update_with_detections(detections=detections_filtered)\r\n  File \"/home/aicads/miniconda3/envs/kisa_cert/lib/python3.8/site-packages/supervision/detection/tools/smoother.py\", line 80, in update_with_detections\r\n    tracker_id = detections.tracker_id[detection_idx]\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n```\r\n\r\nIt appears that the tracker_id is not being generated, which leads to the smoother failing. Shouldn't there be an exception handling or a check to ensure that tracker_id is available before attempting to use it?\r\n\r\nHere is a snippet of the detections object:\r\n\r\n```bash\r\nDetections(xyxy=array([[1256.5, 72.485, 1279.9, 126.03]], dtype=float32), mask=None, confidence=array([0.42017], dtype=float32), class_id=array([0]), tracker_id=array([], dtype=int64), data={'class_name': array(['person'], dtype='<U6')})\r\n```\r\nAs you can see, the tracker_id array is empty.\r\n\r\nPlease advise on how to handle this situation or if there is a bug fix in progress for this issue.\r\n\r\n### Environment\r\n\r\nSupervision 0.20.0\r\n\r\n### Minimal Reproducible Example\r\n\r\n_No response_\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "YoungjaeDev",
        "body": "It's not experiencing the issue with **_supervision 0.19.0_**, so which one is the bug?"
      },
      {
        "user": "SkalskiP",
        "body": "Hi, @YoungjaeDev üëãüèª do you have a tracker plugged into your pipeline?"
      },
      {
        "user": "YoungjaeDev",
        "body": "> Hi, @YoungjaeDev üëãüèª do you have a tracker plugged into your pipeline?\r\n\r\nYes, of course I'm using it. It's the same code, it works for 0.19.0, but not 0.20.0."
      }
    ]
  },
  {
    "issue_number": 1195,
    "title": "Saving Detections - Possible Issue?",
    "author": "n0012",
    "state": "closed",
    "created_at": "2024-05-14T14:58:00Z",
    "updated_at": "2024-05-16T19:04:57Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Question\r\n\r\nMy supervision pipeline is saving detection metadata, per these [instructions](https://supervision.roboflow.com/0.20.0/detection/tools/save_detections/\r\n). However, I'm seeing some possible misalignment between the tracker_id part of the detections dict and the custom_data attributes written to each JSON dictionary value.\r\n\r\njson_sink.append(detections, entry)\r\n\r\nSee below for the tracker_id which comes from the detections object versus the custom data (entry dictionary).\r\n\r\nI re-added tracker_id_2 within entry to be sure the custom values align with the correct detection, and I see some cases where they do not.\r\n\r\nAttached is my script, which is based on the Supervision Speed Estimation [example](https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/ultralytics_example.py). Please provide guidance if you see any issues with how I'm handling the JSON export.\r\n\r\nThe video I'm processing detects many vehicles simultaneously, so it's critical for the detection metadata to align correctly with the respective vehicle detections within the JSON output.\r\n\r\n[\r\n    {\r\n        \"x_min\": 708.8834228515625,\r\n        \"y_min\": 360.1021728515625,\r\n        \"x_max\": 782.1868896484375,\r\n        \"y_max\": 417.46990966796875,\r\n        \"class_id\": 2,\r\n        \"confidence\": 0.8593592047691345,\r\n        \"tracker_id\": 3,\r\n        \"class_name\": \"car\",\r\n        \"video_file\": \"video_segment.mp4\",\r\n        \"tracker_id_2\": \"2\",\r\n        \"coordinate_start\": \"79\",\r\n        \"coordinate_end\": \"113\",\r\n        \"distance_in_feet\": \"34\",\r\n        \"time\": \"0.5\",\r\n        \"detection_time\": \"2024-05-11T15:39:57.833333\",\r\n        \"speed_mph\": \"46.363624\"\r\n    },\r\n    {\r\n        \"x_min\": 683.052490234375,\r\n        \"y_min\": 223.6722412109375,\r\n        \"x_max\": 727.069091796875,\r\n        \"y_max\": 251.96783447265625,\r\n        \"class_id\": 2,\r\n        \"confidence\": 0.8138282299041748,\r\n        \"tracker_id\": 2,\r\n        \"class_name\": \"car\",\r\n        \"video_file\": \"video_segment.mp4\",\r\n        \"tracker_id_2\": \"2\",\r\n        \"coordinate_start\": \"79\",\r\n        \"coordinate_end\": \"113\",\r\n        \"distance_in_feet\": \"34\",\r\n        \"time\": \"0.5\",\r\n        \"detection_time\": \"2024-05-11T15:39:57.833333\",\r\n        \"speed_mph\": \"46.363624\"\r\n    }\r\n]\r\n\r\n```\r\nimport argparse\r\nfrom collections import defaultdict, deque\r\nfrom distutils.util import strtobool\r\nimport os \r\nimport datetime\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\n\r\nimport supervision as sv\r\n\r\nSOURCE = np.array([[722,201], [964,232], [876,524], [183,411]])\r\n\r\nTARGET_WIDTH = 72\r\nTARGET_HEIGHT = 200\r\n\r\nTARGET = np.array(\r\n    [\r\n        [0, 0],\r\n        [TARGET_WIDTH - 1, 0],\r\n        [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\r\n        [0, TARGET_HEIGHT - 1],\r\n    ]\r\n)\r\n\r\n\r\nclass ViewTransformer:\r\n    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\r\n        source = source.astype(np.float32)\r\n        target = target.astype(np.float32)\r\n        self.m = cv2.getPerspectiveTransform(source, target)\r\n\r\n    def transform_points(self, points: np.ndarray) -> np.ndarray:\r\n        if points.size == 0:\r\n            return points\r\n\r\n        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\r\n        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\r\n        return transformed_points.reshape(-1, 2)\r\n\r\n\r\ndef parse_arguments() -> argparse.Namespace:\r\n    parser = argparse.ArgumentParser(\r\n        description=\"Vehicle Speed Estimation using Ultralytics and Supervision\"\r\n    )\r\n    parser.add_argument(\r\n        \"--source_video_path\",\r\n        required=True,\r\n        help=\"Path to the source video file\",\r\n        type=str,\r\n    )\r\n    parser.add_argument(\r\n        \"--target_video_path\",\r\n        required=True,\r\n        help=\"Path to the target video file (output)\",\r\n        type=str,\r\n    )\r\n    parser.add_argument(\r\n        \"--confidence_threshold\",\r\n        default=0.3,\r\n        help=\"Confidence threshold for the model\",\r\n        type=float,\r\n    )\r\n    parser.add_argument(\r\n        \"--iou_threshold\", default=0.7, help=\"IOU threshold for the model\", type=float\r\n    )\r\n    parser.add_argument(\r\n        \"--inference_device\",\r\n        required=False,\r\n        default=\"cpu\",\r\n        help=\"Inference Device, cpu Default, mps on apple silicon mac for example\",\r\n        type=str,\r\n    )\r\n    parser.add_argument(\r\n        \"--verbose_logging\",\r\n        type=lambda x: bool(strtobool(x)),\r\n        default=False,\r\n        help=\"Show verbose inference output\",\r\n    )\r\n\r\n    return parser.parse_args()\r\n\r\ndef parse_video_timestamp(filename):\r\n    timestamp_str = filename.split(\"_\")[-1].split(\".\")[0]  # This assumes the format is always as expected\r\n    return datetime.datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\r\n\r\ndef calculate_detection_timestamp(video_end_time, video_duration, frame_number, fps):\r\n    # Calculate how many seconds before the end time this frame was taken\r\n    seconds_from_end = video_duration - frame_number / fps\r\n    detection_time = video_end_time - datetime.timedelta(seconds=seconds_from_end)\r\n    return detection_time\r\n\r\nif __name__ == \"__main__\":\r\n    args = parse_arguments()\r\n\r\n    video_info = sv.VideoInfo.from_video_path(video_path=args.source_video_path)\r\n    video_file_name = os.path.basename(args.source_video_path)\r\n    video_file_name_without_ext, _ = os.path.splitext(video_file_name)\r\n    video_end_time = parse_video_timestamp(video_file_name_without_ext)\r\n    \r\n    model = YOLO(\"yolov8x.pt\")\r\n\r\n    byte_track = sv.ByteTrack(\r\n        frame_rate=video_info.fps, track_activation_threshold=args.confidence_threshold\r\n    )\r\n\r\n    thickness = sv.calculate_optimal_line_thickness(\r\n        resolution_wh=video_info.resolution_wh\r\n    )\r\n    text_scale = sv.calculate_optimal_text_scale(resolution_wh=video_info.resolution_wh)\r\n    bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=thickness)\r\n    label_annotator = sv.LabelAnnotator(\r\n        text_scale=text_scale,\r\n        text_thickness=thickness,\r\n        text_position=sv.Position.BOTTOM_CENTER,\r\n    )\r\n    trace_annotator = sv.TraceAnnotator(\r\n        thickness=thickness,\r\n        trace_length=video_info.fps * 2,\r\n        position=sv.Position.BOTTOM_CENTER,\r\n    )\r\n\r\n    frame_generator = sv.get_video_frames_generator(source_path=args.source_video_path)\r\n\r\n    fps = video_info.fps\r\n    total_frames = (video_info.total_frames)\r\n    video_duration = total_frames / fps\r\n\r\n    polygon_zone = sv.PolygonZone(polygon=SOURCE)\r\n    view_transformer = ViewTransformer(source=SOURCE, target=TARGET)\r\n\r\n    coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\r\n\r\n    frame_number = 0\r\n\r\n    with sv.VideoSink(args.target_video_path, video_info) as sink:\r\n        with sv.JSONSink(\"output/test.json\") as json_sink:\r\n            for frame in frame_generator:\r\n                frame_number += 1\r\n\r\n                detection_time = calculate_detection_timestamp(video_end_time, video_duration, frame_number, fps)\r\n                detection_time_str = detection_time.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n\r\n                #result = model(frame)[0]\r\n                result = model(frame, device=args.inference_device, verbose=args.verbose_logging)[0]\r\n                detections = sv.Detections.from_ultralytics(result)\r\n                detections = detections[detections.confidence > args.confidence_threshold]\r\n                detections = detections[polygon_zone.trigger(detections)]\r\n                detections = detections.with_nms(threshold=args.iou_threshold)\r\n                detections = byte_track.update_with_detections(detections=detections)\r\n\r\n                points = detections.get_anchors_coordinates(\r\n                    anchor=sv.Position.BOTTOM_CENTER\r\n                )\r\n                points = view_transformer.transform_points(points=points).astype(int)\r\n\r\n                for tracker_id, [_, y] in zip(detections.tracker_id, points):\r\n                    coordinates[tracker_id].append(y)\r\n\r\n                labels = []\r\n                for tracker_id in detections.tracker_id:\r\n\r\n                    time = len(coordinates[tracker_id]) / video_info.fps\r\n                    coordinate_start = coordinates[tracker_id][-1]\r\n                    coordinate_end = coordinates[tracker_id][0]\r\n\r\n                    distance_in_feet = abs(coordinate_start - coordinate_end)\r\n\r\n                    if len(coordinates[tracker_id]) < video_info.fps / 2:\r\n                        labels.append(f\"#{tracker_id}\")\r\n                    else:\r\n                        coordinate_start = coordinates[tracker_id][-1]\r\n                        coordinate_end = coordinates[tracker_id][0]\r\n                        distance = abs(coordinate_start - coordinate_end)\r\n                        time = len(coordinates[tracker_id]) / video_info.fps\r\n                        #speed = distance / time * 3.6\r\n                        #labels.append(f\"#{tracker_id} {int(speed)} km/h\")\r\n                        # Calculate speed in feet per second\r\n                        speed_fps = distance_in_feet / time\r\n                        # Convert speed to mph (1 fps = 0.681818 mph)\r\n                        speed = speed_fps * 0.681818\r\n                        labels.append(f\"#{tracker_id} {int(speed)} mph\")\r\n                        entry = {\r\n                                \"video_file\": video_file_name,\r\n                                \"tracker_id_2\": str(tracker_id),\r\n                                \"coordinate_start\": str(coordinate_start),\r\n                                \"coordinate_end\": str(coordinate_end),\r\n                                \"distance_in_feet\": str(distance_in_feet),\r\n                                \"time\": str(time),\r\n                                \"detection_time\": detection_time.isoformat(),  # Store as ISO 8601 string\r\n                                \"speed_mph\": str(speed)\r\n                            }\r\n                        json_sink.append(detections, entry)\r\n\r\n                annotated_frame = frame.copy()\r\n                annotated_frame = trace_annotator.annotate(\r\n                    scene=annotated_frame, detections=detections\r\n                )\r\n                annotated_frame = bounding_box_annotator.annotate(\r\n                    scene=annotated_frame, detections=detections\r\n                )\r\n                annotated_frame = label_annotator.annotate(\r\n                    scene=annotated_frame, detections=detections, labels=labels\r\n                )\r\n\r\n                sink.write_frame(annotated_frame)\r\n                #cv2.imshow(\"frame\", annotated_frame)\r\n                #if cv2.waitKey(1) & 0xFF == ord(\"q\"):\r\n                #    break\r\n        cv2.destroyAllWindows()\r\n```\r\n\r\n\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @n0012 üëãüèª `json_sink.append` should be called only once for each `detections` object. The problem is that you call `json_sink.append` while looping over `detections.tracker_id`.\r\n\r\n```python\r\n...\r\nfor tracker_id in detections.tracker_id:\r\n   ...\r\n   json_sink.append(detections, entry)\r\n   ...\r\n```\r\n\r\nIf you do it like this you effectively save all your detections multiple times into same csv file. Try this:\r\n\r\n```python\r\n...\r\nfor tracker_id in detections.tracker_id:\r\n   ...\r\n   json_sink.append(detections[detections.tracker_id == tracker_id], entry)\r\n   ...\r\n```"
      },
      {
        "user": "n0012",
        "body": "Thank you, that helps explain it!  I'll give this a try and appreciate the support."
      },
      {
        "user": "SkalskiP",
        "body": "No worries ;) I'm happy to help. I'm closing the issue, but if you have more questions, don't hesitate to reach out."
      }
    ]
  },
  {
    "issue_number": 1194,
    "title": "small objects detection using sahi or combine it with other models ",
    "author": "merbenf",
    "state": "closed",
    "created_at": "2024-05-14T08:51:51Z",
    "updated_at": "2024-05-14T20:45:55Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\n i just wanted to ask for \"sahi\" model wich is used for small objects detection how can i use it in inference with faster r cnn and if there any other models are used for very small (tiny) objects to be used .\r\nthank you all .\n\n### Additional\n\ni already use it with yolov5 and know i wanna test with other models such as faster r cnn to reach the best performance ",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @merbenf üëãüèª We have a whole docs page dedicated to SAHI with usage examples: https://supervision.roboflow.com/develop/how_to/detect_small_objects/"
      },
      {
        "user": "SkalskiP",
        "body": "I'll convert this issue into a discussion and put it into the Q&A section."
      }
    ]
  },
  {
    "issue_number": 1181,
    "title": " Request: as_coco Parameter Adjustment for sv.DetectionDataset in Supervision API",
    "author": "YoungjaeDev",
    "state": "open",
    "created_at": "2024-05-09T04:53:51Z",
    "updated_at": "2024-05-14T11:00:28Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nI am currently utilizing the `as_coco` function from **_sv.DetectionDataset_** within the Supervision API extensively and find it incredibly useful. However, I've encountered an issue when importing these labels into CVAT label tool. It seems that CVAT accepts **category_id starting from 1**, whereas the as_coco output starts with category_id at 0. This discrepancy leads to import failures.\r\n\r\nWould it be possible to add a parameter to the as_coco function to adjust the starting index of category_id? This feature would greatly enhance compatibility with CVAT and streamline the workflow for users facing similar issues.\r\n\r\nThank you for considering this enhancement. I look forward to any updates you might have on this request.\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @YoungjaeDev üëãüèª does CVAT requires only `category_id` to start from 1 or is it also `image_id`?"
      },
      {
        "user": "YoungjaeDev",
        "body": "@SkalskiP \nJust category_id. Image_id had no problem for importing data"
      },
      {
        "user": "YoungjaeDev",
        "body": "@SkalskiP \r\n\r\ncvat.apps.dataset_manager.bindings.CvatImportError: Image C00_012_0007_000000: can't import annotation #0 (bbox): annotation has no label.\r\n\r\nThis is error log "
      }
    ]
  },
  {
    "issue_number": 1190,
    "title": "the update of supervision",
    "author": "OliverLam7725",
    "state": "closed",
    "created_at": "2024-05-13T03:47:03Z",
    "updated_at": "2024-05-13T10:43:23Z",
    "labels": [
      "question"
    ],
    "body": null,
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @OliverLam7725 üëãüèª I don't understand. Could you be a bit more specific? :) What do you mean by `if you updated the items in supervision folder, please update the code as well`?"
      },
      {
        "user": "OliverLam7725",
        "body": "my bad, I misunderstand it. Please delete this form thx!"
      },
      {
        "user": "SkalskiP",
        "body": "@OliverLam7725 no worries ;)"
      }
    ]
  },
  {
    "issue_number": 1186,
    "title": "I want to show zone_annotator all the time",
    "author": "REZIZ-TER",
    "state": "closed",
    "created_at": "2024-05-10T07:41:46Z",
    "updated_at": "2024-05-10T08:02:48Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Question\r\n\r\n**Now I have a problem. zone_annotator It is displayed only when an object is detected. But if there is no object in zone_annotator It'll disappear but that's not what I want. I want it to display all the time.**\r\n\r\n``` python\r\nimport numpy as np\r\nimport supervision as sv\r\nfrom ultralytics import YOLO\r\nimport argparse\r\nimport numpy as np\r\nfrom inference.models.utils import get_roboflow_model\r\n\r\n\r\nmodel_path = \"D:\\\\Private\\\\Y3Project\\\\python_project\\\\Weights\\\\w2024-04-27\\\\best.pt\"\r\nmodel = YOLO(model_path)\r\ntracker = sv.ByteTrack()\r\nbox_annotator = sv.BoundingBoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator()\r\n\r\nZONE_POLYGON = np.array([\r\n    [0.1, 0.1],\r\n    [0.9, 0.1],\r\n    [0.9, 0.9],\r\n    [0.1, 0.9],\r\n    [0.1, 0.1]\r\n])\r\n\r\ndef parse_arguments() -> argparse.Namespace:\r\n    parser = argparse.ArgumentParser(description=\"YOLOv8 live\")\r\n    parser.add_argument(\r\n        \"--webcam-resolution\",\r\n        default=[1280, 720],\r\n        nargs=2,\r\n        type=int\r\n    )\r\n    args = parser.parse_args()\r\n    return args\r\n\r\nargs = parse_arguments()\r\nzone_polygon = (ZONE_POLYGON * np.array(args.webcam_resolution)).astype(int)\r\n\r\nzone = sv.PolygonZone(polygon=zone_polygon, frame_resolution_wh=tuple(args.webcam_resolution))\r\nzone_annotator = sv.PolygonZoneAnnotator(\r\n    zone=zone,\r\n    color=sv.Color.RED,\r\n    text_color=sv.Color.WHITE,\r\n    thickness=1,\r\n    text_thickness=1,\r\n    text_scale=1\r\n)\r\n\r\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\r\n\r\n    results = model(frame)[0]\r\n    detections = sv.Detections.from_ultralytics(results)\r\n    detections = tracker.update_with_detections(detections)\r\n\r\n    if len(detections) > 0 and len(detections.class_id) == len(detections.tracker_id):\r\n        labels = [\r\n            f\"#{tracker_id} {results.names[class_id]}\"\r\n            for class_id, tracker_id\r\n            in zip(detections.class_id, detections.tracker_id)\r\n            ]\r\n\r\n        annotated_frame = box_annotator.annotate(frame.copy(), detections=detections)\r\n        annotated_frame = zone_annotator.annotate(annotated_frame.copy())\r\n        label_annotators = label_annotator.annotate(annotated_frame.copy(), detections=detections, labels=labels)\r\n        mask = zone.trigger(detections=detections)\r\n        return label_annotators\r\n        \r\n    else:\r\n        return frame\r\n    \r\nmyvid=\"D:\\\\Private\\Y3Project\\\\Filter_Pic\\\\7755db6e-8cba-4e35-a645-621b2e813b60.mp4\"\r\nsv.process_video(\r\n    source_path=myvid4,\r\n    target_path=\"D:\\\\Private\\\\Y3Project\\\\python_project\\\\runs\\\\myresult\\\\myvid.mp4\",\r\n    callback=callback\r\n)\r\n```\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": []
  },
  {
    "issue_number": 765,
    "title": "mmdetection instance segmentation inference",
    "author": "DrawingProcess",
    "state": "closed",
    "created_at": "2024-01-22T21:59:19Z",
    "updated_at": "2024-05-09T11:19:42Z",
    "labels": [
      "enhancement",
      "api:detection"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nmmdetection is detection and 'instance segmentation' framework. \r\nso, I think If there is a mask area, I think it should be return.\r\n\r\nlike below\r\n```\r\nreturn cls(\r\n            xyxy=mmdet_results.pred_instances.bboxes.cpu().numpy(),\r\n            mask=mmdet_results.pred_instances.masks.cpu().numpy(),\r\n            confidence=mmdet_results.pred_instances.scores.cpu().numpy(),\r\n            class_id=mmdet_results.pred_instances.labels.cpu().numpy().astype(int),\r\n        )\r\n```\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @DrawingProcess üëãüèª Thanks a lot for your interest in supervision. I'd love to bring support for MMDetection segmentation models to Supervision.\r\n\r\nI see you checked `Yes, I'd like to help by submitting a PR!`. Would you like to help us and add this feature? "
      },
      {
        "user": "DrawingProcess",
        "body": "Thank you for your quick reply. I sent you PR. Please check it.\r\nSee #767 "
      },
      {
        "user": "SkalskiP",
        "body": "This issue was solved with https://github.com/roboflow/supervision/pull/767. Closing the issue."
      }
    ]
  },
  {
    "issue_number": 1182,
    "title": "HaloAnnotator does not work",
    "author": "wilsonlv",
    "state": "closed",
    "created_at": "2024-05-09T07:33:38Z",
    "updated_at": "2024-05-09T08:11:32Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\ncan you give me a complete code? when i use HaloAnnotator, nothing changes in the image, the detections.mask is None\r\n\r\n```python\r\nimport cv2\r\nimport supervision as sv\r\nfrom ultralytics import YOLO\r\n\r\nfilename = '../src/static/dog.png'\r\nimage = cv2.imread(filename)\r\n\r\nmodel = YOLO('../models/yolov8x.pt')\r\nresults = model(image)[0]\r\n\r\ndetections = sv.Detections.from_ultralytics(results)\r\n\r\nannotated_image = sv.HaloAnnotator().annotate(\r\n    scene=image.copy(), detections=detections)\r\n\r\nsv.plot_image(annotated_image)\r\n\r\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @wilsonlv :wave:\r\n\r\nIt's very useful that you checked the `mask` value.\r\n\r\nTo use `HaloAnnotator` you need a segmentation model. For example:\r\n```python\r\nmodel = YOLO('yolov8x-seg')\r\n```"
      }
    ]
  },
  {
    "issue_number": 1120,
    "title": "the result.mp4 cannot be played on the browser",
    "author": "wilsonlv",
    "state": "closed",
    "created_at": "2024-04-17T00:42:45Z",
    "updated_at": "2024-05-08T14:47:13Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\ni dont know why the result.mp4 is much large than the original, and it cannot be played on the browser, the code is as following\r\n\r\n```\r\nimport numpy as np\r\nimport supervision as sv\r\nfrom ultralytics import YOLO\r\n\r\nmodel = YOLO(\"yolov8n.pt\")\r\ntracker = sv.ByteTrack()\r\nbox_annotator = sv.BoundingBoxAnnotator()\r\nlabel_annotator = sv.LabelAnnotator()\r\ntrace_annotator = sv.TraceAnnotator()\r\n\r\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\r\n    results = model(frame)[0]\r\n    detections = sv.Detections.from_ultralytics(results)\r\n    detections = tracker.update_with_detections(detections)\r\n\r\n    labels = [\r\n        f\"#{tracker_id} {results.names[class_id]}\"\r\n        for class_id, tracker_id\r\n        in zip(detections.class_id, detections.tracker_id)\r\n    ]\r\n\r\n    annotated_frame = box_annotator.annotate(\r\n        frame.copy(), detections=detections)\r\n    annotated_frame = label_annotator.annotate(\r\n        annotated_frame, detections=detections, labels=labels)\r\n    return trace_annotator.annotate(\r\n        annotated_frame, detections=detections)\r\n\r\nsv.process_video(\r\n    source_path=\"people-walking.mp4\",\r\n    target_path=\"result.mp4\",\r\n    callback=callback\r\n)\r\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @wilsonlv :wave:\r\n\r\nApologies for the late response and thank you for the report!\r\nI've tested it, and yes, I can reproduce the issue.\r\n\r\nThe reason is that by default, we encode the output with the `mp4v` codec, whereas the input file you likely used was coded with `h264`. While the latter takes up less space, the former works more reliably with OpenCV.\r\n\r\nI'll make a PR shortly, adding codec selection to `sv.process_video`. Although my tests show that the setup isn't trivial - OpenCV _really_ doesn't like telling when it doesn't support the codec.\r\n\r\nIn the meantime, here's a command to convert to `h264`.\r\n```bash\r\nffmpeg -i input-any.mp4 -c:v libx264 output-h264.mp4\r\n```\r\n\r\nHope this helps :wink:\r\n \r\n"
      },
      {
        "user": "LinasKo",
        "body": "Related PR\r\n * #1126 "
      },
      {
        "user": "wilsonlv",
        "body": "@LinasKo  Appreciate for your response, thank you !"
      }
    ]
  },
  {
    "issue_number": 1164,
    "title": "ByteTrack memory consumption increases indefinitely",
    "author": "tc360950",
    "state": "closed",
    "created_at": "2024-05-05T14:54:37Z",
    "updated_at": "2024-05-08T12:35:23Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nByteTrack stores removed tracklets in a list, which is extended with new removed tracklets after every pass (line 489 in [byte_tracker/core.py](https://github.com/roboflow/supervision/blob/develop/supervision/tracker/byte_tracker/core.py#L489):\r\n```\r\nself.lost_tracks = sub_tracks(self.lost_tracks, self.removed_tracks)\r\nself.removed_tracks.extend(removed_stracks)\r\n```\r\nAs a result, size of `removed_tracks` collection grows unbounded and computational cost of \r\n`sub_tracks(self.lost_tracks, self.removed_tracks)` increases with every pass. \r\nThe fix is simple - we only need to keep latest (i.e. the one from the last pass) removed tracklets in the collection - I've submitted a corresponding PR. \r\n\r\n\r\n\r\n        \r\n        \r\n\r\n### Environment\r\n\r\nNot applicable\r\n\r\n### Minimal Reproducible Example\r\n\r\nThis simple script shows that update efficiency decreases steadily:\r\n```\r\nimport random\r\nimport time\r\nfrom typing import Generator\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nfrom supervision import ByteTrack, Detections\r\n\r\nbyte_track = ByteTrack(\r\n    track_activation_threshold=0.25,\r\n    lost_track_buffer=30,\r\n    minimum_matching_threshold=0.8,\r\n    frame_rate=30,\r\n    minimum_consecutive_frames=2,\r\n)\r\n\r\n\r\ndef get_detections() -> Generator[Detections, None, None]:\r\n    start_bboxes = [\r\n        [100, 100, 150, 200],\r\n        [200, 100, 250, 200],\r\n        [400, 100, 450, 200],\r\n        [500, 100, 550, 200],\r\n        [600, 100, 650, 200],\r\n    ]\r\n    bbox_count = len(start_bboxes)\r\n    while True:\r\n        bboxes = np.array(start_bboxes.copy()).reshape((-1, 4))\r\n        for i in range(0, 10):\r\n            for j in range(0, bbox_count):\r\n                bboxes[j][0] -= i * 10\r\n                bboxes[j][2] -= i * 10\r\n            yield Detections(\r\n                xyxy=bboxes,\r\n                confidence=np.array([random.random() for _ in range(0, bbox_count)]),\r\n                class_id=np.array([0 for _ in range(0, bbox_count)]),\r\n            )\r\n\r\n\r\nupdate_times = []\r\nremoved_tracks_count = []\r\ndetections_generator = get_detections()\r\nfor _ in range(0, 50000):\r\n    detections = next(detections_generator)\r\n    start = time.time()\r\n    byte_track.update_with_detections(detections)\r\n    end = time.time()\r\n    removed_tracks_count.append(len(byte_track.removed_tracks))\r\n    update_times.append(end - start)\r\n\r\n\r\nplt.plot(range(0, len(update_times)), update_times)\r\nplt.savefig(\"update_times.png\")\r\nplt.plot(range(0, len(removed_tracks_count)), removed_tracks_count)\r\nplt.savefig(\"removed_tracks.png\")\r\n```\r\n\r\n![image](https://github.com/roboflow/supervision/assets/37670208/bf2a8d91-efc8-4603-8d16-ee618ba80606)\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi, @tc360950! üëãüèª I completely agree that the problem exists. Why do you think storing values from the last pass is enough?"
      },
      {
        "user": "tc360950",
        "body": "Hi @SkalskiP, `removed_tracks` collection is only used on line [489](https://github.com/roboflow/supervision/blob/develop/supervision/tracker/byte_tracker/core.py#L489)\r\n```\r\nself.lost_tracks = sub_tracks(self.lost_tracks, self.removed_tracks)\r\n```\r\nwhere it's used to get rid of lost tracks which have been marked for removal (i.e. those which have not been matched to a detection for a predefined time). Since tracks marked for the removal in the latest pass are added to `removed_tracks` collection on line [490](https://github.com/roboflow/supervision/blob/develop/supervision/tracker/byte_tracker/core.py#L490)\r\n```\r\nself.removed_tracks.extend(removed_stracks)\r\n```\r\nlost tracks which were removed on pass no. N will actually be removed (i.e. moved to `removed_tracks` collection) after pass no. N  + 1 (which btw is another error, although without any dire consequences). This implies that we must at least do (which is what I propose):\r\n``` \r\nself.removed_tracks = removed_stracks\r\n```\r\nNow, as to why this is enough. Once a track lands in `removed_tracks` it does not get out anywhere. I.e. once track is removed from lost_tracks on line 489, it will not reappear there. As a result we do not need to keep whole history, only most recently removed tracks are needed to \"sanitize\" `lost_tracks` collection. \r\n\r\nI agree that it's a risky change, since there are no unit tests for ByteTrack (I can of course come up with some scripts which compare results from both versions on hand crafted or random data (which I did before raising this issue) but it still seems \"not quite right\"). There is an open [PR](https://github.com/roboflow/supervision/pull/1077) with unit tests for ByteTrack. I have not looked at it but maybe that's the best way to introduce this change - add good unit tests for ByteTrack.  \r\n\r\n"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @tc360950 üëãüèª Thank you for explaining your thought process. The ByteTrack code was transferred to Supervision (with minor changes) from another codebase, which is why it slightly deviates from the writing style we have in Supervision and, as you rightly observed, is not tested at all. Thanks a lot for the analysis you conducted."
      }
    ]
  },
  {
    "issue_number": 452,
    "title": "Create sv.Keypoint API",
    "author": "capjamesg",
    "state": "closed",
    "created_at": "2023-10-11T08:41:46Z",
    "updated_at": "2024-05-07T10:38:51Z",
    "labels": [
      "enhancement",
      "api:annotator"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nI would love to see a `sv.Keypoint` API that allows manipulating keypoint detections with supervision.\r\n\r\nTo start, it would be nice to have a `sv.Keypoint` object that contains:\r\n\r\n1. Keypoint vectors\r\n2. Confidence levels for each keypoint vector\r\n\r\nA `sv.Keypoint.from_ultralytics()` method would be helpful as a starting point; we can add more data loaders later. With a data loader, we can add a `sv.KeypointAnnotator` that lets someone visualize keypoint detections.\r\n\r\nEventually, it would be useful to have a method for calculating the angle between two keypoints, useful for comparing poses ([example implementation in JavaScript](https://github.com/capjamesg/hand-pose-detection/blob/main/pose.html#L79)).\n\n### Use case\n\nWe want to make it as easy as possible for people to work with keypoints.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "capjamesg",
        "body": "The API would look something like this (pseudocode below):\r\n\r\n```python\r\n\r\n@dataclass\r\nclass Keypoints:\r\n    keypoints: np.ndarray\r\n    confidence: Optional[np.ndarray] = None\r\n\r\n    def __post_init__(self) -> None:\r\n        \"\"\"\r\n        Validate the keypoints inputs.\r\n        \"\"\"\r\n        n = len(self.class_id)\r\n\r\n        # all keypoints must be same size\r\n        # there must be a confidence for each keypoint\r\n        # each keypoint must contain exactly two elements (keypoints should have the structure [x, y])\r\n        # allow a user to create an empty keypoint with sv.Keypoint.empty()\r\n        \r\n    @classmethod\r\n    def from_ultralytics(cls, ultralytics_results) -> Keypoints:\r\n       pass\r\n\r\n    def calculate_angle(self, k: int) -> int:\r\n        # calculate the angle between two keypoints\r\n        pass\r\n```"
      },
      {
        "user": "capjamesg",
        "body": "For annotating, we would have two annotators:\r\n\r\n1. `PoseAnnotator` for pose estimation.\r\n2. `FaceLandmarkAnnotator` for facial landmarks.\r\n\r\nHere is an example API structure for the `PoseAnnotator`:\r\n\r\n```python\r\nclass PoseAnnotator(BaseAnnotator):\r\n    \"\"\"\r\n    A class for drawing pose keypoints.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\r\n        thickness: int = 2,\r\n        color_map: str = \"class\",\r\n    ):\r\n        \"\"\"\r\n        Args:\r\n            color (Union[Color, ColorPalette]): The color or color palette to use for\r\n                annotating detections.\r\n            radius (int): Radius of the keypoints.\r\n            color_map (str): Strategy for mapping colors to annotations.\r\n                Options are `index`, `class`, or `track`.\r\n        \"\"\"\r\n        self.color: Union[Color, ColorPalette] = color\r\n        self.thickness: int = thickness\r\n        self.color_map: ColorMap = ColorMap(color_map)\r\n\r\n    def annotate(self, scene: np.ndarray, keypoints: Keypoints) -> np.ndarray:\r\n        \"\"\"\r\n        Annotates the given scene with bounding boxes based on the provided keypoints.\r\n\r\n        Args:\r\n            scene (np.ndarray): The image where bounding boxes will be drawn.\r\n            keypoints (Keypoints): Keypoints to annotate.\r\n\r\n        Returns:\r\n            np.ndarray: The annotated image.\r\n\r\n        Example:\r\n            # TODO\r\n        \"\"\"\r\n        # draw circular keypoints on the image at the given locations\r\n\r\n        return scene\r\n```"
      },
      {
        "user": "capjamesg",
        "body": "Notes on data loaders that we should prioritize:\r\n\r\n- YOLOv8\r\n- YOLOv7\r\n- MediaPipe"
      }
    ]
  },
  {
    "issue_number": 1139,
    "title": "Filter Detections Error",
    "author": "XiaJunfly",
    "state": "closed",
    "created_at": "2024-04-25T12:17:26Z",
    "updated_at": "2024-05-07T10:37:49Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Question\r\n\r\nI want to track people separately in two areas, but it keeps failing. What's going on?\r\n\r\nIt shows no label in the second box, but there is one person in the box\r\nThe problem is :The number of labels provided (0) does not match the number of detections (1).\r\n![ÂæÆ‰ø°Êà™Âõæ_20240425201701](https://github.com/roboflow/supervision/assets/49782103/bf6e635b-a0c0-4a45-ae8f-3953656a5a7f)\r\n\r\n### Additional\r\n```python\r\n\r\n  import time\r\n  from tqdm import tqdm\r\n  import cv2\r\n  import numpy as np\r\n  from ultralytics import YOLO\r\n  import supervision as sv\r\n  import matplotlib.pyplot as plt\r\n  \r\n  model = YOLO('yolov8x.pt')\r\n  \r\n  VIDEO_PATH = 'people-walking.mp4'\r\n  \r\n  video_info = sv.VideoInfo.from_video_path(VIDEO_PATH)\r\n  \r\n  polygons = [\r\n      np.array([[100, 800], [500, 800],[500, 100], [100, 100]]),\r\n      np.array([[600, 800],[800, 800], [800, 600], [600, 600]])\r\n  ]\r\n  \r\n  zones = [sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh) for polygon in polygons]\r\n  colors = sv.ColorPalette.default()\r\n  zone_annotators = [sv.PolygonZoneAnnotator(zone=zone, color=colors.by_idx(index), thickness=6)\r\n                     for index, zone in enumerate(zones)]\r\n  box_annotators = [sv.BoundingBoxAnnotator(color=colors.by_idx(index), thickness=2)\r\n                    for index in range(len(polygons))]\r\n  tracker = sv.ByteTrack()\r\n  label_annotators = [sv.LabelAnnotator(color=colors.by_idx(index)) for index in range(len(polygons))]\r\n  trace_annotators = [sv.TraceAnnotator(color=colors.by_idx(index)) for index in range(len(polygons))]\r\n  \r\n  def process_frame(frame: np.ndarray, i) -> np.ndarray:\r\n  \r\n      results = model(frame, imgsz=1280, verbose=False, show=False, device='cuda:0')[0]\r\n  \r\n  \r\n      # supervision \r\n      detections = sv.Detections.from_ultralytics(results)\r\n  \r\n  \r\n      for zone, zone_annotator, box_annotator, label_annotator,  trace_annotator in (\r\n              zip(zones, zone_annotators, box_annotators, label_annotators, trace_annotators)):\r\n          print('i', zone, zone_annotator, box_annotator, label_annotator,  trace_annotator)\r\n  \r\n          mask = zone.trigger(detections=detections)\r\n  \r\n          # get target\r\n          detections_filtered = detections[mask]\r\n          print('dection', detections_filtered)\r\n  \r\n  \r\n          detections_filtered = tracker.update_with_detections(detections_filtered)\r\n          print('detections_filtered.tracker_id', detections_filtered.class_id, detections_filtered.tracker_id)\r\n  \r\n          labels = [\r\n              f\"#{tracker_id} {results.names[class_id]}\"\r\n              for class_id, tracker_id\r\n              in zip(detections_filtered.class_id, detections_filtered.tracker_id)\r\n          ]\r\n          print('label', labels)\r\n  \r\n          # box\r\n          frame = box_annotator.annotate(scene=frame.copy(), detections=detections_filtered)\r\n  \r\n          # ID and labels\r\n          frame = label_annotator.annotate(frame, detections=detections_filtered, labels=labels)\r\n  \r\n          # track\r\n          frame = trace_annotator.annotate(frame, detections=detections_filtered)\r\n          cv2.imshow(\"trac\", frame)\r\n          cv2.waitKey(0)\r\n  \r\n          frame = zone_annotator.annotate(scene=frame)\r\n  \r\n      pbar.update(1)\r\n  \r\n      return frame\r\n  \r\n  filehead = VIDEO_PATH.split('/')[-1]\r\n  OUT_PATH = \"out-\" + filehead\r\n  \r\n  with tqdm(total=video_info.total_frames - 1) as pbar:\r\n      sv.process_video(source_path=VIDEO_PATH, target_path=OUT_PATH, callback=process_frame)\r\n\r\n``` \r\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @XiaJunfly üëãüèª Try running this code:\r\n\r\n```python\r\n  import time\r\n  from tqdm import tqdm\r\n  import cv2\r\n  import numpy as np\r\n  from ultralytics import YOLO\r\n  import supervision as sv\r\n  import matplotlib.pyplot as plt\r\n  \r\n  model = YOLO('yolov8x.pt')\r\n  \r\n  VIDEO_PATH = 'people-walking.mp4'\r\n  \r\n  video_info = sv.VideoInfo.from_video_path(VIDEO_PATH)\r\n  \r\n  polygons = [\r\n      np.array([[100, 800], [500, 800],[500, 100], [100, 100]]),\r\n      np.array([[600, 800],[800, 800], [800, 600], [600, 600]])\r\n  ]\r\n  \r\n  zones = [sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh) for polygon in polygons]\r\n  colors = sv.ColorPalette.default()\r\n  zone_annotators = [sv.PolygonZoneAnnotator(zone=zone, color=colors.by_idx(index), thickness=6)\r\n                     for index, zone in enumerate(zones)]\r\n  box_annotators = [sv.BoundingBoxAnnotator(color=colors.by_idx(index), thickness=2)\r\n                    for index in range(len(polygons))]\r\n  tracker = sv.ByteTrack()\r\n  label_annotators = [sv.LabelAnnotator(color=colors.by_idx(index)) for index in range(len(polygons))]\r\n  trace_annotators = [sv.TraceAnnotator(color=colors.by_idx(index)) for index in range(len(polygons))]\r\n  \r\n  def process_frame(frame: np.ndarray, i) -> np.ndarray:\r\n  \r\n      results = model(frame, imgsz=1280, verbose=False, show=False, device='cuda:0')[0]\r\n  \r\n  \r\n      # supervision \r\n      detections = sv.Detections.from_ultralytics(results)\r\n  \r\n      annotated_frame = frame.copy()\r\n      for zone, zone_annotator, box_annotator, label_annotator,  trace_annotator in (\r\n              zip(zones, zone_annotators, box_annotators, label_annotators, trace_annotators)):\r\n          print('i', zone, zone_annotator, box_annotator, label_annotator,  trace_annotator)\r\n  \r\n          mask = zone.trigger(detections=detections)\r\n  \r\n          # get target\r\n          detections_filtered = detections[mask]\r\n          print('dection', detections_filtered)\r\n  \r\n  \r\n          detections_filtered = tracker.update_with_detections(detections_filtered)\r\n          print('detections_filtered.tracker_id', detections_filtered.class_id, detections_filtered.tracker_id)\r\n  \r\n          labels = [\r\n              f\"#{tracker_id} {results.names[class_id]}\"\r\n              for class_id, tracker_id\r\n              in zip(detections_filtered.class_id, detections_filtered.tracker_id)\r\n          ]\r\n          print('label', labels)\r\n  \r\n          # box\r\n          annotated_frame = box_annotator.annotate(annotated_frame, detections=detections_filtered)\r\n  \r\n          # ID and labels\r\n          annotated_frame = label_annotator.annotate(annotated_frame, detections=detections_filtered, labels=labels)\r\n  \r\n          # track\r\n          annotated_frame = trace_annotator.annotate(annotated_frame, detections=detections_filtered)\r\n          cv2.imshow(\"trac\", frame)\r\n          cv2.waitKey(0)\r\n  \r\n          annotated_frame = zone_annotator.annotate(scene=annotated_frame)\r\n  \r\n      pbar.update(1)\r\n  \r\n      return annotated_frame\r\n  \r\n  filehead = VIDEO_PATH.split('/')[-1]\r\n  OUT_PATH = \"out-\" + filehead\r\n  \r\n  with tqdm(total=video_info.total_frames - 1) as pbar:\r\n      sv.process_video(source_path=VIDEO_PATH, target_path=OUT_PATH, callback=process_frame)\r\n```"
      },
      {
        "user": "XiaJunfly",
        "body": "@skylargivens \r\nThank you for your reply. I tried your code,but it shows the same error.\r\n"
      },
      {
        "user": "SkalskiP",
        "body": "@XiaJunfly can you reproduce this bug in google colab? "
      }
    ]
  },
  {
    "issue_number": 824,
    "title": "[KeyPoints] - create `KeyPoints` class and implement `from_ultralytics` method",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-01-31T12:59:17Z",
    "updated_at": "2024-05-07T10:30:52Z",
    "labels": [
      "enhancement",
      "api:keypoints",
      "Q2.2024"
    ],
    "body": "### Description\r\n\r\n- The `KeyPoints` class should be general. Flexible enough to support pose estimation, retina tracking, face landmarks, and other use cases. \r\n- The `KeyPoints` class should contain the following fields:\r\n    - `xy` (`np.ndarray`): An array of shape `(N, K, 2)` containing the key points coordinates.\r\n    - `confidence` (`Optional[np.ndarray`]): An array of shape `(N, K)` containing the confidence scores.\r\n    - `data` (`Dict[str, Union[np.ndarray, List]]`): A dictionary containing additional data where each key is a string representing the data type, and the value is either a NumPy array or a list of corresponding data.\r\n- The code should be placed in `supervision/keypoints/core.py`.\r\n- Create a new KeyPoints section in the documentation.\r\n- Ensure users know the code is still experimental and subject to change.\r\n\r\n### API\r\n\r\n```python\r\n@dataclass\r\nclass KeyPoints:\r\n    xy: np.ndarray\r\n    confidence: Optional[np.ndarray] = None\r\n    data: Dict[str, Union[np.ndarray, List]] = field(default_factory=dict)\r\n\r\n    def __len__(self):\r\n        return len(self.xy)\r\n\r\n    def __eq__(self, other: KeyPoints):\r\n        pass\r\n\r\n    @classmethod\r\n    def from_ultralytics(cls, ultralytics_results) -> KeyPoints:\r\n        pass\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will definitely speed up the review process. Each change must be tested by the reviewer. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "onuralpszr",
        "body": "I like to work in this one @SkalskiP "
      },
      {
        "user": "SkalskiP",
        "body": "@onuralpszr I like this idea a lot. Let's start with only the `KeyPoints` class. No annotators for now. "
      },
      {
        "user": "onuralpszr",
        "body": "> @onuralpszr I like this idea a lot. Let's start with only the `KeyPoints` class. No annotators for now.\r\n\r\nOkay lets do it."
      }
    ]
  },
  {
    "issue_number": 319,
    "title": "Adjust text to diagonal and vertical lines",
    "author": "jcruz-ferreyra",
    "state": "open",
    "created_at": "2023-08-24T18:34:47Z",
    "updated_at": "2024-05-02T16:17:23Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nI've noticed that the LineZoneAnnotator works wonderfully for horizontal lines. However, when it comes to diagonal or vertical lines, I've found that the \"count\" information remains aligned vertically with respect to the center of the LineZone. Additionally, if an horizontal line is drawn laterally-inverted (with the start point to the right and end point to the left), the \"In\" count still appears over the line, even though it should ideally be positioned under it.\r\n\r\nFor a clearer understanding, I've created an illustration comparing the original behavior to the improved version I've developed:\r\n\r\n*Original Behavior:*\r\n\r\n![original](https://github.com/roboflow/supervision/assets/105461618/7dcbba4e-ca10-47ad-943f-13d08f4e7f15)\r\n\r\n*Improved Behavior:*\r\n\r\n![edited](https://github.com/roboflow/supervision/assets/105461618/4e3c1ec4-2050-4364-b9e4-66ee1e350bf4)\r\n\r\nIn my own project, I needed support for different orientations and directions of LineZones. As a result, I forked the repository and developed a modified version of the LineZoneAnnotator class. In this version, text boxes are right-aligned to the end point of the LineZone, while text orientation is aligned with the direction of it.\r\n\r\nI believe these improvements could enhance the usability and flexibility of the LineZoneAnnotator class for users who require support for different line orientations and directions.\r\n\r\nPlease let me know your thoughts on this proposed change. I would be more than happy to collaborate further on refining and integrating these improvements into the main repository.\n\n### Use case\n\n_No response_\n\n### Additional\n\npd: please ignore changes in the image that are not related to this issue, such as removing the circles at the beginning and end of each line.\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "hardikdava",
        "body": "@jcruz-ferreyra Thank you for suggesting new features and taking interest in `supervision`. We are already working on improving annotators and going to introduce new annotators as well. We will consider your thoughts on this."
      },
      {
        "user": "SkalskiP",
        "body": "Hi @jcruz-ferreyra! I love it! üî• At first, I thought the second image was just 'photoshopped'. The fact you made it so configurable blows my mind. We need to have that!\r\n\r\nAs @hardikdava said, we already started working on an updated line zone version. But I think what you made aligns with our goals. \r\n\r\nWe are a little bit overwhelmed by the amount of work right now. Will you give us a few weeks to get back to this conversation? üôèüèª I'm so sorry I'm slowing down the work so much. "
      }
    ]
  },
  {
    "issue_number": 1152,
    "title": "ultralytics_stream_example do not work",
    "author": "tgbaoo",
    "state": "closed",
    "created_at": "2024-04-29T08:29:57Z",
    "updated_at": "2024-04-30T09:17:43Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\r\n\r\n\r\n### Bug\r\n\r\nI have run the ultralytics_stream_example file for time in zone example, but nothing happen shown as result tracking, despite the deprecated decorector and 2 bug:\r\n1. when passing frame.image to the ultralytics to get the result detection, it must be frame[0].image. (fixed)\r\n2. Is when pass detection to custom sink on_prediction method. (Not be fixed yet)\r\n\r\nPlease check them out.\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n_No response_\r\n\r\n### Minimal Reproducible Example\r\n\r\n_No response_\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "LinasKo",
        "body": "Hi @tgbaoo :wave:\r\n\r\nThank you for reporting the issue!\r\n\r\nI see you're willing to contribute a PR to help us out. I'll assign the issue to you - let me know if you need any help :wink: "
      },
      {
        "user": "tgbaoo",
        "body": "@LinasKo Please check my bug and help me with the second bug that need to be fixed!"
      },
      {
        "user": "LinasKo",
        "body": "There's a little bit in your explanation that I don't understand. Could you please share a Colab showing where the error appears?"
      }
    ]
  },
  {
    "issue_number": 1140,
    "title": "TraceAnnotator fails when there are no detections present due to IndexError",
    "author": "mgithinji",
    "state": "closed",
    "created_at": "2024-04-25T12:54:30Z",
    "updated_at": "2024-04-26T16:52:57Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\nI am running the [speed estimation code](https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/inference_example.py) (inference example) on my computer with the `yolov8n-640` model. \r\n\r\nThe code runs fine for 11 seconds, while the model detects the cars within the Polygon Zone. But at the 11th second mark, the model fails to make a detection within the Polygon Zone and this breaks the code, because the TraceAnnotator expects a list of detections but the list is empty at that second. So it fails and raises an IndexError. Here's the error warning:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/morgangithinji/Workspace/tutorials/speed-camera/speed_camera.py\", line 156, in <module>\r\n    annotated_frame = trace_annotator.annotate(\r\n  File \"/Users/morgangithinji/Workspace/tutorials/speed-camera/.venv/lib/python3.10/site-packages/supervision/utils/conversion.py\", line 21, in wrapper\r\n    return annotate_func(self, scene, *args, **kwargs)\r\n  File \"/Users/morgangithinji/Workspace/tutorials/speed-camera/.venv/lib/python3.10/site-packages/supervision/annotators/core.py\", line 1300, in annotate\r\n    tracker_id = int(detections.tracker_id[detection_idx])\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n```\r\n\r\nTo be fair, I'm running the nano model (`yolov8n-640`), so the inaccuracy of this model is causing missed detections when it blips. But even then, the code should still run fine even if there are no detections made for a given second.\r\n\r\nAlso, in my code, I changed the lines of code from the [inference example code](https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/inference_example.py):\r\n\r\n- 47: `default=\"yolov8x-640\"` -> `default=\"yolov8n-640\"`\r\n- 101: `sv.calculate_dynamic_line_thickness` -> `sv.calculate_optimal_line_thickness`\r\n- 104: `sv.calculate_dynamic_text_scale` -> `sv.calculate_optimal_text_scale`\r\n\r\n`dynamic` now causes an error because the function name was changed. You may also want to consider updating the example scripts to reflect this name change in supervision--it's causing errors.\r\n\n\n### Environment\n\npython 3.10.14\r\nsupervision>=0.20.0\r\ntqdm==4.66.1\r\nrequests\r\nultralytics==8.0.237\r\nsuper-gradients==3.5.0\r\ninference==0.9.8\n\n### Minimal Reproducible Example\n\n```\r\npython speed_camera.py --source_video_path ./data/vehicles.mp4 --target_video_path ./output/vehicles-output.mp4 --roboflow_api_key $ROBOFLOW_API_KEY\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @mgithinji üëãüèª, we updated this code yesterday. I'll take a look."
      },
      {
        "user": "dvignacioglobal",
        "body": "Also having this problem.  I noticed that this happens when there is no person being detected in the video.\r\n\r\n> File \"C:\\Users\\Dennis\\miniconda3\\envs\\PCEE\\Lib\\site-packages\\supervision\\detection\\tools\\smoother.py\", line 80, in update_with_detections\r\n>     tracker_id = detections.tracker_id[detection_idx]\r\n>                  ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\r\n> IndexError: index 0 is out of bounds for axis 0 with size 0"
      },
      {
        "user": "SkalskiP",
        "body": "Hi @mgithinji üëãüèª, I managed to reproduce the bug. I'll do my best to solve this problem. "
      }
    ]
  },
  {
    "issue_number": 1144,
    "title": "Multi-can tracking",
    "author": "Vdol22",
    "state": "closed",
    "created_at": "2024-04-26T11:54:47Z",
    "updated_at": "2024-04-26T12:09:35Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nI have 6 cams connected in a hallway and my task is to track and count people walking in it (there are always many people there), yet I do not understand how I can produce inference on a multiple cameras AND have the same IDs of people from cam1 to cam2,3...6. I use ultralytics for detection and tried their multi-streaming guide, yet if 1 camera catches a frame without objects - it shuts down. Is there any other way to run inference on multiple cameras or am i missing something? Please help.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @Vdol22 üëãüèª Let me start by converting this issue into discussion and moving it to Q&A section."
      }
    ]
  },
  {
    "issue_number": 1143,
    "title": "I want to filter detections so that only the classes \"No Helmet\", \"Person\", \"Rider\" are detected.",
    "author": "REZIZ-TER",
    "state": "closed",
    "created_at": "2024-04-26T11:21:30Z",
    "updated_at": "2024-04-26T12:03:17Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Question\r\n\r\nMy model has 4 classes: `\"No Helmet\", \"Person\", \"Rider\", \"Wear a helmet\"`\r\n**I want to filter detections so that only the classes \"No Helmet\", \"Person\", \"Rider\" are detected. How do I edit or add code?**\r\n\r\nI set selected_classes = [0, 1, 2] to be \"No Helmet\", \"Person\", \"Rider\" \r\nby following the example of Supervision:\r\n```python\r\nimport numpy as np\r\nimport supervision as s\r\nselected_classes = [0, 2, 3]\r\ndetections = sv.Detections(...)\r\ndetections = detections[np.isin(detections.class_id, selected_classes)]\r\n```\r\nand found error\r\n```\r\nTraceback (most recent call last):\r\n   File \"d:\\Private\\Y3Project\\python_project\\PyCode\\preath.py\", line 187, in <module>\r\n     main()\r\n   File \"d:\\Private\\Y3Project\\python_project\\PyCode\\preath.py\", line 143, in main\r\n     mask = zone.trigger(detections=detections)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File \"D:\\anaconda3\\envs\\myenv\\Lib\\site-packages\\supervision\\detection\\tools\\polygon_zone.py\", line 82, in trigger\r\n     clipped_detections = replace(detections, xyxy=clipped_xyxy)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File \"D:\\anaconda3\\envs\\myenv\\Lib\\dataclasses.py\", line 1501, in replace\r\n     return obj.__class__(**changes)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File \"<string>\", line 9, in __init__\r\n   File \"D:\\anaconda3\\envs\\myenv\\Lib\\site-packages\\supervision\\detection\\core.py\", line 120, in __post_init__\r\n     validate_detections_fields(\r\n   File \"D:\\anaconda3\\envs\\myenv\\Lib\\site-packages\\supervision\\validators\\__init__.py\", line 126, in validate_detections_fields\r\n     validate_tracker_id(tracker_id, n)\r\n   File \"D:\\anaconda3\\envs\\myenv\\Lib\\site-packages\\supervision\\validators\\__init__.py\", line 77, in validate_tracker_id\r\n     raise ValueError(\r\nValueError: tracker_id must be a 1D np.ndarray with shape (2,), but got shape (3,)\r\n```\r\n\r\n**My code below.**\r\n```python\r\nimport cv2\r\nimport argparse\r\nfrom ultralytics import YOLO\r\nimport supervision as sv\r\nimport numpy as np\r\nimport time\r\nfrom pymongo import MongoClient\r\nfrom datetime import datetime\r\nfrom pytz import timezone\r\nimport base64\r\nimport torch\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\nprint(f'Using device: {device}')\r\ntorch.cuda.set_device(0)\r\n\r\nmongo_uri = \"myMongo\"\r\ndatabase_name = \"databasename\"\r\ncollection_name = \"collectionname\"\r\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á MongoClient\r\nclient = MongoClient(mongo_uri)\r\ndatabase = client[database_name]\r\ncollection = database[collection_name]\r\n\r\ncooldown_period = 10  # 1 minute\r\n\r\nZONE_POLYGON = np.array([\r\n    [0.1, 0.1],\r\n    [0.9, 0.1],\r\n    [0.9, 0.9],\r\n    [0.1, 0.9],\r\n    [0.1, 0.1]\r\n])\r\n\r\nmodel_path = \"D:\\\\Private\\\\Y3Project\\\\python_project\\\\runs\\\\detect\\\\train\\\\weights\\\\best.pt\"\r\n#rstp_url = 'rtsp://user:user@192.xxx.xx.x:554/'\r\nmyvdo = \"D:\\Private\\mydrive\\myvdo\\ss100met.mp4\"\r\n\r\n\r\ndef parse_arguments() -> argparse.Namespace:\r\n    parser = argparse.ArgumentParser(description=\"YOLOv8 live\")\r\n    parser.add_argument(\r\n        \"--webcam-resolution\",\r\n        default=[1280, 720],\r\n        nargs=2,\r\n        type=int\r\n    )\r\n    args = parser.parse_args()\r\n    return args\r\n\r\n\r\ndef save_image_to_mongodb(image_binary, count_no_helmet, count_rider):\r\n    # Convert time to Thailand timezone\r\n    thai_timezone = timezone('Asia/Bangkok')\r\n    upload_time = datetime.now(thai_timezone).strftime('%Y-%m-%d')\r\n\r\n    # Save image and time in MongoDB\r\n    image_data = {\r\n        \"image\": base64.b64encode(image_binary).decode('utf-8'),\r\n        \"upload_time\": upload_time,\r\n        \"count_no_helmet\": count_no_helmet,\r\n        \"count_rider\": count_rider\r\n    }\r\n\r\n    result = collection.insert_one(image_data)\r\n    print(f\"Image uploaded successfully. Object ID: {result.inserted_id}\")\r\n\r\n\r\ndef main():\r\n    last_save_time = time.time()\r\n    args = parse_arguments()\r\n    frame_width, frame_height = args.webcam_resolution\r\n\r\n    model = YOLO(model_path)\r\n    cap = cv2.VideoCapture(myvdo)\r\n    cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\r\n    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\r\n\r\n    box_annotator = sv.RoundBoxAnnotator(\r\n        thickness=2\r\n    )\r\n\r\n    label_annotator = sv.LabelAnnotator(\r\n        text_position=sv.Position.TOP_CENTER,\r\n        text_thickness=2,\r\n        text_scale=1\r\n    )\r\n\r\n    zone_polygon = (ZONE_POLYGON * np.array(args.webcam_resolution)).astype(int)\r\n    zone = sv.PolygonZone(polygon=zone_polygon, frame_resolution_wh=tuple(args.webcam_resolution))\r\n    zone_annotator = sv.PolygonZoneAnnotator(\r\n        zone=zone,\r\n        color=sv.Color.RED,\r\n        thickness=2,\r\n        text_thickness=4,\r\n        text_scale=2\r\n    )\r\n\r\n    # Initialize FPS display text\r\n    fps_text = \"FPS: calculating...\"\r\n\r\n    fps_start_time = time.time()\r\n    fps_counter = 0\r\n    \r\n    while True:\r\n        ret, frame = cap.read()\r\n        fps_counter += 1\r\n        bg_color = (0, 0, 0)  # Black background\r\n        text_color = (255, 255, 255)  # White text\r\n\r\n        for result in model.track(source=frame, stream=True, persist=True):\r\n            frame = result.orig_img\r\n            selected_classes = [0, 1, 2]\r\n            detections = sv.Detections.from_ultralytics(result)\r\n            detections = detections[np.isin(detections.class_id,selected_classes)]\r\n\r\n            if result.boxes.id is not None:\r\n                detections.tracker_id = result.boxes.id.cpu().numpy().astype(int)\r\n\r\n            labels = []\r\n            if detections.tracker_id is not None:\r\n                labels = [\r\n                    f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\r\n                    for class_id, confidence, tracker_id\r\n                    in zip(detections.class_id, detections.confidence, detections.tracker_id)\r\n\r\n                ]\r\n\r\n            if labels:\r\n                frame = box_annotator.annotate(\r\n                    scene=frame.copy(),\r\n                    detections=detections\r\n                )\r\n                frame = label_annotator.annotate(\r\n                    scene=frame.copy(),\r\n                    detections=detections,\r\n                    labels=labels\r\n                )\r\n            else:\r\n                print(\"No Labels\")\r\n\r\n            mask = zone.trigger(detections=detections)\r\n            frame = zone_annotator.annotate(scene=frame)\r\n            \r\n            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ detections ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö 0\r\n            if len(detections) > 0:\r\n                count_no_helmet = np.count_nonzero((detections.class_id == 0) & (detections.confidence > 0.5) & mask) #No Helmet\r\n                count_rider = np.count_nonzero((detections.class_id == 2) & (detections.confidence > 0.5) & mask) #Rider\r\n\r\n                # Print ‡∏Ñ‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤\r\n                print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏ß‡∏°‡∏´‡∏°‡∏ß‡∏Å: {count_no_helmet}\")\r\n                print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ô‡∏ú‡∏π‡πâ‡∏Ç‡∏±‡∏ö‡∏Ç‡∏µ‡πà: {count_rider}\")\r\n\r\n                if count_no_helmet >= 1 and count_rider >= 1:\r\n                    current_time = time.time()\r\n                    if current_time - last_save_time >= cooldown_period:\r\n                        image_binary = cv2.imencode('.jpg', frame)[1].tobytes()\r\n                        save_image_to_mongodb(image_binary, count_no_helmet, count_rider)\r\n                        print(\"Save Images Successfully\")\r\n                        last_save_time = current_time\r\n            else:\r\n                count_no_helmet = 0\r\n                count_rider = 0\r\n                \r\n        # Update FPS text\r\n        fps_counter += 1\r\n        if time.time() - fps_start_time >= 1:\r\n            fps = fps_counter / (time.time() - fps_start_time)\r\n            fps_counter = 0\r\n            fps_start_time = time.time()\r\n            fps_text = f\"FPS: {fps:.2f}\"     \r\n            \r\n        cv2.rectangle(frame, (10, 10), (200, 50), bg_color, -1)\r\n        cv2.putText(frame, fps_text, (15, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\r\n        cv2.imshow(\"yolov8\", frame)\r\n        if (cv2.waitKey(30) == 27):\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @REZIZ-TER üëãüèª, let me convert this issue into discussion and place it in Q&A section.  "
      }
    ]
  },
  {
    "issue_number": 1044,
    "title": "[ByteTrack] add `minimum_consecutive_frames` to limit the number of falsely assigned tracker IDs",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-03-25T15:29:06Z",
    "updated_at": "2024-04-24T22:25:19Z",
    "labels": [
      "enhancement",
      "api:tracker"
    ],
    "body": "### Description\r\n\r\nExpand the `ByteTrack` API by adding the `minimum_consecutive_frames` argument. \r\nIt will specify how many consecutive frames an object must be detected to be assigned a tracker ID. This will help prevent the creation of accidental tracker IDs in cases of false detection or double detection. Until detection reaches `minimum_consecutive_frames`, it should be assigned `-1`.\r\n\r\n### API\r\n\r\n```python\r\nclass ByteTrack:\r\n    def __init__(\r\n        self,\r\n        track_activation_threshold: float = 0.25,\r\n        lost_track_buffer: int = 30,\r\n        minimum_matching_threshold: float = 0.8,\r\n        frame_rate: int = 30,\r\n        minimum_consecutive_frames: int = 1\r\n    ):\r\n        pass\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "rolson24",
        "body": "Hi @SkalskiP,\r\n\r\nI would be interested in working on this. I have a question about possible implementation, though.\r\nWould we want to keep the tracking algorithm the same and just change the tracker ID (meaning possibly returning multiple tracks with a track_id of -1), or should we change the algorithm to only return tracks that have valid tracker ID's?"
      },
      {
        "user": "rolson24",
        "body": "Nvm, I think the best option is to only return tracks that have valid tracker ID's so we don't have to change how the annotators work."
      },
      {
        "user": "SkalskiP",
        "body": "Yup. Let's drop them for now. "
      }
    ]
  },
  {
    "issue_number": 1134,
    "title": "Adding a function to convert Supervision.Detections to Ultralytics.Results?",
    "author": "Jordan-Pierce",
    "state": "closed",
    "created_at": "2024-04-24T19:46:25Z",
    "updated_at": "2024-04-24T20:29:23Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nDue to the overlap between the two libraries, I think it'd be nice to have the ability to convert back and forth between Supervision.Detections and Ultralytics.Results, instead of it just being a one-way relationship.\n\n### Use case\n\nIt helps if data are already in Supervision format, but needs to be converted back to Ultralytics.Results for random reasons.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hey @Jordan-Pierce! I'm afraid we can't do that. The bidirectional conversion would require us to use `ultralytics` as a dependency. We want to avoid this due to the license used by that package. Our priority is to keep `supervision` under the MIT license."
      },
      {
        "user": "Jordan-Pierce",
        "body": "@SkalskiP understood, thanks for the prompt reply."
      }
    ]
  },
  {
    "issue_number": 373,
    "title": "Is there a way to save a Dataset natively, without polygon conversions?",
    "author": "thomasf1",
    "state": "closed",
    "created_at": "2023-09-21T17:00:24Z",
    "updated_at": "2024-04-12T15:48:42Z",
    "labels": [
      "enhancement",
      "api:datasets"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nIs there a way to save a Dataset natively, without polygon conversions? Would be great If there would be a way to save the dataset with the masks as a zip, all save formats (coco, yolo, voc) do some processing. Ideally there would be a way to natively save a dataset (maybe as a ZIP file).\r\n\r\nMy usecase is experimenting to get the polygon conversion when saving as yolo after a run just right and writing some custom code for it, so it would be helpful to have the \"raw\" state with the masks saved somewhere.\n\n### Additional\n\nI might just have overlooked some obvious way of saving the dataset... Otherwise it is a feature request I guess.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "SkalskiP",
        "body": "Hi, @thomasf1 üëãüèª Sounds interesting. Have you perhaps already thought about the structure of such a dataset? Would each mask be stored as a separate black-and-white photo? How would you store information about the class to which an object with a given mask belongs?"
      },
      {
        "user": "thomasf1",
        "body": "Well, there is already a standard/solution for coco that¬¥s somehow not being used by roboflow:  RLE (_frString)\r\n\r\nAlso, not supporting masks causes other issues: When converting to Polygons, cutouts in masks seem often to be converted as a separate polygon.\r\n\r\nExample: A person having this arms on his hips. The triangle shape inside the arms isn¬¥t part of the person, but surrounded by it. The way supervisions translates that into polygons is as follows:\r\n-  The outer shape of the person as a person polygon (reasonable, expected behaviour)\r\n-  The inner cutout of background as another person polygon (blatantly wrong and counterproductive)\r\n\r\n(not sure if that qualifies as a BUG or is somehow intended behaviour?) \r\n\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1042,
    "title": "[Detections] extend `from_transformers` with segmentation models support",
    "author": "SkalskiP",
    "state": "closed",
    "created_at": "2024-03-25T12:58:25Z",
    "updated_at": "2024-04-11T18:03:26Z",
    "labels": [
      "enhancement",
      "good first issue",
      "api:detection"
    ],
    "body": "### Description\r\n\r\nCurrently, Supervision only supports Transformers object detection models. Let's expand [`from_transformers`](https://github.com/roboflow/supervision/blob/781a064d8aa46e3875378ab6aba1dfdad8bc636c/supervision/detection/core.py#L391) by adding support for segmentation models.\r\n\r\n### API\r\n\r\nThe code below should enable the annotation of an image with segmentation results.\r\n\r\n```python\r\nimport torch\r\nimport supervision as sv\r\nfrom PIL import Image\r\nfrom transformers import DetrImageProcessor, DetrForSegmentation\r\n\r\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\r\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\r\n\r\nimage = Image.open(<PATH TO IMAGE>)\r\ninputs = processor(images=image, return_tensors=\"pt\")\r\n\r\nwith torch.no_grad():\r\n    outputs = model(**inputs)\r\n\r\nwidth, height = image.size\r\ntarget_size = torch.tensor([[height, width]])\r\nresults = processor. post_process_segmentation(\r\n    outputs=outputs, target_sizes=target_size)[0]\r\ndetections = sv.Detections.from_transformers(results)\r\n\r\nmask_annotator = sv.MaskAnnotator()\r\n\r\nannotated_image = mask.annotate(scene=image, detections=detections)\r\n```\r\n\r\n### Additional\r\n\r\n- [Transformers DETR Docs](https://huggingface.co/docs/transformers/en/model_doc/detr)\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will speed up the review process. The reviewer must test each change. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "Griffin-Sullivan",
        "body": "I'd like to take this one."
      },
      {
        "user": "onuralpszr",
        "body": "@Griffin-Sullivan  task is yours good luck :) "
      },
      {
        "user": "Griffin-Sullivan",
        "body": "So the `post_process_segmentation` method only returns `['scores', 'labels', 'masks']`. I found a utility function to get the `xyxy` from the mask https://github.com/roboflow/supervision/blob/42f9d4b69b9d2c97756d69184359bdc484e896a8/supervision/detection/utils.py#L307. Is this the right approach here? When I run everything it works and I get detections but the annotations don't come out right. You can see in my colab there's only one colored line at the top of the annotated picture. \r\n\r\nHere's the colab: https://colab.research.google.com/drive/1j1O95pxlHDPZ1XtmLJ_VcmkYAk_KvrUg?usp=sharing\r\n\r\nAnd my draft PR: https://github.com/roboflow/supervision/pull/1054"
      }
    ]
  },
  {
    "issue_number": 82,
    "title": "Hide console output when objects are detected",
    "author": "jacobpchen",
    "state": "closed",
    "created_at": "2023-05-01T14:33:17Z",
    "updated_at": "2024-04-11T16:37:56Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nIs there a way to hide the output to console when objects are detected?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "SkalskiP",
        "body": "Hi @jacobpchen üëãüèª! Could you help me out what `console output` are we talking about? "
      },
      {
        "user": "jacobpchen",
        "body": "Hi @SkalskiP when the detection is running this is printed in console:\r\n\r\n0: 384x640 (no detections), 1072.9ms\r\nSpeed: 1.7ms preprocess, 1072.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 (no detections), 1055.3ms\r\nSpeed: 1.0ms preprocess, 1055.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 (no detections), 1232.2ms\r\nSpeed: 1.7ms preprocess, 1232.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 3 cars, 1 truck, 955.4ms\r\nSpeed: 1.5ms preprocess, 955.4ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 3 cars, 1 truck, 905.9ms\r\nSpeed: 1.5ms preprocess, 905.9ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 2 cars, 1 truck, 722.2ms\r\nSpeed: 1.0ms preprocess, 722.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 3 cars, 739.7ms\r\nSpeed: 1.2ms preprocess, 739.7ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 2 cars, 1073.4ms\r\nSpeed: 1.3ms preprocess, 1073.4ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 2 cars, 1 truck, 1159.1ms\r\nSpeed: 0.9ms preprocess, 1159.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 2 cars, 877.3ms\r\nSpeed: 2.1ms preprocess, 877.3ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\r\n\r\n0: 384x640 2 cars, 1 truck, 1359.9ms\r\n\r\nI was wondering if there was a way to hide all of this output. "
      }
    ]
  },
  {
    "issue_number": 268,
    "title": "[weighted_box_fussion] - an alternative for `box_non_max_suppression`",
    "author": "hardikdava",
    "state": "open",
    "created_at": "2023-08-06T11:02:35Z",
    "updated_at": "2024-04-09T08:19:59Z",
    "labels": [
      "enhancement",
      "api:detection",
      "Q2.2024"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Description\r\n\r\nCurrent object detection models removes overlapping boxes by `nms` which can reduce accuracy of the final results. It can be avoided by `Weighted Box Fusion` which can accepts all the prediction whether from a single model or multiple models. \r\n\r\nReference: [Weighted Box Fusion](https://arxiv.org/pdf/1910.13302.pdf)\r\nOriginal Implementation: [ensemble-boxes](https://github.com/ZFTurbo/Weighted-Boxes-Fusion/tree/master)\r\n\r\n\r\n\r\n### Use case\r\n\r\n```python\r\nimport supervision as sv\r\n\r\nwbf = sv.WeightedBoxFusion()\r\n\r\nres_a = model_a(image)\r\ndet_a = sv.Detection(res_a)\r\n\r\nres_b = model_b(image)\r\ndet_b = sv.Detection(res_b)\r\n\r\nwbf_detections = wbf([det_a, det_b])\r\n```\r\n\r\n### Additional\r\n\r\n```python\r\n\r\nclass WeightedBoxFusion:\r\n\r\n\tdef __init__(self):\r\n\t\tpass\r\n\t\r\n\tdef __call__(self, detections: Union[Detections, List[Detections]]) -> Detections:\r\n\t\tpass\r\n\t\tresult_detections = Detections(...)\r\n\t\treturn result_detections \r\n\r\n```\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "@hardikdava I have yet to hear of this post-processing approach. Looks interesting. IS it better than NMS?"
      },
      {
        "user": "hardikdava",
        "body": "@SkalskiP yes, it is better in terms of accuracy but not in terms of speed. You can learn about it more and direct comparision on this blogpost: https://learnopencv.com/weighted-boxes-fusion/"
      },
      {
        "user": "AlainPilon",
        "body": "@hardikdava Did you implement the WBF? Even if Supervision doesnt accept the feature, I could use it for my current project. "
      }
    ]
  },
  {
    "issue_number": 264,
    "title": "Redesign Line Counter",
    "author": "hardikdava",
    "state": "closed",
    "created_at": "2023-08-02T20:49:00Z",
    "updated_at": "2024-04-08T12:42:00Z",
    "labels": [
      "enhancement",
      "api: linezone",
      "priority:high"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\r\n\r\n\r\n### Description\r\n\r\nThere are several issues reported by the community for `sv.LineCounter`. These issues needs to be fixed and feature should be robust in terms of speed.\r\n\r\nFeatures/Improvement:\r\n\r\n - [x] Multi-Class counting #93  #94 \r\n - [x] Extra counts bug #87 \r\n - [x] Text color for `in/out` #79 \r\n - [x] API to get info of `in/out` counts for more modification for users along track ids\r\n - [x] objects anchor selection for counting\r\n - [x] objects anchor point drawing\r\n - [x] change line color on crossing\r\n - [x] return counts api and total counts api\r\n - [x] return functions to get crossed detections for `in` and `out`\r\n - [x] introduce `sv.AnchorPointAnnotator` to render points on image\r\n\r\n\r\n### Use case\r\n\r\n_No response_\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "Newthin",
        "body": "how do you print the current current on the zone\r\n"
      }
    ]
  },
  {
    "issue_number": 143,
    "title": "Unicode Label Support",
    "author": "Ucag",
    "state": "closed",
    "created_at": "2023-06-17T13:55:46Z",
    "updated_at": "2024-04-08T12:39:05Z",
    "labels": [
      "enhancement",
      "version: 0.12.0",
      "version: 0.13.0",
      "version 0.14.0"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Description\n\nI'm using supervision with unicode label data. When I assign unicode label information to the `labels` parameter, `supervision` only draw strange squares instead of label text. After search for a moment, I noticed that `cv2` only supports ascii text when using `putText`. The square block is created by `cv2` not `supervision` itself.\n\n### Use case\n\nSupervision is good and love it so much. There are many label information not in native ascii. I think support it would be good.\n\n### Additional\n\nI found a work around way to solve the issue is to use `PIL` to draw the text with user defined `truetype` font. By this way, I think supervision could avoid the `GPL` license problem. \n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "SkalskiP",
        "body": "Hi, @Ucag üëãüèª! Thanks a lot for your interest in supervision üôèüèª ! Adding support for `PIL` as a text rendering engine is 100% on my TODO list. I really wan to do it but I'm quite busy so I doubt I will be able in upcoming weeks :/ "
      },
      {
        "user": "Ucag",
        "body": "I've done a primitive version using PIL to render text with unicode support, If you don't mind, I would like to PR."
      }
    ]
  },
  {
    "issue_number": 87,
    "title": "Separate line and zone counters for separate classes",
    "author": "zburq",
    "state": "closed",
    "created_at": "2023-05-09T02:01:18Z",
    "updated_at": "2024-04-08T12:37:11Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar feature requests.\n\n\n### Question\n\nMy YOLO V8 is detecting & classifying several classes, say `classes = [2, 3, 5, 7]`: \r\n\r\nI then follow examples by @SkalskiP and do something like: \r\n\r\n```python\r\nline_counter = LineZone()\r\nresults = model.track(source=source_video, classes=classes, ... )\r\nfor result in results:\r\n    detections = Detections.from_yolov8(result)\r\n    detections.tracker_id = result.boxes.id.cpu().numpy().astype(int)\r\n    line_counter.trigger(detections=detections)\r\n```\r\nFinally I can return and use `line_counter.in_count` and `line_counter.out_count` for other tasks.\r\n\r\nCurrently, `LineZone().trigger` will `+=1` the in/out counts if a detection of any class crosses the line.  \r\n\r\nI'd like to keep a running tally of in/out counts for each class. Two possibilities:  \r\n- instantiate multiple `LineZone` objects, one for each class, or   \r\n- a single `LineZone` object to keep track of which class triggered how many counts. \r\n\r\nI have modified my local code to accomplish this for now, but this would be a useful feature.  \r\n\r\nPerhaps `LineZone.in_count` and `LineZone.out_count` could old a vector corresponding to the class \r\n\r\n**Potential problem:** Sometimes the classification of the tracked object flips between different classes during the tracking or line-crossing. My instinct is to use the majority class. For this, the line would have to be at the edge of the frame, so that we have seen all the classifications.\r\n\r\nAnyone else solving a similar problem? Any better ideas?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello there, thank you for opening an Issue ! üôèüèª The team was notified and they will get back to you asap."
      },
      {
        "user": "SkalskiP",
        "body": "Hi @zburq üëãüèª!\r\n\r\n> Sometimes, the classification of the tracked object flips between different classes during the tracking or line-crossing. My instinct is to use the majority class.\r\n\r\nYup, using 'majority class' was what I did in the past too. If your model is flickering between classes, that should be able to give you a better class assignment. Other things you could do:\r\n- Train a model that would be more stable and robust.\r\n- Moving window. Take the 'majority class from the last N frames.\r\n\r\n> **Warning**\r\n>  For this, the line would have to be at the edge of the frame so that we have seen all the classifications.\r\n\r\nUnfortunately, this is a bad idea. For us to be able to say that object crossed the line, we need to be able to detect it on both sides of the line reliably. For example, it was on the left, and now it is on the right. If the line is too close to the edge of the frame in many cases, you won't get that final detection when the whole box makes it to the other side.\r\n\r\n> instantiate multiple LineZone objects, one for each class\r\n\r\nThis one feels better to me as you want need to write a lot of custom logic. Simply divide detections into groups and run each part through the associated line counter.\r\n\r\n```python\r\ndetections = Detections.from_yolov8(result)\r\ndetections_0 = detections[detections.class_id == 0]\r\nline_counter_0.trigger(detections=detections_0)\r\n```"
      },
      {
        "user": "maddust",
        "body": "@zburq i have accomplished determining what object and class crossed the line by checking the changes in the tracker state .for each line_counter \r\n\r\nonce the state changes from True to False or viceversa depending on the direction the object crossed you can then extract the class of that object based on the tracked_id.  i will share my implementation to see if it helps. \r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 316,
    "title": "[DetectionDataset] - enable lazy dataset loading",
    "author": "hardikdava",
    "state": "open",
    "created_at": "2023-08-24T08:31:56Z",
    "updated_at": "2024-04-08T12:20:28Z",
    "labels": [
      "enhancement",
      "api:datasets",
      "Q2.2024"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Supervision [issues](https://github.com/roboflow/supervision/issues) and found no similar bug report.\n\n\n### Bug\n\n`sv.DetectionDataset` is loading images unnecessary. It is suggestable that it only loads image when it is necessary. This can be useful for loading large dataset without keeping memory.\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "SkalskiP",
        "body": "Hi @hardikdava üëãüèª!\r\n\r\nHere is my idea. Let's create a set of separate methods `sv.DetectionDataset.generate_from_*`. Unlike `sv.DetectionDataset.from_*`, it would return a Python generator. What do you think?\r\n\r\n- Typing\r\n  \r\n```python\r\nsv.DetectionDataset.generate_from_yolo(\r\n    images_directory_path: str, \r\n    annotations_directory_path: str\r\n) -> Generator[Tuple[str, sv.Detections, np.ndarray], None, None]:\r\n    pass\r\n```\r\n\r\n- Usage example\r\n\r\n```python\r\nfor path, image, detections in sv.DetectionDataset.generate_from_yolo(...):\r\n    pass\r\n```\r\n\r\n- Method names\r\n\r\n```python\r\n- sv.DetectionDataset.generate_from_yolo(...)\r\n- sv.DetectionDataset.generate_from_coco(...)\r\n- sv.DetectionDataset.generate_from_pascal_voc(...)\r\n```"
      },
      {
        "user": "hardikdava",
        "body": "@SkalskiP Is there anyway that we only modify current APIs, otherwise users will be confused between `sv.DetectionsDataset.from_*` and `sv.DetectionDataset.generate_from*` methods."
      },
      {
        "user": "hardikdava",
        "body": "@SkalskiP is it possible that we use callback system for loading images then we do not have to worry about much things. "
      }
    ]
  },
  {
    "issue_number": 791,
    "title": "[PolygonZone] - allow per class counting",
    "author": "SkalskiP",
    "state": "open",
    "created_at": "2024-01-26T13:29:16Z",
    "updated_at": "2024-04-08T12:12:39Z",
    "labels": [
      "enhancement",
      "api:polygonzone",
      "Q2.2024"
    ],
    "body": "### Description\r\n\r\nCurrently, [sv.PolygonZone](https://github.com/roboflow/supervision/blob/3024ddca83ad837651e59d040e2a5ac5b2b4f00f/supervision/detection/tools/polygon_zone.py#L15) provides only current aggregated counts - all classes are thrown into one bucket. In the past, many users have asked us to provide more granular - per class count. This can be achieved by adding `class_in_count` and `class_out_count` dictionaries that will store per-class counts.\r\n\r\n```python\r\nclass PolygonZone:\r\n\r\n    def __init__(\r\n        self,\r\n        polygon: np.ndarray,\r\n        frame_resolution_wh: Tuple[int, int],\r\n        triggering_position: Position = Position.BOTTOM_CENTER,\r\n    ):\r\n        # Existing initialization code...\r\n\r\n        self.class_in_count: Dict[int, int] = {}\r\n        self.class_out_count: Dict[int, int] = {}\r\n\r\n    def trigger(self, detections: Detections) -> np.ndarray:\r\n        crossed_in = np.full(len(detections), False)\r\n        crossed_out = np.full(len(detections), False)\r\n\r\n        # Required logic changes...\r\n```\r\n\r\n### Additional\r\n\r\n- Note: Please share a Google Colab with minimal code to test the new feature. We know it's additional work, but it will definitely speed up the review process. Each change must be tested by the reviewer. Setting up a local environment to do this is time-consuming. Please ensure that Google Colab can be accessed without any issues (make it public). Thank you! üôèüèª ",
    "comments": [
      {
        "user": "karanjakhar",
        "body": "Hi @SkalskiP I would like to work on it. "
      },
      {
        "user": "SkalskiP",
        "body": "Hi @karanjakhar üëãüèª ! Sure, I assigned the issue to you. Have fun! üî• "
      }
    ]
  }
]