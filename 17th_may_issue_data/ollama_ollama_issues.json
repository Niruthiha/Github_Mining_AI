[
  {
    "issue_number": 11087,
    "title": "Loading time of mistral-small3.1 is too long",
    "author": "JitaekJo",
    "state": "closed",
    "created_at": "2025-06-16T13:31:26Z",
    "updated_at": "2025-06-17T14:27:16Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nDear team,\n\nWhen I send a request, mistral-small3.1 is loaded tooooo long.\nFor example, gemma3:27b does a task for 7 sec.\nBut mistral-small3.1 does the same thing for 1.5 min.\nMeanwhile, this symptom appeared few days ago.\n\nPlease resolve this problem.\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will aid in debugging."
      },
      {
        "user": "JitaekJo",
        "body": "I hope this log would help you"
      },
      {
        "user": "JitaekJo",
        "body": "[log.txt](https://github.com/user-attachments/files/20758199/log.txt)"
      }
    ]
  },
  {
    "issue_number": 8977,
    "title": "The Ollama downloader page is blank.",
    "author": "RoshanDong",
    "state": "open",
    "created_at": "2025-02-10T02:37:17Z",
    "updated_at": "2025-06-17T14:14:54Z",
    "labels": [
      "bug",
      "macos"
    ],
    "body": "### What is the issue?\n\nI downloaded the Ollama app from https://ollama.com/download, and when I run the downloader on my Mac, the page goes blank. \n\n![Image](https://github.com/user-attachments/assets/950e6b0a-11a6-4128-b696-ffe21e1e16ad)\n\nMy system chip is **Apple M3 Pro**.\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "LeisureLinux",
        "body": "GFW stopped you. "
      },
      {
        "user": "JamesClarke7283",
        "body": "> ### What is the issue?\n> I downloaded the Ollama app from https://ollama.com/download, and when I run the downloader on my Mac, the page goes blank.\n> \n> ![Image](https://github.com/user-attachments/assets/950e6b0a-11a6-4128-b696-ffe21e1e16ad)\n> \n> My system chip is **Apple M3 Pro**.\n> \n> ### Relevant log output\n> ### OS\n> macOS\n> \n> ### GPU\n> Apple\n> \n> ### CPU\n> Apple\n> \n> ### Ollama version\n> _No response_\n\n@RoshanDong if you live in Mainland China, its likely your internet provider blocking it, as Ollama's web services relies on `Amazon AWS`, which is banned by the CPC, you can find a regularly updated ban list [here](https://github.com/gfwlist/gfwlist/blob/master/list.txt) to check against for any future network issues.\n \nFor a solution, You can use a proxy like Tor, the simplest way to use tor is by installing `The Tor Browser Bundle`, its same process as downloading any other web browser, tor makes your internet connection more private and as a by-product bypass internet censorship, if you use it with `bridges` like Snowflake it can bypass even some of the most strictest blacklist based firewalls and make it much harder for an internet provider to detect you are using a proxy.\n\n## _**SAFETY NOTICE:**_\n\n**If you live in a region, where internet proxies/vpns are strictly forbidden, it may be a worth considering the risk's of possessing such tools in your area.** \n\n## Tor Browser links\n\nThe original page is: https://www.torproject.org/\n\nBut as that's likely blocked you can get unofficial release binaries here if you are in a pinch and cant find it else-ware:\nhttps://github.com/TheTorProject/gettorbrowser/releases\n\nThose ones are very outdated, but if you run that version, you can use that tor browser to download the uptodate version of tor browser from `www.torproject.org` or click update within the older browser (you only have to do this once).\n\nYou can Install tor browser from github from source also if you have the means:\nhttps://github.com/torproject/torbrowser-launcher\n\n\nThe hardest part is getting tor browser when your internet is censored, once you have it, you are essentially unblocked for any website (you can also use tor directly with your installed applications installed on your system, but that is a little more involved, i will keep it simple here).\n\n**Here is an official 3 minute torproject video on how to get started (With proper Mandarin Chinese subtitles for added convenience)**\n\nhttps://github.com/user-attachments/assets/e97bc41d-4a1a-40a9-888a-4b3fd9352de3\n\n__ETIQUETTE REMINDER__: **_Please check your problem isn't your end before opening a bug report, always check for similar issues also so it doesn't add unnecessary burden on the maintainers, i usually don't directly help with these kinds of issues, however i make an exception here as this is likely a internet censorship issue._**\n\nAlso @RoshanDong please close this issue if it is indeed your end, you can still message here and tell me if my solution helped, it just reduces clutter for the maintainers.\n\n\nHope this helps (:\n\nYours Sincerely,\nJames Clarke"
      }
    ]
  },
  {
    "issue_number": 2225,
    "title": "Ollama stops generating output and fails to run models after a few minutes",
    "author": "TheStarAlight",
    "state": "closed",
    "created_at": "2024-01-27T06:22:10Z",
    "updated_at": "2025-06-17T13:46:17Z",
    "labels": [
      "bug"
    ],
    "body": "Hi, I'm running ollama on a Debian server and use the oterm as the interface.\r\nAfter some chats (just less than 10 normal questions) the ollama fails to respond anymore and running `ollama run mixtral` just didn't success (it keeps loading).\r\nI noted that the same issue happened, like in #1863 . Is there a solution at the moment? Also, I'm not the administrator of the server and I even don't know how to restart ollama 😂. The serve process seems to runs as another user named ollama. Can anyone tell me how to restart it?\r\nTo developers: I can provide some debug information if you need, just tell me how to do it.\r\nThanks :D",
    "comments": [
      {
        "user": "TheStarAlight",
        "body": "The model I'm running include mixtral:latest and wizard-math:70b.\r\nI have access to an NVIDIA A100 PCI-e 80GB and the inputs are all simple sentences (no more than 100 words) and I ensure that nobody else is using the GPU (I see from nvitop)."
      },
      {
        "user": "jmorganca",
        "body": "Hi @TheStarAlight, would it be possible to share which version of Ollama you are running? `ollama -v` will print this out. Thanks so much, and I'm sorry you hit this issue"
      },
      {
        "user": "TheStarAlight",
        "body": "@jmorganca Sure! The ollama version is 0.1.20, just installed three days ago via the shell script. Please tell me if you need more information :)"
      }
    ]
  },
  {
    "issue_number": 11043,
    "title": "Please bring back `Q6_K` quantizations in Ollama",
    "author": "Burnarz",
    "state": "closed",
    "created_at": "2025-06-11T01:04:57Z",
    "updated_at": "2025-06-17T13:14:13Z",
    "labels": [
      "feature request"
    ],
    "body": "I understand the desire to reduce and streamline quantization support.  \nBut in my humble opinion, dropping support for `Q6_K` was not the best move.\n\n### The idea is:\nReintroduce support for the `Q6_K` quantization format in Ollama, either as a first-class option or as an advanced override for model loading.\n\n### My use case:\nMany developers (including myself) run Ollama models on **24GB of VRAM** consumer GPUs — like RTX 3090 or 4090.\n\nWhile `Q4_K` is great for memory efficiency, `Q6_K` hits a perfect **sweet spot between performance and output quality**. It leverages available VRAM more effectively, giving us:\n- Noticeably **better generation quality** than `Q4_K`, especially in long-form or nuanced outputs.\n- Still **lightweight enough to run fast**, with acceptable token speeds on 24GB GPUs.\n- Avoids the performance and memory overhead of full `FP16` or `Q8_0`.\n\n### Why it's important:\n- Ollama aims to make local AI practical and efficient — and `Q6_K` is one of the best quant formats for high-end consumer setups.\n- Current quant choices feel like a gap: either too light (`Q4_K`) or too heavy (`Q8_0` / `F16`).\n- Users with capable hardware aren't fully benefiting from the potential performance/quality ratio that `Q6_K` provides.\n\n### Resources:\n- `Q6_K` support was available in older GGUF builds and proven to work well.\n- Several models (like LLaMA, Mistral, Mixtral, etc.) had high-quality Q6_K variants.\n\n### Are you willing to help?\nHappy to test and benchmark Q6_K versions on 24GB hardware and share results with the community.\n",
    "comments": [
      {
        "user": "LarsKort",
        "body": "Why not to use huggingface hub to get Q6_K models?\nExample:\n`ollama pull hf.co/unsloth/Qwen3-30B-A3B-GGUF:Q6_K`"
      },
      {
        "user": "Burnarz",
        "body": "My bad,..  I thought I had read somewhere a discussion between the ollama team, saying that even that wouldn't work... but I haven't tried it....\nThank you."
      }
    ]
  },
  {
    "issue_number": 11095,
    "title": "[Ollama] Error: POST predict: Post \"http://127.0.0.1:36273/completion\": EOF",
    "author": "jaypeche",
    "state": "open",
    "created_at": "2025-06-17T12:13:40Z",
    "updated_at": "2025-06-17T13:09:39Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI have compiled from sources on Gentoo GNU/Linux, but when running ollama models. It returns this error : \n\n```\njay@strix ~ $ ollama run llama3\n>>> hello world \nError: POST predict: Post \"http://127.0.0.1:36611/completion\": EOF\n```\nMy system info : \nhttps://gist.github.com/jaypeche/552f5e3cada5cbd3fb918471a1b816b3\n\nOllama systemd logs : \nhttps://gist.github.com/jaypeche/265bd59b20d7981d88df907fa865f133\n\nOllama ebuild : \nhttps://github.com/gentoo/guru/blob/master/sci-ml/ollama/ollama-0.9.0.ebuild\n\nDo you have any idea ?\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "```\njuin 17 13:42:38 strix ollama[43954]: CUDA error: the provided PTX was compiled with an unsupported toolchain.\n```\nProbably need to raise the issue with the maintainer of the ebuild."
      },
      {
        "user": "jaypeche",
        "body": "I will report this issue to gentoo ebuild maintainer.\nMany thanks"
      }
    ]
  },
  {
    "issue_number": 11096,
    "title": "ollama",
    "author": "ritesh-p9284",
    "state": "open",
    "created_at": "2025-06-17T12:23:22Z",
    "updated_at": "2025-06-17T12:55:02Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nlisten tcp 127.0.0.1:11434: bind: address already in use\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "Stop the other server, or don't run another server."
      }
    ]
  },
  {
    "issue_number": 3575,
    "title": "Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.",
    "author": "Coder-Vishali",
    "state": "closed",
    "created_at": "2024-04-10T12:46:02Z",
    "updated_at": "2025-06-17T12:24:23Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nWhen I execute ollama serve, I face the below issue:\r\n\r\nError: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.\r\n\r\n![image](https://github.com/ollama/ollama/assets/60731083/38b5ce1d-e039-4066-9fc0-7fa18b027722)\r\n\r\n**\r\n![image](https://github.com/ollama/ollama/assets/60731083/86c5173e-4a1b-4270-8c89-145f055b60dd)\r\n**\r\n\r\nThings which I have tired out:\r\n\r\n1.  Restarted my machine\r\n2.  Stop and start the ollama server\r\n3.  Kill the port using: netstat -ano | findstr :<PORT>, taskkill /PID <PID> /F\r\n\n\n### What did you expect to see?\n\n_No response_\n\n### Steps to reproduce\n\n_No response_\n\n### Are there any recent changes that introduced the issue?\n\n_No response_\n\n### OS\n\nWindows\n\n### Architecture\n\nx86\n\n### Platform\n\n_No response_\n\n### Ollama version\n\n_No response_\n\n### GPU\n\n_No response_\n\n### GPU info\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Other software\n\n_No response_",
    "comments": [
      {
        "user": "muxixi727",
        "body": "You can close ollama that is opened locally"
      },
      {
        "user": "4G3NTR0LLC4G3",
        "body": "In order to close the \"local\" ollama go to the bottom right of taskbar on windows click the up arrow, and quit ollama from the small tiny ollama app icon in the small arrow key menu. SO CONFUSING>\r\n\r\nIf you then go back and run ollama serve it should work now."
      },
      {
        "user": "cumtsd",
        "body": "i've got the same problem without solved"
      }
    ]
  },
  {
    "issue_number": 1716,
    "title": "is there a way to calculate token size?",
    "author": "ralyodio",
    "state": "closed",
    "created_at": "2023-12-26T04:48:35Z",
    "updated_at": "2025-06-17T12:04:06Z",
    "labels": [],
    "body": "I don't know if this limitation exists with the api. I'm swtiching from openai to ollama api, and with openai I need to calculate token size and subtract it from the total 4096.\r\n\r\nDo we need to do that for ollama api? If so, how do I caclulate token size of prompt?",
    "comments": [
      {
        "user": "technovangelist",
        "body": "Hi thanks for submitting the issue. Ollama doesn't require you to provide a number representing the quantity of tokens to the api. That said each model has a different context size and once you go over that, answers can degrade. Some models have a context size of 4k but 16k and 32k are showing up too. There are also some with 100k but they will require a huge amount of ram to run. \n\nDoes this answer your question?"
      },
      {
        "user": "ralyodio",
        "body": "yes, thank you."
      },
      {
        "user": "mbrochh",
        "body": "If I may add to this question:\r\n\r\nWhat is the correct way to count the number of tokens when I build my prompt?\r\n\r\nWhen interfacing with OpenAI, I can use the Tiktoken library, but I wonder if that library is also relevant when dealing with all other models that ollama supports?"
      }
    ]
  },
  {
    "issue_number": 7163,
    "title": "Ollama does not run",
    "author": "d3tk",
    "state": "closed",
    "created_at": "2024-10-10T16:27:46Z",
    "updated_at": "2025-06-17T11:07:57Z",
    "labels": [
      "bug",
      "windows"
    ],
    "body": "### What is the issue?\n\nThe process never completes when I try to do ollama run or ollama list. \n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.3.12",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will help in debugging."
      },
      {
        "user": "d3tk",
        "body": "> [Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will help in debugging.\r\n<details>\r\n<summary> Server Logs </summary>\r\n<p>2024/10/10 14:38:26 routes.go:1153: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\dkuts\\\\.ollama\\\\models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\r\ntime=2024-10-10T14:38:26.218-04:00 level=INFO source=images.go:753 msg=\"total blobs: 37\"\r\ntime=2024-10-10T14:38:26.247-04:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\ntime=2024-10-10T14:38:26.249-04:00 level=INFO source=routes.go:1200 msg=\"Listening on 127.0.0.1:11434 (version 0.3.12)\"\r\ntime=2024-10-10T14:38:26.251-04:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[cuda_v11 cuda_v12 rocm_v6.1 cpu cpu_avx cpu_avx2]\"\r\ntime=2024-10-10T14:38:26.252-04:00 level=INFO source=gpu.go:199 msg=\"looking for compatible GPUs\"\r\ntime=2024-10-10T14:38:26.383-04:00 level=INFO source=gpu.go:292 msg=\"detected OS VRAM overhead\" id=GPU-40f13575-6ec5-c1b3-c72e-69caef571bd7 library=cuda compute=8.6 driver=12.6 name=\"NVIDIA GeForce RTX 3080\" overhead=\"81.5 MiB\"\r\ntime=2024-10-10T14:38:26.384-04:00 level=INFO source=types.go:107 msg=\"inference compute\" id=GPU-40f13575-6ec5-c1b3-c72e-69caef571bd7 library=cuda variant=v12 compute=8.6 driver=12.6 name=\"NVIDIA GeForce RTX 3080\" total=\"10.0 GiB\" available=\"8.9 GiB\"\r\n[GIN] 2024/10/10 - 14:39:46 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/10/10 - 14:39:46 | 200 |     26.4183ms |       127.0.0.1 | GET      \"/api/tags\"</p></details>"
      },
      {
        "user": "rick-github",
        "body": "Can you give a demonstration of the problem you are experiencing?"
      }
    ]
  },
  {
    "issue_number": 11094,
    "title": "bge-large-zh-v1.5 error",
    "author": "SiqingHe",
    "state": "open",
    "created_at": "2025-06-17T08:56:08Z",
    "updated_at": "2025-06-17T10:56:59Z",
    "labels": [],
    "body": "when use bge-large-zh-v1.5 ,it raise errors \ncurl http://10.191.0.245:8999/api/embed -d '{\"model\": \"bge-large-zh-v1.5:latest\", \"input\": \"测试文本\"}'\n{\"error\":\"unable to load model: /root/.ollama/models/blobs/sha256-ea5d2af223de4485055c409c369a319bb584fbd2629a5b4f082c6ccd848bfaf7\"}\n\nwhile I have given a permission :\nchmod -R 777 /root/.ollama/models/blobs/\n\ndoes anyone meet this problem?",
    "comments": [
      {
        "user": "SiqingHe",
        "body": "ollama version is 0.7.0"
      },
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) may aid in debugging."
      }
    ]
  },
  {
    "issue_number": 11093,
    "title": "AMD GPU drivers are not installed when nvidia-smi is present",
    "author": "madcato",
    "state": "open",
    "created_at": "2025-06-17T07:54:04Z",
    "updated_at": "2025-06-17T10:55:47Z",
    "labels": [],
    "body": "Trying to update my ollama version, my AMD GPU stopped being used by ollama.\n\nIn the `install.sh` this three lines prevent AMD GPU drivers to be installed:\n```sh\nif check_gpu nvidia-smi; then\n    status \"NVIDIA GPU installed.\"\n    exit 0\nfi\n```\nBecause if `nvidia-smi` is present, the script exits.\n\n\n\nAnalyzing the script, it seems to me that `install.sh`  is not suitable for those who have both an NVIDIA and AMD GPUs.",
    "comments": [
      {
        "user": "rick-github",
        "body": "#10883"
      }
    ]
  },
  {
    "issue_number": 2033,
    "title": "Add Vulkan runner ",
    "author": "maxwell-kalin",
    "state": "open",
    "created_at": "2024-01-17T18:15:00Z",
    "updated_at": "2025-06-17T09:08:48Z",
    "labels": [
      "feature request",
      "amd",
      "intel",
      "gpu"
    ],
    "body": "https://github.com/nomic-ai/llama.cpp\r\n\r\nGPT4All runs Mistral and Mixtral q4 models over 10x faster on my 6600M GPU",
    "comments": [
      {
        "user": "lukas-the-wizard",
        "body": "O hell yeah"
      },
      {
        "user": "easp",
        "body": "Ollama is very close to supporting AMD GPUs through ROCm using mainline llama.cpp."
      },
      {
        "user": "lukas-the-wizard",
        "body": "Yeah but ROCm doesnt run on my GPU from AMD "
      }
    ]
  },
  {
    "issue_number": 11092,
    "title": "Loading time of mistral-small3.1 is too long #11087",
    "author": "JitaekJo",
    "state": "closed",
    "created_at": "2025-06-17T07:26:43Z",
    "updated_at": "2025-06-17T08:32:49Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n\n![Image](https://github.com/user-attachments/assets/6944ebaa-fc4b-45d3-a65c-57e981082021)\n\n**A related problem is the model isn't loaded. As you can see, even though the memory is loaded and ollama is served after providing a prompt, the load is \"0%\" for a long time.\nIs it related to the memory issue? I'm doubt.**\n\n######################################################################\nI've upgraded ollama recently yes.\n\nMystery solved.\n\nThere have been recent changes to the estimation logic to reduce the chance of an OOM. You can force ollama to load more layers into GPU by setting num_gpu as described https://github.com/ollama/ollama/issues/6950#issuecomment-2373663650. This may increase OOMs or cause a https://github.com/ollama/ollama/issues/7584#issuecomment-2466715900.\n\n**And this doesn't seem like an resolution because this guides to load the model on RAM and CPU.\nMy problem is that the model isn't loaded.**\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "JitaekJo",
        "body": "I found a cause of problem. When I feed an IMAGE, MISTRAL isn't be loaded. Please fix this bug."
      }
    ]
  },
  {
    "issue_number": 11054,
    "title": "just downloading manifest counts as a full download",
    "author": "Sk16er",
    "state": "closed",
    "created_at": "2025-06-12T09:09:30Z",
    "updated_at": "2025-06-17T07:58:48Z",
    "labels": [
      "bug",
      "ollama.com"
    ],
    "body": "### What is the issue?\n\nSo this is a issue  which is ollama have a venerability.\nwhich is if any device made a request to download a model, it stats that it downloaded.\nmeaning let suppose i made a request to download a model, eg.\n*Please do not delete my model which i made in this recording*\nthis i made to showcase my class student's.\nand do not cut the downloads in that. \n\nhope you fix this other wise some could mis use this \n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "pdevine",
        "body": "This is expected behaviour. Models are broken up into \"image layers\" which can be shared with other models (and can be referenced by the manifest), so it's possible that a person has already downloaded part of a given model and they don't need to download it again. We also hand off downloads to cloudflare for the individual image layers so it's tricky for ollama.com to know exactly when a download is finished.\n\nThe way this gets protected against is through rate limiting. The automatic rate limiting will kick in if you try to hit the API too much, and then at some point we'll just ban the IP address if we see someone abusing it."
      },
      {
        "user": "Sk16er",
        "body": "Thanks for response but sorry i don't write the issue correctly,😥. But as you say that if someone abuse this you can block that i donot know that, if this is the case then it is clear"
      }
    ]
  },
  {
    "issue_number": 11091,
    "title": "Ollama encountered an error running the model: unexpected EOF while executing qwen2.5vl:3b",
    "author": "smileyboy2019",
    "state": "open",
    "created_at": "2025-06-17T02:51:29Z",
    "updated_at": "2025-06-17T07:21:58Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n![Image](https://github.com/user-attachments/assets/ddf208ad-dce8-4772-b3d5-78b3ee3c6b0a)\nOllama's video memory only takes up over 700, and it hasn't been loaded yet\n![Image](https://github.com/user-attachments/assets/bf06cc0e-144a-452b-8bcb-d625effbe133)\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "smileyboy2019",
        "body": "ollama version 0.8 0.9  os ：centos 7.9"
      },
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will aid in debugging."
      }
    ]
  },
  {
    "issue_number": 10421,
    "title": "AMD RX 6900 XT - ggml_cuda_init: failed to initialize ROCm: no ROCm-capable device is detected",
    "author": "moophlo",
    "state": "open",
    "created_at": "2025-04-26T16:04:59Z",
    "updated_at": "2025-06-17T07:13:10Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI'm running ollama:rocm container in a kubernetes cluster.\nThe worker node of the cluster has 2 GPU:\n\nGPU0: RX7900XT\nGPU1: RX6900XT\n\nI'm setting in the deployment all the relevant variables as per documentation:\n\n\nHIP_VISIBLE_DEVICES:1\nHSA_OVERRIDE_GFX_VERSION:10.3.0\nHCC_AMDGPU_TARGET:gfx1030\nROCR_VISIBLE_DEVICES:1\n\nWhile starting I can see the GPU is properly detected and the other one is filtered out as expected, but then at a certain point:\n\n```\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=memory.go:108 msg=evaluating library=rocm gpu_count=1 available=\"[16.0 GiB]\"\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.vision.block_count default=0\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.head_count_kv default=1\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.key_length default=64\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.value_length default=64\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.head_count_kv default=1\ntime=2025-04-26T15:49:31.376Z level=INFO source=sched.go:722 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d gpu=GPU-7685bd29c5086f3d parallel=1 available=17145991168 required=\"1.1 GiB\"\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"62.5 GiB\" before.free=\"31.5 GiB\" before.free_swap=\"0 B\" now.total=\"62.5 GiB\" now.free=\"31.5 GiB\" now.free_swap=\"0 B\"\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=amd_linux.go:488 msg=\"updating rocm free memory\" gpu=GPU-7685bd29c5086f3d name=1002:73bf before=\"16.0 GiB\" now=\"16.0 GiB\"\ntime=2025-04-26T15:49:31.376Z level=INFO source=server.go:105 msg=\"system memory\" total=\"62.5 GiB\" free=\"31.5 GiB\" free_swap=\"0 B\"\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=memory.go:108 msg=evaluating library=rocm gpu_count=1 available=\"[16.0 GiB]\"\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.vision.block_count default=0\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.head_count_kv default=1\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.key_length default=64\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.value_length default=64\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.head_count_kv default=1\ntime=2025-04-26T15:49:31.376Z level=INFO source=server.go:138 msg=offload library=rocm layers.requested=-1 layers.model=25 layers.offload=25 layers.split=\"\" memory.available=\"[16.0 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"1.1 GiB\" memory.required.partial=\"1.1 GiB\" memory.required.kv=\"3.0 MiB\" memory.required.allocations=\"[1.1 GiB]\" memory.weights.total=\"636.8 MiB\" memory.weights.repeating=\"577.2 MiB\" memory.weights.nonrepeating=\"59.6 MiB\" memory.graph.full=\"8.0 MiB\" memory.graph.partial=\"8.0 MiB\"\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=server.go:262 msg=\"compatible gpu libraries\" compatible=[rocm]\nllama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))\n```\n\nAt the moment I'm running in debug mode to obtain more verbosity.\n\nOne more element to add is that if I switch to GPU0 and I filter out the RX6900XT then is working properly and loading the model on the GPU and not falling back to the CPU backend.\n\nAny help would be really appreciated\n\n\n### Relevant log output\n\n```shell\n2025/04/26 15:48:54 routes.go:1232: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES:1 HSA_OVERRIDE_GFX_VERSION:10.3.0 HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/ollama/models/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:1 http_proxy: https_proxy: no_proxy:]\"\ntime=2025-04-26T15:48:54.977Z level=INFO source=images.go:458 msg=\"total blobs: 52\"\ntime=2025-04-26T15:48:54.978Z level=INFO source=images.go:465 msg=\"total unused blobs removed: 0\"\ntime=2025-04-26T15:48:54.978Z level=INFO source=routes.go:1299 msg=\"Listening on [::]:11434 (version 0.6.6)\"\ntime=2025-04-26T15:48:54.978Z level=DEBUG source=sched.go:107 msg=\"starting llm scheduler\"\ntime=2025-04-26T15:48:54.978Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-04-26T15:48:54.979Z level=DEBUG source=gpu.go:98 msg=\"searching for GPU discovery libraries for NVIDIA\"\ntime=2025-04-26T15:48:54.979Z level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=libcuda.so*\ntime=2025-04-26T15:48:54.979Z level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[/usr/lib/ollama/libcuda.so* /usr/local/nvidia/lib/libcuda.so* /usr/local/nvidia/lib64/libcuda.so* /usr/local/cuda*/targets/*/lib/libcuda.so* /usr/lib/*-linux-gnu/nvidia/current/libcuda.so* /usr/lib/*-linux-gnu/libcuda.so* /usr/lib/wsl/lib/libcuda.so* /usr/lib/wsl/drivers/*/libcuda.so* /opt/cuda/lib*/libcuda.so* /usr/local/cuda/lib*/libcuda.so* /usr/lib*/libcuda.so* /usr/local/lib*/libcuda.so*]\"\ntime=2025-04-26T15:48:55.048Z level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=[]\ntime=2025-04-26T15:48:55.048Z level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=libcudart.so*\ntime=2025-04-26T15:48:55.048Z level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[/usr/lib/ollama/libcudart.so* /usr/local/nvidia/lib/libcudart.so* /usr/local/nvidia/lib64/libcudart.so* /usr/lib/ollama/cuda_v*/libcudart.so* /usr/local/cuda/lib64/libcudart.so* /usr/lib/x86_64-linux-gnu/nvidia/current/libcudart.so* /usr/lib/x86_64-linux-gnu/libcudart.so* /usr/lib/wsl/lib/libcudart.so* /usr/lib/wsl/drivers/*/libcudart.so* /opt/cuda/lib64/libcudart.so* /usr/local/cuda*/targets/aarch64-linux/lib/libcudart.so* /usr/lib/aarch64-linux-gnu/nvidia/current/libcudart.so* /usr/lib/aarch64-linux-gnu/libcudart.so* /usr/local/cuda/lib*/libcudart.so* /usr/lib*/libcudart.so* /usr/local/lib*/libcudart.so*]\"\ntime=2025-04-26T15:48:55.049Z level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=[]\ntime=2025-04-26T15:48:55.049Z level=DEBUG source=amd_linux.go:101 msg=\"evaluating amdgpu node /sys/class/kfd/kfd/topology/nodes/0/properties\"\ntime=2025-04-26T15:48:55.049Z level=DEBUG source=amd_linux.go:121 msg=\"detected CPU /sys/class/kfd/kfd/topology/nodes/0/properties\"\ntime=2025-04-26T15:48:55.049Z level=DEBUG source=amd_linux.go:101 msg=\"evaluating amdgpu node /sys/class/kfd/kfd/topology/nodes/1/properties\"\ntime=2025-04-26T15:48:55.049Z level=DEBUG source=amd_linux.go:206 msg=\"mapping amdgpu to drm sysfs nodes\" amdgpu=/sys/class/kfd/kfd/topology/nodes/1/properties vendor=4098 device=29772 unique_id=7433242190493591676\ntime=2025-04-26T15:48:55.049Z level=DEBUG source=amd_linux.go:240 msg=matched amdgpu=/sys/class/kfd/kfd/topology/nodes/1/properties drm=/sys/class/drm/card1/device\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:318 msg=\"amdgpu memory\" gpu=0 total=\"20.0 GiB\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:319 msg=\"amdgpu memory\" gpu=0 available=\"20.0 GiB\"\ntime=2025-04-26T15:48:55.050Z level=INFO source=amd_linux.go:332 msg=\"filtering out device per user request\" id=GPU-67282e63a5d1287c visible_devices=[1]\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:101 msg=\"evaluating amdgpu node /sys/class/kfd/kfd/topology/nodes/2/properties\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:206 msg=\"mapping amdgpu to drm sysfs nodes\" amdgpu=/sys/class/kfd/kfd/topology/nodes/2/properties vendor=4098 device=29631 unique_id=8540440255474986813\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:219 msg=\"failed to read sysfs node\" file=/sys/class/drm/card1-DP-1/device/vendor error=\"open /sys/class/drm/card1-DP-1/device/vendor: no such file or directory\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:219 msg=\"failed to read sysfs node\" file=/sys/class/drm/card1-DP-2/device/vendor error=\"open /sys/class/drm/card1-DP-2/device/vendor: no such file or directory\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:219 msg=\"failed to read sysfs node\" file=/sys/class/drm/card1-DP-3/device/vendor error=\"open /sys/class/drm/card1-DP-3/device/vendor: no such file or directory\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:219 msg=\"failed to read sysfs node\" file=/sys/class/drm/card1-HDMI-A-1/device/vendor error=\"open /sys/class/drm/card1-HDMI-A-1/device/vendor: no such file or directory\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:219 msg=\"failed to read sysfs node\" file=/sys/class/drm/card1-Writeback-1/device/vendor error=\"open /sys/class/drm/card1-Writeback-1/device/vendor: no such file or directory\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:240 msg=matched amdgpu=/sys/class/kfd/kfd/topology/nodes/2/properties drm=/sys/class/drm/card2/device\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:318 msg=\"amdgpu memory\" gpu=1 total=\"16.0 GiB\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_linux.go:319 msg=\"amdgpu memory\" gpu=1 available=\"16.0 GiB\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_common.go:16 msg=\"evaluating potential rocm lib dir /usr/lib/ollama/rocm\"\ntime=2025-04-26T15:48:55.050Z level=DEBUG source=amd_common.go:44 msg=\"detected ROCM next to ollama executable /usr/lib/ollama/rocm\"\ntime=2025-04-26T15:48:55.050Z level=INFO source=amd_linux.go:389 msg=\"skipping rocm gfx compatibility check\" HSA_OVERRIDE_GFX_VERSION=10.3.0\ntime=2025-04-26T15:48:55.054Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-7685bd29c5086f3d library=rocm variant=\"\" compute=gfx1030 driver=6.12 name=1002:73bf total=\"16.0 GiB\" available=\"16.0 GiB\"\n[GIN] 2025/04/26 - 15:48:55 |\u001b[97;42m 200 \u001b[0m|      45.608µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:48:56 |\u001b[97;42m 200 \u001b[0m|      27.346µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:48:57 |\u001b[97;42m 200 \u001b[0m|      18.863µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:48:58 |\u001b[97;42m 200 \u001b[0m|      20.339µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:48:59 |\u001b[97;42m 200 \u001b[0m|      27.412µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:00 |\u001b[97;42m 200 \u001b[0m|      24.791µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:01 |\u001b[97;42m 200 \u001b[0m|      15.701µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:02 |\u001b[97;42m 200 \u001b[0m|      14.232µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:03 |\u001b[97;42m 200 \u001b[0m|      17.825µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:04 |\u001b[97;42m 200 \u001b[0m|      17.923µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:05 |\u001b[97;42m 200 \u001b[0m|      24.265µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:06 |\u001b[97;42m 200 \u001b[0m|      25.764µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:07 |\u001b[97;42m 200 \u001b[0m|      14.055µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:08 |\u001b[97;42m 200 \u001b[0m|      12.819µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:09 |\u001b[97;42m 200 \u001b[0m|      13.869µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:10 |\u001b[97;42m 200 \u001b[0m|      13.224µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:11 |\u001b[97;42m 200 \u001b[0m|      13.465µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:12 |\u001b[97;42m 200 \u001b[0m|      33.276µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:13 |\u001b[97;42m 200 \u001b[0m|      14.354µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:14 |\u001b[97;42m 200 \u001b[0m|      14.805µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:15 |\u001b[97;42m 200 \u001b[0m|      13.669µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:16 |\u001b[97;42m 200 \u001b[0m|      14.005µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:17 |\u001b[97;42m 200 \u001b[0m|      17.338µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:18 |\u001b[97;42m 200 \u001b[0m|      18.535µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:19 |\u001b[97;42m 200 \u001b[0m|      23.488µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:20 |\u001b[97;42m 200 \u001b[0m|      17.602µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:21 |\u001b[97;42m 200 \u001b[0m|      13.956µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:22 |\u001b[97;42m 200 \u001b[0m|      13.766µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:23 |\u001b[97;42m 200 \u001b[0m|      21.209µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:24 |\u001b[97;42m 200 \u001b[0m|       17.51µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:25 |\u001b[97;42m 200 \u001b[0m|      18.154µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:26 |\u001b[97;42m 200 \u001b[0m|     179.984µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:27 |\u001b[97;42m 200 \u001b[0m|       22.29µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:28 |\u001b[97;42m 200 \u001b[0m|      14.871µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:29 |\u001b[97;42m 200 \u001b[0m|       22.09µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:31 |\u001b[97;42m 200 \u001b[0m|      14.426µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\ntime=2025-04-26T15:49:31.370Z level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-26T15:49:31.371Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"62.5 GiB\" before.free=\"31.7 GiB\" before.free_swap=\"0 B\" now.total=\"62.5 GiB\" now.free=\"31.5 GiB\" now.free_swap=\"0 B\"\ntime=2025-04-26T15:49:31.371Z level=DEBUG source=amd_linux.go:488 msg=\"updating rocm free memory\" gpu=GPU-7685bd29c5086f3d name=1002:73bf before=\"16.0 GiB\" now=\"16.0 GiB\"\ntime=2025-04-26T15:49:31.371Z level=DEBUG source=sched.go:183 msg=\"updating default concurrency\" OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1\ntime=2025-04-26T15:49:31.373Z level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-26T15:49:31.375Z level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=sched.go:226 msg=\"loading first model\" model=/home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=memory.go:108 msg=evaluating library=rocm gpu_count=1 available=\"[16.0 GiB]\"\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.vision.block_count default=0\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.head_count_kv default=1\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.key_length default=64\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.value_length default=64\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.head_count_kv default=1\ntime=2025-04-26T15:49:31.376Z level=INFO source=sched.go:722 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d gpu=GPU-7685bd29c5086f3d parallel=1 available=17145991168 required=\"1.1 GiB\"\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"62.5 GiB\" before.free=\"31.5 GiB\" before.free_swap=\"0 B\" now.total=\"62.5 GiB\" now.free=\"31.5 GiB\" now.free_swap=\"0 B\"\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=amd_linux.go:488 msg=\"updating rocm free memory\" gpu=GPU-7685bd29c5086f3d name=1002:73bf before=\"16.0 GiB\" now=\"16.0 GiB\"\ntime=2025-04-26T15:49:31.376Z level=INFO source=server.go:105 msg=\"system memory\" total=\"62.5 GiB\" free=\"31.5 GiB\" free_swap=\"0 B\"\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=memory.go:108 msg=evaluating library=rocm gpu_count=1 available=\"[16.0 GiB]\"\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.vision.block_count default=0\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.head_count_kv default=1\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.key_length default=64\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.value_length default=64\ntime=2025-04-26T15:49:31.376Z level=WARN source=ggml.go:152 msg=\"key not found\" key=bert.attention.head_count_kv default=1\ntime=2025-04-26T15:49:31.376Z level=INFO source=server.go:138 msg=offload library=rocm layers.requested=-1 layers.model=25 layers.offload=25 layers.split=\"\" memory.available=\"[16.0 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"1.1 GiB\" memory.required.partial=\"1.1 GiB\" memory.required.kv=\"3.0 MiB\" memory.required.allocations=\"[1.1 GiB]\" memory.weights.total=\"636.8 MiB\" memory.weights.repeating=\"577.2 MiB\" memory.weights.nonrepeating=\"59.6 MiB\" memory.graph.full=\"8.0 MiB\" memory.graph.partial=\"8.0 MiB\"\ntime=2025-04-26T15:49:31.376Z level=DEBUG source=server.go:262 msg=\"compatible gpu libraries\" compatible=[rocm]\nllama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = bert\nllama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1\nllama_model_loader: - kv   2:                           bert.block_count u32              = 24\nllama_model_loader: - kv   3:                        bert.context_length u32              = 512\nllama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024\nllama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096\nllama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16\nllama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\nllama_model_loader: - kv   9:                      bert.attention.causal bool             = false\nllama_model_loader: - kv  10:                          bert.pooling_type u32              = 2\nllama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\nllama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101\nllama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100\nllama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101\nllama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103\nllama_model_loader: - type  f32:  243 tensors\nllama_model_loader: - type  f16:  146 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 637.85 MiB (16.02 BPW) \ninit_tokenizer: initializing tokenizer for type 3\nload: control token:    101 '[CLS]' is not marked as EOG\nload: control token:    103 '[MASK]' is not marked as EOG\nload: control token:      0 '[PAD]' is not marked as EOG\nload: control token:    100 '[UNK]' is not marked as EOG\nload: control token:    102 '[SEP]' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 5\nload: token to piece cache size = 0.2032 MB\nprint_info: arch             = bert\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 334.09 M\nprint_info: general.name     = mxbai-embed-large-v1\nprint_info: vocab type       = WPM\nprint_info: n_vocab          = 30522\nprint_info: n_merges         = 0\nprint_info: BOS token        = 101 '[CLS]'\nprint_info: EOS token        = 102 '[SEP]'\nprint_info: UNK token        = 100 '[UNK]'\nprint_info: SEP token        = 102 '[SEP]'\nprint_info: PAD token        = 0 '[PAD]'\nprint_info: MASK token       = 103 '[MASK]'\nprint_info: LF token         = 0 '[PAD]'\nprint_info: EOG token        = 102 '[SEP]'\nprint_info: max token length = 21\nllama_model_load: vocab only - skipping tensors\n[GIN] 2025/04/26 - 15:49:32 |\u001b[97;42m 200 \u001b[0m|      23.128µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\ntime=2025-04-26T15:49:32.367Z level=DEBUG source=server.go:335 msg=\"adding gpu library\" path=/usr/lib/ollama/rocm\ntime=2025-04-26T15:49:32.367Z level=DEBUG source=server.go:343 msg=\"adding gpu dependency paths\" paths=[/usr/lib/ollama/rocm]\ntime=2025-04-26T15:49:32.368Z level=INFO source=server.go:405 msg=\"starting llama server\" cmd=\"/usr/bin/ollama runner --model /home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d --ctx-size 512 --batch-size 512 --n-gpu-layers 25 --verbose --threads 8 --parallel 1 --port 21385\"\ntime=2025-04-26T15:49:32.368Z level=DEBUG source=server.go:423 msg=subprocess environment=\"[PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin LD_LIBRARY_PATH=/usr/lib/ollama/rocm:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib/ollama/rocm:/usr/lib/ollama HIP_VISIBLE_DEVICES=1 ROCR_VISIBLE_DEVICES=GPU-7685bd29c5086f3d ROCM_VISIBLE_DEVICES=1 HSA_OVERRIDE_GFX_VERSION=10.3.0]\"\ntime=2025-04-26T15:49:32.368Z level=INFO source=sched.go:451 msg=\"loaded runners\" count=1\ntime=2025-04-26T15:49:32.368Z level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\ntime=2025-04-26T15:49:32.368Z level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-04-26T15:49:32.377Z level=INFO source=runner.go:853 msg=\"starting go runner\"\ntime=2025-04-26T15:49:32.377Z level=DEBUG source=ggml.go:99 msg=\"ggml backend load all from path\" path=/usr/lib/ollama/rocm\n[GIN] 2025/04/26 - 15:49:33 |\u001b[97;42m 200 \u001b[0m|      19.852µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\nggml_cuda_init: failed to initialize ROCm: no ROCm-capable device is detected\nload_backend: loaded ROCm backend from /usr/lib/ollama/rocm/libggml-hip.so\ntime=2025-04-26T15:49:33.155Z level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=/usr/local/nvidia/lib\ntime=2025-04-26T15:49:33.155Z level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=/usr/local/nvidia/lib64\ntime=2025-04-26T15:49:33.155Z level=DEBUG source=ggml.go:99 msg=\"ggml backend load all from path\" path=/usr/lib/ollama\nload_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-alderlake.so\ntime=2025-04-26T15:49:33.156Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\ntime=2025-04-26T15:49:33.156Z level=INFO source=runner.go:913 msg=\"Server listening on 127.0.0.1:21385\"\nllama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = bert\nllama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1\nllama_model_loader: - kv   2:                           bert.block_count u32              = 24\nllama_model_loader: - kv   3:                        bert.context_length u32              = 512\nllama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024\nllama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096\nllama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16\nllama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\nllama_model_loader: - kv   9:                      bert.attention.causal bool             = false\nllama_model_loader: - kv  10:                          bert.pooling_type u32              = 2\nllama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\nllama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101\nllama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100\nllama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101\nllama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103\nllama_model_loader: - type  f32:  243 tensors\nllama_model_loader: - type  f16:  146 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 637.85 MiB (16.02 BPW) \ninit_tokenizer: initializing tokenizer for type 3\nload: control token:    101 '[CLS]' is not marked as EOG\nload: control token:    103 '[MASK]' is not marked as EOG\nload: control token:      0 '[PAD]' is not marked as EOG\nload: control token:    100 '[UNK]' is not marked as EOG\nload: control token:    102 '[SEP]' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 5\nload: token to piece cache size = 0.2032 MB\nprint_info: arch             = bert\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 512\nprint_info: n_embd           = 1024\nprint_info: n_layer          = 24\nprint_info: n_head           = 16\nprint_info: n_head_kv        = 16\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 1.0e-12\nprint_info: f_norm_rms_eps   = 0.0e+00\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 4096\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 0\nprint_info: pooling type     = 2\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 512\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 335M\nprint_info: model params     = 334.09 M\nprint_info: general.name     = mxbai-embed-large-v1\nprint_info: vocab type       = WPM\nprint_info: n_vocab          = 30522\nprint_info: n_merges         = 0\nprint_info: BOS token        = 101 '[CLS]'\nprint_info: EOS token        = 102 '[SEP]'\nprint_info: UNK token        = 100 '[UNK]'\nprint_info: SEP token        = 102 '[SEP]'\nprint_info: PAD token        = 0 '[PAD]'\nprint_info: MASK token       = 103 '[MASK]'\nprint_info: LF token         = 0 '[PAD]'\nprint_info: EOG token        = 102 '[SEP]'\nprint_info: max token length = 21\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CPU, is_swa = 0\nload_tensors: layer   1 assigned to device CPU, is_swa = 0\nload_tensors: layer   2 assigned to device CPU, is_swa = 0\nload_tensors: layer   3 assigned to device CPU, is_swa = 0\nload_tensors: layer   4 assigned to device CPU, is_swa = 0\nload_tensors: layer   5 assigned to device CPU, is_swa = 0\nload_tensors: layer   6 assigned to device CPU, is_swa = 0\nload_tensors: layer   7 assigned to device CPU, is_swa = 0\nload_tensors: layer   8 assigned to device CPU, is_swa = 0\nload_tensors: layer   9 assigned to device CPU, is_swa = 0\nload_tensors: layer  10 assigned to device CPU, is_swa = 0\nload_tensors: layer  11 assigned to device CPU, is_swa = 0\nload_tensors: layer  12 assigned to device CPU, is_swa = 0\nload_tensors: layer  13 assigned to device CPU, is_swa = 0\nload_tensors: layer  14 assigned to device CPU, is_swa = 0\nload_tensors: layer  15 assigned to device CPU, is_swa = 0\nload_tensors: layer  16 assigned to device CPU, is_swa = 0\nload_tensors: layer  17 assigned to device CPU, is_swa = 0\nload_tensors: layer  18 assigned to device CPU, is_swa = 0\nload_tensors: layer  19 assigned to device CPU, is_swa = 0\nload_tensors: layer  20 assigned to device CPU, is_swa = 0\nload_tensors: layer  21 assigned to device CPU, is_swa = 0\nload_tensors: layer  22 assigned to device CPU, is_swa = 0\nload_tensors: layer  23 assigned to device CPU, is_swa = 0\nload_tensors: layer  24 assigned to device CPU, is_swa = 0\ntime=2025-04-26T15:49:33.372Z level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n[GIN] 2025/04/26 - 15:49:34 |\u001b[97;42m 200 \u001b[0m|       16.28µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:35 |\u001b[97;42m 200 \u001b[0m|      12.119µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:36 |\u001b[97;42m 200 \u001b[0m|      12.501µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:37 |\u001b[97;42m 200 \u001b[0m|      16.986µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:38 |\u001b[97;42m 200 \u001b[0m|      18.783µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:39 |\u001b[97;42m 200 \u001b[0m|      13.436µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:40 |\u001b[97;42m 200 \u001b[0m|      19.586µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:41 |\u001b[97;42m 200 \u001b[0m|      23.479µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:42 |\u001b[97;42m 200 \u001b[0m|      23.868µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:43 |\u001b[97;42m 200 \u001b[0m|      13.501µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:44 |\u001b[97;42m 200 \u001b[0m|      14.025µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:45 |\u001b[97;42m 200 \u001b[0m|       13.98µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:46 |\u001b[97;42m 200 \u001b[0m|      22.553µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:47 |\u001b[97;42m 200 \u001b[0m|      13.092µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:48 |\u001b[97;42m 200 \u001b[0m|      19.752µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:49 |\u001b[97;42m 200 \u001b[0m|      12.943µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:50 |\u001b[97;42m 200 \u001b[0m|      13.455µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:51 |\u001b[97;42m 200 \u001b[0m|      12.638µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:52 |\u001b[97;42m 200 \u001b[0m|      21.598µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\nload_tensors:   CPU_Mapped model buffer size =   637.85 MiB\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 512\nllama_context: n_ctx_per_seq = 512\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 0\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nset_abort_callback: call\nllama_context:        CPU  output buffer size =     0.00 MiB\nllama_context: n_ctx = 512\nllama_context: n_ctx = 512 (padded)\ninit: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\ninit: layer   0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer   1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer   2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer   3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer   4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer   5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer   6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer   7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer   8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer   9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit: layer  23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CPU\ninit:        CPU KV buffer size =    48.00 MiB\nllama_context: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 1\nllama_context: max_nodes = 65536\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\nllama_context: reserving graph for n_tokens = 512, n_seqs = 1\nllama_context: reserving graph for n_tokens = 1, n_seqs = 1\nllama_context: reserving graph for n_tokens = 512, n_seqs = 1\nllama_context:        CPU compute buffer size =    27.01 MiB\nllama_context: graph nodes  = 825\nllama_context: graph splits = 1\ntime=2025-04-26T15:49:53.446Z level=INFO source=server.go:619 msg=\"llama runner started in 21.08 seconds\"\ntime=2025-04-26T15:49:53.446Z level=DEBUG source=sched.go:464 msg=\"finished setting up runner\" model=/home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d\ntime=2025-04-26T15:49:53.450Z level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-26T15:49:53.452Z level=DEBUG source=runner.go:686 msg=\"embedding request\" content=\"tell me more\"\ntime=2025-04-26T15:49:53.454Z level=DEBUG source=cache.go:104 msg=\"loading cache slot\" id=0 cache=0 prompt=5 used=0 remaining=5\n[GIN] 2025/04/26 - 15:49:53 |\u001b[97;42m 200 \u001b[0m| 22.671213222s |  173.249.47.211 |\u001b[97;46m POST    \u001b[0m \"/api/embed\"\ntime=2025-04-26T15:49:53.477Z level=DEBUG source=sched.go:468 msg=\"context for request finished\"\ntime=2025-04-26T15:49:53.477Z level=DEBUG source=sched.go:341 msg=\"runner with non-zero duration has gone idle, adding timer\" modelPath=/home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d duration=5m0s\ntime=2025-04-26T15:49:53.477Z level=DEBUG source=sched.go:359 msg=\"after processing request finished event\" modelPath=/home/ollama/models/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d refCount=0\n[GIN] 2025/04/26 - 15:49:53 |\u001b[97;42m 200 \u001b[0m|      15.649µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:54 |\u001b[97;42m 200 \u001b[0m|      19.318µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:55 |\u001b[97;42m 200 \u001b[0m|      13.768µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:56 |\u001b[97;42m 200 \u001b[0m|      18.993µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:57 |\u001b[97;42m 200 \u001b[0m|      15.537µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:58 |\u001b[97;42m 200 \u001b[0m|      14.674µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:49:59 |\u001b[97;42m 200 \u001b[0m|       14.12µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:00 |\u001b[97;42m 200 \u001b[0m|       13.63µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:01 |\u001b[97;42m 200 \u001b[0m|      13.562µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:02 |\u001b[97;42m 200 \u001b[0m|      14.125µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:03 |\u001b[97;42m 200 \u001b[0m|      14.033µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:04 |\u001b[97;42m 200 \u001b[0m|      13.703µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:05 |\u001b[97;42m 200 \u001b[0m|       14.48µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:06 |\u001b[97;42m 200 \u001b[0m|      16.471µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:07 |\u001b[97;42m 200 \u001b[0m|      15.057µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:08 |\u001b[97;42m 200 \u001b[0m|      14.551µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:09 |\u001b[97;42m 200 \u001b[0m|      13.898µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:10 |\u001b[97;42m 200 \u001b[0m|      18.579µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:12 |\u001b[97;42m 200 \u001b[0m|      14.124µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:13 |\u001b[97;42m 200 \u001b[0m|      13.713µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:14 |\u001b[97;42m 200 \u001b[0m|      16.543µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:15 |\u001b[97;42m 200 \u001b[0m|      22.086µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:16 |\u001b[97;42m 200 \u001b[0m|      13.266µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:17 |\u001b[97;42m 200 \u001b[0m|      13.639µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:18 |\u001b[97;42m 200 \u001b[0m|      14.333µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n[GIN] 2025/04/26 - 15:50:19 |\u001b[97;42m 200 \u001b[0m|      14.326µs |    192.168.10.1 |\u001b[97;44m GET     \u001b[0m \"/\"\n```\n\n### OS\n\nOS: Linux Mint 22.1 x86_64\n\n### GPU\n\nGPU0: AMD ATI Radeon RX 7900 XT/7900 XTX/7900 GRE/7900M\nGPU1: AMD ATI Radeon RX 6800/6800 XT / 6900 XT\n\n### CPU\n\nCPU: Intel i9-14900K (32) @ 5.700GHz\n\n### Ollama version\n\nollama/ollama:0.6.6-rocm\n",
    "comments": [
      {
        "user": "sunarowicz",
        "body": "Same problem here, I get\n`ggml_cuda_init: failed to initialize ROCm: no ROCm-capable device is detected`\n\nOllama version: 0.6.6 (installed in system, not container by official install script)\nROCm installed\nGPU: iGP 780M in Rzyen 7 78003D\nSystem variables used: HSA_OVERRIDE_GFX_VERSION=11.0.0 HCC_AMDGPU_TARGETS=gfx1103 CUDA_VISIBLE_DEVICES=-1\n\nrocminfo (truncated:\n```\n*******                  \nAgent 2                  \n*******                  \n  Name:                    gfx1036                            \n  Uuid:                    GPU-XX                             \n  Marketing Name:          AMD Radeon Graphics                \n  Vendor Name:             AMD               \n```      \nOllama messages (truncated):\n```\nllama_model_load: vocab only - skipping tensors\ntime=2025-05-02T16:00:01.289+02:00 level=INFO source=server.go:405 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /home/ai/.ollama/models/blobs/sha256-5ee4f07cdb9beadbbb293e85803c569b01bd37ed059d2715faa7bb405f31caa6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 37 --threads 8 --parallel 4 --port 40579\"\ntime=2025-05-02T16:00:01.290+02:00 level=INFO source=sched.go:451 msg=\"loaded runners\" count=1\ntime=2025-05-02T16:00:01.290+02:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\ntime=2025-05-02T16:00:01.290+02:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-05-02T16:00:01.300+02:00 level=INFO source=runner.go:853 msg=\"starting go runner\"\nggml_cuda_init: failed to initialize ROCm: no ROCm-capable device is detected\nload_backend: loaded ROCm backend from /usr/local/lib/ollama/rocm/libggml-hip.so\nload_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so\ntime=2025-05-02T16:00:01.344+02:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\ntime=2025-05-02T16:00:01.345+02:00 level=INFO source=runner.go:913 msg=\"Server listening on 127.0.0.1:40579\"\n```\nAny idea how to solve?\n\n"
      },
      {
        "user": "moophlo",
        "body": "Not sure about iGPU but try with these:\n\nHIP_VISIBLE_DEVICES=1\nHSA_OVERRIDE_GFX_VERSION=11.0.0\nHCC_AMDGPU_TARGET=gfx1100\nROCR_VISIBLE_DEVICES=1\n\nActually if you only have the Integrated GPU you shouldn't need any variable at all, ideally it should autodetect"
      },
      {
        "user": "sunarowicz",
        "body": "When I start server with AMD_LOG_LEVEL=3 and OLLAMA_DEBUG=1 I get the following additional info:\n```\ntime=2025-05-02T16:24:57.999+02:00 level=INFO source=runner.go:853 msg=\"starting go runner\"\ntime=2025-05-02T16:24:58.000+02:00 level=DEBUG source=ggml.go:99 msg=\"ggml backend load all from path\" path=/usr/local/lib/ollama/rocm\n:3:rocdevice.cpp            :469 : 145600849622d us:  Initializing HSA stack.\n:3:rocdevice.cpp            :555 : 145600858892d us:  Enumerated GPU agents = 0\n:3:hip_context.cpp          :49  : 145600858897d us:  Direct Dispatch: 1\n:3:hip_device_runtime.cpp   :649 : 145600858914d us:   hipGetDeviceCount ( 0x72165e6c4310 ) \n:3:hip_device_runtime.cpp   :651 : 145600858917d us:  hipGetDeviceCount: Returned hipErrorNoDevice : \nggml_cuda_init: failed to initialize ROCm: no ROCm-capable device is detected\nload_backend: loaded ROCm backend from /usr/local/lib/ollama/rocm/libggml-hip.so\n```\nBut still do not understand why I get \"Enumerated GPU agents = 0\" when rocminfo reports the GPU agent."
      }
    ]
  },
  {
    "issue_number": 11060,
    "title": "New Ollama Engine Causes Severe Performance Degradation (10x Slower)",
    "author": "AmesianX",
    "state": "open",
    "created_at": "2025-06-13T01:09:29Z",
    "updated_at": "2025-06-17T06:05:52Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n<html>\n<body>\n<!--StartFragment--><hr class=\"border-gray-100 dark:border-gray-850\" style=\"box-sizing: border-box; border-width: 1px 0px 0px; border-style: solid; border-color: rgb(236, 236, 236); border-image: initial; margin: 3em 0px; padding: 0px; height: 0px; color: rgb(78, 78, 78); margin-block: 16px; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><h2 dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px 0px 1em; padding: 0px; font-size: 1.5em; font-weight: 600; color: oklch(0.21 0.034 264.665); line-height: 1.33333; margin-block: 4px; --tw-font-weight: 600; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">📝 Technical Reporting</h2><blockquote dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px 0px 1.6em; padding: 0px; color: oklch(0.21 0.034 264.665); border-inline-start: 2px solid rgb(236, 236, 236); quotes: &quot;“&quot; &quot;”&quot; &quot;‘&quot; &quot;’&quot;; padding-inline-start: 1em; font-style: normal; font-weight: 400; margin-block: 0px; --tw-font-weight: 400; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><p dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 1.25em 0px; padding: 0px; margin-block: 0px;\">Hello Ollama Team,</p><div class=\"my-2\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; margin-block: 8px;\"></div><p dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 1.25em 0px; padding: 0px; margin-block: 0px;\">I'm using Ollama for local LLM inference, and I've encountered a <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: inherit;\">critical performance regression</strong> in the latest version of the engine.<br style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px;\">By setting the environment variable <code class=\"codespan cursor-pointer\" style=\"box-sizing: border-box; border: 0px solid rgb(235, 87, 87); margin: 0px; padding: 3px 8px; font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace; font-feature-settings: normal; font-variation-settings: normal; font-size: 0.8em; cursor: pointer; width: auto; overflow-x: auto; color: rgb(235, 87, 87); margin-inline: 2px; border-radius: 6px; background-color: rgb(236, 236, 236); font-weight: 600;\">-e OLLAMA_NEW_ENGINE=false</code>, the inference speed of <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: inherit;\">Qwen3</strong> increases dramatically, while using the new engine causes a <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: inherit;\">significant slowdown</strong>. </p></blockquote><hr class=\"border-gray-100 dark:border-gray-850\" style=\"box-sizing: border-box; border-width: 1px 0px 0px; border-style: solid; border-color: rgb(236, 236, 236); border-image: initial; margin: 3em 0px; padding: 0px; height: 0px; color: rgb(78, 78, 78); margin-block: 16px; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><h3 dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px 0px 0.6em; padding: 0px; font-size: 1.25em; font-weight: 600; color: oklch(0.21 0.034 264.665); line-height: 1.6; margin-block: 4px; --tw-font-weight: 600; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">🔧 Environment Info</h3><ul dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px 0px 1.25em; padding: 0px; list-style: disc; padding-inline-start: 1.625em; margin-block: 0px; color: rgb(78, 78, 78); font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">GPU</strong>: NVIDIA RTX 3090 Ti (1x)</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">OLLAMA Version</strong>: v0.9 (new engine) and v0.7 (old engine)</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">Model</strong>: Qwen3 (or similar large LLM)</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">Test Method</strong>: <code class=\"codespan cursor-pointer\" style=\"box-sizing: border-box; border: 0px solid rgb(235, 87, 87); margin: 0px; padding: 3px 8px; font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace; font-feature-settings: normal; font-variation-settings: normal; font-size: 0.8em; cursor: pointer; width: auto; overflow-x: auto; color: rgb(235, 87, 87); margin-inline: 2px; border-radius: 6px; background-color: rgb(236, 236, 236); font-weight: 600;\">ollama pull qwen3:30b-a3b + Open WebUI </code></li></ul><div class=\"my-2\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px; padding: 0px; margin-block: 8px; color: rgb(78, 78, 78); font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></div><hr class=\"border-gray-100 dark:border-gray-850\" style=\"box-sizing: border-box; border-width: 1px 0px 0px; border-style: solid; border-color: rgb(236, 236, 236); border-image: initial; margin: 3em 0px; padding: 0px; height: 0px; color: rgb(78, 78, 78); margin-block: 16px; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><div class=\"absolute top-1 right-1.5 z-20 invisible group-hover:visible\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px; padding: 0px; visibility: hidden; position: absolute; top: 4px; right: 6px; z-index: 20;\"><div class=\"flex\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px; padding: 0px; display: flex;\"><button class=\"p-1 rounded-lg bg-transparent transition\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px; padding: 4px; font: inherit; letter-spacing: inherit; color: inherit; opacity: 1; background-color: rgba(0, 0, 0, 0); border-radius: 8px; appearance: button; cursor: pointer; transition-property: color, background-color, border-color, outline-color, text-decoration-color, fill, stroke, --tw-gradient-from, --tw-gradient-via, --tw-gradient-to, opacity, box-shadow, transform, translate, scale, rotate, filter, -webkit-backdrop-filter, backdrop-filter; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); transition-duration: 0.15s;\"><svg xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" viewBox=\"0 0 24 24\" stroke-width=\"1.5\" stroke=\"currentColor\" class=\" size-3.5\"><path stroke-linecap=\"round\" stroke-linejoin=\"round\" d=\"M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3\"></path></svg></button></div></div></div><hr class=\"border-gray-100 dark:border-gray-850\" style=\"box-sizing: border-box; border-width: 1px 0px 0px; border-style: solid; border-color: rgb(236, 236, 236); border-image: initial; margin: 3em 0px; padding: 0px; height: 0px; color: rgb(78, 78, 78); margin-block: 16px; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><h3 dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px 0px 0.6em; padding: 0px; font-size: 1.25em; font-weight: 600; color: oklch(0.21 0.034 264.665); line-height: 1.6; margin-block: 4px; --tw-font-weight: 600; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">📌 Problem Description</h3><ul dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px 0px 1.25em; padding: 0px; list-style: disc; padding-inline-start: 1.625em; margin-block: 0px; color: rgb(78, 78, 78); font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><p style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 1.25em 0px; padding: 0px; display: inline; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">With the new engine</strong>:</p><ul dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.75em 0px; padding: 0px; list-style: disc; padding-inline-start: 1.625em; margin-block: 0px;\"><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\">Inference speed is <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">10x slower</strong> (from 100 to 12–60 tokens/second).</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">Response latency is long and unstable</strong> (takes several seconds before the response starts).</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">GPU utilization is inconsistent</strong>, even on a single RTX 3090 Ti.</li></ul></li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><p style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 1.25em 0px; padding: 0px; display: inline; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">With the old engine</strong>:</p><ul dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.75em 0px; padding: 0px; list-style: disc; padding-inline-start: 1.625em; margin-block: 0px;\"><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">High-speed inference</strong> (over 100 tokens/second).</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">Stable and fast response time</strong> (response starts immediately after request).</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\"><strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">Efficient GPU usage</strong>.</li></ul></li></ul><div class=\"my-2\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px; padding: 0px; margin-block: 8px; color: rgb(78, 78, 78); font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></div><hr class=\"border-gray-100 dark:border-gray-850\" style=\"box-sizing: border-box; border-width: 1px 0px 0px; border-style: solid; border-color: rgb(236, 236, 236); border-image: initial; margin: 3em 0px; padding: 0px; height: 0px; color: rgb(78, 78, 78); margin-block: 16px; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><h3 dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px 0px 0.6em; padding: 0px; font-size: 1.25em; font-weight: 600; color: oklch(0.21 0.034 264.665); line-height: 1.6; margin-block: 4px; --tw-font-weight: 600; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">🧠 Possible Cause</h3><ul dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px 0px 1.25em; padding: 0px; list-style: disc; padding-inline-start: 1.625em; margin-block: 0px; color: rgb(78, 78, 78); font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\">The new engine may have <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">changed how the model is loaded or cached</strong>.</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\">There could be <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">issues with GPU memory management</strong> or <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">CUDA interface changes</strong>.</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\">Possible <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">regression in dynamic batching or caching strategies</strong>.</li></ul><div class=\"my-2\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px; padding: 0px; margin-block: 8px; color: rgb(78, 78, 78); font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></div><hr class=\"border-gray-100 dark:border-gray-850\" style=\"box-sizing: border-box; border-width: 1px 0px 0px; border-style: solid; border-color: rgb(236, 236, 236); border-image: initial; margin: 3em 0px; padding: 0px; height: 0px; color: rgb(78, 78, 78); margin-block: 16px; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><h3 dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px 0px 0.6em; padding: 0px; font-size: 1.25em; font-weight: 600; color: oklch(0.21 0.034 264.665); line-height: 1.6; margin-block: 4px; --tw-font-weight: 600; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">📌 Request</h3><ul dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px 0px 1.25em; padding: 0px; list-style: disc; padding-inline-start: 1.625em; margin-block: 0px; color: rgb(78, 78, 78); font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\">Please <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">investigate the performance degradation</strong> in the new engine.</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\">Provide <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">a fix or performance improvement</strong> if possible.</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\">Maintain the <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">option to use the old engine</strong> (<code class=\"codespan cursor-pointer\" style=\"box-sizing: border-box; border: 0px solid rgb(235, 87, 87); margin: 0px; padding: 3px 8px; font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace; font-feature-settings: normal; font-variation-settings: normal; font-size: 0.8em; cursor: pointer; width: auto; overflow-x: auto; color: rgb(235, 87, 87); margin-inline: 2px; border-radius: 6px; background-color: rgb(236, 236, 236); font-weight: 600;\">-e OLLAMA_NEW_ENGINE=false</code> is currently working).</li><li class=\"text-start\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0.5em 0px; padding: 0px; text-align: start; padding-inline-start: 0.375em; margin-block: 0px;\">Add <strong style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px; padding: 0px; font-weight: 600; color: oklch(0.21 0.034 264.665);\">official notes or explanations</strong> about performance changes in future releases.</li></ul><div class=\"my-2\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px; padding: 0px; margin-block: 8px; color: rgb(78, 78, 78); font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></div><hr class=\"border-gray-100 dark:border-gray-850\" style=\"box-sizing: border-box; border-width: 1px 0px 0px; border-style: solid; border-color: rgb(236, 236, 236); border-image: initial; margin: 3em 0px; padding: 0px; height: 0px; color: rgb(78, 78, 78); margin-block: 16px; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><h3 dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid oklch(0.21 0.034 264.665); margin: 0px 0px 0.6em; padding: 0px; font-size: 1.25em; font-weight: 600; color: oklch(0.21 0.034 264.665); line-height: 1.6; margin-block: 4px; --tw-font-weight: 600; font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">📌 Thank You</h3><p dir=\"auto\" style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px 0px 1.25em; padding: 0px; margin-block: 0px; color: rgb(78, 78, 78); font-family: -apple-system, BlinkMacSystemFont, Inter, Vazirmatn, ui-sans-serif, system-ui, &quot;Segoe UI&quot;, Roboto, Ubuntu, Cantarell, &quot;Noto Sans&quot;, sans-serif, &quot;Helvetica Neue&quot;, Arial, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: pre-line; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Thank you for your time and effort in maintaining such a great tool.<br style=\"box-sizing: border-box; border: 0px solid rgb(78, 78, 78); margin: 0px; padding: 0px;\">I look forward to your response.</p><!--EndFragment-->\n</body>\n</html>\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "jmorganca",
        "body": "Hi there, which version of Ollama is this? In a more recent version this performance issue was fixed."
      },
      {
        "user": "jmorganca",
        "body": "Oops, sorry. Just saw the version you had."
      },
      {
        "user": "AmesianX",
        "body": "> Hi there, which version of Ollama is this? In a more recent version this performance issue was fixed.\n\nI always remove the \"ollama/ollama:latest\" docker image completely using \"docker rmi ollama/ollama:latest\" before running Ollama with Docker.\nIf you have fixed any issues in the latest version of Ollama, you must definitely replace the Docker image with the latest version.\nOtherwise, many people will experience inconvenience. "
      }
    ]
  },
  {
    "issue_number": 10970,
    "title": "Support for MedGemma",
    "author": "fighter3005",
    "state": "open",
    "created_at": "2025-06-04T15:47:13Z",
    "updated_at": "2025-06-17T03:23:23Z",
    "labels": [
      "model request"
    ],
    "body": "Hi, I would love to run MedGemma (4B with vision is broken for me) on my local WebUI! Since Gemma3 is supported, this might actually be easy?",
    "comments": [
      {
        "user": "matrix07012",
        "body": "For some reason the quantizations from HF don't work with vision. You have to download full fp16 version of the model and either use that or quantize it yourself with `ollama create -q q8_0 model`"
      },
      {
        "user": "fighter3005",
        "body": "The model does run, but it seems that image encoder is not part of the model weights!? I am not sure what is going on there. I get:\n`level=INFO source=server.go:809 msg=\"llm predict error: Failed to create new sequence: failed to process inputs: this model is missing data required for image input\"`."
      },
      {
        "user": "pupphelper",
        "body": "> For some reason the quantizations from HF don't work with vision. You have to download full fp16 version of the model and either use that or quantize it yourself with `ollama create -q q8_0 model`\n\nwhich fp16 model works? can you link it?  I tried some F16 GGUF same problem, I'm having the same issue, vison not working on openwebui"
      }
    ]
  },
  {
    "issue_number": 11034,
    "title": "Any plan to make MonkeyOCR (https://huggingface.co/echo840/MonkeyOCR) available on Ollama?",
    "author": "Phil2025Code",
    "state": "open",
    "created_at": "2025-06-10T09:05:37Z",
    "updated_at": "2025-06-17T02:17:55Z",
    "labels": [
      "model request"
    ],
    "body": "Any plan to make MonkeyOCR (https://huggingface.co/echo840/MonkeyOCR) available on Ollama? Thanks!",
    "comments": [
      {
        "user": "Phil2025Code",
        "body": "It looks like a SOTA OCR model\n\n![Image](https://github.com/user-attachments/assets/3bb327a5-b70e-4349-859b-1133fc15b2d8)"
      },
      {
        "user": "zhenweiding",
        "body": "https://github.com/Yuliang-Liu/MonkeyOCR/issues/17"
      }
    ]
  },
  {
    "issue_number": 11084,
    "title": "Support importing Qwen3",
    "author": "jwangkun",
    "state": "open",
    "created_at": "2025-06-16T09:31:20Z",
    "updated_at": "2025-06-17T01:17:47Z",
    "labels": [
      "feature request"
    ],
    "body": "### What is the issue?\n\nopying file sha256:8c145d34bb1e0d1d9a76fe06958c7293fd8426b3ba3ced3502b3d26861ee6957 100% \nconverting model \nError: unsupported architecture \"Qwen3ForCausalLM\"\n(base) a123@jwangkun DeepSeek-R1-Qwen3-8B-HngTrust-Fin-0616 % ollama -v\nollama version is 0.9.0\n(base) a123@jwangkun DeepSeek-R1-Qwen3-8B-HngTrust-Fin-0616 % \n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "The ollama import function only supports a subset of architectures.  For un-supported models, you can use [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf_update.py) to convert to GGUF and then import that."
      },
      {
        "user": "Kevin-v92",
        "body": "> The ollama import function only supports a subset of architectures. For un-supported models, you can use [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf_update.py) to convert to GGUF and then import that.\n\nwhen can support it? thanks!"
      }
    ]
  },
  {
    "issue_number": 11064,
    "title": "Magistral ignores nothink (v0.9.0)",
    "author": "chhu",
    "state": "closed",
    "created_at": "2025-06-13T07:28:14Z",
    "updated_at": "2025-06-17T00:06:03Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nProbably a template issue.\n```\nollama run magistral\n>>> /set nothink\nSet 'nothink' mode.\n>>> What is the current date?\n<think>\nOkay, I need to figure out what the current date is. But wait, I'm a text-based AI model, and my knowledge cutoff\nis 2023. That means I don't have real-time information or access to the internet to \n...\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nWindows\n\n### GPU\n\n_No response_\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "mmorys",
        "body": "With ollama 0.9.0, I have a related issue. Thinking occurs both with `/set think` and `/set nothink` but with different fences around the thinking.\n\n## `/set think`\n\nThinking fenced with `Thinking...` and `...done thinking.`.\n\n```text\n>>> What is the current date?\nThinking...\nOkay, the user wants to know the current date. But how do I know [...]\n...done thinking.\n\n**Summary:**\nSince I don't have [...]\n\n**Final Answer:**\nI don't have real-time information [...]\n```\n\n## `/set nothink`\n\nThinking fenced with `<think>` and `</think>`.\n\n```text\n>>> What is the current date?\n<think>\nAlright, I need to figure out the current date. But wait, [...]\n</think>\n\n**Summary:**\nAs an AI without real-time capabilities, [...]\n\nFinal answer in Markdown format:\n\nThe current date is not available [...]\n```\n\n## Template and System Prompt (`ollama show --modelfile magistral`)\n\n```\nTEMPLATE \"\"\"{{- range $i, $_ := .Messages }}\n{{- $last := eq (len (slice $.Messages $i)) 1}}\n{{- if eq .Role \"system\" }}[SYSTEM_PROMPT]{{ .Content }}[/SYSTEM_PROMPT]\n{{- else if eq .Role \"user\" }}[INST]{{ .Content }}[/INST]\n{{- else if eq .Role \"assistant\" }}\n{{- if and $.IsThinkSet (and $last .Thinking) -}}\n<think>\n{{ .Thinking }}\n</think>\n{{- end }}\n{{- if .Content }}{{ .Content }}\n{{- end }}\n{{- if not (eq (len (slice $.Messages $i)) 1) }}</s>\n{{- end }}\n{{- end }}\n{{- end }}\"\"\"\nSYSTEM \"A user will ask you to solve a task. You should first draft your thinking process (inner monologue) until you have derived the final answer. Afterwards, write a self-contained summary of your thoughts (i.e. your summary should be succinct but contain all the critical steps you needed to reach the conclusion). You should use Markdown and Latex to format your response. Write both your thoughts and summary in the same language as the task posed by the user.\n\nYour thinking process must follow the template below:\n<think>\nYour thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate a correct answer.\n</think>\n\nHere, provide a concise summary that reflects your reasoning and presents a clear final answer to the user.\n\nProblem:\"\n```\n"
      },
      {
        "user": "rick-github",
        "body": "It's the same issue.  Madrigal is not a hybrid model in the same way that qwen3 is - there's no switch to enable or disable thinking.  deepseek is the same, but in that template an empty think block is added to the prompt to fool deepseek into thinking it's already done it's thinking.  It currently doesn't work very well (#11010).\n\nThe madrigal template doesn't even attempt the fake think block.  To disable thinking, the system message has to be modified to remove the explicit instructions to do thinking.\n\nThe reason for the different fencing is that when `think` is true, the client knows it needs to capture the thinking and render it differently.  When `think` is false the client doesn't do this, and so the tokens, including the thinking, are just sent to the output."
      },
      {
        "user": "mmorys",
        "body": "That makes sense. I asked Gemini 2.5 Pro to try to understand the issue, and got a solution along the same lines.\n\n---\n\nOf course. Here is a corrected and robust Ollama `Modelfile` that properly implements the `think` and `nothink` functionality.\n\nThe fundamental issue in your examples was a conflict between the **System Prompt** and the **Template**. Your prompt was instructing the model to *manually* write `<think>` tags as part of its main content, while the template was simultaneously (and incorrectly) trying to use Ollama's built-in, but separate, thinking feature.\n\nThis revised `Modelfile` harmonizes the template, system prompt, and parameters to use Ollama's native thinking functionality, which is controllable by the `/set think` and `/set nothink` commands.\n\n```\n# Description: A model that supports controlled thinking output.\n# To use: ollama create my-thinking-model -f ./Modelfile\n\n# Set the base model\nFROM magistral:latest\n\n# Define the template\nTEMPLATE \"\"\"{{- range $i, $_ := .Messages }}\n{{- $last := eq (len (slice $.Messages $i)) 1 }}\n{{- if eq .Role \"system\" -}}\n[SYSTEM_PROMPT]{{ .Content }}[/SYSTEM_PROMPT]\n{{- else if eq .Role \"user\" -}}\n[INST]{{ .Content }}[/INST]\n{{- else if eq .Role \"assistant\" -}}\n{{- if and $.IsThinkSet (and $last .Thinking) -}}\n<think>\n{{ .Thinking }}\n</think>\n{{- end -}}\n{{ .Content -}}\n</s>\n{{- end -}}\n{{- end }}\"\"\"\n\n# Define the system prompt\nSYSTEM \"\"\"You are a helpful AI assistant.\nFirst, think step-by-step about the user's request to build a clear plan and generate a correct answer. Your thinking process will not be shown to the user unless requested.\nAfter you have done your thinking, provide a concise, self-contained summary of your reasoning and the final answer. Use Markdown and LaTeX for formatting when appropriate.\n\"\"\"\n\n# Set model parameters\nPARAMETER stop </s>\nPARAMETER stop <think>\nPARAMETER stop </think>\n# This parameter is crucial for enabling the .Thinking variable in the template\nPARAMETER think true\n```\n\n"
      }
    ]
  },
  {
    "issue_number": 11015,
    "title": "detected OS VRAM overhead",
    "author": "pihapi",
    "state": "open",
    "created_at": "2025-06-08T07:47:06Z",
    "updated_at": "2025-06-16T23:28:47Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nTwo 3060 cards and 2 GB each out of 12 GB on each is not taken into account. The cards are not used anywhere else. What is this \"detected OS VRAM overhead\"? In general, 3 cards and one under the monitor. And then not a single card is fully loaded by 10 GB. In my opinion, earlier in old versions, the cards were fully loaded, I can't say for sure, I didn't pay much attention.\n\n### Relevant log output\n\n```shell\ntime=2025-06-08T12:31:46.461+05:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:24h0m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY:cuda_v12 OLLAMA_LOAD_TIMEOUT:1h0m0s OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\u\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:true OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-06-08T12:31:46.606+05:00 level=INFO source=images.go:479 msg=\"total blobs: 149\"\ntime=2025-06-08T12:31:46.641+05:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-08T12:31:46.671+05:00 level=INFO source=routes.go:1287 msg=\"Listening on 127.0.0.1:11434 (version 0.9.0)\"\ntime=2025-06-08T12:31:46.671+05:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-08T12:31:46.671+05:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-06-08T12:31:46.671+05:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=6 efficiency=0 threads=12\ntime=2025-06-08T12:31:46.963+05:00 level=INFO source=gpu.go:319 msg=\"detected OS VRAM overhead\" id=GPU-f09d2af5-2fd7-5b36-5a41-5d42518d1539 library=cuda compute=8.6 driver=12.7 name=\"NVIDIA GeForce RTX 3060\" overhead=\"1.9 GiB\"\ntime=2025-06-08T12:31:47.121+05:00 level=INFO source=gpu.go:319 msg=\"detected OS VRAM overhead\" id=GPU-ae75581b-1c00-dd57-68e3-c5651d19235e library=cuda compute=8.6 driver=12.7 name=\"NVIDIA GeForce RTX 3060\" overhead=\"1.9 GiB\"\ntime=2025-06-08T12:31:47.124+05:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-f09d2af5-2fd7-5b36-5a41-5d42518d1539 library=cuda variant=v12 compute=8.6 driver=12.7 name=\"NVIDIA GeForce RTX 3060\" total=\"12.0 GiB\" available=\"10.0 GiB\"\ntime=2025-06-08T12:31:47.124+05:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-ae75581b-1c00-dd57-68e3-c5651d19235e library=cuda variant=v12 compute=8.6 driver=12.7 name=\"NVIDIA GeForce RTX 3060\" total=\"12.0 GiB\" available=\"10.0 GiB\"\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "pihapi",
        "body": "As a result, RAM is used beyond the norm, when it is quite possible to use the cards to the maximum."
      },
      {
        "user": "rick-github",
        "body": "OS VRAM overhead is the difference between free VRAM [reported](https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_1g387248458ef4dfe9afe425280f420f41) by the GPU management library and free VRAM [reported](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g376b97f5ab20321ca46f7cfa9511b978) by the card.  It's the overhead added by the management layer."
      }
    ]
  },
  {
    "issue_number": 11044,
    "title": "The Qwen3:8b model exported by llama-factory after training has an error in deploying ollama.",
    "author": "Kevin-v92",
    "state": "closed",
    "created_at": "2025-06-11T02:44:57Z",
    "updated_at": "2025-06-16T23:27:28Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nThe Qwen3:8b model exported by llama-factory after training has an error in deploying ollama..\n\n<img width=\"574\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6e7e494f-1cfd-452e-88c3-e2a3d3c57359\" />\n\n### Relevant log output\n\n```shell\nunsupported architecture \"Qwen3ForCausa1LM\"\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "The ollama import function only supports a subset of architectures.  For un-supported models, you can use [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf_update.py) to convert to GGUF and then import that."
      },
      {
        "user": "Kevin-v92",
        "body": "> ollama 导入功能仅支持部分架构。对于不支持的模型，您可以使用[llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf_update.py)将其转换为 GGUF 格式，然后导入。\n\nwhen can support it? thanks!"
      }
    ]
  },
  {
    "issue_number": 11033,
    "title": "Qwen2.5vl output garbled code issue",
    "author": "smileyboy2019",
    "state": "open",
    "created_at": "2025-06-10T06:02:28Z",
    "updated_at": "2025-06-16T23:26:42Z",
    "labels": [
      "bug",
      "needs more info"
    ],
    "body": "### What is the issue?\n\nQwen2.5vl output garbled code issue\nI don't know why, but through content output, it's all garbled\n![Image](https://github.com/user-attachments/assets/aedf18ae-6c4b-448a-a62b-2800ce29e1be)\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "If you add some details it will be easier to debug the problem."
      },
      {
        "user": "duck-5",
        "body": "Where did you get the model from?\nTry using the `ollama run` command to check if the problem is with the model or with the integration."
      }
    ]
  },
  {
    "issue_number": 11057,
    "title": "Command-r problem with tool use",
    "author": "ArnaudPannatier",
    "state": "open",
    "created_at": "2025-06-12T12:25:22Z",
    "updated_at": "2025-06-16T23:26:14Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nThere seem to be a problem with the command-r model. [It is mentioned](https://ollama.com/library/command-r:latest/blobs/922095537bc1) that it supports tools and the prompt seem to be coherent with that.\n\nHow ever in practice it is not able to call them coherently:\n\n```\ncurl http://localhost:11434/api/chat -d '{\n    \"model\": \"command-r\",\n    \"stream\" : false,\n    \"options\": {\n        \"temperature\": 0.0\n    },\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What's the weather in nyc?\"\n        }\n    ],\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Use this to get weather information.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"required\": [\n                        \"city\"\n                    ],\n                    \"properties\": {\n                        \"city\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"nyc\",\n                                \"sf\"\n                            ]\n                        }\n                    }\n                }\n            }\n        }\n    ]\n}\n``` \n\n``` \n{\"model\":\"command-r\",\"created_at\":\"2025-06-12T12:20:33.618453025Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Action: ```json\\n[\\n    {\\n        \\\"tool_name\\\": \\\"get_weather\\\",\\n        \\\"parameters\\\": {\\n            \\\"city\\\": \\\"nyc\\\"\\n        }    }\\n]\\n```\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":5716502729,\"load_duration\":4448457708,\"prompt_eval_count\":412,\"prompt_eval_duration\":307338700,\"eval_count\":41,\"eval_duration\":959162054}% \n```\n \n```\ncurl http://localhost:11434/api/chat -d '{\n    \"model\": \"llama3.3:70b\",\n    \"stream\" : false,\n    \"options\": {\n        \"temperature\": 0.0\n    },\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Whats the weather in nyc? lksdjhfalsduhfaoiuegfqoiew\"\n        }\n    ],\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Use this to get weather information.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"required\": [\n                        \"city\"\n                    ],\n                    \"properties\": {\n                        \"city\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"nyc\",\n                                \"sf\"\n                            ]\n                        }\n                    }\n                }\n            }\n        }\n    ]\n}'\n``` \ngives:\n```\n{\"model\":\"llama3.3:70b\",\"created_at\":\"2025-06-12T12:20:08.389820201Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\",\"tool_calls\":[{\"function\":{\"name\":\"get_weather\",\"arguments\":{\"city\":\"nyc\"}}}]},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":1112002299,\"load_duration\":11193408,\"prompt_eval_count\":182,\"prompt_eval_duration\":146428587,\"eval_count\":18,\"eval_duration\":953970525}%  \n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 11070,
    "title": "Ollama 0.9.0: ggml.go fails to check return status of C.ggml_backend_sched_graph_compute_async(c.b.sched, c.graph)",
    "author": "stannenb",
    "state": "open",
    "created_at": "2025-06-14T00:26:40Z",
    "updated_at": "2025-06-16T23:24:42Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nAt line 605 of [ollama](https://github.com/ollama/ollama/tree/main)/[ml](https://github.com/ollama/ollama/tree/main/ml)/[backend](https://github.com/ollama/ollama/tree/main/ml/backend)/[ggml](https://github.com/ollama/ollama/tree/main/ml/backend/ggml)/ggml.go\n(https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml.go#L605)\n\nthis code:\n\n```\nfunc (c *Context) Compute(tensors ...ml.Tensor) {\n\tC.ggml_backend_sched_graph_compute_async(c.b.sched, c.graph)\n\tC.ggml_backend_sched_reset(c.b.sched)\n\n\tneedSync := true\n\tsync := func() {\n\t\tif needSync {\n\t\t\tC.ggml_backend_sched_synchronize(c.b.sched)\n\t\t\tneedSync = false\n\t\t}\n\t}\n\n\tfor _, t := range tensors {\n\t\tif C.ggml_nbytes(t.(*Tensor).t) > 0 {\n\t\t\tt.(*Tensor).sync = sync\n\t\t}\n\t}\n}\n```\n\nthis code seemingly fails to check the return status of C.ggml_backend_sched_graph_compute_async. Thus, if inference calculations fail, ollama keeps going and provides nonsensical answers. \n\nIn the case I've discovered, the calculations fail as follows:\n```\nggml_metal_graph_compute: command buffer 1 failed with status 5\nerror: Internal Error (0000000e:Internal Error)\npanic: GGML status: error (operation failed)\n```\n\nWhile that error is logged, ollama continues to process the request rather than note some kind of model failure. \n\nSee https://github.com/ollama/ollama/issues/10986 for full background but the tldr is: Ollama seems to fail on (some) vision models (gemma3) when running on (at least) a Apple M2 Max cpu with  Metal acceleration on. Turning off metal acceleration with /set parameter num_gpu 0 allows the computation to complete. \n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "MrSimonC",
        "body": "I also have this issue with my Mac Mini M2 Pro (but not my Mac Mini M4). I can't directly solve it, but have (with AI's help) illuminated what's going on:\n\n# Ollama Metal Backend Failure on Apple M2 Pro with Gemma3\n\n## Verified Hardware & Metal Family Support\n\nI have tested the following configurations:\n\n- **Mac M2 Pro**:\n  - `MTLGPUFamilyApple8`: ✅ Supported\n  - `MTLGPUFamilyApple9`: ❌ Not Supported\n\n- **Mac M4**:\n  - `MTLGPUFamilyApple9`: ✅ Supported\n\nThis confirms that the Apple M2 Pro lacks support for `MTLGPUFamilyApple9`, which may be required by certain Metal kernels used in Ollama's Metal backend.\n\n## Issue Summary\n\nWhen running vision models like `gemma3` on a Mac M2 Pro with Metal acceleration enabled, Ollama fails silently. The logs indicate:\n\nggml_metal_graph_compute: command buffer 1 failed with status 5\nerror: Internal Error (0000000e:Internal Error)\n\nDespite this error, Ollama continues processing and returns nonsensical outputs instead of halting execution. This behavior is due to the `Compute` function in `ggml.go` not checking the return status of `C.ggml_backend_sched_graph_compute_async`:\n\n```go\nfunc (c *Context) Compute(tensors ...ml.Tensor) {\n    C.ggml_backend_sched_graph_compute_async(c.b.sched, c.graph)\n    C.ggml_backend_sched_reset(c.b.sched)\n\n    needSync := true\n    sync := func() {\n        if needSync {\n            C.ggml_backend_sched_synchronize(c.b.sched)\n            needSync = false\n        }\n    }\n\n    for _, t := range tensors {\n        if C.ggml_nbytes(t.(*Tensor).t) > 0 {\n            t.(*Tensor).sync = sync\n        }\n    }\n}\n```\n\nThe lack of error handling here allows the program to proceed despite the failure in GPU computation.\n\n## Proposed Fix for Maintainers\n\nTo address this issue, the Compute function should be updated to check the return status of C.ggml_backend_sched_graph_compute_async. If the function returns a non-zero value, indicating an error, the program should handle it appropriately, such as by logging the error and halting execution. Here’s a modified version of the function with error checking:\n\n```go\nfunc (c *Context) Compute(tensors ...ml.Tensor) {\n    if status := C.ggml_backend_sched_graph_compute_async(c.b.sched, c.graph); status != 0 {\n        panic(fmt.Sprintf(\"ggml_backend_sched_graph_compute_async failed with status %d\", status))\n    }\n    C.ggml_backend_sched_reset(c.b.sched)\n\n    needSync := true\n    sync := func() {\n        if needSync {\n            C.ggml_backend_sched_synchronize(c.b.sched)\n            needSync = false\n        }\n    }\n\n    for _, t := range tensors {\n        if C.ggml_nbytes(t.(*Tensor).t) > 0 {\n            t.(*Tensor).sync = sync\n        }\n    }\n}\n```\n\nThis change ensures that any failure in the asynchronous GPU computation is detected and handled, preventing the program from continuing with invalid results.\n\n## Why it blows up on family 8\n1.\tTight thread-group scratch limits. Apple’s docs note family 8 still caps total shared memory per compute kernel at ≤ 64 KiB.  ￼\n2.\tggml_backend_sched_graph_compute_async() submits one monolithic graph whose peak scratch exceeds that limit; Metal aborts the buffer with status .error (value 5).  ￼ ￼\n3.\tFamily 9 GPUs add Dynamic Caching – the driver reallocates scratch on-the-fly, so the same graph fits without modification.  ￼\n\n## Workaround\n\nAs a temporary solution, you can disable Metal acceleration by setting the number of GPU layers to 0:\n\n/set parameter num_gpu 0\n\nThis forces Ollama to use CPU computation, which avoids the Metal backend issue on M2 Pro machines.\n\n## Upcoming macOS Release\n\nIt’s anticipated that future macOS updates may enhance Metal support on Apple Silicon, potentially adding support for newer GPU families like MTLGPUFamilyApple9 on M2-class devices. This could resolve compatibility issues with certain Metal kernels used in Ollama. Users should keep their systems updated to benefit from these improvements.\n\n📚 References\n\t•\tOllama Issue #3698\n"
      }
    ]
  },
  {
    "issue_number": 11073,
    "title": "Out of memory on AMD multi GPU instance despite having sufficient VRAM",
    "author": "digiperfect-tech",
    "state": "open",
    "created_at": "2025-06-14T03:38:32Z",
    "updated_at": "2025-06-16T23:23:47Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n\nOllama throws out of memory error despite there being sufficient VRAM. Model size is approx 160GB so in principle should fit even on a single GPU.\n\n```\namd-smi  monitor\nGPU  POWER   GPU_T   MEM_T   GFX_CLK   GFX%   MEM%   ENC%   DEC%      VRAM_USAGE\n  0  151 W   39 °C   32 °C   170 MHz    0 %    0 %    N/A    0 %    0.3/191.7 GB\n  1  152 W   35 °C   30 °C   199 MHz    0 %    0 %    N/A    0 %    0.3/191.7 GB\n  2  151 W   37 °C   30 °C   161 MHz    0 %    0 %    N/A    0 %    0.3/191.7 GB\n  3  151 W   37 °C   32 °C   175 MHz    0 %    0 %    N/A    0 %    0.3/191.7 GB\n  4  154 W   39 °C   32 °C   163 MHz    0 %    0 %    N/A    0 %    0.3/191.7 GB\n  5  153 W   36 °C   30 °C   171 MHz    0 %    0 %    N/A    0 %    0.3/191.7 GB\n  6  152 W   40 °C   32 °C   188 MHz    0 %    0 %    N/A    0 %    0.3/191.7 GB\n  7  156 W   36 °C   32 °C   191 MHz    0 %    0 %    N/A    0 %    0.3/191.7 GB\n```\n\nEven tried with `OLLAMA_SCHED_SPREAD=1; ollama run ....`\n\n\n### Relevant log output\n\n```shell\nError: llama runner process has terminated: cudaMalloc failed: out of memory\nggml_gallocr_reserve_n: failed to allocate ROCm0 buffer of size 35041316864\n\n```\n\n### OS\n\nLinux\n\n### GPU\n\nAMD\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will aid in debugging."
      },
      {
        "user": "digiperfect-tech",
        "body": "For simplicitly - I am trying a small model on single GPU instance - ecountering same error.\n\n\n```\nModel size: 161 GB\nModel: hf.co/unsloth/DeepSeek-R1-0528-GGUF:latest\n\n```\n```\nGPU VRAM: 192 GB\nSystem RAM: 235 GB\n```\n\n\nIncidentally - following MOE model works fine, the total model size is slightly smaller than the model that fails. But IMO - above model should work seamlessly given the sizes and extra system RAM available?\n\n\nModel that works:\n`qwen3:235b-a22b      142GB`\n\n\n\nServer logs (the log file was not present inside ~/.ollama....  but journalctl gave the logs).\n\n```\nJun 15 08:09:58 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 systemd[1]: ollama.service: Deactivated successfully.\nJun 15 08:09:58 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 systemd[1]: ollama.service: Consumed 14.296s CPU time.\nJun 15 08:10:02 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 systemd[1]: ollama.service: Scheduled restart job, restart counter is at 2.\nJun 15 08:10:02 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 systemd[1]: Started ollama.service - Ollama Service.\nJun 15 08:10:02 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:02.110Z level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\nJun 15 08:10:02 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:02.110Z level=INFO source=images.go:479 msg=\"total blobs: 21\"\nJun 15 08:10:02 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:02.110Z level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\nJun 15 08:10:02 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:02.111Z level=INFO source=routes.go:1287 msg=\"Listening on 127.0.0.1:11434 (version 0.9.0)\"\nJun 15 08:10:02 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:02.111Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\nJun 15 08:10:02 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:02.117Z level=INFO source=amd_linux.go:386 msg=\"amdgpu is supported\" gpu=GPU-7524e56f4b0facdf gpu_type=gfx942\nJun 15 08:10:02 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:02.119Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-7524e56f4b0facdf library=rocm variant=\"\" compute=gfx942 driver=6.12 name=1002:74b5 total=\"191.7 GiB\" available=\"191.4 GiB\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: [GIN] 2025/06/15 - 08:10:25 | 200 |      36.657µs |       127.0.0.1 | HEAD     \"/\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: [GIN] 2025/06/15 - 08:10:25 | 200 |   19.514005ms |       127.0.0.1 | POST     \"/api/show\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:25.797Z level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/usr/share/ollama/.ollama/models/blobs/sha256-fe40cbf872192b5cbb7af80d9c33bcb7d52d0d224be09e919249678209ec0c44 gpu=GPU-7524e56f4b0facdf parallel=2 available=205523197952 required=\"167.8 GiB\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:25.797Z level=INFO source=server.go:135 msg=\"system memory\" total=\"235.9 GiB\" free=\"230.2 GiB\" free_swap=\"0 B\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:25.798Z level=INFO source=server.go:168 msg=offload library=rocm layers.requested=-1 layers.model=62 layers.offload=62 layers.split=\"\" memory.available=\"[191.4 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"167.8 GiB\" memory.required.partial=\"167.8 GiB\" memory.required.kv=\"16.2 GiB\" memory.required.allocations=\"[167.8 GiB]\" memory.weights.total=\"150.0 GiB\" memory.weights.repeating=\"149.3 GiB\" memory.weights.nonrepeating=\"725.0 MiB\" memory.graph.full=\"556.3 MiB\" memory.graph.partial=\"1019.5 MiB\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: loaded meta data with 60 key-value pairs and 1086 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-fe40cbf872192b5cbb7af80d9c33bcb7d52d0d224be09e919249678209ec0c44 (version GGUF V3 (latest))\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   1:                               general.type str              = model\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   2:                               general.name str              = Deepseek-R1-0528\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   3:                           general.basename str              = Deepseek-R1-0528\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   5:                         general.size_label str              = 256x20B\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   6:                            general.license str              = mit\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   9:                  general.base_model.0.name str              = DeepSeek R1 0528\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  10:               general.base_model.0.version str              = 0528\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  11:          general.base_model.0.organization str              = Deepseek Ai\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  13:                               general.tags arr[str,3]       = [\"deepseek\", \"unsloth\", \"transformers\"]\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  15:                      deepseek2.block_count u32              = 61\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  16:                   deepseek2.context_length u32              = 163840\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  17:                 deepseek2.embedding_length u32              = 7168\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  18:              deepseek2.feed_forward_length u32              = 18432\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  19:             deepseek2.attention.head_count u32              = 128\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  20:          deepseek2.attention.head_count_kv u32              = 1\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  21:                   deepseek2.rope.freq_base f32              = 10000.000000\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  22: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  23:                deepseek2.expert_used_count u32              = 8\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  24:        deepseek2.leading_dense_block_count u32              = 3\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  25:                       deepseek2.vocab_size u32              = 129280\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  26:            deepseek2.attention.q_lora_rank u32              = 1536\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  27:           deepseek2.attention.kv_lora_rank u32              = 512\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  28:             deepseek2.attention.key_length u32              = 576\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  29:           deepseek2.attention.value_length u32              = 512\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  30:         deepseek2.attention.key_length_mla u32              = 192\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  31:       deepseek2.attention.value_length_mla u32              = 128\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  32:       deepseek2.expert_feed_forward_length u32              = 2048\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  33:                     deepseek2.expert_count u32              = 256\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  34:              deepseek2.expert_shared_count u32              = 1\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  35:             deepseek2.expert_weights_scale f32              = 2.500000\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  36:              deepseek2.expert_weights_norm bool             = true\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  37:               deepseek2.expert_gating_func u32              = 2\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  38:             deepseek2.rope.dimension_count u32              = 64\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  39:                deepseek2.rope.scaling.type str              = yarn\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  40:              deepseek2.rope.scaling.factor f32              = 40.000000\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  41: deepseek2.rope.scaling.original_context_length u32              = 4096\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  42: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.100000\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  43:                       tokenizer.ggml.model str              = gpt2\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  44:                         tokenizer.ggml.pre str              = deepseek-v3\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: [132B blob data]\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  46:                  tokenizer.ggml.token_type arr[i32,129280]  = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  47:                      tokenizer.ggml.merges arr[str,127741]  = [\"Ġ t\", \"Ġ a\", \"i n\", \"Ġ Ġ\", \"h e...\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  48:                tokenizer.ggml.bos_token_id u32              = 0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  49:                tokenizer.ggml.eos_token_id u32              = 1\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  50:            tokenizer.ggml.padding_token_id u32              = 2\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  51:               tokenizer.ggml.add_bos_token bool             = true\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  52:               tokenizer.ggml.add_eos_token bool             = false\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  53:                    tokenizer.chat_template str              = {%- if not add_generation_prompt is d...\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  54:               general.quantization_version u32              = 2\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  55:                          general.file_type u32              = 24\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  56:                      quantize.imatrix.file str              = DeepSeek-R1-0528-GGUF/imatrix_unsloth...\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  57:                   quantize.imatrix.dataset str              = unsloth_calibration_DeepSeek-R1-0528-...\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  58:             quantize.imatrix.entries_count i32              = 659\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  59:              quantize.imatrix.chunks_count i32              = 720\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type  f32:  361 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type q8_0:  122 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type q4_K:   56 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type q5_K:   36 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type q6_K:   17 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq2_xxs:   30 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq3_xxs:   54 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq1_s:  126 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq3_s:  148 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq4_xs:  136 tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: file format = GGUF V3 (latest)\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: file type   = IQ1_S - 1.5625 bpw\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: file size   = 150.51 GiB (1.93 BPW)\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load: special tokens cache size = 818\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load: token to piece cache size = 0.8223 MB\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: arch             = deepseek2\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: vocab_only       = 1\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: model type       = ?B\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: model params     = 671.03 B\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: general.name     = Deepseek-R1-0528\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_layer_dense_lead   = 0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_lora_q             = 0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_lora_kv            = 0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_embd_head_k_mla    = 0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_embd_head_v_mla    = 0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_ff_exp             = 0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_expert_shared      = 0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: expert_weights_scale = 0.0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: expert_weights_norm  = 0\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: expert_gating_func   = unknown\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: rope_yarn_log_mul    = 0.0000\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: vocab type       = BPE\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_vocab          = 129280\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_merges         = 127741\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: BOS token        = 0 '<｜begin▁of▁sentence｜>'\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: EOS token        = 1 '<｜end▁of▁sentence｜>'\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: EOT token        = 1 '<｜end▁of▁sentence｜>'\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: PAD token        = 2 '<｜▁pad▁｜>'\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: LF token         = 201 'Ċ'\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: FIM PRE token    = 128801 '<｜fim▁begin｜>'\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: FIM SUF token    = 128800 '<｜fim▁hole｜>'\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: FIM MID token    = 128802 '<｜fim▁end｜>'\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: EOG token        = 1 '<｜end▁of▁sentence｜>'\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: max token length = 256\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_load: vocab only - skipping tensors\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:25.967Z level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-fe40cbf872192b5cbb7af80d9c33bcb7d52d0d224be09e919249678209ec0c44 --ctx-size 131072 --batch-size 512 --n-gpu-layers 62 --threads 20 --parallel 2 --port 40669\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:25.968Z level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:25.968Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:25.968Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:25.976Z level=INFO source=runner.go:815 msg=\"starting go runner\"\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: ggml_cuda_init: found 1 ROCm devices:\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]:   Device 0: AMD Instinct MI300X VF, gfx942:sramecc+:xnack- (0x942), VMM: no, Wave Size: 64\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load_backend: loaded ROCm backend from /usr/local/lib/ollama/rocm/libggml-hip.so\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:27.049Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 ROCm.0.NO_VMM=1 ROCm.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_load_from_file_impl: using device ROCm0 (AMD Instinct MI300X VF) - 195958 MiB free\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:27.049Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:40669\"\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: loaded meta data with 60 key-value pairs and 1086 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-fe40cbf872192b5cbb7af80d9c33bcb7d52d0d224be09e919249678209ec0c44 (version GGUF V3 (latest))\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   1:                               general.type str              = model\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   2:                               general.name str              = Deepseek-R1-0528\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   3:                           general.basename str              = Deepseek-R1-0528\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   5:                         general.size_label str              = 256x20B\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   6:                            general.license str              = mit\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv   9:                  general.base_model.0.name str              = DeepSeek R1 0528\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  10:               general.base_model.0.version str              = 0528\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  11:          general.base_model.0.organization str              = Deepseek Ai\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  13:                               general.tags arr[str,3]       = [\"deepseek\", \"unsloth\", \"transformers\"]\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  15:                      deepseek2.block_count u32              = 61\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  16:                   deepseek2.context_length u32              = 163840\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  17:                 deepseek2.embedding_length u32              = 7168\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  18:              deepseek2.feed_forward_length u32              = 18432\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  19:             deepseek2.attention.head_count u32              = 128\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  20:          deepseek2.attention.head_count_kv u32              = 1\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  21:                   deepseek2.rope.freq_base f32              = 10000.000000\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  22: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  23:                deepseek2.expert_used_count u32              = 8\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  24:        deepseek2.leading_dense_block_count u32              = 3\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  25:                       deepseek2.vocab_size u32              = 129280\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  26:            deepseek2.attention.q_lora_rank u32              = 1536\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  27:           deepseek2.attention.kv_lora_rank u32              = 512\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  28:             deepseek2.attention.key_length u32              = 576\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  29:           deepseek2.attention.value_length u32              = 512\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  30:         deepseek2.attention.key_length_mla u32              = 192\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  31:       deepseek2.attention.value_length_mla u32              = 128\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  32:       deepseek2.expert_feed_forward_length u32              = 2048\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  33:                     deepseek2.expert_count u32              = 256\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  34:              deepseek2.expert_shared_count u32              = 1\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  35:             deepseek2.expert_weights_scale f32              = 2.500000\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  36:              deepseek2.expert_weights_norm bool             = true\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  37:               deepseek2.expert_gating_func u32              = 2\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  38:             deepseek2.rope.dimension_count u32              = 64\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  39:                deepseek2.rope.scaling.type str              = yarn\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  40:              deepseek2.rope.scaling.factor f32              = 40.000000\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  41: deepseek2.rope.scaling.original_context_length u32              = 4096\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  42: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.100000\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  43:                       tokenizer.ggml.model str              = gpt2\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  44:                         tokenizer.ggml.pre str              = deepseek-v3\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: [132B blob data]\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  46:                  tokenizer.ggml.token_type arr[i32,129280]  = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  47:                      tokenizer.ggml.merges arr[str,127741]  = [\"Ġ t\", \"Ġ a\", \"i n\", \"Ġ Ġ\", \"h e...\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  48:                tokenizer.ggml.bos_token_id u32              = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  49:                tokenizer.ggml.eos_token_id u32              = 1\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  50:            tokenizer.ggml.padding_token_id u32              = 2\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  51:               tokenizer.ggml.add_bos_token bool             = true\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  52:               tokenizer.ggml.add_eos_token bool             = false\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  53:                    tokenizer.chat_template str              = {%- if not add_generation_prompt is d...\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  54:               general.quantization_version u32              = 2\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  55:                          general.file_type u32              = 24\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  56:                      quantize.imatrix.file str              = DeepSeek-R1-0528-GGUF/imatrix_unsloth...\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  57:                   quantize.imatrix.dataset str              = unsloth_calibration_DeepSeek-R1-0528-...\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  58:             quantize.imatrix.entries_count i32              = 659\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - kv  59:              quantize.imatrix.chunks_count i32              = 720\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type  f32:  361 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type q8_0:  122 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type q4_K:   56 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type q5_K:   36 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type q6_K:   17 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq2_xxs:   30 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq3_xxs:   54 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq1_s:  126 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq3_s:  148 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_model_loader: - type iq4_xs:  136 tensors\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: file format = GGUF V3 (latest)\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: file type   = IQ1_S - 1.5625 bpw\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: file size   = 150.51 GiB (1.93 BPW)\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load: special tokens cache size = 818\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load: token to piece cache size = 0.8223 MB\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: arch             = deepseek2\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: vocab_only       = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_ctx_train      = 163840\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_embd           = 7168\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_layer          = 61\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_head           = 128\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_head_kv        = 1\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_rot            = 64\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_swa            = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_swa_pattern    = 1\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_embd_head_k    = 576\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_embd_head_v    = 512\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_gqa            = 128\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_embd_k_gqa     = 576\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_embd_v_gqa     = 512\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: f_norm_eps       = 0.0e+00\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: f_norm_rms_eps   = 1.0e-06\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: f_clamp_kqv      = 0.0e+00\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: f_max_alibi_bias = 0.0e+00\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: f_logit_scale    = 0.0e+00\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: f_attn_scale     = 0.0e+00\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_ff             = 18432\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_expert         = 256\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_expert_used    = 8\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: causal attn      = 1\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: pooling type     = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: rope type        = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: rope scaling     = yarn\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: freq_base_train  = 10000.0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: freq_scale_train = 0.025\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_ctx_orig_yarn  = 4096\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: rope_finetuned   = unknown\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: ssm_d_conv       = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: ssm_d_inner      = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: ssm_d_state      = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: ssm_dt_rank      = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: ssm_dt_b_c_rms   = 0\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: model type       = 671B\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: model params     = 671.03 B\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: general.name     = Deepseek-R1-0528\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_layer_dense_lead   = 3\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_lora_q             = 1536\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_lora_kv            = 512\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_embd_head_k_mla    = 192\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_embd_head_v_mla    = 128\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_ff_exp             = 2048\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_expert_shared      = 1\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: expert_weights_scale = 2.5\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: expert_weights_norm  = 1\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: expert_gating_func   = sigmoid\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: rope_yarn_log_mul    = 0.1000\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: vocab type       = BPE\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_vocab          = 129280\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: n_merges         = 127741\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: BOS token        = 0 '<｜begin▁of▁sentence｜>'\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: EOS token        = 1 '<｜end▁of▁sentence｜>'\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: EOT token        = 1 '<｜end▁of▁sentence｜>'\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: PAD token        = 2 '<｜▁pad▁｜>'\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: LF token         = 201 'Ċ'\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: FIM PRE token    = 128801 '<｜fim▁begin｜>'\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: FIM SUF token    = 128800 '<｜fim▁hole｜>'\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: FIM MID token    = 128802 '<｜fim▁end｜>'\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: EOG token        = 1 '<｜end▁of▁sentence｜>'\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: print_info: max token length = 256\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load_tensors: loading model tensors, this can take a while... (mmap = true)\nJun 15 08:10:27 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:27.222Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nJun 15 08:10:31 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load_tensors: offloading 61 repeating layers to GPU\nJun 15 08:10:31 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load_tensors: offloading output layer to GPU\nJun 15 08:10:31 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load_tensors: offloaded 62/62 layers to GPU\nJun 15 08:10:31 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load_tensors:        ROCm0 model buffer size = 153627.73 MiB\nJun 15 08:10:31 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: load_tensors:   CPU_Mapped model buffer size =   497.11 MiB\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: constructing llama_context\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: n_seq_max     = 2\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: n_ctx         = 131072\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: n_ctx_per_seq = 65536\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: n_batch       = 1024\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: n_ubatch      = 512\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: causal_attn   = 1\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: flash_attn    = 0\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: freq_base     = 10000.0\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: freq_scale    = 0.025\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context: n_ctx_per_seq (65536) < n_ctx_train (163840) -- the full capacity of the model will not be utilized\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_context:  ROCm_Host  output buffer size =     1.04 MiB\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_kv_cache_unified: kv_size = 131072, type_k = 'f16', type_v = 'f16', n_layer = 61, can_shift = 1, padding = 32\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_kv_cache_unified:      ROCm0 KV buffer size = 16592.00 MiB\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_kv_cache_unified: KV self size  = 16592.00 MiB, K (f16): 8784.00 MiB, V (f16): 7808.00 MiB\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: ggml_backend_cuda_buffer_type_alloc_buffer: allocating 33418.00 MiB on device 0: cudaMalloc failed: out of memory\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: ggml_gallocr_reserve_n: failed to allocate ROCm0 buffer of size 35041316864\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: llama_init_from_model: failed to initialize the context: failed to allocate compute pp buffers\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: panic: unable to create llama context\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: goroutine 50 [running]:\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0xc0004e2360, {0x3e, 0x0, 0x1, {0x0, 0x0, 0x0}, 0xc0005036d0, 0x0}, {0x7ffcddb8bc94, ...}, ...)\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]:         github.com/ollama/ollama/runner/llamarunner/runner.go:757 +0x389\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: created by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]:         github.com/ollama/ollama/runner/llamarunner/runner.go:848 +0xb57\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:39.374Z level=ERROR source=server.go:457 msg=\"llama runner terminated\" error=\"exit status 2\"\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:39.510Z level=ERROR source=sched.go:489 msg=\"error loading llama server\" error=\"llama runner process has terminated: cudaMalloc failed: out of memory\\nggml_gallocr_reserve_n: failed to allocate ROCm0 buffer of size 35041316864\"\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: [GIN] 2025/06/15 - 08:10:39 | 500 | 13.740989178s |       127.0.0.1 | POST     \"/api/generate\"\nJun 15 08:10:44 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:44.511Z level=WARN source=sched.go:687 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.001273245 runner.size=\"167.8 GiB\" runner.vram=\"167.8 GiB\" runner.parallel=2 runner.pid=5741 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-fe40cbf872192b5cbb7af80d9c33bcb7d52d0d224be09e919249678209ec0c44\nJun 15 08:10:44 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:44.760Z level=WARN source=sched.go:687 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.250772528 runner.size=\"167.8 GiB\" runner.vram=\"167.8 GiB\" runner.parallel=2 runner.pid=5741 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-fe40cbf872192b5cbb7af80d9c33bcb7d52d0d224be09e919249678209ec0c44\n```"
      },
      {
        "user": "rick-github",
        "body": "```\nJun 15 08:10:25 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: time=2025-06-15T08:10:25.798Z level=INFO\n source=server.go:168 msg=offload library=rocm layers.requested=-1 layers.model=62 layers.offload=62 layers.split=\"\"\n memory.available=\"[191.4 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"167.8 GiB\"\n memory.required.partial=\"167.8 GiB\" memory.required.kv=\"16.2 GiB\" memory.required.allocations=\"[167.8 GiB]\"\n memory.weights.total=\"150.0 GiB\" memory.weights.repeating=\"149.3 GiB\" memory.weights.nonrepeating=\"725.0 MiB\"\n memory.graph.full=\"556.3 MiB\" memory.graph.partial=\"1019.5 MiB\"\n```\nollama server estimates need 167.8G of 191.4G to offload all 62 layers.\n```\nJun 15 08:10:39 ml-ai-ubuntu-gpu-mi300x1-192gb-atl1 ollama[5704]: ggml_backend_cuda_buffer_type_alloc_buffer: allocating 33418.00 MiB on device 0: cudaMalloc failed: out of memory\n```\nollama runner dies with OOM allocating 33G during the model load.\n\nThe estimation logic is sometimes a little inaccurate, but there should be 23G of free VRAM so even if the estimation is a little off, there should be free VRAM to absorb the error.\n\nCan you show the logs of the failure when `OLLAMA_SCHED_SPREAD=1`? "
      }
    ]
  },
  {
    "issue_number": 10740,
    "title": "0.6.8 already have weird memory usage (also 0.7.0)",
    "author": "Fade78",
    "state": "open",
    "created_at": "2025-05-16T15:57:03Z",
    "updated_at": "2025-06-16T22:02:19Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nHere is loaded regular (4QM) Qwen3:32b with 19000 context length. I crafted this context length so it goes right into my three dedicated 4060TIs (16GB). So I know that if I increase a little the context it will go to 48GB then be split on GPU/CPU.\n\nThere is only one problem. The VRAM used is not actually 48GB. It's 70% of 48GB on 0.6.8 and I had another case with 50% on 0.7.0. It means that ollama will split on the CPU even if there is VRAM available, dramatically slowing down the inference speed.\n\n![Image](https://github.com/user-attachments/assets/100d4e74-cc73-4db0-a988-bc7751593420)\n\nSo here you can see: ollama reports 47GB usage and, a long time ago (previous versions), it would have mean almost 100% VRAM usage on my 3x16GB rig.\n\nAgain, if I increase the context, it will split, that's how I fine tune the context length.\n\nHere are some logs:\n```\n[GIN] 2025/05/16 - 15:44:06 | 500 | 49.226007283s |   192.168.10.11 | POST     \"/api/chat\"\n2025/05/16 15:44:07 routes.go:1233: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:36h0m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:2 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-05-16T15:44:07.744Z level=INFO source=images.go:463 msg=\"total blobs: 335\"\ntime=2025-05-16T15:44:07.746Z level=INFO source=images.go:470 msg=\"total unused blobs removed: 0\"\ntime=2025-05-16T15:44:07.747Z level=INFO source=routes.go:1300 msg=\"Listening on [::]:11434 (version 0.6.8)\"\ntime=2025-05-16T15:44:07.747Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-05-16T15:44:08.171Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-abd5d393-e831-c64e-d4eb-1c8797fc1fbf library=cuda variant=v12 compute=8.9 driver=12.8 name=\"NVIDIA GeForce RTX 4060 Ti\" total=\"15.6 GiB\" available=\"15.5 GiB\"\ntime=2025-05-16T15:44:08.172Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-9fea6450-0337-d707-5f2f-003347856eed library=cuda variant=v12 compute=8.9 driver=12.8 name=\"NVIDIA GeForce RTX 4060 Ti\" total=\"15.6 GiB\" available=\"15.5 GiB\"\ntime=2025-05-16T15:44:08.172Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-a5324074-2159-7d4d-b32e-31afd9176c45 library=cuda variant=v12 compute=8.9 driver=12.8 name=\"NVIDIA GeForce RTX 4060 Ti\" total=\"15.6 GiB\" available=\"15.5 GiB\"\ntime=2025-05-16T15:44:08.432Z level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-16T15:44:08.738Z level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-16T15:44:08.744Z level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-16T15:44:08.744Z level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen3.vision.block_count default=0\ntime=2025-05-16T15:44:08.745Z level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen3.vision.block_count default=0\ntime=2025-05-16T15:44:08.745Z level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen3.vision.block_count default=0\ntime=2025-05-16T15:44:08.746Z level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen3.vision.block_count default=0\ntime=2025-05-16T15:44:08.746Z level=INFO source=sched.go:770 msg=\"new model will fit in available VRAM, loading\" model=/root/.ollama/models/blobs/sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312 library=cuda parallel=1 required=\"44.0 GiB\"\ntime=2025-05-16T15:44:09.043Z level=INFO source=server.go:106 msg=\"system memory\" total=\"62.8 GiB\" free=\"60.6 GiB\" free_swap=\"0 B\"\ntime=2025-05-16T15:44:09.043Z level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen3.vision.block_count default=0\ntime=2025-05-16T15:44:09.043Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split=22,22,21 memory.available=\"[15.5 GiB 15.5 GiB 15.5 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"44.0 GiB\" memory.required.partial=\"44.0 GiB\" memory.required.kv=\"4.6 GiB\" memory.required.allocations=\"[14.6 GiB 15.3 GiB 14.2 GiB]\" memory.weights.total=\"18.4 GiB\" memory.weights.repeating=\"17.8 GiB\" memory.weights.nonrepeating=\"608.6 MiB\" memory.graph.full=\"6.2 GiB\" memory.graph.partial=\"6.2 GiB\"\nllama_model_loader: loaded meta data with 27 key-value pairs and 707 tensors from /root/.ollama/models/blobs/sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 32B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 32B\nllama_model_loader: - kv   5:                          qwen3.block_count u32              = 64\nllama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120\nllama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 25600\nllama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 64\nllama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  257 tensors\nllama_model_loader: - type  f16:   64 tensors\nllama_model_loader: - type q4_K:  353 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 18.81 GiB (4.93 BPW) \nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 32.76 B\nprint_info: general.name     = Qwen3 32B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-05-16T15:44:09.143Z level=INFO source=server.go:410 msg=\"starting llama server\" cmd=\"/usr/bin/ollama runner --model /root/.ollama/models/blobs/sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312 --ctx-size 19000 --batch-size 512 --n-gpu-layers 65 --threads 16 --parallel 1 --tensor-split 22,22,21 --port 36533\"\ntime=2025-05-16T15:44:09.143Z level=INFO source=sched.go:452 msg=\"loaded runners\" count=1\ntime=2025-05-16T15:44:09.143Z level=INFO source=server.go:589 msg=\"waiting for llama runner to start responding\"\ntime=2025-05-16T15:44:09.143Z level=INFO source=server.go:623 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-05-16T15:44:09.150Z level=INFO source=runner.go:853 msg=\"starting go runner\"\nload_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-icelake.so\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 3 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 2: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\nload_backend: loaded CUDA backend from /usr/lib/ollama/cuda_v12/libggml-cuda.so\ntime=2025-05-16T15:44:09.331Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 CUDA.1.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.1.USE_GRAPHS=1 CUDA.1.PEER_MAX_BATCH_SIZE=128 CUDA.2.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.2.USE_GRAPHS=1 CUDA.2.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\ntime=2025-05-16T15:44:09.332Z level=INFO source=runner.go:913 msg=\"Server listening on 127.0.0.1:36533\"\ntime=2025-05-16T15:44:09.395Z level=INFO source=server.go:623 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Ti) - 15831 MiB free\nllama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 4060 Ti) - 15831 MiB free\nllama_model_load_from_file_impl: using device CUDA2 (NVIDIA GeForce RTX 4060 Ti) - 15831 MiB free\nllama_model_loader: loaded meta data with 27 key-value pairs and 707 tensors from /root/.ollama/models/blobs/sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 32B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 32B\nllama_model_loader: - kv   5:                          qwen3.block_count u32              = 64\nllama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120\nllama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 25600\nllama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 64\nllama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  257 tensors\nllama_model_loader: - type  f16:   64 tensors\nllama_model_loader: - type q4_K:  353 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 18.81 GiB (4.93 BPW) \nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 64\nprint_info: n_head           = 64\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 25600\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 32B\nprint_info: model params     = 32.76 B\nprint_info: general.name     = Qwen3 32B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\n[GIN] 2025/05/16 - 15:44:10 | 200 |       28.53µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/16 - 15:44:10 | 200 |      102.47µs |       127.0.0.1 | GET      \"/api/ps\"\nload_tensors: offloading 64 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 65/65 layers to GPU\nload_tensors:        CUDA0 model buffer size =  6300.10 MiB\nload_tensors:        CUDA1 model buffer size =  6171.19 MiB\nload_tensors:        CUDA2 model buffer size =  6371.11 MiB\nload_tensors:   CPU_Mapped model buffer size =   417.30 MiB\n[GIN] 2025/05/16 - 15:44:12 | 200 |      24.929µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/16 - 15:44:12 | 200 |      34.071µs |       127.0.0.1 | GET      \"/api/ps\"\n[GIN] 2025/05/16 - 15:44:14 | 200 |       25.22µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/16 - 15:44:14 | 200 |       25.99µs |       127.0.0.1 | GET      \"/api/ps\"\n[GIN] 2025/05/16 - 15:44:16 | 200 |      22.209µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/16 - 15:44:16 | 200 |       23.17µs |       127.0.0.1 | GET      \"/api/ps\"\n[GIN] 2025/05/16 - 15:44:18 | 200 |       26.36µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/16 - 15:44:18 | 200 |      27.531µs |       127.0.0.1 | GET      \"/api/ps\"\n[GIN] 2025/05/16 - 15:44:20 | 200 |       26.81µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/16 - 15:44:20 | 200 |       39.83µs |       127.0.0.1 | GET      \"/api/ps\"\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 19000\nllama_context: n_ctx_per_seq = 19000\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (19000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     0.60 MiB\ninit: kv_size = 19008, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1\ninit:      CUDA0 KV buffer size =  1633.50 MiB\ninit:      CUDA1 KV buffer size =  1633.50 MiB\ninit:      CUDA2 KV buffer size =  1485.00 MiB\nllama_context: KV self size  = 4752.00 MiB, K (f16): 2376.00 MiB, V (f16): 2376.00 MiB\nllama_context: pipeline parallelism enabled (n_copies=4)\nllama_context:      CUDA0 compute buffer size =  2616.51 MiB\nllama_context:      CUDA1 compute buffer size =  2616.51 MiB\nllama_context:      CUDA2 compute buffer size =  2616.52 MiB\nllama_context:  CUDA_Host compute buffer size =   158.52 MiB\nllama_context: graph nodes  = 2438\nllama_context: graph splits = 4\ntime=2025-05-16T15:44:21.182Z level=INFO source=server.go:628 msg=\"llama runner started in 12.04 seconds\"\n```\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "johnnysn",
        "body": "It looks like I'm facing the same problem here. But the issue seems to be related to Qwen3 only. My server has 3 RTX 3090's and when I try to deploy Qwen3-32B-Q6_K using a 32k context size, the logs report a total required memory of 67.3 GiB, even though the actual VRAM usage is about 36 GiB. If I increase the context size just a tiny bit, Ollama starts to set some layers to the CPU, which significantly hurts performance.\n\nUPDATE: Just upgraded to version 0.7.0 and the behavior persists\n\nThe logs:\n\n```\ngpu=GPU-87562afb-5b3f-53ed-67d0-0ca22728310e name=\"NVIDIA GeForce RTX 3090\" overhead=\"0 B\" before.total=\"23.6 GiB\" before.free=\"23.1 GiB\" now.total=\"23.6 GiB\" now.free=\"23.1 GiB\" now.used=\"459.5 MiB\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.453-03:00 level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-71bb38d6-a9e1-a8fb-81a1-be53f9c998dd name=\"NVIDIA GeForce RTX 3090\" overhead=\"0 B\" before.total=\"23.6 GiB\" before.free=\"23.3 GiB\" now.total=\"23.6 GiB\" now.free=\"23.3 GiB\" now.used=\"276.9 MiB\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.629-03:00 level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-db8680a0-d1b7-7702-914c-fab5322aaa6a name=\"NVIDIA GeForce RTX 3090\" overhead=\"0 B\" before.total=\"23.6 GiB\" before.free=\"23.3 GiB\" now.total=\"23.6 GiB\" now.free=\"23.3 GiB\" now.used=\"276.9 MiB\"\nmai 17 11:51:48 urano ollama[1900]: releasing cuda driver library\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.629-03:00 level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split=22,22,21 memory.available=\"[23.3 GiB 23.3 GiB 23.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"67.3 GiB\" memory.required.partial=\"67.3 GiB\" memory.required.kv=\"8.0 GiB\" memory.required.allocations=\"[22.6 GiB 22.7 GiB 22.1 GiB]\" memory.weights.total=\"24.4 GiB\" memory.weights.repeating=\"23.8 GiB\" memory.weights.nonrepeating=\"608.6 MiB\" memory.graph.full=\"10.7 GiB\" memory.graph.partial=\"10.7 GiB\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.629-03:00 level=INFO source=server.go:186 msg=\"enabling flash attention\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.629-03:00 level=WARN source=server.go:194 msg=\"kv cache type not supported by model\" type=\"\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.629-03:00 level=DEBUG source=server.go:263 msg=\"compatible gpu libraries\" compatible=\"[cuda_v12 cuda_v11]\"\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: loaded meta data with 28 key-value pairs and 707 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-c4c7c3cb6da260df1fe1d3cfd090a32dc7cc348f1278158be18e301f390d6f6e (version GGUF V3 (latest))\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   1:                               general.type str              = model\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 32B Instruct\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   3:                           general.finetune str              = Instruct\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   4:                           general.basename str              = Qwen3\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   5:                         general.size_label str              = 32B\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 64\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 5120\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 25600\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 64\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - kv  27:                          general.file_type u32              = 18\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - type  f32:  257 tensors\nmai 17 11:51:48 urano ollama[1900]: llama_model_loader: - type q6_K:  450 tensors\nmai 17 11:51:48 urano ollama[1900]: print_info: file format = GGUF V3 (latest)\nmai 17 11:51:48 urano ollama[1900]: print_info: file type   = Q6_K\nmai 17 11:51:48 urano ollama[1900]: print_info: file size   = 25.03 GiB (6.56 BPW)\nmai 17 11:51:48 urano ollama[1900]: init_tokenizer: initializing tokenizer for type 2\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151660 '<|fim_middle|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151653 '<|vision_end|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151648 '<|box_start|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151649 '<|box_end|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151655 '<|image_pad|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151651 '<|quad_end|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151652 '<|vision_start|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151654 '<|vision_pad|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151656 '<|video_pad|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151644 '<|im_start|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: control token: 151650 '<|quad_start|>' is not marked as EOG\nmai 17 11:51:48 urano ollama[1900]: load: special tokens cache size = 26\nmai 17 11:51:48 urano ollama[1900]: load: token to piece cache size = 0.9311 MB\nmai 17 11:51:48 urano ollama[1900]: print_info: arch             = qwen3\nmai 17 11:51:48 urano ollama[1900]: print_info: vocab_only       = 1\nmai 17 11:51:48 urano ollama[1900]: print_info: model type       = ?B\nmai 17 11:51:48 urano ollama[1900]: print_info: model params     = 32.76 B\nmai 17 11:51:48 urano ollama[1900]: print_info: general.name     = Qwen3 32B Instruct\nmai 17 11:51:48 urano ollama[1900]: print_info: vocab type       = BPE\nmai 17 11:51:48 urano ollama[1900]: print_info: n_vocab          = 151936\nmai 17 11:51:48 urano ollama[1900]: print_info: n_merges         = 151387\nmai 17 11:51:48 urano ollama[1900]: print_info: BOS token        = 151643 '<|endoftext|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: EOS token        = 151645 '<|im_end|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: EOT token        = 151645 '<|im_end|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: PAD token        = 151643 '<|endoftext|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: LF token         = 198 'Ċ'\nmai 17 11:51:48 urano ollama[1900]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: FIM MID token    = 151660 '<|fim_middle|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: FIM REP token    = 151663 '<|repo_name|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: FIM SEP token    = 151664 '<|file_sep|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: EOG token        = 151643 '<|endoftext|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: EOG token        = 151645 '<|im_end|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: EOG token        = 151662 '<|fim_pad|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: EOG token        = 151663 '<|repo_name|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: EOG token        = 151664 '<|file_sep|>'\nmai 17 11:51:48 urano ollama[1900]: print_info: max token length = 256\nmai 17 11:51:48 urano ollama[1900]: llama_model_load: vocab only - skipping tensors\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.813-03:00 level=DEBUG source=server.go:339 msg=\"adding gpu library\" path=/usr/local/lib/ollama/cuda_v12\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.813-03:00 level=DEBUG source=server.go:346 msg=\"adding gpu dependency paths\" paths=[/usr/local/lib/ollama/cuda_v12]\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.813-03:00 level=INFO source=server.go:410 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-c4c7c3cb6da260df1fe1d3cfd090a32dc7cc348f1278158be18e301f390d6f6e --ctx-size 32768 --batch-size 512 --n-gpu-layers 65 --verbose --threads 32 --flash-attn --parallel 1 --tensor-split 22,22,21 --port 43719\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.813-03:00 level=DEBUG source=server.go:429 msg=subprocess environment=\"[PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin OLLAMA_HOST=0.0.0.0 OLLAMA_KEEP_ALIVE=10m OLLAMA_DEBUG=1 OLLAMA_FLASH_ATTENTION=true OLLAMA_MAX_LOADED_MODELS=9 OLLAMA_LIBRARY_PATH=/usr/local/lib/ollama:/usr/local/lib/ollama/cuda_v12 LD_LIBRARY_PATH=/usr/local/lib/ollama/cuda_v12:/usr/local/lib/ollama/cuda_v12:/usr/local/lib/ollama:/usr/local/lib/ollama CUDA_VISIBLE_DEVICES=GPU-71bb38d6-a9e1-a8fb-81a1-be53f9c998dd,GPU-db8680a0-d1b7-7702-914c-fab5322aaa6a,GPU-87562afb-5b3f-53ed-67d0-0ca22728310e]\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.814-03:00 level=INFO source=sched.go:452 msg=\"loaded runners\" count=1\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.814-03:00 level=INFO source=server.go:589 msg=\"waiting for llama runner to start responding\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.814-03:00 level=INFO source=server.go:623 msg=\"waiting for server to become available\" status=\"llm server not responding\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.826-03:00 level=INFO source=runner.go:853 msg=\"starting go runner\"\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.826-03:00 level=DEBUG source=ggml.go:93 msg=\"ggml backend load all from path\" path=/usr/local/lib/ollama\nmai 17 11:51:48 urano ollama[1900]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so\nmai 17 11:51:48 urano ollama[1900]: time=2025-05-17T11:51:48.830-03:00 level=DEBUG source=ggml.go:93 msg=\"ggml backend load all from path\" path=/usr/local/lib/ollama/cuda_v12\nmai 17 11:51:48 urano ollama[1900]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nmai 17 11:51:48 urano ollama[1900]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nmai 17 11:51:48 urano ollama[1900]: ggml_cuda_init: found 3 CUDA devices:\nmai 17 11:51:48 urano ollama[1900]:   Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nmai 17 11:51:48 urano ollama[1900]:   Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nmai 17 11:51:48 urano ollama[1900]:   Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nmai 17 11:51:49 urano ollama[1900]: load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\nmai 17 11:51:49 urano ollama[1900]: time=2025-05-17T11:51:49.150-03:00 level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 CUDA.1.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.1.USE_GRAPHS=1 CUDA.1.PEER_MAX_BATCH_SIZE=128 CUDA.2.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.2.USE_GRAPHS=1 CUDA.2.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\nmai 17 11:51:49 urano ollama[1900]: time=2025-05-17T11:51:49.151-03:00 level=INFO source=runner.go:913 msg=\"Server listening on 127.0.0.1:43719\"\nmai 17 11:51:49 urano ollama[1900]: llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23873 MiB free\nmai 17 11:51:49 urano ollama[1900]: llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 3090) - 23873 MiB free\nmai 17 11:51:49 urano ollama[1900]: llama_model_load_from_file_impl: using device CUDA2 (NVIDIA GeForce RTX 3090) - 23657 MiB free\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: loaded meta data with 28 key-value pairs and 707 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-c4c7c3cb6da260df1fe1d3cfd090a32dc7cc348f1278158be18e301f390d6f6e (version GGUF V3 (latest))\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   1:                               general.type str              = model\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 32B Instruct\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   3:                           general.finetune str              = Instruct\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   4:                           general.basename str              = Qwen3\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   5:                         general.size_label str              = 32B\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 64\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 5120\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 25600\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 64\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\nmai 17 11:51:49 urano ollama[1900]: time=2025-05-17T11:51:49.316-03:00 level=INFO source=server.go:623 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - kv  27:                          general.file_type u32              = 18\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - type  f32:  257 tensors\nmai 17 11:51:49 urano ollama[1900]: llama_model_loader: - type q6_K:  450 tensors\nmai 17 11:51:49 urano ollama[1900]: print_info: file format = GGUF V3 (latest)\nmai 17 11:51:49 urano ollama[1900]: print_info: file type   = Q6_K\nmai 17 11:51:49 urano ollama[1900]: print_info: file size   = 25.03 GiB (6.56 BPW)\nmai 17 11:51:49 urano ollama[1900]: init_tokenizer: initializing tokenizer for type 2\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151660 '<|fim_middle|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151653 '<|vision_end|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151648 '<|box_start|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151649 '<|box_end|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151655 '<|image_pad|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151651 '<|quad_end|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151652 '<|vision_start|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151654 '<|vision_pad|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151656 '<|video_pad|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151644 '<|im_start|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: control token: 151650 '<|quad_start|>' is not marked as EOG\nmai 17 11:51:49 urano ollama[1900]: load: special tokens cache size = 26\nmai 17 11:51:49 urano ollama[1900]: load: token to piece cache size = 0.9311 MB\nmai 17 11:51:49 urano ollama[1900]: print_info: arch             = qwen3\nmai 17 11:51:49 urano ollama[1900]: print_info: vocab_only       = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: n_ctx_train      = 40960\nmai 17 11:51:49 urano ollama[1900]: print_info: n_embd           = 5120\nmai 17 11:51:49 urano ollama[1900]: print_info: n_layer          = 64\nmai 17 11:51:49 urano ollama[1900]: print_info: n_head           = 64\nmai 17 11:51:49 urano ollama[1900]: print_info: n_head_kv        = 8\nmai 17 11:51:49 urano ollama[1900]: print_info: n_rot            = 128\nmai 17 11:51:49 urano ollama[1900]: print_info: n_swa            = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: n_swa_pattern    = 1\nmai 17 11:51:49 urano ollama[1900]: print_info: n_embd_head_k    = 128\nmai 17 11:51:49 urano ollama[1900]: print_info: n_embd_head_v    = 128\nmai 17 11:51:49 urano ollama[1900]: print_info: n_gqa            = 8\nmai 17 11:51:49 urano ollama[1900]: print_info: n_embd_k_gqa     = 1024\nmai 17 11:51:49 urano ollama[1900]: print_info: n_embd_v_gqa     = 1024\nmai 17 11:51:49 urano ollama[1900]: print_info: f_norm_eps       = 0.0e+00\nmai 17 11:51:49 urano ollama[1900]: print_info: f_norm_rms_eps   = 1.0e-06\nmai 17 11:51:49 urano ollama[1900]: print_info: f_clamp_kqv      = 0.0e+00\nmai 17 11:51:49 urano ollama[1900]: print_info: f_max_alibi_bias = 0.0e+00\nmai 17 11:51:49 urano ollama[1900]: print_info: f_logit_scale    = 0.0e+00\nmai 17 11:51:49 urano ollama[1900]: print_info: f_attn_scale     = 0.0e+00\nmai 17 11:51:49 urano ollama[1900]: print_info: n_ff             = 25600\nmai 17 11:51:49 urano ollama[1900]: print_info: n_expert         = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: n_expert_used    = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: causal attn      = 1\nmai 17 11:51:49 urano ollama[1900]: print_info: pooling type     = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: rope type        = 2\nmai 17 11:51:49 urano ollama[1900]: print_info: rope scaling     = linear\nmai 17 11:51:49 urano ollama[1900]: print_info: freq_base_train  = 1000000.0\nmai 17 11:51:49 urano ollama[1900]: print_info: freq_scale_train = 1\nmai 17 11:51:49 urano ollama[1900]: print_info: n_ctx_orig_yarn  = 40960\nmai 17 11:51:49 urano ollama[1900]: print_info: rope_finetuned   = unknown\nmai 17 11:51:49 urano ollama[1900]: print_info: ssm_d_conv       = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: ssm_d_inner      = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: ssm_d_state      = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: ssm_dt_rank      = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: ssm_dt_b_c_rms   = 0\nmai 17 11:51:49 urano ollama[1900]: print_info: model type       = 32B\nmai 17 11:51:49 urano ollama[1900]: print_info: model params     = 32.76 B\nmai 17 11:51:49 urano ollama[1900]: print_info: general.name     = Qwen3 32B Instruct\nmai 17 11:51:49 urano ollama[1900]: print_info: vocab type       = BPE\nmai 17 11:51:49 urano ollama[1900]: print_info: n_vocab          = 151936\nmai 17 11:51:49 urano ollama[1900]: print_info: n_merges         = 151387\nmai 17 11:51:49 urano ollama[1900]: print_info: BOS token        = 151643 '<|endoftext|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: EOS token        = 151645 '<|im_end|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: EOT token        = 151645 '<|im_end|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: PAD token        = 151643 '<|endoftext|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: LF token         = 198 'Ċ'\nmai 17 11:51:49 urano ollama[1900]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: FIM MID token    = 151660 '<|fim_middle|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: FIM REP token    = 151663 '<|repo_name|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: FIM SEP token    = 151664 '<|file_sep|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: EOG token        = 151643 '<|endoftext|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: EOG token        = 151645 '<|im_end|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: EOG token        = 151662 '<|fim_pad|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: EOG token        = 151663 '<|repo_name|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: EOG token        = 151664 '<|file_sep|>'\nmai 17 11:51:49 urano ollama[1900]: print_info: max token length = 256\nmai 17 11:51:49 urano ollama[1900]: load_tensors: loading model tensors, this can take a while... (mmap = true)\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  22 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  23 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  24 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  25 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  26 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  27 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  28 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  29 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  30 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  31 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  32 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  33 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  34 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  35 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  36 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  37 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  38 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  39 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  40 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  41 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  42 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  43 assigned to device CUDA1, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  44 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  45 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  46 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  47 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  48 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  49 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  50 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  51 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  52 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  53 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  54 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  55 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  56 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  57 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  58 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  59 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  60 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  61 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  62 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  63 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: layer  64 assigned to device CUDA2, is_swa = 0\nmai 17 11:51:49 urano ollama[1900]: load_tensors: tensor 'token_embd.weight' (q6_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\nmai 17 11:51:51 urano ollama[1900]: load_tensors: offloading 64 repeating layers to GPU\nmai 17 11:51:51 urano ollama[1900]: load_tensors: offloading output layer to GPU\nmai 17 11:51:51 urano ollama[1900]: load_tensors: offloaded 65/65 layers to GPU\nmai 17 11:51:51 urano ollama[1900]: load_tensors:        CUDA0 model buffer size =  8392.68 MiB\nmai 17 11:51:51 urano ollama[1900]: load_tensors:        CUDA1 model buffer size =  8392.68 MiB\nmai 17 11:51:51 urano ollama[1900]: load_tensors:        CUDA2 model buffer size =  8238.30 MiB\nmai 17 11:51:51 urano ollama[1900]: load_tensors:   CPU_Mapped model buffer size =   608.57 MiB\nmai 17 11:51:52 urano ollama[1900]: time=2025-05-17T11:51:52.076-03:00 level=DEBUG source=server.go:634 msg=\"model load progress 0.11\"\nmai 17 11:51:52 urano ollama[1900]: time=2025-05-17T11:51:52.327-03:00 level=DEBUG source=server.go:634 msg=\"model load progress 0.28\"\nmai 17 11:51:52 urano ollama[1900]: time=2025-05-17T11:51:52.578-03:00 level=DEBUG source=server.go:634 msg=\"model load progress 0.45\"\nmai 17 11:51:52 urano ollama[1900]: time=2025-05-17T11:51:52.828-03:00 level=DEBUG source=server.go:634 msg=\"model load progress 0.63\"\nmai 17 11:51:53 urano ollama[1900]: time=2025-05-17T11:51:53.079-03:00 level=DEBUG source=server.go:634 msg=\"model load progress 0.80\"\nmai 17 11:51:53 urano ollama[1900]: time=2025-05-17T11:51:53.330-03:00 level=DEBUG source=server.go:634 msg=\"model load progress 0.97\"\nmai 17 11:51:53 urano ollama[1900]: time=2025-05-17T11:51:53.581-03:00 level=DEBUG source=server.go:634 msg=\"model load progress 0.98\"\nmai 17 11:51:54 urano ollama[1900]: llama_context: constructing llama_context\nmai 17 11:51:54 urano ollama[1900]: llama_context: n_seq_max     = 1\nmai 17 11:51:54 urano ollama[1900]: llama_context: n_ctx         = 32768\nmai 17 11:51:54 urano ollama[1900]: llama_context: n_ctx_per_seq = 32768\nmai 17 11:51:54 urano ollama[1900]: llama_context: n_batch       = 512\nmai 17 11:51:54 urano ollama[1900]: llama_context: n_ubatch      = 512\nmai 17 11:51:54 urano ollama[1900]: llama_context: causal_attn   = 1\nmai 17 11:51:54 urano ollama[1900]: llama_context: flash_attn    = 1\nmai 17 11:51:54 urano ollama[1900]: llama_context: freq_base     = 1000000.0\nmai 17 11:51:54 urano ollama[1900]: llama_context: freq_scale    = 1\nmai 17 11:51:54 urano ollama[1900]: llama_context: n_ctx_per_seq (32768) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nmai 17 11:51:54 urano ollama[1900]: set_abort_callback: call\nmai 17 11:51:54 urano ollama[1900]: llama_context:  CUDA_Host  output buffer size =     0.60 MiB\nmai 17 11:51:54 urano ollama[1900]: llama_context: n_ctx = 32768\nmai 17 11:51:54 urano ollama[1900]: llama_context: n_ctx = 32768 (padded)\nmai 17 11:51:54 urano ollama[1900]: init: kv_size = 32768, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1\nmai 17 11:51:54 urano ollama[1900]: init: layer   0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer   1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer   2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer   3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer   4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer   5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer   6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer   7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer   8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer   9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA0\nmai 17 11:51:54 urano ollama[1900]: init: layer  22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA1\nmai 17 11:51:54 urano ollama[1900]: init: layer  44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  48: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  49: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  50: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  51: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  52: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  53: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  54: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  55: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  56: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  57: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  58: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  59: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  60: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  61: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  62: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init: layer  63: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = CUDA2\nmai 17 11:51:54 urano ollama[1900]: init:      CUDA0 KV buffer size =  2816.00 MiB\nmai 17 11:51:54 urano ollama[1900]: init:      CUDA1 KV buffer size =  2816.00 MiB\nmai 17 11:51:54 urano ollama[1900]: init:      CUDA2 KV buffer size =  2560.00 MiB\nmai 17 11:51:54 urano ollama[1900]: llama_context: KV self size  = 8192.00 MiB, K (f16): 4096.00 MiB, V (f16): 4096.00 MiB\nmai 17 11:51:54 urano ollama[1900]: llama_context: enumerating backends\nmai 17 11:51:54 urano ollama[1900]: llama_context: backend_ptrs.size() = 4\nmai 17 11:51:54 urano ollama[1900]: llama_context: max_nodes = 65536\nmai 17 11:51:54 urano ollama[1900]: llama_context: pipeline parallelism enabled (n_copies=4)\nmai 17 11:51:54 urano ollama[1900]: llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\nmai 17 11:51:54 urano ollama[1900]: llama_context: reserving graph for n_tokens = 512, n_seqs = 1\nmai 17 11:51:54 urano ollama[1900]: time=2025-05-17T11:51:54.333-03:00 level=DEBUG source=server.go:634 msg=\"model load progress 1.00\"\nmai 17 11:51:54 urano ollama[1900]: llama_context: reserving graph for n_tokens = 1, n_seqs = 1\nmai 17 11:51:54 urano ollama[1900]: llama_context: reserving graph for n_tokens = 512, n_seqs = 1\nmai 17 11:51:54 urano ollama[1900]: llama_context:      CUDA0 compute buffer size =   470.01 MiB\nmai 17 11:51:54 urano ollama[1900]: llama_context:      CUDA1 compute buffer size =   304.01 MiB\nmai 17 11:51:54 urano ollama[1900]: llama_context:      CUDA2 compute buffer size =   474.77 MiB\nmai 17 11:51:54 urano ollama[1900]: llama_context:  CUDA_Host compute buffer size =   266.02 MiB\nmai 17 11:51:54 urano ollama[1900]: llama_context: graph nodes  = 2183\nmai 17 11:51:54 urano ollama[1900]: llama_context: graph splits = 4\nmai 17 11:51:54 urano ollama[1900]: time=2025-05-17T11:51:54.584-03:00 level=INFO source=server.go:628 msg=\"llama runner started in 5.77 seconds\"\n```\n\nVRAM usage (nvtop):\n\n![Image](https://github.com/user-attachments/assets/89f3944c-0243-46a6-b3b3-6049613cdfa7)"
      },
      {
        "user": "Desslar",
        "body": "Yes, same issue. 3x 3090 using about 50% of available vram and pushing the rest to system ram.  Not just qwen 3 but also Llama3.3 70B in my limited testing.  I can barely fit 3.3 q4 with 2k context over 72GB VRAM whereas before I could fit 8K context with just 48GB. \n\nI think ollama is not estimating Vram usage correctly"
      },
      {
        "user": "woojh3690",
        "body": "Same issue."
      }
    ]
  },
  {
    "issue_number": 10911,
    "title": "There still is VRAM estimation issue in 0.9.0 version",
    "author": "konn-submarine-bu",
    "state": "open",
    "created_at": "2025-05-30T07:49:57Z",
    "updated_at": "2025-06-16T22:02:13Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n![Image](https://github.com/user-attachments/assets/e6d66f24-5acd-4110-80ee-8f15b3f7e8b3)\n\n### Relevant log output\n\n```shell\ntime=2025-05-30T15:42:11.816+08:00 level=WARN source=sched.go:687 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.0187921 runner.size=\"42.6 GiB\" runner.vram=\"42.6 GiB\" runner.parallel=10 runner.pid=7948 runner.model=C:\\Users\\dru1szh\\.ollama\\models\\blobs\\sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312\ntime=2025-05-30T15:42:11.929+08:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"127.7 GiB\" free=\"77.9 GiB\" free_swap=\"96.1 GiB\"\ntime=2025-05-30T15:42:11.931+08:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=31 layers.split=\"\" memory.available=\"[46.3 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"66.1 GiB\" memory.required.partial=\"46.0 GiB\" memory.required.kv=\"20.0 GiB\" memory.required.allocations=\"[46.0 GiB]\" memory.weights.total=\"18.4 GiB\" memory.weights.repeating=\"17.8 GiB\" memory.weights.nonrepeating=\"608.6 MiB\" memory.graph.full=\"26.7 GiB\" memory.graph.partial=\"26.7 GiB\"\nllama_model_loader: loaded meta data with 27 key-value pairs and 707 tensors from C:\\Users\\dru1szh\\.ollama\\models\\blobs\\sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 32B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 32B\nllama_model_loader: - kv   5:                          qwen3.block_count u32              = 64\nllama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120\nllama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 25600\nllama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 64\nllama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\ntime=2025-05-30T15:42:12.066+08:00 level=WARN source=sched.go:687 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.2690338 runner.size=\"42.6 GiB\" runner.vram=\"42.6 GiB\" runner.parallel=10 runner.pid=7948 runner.model=C:\\Users\\dru1szh\\.ollama\\models\\blobs\\sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  257 tensors\nllama_model_loader: - type  f16:   64 tensors\nllama_model_loader: - type q4_K:  353 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 18.81 GiB (4.93 BPW) \nload: special tokens cache size = 26\ntime=2025-05-30T15:42:12.316+08:00 level=WARN source=sched.go:687 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.5189133 runner.size=\"42.6 GiB\" runner.vram=\"42.6 GiB\" runner.parallel=10 runner.pid=7948 runner.model=C:\\Users\\dru1szh\\.ollama\\models\\blobs\\sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 32.76 B\nprint_info: general.name     = Qwen3 32B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-05-30T15:42:12.364+08:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\SHV4SZH\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model C:\\\\Users\\\\dru1szh\\\\.ollama\\\\models\\\\blobs\\\\sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312 --ctx-size 81920 --batch-size 512 --n-gpu-layers 31 --threads 40 --no-mmap --parallel 10 --port 53441\"\ntime=2025-05-30T15:42:13.019+08:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-05-30T15:42:13.019+08:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-05-30T15:42:13.021+08:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-05-30T15:42:13.365+08:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\nload_backend: loaded CPU backend from C:\\Users\\SHV4SZH\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-skylakex.dll\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA RTX A6000, compute capability 8.6, VMM: yes\nload_backend: loaded CUDA backend from C:\\Users\\SHV4SZH\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\\ggml-cuda.dll\ntime=2025-05-30T15:42:13.743+08:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)\ntime=2025-05-30T15:42:13.748+08:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:53441\"\ntime=2025-05-30T15:42:13.780+08:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA RTX A6000) - 47545 MiB free\nllama_model_loader: loaded meta data with 27 key-value pairs and 707 tensors from C:\\Users\\dru1szh\\.ollama\\models\\blobs\\sha256-3291abe70f16ee9682de7bfae08db5373ea9d6497e614aaad63340ad421d6312 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 32B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 32B\nllama_model_loader: - kv   5:                          qwen3.block_count u32              = 64\nllama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120\nllama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 25600\nllama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 64\nllama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  257 tensors\nllama_model_loader: - type  f16:   64 tensors\nllama_model_loader: - type q4_K:  353 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 18.81 GiB (4.93 BPW) \nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 64\nprint_info: n_head           = 64\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 25600\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 32B\nprint_info: model params     = 32.76 B\nprint_info: general.name     = Qwen3 32B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 31 repeating layers to GPU\nload_tensors: offloaded 31/65 layers to GPU\nload_tensors:    CUDA_Host model buffer size =  9994.29 MiB\nload_tensors:        CUDA0 model buffer size =  8848.12 MiB\nload_tensors:          CPU model buffer size =   417.30 MiB\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 10\nllama_context: n_ctx         = 81920\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 5120\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     5.99 MiB\nllama_kv_cache_unified: kv_size = 81920, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32\nllama_kv_cache_unified:      CUDA0 KV buffer size =  9920.00 MiB\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "johnnysn",
        "body": "Ollama seems to be overestimating memory use, specially for qwen3. With two RTX 3090's I am being able to run qwen3:32b-q8_0 with a context size of up to 12K tokens before Ollama starts pushing layers to the system RAM.\n\n```bash\n$ ollama -v\n\nollama version is 0.9.0\n```\n`ollama ps` shows  a VRAM use of 47 GB:\n\n```bash\n$ ollama ps\n\nNAME                ID              SIZE     PROCESSOR    UNTIL              \nqwen3:lg-or-x1.5    ed491e0bbed5    47 GB    100% GPU     4 minutes from now\n```\n\nBut there is actually plenty of VRAM available on the system:\n\n```bash\n$ nvidia-smi\n\nSat May 31 12:09:09 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n| 30%   36C    P0            182W /  200W |   18500MiB /  24576MiB |     45%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA GeForce RTX 3090        Off |   00000000:21:00.0 Off |                  N/A |\n|  0%   41C    P0            181W /  200W |   18653MiB /  24576MiB |     50%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A      2022      G   /usr/lib/xorg/Xorg                              9MiB |\n|    0   N/A  N/A      3026      G   /usr/bin/gnome-shell                            8MiB |\n|    0   N/A  N/A     17892      C   /usr/local/bin/ollama                       18454MiB |\n|    1   N/A  N/A      2022      G   /usr/lib/xorg/Xorg                              4MiB |\n|    1   N/A  N/A     17892      C   /usr/local/bin/ollama                       18630MiB |\n+-----------------------------------------------------------------------------------------+\n```\n\nIs there anything specific to this model architecture that might be causing this behavior? I can fit a much larger context window in the GPUs with qwen2.5:32b-instruct-q8_0, which is about the same size."
      },
      {
        "user": "johnnysn",
        "body": "I just tested the default quantized version of qwen3:32b, which is much smaller than the q8 version. The VRAM over-estimation looks significantly worse... for a context window size of just 27.5K, ollama reports 48 GB of VRAM usage, when the system is actually consuming less than 28 GB.\n\n```bash\n$ ollama ps\n\nNAME              ID              SIZE     PROCESSOR    UNTIL              \nqwen3:lg-df-x4    7e3b54845a5b    48 GB    100% GPU     5 minutes from now\n```\n\n```bash\nnvidia-smi\n\nSat May 31 14:52:05 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n|  0%   35C    P0            182W /  200W |   13658MiB /  24576MiB |     48%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA GeForce RTX 3090        Off |   00000000:21:00.0 Off |                  N/A |\n|  0%   37C    P0            187W /  200W |   13749MiB /  24576MiB |     49%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A      1999      G   /usr/lib/xorg/Xorg                              9MiB |\n|    0   N/A  N/A      3075      G   /usr/bin/gnome-shell                            8MiB |\n|    0   N/A  N/A    199197      C   /usr/local/bin/ollama                       13612MiB |\n|    1   N/A  N/A      1999      G   /usr/lib/xorg/Xorg                              4MiB |\n|    1   N/A  N/A    199197      C   /usr/local/bin/ollama                       13726MiB |\n+-----------------------------------------------------------------------------------------+\n```\nIt looks like the problem scales with the context size and the number of available GPUs (#10740 )"
      },
      {
        "user": "ccebelenski",
        "body": "The estimation (and reporting of memory use through Ollama PS) is really off.  For example:\nNAME               ID              SIZE     PROCESSOR          UNTIL\ndeepseek-r1:32b    edba8017331d    80 GB    39%/61% CPU/GPU    57 minutes from now\n\nThat's not even physically possible on that particular machine - and when running doesn't even touch the CPU at all, so the percentage offloading is off also.  Worse, this is spread across 4 cards, and one card isn't being used at all somehow - reporting almost no memory usage with NVTOP.  \n\nJust do a quick actual calculation, total VRAM usage is actually around 42GB, or slightly more than half of that reported.  I can provide logs if needed, but they're not substantially different from the above example.\n\nFlash attention set, quantized models (Q4) and quantized KV cache (Q8).  num_gpu is 65 (total for this model).  \nContext is 128K.\nGPU's are 4x 4090 Ti 16GB cards.\n(Just FYI actually getting not-bad generation speed - around 14 TPS)\n\n"
      }
    ]
  },
  {
    "issue_number": 10359,
    "title": "Memory allocation or estimation problem",
    "author": "apunkt",
    "state": "open",
    "created_at": "2025-04-21T08:45:32Z",
    "updated_at": "2025-06-16T22:02:06Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nNot sure if it is a real issue or just me not fully understanding how things work inside ollama, but experiencing the following:\n\nSetup: \n2 NVIDIA M40 24GB + 1 NVIDA T4 16GB\n\nOllama config:\nOLLAMA_FLASH_ATTENTION=1\nOLLAMA_KEEP_ALIVE=10m\n\nRunning mistral-small3.1:latest with standard context window size 2048:\nollama ps reports 10GB VRAM usage, which can be confirmed by nvidia-smi\n\nRunning mistral-small3.1:latest with extended context window size 32768:\nollama ps\n\n`mistral-small3.1:latest    b9aaf0c2586a    59 GB    100% GPU     9 minutes from now `\n\nwhile nvidia-smi reports:\n\n 0   N/A  N/A   1442364 | C   /usr/local/bin/ollama | 11856MiB\n 1   N/A  N/A   1442364 | C   /usr/local/bin/ollama |  8624MiB \n 2   N/A  N/A   1442364 | C   /usr/local/bin/ollama |  6214MiB \n\nwhich sums up to 26694 MiB VRAM usage.\n\nNot sure if this is expected due to Flash Attention usage??\nIs nvidia-smi reporting real world usage and ollama ps max allocation?\nOllama however reports 2.2x times the real world VRAM usage.\n\nThe problem for me arises when trying to load another model at the same time!\nThere is plenty of VRAM left to load another model and keep mistral loaded, but when loading another model (also with extended context window) ollama *unloads* mistral first and then loads another model that would have fit perfectly in the remaining VRAM.\nThis behaviour is independent from model! It is _not_ a mistral problem as the same shows with other models, too. Further the 2nd model is also not loaded into CPU. If it is a memory allocation problem my expectation would be that it gets loaded into CPU & RAM, but one model is unloaded the other is loaded 100% GPU.\n\nThis brings the following problems for my use case:\nI only can have a single model loaded in VRAM for extended context window.\nAs my users use a variety of models ollama is busy in loading unloading models causing heavy delay and latency.\nVRAM usage is far from efficient.\n\nIf I am using the standard 2048 token context window size I can load 6 different models in VRAM in parallel and all working simultaneously.\nSo my expectation for bigger context windows would be that I can load as many models in parallel that fit into VRAM as they do with standard context window size.\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nLinux\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "apunkt",
        "body": "Adding a 4th card causes size estimation for same model with same settings to be different:\n\n3 Cards:\n| NAME |ID |SIZE|PROCESSOR | UNTIL |\n--- | --- | --- | --- |--- |\n| cogito:14b |  d0cac86a2347 | 44 GB | 100% GPU | 9 minutes from now |\n\n4 Cards:\n| NAME |ID |SIZE|PROCESSOR | UNTIL |\n--- | --- | --- | --- |--- |\n| cogito:14b | d0cac86a2347 | 52 GB  | 15%/85% CPU/GPU | 9 minutes from now |\n\nso more cards => more VRAM requirements with exact same settings?\n"
      },
      {
        "user": "c0008",
        "body": "I ran into the same problem after I enabled flash attention. I have installed two graphic cards (2x 16GB) and the VRAM usage reported by Ollama is too high which is causing unnecessary CPU offloading.\nOllama version is 0.6.5\n\nFor example ollama ps was reporting 33GB memory usage for Qwen2.5 Coder 32B Q5 but according to System monitor VRAM usage was much lower:\nGPU1 13.2 GiB\nGPU2 12.6 GiB\ntotal 25.8 GiB\n\nWithout flash attention these numbers did match.\n\nThis is the memory calculation from the logs:\n> level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=63 layers.split=30,33 memory.available=\"[14.6 GiB 15.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"30.8 GiB\" memory.required.partial=\"29.8 GiB\" memory.required.kv=\"3.1 GiB\" memory.required.allocations=\"[14.3 GiB 15.5 GiB]\" memory.weights.total=\"21.2 GiB\" memory.weights.repeating=\"20.6 GiB\" memory.weights.nonrepeating=\"609.1 MiB\" memory.graph.full=\"2.5 GiB\" memory.graph.partial=\"2.5 GiB\""
      },
      {
        "user": "rick-github",
        "body": "#6160"
      }
    ]
  },
  {
    "issue_number": 10553,
    "title": "mistral-small3.1:24b-instruct-2503 architecture mistral3?",
    "author": "MarkWard0110",
    "state": "open",
    "created_at": "2025-05-03T23:22:35Z",
    "updated_at": "2025-06-16T21:49:07Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI need help understanding the differences I am finding when running `mistral-small3.1:24b-instruct-2503`\n\nOllama's model repo `mistral-small3.1:24b-instruct-2503` has model architecture `mistral3`.\nThe previous `mistral-small:24b-instruct-2501` model architecture was `llama`\nThe hugging face `hf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M` has model architecture `llama`\n\nThe `mistral-small3.1:24b-instruct-2503` appears to be slower and requires more RAM for the same context sizes.  However,  `hf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M` seems to be closer to `mistral-small:24b-instruct-2501`\n\nI have the following metrics recorded for the model, time, and tokens per second.\n\nThe new model using `mistral3` is significantly slower than the previous release using `llama`.  The hugging face using `llama` has similar tokens per second.\n```\nmistral-small:24b-instruct-2501-q4_K_M\t00:00:14.1491177\t56.69\nmistral-small:24b-instruct-2501-q4_K_M\t00:00:13.4088150\t56.25\nmistral-small:24b-instruct-2501-q4_K_M\t00:00:09.0160801\t56.25\nmistral-small:24b-instruct-2501-q4_K_M\t00:00:01.1078162\t56.54\nmistral-small:24b-instruct-2501-q4_K_M\t00:00:06.5794542\t56.4\nmistral-small:24b-instruct-2501-q4_K_M\t00:00:02.7945777\t56.69\nmistral-small:24b-instruct-2501-q4_K_M\t00:00:00.1930713\t49.39\n\nmistral-small3.1:24b-instruct-2503-q4_K_M\t00:00:45.9877791\t12.25\nmistral-small3.1:24b-instruct-2503-q4_K_M\t00:00:56.6890009\t16.59\nmistral-small3.1:24b-instruct-2503-q4_K_M\t00:00:29.2965343\t16.15\nmistral-small3.1:24b-instruct-2503-q4_K_M\t00:00:07.4886711\t12.94\nmistral-small3.1:24b-instruct-2503-q4_K_M\t00:00:29.0066623\t12.99\nmistral-small3.1:24b-instruct-2503-q4_K_M\t00:00:09.2215099\t14.72\nmistral-small3.1:24b-instruct-2503-q4_K_M\t00:00:00.5051990\t21.35\n\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t00:00:15.5489702\t56.74\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t00:00:16.2742764\t56.33\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t00:00:09.8189027\t56.3\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t00:00:01.2301923\t56.14\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t00:00:05.1030198\t56.51\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t00:00:03.1872401\t56.34\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t00:00:00.9599150\t54.62\n\nmistral-small:24b-instruct-2501-q8_0\t00:00:29.3273764\t29.94\nmistral-small:24b-instruct-2501-q8_0\t00:00:31.6365711\t29.84\nmistral-small:24b-instruct-2501-q8_0\t00:00:15.6844838\t29.92\nmistral-small:24b-instruct-2501-q8_0\t00:00:01.5447448\t30.1\nmistral-small:24b-instruct-2501-q8_0\t00:00:08.9758766\t29.93\nmistral-small:24b-instruct-2501-q8_0\t00:00:05.2431780\t29.99\nmistral-small:24b-instruct-2501-q8_0\t00:00:00.2929514\t31.5\n\nmistral-small3.1:24b-instruct-2503-q8_0\t00:00:45.3856788\t15.64\nmistral-small3.1:24b-instruct-2503-q8_0\t00:01:26.4857499\t13.34\nmistral-small3.1:24b-instruct-2503-q8_0\t00:00:34.0899852\t14.12\nmistral-small3.1:24b-instruct-2503-q8_0\t00:00:04.2336628\t15.58\nmistral-small3.1:24b-instruct-2503-q8_0\t00:00:22.6926162\t14.09\nmistral-small3.1:24b-instruct-2503-q8_0\t00:00:10.7818053\t13.58\nmistral-small3.1:24b-instruct-2503-q8_0\t00:00:00.7040794\t14.33\n```\n\nThe memory used is also different. The following is model, context size, and memory used.  There is also something very weird with the memory allocations with the 3.1 Ollama `mistral3` version\n\n\n```\nmistral-small:24b-instruct-2501-q4_K_M\t2048\t14.3GiB\nmistral-small:24b-instruct-2501-q4_K_M\t4096\t14.7GiB\nmistral-small:24b-instruct-2501-q4_K_M\t8192\t15.6GiB\nmistral-small:24b-instruct-2501-q4_K_M\t16384\t17.4GiB\nmistral-small:24b-instruct-2501-q4_K_M\t32768\t21.0GiB\n\nmistral-small3.1:24b-instruct-2503-q4_K_M\t2048\t24.9GiB\nmistral-small3.1:24b-instruct-2503-q4_K_M\t4096\t25.6GiB\nmistral-small3.1:24b-instruct-2503-q4_K_M\t8192\t27.1GiB\nmistral-small3.1:24b-instruct-2503-q4_K_M\t16384\t30.1GiB\nmistral-small3.1:24b-instruct-2503-q4_K_M\t32768\t36.1GiB\nmistral-small3.1:24b-instruct-2503-q4_K_M\t65536\t41.0GiB\nmistral-small3.1:24b-instruct-2503-q4_K_M\t131072\t33.9GiB\n\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t2048\t14.3GiB\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t4096\t14.7GiB\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t8192\t15.6GiB\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t16384\t17.4GiB\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t32768\t21.0GiB\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t65536\t34.0GiB\nhf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\t131072\t53.8GiB\n\n\nmistral-small:24b-instruct-2501-q8_0\t2048\t26.5GiB\nmistral-small:24b-instruct-2501-q8_0\t4096\t26.9GiB\nmistral-small:24b-instruct-2501-q8_0\t8192\t27.5GiB\nmistral-small:24b-instruct-2501-q8_0\t16384\t29.6GiB\nmistral-small:24b-instruct-2501-q8_0\t32768\t34.4GiB\n\nmistral-small3.1:24b-instruct-2503-q8_0\t2048\t35.1GiB\nmistral-small3.1:24b-instruct-2503-q8_0\t4096\t35.8GiB\nmistral-small3.1:24b-instruct-2503-q8_0\t8192\t37.3GiB\nmistral-small3.1:24b-instruct-2503-q8_0\t16384\t40.3GiB\nmistral-small3.1:24b-instruct-2503-q8_0\t32768\t46.3GiB\nmistral-small3.1:24b-instruct-2503-q8_0\t65536\t50.3GiB\nmistral-small3.1:24b-instruct-2503-q8_0\t131072\t42.8GiB\n\n```\n\n\n```\n\nollama show hf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\n  Model\n    architecture        llama\n    parameters          23.6B\n    context length      131072\n    embedding length    5120\n    quantization        unknown\n\n  Capabilities\n    completion\n    tools\n\n  Projector\n    architecture        clip\n    parameters          438.96M\n    embedding length    1024\n    dimensions          5120\n\n  Parameters\n    stop    \"[INST]\"\n\nollama show mistral-small3.1:24b-instruct-2503-q4_K_M\n  Model\n    architecture        mistral3\n    parameters          24.0B\n    context length      131072\n    embedding length    5120\n    quantization        Q4_K_M\n\n  Capabilities\n    completion\n    vision\n    tools\n\n  Parameters\n    num_ctx    4096\n\n  System\n    You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI, a French startup\n      headquartered in Paris.\n    You power an AI assistant called Le Chat.\n\n\n\nollama show mistral-small:24b-instruct-2501-q4_K_M\n  Model\n    architecture        llama\n    parameters          23.6B\n    context length      32768\n    embedding length    5120\n    quantization        Q4_K_M\n\n  Capabilities\n    completion\n    tools\n\n  Parameters\n    temperature    0.15\n\n  System\n    You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup\n      headquartered in Paris. Your knowledge base was last updated on 2023-10-01. When you're not sure\n      about some information, you say that you don't have the information and don't make up anything.\n      If the user's question is not clear, ambiguous, or does not provide enough context for you to\n      accurately answer the question, you do not try to answer it right away and you rather ask the user\n      to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or\n      \"When is the next flight to Tokyo\" => \"Where do you travel from?\")\n\n  License\n    Apache License\n    Version 2.0, January 2004\n\n\n\nollama show mistral-small3.1:24b-instruct-2503-q8_0\n  Model\n    architecture        mistral3\n    parameters          24.0B\n    context length      131072\n    embedding length    5120\n    quantization        Q8_0\n\n  Capabilities\n    completion\n    vision\n    tools\n\n  Parameters\n    num_ctx    4096\n\n  System\n    You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI, a French startup\n      headquartered in Paris.\n    You power an AI assistant called Le Chat.\n\nollama show mistral-small:24b-instruct-2501-q8_0\n  Model\n    architecture        llama\n    parameters          23.6B\n    context length      32768\n    embedding length    5120\n    quantization        Q8_0\n\n  Capabilities\n    completion\n    tools\n\n  Parameters\n    temperature    0.15\n\n  System\n    You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup\n      headquartered in Paris. Your knowledge base was last updated on 2023-10-01. When you're not sure\n      about some information, you say that you don't have the information and don't make up anything.\n      If the user's question is not clear, ambiguous, or does not provide enough context for you to\n      accurately answer the question, you do not try to answer it right away and you rather ask the user\n      to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or\n      \"When is the next flight to Tokyo\" => \"Where do you travel from?\")\n\n  License\n    Apache License\n    Version 2.0, January 2004\n\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.6.7",
    "comments": [
      {
        "user": "rick-github",
        "body": "You don't indicate the hardware that you are running on.  If I test this on 2x a100 with 40G, I get comparable results:\n```console\n$ for i in mistral-small:24b-instruct-2501-q4_K_M mistral-small3.1:24b-instruct-2503-q4_K_M  ; do ollama run --verbose $i hello ; done\nHello! How can I assist you today?\n\ntotal duration:       6.268738597s\nload duration:        5.73568526s\nprompt eval count:    162 token(s)\nprompt eval duration: 337.648618ms\nprompt eval rate:     479.79 tokens/s\neval count:           10 token(s)\neval duration:        194.323291ms\neval rate:            51.46 tokens/s\n\nHello! How can I assist you today?\n\ntotal duration:       6.516360924s\nload duration:        5.840908652s\nprompt eval count:    359 token(s)\nprompt eval duration: 500.315958ms\nprompt eval rate:     717.55 tokens/s\neval count:           10 token(s)\neval duration:        174.212716ms\neval rate:            57.40 tokens/s\n$ ollama ps\nNAME                                         ID              SIZE     PROCESSOR    UNTIL            \nmistral-small3.1:24b-instruct-2503-q4_K_M    b9aaf0c2586a    26 GB    100% GPU     2 hours from now    \nmistral-small:24b-instruct-2501-q4_K_M       8039dd90c113    15 GB    100% GPU     2 hours from now \n```\nNote that 3.1 can process images and the weights for the vision model results in a much larger VRAM footprint.  This can cause the model to spill layers to system RAM, resulting in slower inference.\n\nI was unable to test the bartowski model since the version I downloaded has an unsupported vision projector.   The repo shows that this was [added](https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/commit/f73dfd9e812922fb503a993e3fa5671424f486d3) a couple of days ago for llama.cpp support."
      },
      {
        "user": "MarkWard0110",
        "body": "@rick-github \nmy hardware\nintel core i9 14900K\n96GB RAM (6400 MT/s)\nRTX 3090 24GB\nRTX 4070 TI Super 16GB\n\nso you are saying the difference might be due to the newer 3.1 having the vision model and now that the 3.1 architecture is mistral3 the additional memory and lower tokens could be the vision model is active also?  and when I was testing with the bartowski having the llama architecture could have only been the LLM?\n\nI'll see if I can compare with the updated bartowski model but there might be details that I am not understanding.  I ran the pull again and it seems there is no change and still has the architecture as llama\n```\nollama pull hf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\npulling manifest\npulling c5743c1bf39d: 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████████████▏  14 GB\npulling 6db27cd4e277: 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████████████▏  695 B\npulling f5add93ad360: 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 878 MB\npulling 4d1dedbfd2bd: 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████████████▏   19 B\npulling 57122ba533ca: 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████████████▏  626 B\nverifying sha256 digest\nwriting manifest\nsuccess\n```"
      },
      {
        "user": "thot-experiment",
        "body": "> \"Note that 3.1 can process images and the weights for the vision model results in a much larger VRAM footprint\"\n\n@rick-github this is actually incorrect, the vision tower/projector are not 11 gigs and ollama's memory estimation/measurement is still broken\n```\n> ollama --version\nollama version is 0.6.8\n> ollama ps\nNAME            ID              SIZE     PROCESSOR    UNTIL\nm31.32k:q5ks    03154eff3cf3    31 GB    100% GPU     Forever\n```\nvs nvidia-smi taken *during* vision inference\n```\n> nvidia-smi\nWed May  7 11:26:21 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 576.02                 Driver Version: 576.02         CUDA Version: 12.9     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   1  Quadro GV100                 WDDM  |   00000000:08:00.0 Off |                  Off |\n| 46%   51C    P2            204W /  250W |   19963MiB /  32768MiB |     89%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n```\n"
      }
    ]
  },
  {
    "issue_number": 10041,
    "title": "gemma EOF error on image input due to improper memory management",
    "author": "Master-Pr0grammer",
    "state": "closed",
    "created_at": "2025-03-29T16:25:15Z",
    "updated_at": "2025-06-16T21:47:12Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n\n### Description:\nWhen running gemma 3 with image inputs on some systems, the model crashes with a EOF error due to improper memory management.\n\n### Setup:\nI am running gemma3:12b with on a system with a gtx 1080ti and a gtx 1050ti. when loading the model, ollama splits the model on to the two different GPU's, (see the attached nvidia-smi file for vram usage). As you can see, I still have a significant amount of VRAM left on the 1080ti (~4.5Gb), but I only have ~500Mb on the 1050ti.\n\n### Possible Explanation:\nI am not super familiar with the backend of ollama, but I am going to take a guess that it has to do with the GPU split, and not properly pre-allocating memory, as evident by this line in the error log:\n\n`Mar 29 11:58:38 watson ollama[1408032]: ggml_backend_cuda_buffer_type_alloc_buffer: allocating 1195.28 MiB on device 1: cudaMalloc failed: out of memory`\n\nIt is trying to allocate more memory after the model is already loaded, onto the device with less remaining memory (1050ti). again, I don't know anything about the backend, but I feel like the context length should be pre-allocated, and adding images would simply take up more of this pre-allocated context, and it shouldn't try to allocate more after the fact in order to prevent these kinds of crashes.\n\n### Reproducability:\n\nPython script I used to produce error:\n```python3\nimport ollama\n\nclient = ollama.Client(host='http://192.168.50.221:11434/')\nresponse = client.chat(\n    model='gemma3:12b',\n    messages=[\n        {'role':'system', 'content':'You are a helpful AI assistant'},\n        {'role':'user', 'content':'what is in this image?', 'images':['Screenshot 2025-03-13 003328.png']}\n    ]\n)\n\nprint(response.message.content)\n```\n\nError Output:\n```\nTraceback (most recent call last):\n  File \"\\Desktop\\test.py\", line 4, in <module>\n    response = client.chat(\n               ^^^^^^^^^^^^\n  File \"...\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ollama\\_client.py\", line 333, in chat\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"...\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ollama\\_client.py\", line 178, in _request\n    return cls(**self._request_raw(*args, **kwargs).json())\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ollama\\_client.py\", line 122, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: POST predict: Post \"http://127.0.0.1:33823/completion\": EOF (status code: 500)\n```\n\nThe image I used was just a screenshot of my screen at standard 1080p resolution.\nI hope this helps, but I doubt you will be able to reproduce this error without a system that uses GPU split with one or two GPU's low on VRAM.\n\n### Relevant Files:\n[nvidia-smi.txt](https://github.com/user-attachments/files/19521995/nvidia-smi.txt)\n\n### Relevant log output\n\n```shell\nMar 29 11:58:35 watson ollama[1408032]: time=2025-03-29T11:58:35.421-04:00 level=WARN source=sched.go:647 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.227315301 model=/usr/share/ollama/.ollama/models/blobs/sha256-adca500fad9b54c565ae672184e0c9eb690eb6014ba63f8ec13849d4f73a32d3\nMar 29 11:58:35 watson ollama[1408032]: time=2025-03-29T11:58:35.672-04:00 level=WARN source=sched.go:647 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.477852175 model=/usr/share/ollama/.ollama/models/blobs/sha256-adca500fad9b54c565ae672184e0c9eb690eb6014ba63f8ec13849d4f73a32d3\nMar 29 11:58:35 watson ollama[1408032]: time=2025-03-29T11:58:35.891-04:00 level=INFO source=sched.go:731 msg=\"new model will fit in available VRAM, loading\" model=/usr/share/ollama/.ollama/models/blobs/sha256-adca500fad9b54c565ae672184e0c9eb690eb6014ba63f8ec13849d4f73a32d3 library=cuda parallel=4 required=\"14.3 GiB\"\nMar 29 11:58:35 watson ollama[1408032]: time=2025-03-29T11:58:35.951-04:00 level=WARN source=sched.go:647 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.756531494 model=/usr/share/ollama/.ollama/models/blobs/sha256-adca500fad9b54c565ae672184e0c9eb690eb6014ba63f8ec13849d4f73a32d3\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.135-04:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"15.6 GiB\" free=\"14.2 GiB\" free_swap=\"2.9 GiB\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.138-04:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=49 layers.split=38,11 memory.available=\"[10.6 GiB 3.9 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"14.3 GiB\" memory.required.partial=\"14.3 GiB\" memory.required.kv=\"1.9 GiB\" memory.required.allocations=\"[10.5 GiB 3.7 GiB]\" memory.weights.total=\"6.8 GiB\" memory.weights.repeating=\"6.0 GiB\" memory.weights.nonrepeating=\"787.5 MiB\" memory.graph.full=\"1.3 GiB\" memory.graph.partial=\"1.3 GiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.261-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.268-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.add_eot_token default=false\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.271-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.280-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.280-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.local.freq_base default=10000\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.280-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.global.freq_base default=1e+06\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.280-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.freq_scale default=1\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.280-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.mm_tokens_per_image default=256\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.280-04:00 level=INFO source=server.go:405 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-adca500fad9b54c565ae672184e0c9eb690eb6014ba63f8ec13849d4f73a32d3 --ctx-size 8192 --batch-size 512 --n-gpu-layers 49 --threads 4 --no-mmap --parallel 4 --tensor-split 38,11 --port 33823\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.281-04:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.281-04:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.281-04:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.294-04:00 level=INFO source=runner.go:765 msg=\"starting ollama engine\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.297-04:00 level=INFO source=runner.go:828 msg=\"Server listening on 127.0.0.1:33823\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.429-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=general.name default=\"\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.429-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=general.description default=\"\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.429-04:00 level=INFO source=ggml.go:69 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=1065 num_key_values=36\nMar 29 11:58:36 watson ollama[1408032]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nMar 29 11:58:36 watson ollama[1408032]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nMar 29 11:58:36 watson ollama[1408032]: ggml_cuda_init: found 2 CUDA devices:\nMar 29 11:58:36 watson ollama[1408032]:   Device 0: NVIDIA GeForce GTX 1080 Ti, compute capability 6.1, VMM: yes\nMar 29 11:58:36 watson ollama[1408032]:   Device 1: NVIDIA GeForce GTX 1050 Ti, compute capability 6.1, VMM: yes\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.532-04:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nMar 29 11:58:36 watson ollama[1408032]: load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\nMar 29 11:58:36 watson ollama[1408032]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.560-04:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 CUDA.1.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.1.USE_GRAPHS=1 CUDA.1.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.631-04:00 level=INFO source=ggml.go:291 msg=\"model weights\" buffer=CUDA1 size=\"2.9 GiB\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.631-04:00 level=INFO source=ggml.go:291 msg=\"model weights\" buffer=CPU size=\"787.5 MiB\"\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.631-04:00 level=INFO source=ggml.go:291 msg=\"model weights\" buffer=CUDA0 size=\"4.7 GiB\"\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.090-04:00 level=INFO source=ggml.go:383 msg=\"compute graph\" backend=CUDA0 buffer_type=CUDA0\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.090-04:00 level=INFO source=ggml.go:383 msg=\"compute graph\" backend=CUDA1 buffer_type=CUDA1\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.090-04:00 level=INFO source=ggml.go:383 msg=\"compute graph\" backend=CPU buffer_type=CUDA_Host\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.091-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.093-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.add_eot_token default=false\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.096-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.103-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.103-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.local.freq_base default=10000\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.103-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.global.freq_base default=1e+06\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.103-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.freq_scale default=1\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.103-04:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.mm_tokens_per_image default=256\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.291-04:00 level=INFO source=server.go:619 msg=\"llama runner started in 2.01 seconds\"\nMar 29 11:58:38 watson ollama[1408032]: ggml_backend_cuda_buffer_type_alloc_buffer: allocating 1195.28 MiB on device 1: cudaMalloc failed: out of memory\nMar 29 11:58:38 watson ollama[1408032]: ggml_gallocr_reserve_n: failed to allocate CUDA1 buffer of size 1253346304\nMar 29 11:58:38 watson ollama[1408032]: SIGSEGV: segmentation violation\nMar 29 11:58:38 watson ollama[1408032]: PC=0x59fdcdf550c0 m=11 sigcode=1 addr=0x58\nMar 29 11:58:38 watson ollama[1408032]: signal arrived during cgo execution\nMar 29 11:58:38 watson ollama[1408032]: goroutine 20 gp=0xc0001c7880 m=11 mp=0xc000307808 [syscall]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.cgocall(0x59fdcdfa90d0, 0xc000229aa8)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/cgocall.go:167 +0x4b fp=0xc000229a80 sp=0xc000229a48 pc=0x59fdcd17496b\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/ml/backend/ggml._Cfunc_ggml_backend_sched_graph_compute_async(0x7a2d38003a70, 0x7a303a400d50)\nMar 29 11:58:38 watson ollama[1408032]:         _cgo_gotypes.go:485 +0x4a fp=0xc000229aa8 sp=0xc000229a80 pc=0x59fdcd56e9aa\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/ml/backend/ggml.Context.Compute.func1(...)\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/ml/backend/ggml/ggml.go:524\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/ml/backend/ggml.Context.Compute({0xc000412200, 0x7a2d38003940, 0x7a303a400d50, 0x0, 0x2000}, {0xc0031266f0, 0x1, 0xc00258a120?})\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/ml/backend/ggml/ggml.go:524 +0xbd fp=0xc000229b38 sp=0xc000229aa8 pc=0x59fdcd5774fd\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/ml/backend/ggml.(*Context).Compute(0xc0030f44b0?, {0xc0031266f0?, 0xc00258a090?, 0x1?})\nMar 29 11:58:38 watson ollama[1408032]:         <autogenerated>:1 +0x72 fp=0xc000229bb0 sp=0xc000229b38 pc=0x59fdcd57cf72\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/model.Forward({0x59fdce45c600, 0xc0030f44b0}, {0x59fdce453c90, 0xc00030a0e0}, {0xc0030c1000, 0x11e, 0x200}, {{0x59fdce464c10, 0xc00258a0a8}, {0xc00258a090, ...}, ...})\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/model/model.go:312 +0x2b8 fp=0xc000229c90 sp=0xc000229bb0 pc=0x59fdcd5a41f8\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/runner/ollamarunner.(*Server).processBatch(0xc00053b9e0)\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:424 +0x3fe fp=0xc000229f98 sp=0xc000229c90 pc=0x59fdcd62775e\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/runner/ollamarunner.(*Server).run(0xc00053b9e0, {0x59fdce454fc0, 0xc000531db0})\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:336 +0x4e fp=0xc000229fb8 sp=0xc000229f98 pc=0x59fdcd62730e\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/runner/ollamarunner.Execute.gowrap2()\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:805 +0x28 fp=0xc000229fe0 sp=0xc000229fb8 pc=0x59fdcd62b708\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000229fe8 sp=0xc000229fe0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:805 +0xb37\nMar 29 11:58:38 watson ollama[1408032]: goroutine 1 gp=0xc000002380 m=nil [IO wait]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000225628 sp=0xc000225608 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.netpollblock(0xc000225678?, 0xcd111426?, 0xfd?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/netpoll.go:575 +0xf7 fp=0xc000225660 sp=0xc000225628 pc=0x59fdcd13ca57\nMar 29 11:58:38 watson ollama[1408032]: internal/poll.runtime_pollWait(0x7a30a2a6deb0, 0x72)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/netpoll.go:351 +0x85 fp=0xc000225680 sp=0xc000225660 pc=0x59fdcd176e85\nMar 29 11:58:38 watson ollama[1408032]: internal/poll.(*pollDesc).wait(0xc00050f300?, 0x900000036?, 0x0)\nMar 29 11:58:38 watson ollama[1408032]:         internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0002256a8 sp=0xc000225680 pc=0x59fdcd1fe307\nMar 29 11:58:38 watson ollama[1408032]: internal/poll.(*pollDesc).waitRead(...)\nMar 29 11:58:38 watson ollama[1408032]:         internal/poll/fd_poll_runtime.go:89\nMar 29 11:58:38 watson ollama[1408032]: internal/poll.(*FD).Accept(0xc00050f300)\nMar 29 11:58:38 watson ollama[1408032]:         internal/poll/fd_unix.go:620 +0x295 fp=0xc000225750 sp=0xc0002256a8 pc=0x59fdcd2036d5\nMar 29 11:58:38 watson ollama[1408032]: net.(*netFD).accept(0xc00050f300)\nMar 29 11:58:38 watson ollama[1408032]:         net/fd_unix.go:172 +0x29 fp=0xc000225808 sp=0xc000225750 pc=0x59fdcd2764e9\nMar 29 11:58:38 watson ollama[1408032]: net.(*TCPListener).accept(0xc00053c000)\nMar 29 11:58:38 watson ollama[1408032]:         net/tcpsock_posix.go:159 +0x1b fp=0xc000225858 sp=0xc000225808 pc=0x59fdcd28be9b\nMar 29 11:58:38 watson ollama[1408032]: net.(*TCPListener).Accept(0xc00053c000)\nMar 29 11:58:38 watson ollama[1408032]:         net/tcpsock.go:380 +0x30 fp=0xc000225888 sp=0xc000225858 pc=0x59fdcd28ad50\nMar 29 11:58:38 watson ollama[1408032]: net/http.(*onceCloseListener).Accept(0xc000126360?)\nMar 29 11:58:38 watson ollama[1408032]:         <autogenerated>:1 +0x24 fp=0xc0002258a0 sp=0xc000225888 pc=0x59fdcd4a2384\nMar 29 11:58:38 watson ollama[1408032]: net/http.(*Server).Serve(0xc0001e8e00, {0x59fdce452cf8, 0xc00053c000})\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:3424 +0x30c fp=0xc0002259d0 sp=0xc0002258a0 pc=0x59fdcd479c4c\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/runner/ollamarunner.Execute({0xc000034170, 0x11, 0x11})\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:829 +0xec9 fp=0xc000225d08 sp=0xc0002259d0 pc=0x59fdcd62b469\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/runner.Execute({0xc000034150?, 0x0?, 0x0?})\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/runner/runner.go:20 +0xc9 fp=0xc000225d30 sp=0xc000225d08 pc=0x59fdcd62c0e9\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/cmd.NewCLI.func2(0xc0001e9200?, {0x59fdcdfc4055?, 0x4?, 0x59fdcdfc4059?})\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/cmd/cmd.go:1329 +0x45 fp=0xc000225d58 sp=0xc000225d30 pc=0x59fdcdd79b25\nMar 29 11:58:38 watson ollama[1408032]: github.com/spf13/cobra.(*Command).execute(0xc000128f08, {0xc00053b7a0, 0x12, 0x12})\nMar 29 11:58:38 watson ollama[1408032]:         github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc000225e78 sp=0xc000225d58 pc=0x59fdcd2efb3c\nMar 29 11:58:38 watson ollama[1408032]: github.com/spf13/cobra.(*Command).ExecuteC(0xc0004a2f08)\nMar 29 11:58:38 watson ollama[1408032]:         github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc000225f30 sp=0xc000225e78 pc=0x59fdcd2f0385\nMar 29 11:58:38 watson ollama[1408032]: github.com/spf13/cobra.(*Command).Execute(...)\nMar 29 11:58:38 watson ollama[1408032]:         github.com/spf13/cobra@v1.7.0/command.go:992\nMar 29 11:58:38 watson ollama[1408032]: github.com/spf13/cobra.(*Command).ExecuteContext(...)\nMar 29 11:58:38 watson ollama[1408032]:         github.com/spf13/cobra@v1.7.0/command.go:985\nMar 29 11:58:38 watson ollama[1408032]: main.main()\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/main.go:12 +0x4d fp=0xc000225f50 sp=0xc000225f30 pc=0x59fdcdd79e8d\nMar 29 11:58:38 watson ollama[1408032]: runtime.main()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:283 +0x29d fp=0xc000225fe0 sp=0xc000225f50 pc=0x59fdcd14405d\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000225fe8 sp=0xc000225fe0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: goroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000064fa8 sp=0xc000064f88 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.goparkunlock(...)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:441\nMar 29 11:58:38 watson ollama[1408032]: runtime.forcegchelper()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:348 +0xb8 fp=0xc000064fe0 sp=0xc000064fa8 pc=0x59fdcd144398\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000064fe8 sp=0xc000064fe0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by runtime.init.7 in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:336 +0x1a\nMar 29 11:58:38 watson ollama[1408032]: goroutine 3 gp=0xc000003340 m=nil [GC sweep wait]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000065780 sp=0xc000065760 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.goparkunlock(...)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:441\nMar 29 11:58:38 watson ollama[1408032]: runtime.bgsweep(0xc00007e000)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgcsweep.go:316 +0xdf fp=0xc0000657c8 sp=0xc000065780 pc=0x59fdcd12ea5f\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcenable.gowrap1()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:204 +0x25 fp=0xc0000657e0 sp=0xc0000657c8 pc=0x59fdcd122e45\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000657e8 sp=0xc0000657e0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by runtime.gcenable in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:204 +0x66\nMar 29 11:58:38 watson ollama[1408032]: goroutine 4 gp=0xc000003500 m=nil [GC scavenge wait]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x1285dfdc?, 0x127e78d7?, 0x0?, 0x0?, 0x0?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000065f78 sp=0xc000065f58 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.goparkunlock(...)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:441\nMar 29 11:58:38 watson ollama[1408032]: runtime.(*scavengerState).park(0x59fdcecbb1c0)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgcscavenge.go:425 +0x49 fp=0xc000065fa8 sp=0xc000065f78 pc=0x59fdcd12c4a9\nMar 29 11:58:38 watson ollama[1408032]: runtime.bgscavenge(0xc00007e000)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgcscavenge.go:658 +0x59 fp=0xc000065fc8 sp=0xc000065fa8 pc=0x59fdcd12ca39\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcenable.gowrap2()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:205 +0x25 fp=0xc000065fe0 sp=0xc000065fc8 pc=0x59fdcd122de5\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000065fe8 sp=0xc000065fe0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by runtime.gcenable in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:205 +0xa5\nMar 29 11:58:38 watson ollama[1408032]: goroutine 5 gp=0xc000003dc0 m=nil [finalizer wait]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x1b8?, 0xc000002380?, 0x1?, 0x23?, 0xc000064688?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000064630 sp=0xc000064610 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.runfinq()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mfinal.go:196 +0x107 fp=0xc0000647e0 sp=0xc000064630 pc=0x59fdcd121e07\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000647e8 sp=0xc0000647e0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by runtime.createfing in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mfinal.go:166 +0x3d\nMar 29 11:58:38 watson ollama[1408032]: goroutine 6 gp=0xc0001c68c0 m=nil [chan receive]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0xc0002197c0?, 0xc0030fb0e0?, 0x60?, 0x67?, 0x59fdcd25d228?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000066718 sp=0xc0000666f8 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.chanrecv(0xc0000423f0, 0x0, 0x1)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/chan.go:664 +0x445 fp=0xc000066790 sp=0xc000066718 pc=0x59fdcd114005\nMar 29 11:58:38 watson ollama[1408032]: runtime.chanrecv1(0x0?, 0x0?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/chan.go:506 +0x12 fp=0xc0000667b8 sp=0xc000066790 pc=0x59fdcd113b92\nMar 29 11:58:38 watson ollama[1408032]: runtime.unique_runtime_registerUniqueMapCleanup.func2(...)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1796\nMar 29 11:58:38 watson ollama[1408032]: runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1799 +0x2f fp=0xc0000667e0 sp=0xc0000667b8 pc=0x59fdcd125fef\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000667e8 sp=0xc0000667e0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by unique.runtime_registerUniqueMapCleanup in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1794 +0x85\nMar 29 11:58:38 watson ollama[1408032]: goroutine 7 gp=0xc0001c7180 m=nil [GC worker (idle)]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x22a48fd53c1d7?, 0x3?, 0xf3?, 0x4f?, 0x0?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000066f38 sp=0xc000066f18 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcBgMarkWorker(0xc0000439d0)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1423 +0xe9 fp=0xc000066fc8 sp=0xc000066f38 pc=0x59fdcd125309\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcBgMarkStartWorkers.gowrap1()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1339 +0x25 fp=0xc000066fe0 sp=0xc000066fc8 pc=0x59fdcd1251e5\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000066fe8 sp=0xc000066fe0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1339 +0x105\nMar 29 11:58:38 watson ollama[1408032]: goroutine 8 gp=0xc0001c7340 m=nil [GC worker (idle)]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x22a4912efa1bd?, 0x3?, 0x8f?, 0xd0?, 0x0?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000067738 sp=0xc000067718 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcBgMarkWorker(0xc0000439d0)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1423 +0xe9 fp=0xc0000677c8 sp=0xc000067738 pc=0x59fdcd125309\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcBgMarkStartWorkers.gowrap1()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1339 +0x25 fp=0xc0000677e0 sp=0xc0000677c8 pc=0x59fdcd1251e5\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000677e8 sp=0xc0000677e0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1339 +0x105\nMar 29 11:58:38 watson ollama[1408032]: goroutine 9 gp=0xc0001c7500 m=nil [GC worker (idle)]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x22a4912efbbf0?, 0x3?, 0x3f?, 0xc4?, 0x0?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000067f38 sp=0xc000067f18 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcBgMarkWorker(0xc0000439d0)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1423 +0xe9 fp=0xc000067fc8 sp=0xc000067f38 pc=0x59fdcd125309\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcBgMarkStartWorkers.gowrap1()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1339 +0x25 fp=0xc000067fe0 sp=0xc000067fc8 pc=0x59fdcd1251e5\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000067fe8 sp=0xc000067fe0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1339 +0x105\nMar 29 11:58:38 watson ollama[1408032]: goroutine 18 gp=0xc000102380 m=nil [GC worker (idle)]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x22a4912efa23c?, 0x1?, 0xc5?, 0xfb?, 0x0?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000060738 sp=0xc000060718 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcBgMarkWorker(0xc0000439d0)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1423 +0xe9 fp=0xc0000607c8 sp=0xc000060738 pc=0x59fdcd125309\nMar 29 11:58:38 watson ollama[1408032]: runtime.gcBgMarkStartWorkers.gowrap1()\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1339 +0x25 fp=0xc0000607e0 sp=0xc0000607c8 pc=0x59fdcd1251e5\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000607e8 sp=0xc0000607e0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         runtime/mgc.go:1339 +0x105\nMar 29 11:58:38 watson ollama[1408032]: goroutine 10 gp=0xc0001c6700 m=nil [select]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0xc000049a08?, 0x2?, 0x0?, 0xc6?, 0xc000049864?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc000049678 sp=0xc000049658 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.selectgo(0xc000049a08, 0xc000049860, 0x11e?, 0x0, 0x4?, 0x1)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/select.go:351 +0x837 fp=0xc0000497b0 sp=0xc000049678 pc=0x59fdcd156557\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/runner/ollamarunner.(*Server).completion(0xc00053b9e0, {0x59fdce452ed8, 0xc0052440e0}, 0xc003021e00)\nMar 29 11:58:38 watson ollama[1408032]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:621 +0xae5 fp=0xc000049ac0 sp=0xc0000497b0 pc=0x59fdcd6298c5\nMar 29 11:58:38 watson ollama[1408032]: github.com/ollama/ollama/runner/ollamarunner.(*Server).completion-fm({0x59fdce452ed8?, 0xc0052440e0?}, 0xc000223b40?)\nMar 29 11:58:38 watson ollama[1408032]:         <autogenerated>:1 +0x36 fp=0xc000049af0 sp=0xc000049ac0 pc=0x59fdcd62bf56\nMar 29 11:58:38 watson ollama[1408032]: net/http.HandlerFunc.ServeHTTP(0xc000550000?, {0x59fdce452ed8?, 0xc0052440e0?}, 0xc000223b60?)\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:2294 +0x29 fp=0xc000049b18 sp=0xc000049af0 pc=0x59fdcd476289\nMar 29 11:58:38 watson ollama[1408032]: net/http.(*ServeMux).ServeHTTP(0x59fdcd11c325?, {0x59fdce452ed8, 0xc0052440e0}, 0xc003021e00)\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:2822 +0x1c4 fp=0xc000049b68 sp=0xc000049b18 pc=0x59fdcd478184\nMar 29 11:58:38 watson ollama[1408032]: net/http.serverHandler.ServeHTTP({0x59fdce44f570?}, {0x59fdce452ed8?, 0xc0052440e0?}, 0x1?)\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:3301 +0x8e fp=0xc000049b98 sp=0xc000049b68 pc=0x59fdcd495c0e\nMar 29 11:58:38 watson ollama[1408032]: net/http.(*conn).serve(0xc000126360, {0x59fdce454f88, 0xc0000ebd40})\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:2102 +0x625 fp=0xc000049fb8 sp=0xc000049b98 pc=0x59fdcd474785\nMar 29 11:58:38 watson ollama[1408032]: net/http.(*Server).Serve.gowrap3()\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:3454 +0x28 fp=0xc000049fe0 sp=0xc000049fb8 pc=0x59fdcd47a048\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000049fe8 sp=0xc000049fe0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by net/http.(*Server).Serve in goroutine 1\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:3454 +0x485\nMar 29 11:58:38 watson ollama[1408032]: goroutine 1071 gp=0xc0001c7c00 m=nil [IO wait]:\nMar 29 11:58:38 watson ollama[1408032]: runtime.gopark(0x0?, 0xc000061768?, 0x93?, 0x5f?, 0xb?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/proc.go:435 +0xce fp=0xc0000615d8 sp=0xc0000615b8 pc=0x59fdcd177c6e\nMar 29 11:58:38 watson ollama[1408032]: runtime.netpollblock(0x59fdcd19b0f8?, 0xcd111426?, 0xfd?)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/netpoll.go:575 +0xf7 fp=0xc000061610 sp=0xc0000615d8 pc=0x59fdcd13ca57\nMar 29 11:58:38 watson ollama[1408032]: internal/poll.runtime_pollWait(0x7a30a2a6dd98, 0x72)\nMar 29 11:58:38 watson ollama[1408032]:         runtime/netpoll.go:351 +0x85 fp=0xc000061630 sp=0xc000061610 pc=0x59fdcd176e85\nMar 29 11:58:38 watson ollama[1408032]: internal/poll.(*pollDesc).wait(0xc00050e100?, 0xc0002120a1?, 0x0)\nMar 29 11:58:38 watson ollama[1408032]:         internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000061658 sp=0xc000061630 pc=0x59fdcd1fe307\nMar 29 11:58:38 watson ollama[1408032]: internal/poll.(*pollDesc).waitRead(...)\nMar 29 11:58:38 watson ollama[1408032]:         internal/poll/fd_poll_runtime.go:89\nMar 29 11:58:38 watson ollama[1408032]: internal/poll.(*FD).Read(0xc00050e100, {0xc0002120a1, 0x1, 0x1})\nMar 29 11:58:38 watson ollama[1408032]:         internal/poll/fd_unix.go:165 +0x27a fp=0xc0000616f0 sp=0xc000061658 pc=0x59fdcd1ff5fa\nMar 29 11:58:38 watson ollama[1408032]: net.(*netFD).Read(0xc00050e100, {0xc0002120a1?, 0x59fdcd56d689?, 0xc000061770?})\nMar 29 11:58:38 watson ollama[1408032]:         net/fd_posix.go:55 +0x25 fp=0xc000061738 sp=0xc0000616f0 pc=0x59fdcd274545\nMar 29 11:58:38 watson ollama[1408032]: net.(*conn).Read(0xc000068208, {0xc0002120a1?, 0xc0030471c0?, 0x59fdcd56d640?})\nMar 29 11:58:38 watson ollama[1408032]:         net/net.go:194 +0x45 fp=0xc000061780 sp=0xc000061738 pc=0x59fdcd282905\nMar 29 11:58:38 watson ollama[1408032]: net/http.(*connReader).backgroundRead(0xc000212090)\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:690 +0x37 fp=0xc0000617c8 sp=0xc000061780 pc=0x59fdcd46e657\nMar 29 11:58:38 watson ollama[1408032]: net/http.(*connReader).startBackgroundRead.gowrap2()\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:686 +0x25 fp=0xc0000617e0 sp=0xc0000617c8 pc=0x59fdcd46e585\nMar 29 11:58:38 watson ollama[1408032]: runtime.goexit({})\nMar 29 11:58:38 watson ollama[1408032]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000617e8 sp=0xc0000617e0 pc=0x59fdcd17f3a1\nMar 29 11:58:38 watson ollama[1408032]: created by net/http.(*connReader).startBackgroundRead in goroutine 10\nMar 29 11:58:38 watson ollama[1408032]:         net/http/server.go:686 +0xb6\nMar 29 11:58:38 watson ollama[1408032]: rax    0x7a2d3837e8e0\nMar 29 11:58:38 watson ollama[1408032]: rbx    0x7a2d3837e850\nMar 29 11:58:38 watson ollama[1408032]: rcx    0x3\nMar 29 11:58:38 watson ollama[1408032]: rdx    0x7a2d38521b50\nMar 29 11:58:38 watson ollama[1408032]: rdi    0x0\nMar 29 11:58:38 watson ollama[1408032]: rsi    0x7a2fa0a00030\nMar 29 11:58:38 watson ollama[1408032]: rbp    0x7a2d38521b48\nMar 29 11:58:38 watson ollama[1408032]: rsp    0x7a303b3ffc48\nMar 29 11:58:38 watson ollama[1408032]: r8     0x4\nMar 29 11:58:38 watson ollama[1408032]: r9     0xc000068048\nMar 29 11:58:38 watson ollama[1408032]: r10    0x1\nMar 29 11:58:38 watson ollama[1408032]: r11    0x216\nMar 29 11:58:38 watson ollama[1408032]: r12    0x1\nMar 29 11:58:38 watson ollama[1408032]: r13    0x7a2d38003bc8\nMar 29 11:58:38 watson ollama[1408032]: r14    0xc2f\nMar 29 11:58:38 watson ollama[1408032]: r15    0x7a2d3837e850\nMar 29 11:58:38 watson ollama[1408032]: rip    0x59fdcdf550c0\nMar 29 11:58:38 watson ollama[1408032]: rflags 0x10206\nMar 29 11:58:38 watson ollama[1408032]: cs     0x33\nMar 29 11:58:38 watson ollama[1408032]: fs     0x0\nMar 29 11:58:38 watson ollama[1408032]: gs     0x0\nMar 29 11:58:38 watson ollama[1408032]: [GIN] 2025/03/29 - 11:58:38 | 500 |  8.841319466s |  192.168.50.221 | POST     \"/api/chat\"\nMar 29 11:58:38 watson ollama[1408032]: time=2025-03-29T11:58:38.730-04:00 level=ERROR source=server.go:449 msg=\"llama runner terminated\" error=\"exit status 2\"\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.6.3",
    "comments": [
      {
        "user": "Master-Pr0grammer",
        "body": "This is a similar bug related to https://github.com/ollama/ollama/issues/9699#issuecomment-2763272669, but I felt like my issue is more isolated and that I could provide more detailed info to help resolve the underlying issue"
      },
      {
        "user": "rick-github",
        "body": "```\nMar 29 11:58:36 watson ollama[1408032]: time=2025-03-29T11:58:36.138-04:00 level=INFO source=server.go:138 msg=offload\n library=cuda layers.requested=-1 layers.model=49 layers.offload=49 layers.split=38,11 memory.available=\"[10.6 GiB 3.9 GiB]\"\n memory.gpu_overhead=\"0 B\" memory.required.full=\"14.3 GiB\" memory.required.partial=\"14.3 GiB\" memory.required.kv=\"1.9 GiB\"\n memory.required.allocations=\"[10.5 GiB 3.7 GiB]\" memory.weights.total=\"6.8 GiB\" memory.weights.repeating=\"6.0 GiB\"\n memory.weights.nonrepeating=\"787.5 MiB\" memory.graph.full=\"1.3 GiB\" memory.graph.partial=\"1.3 GiB\"\n projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\n```\nThe context is pre-allocated, but there are temporary allocations during inference.  ollama has estimated that it can use [10.5G, 3.7G] of [10.6G, 3.9G] so there's not a lot of room for these allocations.  Mitigations can be found [here](https://github.com/ollama/ollama/issues/8597#issuecomment-2614533288).  "
      },
      {
        "user": "Master-Pr0grammer",
        "body": "@rick-github Thanks for the feedback, but if you take a look at the nvidia-smi txt file I attached above, once the model is fully loaded, I still have 4.5 GB remaining on device 0 to allocate, which is more than enough. \n\nThe problem is that it is trying to make the allocations on device 1, which only has ~400Mib remaining.\n\n\"ollama has estimated that it can use [10.5G, 3.7G] of [10.6G, 3.9G] so there's not a lot of room for these allocations.\" - this is my point, it can use [10.5G, 3.7G], but its not, and its only using half, and its trying to allocate more."
      }
    ]
  },
  {
    "issue_number": 10952,
    "title": "Cannot run multiple models concurrently",
    "author": "trdischat",
    "state": "closed",
    "created_at": "2025-06-02T21:48:45Z",
    "updated_at": "2025-06-16T19:29:12Z",
    "labels": [
      "bug",
      "needs more info"
    ],
    "body": "### What is the issue?\n\nI am running Ollama on a separate Linux server on my LAN. The server has 96G of RAM and an RTX 3060 with 12G of VRAM.  I am trying to run more than one model concurrently so that I don't have to wait for models to reload constantly. I set the following environment variables in ollama.service:\n```\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\nEnvironment=\"OLLAMA_ORIGINS=*\"\nEnvironment=\"OLLAMA_KEEP_ALIVE=-1\"\nEnvironment=\"OLLAMA_NUM_PARALLEL=4\"\nEnvironment=\"OLLAMA_MAX_LOADED_MODELS=4\"\nEnvironment=\"OLLAMA_DEBUG=1\"\n```\nIt doesn't matter what model I use, nothing runs concurrently, and querying one model always unloads the other model from memory, regardless of model size. I tried this with many different models, including fairly small models (mistral:7b and llama2:7b) that should have fit into VRAM with no problem. FWIW, I set up the exact some software and models on my Windows desktop computer and it can run the models concurrently without a problem. Is there an issue with the Linux version of Ollama or is it something in my Linux setup?\n\n[ollama.log](https://github.com/user-attachments/files/20559277/ollama.log)\n\n\n### Relevant log output\n\nLog file attached.\n\nNot sure if it is relevant, but the Linux logs contain these lines that are not present in the Windows logs:\n\n```\nmsg=\"runner with non-zero duration has gone idle, adding timer\"\nmsg=\"found an idle runner to unload\"\nmsg=\"resetting model to expire immediately to make room\"\nmsg=\"waiting for pending requests to complete and unload to occur\"\nmsg=\"runner expired event received\"\nmsg=\"got lock to unload expired event\"\n```\n\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "Mugl3",
        "body": "Seeing the same here. Windows with v0.90.0"
      },
      {
        "user": "trdischat",
        "body": "I am really confused now. I added a second RTX 3060 to my server, and now I can load and run multiple models concurrently. The two models I am testing with, gemma3:1b and qwen2.5-coder:1.5b are quite small. Each RTX 3060 has 12GB of VRAM. So I went from 12GB VRAM to 24GB VRAM. Concurrent loading should have worked with either one card or two. But adding the second card seems to have made a difference.\n\nReviewing the log files from today and yesterday was interesting:\n\n| Memory             | Yesterday | Today |\n| ------------------ | :-------: | :---: |\n| gemma3:1b          |    3.0    |  2.1  |\n| qwen2.5-coder:1.5b |    8.1    |  4.7  |\n| Available VRAM     |    12     |  24   |\n\nThe amount of memory used by these models changed pretty significantly. Overall, my impression is that you either need to have a massive amount of VRAM or use really small models if you want to run multiple models concurrently."
      },
      {
        "user": "NGC13009",
        "body": "@trdischat \n```text\nEnvironment=\"OLLAMA_NUM_PARALLEL=1\"   # fix to 1\nEnvironment=\"OLLAMA_MAX_LOADED_MODELS=4\"\n```\n\n模型并行数会导致实际kv cache需要的显存达到单个模型的多倍（因为每个并发都需要独立的cache）。比如，假设您默认设置的是2048 num_ctx，并行数为4，则实际上会请求等价于 8192 num_ctx 的显存。对于7b规模的模型（比如qwen之类的），可能需要大概8G显存左右（假设是q4量化）。同时启动两个会超出显卡最大显存容量，导致较旧的模型关闭。\n\n如果并行数为1，则请求是被一个个处理的，就不存在这个问题。\n\nOLLAMA_MAX_LOADED_MODELS 不需要特殊设置，这个代表的是最大同时载入模型数，不是一定要按照这个去分配显存之类的。"
      }
    ]
  },
  {
    "issue_number": 6230,
    "title": "Add Generate Embedding for Sparse vector",
    "author": "shashade2012",
    "state": "open",
    "created_at": "2024-08-07T10:26:27Z",
    "updated_at": "2025-06-16T18:43:53Z",
    "labels": [
      "feature request"
    ],
    "body": "I find ollama already support bge-m3, \r\nbecause bge-m3 can generate sparse vector. Is there any way for generate sparse embeddings?",
    "comments": [
      {
        "user": "dbc-2024",
        "body": "+1"
      },
      {
        "user": "davide445",
        "body": "+1"
      },
      {
        "user": "roylknghv",
        "body": "+1"
      }
    ]
  },
  {
    "issue_number": 11086,
    "title": "https://x.com/iamdisplacement/status/1742039578478072056/analytics",
    "author": "Indagaotass72",
    "state": "closed",
    "created_at": "2025-06-16T11:12:50Z",
    "updated_at": "2025-06-16T11:19:08Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n> https://docs.x.com/success-stories#browse-success-storieshttps://x.com/iamdisplacement/status/1742039578478072056/analytics////\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 10922,
    "title": "`/set nothink` only applies to the first message.",
    "author": "orbitalquark",
    "state": "closed",
    "created_at": "2025-05-31T01:08:54Z",
    "updated_at": "2025-06-16T10:26:16Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nThank you for creating the `/set nothink` option! I've noticed that it only works for the first message though. After that, I get thinking responses. For example:\n\n```\n% ollama run deepseek-r1\n>>> /set nothink\nSet 'nothink' mode.\n>>> hi\nHello! 👋 How can I help you today?\n\n>>> Do you know today's date? If not, just say \"I don't know\"\nOkay, the user is asking if I know today's date. Let me think about this \ncarefully.\n\nHmm, as DeepSeek, my knowledge cutoff is July 2024, so I can't provide \nreal-time information like current dates or news updates beyond that \npoint. \n\nThe user seems to be testing my capabilities - they're checking if I have \naccess to live data or just static knowledge from before July 2024. This \nis a common type of query people ask AI assistants for.\n\nI should respond honestly about my limitations while still being helpful. \nThe system message clearly states that I can't provide real-time \ninformation, so it's important to acknowledge this transparently without \nmaking excuses.\n\nThe user might be frustrated by not getting the current date directly, but \nhonesty is better than pretending. Maybe they're asking because they need \nto know today's date for some specific purpose? Though they didn't specify \nwhat they actually want to do with the date information.\n\nI'll keep my response simple and direct - just stating that I don't have \naccess to real-time data while maintaining my helpful demeanor. No need to \noverexplain unless asked.\n</think>\nI don't know today's date, but as an AI created by **DeepSeek**, I can \nhelp you find out the current date or any historical dates based on \navailable information up to July 2024. You can always ask me for a \nspecific date range if that helps!\n```\n\nI would have expected there to be no thinking at all in the second response, and it probably would just throw a random date at me.\n\n```\n% ollama --version\nollama version is 0.9.0\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "orbitalquark",
        "body": "Looking into this further, it's possible the template for the deepseek-r1:latest model is wrong. If I use this:\n\n```\nTEMPLATE \"\"\"{{- if .System }}{{ .System }}{{ end }}\n{{- range $i, $_ := .Messages }}\n{{- $last := eq (len (slice $.Messages $i)) 1}}\n{{- if eq .Role \"user\" }}<｜User｜>{{ .Content }}\n{{- else if eq .Role \"assistant\" }}<｜Assistant｜>\n  {{- if and $.IsThinkSet $.Think (and $last .Thinking) -}}\n<think>\n{{ .Thinking }}\n</think>\n{{- end }}{{ .Content }}{{- if not $last }}<｜end▁of▁sentence｜>{{- end }}\n{{- end }}\n{{- if and $last (ne .Role \"assistant\") }}<｜Assistant｜>\n{{- if and $.IsThinkSet (not $.Think) -}}\n<think>\n\n</think>\n\n{{ end }}\n{{- end -}}\n{{- end }}\"\"\"\n```\n\nthen it looks like it works.\n\nI changed `  {{- if and $.IsThinkSet (and $last .Thinking) -}}` to `  {{- if and $.IsThinkSet $.Think (and $last .Thinking) -}}` (note the extra `$.Think` operand)."
      },
      {
        "user": "orbitalquark",
        "body": "After testing more, I'm getting mixed results. Sometimes it looks like the model is not thinking, and other times it gives its thinking followed by `</think>` with no opening `<think>`, as shown in the initial description. I don't know what's going on."
      },
      {
        "user": "dengyunsheng250",
        "body": "> 经过更多测试后，我得到了一些结果。有时看起来模型没有思考，有时它却像最初的描述中那样，先思考，然后`</think>`没有开头`<think>`。我不知道发生了什么。\ni guess it may  just remove the think tag"
      }
    ]
  },
  {
    "issue_number": 10835,
    "title": "Large Model Not Splitting Between SXM Cards",
    "author": "sempervictus",
    "state": "closed",
    "created_at": "2025-05-23T15:22:27Z",
    "updated_at": "2025-06-16T10:23:53Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nLoading `command-a` on a 4x32 SXM2 V100 setup somehow fails to recognize the overall available capacity of VRAM and loads everything into host memory. The same Ollama versions load `command-r` fine\n\n### Relevant log output\n\n```shell\ntime=2025-05-23T15:08:22.645Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\ntime=2025-05-23T15:08:22.645Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:45491\"\nllama_model_loader: loaded meta data with 36 key-value pairs and 514 tensors from /root/.ollama/models/blobs/sha256-ffd0081a97182da52ef3c58dcafde851cbd436ce82f71fc5ed9973828bf78a8f (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = cohere2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = A Model\nllama_model_loader: - kv   3:                         general.size_label str              = 111B\nllama_model_loader: - kv   4:                        cohere2.block_count u32              = 64\nllama_model_loader: - kv   5:                     cohere2.context_length u32              = 16384\nllama_model_loader: - kv   6:                   cohere2.embedding_length u32              = 12288\nllama_model_loader: - kv   7:                cohere2.feed_forward_length u32              = 36864\nllama_model_loader: - kv   8:               cohere2.attention.head_count u32              = 96\nllama_model_loader: - kv   9:            cohere2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  10:                     cohere2.rope.freq_base f32              = 50000.000000\nllama_model_loader: - kv  11:       cohere2.attention.layer_norm_epsilon f32              = 0.000010\nllama_model_loader: - kv  12:               cohere2.attention.key_length u32              = 128\nllama_model_loader: - kv  13:             cohere2.attention.value_length u32              = 128\nllama_model_loader: - kv  14:                        cohere2.logit_scale f32              = 0.250000\nllama_model_loader: - kv  15:           cohere2.attention.sliding_window u32              = 4096\nllama_model_loader: - kv  16:                         cohere2.vocab_size u32              = 256000\nllama_model_loader: - kv  17:               cohere2.rope.dimension_count u32              = 128\nllama_model_loader: - kv  18:                  cohere2.rope.scaling.type str              = none\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = command-r\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", ...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...\nllama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,253333]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ a...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 5\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 255001\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 1\nllama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:           tokenizer.chat_template.tool_use str              = {%- macro document_turn(documents) -%...\nllama_model_loader: - kv  31:                tokenizer.chat_template.rag str              = {% set tools = [] %}\\n{%- macro docume...\nllama_model_loader: - kv  32:                   tokenizer.chat_templates arr[str,2]       = [\"tool_use\", \"rag\"]\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% if documents %}\\n{% set tools = [] ...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - kv  35:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  384 tensors\nllama_model_loader: - type q6_K:   65 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 62.51 GiB (4.84 BPW) \ntime=2025-05-23T15:08:22.837Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 41\nload: token to piece cache size = 1.8428 MB\nprint_info: arch             = cohere2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 16384\nprint_info: n_embd           = 12288\nprint_info: n_layer          = 64\nprint_info: n_head           = 96\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 4096\nprint_info: n_swa_pattern    = 4\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 12\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 1.0e-05\nprint_info: f_norm_rms_eps   = 0.0e+00\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 2.5e-01\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 36864\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = none\nprint_info: freq_base_train  = 50000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 16384\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = ?B\nprint_info: model params     = 111.06 B\nprint_info: general.name     = A Model\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 256000\nprint_info: n_merges         = 253333\nprint_info: BOS token        = 5 '<BOS_TOKEN>'\nprint_info: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'\nprint_info: UNK token        = 1 '<UNK>'\nprint_info: PAD token        = 0 '<PAD>'\nprint_info: LF token         = 206 'Ċ'\nprint_info: FIM PAD token    = 0 '<PAD>'\nprint_info: EOG token        = 0 '<PAD>'\nprint_info: EOG token        = 255001 '<|END_OF_TURN_TOKEN|>'\nprint_info: max token length = 1024\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors:          CPU model buffer size = 64014.98 MiB\ntime=2025-05-23T15:09:00.386Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 131072\nllama_context: n_ctx_per_seq = 131072\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 50000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (131072) > n_ctx_train (16384) -- possible training context overflow\nllama_context:        CPU  output buffer size =     1.02 MiB\nllama_kv_cache_unified: kv_size = 131072, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32\ntime=2025-05-23T15:09:15.966Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_kv_cache_unified:        CPU KV buffer size = 32768.00 MiB\nllama_kv_cache_unified: KV self size  = 32768.00 MiB, K (f16): 16384.00 MiB, V (f16): 16384.00 MiB\nllama_context:        CPU compute buffer size = 25184.01 MiB\nllama_context: graph nodes  = 2024\nllama_context: graph splits = 1\ntime=2025-05-23T15:09:26.493Z level=INFO source=server.go:630 msg=\"llama runner started in 63.91 seconds\"\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.7.1-rc2",
    "comments": [
      {
        "user": "rick-github",
        "body": "Need a full log, particularly the memory estimation logic."
      },
      {
        "user": "sempervictus",
        "body": "Roger, all the preceding lines - did this on a clean start:\n```\ntime=2025-05-23T15:07:34.828Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-W library=cuda variant=v12 compute=7.0 driver=12.8 name=\"Tesla V100-SXM2-32GB\" total=\"31.7 GiB\" available=\"31.4 GiB\"\ntime=2025-05-23T15:07:34.828Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-X library=cuda variant=v12 compute=7.0 driver=12.8 name=\"Tesla V100-SXM2-32GB\" total=\"31.7 GiB\" available=\"31.4 GiB\"\ntime=2025-05-23T15:07:34.828Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-Y library=cuda variant=v12 compute=7.0 driver=12.8 name=\"Tesla V100-SXM2-32GB\" total=\"31.7 GiB\" available=\"31.4 GiB\"\ntime=2025-05-23T15:07:34.828Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-Z library=cuda variant=v12 compute=7.0 driver=12.8 name=\"Tesla V100-SXM2-32GB\" total=\"31.7 GiB\" available=\"31.4 GiB\"\n[GIN] 2025/05/23 - 15:08:13 | 200 |     638.537µs |     172.18.0.19 | GET      \"/api/tags\"\n[GIN] 2025/05/23 - 15:08:15 | 200 |      82.343µs |     172.18.0.19 | GET      \"/api/version\"\ntime=2025-05-23T15:08:21.951Z level=INFO source=server.go:135 msg=\"system memory\" total=\"754.6 GiB\" free=\"730.3 GiB\" free_swap=\"0 B\"\ntime=2025-05-23T15:08:21.951Z level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=0 layers.split=\"\" memory.available=\"[31.4 GiB 31.4 GiB 31.4 GiB 31.4 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"94.5 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"32.0 GiB\" memory.required.allocations=\"[0 B 0 B 0 B 0 B]\" memory.weights.total=\"62.5 GiB\" memory.weights.repeating=\"60.1 GiB\" memory.weights.nonrepeating=\"2.4 GiB\" memory.graph.full=\"64.0 GiB\" memory.graph.partial=\"64.0 GiB\"\nllama_model_loader: loaded meta data with 36 key-value pairs and 514 tensors from /root/.ollama/models/blobs/sha256-ffd0081a97182da52ef3c58dcafde851cbd436ce82f71fc5ed9973828bf78a8f (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = cohere2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = A Model\nllama_model_loader: - kv   3:                         general.size_label str              = 111B\nllama_model_loader: - kv   4:                        cohere2.block_count u32              = 64\nllama_model_loader: - kv   5:                     cohere2.context_length u32              = 16384\nllama_model_loader: - kv   6:                   cohere2.embedding_length u32              = 12288\nllama_model_loader: - kv   7:                cohere2.feed_forward_length u32              = 36864\nllama_model_loader: - kv   8:               cohere2.attention.head_count u32              = 96\nllama_model_loader: - kv   9:            cohere2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  10:                     cohere2.rope.freq_base f32              = 50000.000000\nllama_model_loader: - kv  11:       cohere2.attention.layer_norm_epsilon f32              = 0.000010\nllama_model_loader: - kv  12:               cohere2.attention.key_length u32              = 128\nllama_model_loader: - kv  13:             cohere2.attention.value_length u32              = 128\nllama_model_loader: - kv  14:                        cohere2.logit_scale f32              = 0.250000\nllama_model_loader: - kv  15:           cohere2.attention.sliding_window u32              = 4096\nllama_model_loader: - kv  16:                         cohere2.vocab_size u32              = 256000\nllama_model_loader: - kv  17:               cohere2.rope.dimension_count u32              = 128\nllama_model_loader: - kv  18:                  cohere2.rope.scaling.type str              = none\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = command-r\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", ...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...\nllama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,253333]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ a...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 5\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 255001\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 1\nllama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:           tokenizer.chat_template.tool_use str              = {%- macro document_turn(documents) -%...\nllama_model_loader: - kv  31:                tokenizer.chat_template.rag str              = {% set tools = [] %}\\n{%- macro docume...\nllama_model_loader: - kv  32:                   tokenizer.chat_templates arr[str,2]       = [\"tool_use\", \"rag\"]\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% if documents %}\\n{% set tools = [] ...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - kv  35:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  384 tensors\nllama_model_loader: - type q6_K:   65 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 62.51 GiB (4.84 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 41\nload: token to piece cache size = 1.8428 MB\nprint_info: arch             = cohere2\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 111.06 B\nprint_info: general.name     = A Model\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 256000\nprint_info: n_merges         = 253333\nprint_info: BOS token        = 5 '<BOS_TOKEN>'\nprint_info: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'\nprint_info: UNK token        = 1 '<UNK>'\nprint_info: PAD token        = 0 '<PAD>'\nprint_info: LF token         = 206 'Ċ'\nprint_info: FIM PAD token    = 0 '<PAD>'\nprint_info: EOG token        = 0 '<PAD>'\nprint_info: EOG token        = 255001 '<|END_OF_TURN_TOKEN|>'\nprint_info: max token length = 1024\nllama_model_load: vocab only - skipping tensors\ntime=2025-05-23T15:08:22.585Z level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/usr/bin/ollama runner --model /root/.ollama/models/blobs/sha256-ffd0081a97182da52ef3c58dcafde851cbd436ce82f71fc5ed9973828bf78a8f --ctx-size 131072 --batch-size 512 --threads 40 --no-mmap --parallel 1 --multiuser-cache --port 45491\"\ntime=2025-05-23T15:08:22.585Z level=INFO source=sched.go:472 msg=\"loaded runners\" count=1\ntime=2025-05-23T15:08:22.585Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-05-23T15:08:22.586Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-05-23T15:08:22.600Z level=INFO source=runner.go:815 msg=\"starting go runner\"\nload_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-skylakex.so\ntime=2025-05-23T15:08:22.645Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\ntime=2025-05-23T15:08:22.645Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:45491\"\n```"
      },
      {
        "user": "rick-github",
        "body": "```\ntime=2025-05-23T15:08:21.951Z level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=65\n layers.offload=0 layers.split=\"\" memory.available=\"[31.4 GiB 31.4 GiB 31.4 GiB 31.4 GiB]\" memory.gpu_overhead=\"0 B\"\n memory.required.full=\"94.5 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"32.0 GiB\"\n memory.required.allocations=\"[0 B 0 B 0 B 0 B]\" memory.weights.total=\"62.5 GiB\" memory.weights.repeating=\"60.1 GiB\"\n memory.weights.nonrepeating=\"2.4 GiB\" memory.graph.full=\"64.0 GiB\" memory.graph.partial=\"64.0 GiB\"\n```\nContext size of 131072 tokens has increased the size of the memory graph (64G) to where it will not fit on a device (31.4G).  As a result, the model is run in system RAM."
      }
    ]
  },
  {
    "issue_number": 10916,
    "title": "Cannot get Ollama to utilize two 3090s for processing small/large models?",
    "author": "wingraver",
    "state": "closed",
    "created_at": "2025-05-30T15:14:35Z",
    "updated_at": "2025-06-16T10:21:37Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nHaving tried small (8B) and large (70B) models alike, thinking &/or vision models, and currently updated to version 0.9.0, I have not been able to ever get Ollama to fully utilize the **two 3090s** on my machine. Even when \"ollama ps\" says it's using GPU 100%, task manager and process explorer (and the fans on my PC) suggest otherwise.\n\nSometimes there's a brief spike in usage that quickly falls away and most all of the activity is seen on the CPU.\n\nMaybe it's me... Is there some specific steps that need to be followed to ensure maximum utilization of the GPUs? Do I need to buy an NVLink, apply some special global configuration... feels like a waste. Thanks in advance for any help in getting this right...\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) may aid in debugging."
      },
      {
        "user": "wingraver",
        "body": "[GIN] 2025/05/30 - 10:55:39 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/30 - 10:55:39 | 200 |      7.0382ms |       127.0.0.1 | GET      \"/api/tags\"\n[GIN] 2025/05/30 - 10:56:02 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/30 - 10:56:02 | 200 |     47.2215ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-05-30T10:56:02.910-05:00 level=INFO source=sched.go:804 msg=\"new model will fit in available VRAM, loading\" model=C:\\Users\\\\.ollama\\models\\blobs\\sha256-de447d788da3df6b4ea340408b13fc2c3a2043a2dfc19178b12d501a4bd96484 library=cuda parallel=2 required=\"41.2 GiB\"\ntime=2025-05-30T10:56:02.943-05:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"95.8 GiB\" free=\"72.6 GiB\" free_swap=\"71.7 GiB\"\ntime=2025-05-30T10:56:02.944-05:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split=33,32 memory.available=\"[22.8 GiB 22.7 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"41.2 GiB\" memory.required.partial=\"41.2 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[21.0 GiB 20.2 GiB]\" memory.weights.total=\"31.9 GiB\" memory.weights.repeating=\"31.2 GiB\" memory.weights.nonrepeating=\"788.3 MiB\" memory.graph.full=\"2.7 GiB\" memory.graph.partial=\"2.7 GiB\"\nllama_model_loader: loaded meta data with 27 key-value pairs and 707 tensors from C:\\Users\\\\.ollama\\models\\blobs\\sha256-de447d788da3df6b4ea340408b13fc2c3a2043a2dfc19178b12d501a4bd96484 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 32B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 32B\nllama_model_loader: - kv   5:                          qwen3.block_count u32              = 64\nllama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120\nllama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 25600\nllama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 64\nllama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 7\nllama_model_loader: - type  f32:  257 tensors\nllama_model_loader: - type  f16:   64 tensors\nllama_model_loader: - type q8_0:  386 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0\nprint_info: file size   = 32.71 GiB (8.58 BPW) \nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 32.76 B\nprint_info: general.name     = Qwen3 32B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-05-30T10:56:03.111-05:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model C:\\\\Users\\\\\\\\.ollama\\\\models\\\\blobs\\\\sha256-de447d788da3df6b4ea340408b13fc2c3a2043a2dfc19178b12d501a4bd96484 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 32 --no-mmap --parallel 2 --tensor-split 33,32 --port 64789\"\ntime=2025-05-30T10:56:03.116-05:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-05-30T10:56:03.116-05:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-05-30T10:56:03.116-05:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-05-30T10:56:03.181-05:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\nload_backend: loaded CPU backend from C:\\Users\\\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-haswell.dll\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\nload_backend: loaded CUDA backend from C:\\Users\\\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\\ggml-cuda.dll\ntime=2025-05-30T10:56:03.556-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 CUDA.1.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.1.USE_GRAPHS=1 CUDA.1.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)\ntime=2025-05-30T10:56:03.558-05:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:64789\"\ntime=2025-05-30T10:56:03.618-05:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23306 MiB free\nllama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 3090) - 23306 MiB free\nllama_model_loader: loaded meta data with 27 key-value pairs and 707 tensors from C:\\Users\\\\.ollama\\models\\blobs\\sha256-de447d788da3df6b4ea340408b13fc2c3a2043a2dfc19178b12d501a4bd96484 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 32B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 32B\nllama_model_loader: - kv   5:                          qwen3.block_count u32              = 64\nllama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120\nllama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 25600\nllama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 64\nllama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 7\nllama_model_loader: - type  f32:  257 tensors\nllama_model_loader: - type  f16:   64 tensors\nllama_model_loader: - type q8_0:  386 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0\nprint_info: file size   = 32.71 GiB (8.58 BPW) \nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 64\nprint_info: n_head           = 64\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 25600\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 32B\nprint_info: model params     = 32.76 B\nprint_info: general.name     = Qwen3 32B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 64 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 65/65 layers to GPU\nload_tensors:    CUDA_Host model buffer size =   788.24 MiB\nload_tensors:        CUDA0 model buffer size = 16460.07 MiB\nload_tensors:        CUDA1 model buffer size = 16250.75 MiB\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 2\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 1024\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     1.20 MiB\nllama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32\nllama_kv_cache_unified:      CUDA0 KV buffer size =  1056.00 MiB\nllama_kv_cache_unified:      CUDA1 KV buffer size =   992.00 MiB\nllama_kv_cache_unified: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nllama_context: pipeline parallelism enabled (n_copies=4)\nllama_context:      CUDA0 compute buffer size =  1180.01 MiB\nllama_context:      CUDA1 compute buffer size =  1180.02 MiB\nllama_context:  CUDA_Host compute buffer size =    74.02 MiB\nllama_context: graph nodes  = 2438\nllama_context: graph splits = 3\ntime=2025-05-30T10:56:16.401-05:00 level=INFO source=server.go:630 msg=\"llama runner started in 13.28 seconds\"\n[GIN] 2025/05/30 - 10:56:16 | 200 |   13.5785235s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2025/05/30 - 10:57:57 | 200 |          1m2s |       127.0.0.1 | POST     \"/api/chat\"\n\n@rick-github this it?"
      },
      {
        "user": "rick-github",
        "body": "```\ntime=2025-05-30T10:56:02.944-05:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1\n layers.model=65 layers.offload=65 layers.split=33,32 memory.available=\"[22.8 GiB 22.7 GiB]\" memory.gpu_overhead=\"0 B\"\n memory.required.full=\"41.2 GiB\" memory.required.partial=\"41.2 GiB\" memory.required.kv=\"2.0 GiB\"\n memory.required.allocations=\"[21.0 GiB 20.2 GiB]\" memory.weights.total=\"31.9 GiB\" memory.weights.repeating=\"31.2 GiB\"\n memory.weights.nonrepeating=\"788.3 MiB\" memory.graph.full=\"2.7 GiB\" memory.graph.partial=\"2.7 GiB\"\n```\nollama has estimated using 41.G of 45.5G to load the model in the GPU, so has calculated that it can offload all 65 of the models 65 layers.\n```\nload_tensors: offloading 64 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 65/65 layers to GPU\nload_tensors: CUDA0 model buffer size = 16460.07 MiB\nload_tensors: CUDA1 model buffer size = 16250.75 MiB\n```\nThe runner has loaded all layers into the GPU.  All indications are that the model is running 100% on the GPU.\n\n>  Even when \"ollama ps\" says it's using GPU 100%, task manager and process explorer (and the fans on my PC) suggest otherwise.\n\nNote that while the GPUs are doing inference, the CPU is the controller - it is busy sending instructions to the GPUs on what kernels to launch, moving intermediate results between the GPUs, etc.  So the CPU will still be busy even when the GPUs are the only devices processing the model weights."
      }
    ]
  },
  {
    "issue_number": 11085,
    "title": "Auto-unload model or release GPU memory when idle",
    "author": "barthwalsaurabh0",
    "state": "open",
    "created_at": "2025-06-16T10:06:40Z",
    "updated_at": "2025-06-16T10:10:12Z",
    "labels": [
      "feature request"
    ],
    "body": "\n**Is your feature request related to a problem? Please describe.**\nYes. When using `ollama serve`, models remain loaded in GPU memory even when not actively handling requests. This leads to unnecessary GPU usage, which can be problematic on shared systems or when switching between different workloads.\n\n---\n\n**Describe the solution you'd like**\nIntroduce a feature that automatically unloads a model or releases GPU memory after a configurable period of inactivity. This could include:\n\n* Monitoring for idle time (no incoming requests or inactivity on the model)\n* Unloading or stopping the model automatically after a timeout\n* Optionally reloading the model automatically when a new request arrives\n\nThis would function similarly to cache eviction or auto-scaling mechanisms.\n\n---\n\n**Describe alternatives you’ve considered**\n\n* Manually running `ollama stop <model>` to free GPU memory\n* Using `ollama run` instead of `serve`, but that doesn't support persistent API access\n* Writing a custom idle-time monitoring script to stop models automatically\n\n---\n\n**Additional context**\nThis feature would be very useful in environments with limited GPU resources, such as shared development machines, personal laptops, or multi-tenant servers.\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately"
      }
    ]
  },
  {
    "issue_number": 11061,
    "title": "the first query inferenceUSE GPU , Then the model switch to CPU",
    "author": "suchenRoad",
    "state": "open",
    "created_at": "2025-06-13T06:35:04Z",
    "updated_at": "2025-06-16T07:03:02Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nmodel:Qwen3:32b\ncuda:12.8\nGPU:L20*2\n\nthe first query inference\n![Image](https://github.com/user-attachments/assets/20e8ac8a-547f-4a57-9053-ddfda071eb23)\n\nthe second query inference\n![Image](https://github.com/user-attachments/assets/64687818-bbbd-4fa1-a7ad-023468771b0d)\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.1-rc0",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will aid in debugging."
      },
      {
        "user": "suchenRoad",
        "body": "testing use terminal is normal， logs as follow：\n\n![Image](https://github.com/user-attachments/assets/4aa5f843-6155-466c-99fe-6e1c44453087)\n\n\ntesting use ollama API, switch to CPU， logs as follow：\n\n![Image](https://github.com/user-attachments/assets/84aeac23-ccc8-45cb-85a6-9adc93e005eb)\n\n![Image](https://github.com/user-attachments/assets/c17358fc-f6c8-4953-bb0e-298dcf61920f)\n\n![Image](https://github.com/user-attachments/assets/e9265acc-37b6-407a-b682-8a2467c25e75)\n\n\n![Image](https://github.com/user-attachments/assets/2e742278-4252-46d4-bb25-bbc829edd5fb)\n\n![Image](https://github.com/user-attachments/assets/757f17ae-5feb-482e-95c7-a4c1bfce6d3d)\n"
      },
      {
        "user": "rick-github",
        "body": "Text logs."
      }
    ]
  },
  {
    "issue_number": 11082,
    "title": "how to set OLLAMA_NUM_PARALLEL in Ollama docker container ?",
    "author": "doyoungim999",
    "state": "closed",
    "created_at": "2025-06-16T00:01:01Z",
    "updated_at": "2025-06-16T00:46:36Z",
    "labels": [
      "question"
    ],
    "body": "### What is the issue?\n\nI want to limit CPU usage in ollama container. \nHow to add \"OLLAMA_NUM_PARALLEL=4\" in ollama docker container? \n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "`OLLAMA_NUM_PARALLEL` doesn't really have anything to do with limiting CPU usage, but this is how to set it in docker compose:\n\n```yaml\nservices:\n  ollama:\n    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}\n    environment:\n      - OLLAMA_NUM_PARALLEL=4\n    ports:\n      - 11434:11434\n```\nOr via docker run:\n```\ndocker run --rm --env OLLAMA_NUM_PARALLEL=4 -p 11434:11434 -d --name ollama ollama/ollama\n```"
      },
      {
        "user": "doyoungim999",
        "body": "I am deploying on OpenShift  with the following :\nspec:\n  containers: \n     - name: ollama \n       image:  ollama:latest \n       env:\n          - name: OLLAMA_NUM_PARALLEL\n            value: \"4\" "
      }
    ]
  },
  {
    "issue_number": 3368,
    "title": "Reranking models",
    "author": "YuanfengZhang",
    "state": "open",
    "created_at": "2024-03-27T07:41:15Z",
    "updated_at": "2025-06-16T00:27:25Z",
    "labels": [],
    "body": "### What model would you like?\r\n\r\nTill now, ollama supports LLM and embedding models. I wonder if it could support popular reranking models later?\r\nSuch as:\r\n1. [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)\r\n2. [mixedbread-ai/mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)\r\n3. [amberoad/bert-multilingual-passage-reranking-msmarco](https://huggingface.co/amberoad/bert-multilingual-passage-reranking-msmarco)\r\n\r\nThx.",
    "comments": [
      {
        "user": "BradKML",
        "body": "Seconding this since there are apps that require them"
      },
      {
        "user": "kuschzzp",
        "body": "Is the reranker model planned?"
      },
      {
        "user": "CHEFRA",
        "body": "netease-youdao/bce-reranker-base_v1\r\nBAAI/bge-reranker-v2-m3\r\nthx"
      }
    ]
  },
  {
    "issue_number": 9503,
    "title": "NVIDIA GPU drivers not loaded on Jeston Orin Nano",
    "author": "virtualJonesie",
    "state": "open",
    "created_at": "2025-03-04T21:23:16Z",
    "updated_at": "2025-06-15T23:04:13Z",
    "labels": [
      "bug",
      "linux",
      "nvidia"
    ],
    "body": "### What is the issue?\n\nOllama does not load GPU drivers on the NVIDIA Jetson Orin Nano, going back to 0.5.8. \n\nThe last version to load these drivers successfully was 0.5.7.\n\nI am running the latest version of the NVIDIA Jetpack 6.2 from NVIDIA on a newly flashed system. All updates have been applied via apt.\n\n\n\n### Relevant log output\n\n```shell\nollama      | time=2025-03-04T20:48:09.206Z level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=\"[/usr/lib/ollama/cuda_v11/libcudart.so.11.3.109 /usr/lib/ollama/cuda_v12/libcudart.so.12.4.127]\"\nollama      | cudaSetDevice err: 35\nollama      | time=2025-03-04T20:48:09.209Z level=DEBUG source=gpu.go:574 msg=\"Unable to load cudart library /usr/lib/ollama/cuda_v11/libcudart.so.11.3.109: your nvidia driver is too old or missing.  If you have a CUDA GPU please upgrade to run ollama\"\nollama      | cudaSetDevice err: 35\nollama      | time=2025-03-04T20:48:09.210Z level=DEBUG source=gpu.go:574 msg=\"Unable to load cudart library /usr/lib/ollama/cuda_v12/libcudart.so.12.4.127: your nvidia driver is too old or missing.  If you have a CUDA GPU please upgrade to run ollama\"\nollama      | time=2025-03-04T20:48:09.210Z level=DEBUG source=amd_linux.go:419 msg=\"amdgpu driver not detected /sys/module/amdgpu\"\nollama      | time=2025-03-04T20:48:09.210Z level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\nollama      | time=2025-03-04T20:48:09.211Z level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=cpu variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"7.4 GiB\" available=\"6.5 GiB\"\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nOther\n\n### Ollama version\n\n0.5.12",
    "comments": [
      {
        "user": "dhiltgen",
        "body": "I just tried on a AGX Orin with Jetpack 6 and it was able to run on the iGPU.\n\n```\ntime=2025-03-04T17:15:10.605-08:00 level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[/tmp/daniel_ollama_test/lib/ollama/libcuda.so* /tmp/daniel_ollama_test/bin/libcuda.so* /usr/local/cuda*/targets/*/lib/libcuda.so* /usr/lib/*-linux-gnu/nvidia/current/libcuda.so* /usr/lib/*-linux-gnu/libcuda.so* /usr/lib/wsl/lib/libcuda.so* /usr/lib/wsl/drivers/*/libcuda.so* /opt/cuda/lib*/libcuda.so* /usr/local/cuda/lib*/libcuda.so* /usr/lib*/libcuda.so* /usr/local/lib*/libcuda.so*]\"\ntime=2025-03-04T17:15:10.614-08:00 level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=[/usr/lib/aarch64-linux-gnu/nvidia/libcuda.so.1.1]\ninitializing /usr/lib/aarch64-linux-gnu/nvidia/libcuda.so.1.1\ndlsym: cuInit - 0xffff30366540\ndlsym: cuDriverGetVersion - 0xffff30366570\ndlsym: cuDeviceGetCount - 0xffff303665d0\ndlsym: cuDeviceGet - 0xffff303665a0\ndlsym: cuDeviceGetAttribute - 0xffff30366720\ndlsym: cuDeviceGetUuid - 0xffff30366630\ndlsym: cuDeviceGetName - 0xffff30366600\ndlsym: cuCtxCreate_v3 - 0xffff303669f0\ndlsym: cuMemGetInfo_v2 - 0xffff3036f0d0\ndlsym: cuCtxDestroy - 0xffff303aa9c0\ncalling cuInit\ncalling cuDriverGetVersion\nraw version 0x2ef4\nCUDA driver version: 12.2\ncalling cuDeviceGetCount\ndevice count 1\ntime=2025-03-04T17:15:10.746-08:00 level=DEBUG source=gpu.go:125 msg=\"detected GPUs\" count=1 library=/usr/lib/aarch64-linux-gnu/nvidia/libcuda.so.1.1\n[GPU-67834ba8-0312-50b2-9286-9b3b02e80059] CUDA totalMem 62841 mb\n[GPU-67834ba8-0312-50b2-9286-9b3b02e80059] CUDA freeMem 56142 mb\n[GPU-67834ba8-0312-50b2-9286-9b3b02e80059] Compute Capability 8.7\ntime=2025-03-04T17:15:10.874-08:00 level=DEBUG source=amd_linux.go:419 msg=\"amdgpu driver not detected /sys/module/amdgpu\"\nreleasing cuda driver library\ntime=2025-03-04T17:15:10.874-08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-67834ba8-0312-50b2-9286-9b3b02e80059 library=cuda variant=jetpack6 compute=8.7 driver=12.2 name=Orin total=\"61.4 GiB\" available=\"54.8 GiB\"\n```\n\nIs it possible you somehow uninstalled the base CUDA driver packages on your system? \n\nOn my system I see it installed via:\n```\n# dpkg -S /usr/lib/aarch64-linux-gnu/nvidia/libcuda.so.1.1\nnvidia-l4t-cuda: /usr/lib/aarch64-linux-gnu/nvidia/libcuda.so.1.1\n```"
      },
      {
        "user": "virtualJonesie",
        "body": "It appears to be there. I installed via SDKManager. \n```\ndpkg -S /usr/lib/aarch64-linux-gnu/nvidia/libcuda.so.1.1\nnvidia-l4t-cuda: /usr/lib/aarch64-linux-gnu/nvidia/libcuda.so.1.1\n```\n\nIt is only Ollama older than 0.5.8. I ran every version, starting at 0.5.12 moving backwards until it worked. Finally had success at 0.5.7."
      },
      {
        "user": "dhiltgen",
        "body": "Can you share a more complete server log from startup with `OLLAMA_DEBUG=1` set?"
      }
    ]
  },
  {
    "issue_number": 11081,
    "title": "XiaomiMiMo support",
    "author": "Alias4D",
    "state": "open",
    "created_at": "2025-06-15T17:42:38Z",
    "updated_at": "2025-06-15T17:47:02Z",
    "labels": [
      "model request"
    ],
    "body": "XiaomiMiMo/MiMo-VL-7B-RL-GGUF\nXiaomiMiMo/MiMo-VL-7B-SFT-GGUF\n\nits download successfully \nrun text chat ok but no image support , no vision support",
    "comments": []
  },
  {
    "issue_number": 10758,
    "title": "serving model(llama4:scout) fails on gfx1151",
    "author": "pnjacket",
    "state": "open",
    "created_at": "2025-05-17T21:34:32Z",
    "updated_at": "2025-06-15T11:06:17Z",
    "labels": [
      "bug",
      "amd"
    ],
    "body": "### What is the issue?\n\n[output.log](https://github.com/user-attachments/files/20266627/output.log)\n\nI am trying to load llama4:scout using the 0.7.0 and getting an exception at the moment. you can see the full log on the attached file.\nThe system is AMD AI MAX 395+(Strix halo) with the enough ram assigned to igpu. \n![Image](https://github.com/user-attachments/assets/18382c5b-4e44-43aa-be1a-723683e35cb4)\n\n\nI am not too sure how to interpret the error message I am getting from the log. Any help would be appreciated.\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nWindows\n\n### GPU\n\nAMD\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.7.0\n\nUpdate: Unfortunately, I have ended up returning the unit because something is clearly broken on the hardware or BIOS side. Aside from random crashes, I am losing the display way too often making it unusable. I am trying to source one from another brand. The choices are very limited at this point, and I won't be able to verify the fix even if one is available until I get my replacement one day.",
    "comments": [
      {
        "user": "rick-github",
        "body": "```\ntime=2025-05-17T17:18:03.638-04:00 level=DEBUG source=server.go:639 msg=\"model load completed, waiting for server to become available\" status=\"llm server loading model\"\nException 0xc0000005 0x1 0x10 0x7ffc02e49176\nPC=0x7ffc02e49176\nsignal arrived during external code execution\n\nruntime.cgocall(0x7ff6cc2b0c40, 0xc0018c1b30)\n\tC:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/cgocall.go:167 +0x3e fp=0xc0018c1b08 sp=0xc0018c1aa0 pc=0x7ff6cb5f241e\ngithub.com/ollama/ollama/ml/backend/ggml._Cfunc_ggml_backend_sched_reserve(0x1f52e37d890, 0x20536f56040)\n```\nThe runner tried to write to an invalid memory location (0x10) and got killed with an access violation.  The error occurred in ggml_backend_sched_reserve() when the runner was trying to reserve the worst case graph.  Possibly some pointer was zero and the code tried to reference the pointer with a 16 byte offset and got killed.  Does the problem repeat with ollama version 0.6.8?"
      },
      {
        "user": "pnjacket",
        "body": "Thank you for the response. I quickly tried 0.6.8 and ended up with the same result.\nI made a comment to report that then it suddenly hit me that I may want to try 0.6.7 to see if that is to make sure the first version with llama4 support, and then 0.6.7 loaded just fine.\nHowever, the first query I sent to the 0.6.7 failed with basically the same error that was happening in the later versions.\n\n[output067.log](https://github.com/user-attachments/files/20269799/output067.log)\n\nI tried one with smaller model gemma3:4b and it works perfectly fine. I am going to download another sizeable model to see if they work fine and report back soon. If you have anything I want to try, please let me know.\n\nUpdate: I tried deepseek-r1:70b and qwen:110b, and both ran fine meaning it is probably not the size of the model that is causing it to go bust. qwen2.5vl:72b did not run with out of memory error while the system obviously has enough memory at least on GPU side. qwen2.5vl:72b is probably too early to be supported correctly for many platform???\nAnyways, none of these provide me why llama4:scout is failing, but just added here only as an additional information."
      },
      {
        "user": "pnjacket",
        "body": "I made it run by setting an environment variable as following\n```powershell\n# in powershell\n$env:OLLAMA_NUM_PARALLEL=1\n```\nAfter a careful examination of the log, I found this setting is set to 2 by default. With some digging of other issues in github, I found that this settings seems to affect the memory usage. I gave the new value a shot and that fixed the issue for me. I am not sure what was the basis of setting the default for this value to 2. If anyone can explain, that will be greatly appreciated.\n\nAt this point, I think the issue is nothing to do with the specific GPU, rather, it is just lack of available resources.I will close this issue soon if someone can confirm that."
      }
    ]
  },
  {
    "issue_number": 11079,
    "title": "\"unexpected end of JSON input\" when providing tools for `phi4-mini`",
    "author": "icefairy64",
    "state": "closed",
    "created_at": "2025-06-15T07:11:27Z",
    "updated_at": "2025-06-15T07:28:22Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nWhen performing a simple chat request with tools for `phi4-mini` specifically I get an error response: `{\"error\":\"unexpected end of JSON input\"}`.\n\nAn example of a request triggering this:\n\n```bash\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"phi4-mini\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the weather today in Paris?\"\n    }\n  ],\n  \"stream\": false,\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather for a location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The location to get the weather for, e.g. San Francisco, CA\"\n            },\n            \"format\": {\n              \"type\": \"string\",\n              \"description\": \"The format to return the weather in, e.g. 'celsius' or 'fahrenheit'\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n          },\n          \"required\": [\"location\", \"format\"]\n        }\n      }\n    }\n  ]\n}'\n```\n\nTool calling works fine with Qwen3.\n\n### Relevant log output\n\n```shell\ntime=2025-06-15T07:09:02.948Z level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:DEBUG OLLAMA_FLASH_ATTENTION:true OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE:q4_0 OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-06-15T07:09:02.950Z level=INFO source=images.go:479 msg=\"total blobs: 45\"\ntime=2025-06-15T07:09:02.950Z level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-15T07:09:02.950Z level=INFO source=routes.go:1287 msg=\"Listening on [::]:11434 (version 0.9.0)\"\ntime=2025-06-15T07:09:02.950Z level=DEBUG source=sched.go:108 msg=\"starting llm scheduler\"\ntime=2025-06-15T07:09:02.950Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-15T07:09:02.950Z level=DEBUG source=gpu.go:98 msg=\"searching for GPU discovery libraries for NVIDIA\"\ntime=2025-06-15T07:09:02.950Z level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=libcuda.so*\ntime=2025-06-15T07:09:02.950Z level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[/usr/lib/ollama/libcuda.so* /usr/local/nvidia/lib/libcuda.so* /usr/local/nvidia/lib64/libcuda.so* /usr/local/cuda*/targets/*/lib/libcuda.so* /usr/lib/*-linux-gnu/nvidia/current/libcuda.so* /usr/lib/*-linux-gnu/libcuda.so* /usr/lib/wsl/lib/libcuda.so* /usr/lib/wsl/drivers/*/libcuda.so* /opt/cuda/lib*/libcuda.so* /usr/local/cuda/lib*/libcuda.so* /usr/lib*/libcuda.so* /usr/local/lib*/libcuda.so*]\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=[]\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=libcudart.so*\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[/usr/lib/ollama/libcudart.so* /usr/local/nvidia/lib/libcudart.so* /usr/local/nvidia/lib64/libcudart.so* /usr/lib/ollama/cuda_v*/libcudart.so* /usr/local/cuda/lib64/libcudart.so* /usr/lib/x86_64-linux-gnu/nvidia/current/libcudart.so* /usr/lib/x86_64-linux-gnu/libcudart.so* /usr/lib/wsl/lib/libcudart.so* /usr/lib/wsl/drivers/*/libcudart.so* /opt/cuda/lib64/libcudart.so* /usr/local/cuda*/targets/aarch64-linux/lib/libcudart.so* /usr/lib/aarch64-linux-gnu/nvidia/current/libcudart.so* /usr/lib/aarch64-linux-gnu/libcudart.so* /usr/local/cuda/lib*/libcudart.so* /usr/lib*/libcudart.so* /usr/local/lib*/libcudart.so*]\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=[]\ntime=2025-06-15T07:09:02.951Z level=WARN source=amd_linux.go:61 msg=\"ollama recommends running the https://www.amd.com/en/support/linux-drivers\" error=\"amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_linux.go:101 msg=\"evaluating amdgpu node /sys/class/kfd/kfd/topology/nodes/0/properties\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_linux.go:121 msg=\"detected CPU /sys/class/kfd/kfd/topology/nodes/0/properties\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_linux.go:101 msg=\"evaluating amdgpu node /sys/class/kfd/kfd/topology/nodes/1/properties\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_linux.go:206 msg=\"mapping amdgpu to drm sysfs nodes\" amdgpu=/sys/class/kfd/kfd/topology/nodes/1/properties vendor=4098 device=29772 unique_id=3545464219138192957\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_linux.go:240 msg=matched amdgpu=/sys/class/kfd/kfd/topology/nodes/1/properties drm=/sys/class/drm/card0/device\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_linux.go:318 msg=\"amdgpu memory\" gpu=0 total=\"20.0 GiB\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_linux.go:319 msg=\"amdgpu memory\" gpu=0 available=\"18.0 GiB\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_common.go:16 msg=\"evaluating potential rocm lib dir /usr/lib/ollama/rocm\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_common.go:44 msg=\"detected ROCM next to ollama executable /usr/lib/ollama/rocm\"\ntime=2025-06-15T07:09:02.951Z level=DEBUG source=amd_linux.go:371 msg=\"rocm supported GPUs\" types=\"[gfx1010 gfx1012 gfx1030 gfx1100 gfx1101 gfx1102 gfx1151 gfx1200 gfx1201 gfx900 gfx906 gfx908 gfx90a gfx942]\"\ntime=2025-06-15T07:09:02.951Z level=INFO source=amd_linux.go:386 msg=\"amdgpu is supported\" gpu=GPU-313404ec3196f63d gpu_type=gfx1100\ntime=2025-06-15T07:09:02.951Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-313404ec3196f63d library=rocm variant=\"\" compute=gfx1100 driver=0.0 name=1002:744c total=\"20.0 GiB\" available=\"18.0 GiB\"\ntime=2025-06-15T07:09:05.478Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-15T07:09:05.478Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"94.3 GiB\" before.free=\"86.5 GiB\" before.free_swap=\"8.0 GiB\" now.total=\"94.3 GiB\" now.free=\"86.5 GiB\" now.free_swap=\"8.0 GiB\"\ntime=2025-06-15T07:09:05.478Z level=DEBUG source=amd_linux.go:488 msg=\"updating rocm free memory\" gpu=GPU-313404ec3196f63d name=1002:744c before=\"18.0 GiB\" now=\"18.0 GiB\"\ntime=2025-06-15T07:09:05.478Z level=DEBUG source=sched.go:185 msg=\"updating default concurrency\" OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1\ntime=2025-06-15T07:09:05.488Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=sched.go:228 msg=\"loading first model\" model=/root/.ollama/models/blobs/sha256-3c168af1dea0a414299c7d9077e100ac763370e5a98b3c53801a958a47f0a5db\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=memory.go:111 msg=evaluating library=rocm gpu_count=1 available=\"[18.0 GiB]\"\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.vision.block_count default=0\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"94.3 GiB\" before.free=\"86.5 GiB\" before.free_swap=\"8.0 GiB\" now.total=\"94.3 GiB\" now.free=\"86.5 GiB\" now.free_swap=\"8.0 GiB\"\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=amd_linux.go:488 msg=\"updating rocm free memory\" gpu=GPU-313404ec3196f63d name=1002:744c before=\"18.0 GiB\" now=\"18.0 GiB\"\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.key_length default=128\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.value_length default=128\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.key_length default=128\ntime=2025-06-15T07:09:05.498Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.value_length default=128\ntime=2025-06-15T07:09:05.498Z level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-3c168af1dea0a414299c7d9077e100ac763370e5a98b3c53801a958a47f0a5db gpu=GPU-313404ec3196f63d parallel=2 available=19373862912 required=\"3.2 GiB\"\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"94.3 GiB\" before.free=\"86.5 GiB\" before.free_swap=\"8.0 GiB\" now.total=\"94.3 GiB\" now.free=\"86.5 GiB\" now.free_swap=\"8.0 GiB\"\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=amd_linux.go:488 msg=\"updating rocm free memory\" gpu=GPU-313404ec3196f63d name=1002:744c before=\"18.0 GiB\" now=\"18.0 GiB\"\ntime=2025-06-15T07:09:05.499Z level=INFO source=server.go:135 msg=\"system memory\" total=\"94.3 GiB\" free=\"86.5 GiB\" free_swap=\"8.0 GiB\"\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=memory.go:111 msg=evaluating library=rocm gpu_count=1 available=\"[18.0 GiB]\"\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.vision.block_count default=0\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"94.3 GiB\" before.free=\"86.5 GiB\" before.free_swap=\"8.0 GiB\" now.total=\"94.3 GiB\" now.free=\"86.5 GiB\" now.free_swap=\"8.0 GiB\"\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=amd_linux.go:488 msg=\"updating rocm free memory\" gpu=GPU-313404ec3196f63d name=1002:744c before=\"18.0 GiB\" now=\"18.0 GiB\"\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.key_length default=128\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.value_length default=128\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.key_length default=128\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.value_length default=128\ntime=2025-06-15T07:09:05.499Z level=INFO source=server.go:168 msg=offload library=rocm layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[18.0 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"3.2 GiB\" memory.required.partial=\"3.2 GiB\" memory.required.kv=\"256.0 MiB\" memory.required.allocations=\"[3.2 GiB]\" memory.weights.total=\"2.3 GiB\" memory.weights.repeating=\"1.8 GiB\" memory.weights.nonrepeating=\"480.8 MiB\" memory.graph.full=\"128.0 MiB\" memory.graph.partial=\"128.0 MiB\"\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.key_length default=128\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=phi3.attention.value_length default=128\ntime=2025-06-15T07:09:05.499Z level=INFO source=server.go:211 msg=\"enabling flash attention\"\ntime=2025-06-15T07:09:05.499Z level=DEBUG source=server.go:284 msg=\"compatible gpu libraries\" compatible=[rocm]\nllama_model_loader: loaded meta data with 36 key-value pairs and 196 tensors from /root/.ollama/models/blobs/sha256-3c168af1dea0a414299c7d9077e100ac763370e5a98b3c53801a958a47f0a5db (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\nllama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\nllama_model_loader: - kv   2:                               general.type str              = model\nllama_model_loader: - kv   3:                               general.name str              = Phi 4 Mini Instruct\nllama_model_loader: - kv   4:                           general.finetune str              = instruct\nllama_model_loader: - kv   5:                           general.basename str              = Phi-4\nllama_model_loader: - kv   6:                         general.size_label str              = mini\nllama_model_loader: - kv   7:                            general.license str              = mit\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\nllama_model_loader: - kv   9:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\nllama_model_loader: - kv  10:                          general.languages arr[str,24]      = [\"multilingual\", \"ar\", \"zh\", \"cs\", \"d...\nllama_model_loader: - kv  11:                        phi3.context_length u32              = 131072\nllama_model_loader: - kv  12:  phi3.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  13:                      phi3.embedding_length u32              = 3072\nllama_model_loader: - kv  14:                   phi3.feed_forward_length u32              = 8192\nllama_model_loader: - kv  15:                           phi3.block_count u32              = 32\nllama_model_loader: - kv  16:                  phi3.attention.head_count u32              = 24\nllama_model_loader: - kv  17:               phi3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  18:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  19:                  phi3.rope.dimension_count u32              = 96\nllama_model_loader: - kv  20:                        phi3.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = gpt-4o\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,199742]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"e r\", ...\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 199999\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 199999\nllama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 199999\nllama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 199999\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - kv  35:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:   67 tensors\nllama_model_loader: - type q4_K:   80 tensors\nllama_model_loader: - type q5_K:   32 tensors\nllama_model_loader: - type q6_K:   17 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.31 GiB (5.18 BPW) \ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 200024 '<|/tool|>' is not marked as EOG\nload: control token: 200023 '<|tool|>' is not marked as EOG\nload: control token: 200022 '<|system|>' is not marked as EOG\nload: control token: 200021 '<|user|>' is not marked as EOG\nload: control token: 200025 '<|tool_call|>' is not marked as EOG\nload: control token: 200027 '<|tool_response|>' is not marked as EOG\nload: control token: 200028 '<|tag|>' is not marked as EOG\nload: control token: 200026 '<|/tool_call|>' is not marked as EOG\nload: control token: 200018 '<|endofprompt|>' is not marked as EOG\nload: control token: 200019 '<|assistant|>' is not marked as EOG\nload: special tokens cache size = 12\nload: token to piece cache size = 1.3333 MB\nprint_info: arch             = phi3\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 3.84 B\nprint_info: general.name     = Phi 4 Mini Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 200064\nprint_info: n_merges         = 199742\nprint_info: BOS token        = 199999 '<|endoftext|>'\nprint_info: EOS token        = 199999 '<|endoftext|>'\nprint_info: EOT token        = 199999 '<|endoftext|>'\nprint_info: UNK token        = 199999 '<|endoftext|>'\nprint_info: PAD token        = 199999 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: EOG token        = 199999 '<|endoftext|>'\nprint_info: EOG token        = 200020 '<|end|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-15T07:09:05.685Z level=DEBUG source=server.go:360 msg=\"adding gpu library\" path=/usr/lib/ollama/rocm\ntime=2025-06-15T07:09:05.685Z level=DEBUG source=server.go:367 msg=\"adding gpu dependency paths\" paths=[/usr/lib/ollama/rocm]\ntime=2025-06-15T07:09:05.685Z level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/usr/bin/ollama runner --model /root/.ollama/models/blobs/sha256-3c168af1dea0a414299c7d9077e100ac763370e5a98b3c53801a958a47f0a5db --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 8 --flash-attn --kv-cache-type q4_0 --parallel 2 --port 36267\"\ntime=2025-06-15T07:09:05.685Z level=DEBUG source=server.go:432 msg=subprocess PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin LD_LIBRARY_PATH=/usr/lib/ollama/rocm:/usr/lib/ollama/rocm:/usr/lib/ollama:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib/ollama OLLAMA_KV_CACHE_TYPE=q4_0 OLLAMA_HOST=0.0.0.0:11434 OLLAMA_DEBUG=1 OLLAMA_FLASH_ATTENTION=1 OLLAMA_MAX_LOADED_MODELS=3 OLLAMA_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/ollama/rocm ROCR_VISIBLE_DEVICES=GPU-313404ec3196f63d\ntime=2025-06-15T07:09:05.686Z level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-15T07:09:05.686Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-15T07:09:05.686Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-06-15T07:09:05.692Z level=INFO source=runner.go:815 msg=\"starting go runner\"\ntime=2025-06-15T07:09:05.692Z level=DEBUG source=ggml.go:94 msg=\"ggml backend load all from path\" path=/usr/lib/ollama\nload_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-skylakex.so\ntime=2025-06-15T07:09:05.694Z level=DEBUG source=ggml.go:94 msg=\"ggml backend load all from path\" path=/usr/lib/ollama/rocm\n/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon Graphics, gfx1100 (0x1100), VMM: no, Wave Size: 32\nload_backend: loaded ROCm backend from /usr/lib/ollama/rocm/libggml-hip.so\ntime=2025-06-15T07:09:06.406Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 ROCm.0.NO_VMM=1 ROCm.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\nllama_model_load_from_file_impl: using device ROCm0 (AMD Radeon Graphics) - 20394 MiB free\ntime=2025-06-15T07:09:06.406Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:36267\"\nllama_model_loader: loaded meta data with 36 key-value pairs and 196 tensors from /root/.ollama/models/blobs/sha256-3c168af1dea0a414299c7d9077e100ac763370e5a98b3c53801a958a47f0a5db (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\nllama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\nllama_model_loader: - kv   2:                               general.type str              = model\nllama_model_loader: - kv   3:                               general.name str              = Phi 4 Mini Instruct\nllama_model_loader: - kv   4:                           general.finetune str              = instruct\nllama_model_loader: - kv   5:                           general.basename str              = Phi-4\nllama_model_loader: - kv   6:                         general.size_label str              = mini\nllama_model_loader: - kv   7:                            general.license str              = mit\nllama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\nllama_model_loader: - kv   9:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\nllama_model_loader: - kv  10:                          general.languages arr[str,24]      = [\"multilingual\", \"ar\", \"zh\", \"cs\", \"d...\nllama_model_loader: - kv  11:                        phi3.context_length u32              = 131072\nllama_model_loader: - kv  12:  phi3.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  13:                      phi3.embedding_length u32              = 3072\nllama_model_loader: - kv  14:                   phi3.feed_forward_length u32              = 8192\nllama_model_loader: - kv  15:                           phi3.block_count u32              = 32\nllama_model_loader: - kv  16:                  phi3.attention.head_count u32              = 24\nllama_model_loader: - kv  17:               phi3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  18:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  19:                  phi3.rope.dimension_count u32              = 96\nllama_model_loader: - kv  20:                        phi3.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = gpt-4o\ntime=2025-06-15T07:09:06.439Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,199742]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"e r\", ...\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 199999\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 199999\nllama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 199999\nllama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 199999\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - kv  35:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:   67 tensors\nllama_model_loader: - type q4_K:   80 tensors\nllama_model_loader: - type q5_K:   32 tensors\nllama_model_loader: - type q6_K:   17 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.31 GiB (5.18 BPW) \ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 200024 '<|/tool|>' is not marked as EOG\nload: control token: 200023 '<|tool|>' is not marked as EOG\nload: control token: 200022 '<|system|>' is not marked as EOG\nload: control token: 200021 '<|user|>' is not marked as EOG\nload: control token: 200025 '<|tool_call|>' is not marked as EOG\nload: control token: 200027 '<|tool_response|>' is not marked as EOG\nload: control token: 200028 '<|tag|>' is not marked as EOG\nload: control token: 200026 '<|/tool_call|>' is not marked as EOG\nload: control token: 200018 '<|endofprompt|>' is not marked as EOG\nload: control token: 200019 '<|assistant|>' is not marked as EOG\nload: special tokens cache size = 12\nload: token to piece cache size = 1.3333 MB\nprint_info: arch             = phi3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 3072\nprint_info: n_layer          = 32\nprint_info: n_head           = 24\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 96\nprint_info: n_swa            = 262144\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 3\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 8192\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 4096\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 3B\nprint_info: model params     = 3.84 B\nprint_info: general.name     = Phi 4 Mini Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 200064\nprint_info: n_merges         = 199742\nprint_info: BOS token        = 199999 '<|endoftext|>'\nprint_info: EOS token        = 199999 '<|endoftext|>'\nprint_info: EOT token        = 199999 '<|endoftext|>'\nprint_info: UNK token        = 199999 '<|endoftext|>'\nprint_info: PAD token        = 199999 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: EOG token        = 199999 '<|endoftext|>'\nprint_info: EOG token        = 200020 '<|end|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device ROCm0, is_swa = 0\nload_tensors: layer   1 assigned to device ROCm0, is_swa = 0\nload_tensors: layer   2 assigned to device ROCm0, is_swa = 0\nload_tensors: layer   3 assigned to device ROCm0, is_swa = 0\nload_tensors: layer   4 assigned to device ROCm0, is_swa = 0\nload_tensors: layer   5 assigned to device ROCm0, is_swa = 0\nload_tensors: layer   6 assigned to device ROCm0, is_swa = 0\nload_tensors: layer   7 assigned to device ROCm0, is_swa = 0\nload_tensors: layer   8 assigned to device ROCm0, is_swa = 0\nload_tensors: layer   9 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  10 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  11 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  12 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  13 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  14 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  15 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  16 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  17 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  18 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  19 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  20 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  21 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  22 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  23 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  24 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  25 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  26 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  27 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  28 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  29 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  30 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  31 assigned to device ROCm0, is_swa = 0\nload_tensors: layer  32 assigned to device ROCm0, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (q6_K) (and 0 others) cannot be used with preferred buffer type ROCm_Host, using CPU instead\nload_tensors: offloading 32 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 33/33 layers to GPU\nload_tensors:        ROCm0 model buffer size =  2368.57 MiB\nload_tensors:   CPU_Mapped model buffer size =   480.81 MiB\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 2\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 1024\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 1\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nset_abort_callback: call\nllama_context:  ROCm_Host  output buffer size =     1.55 MiB\ncreate_memory: n_ctx = 8192 (padded)\nllama_kv_cache_unified: kv_size = 8192, type_k = 'q4_0', type_v = 'q4_0', n_layer = 32, can_shift = 1, padding = 256\nllama_kv_cache_unified: layer   0: dev = ROCm0\nllama_kv_cache_unified: layer   1: dev = ROCm0\nllama_kv_cache_unified: layer   2: dev = ROCm0\nllama_kv_cache_unified: layer   3: dev = ROCm0\nllama_kv_cache_unified: layer   4: dev = ROCm0\nllama_kv_cache_unified: layer   5: dev = ROCm0\nllama_kv_cache_unified: layer   6: dev = ROCm0\nllama_kv_cache_unified: layer   7: dev = ROCm0\nllama_kv_cache_unified: layer   8: dev = ROCm0\nllama_kv_cache_unified: layer   9: dev = ROCm0\nllama_kv_cache_unified: layer  10: dev = ROCm0\nllama_kv_cache_unified: layer  11: dev = ROCm0\nllama_kv_cache_unified: layer  12: dev = ROCm0\nllama_kv_cache_unified: layer  13: dev = ROCm0\nllama_kv_cache_unified: layer  14: dev = ROCm0\nllama_kv_cache_unified: layer  15: dev = ROCm0\nllama_kv_cache_unified: layer  16: dev = ROCm0\nllama_kv_cache_unified: layer  17: dev = ROCm0\nllama_kv_cache_unified: layer  18: dev = ROCm0\nllama_kv_cache_unified: layer  19: dev = ROCm0\nllama_kv_cache_unified: layer  20: dev = ROCm0\nllama_kv_cache_unified: layer  21: dev = ROCm0\nllama_kv_cache_unified: layer  22: dev = ROCm0\nllama_kv_cache_unified: layer  23: dev = ROCm0\nllama_kv_cache_unified: layer  24: dev = ROCm0\nllama_kv_cache_unified: layer  25: dev = ROCm0\nllama_kv_cache_unified: layer  26: dev = ROCm0\nllama_kv_cache_unified: layer  27: dev = ROCm0\nllama_kv_cache_unified: layer  28: dev = ROCm0\nllama_kv_cache_unified: layer  29: dev = ROCm0\nllama_kv_cache_unified: layer  30: dev = ROCm0\nllama_kv_cache_unified: layer  31: dev = ROCm0\nllama_kv_cache_unified:      ROCm0 KV buffer size =   288.00 MiB\nllama_kv_cache_unified: KV self size  =  288.00 MiB, K (q4_0):  144.00 MiB, V (q4_0):  144.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 2\nllama_context: max_nodes = 65536\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\nllama_context: reserving graph for n_tokens = 512, n_seqs = 1\nllama_context: reserving graph for n_tokens = 1, n_seqs = 1\nllama_context: reserving graph for n_tokens = 512, n_seqs = 1\nllama_context:      ROCm0 compute buffer size =   402.75 MiB\nllama_context:  ROCm_Host compute buffer size =    22.01 MiB\nllama_context: graph nodes  = 1223\nllama_context: graph splits = 2\ntime=2025-06-15T07:09:06.940Z level=INFO source=server.go:630 msg=\"llama runner started in 1.25 seconds\"\ntime=2025-06-15T07:09:06.940Z level=DEBUG source=sched.go:495 msg=\"finished setting up\" runner.name=registry.ollama.ai/library/phi4-mini:latest runner.inference=rocm runner.devices=1 runner.size=\"3.2 GiB\" runner.vram=\"3.2 GiB\" runner.parallel=2 runner.pid=14 runner.model=/root/.ollama/models/blobs/sha256-3c168af1dea0a414299c7d9077e100ac763370e5a98b3c53801a958a47f0a5db runner.num_ctx=8192\ntime=2025-06-15T07:09:06.941Z level=ERROR source=routes.go:1530 msg=\"failed to create tool parser\" error=\"unexpected end of JSON input\"\n[GIN] 2025/06/15 - 07:09:06 | 500 |  1.474349537s |   192.168.28.79 | POST     \"/api/chat\"\ntime=2025-06-15T07:09:06.941Z level=DEBUG source=sched.go:503 msg=\"context for request finished\"\ntime=2025-06-15T07:09:06.941Z level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/phi4-mini:latest runner.inference=rocm runner.devices=1 runner.size=\"3.2 GiB\" runner.vram=\"3.2 GiB\" runner.parallel=2 runner.pid=14 runner.model=/root/.ollama/models/blobs/sha256-3c168af1dea0a414299c7d9077e100ac763370e5a98b3c53801a958a47f0a5db runner.num_ctx=8192 duration=5m0s\ntime=2025-06-15T07:09:06.941Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/phi4-mini:latest runner.inference=rocm runner.devices=1 runner.size=\"3.2 GiB\" runner.vram=\"3.2 GiB\" runner.parallel=2 runner.pid=14 runner.model=/root/.ollama/models/blobs/sha256-3c168af1dea0a414299c7d9077e100ac763370e5a98b3c53801a958a47f0a5db runner.num_ctx=8192 refCount=0\n```\n\n### OS\n\nDocker\n\n### GPU\n\nAMD\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "icefairy64",
        "body": "Closing as duplicate of #9437."
      }
    ]
  },
  {
    "issue_number": 4806,
    "title": "codegemma broken on releases after v0.1.39",
    "author": "evertjr",
    "state": "closed",
    "created_at": "2024-06-04T05:05:04Z",
    "updated_at": "2025-06-15T06:12:09Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI use codegemma with continue.dev extension on vscode, it works fine on version 0.1.39. but on the last two releases it doesn't generate completions and behave very strangely in terminal.\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "jmorganca",
        "body": "Sorry this happened. Are you using `codegemma:code`?"
      },
      {
        "user": "evertjr",
        "body": "> Sorry this happened. Are you using `codegemma:code`?\r\n\r\n I use codegemma:2b-code-v1.1-q8_0  only for tab autocomplete, it's very good but I noticed it stopped giving suggestions.\r\n\r\nI tried to revert to old versions of the continue extension but no luck, but as soon as I reverted Ollama to the mentioned version it worked again. "
      },
      {
        "user": "robwilkes",
        "body": "For me it seems to give no outputs.\r\n\r\n```\r\n$ ollama run codegemma:7b-code-q6_K\r\n>>> <|fim_prefix|>import datetime\r\n... def calculate_age(birth_year):\r\n...     \"\"\"Calculates a person's age based on their birth year.\"\"\"\r\n...     current_year = datetime.date.today().year\r\n...     <|fim_suffix|>\r\n...     return age<|fim_middle|>\r\n\r\n\r\n\r\n>>> \r\n... <|fim_prefix|>import datetime\r\n... def calculate_age(birth_year):\r\n...     \"\"\"Calculates a person's age based on their birth year.\"\"\"\r\n...     current_year = datetime.date.today().year\r\n...     <|fim_suffix|>\r\n...     return age<|fim_middle|>\r\n... \r\n\r\n\r\n>>> Send a message (/? for help)\r\n```\r\n\r\nThe instruct models work, but the code models seem to generate no output"
      }
    ]
  },
  {
    "issue_number": 11076,
    "title": "[Feature] - Configuration File Support for Server Settings",
    "author": "ondrovic",
    "state": "open",
    "created_at": "2025-06-15T02:47:04Z",
    "updated_at": "2025-06-15T02:47:04Z",
    "labels": [
      "feature request"
    ],
    "body": "# Feature Request: Configuration File Support for Server Settings\n\n## Summary\nRequest for a standardized configuration file to manage Ollama server settings, particularly network binding configurations, as an alternative to environment variables.\n\n## Problem Statement\nCurrently, Ollama requires environment variables for all configuration options, including basic server settings like `OLLAMA_HOST` for network binding. This approach presents several usability and operational challenges:\n\n### Current Issues:\n1. **Platform Inconsistency**: Environment variable management differs significantly across operating systems (Windows system properties vs. Linux shell profiles vs. macOS launchd configurations)\n\n2. **Poor Developer Experience**: Setting up network access requires:\n   - Manually configuring environment variables\n   - Remembering to set them before each execution\n   - Platform-specific knowledge for permanent configuration\n\n3. **Deployment Complexity**: \n   - No standardized way to version-control server configurations\n   - Difficult to manage multiple deployment environments\n   - Requires custom scripts or platform-specific service configurations\n\n4. **Discoverability**: Users must consult documentation to find available configuration options rather than having a self-documenting configuration file\n\n## Proposed Solution\nImplement a configuration file system that:\n\n### 1. **Standard Configuration File Format**\n- Use a widely-adopted format (TOML, YAML, or JSON)\n- Follow OS-specific standard configuration paths:\n  - Linux: `~/.config/ollama/config.toml` or `/etc/ollama/config.toml`\n  - macOS: `~/Library/Application Support/ollama/config.toml`\n  - Windows: `%APPDATA%\\ollama\\config.toml`\n\n### 2. **Hierarchical Configuration Precedence**\nConfiguration lookup order (highest to lowest priority):\n1. Command-line flags\n2. Environment variables (maintaining backward compatibility)\n3. User configuration file (`~/.config/ollama/config.toml`)\n4. System-wide configuration file (`/etc/ollama/config.toml`)\n5. Built-in defaults\n\n### 3. **Example Configuration File**\n```toml\n# Ollama Server Configuration\n\n[server]\n# Network binding configuration\n# Set to \"0.0.0.0:11434\" to bind to all interfaces\n# Set to \"127.0.0.1:11434\" for localhost only (default)\nhost = \"0.0.0.0:11434\"\n\n# Allowed origins for CORS\norigins = [\"http://localhost:3000\", \"https://example.com\"]\n\n[models]\n# Directory to store models\npath = \"~/.ollama/models\"\n\n# Model loading behavior\nkeep_alive = \"5m\"\nmax_loaded_models = 3\nmax_queue = 512\n\n[performance]\n# Number of parallel requests\nnum_parallel = 4\n\n# GPU configuration\ngpu_overhead = 0\n\n[logging]\n# Enable debug logging\ndebug = false\n```\n\n## Benefits\n\n### **For Users:**\n- **Simplified Setup**: Single configuration file for all settings\n- **Version Control**: Configuration can be tracked in dotfiles/repositories\n- **Self-Documenting**: Configuration options visible with descriptions\n- **Platform Agnostic**: Same configuration works across operating systems\n\n### **For System Administrators:**\n- **Standardized Deployment**: Consistent configuration management\n- **Environment Management**: Easy to maintain dev/staging/production configs\n- **Security**: Centralized configuration with appropriate file permissions\n\n### **For Developers:**\n- **Integration**: Easier to build tooling around standardized configuration\n- **Documentation**: Configuration schema can be automatically generated\n- **Testing**: Simplified configuration for different test scenarios\n\n## Implementation Considerations\n\n1. **Backward Compatibility**: Environment variables should continue to work and take precedence over file configuration\n2. **Configuration Validation**: Validate configuration at startup with helpful error messages\n3. **Hot Reloading**: Consider supporting configuration reload without restart (future enhancement)\n4. **Documentation**: Update all relevant documentation to show both environment variable and configuration file approaches\n\n## Use Case Example\n**Current Workflow (Environment Variables):**\n```bash\n# Windows - requires setting system environment variable or:\nset OLLAMA_HOST=0.0.0.0:11434\nollama serve\n\n# Linux/macOS - requires remembering to set every time or modifying shell profile\nexport OLLAMA_HOST=0.0.0.0:11434\nollama serve\n```\n\n**Proposed Workflow (Configuration File):**\n```bash\n# Create config once\necho 'host = \"0.0.0.0:11434\"' > ~/.config/ollama/config.toml\n\n# Just run - works everywhere\nollama serve\n```\n\n## Related Issues\nThis request builds upon previous configuration-related discussions:\n- Related to the broader configuration file request in #7089\n- Addresses deployment concerns mentioned in #3516\n- Complements model management configuration requests in #4883\n\n## Conclusion\nA configuration file system would significantly improve Ollama's usability, particularly for network configuration scenarios, while maintaining full backward compatibility with the current environment variable approach. This enhancement would align Ollama with standard practices used by other server applications and development tools.",
    "comments": []
  },
  {
    "issue_number": 10688,
    "title": "Memory leak during inference using Gemma3 with structured output",
    "author": "leokeba",
    "state": "closed",
    "created_at": "2025-05-13T15:22:07Z",
    "updated_at": "2025-06-15T02:42:03Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI am getting what looks like a big memory leak when using ollama on a debian host with structured output with gemma3-12b. It's running on a 3090Ti with default settings.\n\nMemory (system RAM) usage grows very fast (about 30mb/sec) continuously until it saturates the host and hangs. I then need to restart the ollama server and the same thing happens as soon as I start inferencing again. If I stop the inference script the memory usage does not go down, I always have to kill the process to clear the RAM.\n\nI'm attaching the log output from the server, is anything else needed to look into this ?\n\n### Relevant log output\n\n```shell\nleo@debian-nvidia:~$ ollama serve\n2025/05/13 16:48:00 routes.go:1233: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/leo/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-05-13T16:48:00.561+02:00 level=INFO source=images.go:463 msg=\"total blobs: 33\"\ntime=2025-05-13T16:48:00.562+02:00 level=INFO source=images.go:470 msg=\"total unused blobs removed: 0\"\ntime=2025-05-13T16:48:00.562+02:00 level=INFO source=routes.go:1300 msg=\"Listening on 127.0.0.1:11434 (version 0.6.8)\"\ntime=2025-05-13T16:48:00.562+02:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-05-13T16:48:00.784+02:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-d744590e-2a3b-8e2e-f4bc-988c67d6c902 library=cuda variant=v12 compute=8.6 driver=12.8 name=\"NVIDIA GeForce RTX 3090 Ti\" total=\"23.6 GiB\" available=\"23.3 GiB\"\n[GIN] 2025/05/13 - 16:48:37 | 200 |     980.376µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:37.849+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:48:37.996+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:48:38.026+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:48:38.028+02:00 level=INFO source=sched.go:754 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/home/leo/.ollama/models/blobs/sha256-e8ad13eff07a78d89926e9e8b882317d082ef5bf9768ad7b50fcdbbcd63748de gpu=GPU-d744590e-2a3b-8e2e-f4bc-988c67d6c902 parallel=2 available=24974786560 required=\"11.0 GiB\"\ntime=2025-05-13T16:48:38.137+02:00 level=INFO source=server.go:106 msg=\"system memory\" total=\"31.2 GiB\" free=\"29.3 GiB\" free_swap=\"542.1 MiB\"\ntime=2025-05-13T16:48:38.139+02:00 level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=49 layers.split=\"\" memory.available=\"[23.3 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"11.0 GiB\" memory.required.partial=\"11.0 GiB\" memory.required.kv=\"1.3 GiB\" memory.required.allocations=\"[11.0 GiB]\" memory.weights.total=\"6.8 GiB\" memory.weights.repeating=\"6.0 GiB\" memory.weights.nonrepeating=\"787.5 MiB\" memory.graph.full=\"519.5 MiB\" memory.graph.partial=\"1.3 GiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\ntime=2025-05-13T16:48:38.192+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:48:38.193+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=tokenizer.ggml.add_eot_token default=false\ntime=2025-05-13T16:48:38.196+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07\ntime=2025-05-13T16:48:38.196+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.rope.local.freq_base default=10000\ntime=2025-05-13T16:48:38.196+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.rope.global.freq_base default=1e+06\ntime=2025-05-13T16:48:38.196+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.rope.freq_scale default=1\ntime=2025-05-13T16:48:38.196+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.mm_tokens_per_image default=256\ntime=2025-05-13T16:48:38.197+02:00 level=INFO source=server.go:410 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model /home/leo/.ollama/models/blobs/sha256-e8ad13eff07a78d89926e9e8b882317d082ef5bf9768ad7b50fcdbbcd63748de --ctx-size 8192 --batch-size 512 --n-gpu-layers 49 --threads 10 --parallel 2 --port 33857\"\ntime=2025-05-13T16:48:38.197+02:00 level=INFO source=sched.go:452 msg=\"loaded runners\" count=1\ntime=2025-05-13T16:48:38.197+02:00 level=INFO source=server.go:589 msg=\"waiting for llama runner to start responding\"\ntime=2025-05-13T16:48:38.197+02:00 level=INFO source=server.go:623 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-05-13T16:48:38.205+02:00 level=INFO source=runner.go:851 msg=\"starting ollama engine\"\ntime=2025-05-13T16:48:38.205+02:00 level=INFO source=runner.go:914 msg=\"Server listening on 127.0.0.1:33857\"\ntime=2025-05-13T16:48:38.255+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:48:38.256+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.name default=\"\"\ntime=2025-05-13T16:48:38.256+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.description default=\"\"\ntime=2025-05-13T16:48:38.256+02:00 level=INFO source=ggml.go:72 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=1065 num_key_values=37\nload_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\nload_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\ntime=2025-05-13T16:48:38.313+02:00 level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\ntime=2025-05-13T16:48:38.385+02:00 level=INFO source=ggml.go:298 msg=\"model weights\" buffer=CUDA0 size=\"7.6 GiB\"\ntime=2025-05-13T16:48:38.385+02:00 level=INFO source=ggml.go:298 msg=\"model weights\" buffer=CPU size=\"787.5 MiB\"\ntime=2025-05-13T16:48:38.449+02:00 level=INFO source=server.go:623 msg=\"waiting for server to become available\" status=\"llm server loading model\"\ntime=2025-05-13T16:48:43.663+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=tokenizer.ggml.add_eot_token default=false\ntime=2025-05-13T16:48:43.667+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07\ntime=2025-05-13T16:48:43.667+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.rope.local.freq_base default=10000\ntime=2025-05-13T16:48:43.667+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.rope.global.freq_base default=1e+06\ntime=2025-05-13T16:48:43.667+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.rope.freq_scale default=1\ntime=2025-05-13T16:48:43.667+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.mm_tokens_per_image default=256\ntime=2025-05-13T16:48:43.694+02:00 level=INFO source=ggml.go:553 msg=\"compute graph\" backend=CUDA0 buffer_type=CUDA0 size=\"308.0 MiB\"\ntime=2025-05-13T16:48:43.694+02:00 level=INFO source=ggml.go:553 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"7.5 MiB\"\ntime=2025-05-13T16:48:43.713+02:00 level=INFO source=server.go:628 msg=\"llama runner started in 5.52 seconds\"\n[GIN] 2025/05/13 - 16:48:45 | 200 |  7.544991459s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:45 | 200 |     858.796µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:45.387+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:46 | 200 |  990.816217ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:46 | 200 |     821.351µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:46.381+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:47 | 200 |  768.481138ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:47 | 200 |       828.9µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:47.153+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:47 | 200 |  762.318591ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:47 | 200 |     813.687µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:47.918+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:48 | 200 |  763.796017ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:48 | 200 |     935.834µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:48.686+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:49 | 200 |  762.516804ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:49 | 200 |     835.289µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:49.451+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:50 | 200 |  755.520694ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:50 | 200 |     843.444µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:50.209+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:50 | 200 |  754.848081ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:50 | 200 |     861.046µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:50.969+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:51 | 200 |  745.156696ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:51 | 200 |     863.531µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:51.716+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:52 | 200 |  1.054927393s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:52 | 200 |     855.916µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:52.775+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:53 | 200 |  994.745329ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:53 | 200 |     854.313µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:53.773+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:54 | 200 |  1.049445941s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:54 | 200 |     811.638µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:54.824+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:55 | 200 |  850.098955ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:55 | 200 |     792.085µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:55.678+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:56 | 200 |  993.567647ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:56 | 200 |     817.565µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:56.675+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:57 | 200 |  984.835547ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:57 | 200 |     816.258µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:57.663+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:58 | 200 |  1.027214846s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:58 | 200 |     815.524µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:58.693+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:48:59 | 200 |  1.056577493s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:48:59 | 200 |     932.081µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:48:59.754+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:00 | 200 |  988.791845ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:00 | 200 |     821.784µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:00.745+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:01 | 200 |  1.002698372s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:01 | 200 |     832.131µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:01.751+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:02 | 200 |  1.061438887s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:02 | 200 |     853.777µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:02.817+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:03 | 200 |  1.073778163s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:03 | 200 |     818.274µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:03.893+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:04 | 200 |  978.437141ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:04 | 200 |     820.064µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:04.875+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:05 | 200 |  984.538035ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:05 | 200 |     815.883µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:05.863+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:06 | 200 |  984.138888ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:06 | 200 |     828.386µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:06.849+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:07 | 200 |  984.296063ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:07 | 200 |     802.968µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:07.837+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:08 | 200 |  1.076913058s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:08 | 200 |      812.68µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:08.917+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:09 | 200 |  994.295375ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:09 | 200 |     829.005µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:09.915+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:10 | 200 |  925.360822ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:10 | 200 |     837.753µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:10.843+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:11 | 200 |  1.007319818s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:11 | 200 |      848.73µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:11.854+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:12 | 200 |  1.000909106s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:12 | 200 |      844.68µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:12.858+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:13 | 200 |  1.008300583s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:13 | 200 |     801.465µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:13.869+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:14 | 200 |  1.006693507s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:14 | 200 |     831.461µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:14.880+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:15 | 200 |  950.929392ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:15 | 200 |     841.006µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:15.833+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:16 | 200 |  952.581587ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:16 | 200 |     839.864µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:16.789+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:17 | 200 |   997.29827ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:17 | 200 |     820.078µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:17.790+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:18 | 200 |  1.056098038s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:18 | 200 |     807.002µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:18.849+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:19 | 200 |  994.453749ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:19 | 200 |     803.421µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:19.846+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:20 | 200 |   988.04637ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:20 | 200 |     837.535µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:20.838+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:21 | 200 |  932.301373ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:21 | 200 |     839.469µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:21.773+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:22 | 200 |  1.059917962s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:22 | 200 |      824.76µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:22.836+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:24 | 200 |  1.269081446s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:24 | 200 |     820.431µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:24.109+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:24 | 200 |   730.49696ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:24 | 200 |     837.096µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:24.842+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:25 | 200 |  749.720052ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:25 | 200 |     893.069µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:25.596+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:26 | 200 |    1.0075791s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:26 | 200 |     826.889µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:26.607+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:27 | 200 |  985.420873ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:27 | 200 |     809.116µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:27.595+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:28 | 200 |  1.078693931s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:28 | 200 |     836.868µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:28.676+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:29 | 200 |  1.026126364s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:29 | 200 |     814.288µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:29.707+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:30 | 200 |   984.94677ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:30 | 200 |     817.913µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:30.694+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:31 | 200 |  1.032658607s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:31 | 200 |     794.625µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:31.730+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:32 | 200 |  1.066533978s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:33 | 200 |     836.575µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:33.168+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:34 | 200 |  1.447059891s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:34 | 200 |     841.524µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:34.618+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:35 | 200 |  1.027703899s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:35 | 200 |     863.036µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:35.648+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:36 | 200 |  994.915362ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:36 | 200 |     803.869µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:36.647+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:38 | 200 |   1.58195298s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:38 | 200 |     884.533µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:38.232+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:39 | 200 |  1.142600246s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:39 | 200 |      825.11µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:39.377+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:40 | 200 |  1.080568844s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:40 | 200 |     815.512µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:40.462+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:41 | 200 |  1.146987692s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:41 | 200 |     811.222µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:41.611+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:42 | 200 |  979.825221ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:42 | 200 |     852.475µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:42.594+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:43 | 200 |  982.748158ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:43 | 200 |     820.496µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:43.581+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:44 | 200 |  1.100700606s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:44 | 200 |     862.996µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:44.684+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:46 | 200 |  1.497456592s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:46 | 200 |     853.133µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:46.185+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:47 | 200 |  1.064251921s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:47 | 200 |     890.363µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:47.253+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:48 | 200 |  919.330615ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:48 | 200 |     887.668µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:48.175+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:49 | 200 |  937.752028ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:49 | 200 |     826.643µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:49.116+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:50 | 200 |  991.541073ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:50 | 200 |      815.88µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:50.112+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:51 | 200 |  997.471176ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:51 | 200 |     835.182µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:51.112+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:52 | 200 |  1.051647686s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:52 | 200 |     817.042µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:52.166+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:53 | 200 |  1.104912045s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:53 | 200 |     834.291µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:53.275+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:54 | 200 |  945.281302ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:54 | 200 |    1.186419ms |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:54.224+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:55 | 200 |  898.399962ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:55 | 200 |     796.173µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:55.125+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:56 | 200 |  1.108278874s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:56 | 200 |     877.902µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:56.237+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:57 | 200 |  922.307376ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:57 | 200 |     818.859µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:57.162+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:58 | 200 |  1.093775179s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:58 | 200 |     826.919µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:58.259+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:49:59 | 200 |  937.066277ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:49:59 | 200 |     817.475µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:49:59.200+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:00 | 200 |  902.689181ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:00 | 200 |     830.142µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:00.105+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:01 | 200 |  1.583952845s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:01 | 200 |     827.615µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:01.692+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:02 | 200 |  1.105850744s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:02 | 200 |      842.06µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:02.802+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:03 | 200 |  928.793041ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:03 | 200 |     827.222µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:03.733+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:04 | 200 |  910.071569ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:04 | 200 |    1.226551ms |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:04.648+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:05 | 200 |  936.489608ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:05 | 200 |     833.971µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:05.590+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:06 | 200 |  1.164017222s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:06 | 200 |     832.653µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:06.754+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:07 | 200 |  1.022418572s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:07 | 200 |     837.561µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:07.781+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:08 | 200 |  969.502488ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:08 | 200 |     779.343µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:08.753+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:09 | 200 |  961.229721ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:09 | 200 |     821.739µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:09.717+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:10 | 200 |  1.200781498s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:10 | 200 |     838.319µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:10.921+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:11 | 200 |  1.050320821s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:11 | 200 |      814.57µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:11.975+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:12 | 200 |  985.961573ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:12 | 200 |     904.037µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:12.964+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:14 | 200 |  1.193615905s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:14 | 200 |     820.404µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:14.160+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:15 | 200 |  959.403609ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:15 | 200 |     831.935µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:15.124+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:16 | 200 |  1.176947939s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:16 | 200 |     831.985µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:16.303+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:17 | 200 |  923.669162ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:17 | 200 |     784.075µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:17.230+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:18 | 200 |   969.61224ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:18 | 200 |     806.076µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:18.203+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:19 | 200 |  1.001123719s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:19 | 200 |     812.874µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:19.207+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:20 | 200 |  948.493703ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:20 | 200 |     829.258µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:20.159+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:21 | 200 |  929.241283ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:21 | 200 |     857.089µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:21.092+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:22 | 200 |  1.107269829s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:22 | 200 |     805.287µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:22.202+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:23 | 200 |  1.089536435s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:50:23 | 200 |     836.418µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:50:23.295+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:50:23 | 500 |   494.37497ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:36 | 200 |     819.058µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:36.739+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:51:36.894+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:51:36.925+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:51:36.929+02:00 level=INFO source=sched.go:517 msg=\"updated VRAM based on existing loaded models\" gpu=GPU-d744590e-2a3b-8e2e-f4bc-988c67d6c902 library=cuda total=\"23.6 GiB\" available=\"12.6 GiB\"\ntime=2025-05-13T16:51:36.929+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.block_count default=0\ntime=2025-05-13T16:51:36.930+02:00 level=INFO source=sched.go:754 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/home/leo/.ollama/models/blobs/sha256-2fc97d5d1c3217ac90931d7992762a418e09b2c5c49c030c6c1f9e8fbb221013 gpu=GPU-d744590e-2a3b-8e2e-f4bc-988c67d6c902 parallel=2 available=13522255936 required=\"10.0 GiB\"\ntime=2025-05-13T16:51:37.047+02:00 level=INFO source=server.go:106 msg=\"system memory\" total=\"31.2 GiB\" free=\"25.2 GiB\" free_swap=\"542.6 MiB\"\ntime=2025-05-13T16:51:37.047+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.block_count default=0\ntime=2025-05-13T16:51:37.048+02:00 level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=49 layers.split=\"\" memory.available=\"[12.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"10.0 GiB\" memory.required.partial=\"10.0 GiB\" memory.required.kv=\"1.3 GiB\" memory.required.allocations=\"[10.0 GiB]\" memory.weights.total=\"7.6 GiB\" memory.weights.repeating=\"6.8 GiB\" memory.weights.nonrepeating=\"787.7 MiB\" memory.graph.full=\"519.6 MiB\" memory.graph.partial=\"1.3 GiB\"\ntime=2025-05-13T16:51:37.098+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:51:37.099+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=tokenizer.ggml.add_eot_token default=false\ntime=2025-05-13T16:51:37.101+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.image_size default=0\ntime=2025-05-13T16:51:37.101+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.patch_size default=0\ntime=2025-05-13T16:51:37.101+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.num_channels default=0\ntime=2025-05-13T16:51:37.101+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.block_count default=0\ntime=2025-05-13T16:51:37.101+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.embedding_length default=0\ntime=2025-05-13T16:51:37.101+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.attention.head_count default=0\ntime=2025-05-13T16:51:37.101+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.image_size default=0\ntime=2025-05-13T16:51:37.101+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.patch_size default=0\ntime=2025-05-13T16:51:37.101+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.attention.layer_norm_epsilon default=0\ntime=2025-05-13T16:51:37.103+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.rope.freq_scale default=1\ntime=2025-05-13T16:51:37.103+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.mm_tokens_per_image default=256\ntime=2025-05-13T16:51:37.103+02:00 level=INFO source=server.go:410 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model /home/leo/.ollama/models/blobs/sha256-2fc97d5d1c3217ac90931d7992762a418e09b2c5c49c030c6c1f9e8fbb221013 --ctx-size 8192 --batch-size 512 --n-gpu-layers 49 --threads 10 --parallel 2 --port 42857\"\ntime=2025-05-13T16:51:37.103+02:00 level=INFO source=sched.go:452 msg=\"loaded runners\" count=2\ntime=2025-05-13T16:51:37.104+02:00 level=INFO source=server.go:589 msg=\"waiting for llama runner to start responding\"\ntime=2025-05-13T16:51:37.104+02:00 level=INFO source=server.go:623 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-05-13T16:51:37.111+02:00 level=INFO source=runner.go:851 msg=\"starting ollama engine\"\ntime=2025-05-13T16:51:37.112+02:00 level=INFO source=runner.go:914 msg=\"Server listening on 127.0.0.1:42857\"\ntime=2025-05-13T16:51:37.165+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-05-13T16:51:37.166+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.name default=\"\"\ntime=2025-05-13T16:51:37.166+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.description default=\"\"\ntime=2025-05-13T16:51:37.166+02:00 level=INFO source=ggml.go:72 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=1737 num_key_values=32\nload_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\nload_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\ntime=2025-05-13T16:51:37.204+02:00 level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\ntime=2025-05-13T16:51:37.285+02:00 level=INFO source=ggml.go:298 msg=\"model weights\" buffer=CUDA0 size=\"8.4 GiB\"\ntime=2025-05-13T16:51:37.285+02:00 level=INFO source=ggml.go:298 msg=\"model weights\" buffer=CPU size=\"787.7 MiB\"\ntime=2025-05-13T16:51:37.355+02:00 level=INFO source=server.go:623 msg=\"waiting for server to become available\" status=\"llm server loading model\"\ntime=2025-05-13T16:51:38.979+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=tokenizer.ggml.add_eot_token default=false\ntime=2025-05-13T16:51:38.981+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.image_size default=0\ntime=2025-05-13T16:51:38.981+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.patch_size default=0\ntime=2025-05-13T16:51:38.981+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.num_channels default=0\ntime=2025-05-13T16:51:38.981+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.block_count default=0\ntime=2025-05-13T16:51:38.981+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.embedding_length default=0\ntime=2025-05-13T16:51:38.981+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.attention.head_count default=0\ntime=2025-05-13T16:51:38.981+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.image_size default=0\ntime=2025-05-13T16:51:38.981+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.patch_size default=0\ntime=2025-05-13T16:51:38.981+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.vision.attention.layer_norm_epsilon default=0\ntime=2025-05-13T16:51:38.983+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.rope.freq_scale default=1\ntime=2025-05-13T16:51:38.983+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=gemma3.mm_tokens_per_image default=256\ntime=2025-05-13T16:51:39.009+02:00 level=INFO source=ggml.go:553 msg=\"compute graph\" backend=CUDA0 buffer_type=CUDA0 size=\"308.0 MiB\"\ntime=2025-05-13T16:51:39.009+02:00 level=INFO source=ggml.go:553 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"7.5 MiB\"\ntime=2025-05-13T16:51:39.109+02:00 level=INFO source=server.go:628 msg=\"llama runner started in 2.01 seconds\"\n[GIN] 2025/05/13 - 16:51:40 | 200 |  3.625931541s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:40 | 200 |    1.304686ms |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:40.368+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:41 | 200 |  1.173288276s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:41 | 200 |     884.953µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:41.545+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:42 | 200 |  1.093041205s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:42 | 200 |     876.112µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:42.641+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:43 | 200 |  1.190148307s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:43 | 200 |     927.104µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:43.835+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:44 | 200 |  1.117178999s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:44 | 200 |     805.534µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:44.955+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:46 | 200 |  1.188636665s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:46 | 200 |     1.21855ms |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:46.148+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:47 | 200 |  1.126001863s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:47 | 200 |     842.821µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:47.277+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:48 | 200 |  1.062130448s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:48 | 200 |      853.01µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:48.342+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:49 | 200 |  1.192205675s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:49 | 200 |     828.288µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:49.537+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:50 | 200 |  1.064617413s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:50 | 200 |     826.038µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:50.605+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:51 | 200 |  1.181298946s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:51 | 200 |     912.704µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:51.792+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:52 | 200 |  1.157731531s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:52 | 200 |     827.216µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:52.951+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:54 | 200 |  1.144311709s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:54 | 200 |     852.097µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:54.098+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:55 | 200 |  1.151178661s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:55 | 200 |     823.791µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:55.253+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:56 | 200 |  1.098051386s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:56 | 200 |     830.556µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:56.354+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:57 | 200 |  1.095882359s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:57 | 200 |     823.501µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:57.455+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:58 | 200 |  1.192077006s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:58 | 200 |     830.431µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:58.649+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:51:59 | 200 |  1.179899598s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:51:59 | 200 |     865.571µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:51:59.831+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:00 | 200 |  1.190354606s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:00 | 200 |     823.648µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:01.025+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:02 | 200 |  1.058573208s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:02 | 200 |     808.631µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:02.087+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:03 | 200 |  1.189769749s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:03 | 200 |     843.265µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:03.282+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:04 | 200 |  1.148332443s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:04 | 200 |     828.562µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:04.432+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:05 | 200 |  1.116938469s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:05 | 200 |     840.112µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:05.552+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:06 | 200 |  1.185360654s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:06 | 200 |     830.644µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:06.741+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:07 | 200 |  1.187190771s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:07 | 200 |     837.899µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:07.931+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:09 | 200 |  1.200790074s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:09 | 200 |     899.137µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:09.135+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:10 | 200 |  1.135805072s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:10 | 200 |     1.33389ms |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:10.275+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:11 | 200 |  1.178719063s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:11 | 200 |     792.231µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:11.457+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:12 | 200 |  1.068732647s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:12 | 200 |    1.358903ms |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:12.530+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:13 | 200 |  1.084584064s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:13 | 200 |     815.049µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:13.617+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:14 | 200 |  1.099273485s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:14 | 200 |     872.335µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:14.720+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:15 | 200 |  1.291627781s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:15 | 200 |     853.691µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:16.017+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:17 | 200 |  1.165006914s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:17 | 200 |     857.917µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:17.183+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:18 | 200 |  1.185455866s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:18 | 200 |      851.02µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:18.372+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:19 | 200 |  1.182260809s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:19 | 200 |     833.657µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:19.558+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:20 | 200 |  1.100934571s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:20 | 200 |     838.036µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:20.662+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:21 | 200 |  1.067580922s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:21 | 200 |      845.81µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:21.734+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:22 | 200 |  824.388844ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:22 | 200 |     851.607µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:22.561+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:23 | 200 |  1.157841226s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:23 | 200 |     808.772µs |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:23.722+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:24 | 200 |  1.226060831s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/13 - 16:52:24 | 200 |     1.02996ms |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-05-13T16:52:24.951+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/05/13 - 16:52:26 | 200 |  1.300700119s |       127.0.0.1 | POST     \"/api/chat\"\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.6.8",
    "comments": [
      {
        "user": "leokeba",
        "body": "Here's a minimal script to reproduce the issue : \n```\nfrom pydantic import BaseModel\nfrom enum import StrEnum\nfrom typing import List\nimport ollama\n\nclass TweetSubject(StrEnum):\n    REGLES_DE_CRISE_ADAPTATIONS = \"Règles de crise / adaptations\"\n    MEDECINE_PROTECTION_SANITAIRE = \"Médecine / protection sanitaire\"\n    VACCIN = \"Vaccin\"\n    PENURIE_GESTION_MATERIEL = \"Pénurie / gestion matériel\"\n    PASS_SANITAIRE = \"Pass sanitaire\"\n    AUTRES = \"Autres\"\n    NSP = \"NSP\"\n\nsubjects_description = {\n    TweetSubject.REGLES_DE_CRISE_ADAPTATIONS: \"règles édictées pour répondre à la situation de pandémie de covid19 et façons de s’y adapter. Aspect légal, autorisations, interdictions, etc.\",\n    TweetSubject.MEDECINE_PROTECTION_SANITAIRE: \"sujets médicaux, liés au covid19 ou à d’autres maladies, aux risques qu’elles entraînent et aux façons de s’en protéger.\",\n    TweetSubject.VACCIN: \"vaccin contre le covid19. Ne se combine pas avec un autre sujet sauf si plusieurs questions bien distinctes.\",\n    TweetSubject.PENURIE_GESTION_MATERIEL: \"gestion par l’état du matériel nécessaire pour lutter contre le covid19, y compris les moyens humains des hôpitaux, le prix des moyens de protection et leur accessibilité.\",\n    TweetSubject.PASS_SANITAIRE: \"pass sanitaire, passeport vaccinal ou toute différenciation de droit entre personnes vaccinées et personnes non-vaccinées, ou toute autorisation ou interdiction liée au résultat d’un test. Si le sujet « pass sanitaire » est présent dans le message, pas de combinaison avec un autre. Si les sujets « pass sanitaire » et « vaccin » sont présents, indiquer « pass sanitaire », sauf si plusieurs questions bien distinctes.\",\n    TweetSubject.AUTRES: \"sujet qui ne rentre dans aucune des autres catégories.\",\n    TweetSubject.NSP: \"s’il n’y a pas d’élément susceptible d’éclairer sur le sujet du tweet\"\n}\n\nclass TweetEmotion(StrEnum):\n    MECONTENTEMENT_COLERE = \"Mécontentement / colère\"\n    GRATITUDE_VALIDATION = \"Gratitude / validation\"\n    NEUTRE = \"Neutre\"\n    AUTRE = \"Autre\"\n    NSP = \"NSP\"\n\nemotions_description = {\n    TweetEmotion.MECONTENTEMENT_COLERE: \"expression d’une émotion pouvant aller d’un léger mécontentement à la colère la plus violente.\",\n    TweetEmotion.GRATITUDE_VALIDATION: \"expression d’une gratitude ou validation de propos ou d’actions.\",\n    TweetEmotion.NEUTRE: \"pas d’émotion particulière détectable.\",\n    TweetEmotion.AUTRE: \"expression d’une émotion qui ne rentre dans aucune des autres catégories.\",\n    TweetEmotion.NSP: \"s’il n’y a pas d’élément susceptible d’éclairer sur l’émotion du tweet.\"\n}\n\nclass TweetAnalysis(BaseModel):\n    subject: List[TweetSubject]\n    emotion: List[TweetEmotion]\n\nsubject_list = [subject.value for subject in TweetSubject]\nemotion_list = [emotion.value for emotion in TweetEmotion]\n\ndef build_analysis_prompt(tweet):\n    subject_list_string = '''\n    - '''.join(f\"'{subject}' : {subjects_description[subject]}\" for subject in subject_list)\n    emotion_list_string = '''\n    - '''.join(f\"'{emotion}' : {emotions_description[emotion]}\" for emotion in emotion_list)\n\n    prompt = f\"\"\"Analyze the following tweet and return the subjects and emotions that best characterizes its contents as a json object.\n                Do not return more than 2 subjects and 2 emotions.\n    \n                Tweet: {tweet}\n\n                Subjects: \n                - {subject_list_string}\n\n                Emotions: \n                - {emotion_list_string}\"\"\"\n    return prompt\n\ndef get_ollama_structured_response(prompt: str, model: BaseModel):\n        \"\"\"\n        Sends a prompt to the Ollama model and returns the response content.\n        \"\"\"\n        response = ollama.chat(\n            model='gemma3:12b', \n            messages=[\n                {\n                    'role': 'user',\n                    'content': prompt,\n                }\n            ],\n            format=model.model_json_schema(),\n        )\n        return model.model_validate_json(response['message']['content'])\n\ntweet1 = '''Il n'y a pas que la Chine et l'Italie dont s'inspirer         \nTaiwan semble exemplaire dans son traitement du #Coronavirus  #Onvousrépond \nhttps://t.co/kr8LE9C0vg'''\n\ntweet2 = '@la_muse88 Ça dépendra des résultats du 1er tour...si LR est en tête pas de confinement avant le 2e tour, sinon....#OnVousRepond #France2'\n\ntweet3 = '''1- 50% des malades en réanimation on moins de 65ans\n2- Est-ce que le fait d'être fumeur est un facteur à risque ?\n3-Pourquoi les bureaux de tabac restent ouverts (non alimentaire)? \n #michelcimes #France2 #OnVousRepond #COVIDー19'''\n\nwhile 1:\n    response = get_ollama_structured_response(build_analysis_prompt(tweet2), TweetAnalysis)\n    print(response)\n    response = get_ollama_structured_response(build_analysis_prompt(tweet1), TweetAnalysis)\n    print(response)\n    response = get_ollama_structured_response(build_analysis_prompt(tweet3), TweetAnalysis)\n    print(response)\n```"
      },
      {
        "user": "rick-github",
        "body": "```python\n#!/usr/bin/env python3\n\nfrom pydantic import BaseModel\nimport ollama\n\nollama_url = \"http://localhost:11434\"\nmodel = \"gemma3:12b\"\n\nclass result(BaseModel):\n    parmeter1: str\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"Extract the information.\"},\n    {\"role\": \"user\", \"content\": \"parameter 1 is 'hello'.\"},\n]\n\ndef f():\n  response = ollama.chat(\n      model=model,\n      messages=messages,\n      format=result.model_json_schema())\n  r = result.model_validate_json(response.message.content)\n  print(r)\n\nf()\n```\n\n![Image](https://github.com/user-attachments/assets/8b7f96fd-08ed-458a-9bd8-412cbed967a1)\n\nLinear increase points to unreleased buffer,"
      },
      {
        "user": "ParthSareen",
        "body": "Sorry about that! And thanks @leokeba @rick-github for the logs and the data. Will look into it"
      }
    ]
  },
  {
    "issue_number": 11075,
    "title": "TooFunction declaration should allow to refer types from `$ref` declarations",
    "author": "kudla",
    "state": "open",
    "created_at": "2025-06-15T01:21:30Z",
    "updated_at": "2025-06-15T01:21:30Z",
    "labels": [
      "feature request"
    ],
    "body": "Hey guys\n\nAs there's declared option to define custom type `$defs`section within `ToolFunction` structure still there's no possibility to refer them from the `Properties` declarations with existing structure.\n\nSo should this work extending the structure with `Ref` property.\n\nI've create a pull request with a change #11074\n\nThank you",
    "comments": []
  },
  {
    "issue_number": 5984,
    "title": "[Linux] Install ollama to /home/user/ space",
    "author": "bebyx",
    "state": "closed",
    "created_at": "2024-07-26T13:48:42Z",
    "updated_at": "2025-06-14T23:43:24Z",
    "labels": [
      "feature request"
    ],
    "body": "I have several partitions on my Linux machine. My system partition (`/`) is small and I didn't have enough space for models. However, I have a lot of space in the `/home/` partition. \r\n\r\nSetting `$OLLAMA_MODELS` to `/home/user/.ollama/models` didn't help; it still installed them to `/usr/share/ollama/.ollama/models`. \r\n\r\nI didn't see an option for installing **ollama** to the home space (somewhere under `/home/user/`). So, I created a [script](https://gist.github.com/bebyx/8cecd83a2d7c94a8f7c6352298db7c9e) to do that for my convenience.\r\n\r\n\r\n```bash\r\n#!/bin/bash\r\n\r\ncurl -L https://ollama.com/download/ollama-linux-amd64 -o $HOME/.local/bin/ollama\r\nchmod +x $HOME/.local/bin/ollama\r\n\r\ncat <<EOF > ~/.config/systemd/user/ollama.service\r\n[Unit]\r\nDescription=Ollama Service\r\nAfter=network-online.target\r\n\r\n[Service]\r\nExecStart=$HOME/.local/bin/ollama serve\r\nRestart=always\r\nRestartSec=3\r\n\r\n[Install]\r\nWantedBy=default.target\r\nEOF\r\n\r\nsystemctl --user daemon-reload\r\nsystemctl --user restart ollama\r\n```\r\n\r\nJust wondering if you would consider providing an alternative script to install **ollama** to the home user space. My script can be used as a foundation.",
    "comments": [
      {
        "user": "jmorganca",
        "body": "Hi @bebyx, sorry the install script isn't flexible enough to support that. Check out the manual install instructions which aren't much more involved (just make sure to install your Nvidia/AMD drivers if you have a GPU): https://github.com/ollama/ollama/blob/main/docs/linux.md#manual-install"
      },
      {
        "user": "dewdgi",
        "body": "Is there a reason why ollama can't provide something like \n```[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/bin/ollama serve\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=$PATH\"\n\n[Install]\nWantedBy=default.target\n```\nin **/etc/systemd/user/ollama.service**?\nIt is very useful, if i have separated /home partition and i want to install ollama models in my user directory. \nIf ollama is launched under user, i don't need to adjust anything like environment variables or permissions"
      }
    ]
  },
  {
    "issue_number": 433,
    "title": "Add a way for user to approve CORS requests from origins without running a separate `ollama serve`",
    "author": "jmorganca",
    "state": "open",
    "created_at": "2023-08-27T19:14:09Z",
    "updated_at": "2025-06-14T20:48:09Z",
    "labels": [
      "feature request"
    ],
    "body": "Quite a few tools built on Ollama are hosted on alternate origins like `chrome-extension://` (chrome extensions) or `http://example.com` (websites that want to use the user's local Ollama instance).\r\n\r\nTools like Figma, Zoom and Slack have a way to \"allow\" the browser clients to interact with a server hosted on `localhost` by prompting the user once to approve an origin and then remembering this decision. We would need to be careful with the form factor here, but a dialog window (similar to Apple's [universal links](https://developer.apple.com/documentation/xcode/allowing-apps-and-websites-to-link-to-your-content)) could be a helpful starting point.",
    "comments": [
      {
        "user": "f0rodo",
        "body": "Localhost does work btw. Just not `chrome-extensions://` which honestly is probably a good privacy policy. Chrome extensions are notorious for data/privacy abuse. https://github.com/extesy/hoverzoom/discussions/670 "
      },
      {
        "user": "Nols1000",
        "body": "Hi I've added a PR that addresses this issue. It would be nice, if you could take a look and provide feedback."
      },
      {
        "user": "nikhiljohn10",
        "body": "What about including a question/checkbox in install script to allow browser extension access? If allowed, then add browser extension URL patterns to `OLLAMA_ORIGINS` by default. Only possible in windows and Linux, i suppose. Not sure about mac users since its just zip file and drag and drop to install. Any thoughts anyone?"
      }
    ]
  },
  {
    "issue_number": 9944,
    "title": "Allow BF16 and F32 model import from tensors files without F16 conversion",
    "author": "rjmalagon",
    "state": "open",
    "created_at": "2025-03-22T16:15:57Z",
    "updated_at": "2025-06-14T15:45:39Z",
    "labels": [
      "feature request"
    ],
    "body": "When creating a model from BF16 tensors with `ollama create`, it defaults to F16 conversion, even if `-q bf16` switch is provided, where it first converts to F16 at load and then quantize it to BF16. While this is textually correct from ´-q´ switch use, there is not a proper switch to import BF16 or F32 without F16 conversion.  \n\nMany models on tensor/safetensors already come on BF16 (and F32), it is a somewhat a downgrade to first convert to F32/F16 if F32/BF16 (or pure F32/F32) is intended.\n\n```\nollama create -f Modelfile-gemma3 -q bf16 gemma-3:12-it-bf16\ngathering model components \ncopying file sha256:50b2f405ba56a26d4913fd772089992252d7f942123cc0a034d96424221ba946 100% \ncopying file sha256:788cc42a1a92835df62d9a3791f47105f63504c7c404637a73288e9b11bc7b82 100% \ncopying file sha256:bfe25c2735e395407beb78456ea9a6984a1f00d8c16fa04a8b75f2a614cf53e1 100% \ncopying file sha256:ed14bd4908c98fed9f61e8cd410167e0846de9abd78e0452ab092072e5d9252d 100% \ncopying file sha256:f688d6bb20c5017601c4011de7ca656da8485b540b05013efdaf986c0fcc918d 100% \ncopying file sha256:3ffd5f11778dc73e2b69b3c00535e4121e1badf7018136263cd17b5b34fbaa53 100% \ncopying file sha256:2f7b0adf4fb469770bb1490e3e35df87b1dc578246c5e7e6fc76ecf33213a397 100% \ncopying file sha256:4667f2089529e8e7657cfb6d1c19910ae71ff5f28aa7ab2ff2763330affad795 100% \ncopying file sha256:4847447e92599833e8dbaa3067cd201c3bb5c052efa91f11ba891e43234f7832 100% \ncopying file sha256:891bd54eed03cba9ee1e705533a02a8217fcc29f356e4a1f53e5fd0d178883ad 100% \ncopying file sha256:7cee411d9d57324e50ce064a192cc5a858276d508611b12fc599e0c9767112e0 100% \ncopying file sha256:8bc75a29a730c9e743cad013feda3b0991a913fafe787c58a1c6e20afad97723 100% \ncopying file sha256:fe16baf728db49457cde32802cd7efc0ac8a7a9877dbe22fe3322b2d9dc6ccd9 100% \ncopying file sha256:39172c4124d3470341bbbb25f2926fd97edf68f0fe3a9fa4cde6acb9b7ed2cc6 100% \ncopying file sha256:fd9324becc53c4be610db39e13a613006f09fd6ef71a95fb6320dc33157490a3 100% \ncopying file sha256:1299c11d7cf632ef3b4e11937501358ada021bbdf7c47638d13c0ee982f2e79c 100% \nconverting model \nquantizing F16 model to BF16\ncreating new layer sha256:52201f498c01049fdbce5e05094db5b868ff23704baf2571cf4d071967b51920 \nusing existing layer sha256:e0a42594d802e5d31cdc786deb4823edb8adff66094d49de8fffe976d753e348 \nusing existing layer sha256:dd084c7d92a3c1c14cc09ae77153b903fd2024b64a100a0cc8ec9316063d2dbc \nusing existing layer sha256:d3a76cb8c4a07d0a6c82ac6e839f98816b5077699d393b2cc77008c16d8078ac \nwriting manifest \nsuccess \n```\n\nAs a note, this same example with llama.cpp convert_hf_to_gguf.py script converts torch.bfloat16 to F32/BF16\n\n```\nINFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {5120, 131072}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {32768, 5120}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 32768}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 32768}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1024}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 5120}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 4096}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1024}\n```\n\nI already tried to mod the \"reader_safetensors.go\" file, but my lack of GO dev skills only ugly butch the routine.\n",
    "comments": [
      {
        "user": "rjmalagon",
        "body": "I see this line 258 on server/server.go https://github.com/ollama/ollama/blob/ce929984a33230269905e0e3cfa335cb8d6ba781/server/create.go#L258 \n¿Is this the only part needed to change to select higher precision on safetensors import? \n\nIt would be nice to autodetect the origin model precision to select according to it, or give the user a switch to select desirable intermediate conversion precision."
      },
      {
        "user": "rjmalagon",
        "body": "Well, I already found out how to get F32 convert before quantization.\nThis line can be changed to force F32 on all, ugly and hacky but works. The best I can do as a golang muggle.\n\nhttps://github.com/ollama/ollama/blob/ce929984a33230269905e0e3cfa335cb8d6ba781/convert/reader.go#L50"
      },
      {
        "user": "pdevine",
        "body": "This is definitely something I'd like to get done soon. I actually started a partial implementation the other day and it ended up being a bigger change than I was anticipating. It's easy enough to hack in the BF16 conversion part, but the BF16 backend isn't actually compiled in yet so you can't actually run the weights after you've converted them."
      }
    ]
  },
  {
    "issue_number": 9911,
    "title": "panic: interface conversion: interface {} is *ggml.array, not uint32",
    "author": "dpk-it",
    "state": "closed",
    "created_at": "2025-03-20T16:02:03Z",
    "updated_at": "2025-06-14T14:25:10Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nCrashes while loading model with error \"panic: interface conversion: interface {} is *ggml.array, not uint32\"\n\nENV\n - WSL2\n - docker \n - model: hf.co/bartowski/nvidia_Llama-3_3-Nemotron-Super-49B-v1-GGUF:Q5_K_L\n\n\n### Relevant log output\n\n```shell\n2025/03/20 15:28:55 routes.go:1230: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:true OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE:q8_0 OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-03-20T15:28:55.339Z level=INFO source=images.go:432 msg=\"total blobs: 178\"\ntime=2025-03-20T15:28:55.342Z level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\ntime=2025-03-20T15:28:55.343Z level=INFO source=routes.go:1297 msg=\"Listening on [::]:11434 (version 0.6.2)\"\ntime=2025-03-20T15:28:55.343Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-03-20T15:28:56.013Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-e8a01a94-7d0f-f68d-f1b9-6d652b29b486 library=cuda variant=v12 compute=12.0 driver=12.8 name=\"NVIDIA GeForce RTX 5090\" total=\"31.8 GiB\" available=\"30.1 GiB\"\ntime=2025-03-20T15:28:56.013Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-f5e9aa1d-8aae-882d-3c1a-8439274b917e library=cuda variant=v12 compute=8.9 driver=12.8 name=\"NVIDIA GeForce RTX 4070 Ti SUPER\" total=\"16.0 GiB\" available=\"14.7 GiB\"\ntime=2025-03-20T15:28:56.013Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-a3276781-03b0-19e3-9aff-f476adf829ef library=cuda variant=v12 compute=8.9 driver=12.8 name=\"NVIDIA GeForce RTX 4060 Ti\" total=\"16.0 GiB\" available=\"14.9 GiB\"\n[GIN] 2025/03/20 - 15:34:45 | 200 |     100.781µs |      172.18.0.3 | GET      \"/api/version\"\n[GIN] 2025/03/20 - 15:34:47 | 200 |      50.633µs |      172.18.0.3 | GET      \"/api/version\"\n[GIN] 2025/03/20 - 15:35:03 | 200 |   64.828297ms |      172.18.0.3 | GET      \"/api/tags\"\n[GIN] 2025/03/20 - 15:35:06 | 200 |    6.305021ms |      172.18.0.3 | GET      \"/api/tags\"\n[GIN] 2025/03/20 - 15:35:30 | 200 |  3.041023547s |      172.18.0.3 | DELETE   \"/api/delete\"\n[GIN] 2025/03/20 - 15:35:30 | 200 |     6.30305ms |      172.18.0.3 | GET      \"/api/tags\"\n[GIN] 2025/03/20 - 15:37:57 | 200 |    5.024087ms |      172.18.0.3 | GET      \"/api/tags\"\ntime=2025-03-20T15:38:11.231Z level=INFO source=download.go:176 msg=\"downloading 092076b88a67 in 37 1 GB part(s)\"\ntime=2025-03-20T15:59:30.820Z level=INFO source=download.go:176 msg=\"downloading b78301c0df4d in 1 38 B part(s)\"\ntime=2025-03-20T15:59:31.920Z level=INFO source=download.go:176 msg=\"downloading ad4b6174552e in 1 191 B part(s)\"\n[GIN] 2025/03/20 - 15:59:57 | 200 |         20m1s |      172.18.0.3 | POST     \"/api/pull\"\n[GIN] 2025/03/20 - 15:59:57 | 200 |    5.849528ms |      172.18.0.3 | GET      \"/api/tags\"\n[GIN] 2025/03/20 - 16:00:04 | 200 |       49.06µs |      172.18.0.3 | GET      \"/api/version\"\ntime=2025-03-20T16:00:14.761Z level=WARN source=ggml.go:149 msg=\"key not found\" key=deci.vision.block_count default=0\npanic: interface conversion: interface {} is *ggml.array, not uint32\n\ngoroutine 29 [running]:\ngithub.com/ollama/ollama/fs/ggml.keyValue[...](0xc000e80540, {0x5598eeaabd4d, 0x14}, {0xc00070fcf0, 0x1, 0x0})\n        github.com/ollama/ollama/fs/ggml/ggml.go:146 +0x2de\ngithub.com/ollama/ollama/fs/ggml.KV.Uint(...)\n        github.com/ollama/ollama/fs/ggml/ggml.go:96\ngithub.com/ollama/ollama/fs/ggml.KV.HeadCount(...)\n        github.com/ollama/ollama/fs/ggml/ggml.go:56\ngithub.com/ollama/ollama/fs/ggml.KV.EmbeddingHeadCount(0xc000e80540)\n        github.com/ollama/ollama/fs/ggml/ggml.go:64 +0x5e\ngithub.com/ollama/ollama/fs/ggml.KV.EmbeddingHeadCountK(0xc000e80540)\n        github.com/ollama/ollama/fs/ggml/ggml.go:72 +0x18\ngithub.com/ollama/ollama/fs/ggml.GGML.SupportsFlashAttention({{0x5598eef1f0e8?, 0xc000142fa0?}, {0x5598eef1f098?, 0xc0001c3808?}})\n        github.com/ollama/ollama/fs/ggml/ggml.go:648 +0x159\ngithub.com/ollama/ollama/llm.EstimateGPULayers({_, _, _}, _, {_, _, _}, {{0x2000, 0x200, 0xffffffffffffffff, ...}, ...})\n        github.com/ollama/ollama/llm/memory.go:133 +0x568\ngithub.com/ollama/ollama/llm.PredictServerFit({0xc0006c3ba8?, 0x5598edc6bde5?, 0xc0006c38c0?}, 0xc000560b20, {0xc0006c3918?, _, _}, {0x0, 0x0, 0x0}, ...)\n        github.com/ollama/ollama/llm/memory.go:23 +0xbd\ngithub.com/ollama/ollama/server.pickBestFullFitByLibrary(0xc000150000, 0xc000560b20, {0xc0004fd508?, 0x3?, 0x4?}, 0xc000f27cf8)\n        github.com/ollama/ollama/server/sched.go:714 +0x6f3\ngithub.com/ollama/ollama/server.(*Scheduler).processPending(0xc000690060, {0x5598eef23020, 0xc0001b0af0})\n        github.com/ollama/ollama/server/sched.go:226 +0xe6b\ngithub.com/ollama/ollama/server.(*Scheduler).Run.func1()\n        github.com/ollama/ollama/server/sched.go:108 +0x1f\ncreated by github.com/ollama/ollama/server.(*Scheduler).Run in goroutine 1\n        github.com/ollama/ollama/server/sched.go:107 +0xb1\n```\n\n```\n2025/03/20 18:20:03 routes.go:1230: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-03-20T18:20:03.341Z level=INFO source=images.go:432 msg=\"total blobs: 176\"\ntime=2025-03-20T18:20:03.344Z level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\ntime=2025-03-20T18:20:03.346Z level=INFO source=routes.go:1297 msg=\"Listening on [::]:11434 (version 0.6.2)\"\ntime=2025-03-20T18:20:03.346Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-03-20T18:20:04.001Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-e8a01a94-7d0f-f68d-f1b9-6d652b29b486 library=cuda variant=v12 compute=12.0 driver=12.8 name=\"NVIDIA GeForce RTX 5090\" total=\"31.8 GiB\" available=\"30.1 GiB\"\ntime=2025-03-20T18:20:04.001Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-f5e9aa1d-8aae-882d-3c1a-8439274b917e library=cuda variant=v12 compute=8.9 driver=12.8 name=\"NVIDIA GeForce RTX 4070 Ti SUPER\" total=\"16.0 GiB\" available=\"14.7 GiB\"\ntime=2025-03-20T18:20:04.001Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-a3276781-03b0-19e3-9aff-f476adf829ef library=cuda variant=v12 compute=8.9 driver=12.8 name=\"NVIDIA GeForce RTX 4060 Ti\" total=\"16.0 GiB\" available=\"14.9 GiB\"\ntime=2025-03-20T18:20:26.207Z level=WARN source=ggml.go:149 msg=\"key not found\" key=deci.vision.block_count default=0\npanic: interface conversion: interface {} is *ggml.array, not uint32\n\ngoroutine 53 [running]:\ngithub.com/ollama/ollama/fs/ggml.keyValue[...](0xc0004e8600, {0x555833920d4d, 0x14}, {0xc000657890, 0x1, 0x555833cb4760})\n        github.com/ollama/ollama/fs/ggml/ggml.go:146 +0x2de\ngithub.com/ollama/ollama/fs/ggml.KV.Uint(...)\n        github.com/ollama/ollama/fs/ggml/ggml.go:96\ngithub.com/ollama/ollama/fs/ggml.KV.HeadCount(...)\n        github.com/ollama/ollama/fs/ggml/ggml.go:56\ngithub.com/ollama/ollama/fs/ggml.GGML.GraphSize({{0x555833d940e8?, 0xc000122be0?}, {0x555833d94098?, 0xc0001c3808?}}, 0x2000, 0x200, {0x0, 0x0})\n        github.com/ollama/ollama/fs/ggml/ggml.go:418 +0x137\ngithub.com/ollama/ollama/llm.EstimateGPULayers({_, _, _}, _, {_, _, _}, {{0x2000, 0x200, 0xffffffffffffffff, ...}, ...})\n        github.com/ollama/ollama/llm/memory.go:140 +0x659\ngithub.com/ollama/ollama/llm.PredictServerFit({0xc00061dba8?, 0x555832ae0de5?, 0xc00061d8c0?}, 0xc0002b6be0, {0xc00061d918?, _, _}, {0x0, 0x0, 0x0}, ...)\n        github.com/ollama/ollama/llm/memory.go:23 +0xbd\ngithub.com/ollama/ollama/server.pickBestFullFitByLibrary(0xc000134000, 0xc0002b6be0, {0xc0004cb508?, 0x3?, 0x4?}, 0xc000047cf8)\n        github.com/ollama/ollama/server/sched.go:714 +0x6f3\ngithub.com/ollama/ollama/server.(*Scheduler).processPending(0xc000422060, {0x555833d98020, 0xc00013e140})\n        github.com/ollama/ollama/server/sched.go:226 +0xe6b\ngithub.com/ollama/ollama/server.(*Scheduler).Run.func1()\n        github.com/ollama/ollama/server/sched.go:108 +0x1f\ncreated by github.com/ollama/ollama/server.(*Scheduler).Run in goroutine 1\n        github.com/ollama/ollama/server/sched.go:107 +0xb1\n```\n\n### OS\n\nDocker\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.6.2",
    "comments": [
      {
        "user": "nickheyer",
        "body": "yep, same: interface conversion: interface {} is *ggml.array, not uint32"
      },
      {
        "user": "dpk-it",
        "body": "@pdevine I think it's better to leave the bug open until the issue is fixed"
      },
      {
        "user": "pdevine",
        "body": "Hey @dpk-it thanks for posting the issue.  There were a bunch of nemotron related issues I was trying to consolidate into one bug. Having multiple issues for the same model makes the issue tracker super difficult to manage.\n\nThat said, looking through the trace it looks like it failed looking for `key=deci.vision.block_count` which is not something we support. I'm not sure how bartowski is converting these images.\n"
      }
    ]
  },
  {
    "issue_number": 1890,
    "title": "A way to update all downloaded models",
    "author": "Zig1375",
    "state": "open",
    "created_at": "2024-01-10T10:03:21Z",
    "updated_at": "2025-06-14T10:56:57Z",
    "labels": [
      "feature request"
    ],
    "body": "I'd like to have a way to update all downloaded models.\r\nRight now I have to pull each model separately.",
    "comments": [
      {
        "user": "rgaidot",
        "body": "You can update all models like this:\r\n\r\n```bash\r\n#!/bin/bash\r\n\r\nollama list | tail -n +2 | awk '{print $1}' | while read -r model; do\r\n  ollama pull $model\r\ndone\r\n```"
      },
      {
        "user": "pdevine",
        "body": "See #2179 "
      },
      {
        "user": "yarons",
        "body": "Another option (`awk` variant dependant but should work on most popular awk variants in both mac and Linux):\r\n```\r\nollama list | awk -F: 'NR>1 && !/reviewer/ {system(\"ollama pull \"$1)}'\r\n```\r\n\r\nExplanation:\r\n`ollama list` - lists all the models including the header line and the \"reviewer\" model (can't be updated).\r\n\r\n#### `awk`:\r\n* `-F :` - set the field separator to \":\" (this way we can capture the name of the model without the tag - **ollama3**:latest).\r\n* `NR > 1` - skip the first (header) line.\r\n* `&&` - \"and\" relation between the criteria.\r\n* `!/reviewer/` - filter out the `reviewer` model.\r\n* `system(\"ollama pull \"$1)` - will run a system command: `ollama pull <model>` where model is line dependant, this should run separately for every `$1` (first column separated by \":\") found.\r\n\r\nTo pull with the tag simply remove the `-F:`:\r\n```\r\nollama list | awk 'NR>1 && !/reviewer/ {system(\"ollama pull \"$1)}'\r\n```"
      }
    ]
  },
  {
    "issue_number": 11067,
    "title": "Can CPU-GPU mixed run in 8 x 4090D about deepseek-r1:671b-fp16?",
    "author": "justlikebohr",
    "state": "open",
    "created_at": "2025-06-13T16:01:25Z",
    "updated_at": "2025-06-14T04:08:14Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI need to run the deepseek-r1:671b-fp16 used GPU, but it seems to use cpu to load all layers. I use deepseek-r1:1.5b to check the ollama and GPU both can run before. And I guess the model is too large to load in GPU(24GB per GPU), but I need to find a way to run the model in mixed way, at least load one layer in GPU. Hope to reply, thanks.\n\n### Relevant log output\n\n```shell\nllama_model_loader: - kv   3:                      deepseek2.block_count u32              = 61\nllama_model_loader: - kv   4:                   deepseek2.context_length u32              = 163840\nllama_model_loader: - kv   5:                 deepseek2.embedding_length u32              = 7168\nllama_model_loader: - kv   6:              deepseek2.feed_forward_length u32              = 18432\nllama_model_loader: - kv   7:             deepseek2.attention.head_count u32              = 128\nllama_model_loader: - kv   8:          deepseek2.attention.head_count_kv u32              = 128\nllama_model_loader: - kv   9:                   deepseek2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  10: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  11:                deepseek2.expert_used_count u32              = 8\nllama_model_loader: - kv  12:                          general.file_type u32              = 1\nllama_model_loader: - kv  13:        deepseek2.leading_dense_block_count u32              = 3\nllama_model_loader: - kv  14:                       deepseek2.vocab_size u32              = 129280\nllama_model_loader: - kv  15:            deepseek2.attention.q_lora_rank u32              = 1536\nllama_model_loader: - kv  16:           deepseek2.attention.kv_lora_rank u32              = 512\nllama_model_loader: - kv  17:             deepseek2.attention.key_length u32              = 192\nllama_model_loader: - kv  18:           deepseek2.attention.value_length u32              = 128\nllama_model_loader: - kv  19:       deepseek2.expert_feed_forward_length u32              = 2048\nllama_model_loader: - kv  20:                     deepseek2.expert_count u32              = 256\nllama_model_loader: - kv  21:              deepseek2.expert_shared_count u32              = 1\nllama_model_loader: - kv  22:             deepseek2.expert_weights_scale f32              = 2.500000\nllama_model_loader: - kv  23:              deepseek2.expert_weights_norm bool             = true\nllama_model_loader: - kv  24:               deepseek2.expert_gating_func u32              = 2\nllama_model_loader: - kv  25:             deepseek2.rope.dimension_count u32              = 64\nllama_model_loader: - kv  26:                deepseek2.rope.scaling.type str              = yarn\nllama_model_loader: - kv  27:              deepseek2.rope.scaling.factor f32              = 40.000000\nllama_model_loader: - kv  28: deepseek2.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  29: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.100000\nllama_model_loader: - kv  30:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  31:                         tokenizer.ggml.pre str              = deepseek-v3\nllama_model_loader: - kv  32:                      tokenizer.ggml.tokens arr[str,129280]  = [\"<｜begin▁of▁sentence｜>\", \"<�...\nllama_model_loader: - kv  33:                  tokenizer.ggml.token_type arr[i32,129280]  = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  34:                      tokenizer.ggml.merges arr[str,127741]  = [\"Ġ t\", \"Ġ a\", \"i n\", \"Ġ Ġ\", \"h e...\nllama_model_loader: - kv  35:                tokenizer.ggml.bos_token_id u32              = 0\nllama_model_loader: - kv  36:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 1\nllama_model_loader: - kv  38:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  39:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  40:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  41:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  361 tensors\nllama_model_loader: - type  f16:  664 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 1250.08 GiB (16.00 BPW) \ninit_tokenizer: initializing tokenizer for type 2\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: layer   0 assigned to device CPU, is_swa = 0\nload_tensors: layer   1 assigned to device CPU, is_swa = 0\nload_tensors: layer   2 assigned to device CPU, is_swa = 0\nload_tensors: layer   3 assigned to device CPU, is_swa = 0\nload_tensors: layer   4 assigned to device CPU, is_swa = 0\nload_tensors: layer   5 assigned to device CPU, is_swa = 0\nload_tensors: layer   6 assigned to device CPU, is_swa = 0\nload_tensors: layer   7 assigned to device CPU, is_swa = 0\nload_tensors: layer   8 assigned to device CPU, is_swa = 0\nload_tensors: layer   9 assigned to device CPU, is_swa = 0\nload_tensors: layer  10 assigned to device CPU, is_swa = 0\nload_tensors: layer  11 assigned to device CPU, is_swa = 0\nload_tensors: layer  12 assigned to device CPU, is_swa = 0\nload_tensors: layer  13 assigned to device CPU, is_swa = 0\nload_tensors: layer  14 assigned to device CPU, is_swa = 0\nload_tensors: layer  15 assigned to device CPU, is_swa = 0\nload_tensors: layer  16 assigned to device CPU, is_swa = 0\nload_tensors: layer  17 assigned to device CPU, is_swa = 0\nload_tensors: layer  18 assigned to device CPU, is_swa = 0\nload_tensors: layer  19 assigned to device CPU, is_swa = 0\nload_tensors: layer  20 assigned to device CPU, is_swa = 0\nload_tensors: layer  21 assigned to device CPU, is_swa = 0\nload_tensors: layer  22 assigned to device CPU, is_swa = 0\nload_tensors: layer  23 assigned to device CPU, is_swa = 0\nload_tensors: layer  24 assigned to device CPU, is_swa = 0\nload_tensors: layer  25 assigned to device CPU, is_swa = 0\nload_tensors: layer  26 assigned to device CPU, is_swa = 0\nload_tensors: layer  27 assigned to device CPU, is_swa = 0\nload_tensors: layer  28 assigned to device CPU, is_swa = 0\nload_tensors: layer  29 assigned to device CPU, is_swa = 0\nload_tensors: layer  30 assigned to device CPU, is_swa = 0\nload_tensors: layer  31 assigned to device CPU, is_swa = 0\nload_tensors: layer  32 assigned to device CPU, is_swa = 0\nload_tensors: layer  33 assigned to device CPU, is_swa = 0\nload_tensors: layer  34 assigned to device CPU, is_swa = 0\nload_tensors: layer  35 assigned to device CPU, is_swa = 0\nload_tensors: layer  36 assigned to device CPU, is_swa = 0\nload_tensors: layer  37 assigned to device CPU, is_swa = 0\nload_tensors: layer  38 assigned to device CPU, is_swa = 0\nload_tensors: layer  39 assigned to device CPU, is_swa = 0\nload_tensors: layer  40 assigned to device CPU, is_swa = 0\nload_tensors: layer  41 assigned to device CPU, is_swa = 0\nload_tensors: layer  42 assigned to device CPU, is_swa = 0\nload_tensors: layer  43 assigned to device CPU, is_swa = 0\nload_tensors: layer  44 assigned to device CPU, is_swa = 0\nload_tensors: layer  45 assigned to device CPU, is_swa = 0\nload_tensors: layer  46 assigned to device CPU, is_swa = 0\nload_tensors: layer  47 assigned to device CPU, is_swa = 0\nload_tensors: layer  48 assigned to device CPU, is_swa = 0\nload_tensors: layer  49 assigned to device CPU, is_swa = 0\nload_tensors: layer  50 assigned to device CPU, is_swa = 0\nload_tensors: layer  51 assigned to device CPU, is_swa = 0\nload_tensors: layer  52 assigned to device CPU, is_swa = 0\nload_tensors: layer  53 assigned to device CPU, is_swa = 0\nload_tensors: layer  54 assigned to device CPU, is_swa = 0\nload_tensors: layer  55 assigned to device CPU, is_swa = 0\nload_tensors: layer  56 assigned to device CPU, is_swa = 0\nload_tensors: layer  57 assigned to device CPU, is_swa = 0\nload_tensors: layer  58 assigned to device CPU, is_swa = 0\nload_tensors: layer  59 assigned to device CPU, is_swa = 0\nload_tensors: layer  60 assigned to device CPU, is_swa = 0\nload_tensors: layer  61 assigned to device CPU, is_swa = 0\nload_tensors:          CPU model buffer size = 1280086.27 MiB\nload_all_data: no device found for buffer type CPU for async uploads\ntime=2025-06-13T16:00:05.646Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "A full log will result in better analysis.  The likely cause is that context size or parallelism results in a memory graph too large to fit on a GPU."
      },
      {
        "user": "justlikebohr",
        "body": "Thanks, I show the log about 671b(runs in CPU) and 1.5b(runs in GPU) to compare why 671b can't run in GPU, and after hearing your advice, I try to modify the num_ctx from 4096 to 64, and it doesn't work yet, and the parallel_num is 1. \nCan I get a feasible solutions to modify the env_param or modify sources code to succeed in running 671b in CPU-GPU way.\n\n[log_serve_1.5b.log](https://github.com/user-attachments/files/20735498/log_serve_1.5b.log)\n[log_serve_671b.log](https://github.com/user-attachments/files/20735499/log_serve_671b.log)"
      }
    ]
  },
  {
    "issue_number": 11072,
    "title": "Exception in llama_decode after successful generations",
    "author": "ChadSexington",
    "state": "closed",
    "created_at": "2025-06-14T01:02:12Z",
    "updated_at": "2025-06-14T02:14:11Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI've hit the same issue that was hit in https://github.com/ollama/ollama/issues/9972 . I experience this crash with any model I've tried (gemma3, qwen3, llama3.2)\n\nNote that the crash does not always occur. If I provide a very simple prompt such as \"Hello\", the crash usually does not happen. Any complex prompt is likely to cause the crash.\n\nI attempted to run without GPU as suggested, which worked without issue, even with very complex prompts:\n```\n$ ollama run llama3.2:1b\n>>> /set parameter num_gpu 0\nSet parameter 'num_gpu' to '0'\n```\n\nI attempted to install an earlier version of ollama (v0.8.0) and hit the same issues.\n\n### Relevant log output\n\n```shell\ntime=2025-06-13T20:56:24.807-04:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\Tim\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-06-13T20:56:24.808-04:00 level=INFO source=images.go:479 msg=\"total blobs: 5\"\ntime=2025-06-13T20:56:24.808-04:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-13T20:56:24.809-04:00 level=INFO source=routes.go:1287 msg=\"Listening on 127.0.0.1:11434 (version 0.9.0)\"\ntime=2025-06-13T20:56:24.809-04:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-13T20:56:24.809-04:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-06-13T20:56:24.809-04:00 level=INFO source=gpu_windows.go:183 msg=\"efficiency cores detected\" maxEfficiencyClass=1\ntime=2025-06-13T20:56:24.809-04:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=20 efficiency=12 threads=28\ntime=2025-06-13T20:56:25.021-04:00 level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=rocm variant=\"\" compute=gfx1101 driver=6.0 name=\"AMD Radeon RX 7800 XT\" total=\"16.0 GiB\" available=\"15.9 GiB\"\n[GIN] 2025/06/13 - 20:56:28 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/13 - 20:56:28 | 200 |     34.5001ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-06-13T20:56:29.126-04:00 level=INFO source=sched.go:189 msg=\"one or more GPUs detected that are unable to accurately report free memory - disabling default concurrency\"\ntime=2025-06-13T20:56:29.158-04:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=C:\\Users\\Tim\\.ollama\\models\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=0 parallel=2 available=17028874240 required=\"2.5 GiB\"\ntime=2025-06-13T20:56:29.418-04:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"31.8 GiB\" free=\"18.5 GiB\" free_swap=\"15.5 GiB\"\ntime=2025-06-13T20:56:29.419-04:00 level=INFO source=server.go:168 msg=offload library=rocm layers.requested=-1 layers.model=17 layers.offload=17 layers.split=\"\" memory.available=\"[15.9 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"2.5 GiB\" memory.required.partial=\"2.5 GiB\" memory.required.kv=\"256.0 MiB\" memory.required.allocations=\"[2.5 GiB]\" memory.weights.total=\"1.2 GiB\" memory.weights.repeating=\"986.2 MiB\" memory.weights.nonrepeating=\"266.2 MiB\" memory.graph.full=\"544.0 MiB\" memory.graph.partial=\"554.3 MiB\"\nllama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\\Users\\Tim\\.ollama\\models\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\nllama_model_loader: - kv   5:                         general.size_label str              = 1B\nllama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   8:                          llama.block_count u32              = 16\nllama_model_loader: - kv   9:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048\nllama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\nllama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64\nllama_model_loader: - kv  17:               llama.attention.value_length u32              = 64\nllama_model_loader: - kv  18:                          general.file_type u32              = 7\nllama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   34 tensors\nllama_model_loader: - type q8_0:  113 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0\nprint_info: file size   = 1.22 GiB (8.50 BPW)\nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 1.24 B\nprint_info: general.name     = Llama 3.2 1B Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<|begin_of_text|>'\nprint_info: EOS token        = 128009 '<|eot_id|>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-13T20:56:29.672-04:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\Tim\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model C:\\\\Users\\\\Tim\\\\.ollama\\\\models\\\\blobs\\\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 8192 --batch-size 512 --n-gpu-layers 17 --threads 8 --parallel 2 --port 52482\"\ntime=2025-06-13T20:56:29.675-04:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-13T20:56:29.676-04:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-13T20:56:29.676-04:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-06-13T20:56:29.699-04:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\nload_backend: loaded CPU backend from C:\\Users\\Tim\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-alderlake.dll\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7800 XT, gfx1101 (0x1101), VMM: no, Wave Size: 32\nload_backend: loaded ROCm backend from C:\\Users\\Tim\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\rocm\\ggml-hip.dll\ntime=2025-06-13T20:56:29.730-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 ROCm.0.NO_VMM=1 ROCm.0.NO_PEER_COPY=1 ROCm.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)\ntime=2025-06-13T20:56:29.730-04:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:52482\"\nllama_model_load_from_file_impl: using device ROCm0 (AMD Radeon RX 7800 XT) - 16240 MiB free\ntime=2025-06-13T20:56:29.927-04:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from C:\\Users\\Tim\\.ollama\\models\\blobs\\sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\nllama_model_loader: - kv   5:                         general.size_label str              = 1B\nllama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   8:                          llama.block_count u32              = 16\nllama_model_loader: - kv   9:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048\nllama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\nllama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64\nllama_model_loader: - kv  17:               llama.attention.value_length u32              = 64\nllama_model_loader: - kv  18:                          general.file_type u32              = 7\nllama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   34 tensors\nllama_model_loader: - type q8_0:  113 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0\nprint_info: file size   = 1.22 GiB (8.50 BPW)\nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 16\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 8192\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 1B\nprint_info: model params     = 1.24 B\nprint_info: general.name     = Llama 3.2 1B Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<|begin_of_text|>'\nprint_info: EOS token        = 128009 '<|eot_id|>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 16 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 17/17 layers to GPU\nload_tensors:        ROCm0 model buffer size =  1252.41 MiB\nload_tensors:   CPU_Mapped model buffer size =   266.16 MiB\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 2\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 1024\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 500000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:  ROCm_Host  output buffer size =     0.99 MiB\nllama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1, padding = 32\nllama_kv_cache_unified:      ROCm0 KV buffer size =   256.00 MiB\nllama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\nllama_context:      ROCm0 compute buffer size =   544.00 MiB\nllama_context:  ROCm_Host compute buffer size =    20.01 MiB\nllama_context: graph nodes  = 550\nllama_context: graph splits = 2\ntime=2025-06-13T20:56:31.179-04:00 level=INFO source=server.go:630 msg=\"llama runner started in 1.50 seconds\"\n[GIN] 2025/06/13 - 20:56:31 | 200 |    2.3487354s |       127.0.0.1 | POST     \"/api/generate\"\nException 0xc0000005 0x0 0x300020000 0x7ff9414451fe\nPC=0x7ff9414451fe\nsignal arrived during external code execution\n\nruntime.cgocall(0x7ff78a3e5dc0, 0xc000507bd8)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/cgocall.go:167 +0x3e fp=0xc000507bb0 sp=0xc000507b48 pc=0x7ff78972241e\ngithub.com/ollama/ollama/llama._Cfunc_llama_decode(0x1f821e21820, {0x1, 0x1f84eb78020, 0x0, 0x1f84eb74ff0, 0x1f84eb79030, 0x1f82cf50450, 0x1f82cf76860})\n        _cgo_gotypes.go:597 +0x50 fp=0xc000507bd8 sp=0xc000507bb0 pc=0x7ff789ae7330\ngithub.com/ollama/ollama/llama.(*Context).Decode.func1(...)\n        C:/a/ollama/ollama/llama/llama.go:133\ngithub.com/ollama/ollama/llama.(*Context).Decode(0x7ff78b477480?, 0x1?)\n        C:/a/ollama/ollama/llama/llama.go:133 +0xed fp=0xc000507cc0 sp=0xc000507bd8 pc=0x7ff789ae97ed\ngithub.com/ollama/ollama/runner/llamarunner.(*Server).processBatch(0xc00052c000, 0xc0002801e0, 0xc000507f28)\n        C:/a/ollama/ollama/runner/llamarunner/runner.go:436 +0x209 fp=0xc000507ee8 sp=0xc000507cc0 pc=0x7ff789b9fc49\ngithub.com/ollama/ollama/runner/llamarunner.(*Server).run(0xc00052c000, {0x7ff78aac2540, 0xc000182fa0})\n        C:/a/ollama/ollama/runner/llamarunner/runner.go:341 +0x1bb fp=0xc000507fb8 sp=0xc000507ee8 pc=0x7ff789b9f8bb\ngithub.com/ollama/ollama/runner/llamarunner.Execute.gowrap2()\n        C:/a/ollama/ollama/runner/llamarunner/runner.go:855 +0x28 fp=0xc000507fe0 sp=0xc000507fb8 pc=0x7ff789ba4088\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000507fe8 sp=0xc000507fe0 pc=0x7ff78972cfe1\ncreated by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1\n        C:/a/ollama/ollama/runner/llamarunner/runner.go:855 +0xc37\n\ngoroutine 1 gp=0xc0000021c0 m=nil [IO wait]:\nruntime.gopark(0x7ff78972e7e0?, 0x7ff78b404220?, 0x20?, 0xe0?, 0xc00052e0cc?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc0005174c0 sp=0xc0005174a0 pc=0x7ff7897257ee\nruntime.netpollblock(0x34c?, 0x896c03c6?, 0xf7?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/netpoll.go:575 +0xf7 fp=0xc0005174f8 sp=0xc0005174c0 pc=0x7ff7896eb697\ninternal/poll.runtime_pollWait(0x1f8742db830, 0x72)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/netpoll.go:351 +0x85 fp=0xc000517518 sp=0xc0005174f8 pc=0x7ff789724985\ninternal/poll.(*pollDesc).wait(0x7ff7897b9773?, 0x7ff7896d1ed6?, 0x0)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000517540 sp=0xc000517518 pc=0x7ff7897bad67\ninternal/poll.execIO(0xc00052e020, 0xc0005175e8)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/internal/poll/fd_windows.go:177 +0x105 fp=0xc0005175b8 sp=0xc000517540 pc=0x7ff7897bc1c5\ninternal/poll.(*FD).acceptOne(0xc00052e008, 0x434, {0xc000238000?, 0xc000517648?, 0x7ff78972aff7?}, 0xc000517688?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/internal/poll/fd_windows.go:946 +0x65 fp=0xc000517618 sp=0xc0005175b8 pc=0x7ff7897c0745\ninternal/poll.(*FD).Accept(0xc00052e008, 0xc0005177c8)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/internal/poll/fd_windows.go:980 +0x1b6 fp=0xc0005176d0 sp=0xc000517618 pc=0x7ff7897c0a76\nnet.(*netFD).accept(0xc00052e008)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/fd_windows.go:182 +0x4b fp=0xc0005177e8 sp=0xc0005176d0 pc=0x7ff78983158b\nnet.(*TCPListener).accept(0xc0000b3600)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/tcpsock_posix.go:159 +0x1b fp=0xc000517838 sp=0xc0005177e8 pc=0x7ff7898475db\nnet.(*TCPListener).Accept(0xc0000b3600)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/tcpsock.go:380 +0x30 fp=0xc000517868 sp=0xc000517838 pc=0x7ff789846390\nnet/http.(*onceCloseListener).Accept(0xc000236000?)\n        <autogenerated>:1 +0x24 fp=0xc000517880 sp=0xc000517868 pc=0x7ff789a5f784\nnet/http.(*Server).Serve(0xc000524100, {0x7ff78aabff80, 0xc0000b3600})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:3424 +0x30c fp=0xc0005179b0 sp=0xc000517880 pc=0x7ff789a3704c\ngithub.com/ollama/ollama/runner/llamarunner.Execute({0xc0000d2020, 0xe, 0xe})\n        C:/a/ollama/ollama/runner/llamarunner/runner.go:875 +0x100a fp=0xc000517d08 sp=0xc0005179b0 pc=0x7ff789ba3dca\ngithub.com/ollama/ollama/runner.Execute({0xc0000d2010?, 0x0?, 0x0?})\n        C:/a/ollama/ollama/runner/runner.go:22 +0xd4 fp=0xc000517d30 sp=0xc000517d08 pc=0x7ff789c22b34\ngithub.com/ollama/ollama/cmd.NewCLI.func2(0xc0000d3500?, {0x7ff78a8e8adb?, 0x4?, 0x7ff78a8e8adf?})\n        C:/a/ollama/ollama/cmd/cmd.go:1529 +0x45 fp=0xc000517d58 sp=0xc000517d30 pc=0x7ff78a374785\ngithub.com/spf13/cobra.(*Command).execute(0xc00074ef08, {0xc000001420, 0xe, 0xe})\n        C:/Users/runneradmin/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc000517e78 sp=0xc000517d58 pc=0x7ff7898ac05c\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc000720f08)\n        C:/Users/runneradmin/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc000517f30 sp=0xc000517e78 pc=0x7ff7898ac8a5\ngithub.com/spf13/cobra.(*Command).Execute(...)\n        C:/Users/runneradmin/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\ngithub.com/spf13/cobra.(*Command).ExecuteContext(...)\n        C:/Users/runneradmin/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:985\nmain.main()\n        C:/a/ollama/ollama/main.go:12 +0x4d fp=0xc000517f50 sp=0xc000517f30 pc=0x7ff78a37520d\nruntime.main()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:283 +0x27d fp=0xc000517fe0 sp=0xc000517f50 pc=0x7ff7896f467d\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000517fe8 sp=0xc000517fe0 pc=0x7ff78972cfe1\n\ngoroutine 2 gp=0xc0000028c0 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000099fa8 sp=0xc000099f88 pc=0x7ff7897257ee\nruntime.goparkunlock(...)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:441\nruntime.forcegchelper()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:348 +0xb8 fp=0xc000099fe0 sp=0xc000099fa8 pc=0x7ff7896f4998\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000099fe8 sp=0xc000099fe0 pc=0x7ff78972cfe1\ncreated by runtime.init.7 in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:336 +0x1a\n\ngoroutine 3 gp=0xc000002c40 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00009bf80 sp=0xc00009bf60 pc=0x7ff7897257ee\nruntime.goparkunlock(...)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:441\nruntime.bgsweep(0xc0000a8000)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgcsweep.go:316 +0xdf fp=0xc00009bfc8 sp=0xc00009bf80 pc=0x7ff7896dd75f\nruntime.gcenable.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:204 +0x25 fp=0xc00009bfe0 sp=0xc00009bfc8 pc=0x7ff7896d1b25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00009bfe8 sp=0xc00009bfe0 pc=0x7ff78972cfe1\ncreated by runtime.gcenable in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:204 +0x66\n\ngoroutine 4 gp=0xc000002e00 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x7ff78aaad728?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc0000aff78 sp=0xc0000aff58 pc=0x7ff7897257ee\nruntime.goparkunlock(...)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:441\nruntime.(*scavengerState).park(0x7ff78b42a920)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgcscavenge.go:425 +0x49 fp=0xc0000affa8 sp=0xc0000aff78 pc=0x7ff7896db1a9\nruntime.bgscavenge(0xc0000a8000)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgcscavenge.go:658 +0x59 fp=0xc0000affc8 sp=0xc0000affa8 pc=0x7ff7896db739\nruntime.gcenable.gowrap2()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:205 +0x25 fp=0xc0000affe0 sp=0xc0000affc8 pc=0x7ff7896d1ac5\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000affe8 sp=0xc0000affe0 pc=0x7ff78972cfe1\ncreated by runtime.gcenable in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:205 +0xa5\n\ngoroutine 5 gp=0xc000003340 m=nil [finalizer wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc0000b1e30 sp=0xc0000b1e10 pc=0x7ff7897257ee\nruntime.runfinq()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mfinal.go:196 +0x107 fp=0xc0000b1fe0 sp=0xc0000b1e30 pc=0x7ff7896d0aa7\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000b1fe8 sp=0xc0000b1fe0 pc=0x7ff78972cfe1\ncreated by runtime.createfing in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mfinal.go:166 +0x3d\n\ngoroutine 6 gp=0xc000003dc0 m=nil [chan receive]:\nruntime.gopark(0xc000187860?, 0xc0004ce018?, 0x60?, 0xdf?, 0x7ff78981a5c8?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00009df18 sp=0xc00009def8 pc=0x7ff7897257ee\nruntime.chanrecv(0xc0000b63f0, 0x0, 0x1)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/chan.go:664 +0x445 fp=0xc00009df90 sp=0xc00009df18 pc=0x7ff7896c2d05\nruntime.chanrecv1(0x7ff7896f47e0?, 0xc00009df76?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/chan.go:506 +0x12 fp=0xc00009dfb8 sp=0xc00009df90 pc=0x7ff7896c2892\nruntime.unique_runtime_registerUniqueMapCleanup.func2(...)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1796\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1799 +0x2f fp=0xc00009dfe0 sp=0xc00009dfb8 pc=0x7ff7896d4d4f\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00009dfe8 sp=0xc00009dfe0 pc=0x7ff78972cfe1\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1794 +0x85\n\ngoroutine 7 gp=0xc0004181c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc0000abf38 sp=0xc0000abf18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0000abfc8 sp=0xc0000abf38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc0000abfe0 sp=0xc0000abfc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000abfe8 sp=0xc0000abfe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 18 gp=0xc0001061c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000113f38 sp=0xc000113f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000113fc8 sp=0xc000113f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000113fe0 sp=0xc000113fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000113fe8 sp=0xc000113fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 34 gp=0xc000484000 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00010ff38 sp=0xc00010ff18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00010ffc8 sp=0xc00010ff38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc00010ffe0 sp=0xc00010ffc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00010ffe8 sp=0xc00010ffe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 8 gp=0xc000418380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc0000adf38 sp=0xc0000adf18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0000adfc8 sp=0xc0000adf38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc0000adfe0 sp=0xc0000adfc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000adfe8 sp=0xc0000adfe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 9 gp=0xc000418540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000473f38 sp=0xc000473f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000473fc8 sp=0xc000473f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000473fe0 sp=0xc000473fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000473fe8 sp=0xc000473fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 19 gp=0xc000106380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000115f38 sp=0xc000115f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000115fc8 sp=0xc000115f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000115fe0 sp=0xc000115fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000115fe8 sp=0xc000115fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 35 gp=0xc0004841c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000111f38 sp=0xc000111f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000111fc8 sp=0xc000111f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000111fe0 sp=0xc000111fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000111fe8 sp=0xc000111fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 10 gp=0xc000418700 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000475f38 sp=0xc000475f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000475fc8 sp=0xc000475f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000475fe0 sp=0xc000475fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000475fe8 sp=0xc000475fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 20 gp=0xc000106540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00046ff38 sp=0xc00046ff18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00046ffc8 sp=0xc00046ff38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc00046ffe0 sp=0xc00046ffc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00046ffe8 sp=0xc00046ffe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 36 gp=0xc000484380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00048bf38 sp=0xc00048bf18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00048bfc8 sp=0xc00048bf38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc00048bfe0 sp=0xc00048bfc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00048bfe8 sp=0xc00048bfe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 11 gp=0xc0004188c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000487f38 sp=0xc000487f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000487fc8 sp=0xc000487f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000487fe0 sp=0xc000487fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000487fe8 sp=0xc000487fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 12 gp=0xc000418a80 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000489f38 sp=0xc000489f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000489fc8 sp=0xc000489f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000489fe0 sp=0xc000489fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000489fe8 sp=0xc000489fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 21 gp=0xc000106700 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000471f38 sp=0xc000471f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000471fc8 sp=0xc000471f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000471fe0 sp=0xc000471fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000471fe8 sp=0xc000471fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 37 gp=0xc000484540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00048df38 sp=0xc00048df18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00048dfc8 sp=0xc00048df38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc00048dfe0 sp=0xc00048dfc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00048dfe8 sp=0xc00048dfe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 13 gp=0xc000418c40 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00047bf38 sp=0xc00047bf18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00047bfc8 sp=0xc00047bf38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc00047bfe0 sp=0xc00047bfc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00047bfe8 sp=0xc00047bfe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 22 gp=0xc0001068c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000477f38 sp=0xc000477f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000477fc8 sp=0xc000477f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000477fe0 sp=0xc000477fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000477fe8 sp=0xc000477fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 38 gp=0xc000484700 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000495f38 sp=0xc000495f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000495fc8 sp=0xc000495f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000495fe0 sp=0xc000495fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000495fe8 sp=0xc000495fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 14 gp=0xc000418e00 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00047df38 sp=0xc00047df18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00047dfc8 sp=0xc00047df38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc00047dfe0 sp=0xc00047dfc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00047dfe8 sp=0xc00047dfe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 23 gp=0xc000106a80 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000479f38 sp=0xc000479f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000479fc8 sp=0xc000479f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000479fe0 sp=0xc000479fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000479fe8 sp=0xc000479fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 39 gp=0xc0004848c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000497f38 sp=0xc000497f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000497fc8 sp=0xc000497f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000497fe0 sp=0xc000497fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000497fe8 sp=0xc000497fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 15 gp=0xc000418fc0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000491f38 sp=0xc000491f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000491fc8 sp=0xc000491f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000491fe0 sp=0xc000491fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000491fe8 sp=0xc000491fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 24 gp=0xc000106c40 m=nil [GC worker (idle)]:\nruntime.gopark(0xe76cae8197c?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00011df38 sp=0xc00011df18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00011dfc8 sp=0xc00011df38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc00011dfe0 sp=0xc00011dfc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00011dfe8 sp=0xc00011dfe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 40 gp=0xc000484a80 m=nil [GC worker (idle)]:\nruntime.gopark(0xe76cae8197c?, 0x0?, 0x0?, 0x0?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000119f38 sp=0xc000119f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000119fc8 sp=0xc000119f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000119fe0 sp=0xc000119fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000119fe8 sp=0xc000119fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 16 gp=0xc000419180 m=nil [GC worker (idle)]:\nruntime.gopark(0xe76cae8197c?, 0x1?, 0x78?, 0xd5?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000493f38 sp=0xc000493f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000493fc8 sp=0xc000493f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000493fe0 sp=0xc000493fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000493fe8 sp=0xc000493fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 25 gp=0xc000106e00 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff78b479400?, 0x1?, 0x78?, 0xd5?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00011ff38 sp=0xc00011ff18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00011ffc8 sp=0xc00011ff38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc00011ffe0 sp=0xc00011ffc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00011ffe8 sp=0xc00011ffe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 41 gp=0xc000484c40 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff78b479400?, 0x1?, 0x78?, 0xd5?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00011bf38 sp=0xc00011bf18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00011bfc8 sp=0xc00011bf38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc00011bfe0 sp=0xc00011bfc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00011bfe8 sp=0xc00011bfe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 50 gp=0xc000419340 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff78b479400?, 0x1?, 0x78?, 0xd5?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000505f38 sp=0xc000505f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000505fc8 sp=0xc000505f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000505fe0 sp=0xc000505fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000505fe8 sp=0xc000505fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 26 gp=0xc000106fc0 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff78b479400?, 0x1?, 0x78?, 0xd5?, 0x0?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000501f38 sp=0xc000501f18 pc=0x7ff7897257ee\nruntime.gcBgMarkWorker(0xc0000b7810)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000501fc8 sp=0xc000501f38 pc=0x7ff7896d4049\nruntime.gcBgMarkStartWorkers.gowrap1()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x25 fp=0xc000501fe0 sp=0xc000501fc8 pc=0x7ff7896d3f25\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000501fe8 sp=0xc000501fe0 pc=0x7ff78972cfe1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 42 gp=0xc000484fc0 m=nil [select]:\nruntime.gopark(0xc00004ba78?, 0x2?, 0x4?, 0x0?, 0xc00004b8c4?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc00004b6f8 sp=0xc00004b6d8 pc=0x7ff7897257ee\nruntime.selectgo(0xc00004ba78, 0xc00004b8c0, 0xc00076d4c0?, 0x0, 0x1?, 0x1)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/select.go:351 +0x837 fp=0xc00004b830 sp=0xc00004b6f8 pc=0x7ff789705cd7\ngithub.com/ollama/ollama/runner/llamarunner.(*Server).completion(0xc00052c000, {0x7ff78aac0130, 0xc0001360e0}, 0xc00012e140)\n        C:/a/ollama/ollama/runner/llamarunner/runner.go:624 +0xb34 fp=0xc00004bac0 sp=0xc00004b830 pc=0x7ff789ba1834\ngithub.com/ollama/ollama/runner/llamarunner.(*Server).completion-fm({0x7ff78aac0130?, 0xc0001360e0?}, 0xc00004bb40?)\n        <autogenerated>:1 +0x36 fp=0xc00004baf0 sp=0xc00004bac0 pc=0x7ff789ba45f6\nnet/http.HandlerFunc.ServeHTTP(0xc000722180?, {0x7ff78aac0130?, 0xc0001360e0?}, 0xc00004bb60?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:2294 +0x29 fp=0xc00004bb18 sp=0xc00004baf0 pc=0x7ff789a33689\nnet/http.(*ServeMux).ServeHTTP(0x7ff7896cb025?, {0x7ff78aac0130, 0xc0001360e0}, 0xc00012e140)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:2822 +0x1c4 fp=0xc00004bb68 sp=0xc00004bb18 pc=0x7ff789a35584\nnet/http.serverHandler.ServeHTTP({0x7ff78aabc790?}, {0x7ff78aac0130?, 0xc0001360e0?}, 0x1?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:3301 +0x8e fp=0xc00004bb98 sp=0xc00004bb68 pc=0x7ff789a5300e\nnet/http.(*conn).serve(0xc000236000, {0x7ff78aac2508, 0xc0005223c0})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:2102 +0x625 fp=0xc00004bfb8 sp=0xc00004bb98 pc=0x7ff789a31b85\nnet/http.(*Server).Serve.gowrap3()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:3454 +0x28 fp=0xc00004bfe0 sp=0xc00004bfb8 pc=0x7ff789a37448\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00004bfe8 sp=0xc00004bfe0 pc=0x7ff78972cfe1\ncreated by net/http.(*Server).Serve in goroutine 1\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:3454 +0x485\n\ngoroutine 28 gp=0xc000485180 m=nil [IO wait]:\nruntime.gopark(0x0?, 0xc000230020?, 0xc8?, 0x0?, 0xc0002300cc?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/proc.go:435 +0xce fp=0xc000305d58 sp=0xc000305d38 pc=0x7ff7897257ee\nruntime.netpollblock(0x37c?, 0x896c03c6?, 0xf7?)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/netpoll.go:575 +0xf7 fp=0xc000305d90 sp=0xc000305d58 pc=0x7ff7896eb697\ninternal/poll.runtime_pollWait(0x1f8742db718, 0x72)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/netpoll.go:351 +0x85 fp=0xc000305db0 sp=0xc000305d90 pc=0x7ff789724985\ninternal/poll.(*pollDesc).wait(0x37c?, 0x72?, 0x0)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000305dd8 sp=0xc000305db0 pc=0x7ff7897bad67\ninternal/poll.execIO(0xc000230020, 0x7ff78a95ce50)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/internal/poll/fd_windows.go:177 +0x105 fp=0xc000305e50 sp=0xc000305dd8 pc=0x7ff7897bc1c5\ninternal/poll.(*FD).Read(0xc000230008, {0xc0002320d1, 0x1, 0x1})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/internal/poll/fd_windows.go:438 +0x29b fp=0xc000305ef0 sp=0xc000305e50 pc=0x7ff7897bce9b\nnet.(*netFD).Read(0xc000230008, {0xc0002320d1?, 0xc000282098?, 0xc000305f70?})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/fd_posix.go:55 +0x25 fp=0xc000305f38 sp=0xc000305ef0 pc=0x7ff78982f6a5\nnet.(*conn).Read(0xc000234000, {0xc0002320d1?, 0x0?, 0x0?})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/net.go:194 +0x45 fp=0xc000305f80 sp=0xc000305f38 pc=0x7ff78983eb85\nnet/http.(*connReader).backgroundRead(0xc0002320c0)\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:690 +0x37 fp=0xc000305fc8 sp=0xc000305f80 pc=0x7ff789a2ba57\nnet/http.(*connReader).startBackgroundRead.gowrap2()\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:686 +0x25 fp=0xc000305fe0 sp=0xc000305fc8 pc=0x7ff789a2b985\nruntime.goexit({})\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000305fe8 sp=0xc000305fe0 pc=0x7ff78972cfe1\ncreated by net/http.(*connReader).startBackgroundRead in goroutine 42\n        C:/hostedtoolcache/windows/go/1.24.0/x64/src/net/http/server.go:686 +0xb6\nrax     0x4\nrbx     0x1f947aad170\nrcx     0x1f947b3df60\nrdx     0xa\nrdi     0x4\nrsi     0x300020000\nrbp     0x0\nrsp     0xa5c139db30\nr8      0x4\nr9      0x300020000\nr10     0x1\nr11     0xa5c139db68\nr12     0x4\nr13     0x1f94aacdf00\nr14     0x1f947b3df60\nr15     0x1f947b3e070\nrip     0x7ff9414451fe\nrflags  0x10202\ncs      0x33\nfs      0x53\ngs      0x2b\n[GIN] 2025/06/13 - 20:56:47 | 200 |     2.583665s |       127.0.0.1 | POST     \"/api/chat\"\n```\n\n### OS\n\nWindows\n\n### GPU\n\nAMD\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "ChadSexington",
        "body": "Updating the AMD graphics driver appears to have resolved the issue, upgraded from driver version 24.30 to 25.10"
      }
    ]
  },
  {
    "issue_number": 10969,
    "title": "Request: Allow configurable timeout for “waiting for llama runner to start”",
    "author": "brian0913579",
    "state": "closed",
    "created_at": "2025-06-04T12:19:49Z",
    "updated_at": "2025-06-13T22:33:07Z",
    "labels": [
      "feature request"
    ],
    "body": "I'm running large models like `qwen2.5-coder:32b` using Ollama on macOS, with models stored on an external SSD (e.g., `/Volumes/T7/ollama-models`). When I try to run the model with:\n\n```bash\nOLLAMA_MODELS=/Volumes/T7/ollama-models ollama run qwen2.5-coder:32b\n```\n\nI consistently get the error:\n```\nError: timed out waiting for llama runner to start - progress 0.00 -\n```\n\nAfter investigation, it appears this is due to the model taking time to load from the external drive into RAM or GPU memory. However, there is currently no exposed option to extend or configure the timeout duration for the runner startup.\n\n##  Feature Request\n\nPlease allow the timeout to be configurable, either via:\n- an environment variable (e.g., `OLLAMA_RUNNER_TIMEOUT`)\n- a CLI flag (e.g., `--timeout`)\n- or a config file setting\n\n### Why This Matters\n\nThis would be especially useful for users with:\n- large models (13B, 32B)\n- slower or external storage\n- limited memory, which increases load time\n\nThanks for building such an awesome tool — looking forward to future improvements!\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "[`OLLAMA_LOAD_TIMEOUT`](https://github.com/ollama/ollama/blob/5c42800fca4da07d1c362c0f190429993e53c3b5/envconfig/config.go#L261)"
      }
    ]
  },
  {
    "issue_number": 10966,
    "title": "Determine thinking capability via API ?",
    "author": "bakman2",
    "state": "closed",
    "created_at": "2025-06-04T05:08:45Z",
    "updated_at": "2025-06-13T22:32:31Z",
    "labels": [
      "feature request"
    ],
    "body": "How can i determine if a model has thinking capability via API ?\nThis is useful when having a front-end where one can enable/disable thinking when it is available. ",
    "comments": [
      {
        "user": "rick-github",
        "body": "```\n$ curl -s localhost:11434/api/show -d '{\"model\":\"qwen3\"}' | jq .capabilities\n[\n  \"completion\",\n  \"tools\",\n  \"thinking\"\n]\n```"
      },
      {
        "user": "bakman2",
        "body": "That's what i was looking for indeed, however:\n\n```\ncurl -s localhost:11434/api/show -d '{\"model\":\"qwen3:1.7b\"}' | jq .capabilities\n[\n  \"completion\",\n  \"tools\"\n]\n```\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"qwen3:1.7b\",\n  \"prompt\": \"hi\"\n}'\n\n{\n\"model\":\"qwen3:1.7b\",\n\"created_at\":\"2025-06-05T05:01:24.664308446Z\",\n\"response\":\"\\u003cthink\\u003e\",\n\"done\":false\n}\n```"
      },
      {
        "user": "rick-github",
        "body": "Have you re-pulled the model since 0.9.0 was released?  Thinking support requires an updated template."
      }
    ]
  },
  {
    "issue_number": 10948,
    "title": "\"model thrashing\" even though OLLAMA_KEEP_ALIVE=-1 is set",
    "author": "meidaid",
    "state": "closed",
    "created_at": "2025-06-02T10:42:36Z",
    "updated_at": "2025-06-13T22:30:21Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n### Similar to\n\n1. https://github.com/ollama/ollama/issues/8903\n2. https://github.com/ollama/ollama/pull/10003\n\n### Problem Descriptionn\nI created two models using Modelfiles that both are based on `llama3.2:1b`.\nWe will them models `A` and `B`.\n\nI run Ollama in a docker container and the environment variable setting `OLLAMA_KEEP_ALIVE=-1` is already set and is visible to all processes.\n\nI then open three `bash` sessions to the Ollama container. \n\nIn the first I run `watch ollama ps` so I can observe changes.  At first the list is empty.\n\nIn the second I run `ollama run A` and the first terminal lists `A` as being in memory `UNTIL` `Forever`.\n\nIn the third I run `ollama run B`.\n\nThe expected behavior is that both model `A` and model `B` listed as `UNTIL` `Forever`, but instead model`A` vanishes from the list to be replaced by `B`.\n\nThis is unexpected behavior.\n\nThe expected behavior is that both would be listed and that performance would improve due to neither needing to be reloaded.  But, if I repeatedly keep switching models, the visible model gets bumped.\n\nFor the record, each model would only take up about 4GB each in RAM, so the 32GB is quite ample.\n\n### Personal View\n\nThere is a possibility that my problem represents an edge case of the following:\n\n1. https://github.com/ollama/ollama/issues/8903\n2. https://github.com/ollama/ollama/pull/10003\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nUbuntu Linux 22.04 LTS\n\n### GPU\n\nNone\n\n### CPU\n\nAMD Ryzen 7 8-core/16-thread\n\n### RAM \n\n32GB\n\n### Ollama version\n\nv0.9.0\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "ollama does not support loading a GGUF file multiple times.  #3902"
      }
    ]
  },
  {
    "issue_number": 10947,
    "title": "ollama + Llama4 image-to-text error",
    "author": "stevenTzai",
    "state": "closed",
    "created_at": "2025-06-02T06:58:14Z",
    "updated_at": "2025-06-13T22:29:56Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nHi~\nMy env:\n1. OS: ubuntu 22.04\n2. APP: ollama 0.9.0\n3. Model: Llama 4\n4. GPU: Nvidia V100\n\nissue:  ollama + Llama4 image-to-text error\nI try ollama version and result below:\n\nollama        model     text-to-text     image-to-text\n-----------------------------------------------------------\n0.6.8           Llama4          OK                  FAIL\n0.7.0-rc1     Llama4          OK                   OK\n0.7.0           Llama4          OK                   FAIL\n0.8.0           Llama4          OK                   FAIL\n0.9.0           Llama4          OK                   FAIL\n\nversion: 0.7.0-rc1  is work fine, but other ollama version ,the Llama4  image-to-text failed.\n\nAnyone have the same issue?\n\nor just nvidia v100 not support ?\n\n\n### Relevant log output\n\n```shell\ntime=2025-06-02T13:41:13.471+08:00 level=DEBUG source=sched.go:503 msg=\"context for request finished\"\ntime=2025-06-02T13:41:13.471+08:00 level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/llama4:latest runner.inference=cuda runner.devices=3 runner.size=\"85.3 GiB\" runner.vram=\"85.3 GiB\" runner.parallel=15 runner.pid=49 runner.model=/root/.ollama/models/blobs/sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5 runner.num_ctx=61440 duration=2562047h47m16.854775807s\ntime=2025-06-02T13:41:13.471+08:00 level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/llama4:latest runner.inference=cuda runner.devices=3 runner.size=\"85.3 GiB\" runner.vram=\"85.3 GiB\" runner.parallel=15 runner.pid=49 runner.model=/root/.ollama/models/blobs/sha256-9d507a36062c2845dd3bb3e93364e9abc1607118acd8650727a700f72fb126e5 runner.num_ctx=61440 refCount=0\ntime=2025-06-02T13:41:13.689+08:00 level=ERROR source=server.go:457 msg=\"llama runner terminated\" error=\"exit status 2\"\n```\n\n### OS\n\nDocker\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will aid in debugging, but the most likely cause is out-of-memory (OOM).  See [here](https://github.com/ollama/ollama/issues/8597#issuecomment-2614533288) for mitigations."
      }
    ]
  },
  {
    "issue_number": 10937,
    "title": "JSON format schema ignored on /v1/ endpoint",
    "author": "ponychicken",
    "state": "closed",
    "created_at": "2025-06-01T13:33:44Z",
    "updated_at": "2025-06-13T22:29:13Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n### What is the issue?\n\nThe JSON schema , or at least the \"format\": \"date-time\" does not work on the v1 endpoint.\n\n## Reproduction Steps\n```bash\ncurl http://localhost:11434/v1/chat/completions -d '{\n  \"model\": \"gemma3:1b\", \n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Here is some event info: The meeting is scheduled for Thursday (20th October 2025) afternoon. Please provide a JSON object with the event name and the date (human readable) of the meeting.\"\n    }\n  ],\n  \"response_format\": {\n    \"type\": \"json_object\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"event\": { \"type\": \"string\" },\n        \"date\": {\n          \"type\": \"string\",\n          \"format\": \"date-time\"\n        }\n      },\n      \"required\": [\"event\", \"date\"]\n    }\n  },\n  \"stream\": false\n}'\n```\n\n## Actual Output\n```json\n{\"event_name\": \"Meeting\", \"date\": \"20th October 2025\"}\n```\n\n## Expected Output\n```json\n{\"event\": \"Meeting\", \"date\": \"2025-10-20T14:00:00Z\"}\n```\n\n## Environment\n- Ollama API endpoint\n- Model: gemma3:4b\n- Date: 2025-06-01\n\n## Notes\nI am not prompting the model to output ISO to make the bug more apparent. It also happens when asking for ISO strings but less often.\n\n### Relevant log output\n\n```shell\nNo relevant output.\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n0.9.0\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "mlaihk",
        "body": "I also noticed that non-native tool calling LLMs are working a lot worse in open webui in terms of invoking tools.  This is probably why....."
      },
      {
        "user": "rick-github",
        "body": "```diff\n--- ./10937.sh.orig\t2025-06-03 03:10:32.272624315 +0200\n+++ ./10937.sh\t2025-06-03 03:10:15.537367732 +0200\n@@ -9,17 +9,18 @@\n     }\n   ],\n   \"response_format\": {\n-    \"type\": \"json_object\",\n-    \"schema\": {\n-      \"type\": \"object\",\n-      \"properties\": {\n-        \"event\": { \"type\": \"string\" },\n-        \"date\": {\n-          \"type\": \"string\",\n-          \"format\": \"date-time\"\n-        }\n-      },\n-      \"required\": [\"event\", \"date\"]\n+    \"type\": \"json_schema\",\n+    \"json_schema\": {\n+      \"schema\": {\n+        \"properties\": {\n+          \"event\": { \"type\": \"string\" },\n+          \"date\": {\n+            \"type\": \"string\",\n+            \"format\": \"date-time\"\n+          }\n+        },\n+        \"required\": [\"event\", \"date\"]\n+      }\n     }\n   },\n   \"stream\": false\n```\n```console\n$ ./10937.sh | jq -r '.choices[].message.content'\n{\"event\": \"Meeting\", \"date\": \"2025-10-20T14:00:00Z\"}\n```"
      }
    ]
  },
  {
    "issue_number": 10941,
    "title": "Models don't have memory.",
    "author": "o1243535241",
    "state": "closed",
    "created_at": "2025-06-01T21:25:31Z",
    "updated_at": "2025-06-13T22:28:45Z",
    "labels": [
      "bug"
    ],
    "body": "I want trein models then close ollama, then open ollama, and models must remember all. Now they don't remember what about I wrote before.\n\nCan you fix this, please?\n\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "Memory is not a function of the server.  The client implements it, normally by recording the `messages[]` sent to the server.  Using the ollama CLI you can save state with the [`/save`](https://github.com/ollama/ollama/blob/5c42800fca4da07d1c362c0f190429993e53c3b5/cmd/interactive.go#L38) command and re-load it later with the `/load` command."
      },
      {
        "user": "Yami-Bitshark",
        "body": "1. To close ollama, open it again to start a new chat session is the normal and expected behaviour. \n2. And ollama is an inference server, it is not supposed to manage the model memory.\n3. You must handle the memory on the client side, you can either concat messages and send them with each request, or give the agent a tool to navigate history or you might have an other approach, depends on the client. Remember, managing context and memory, is the responsability of the framework, or the developper in general.\n4. As mentionned above, in a chat session you can use /save /load commands."
      }
    ]
  },
  {
    "issue_number": 2315,
    "title": "Apple gpu support for Linux",
    "author": "maxiwee69",
    "state": "closed",
    "created_at": "2024-02-02T00:17:53Z",
    "updated_at": "2025-06-13T21:48:50Z",
    "labels": [],
    "body": "So maybe you know about [https://asahilinux.org/](https://asahilinux.org/), if not, it’s Fedora for m series Mac’s. But when i tried to get ollama to run on it, i got it told me `WARNING: No NVIDIA GPU detected. Ollama will run in CPU-only mode`, i know fixing this would only be a fix for such a small amount of people but i would highly appreciate it.",
    "comments": [
      {
        "user": "MichaelFomenko",
        "body": "There are no Drivers for Apple Silicon GPU in Linux yet to do GPGPU. This have nothing to do with Ollama."
      },
      {
        "user": "igorschlum",
        "body": "@maxiwee69 as it's not a issue in Ollama, could you please close the Issue?"
      },
      {
        "user": "maxiwee69",
        "body": "But just a question, will gpu support be implemented once the drivers are supporting GPGPU?"
      }
    ]
  },
  {
    "issue_number": 3144,
    "title": "add /metrics endpoint",
    "author": "codearranger",
    "state": "open",
    "created_at": "2024-03-14T16:39:01Z",
    "updated_at": "2025-06-13T20:58:25Z",
    "labels": [
      "feature request",
      "api"
    ],
    "body": "It would be nice of ollama had a /metrics endpoint for collecting metrics for prometheus or other monitoring tools.\r\n\r\nhttps://prometheus.io/docs/guides/go-application/\r\n\r\nSome metrics to include might be,\r\nGPU utilization, memory utilization, CPU utilzation, layers used, request counts, etc.",
    "comments": [
      {
        "user": "amila-ku",
        "body": "I would like to work on this one. I have worked on several Prometheus metrics integrations on Go apps before."
      },
      {
        "user": "aliirz",
        "body": "+1"
      },
      {
        "user": "yuliyantsvetkov",
        "body": "I can help with cardinality exploration, sizing of labels, reviews, but I haven't opened the full code base to check where we can add the metric counters.\r\n\r\nBy default the prometheus go module exports some system metrics like GC, Mem, Routines, yet that is just the app base.\r\n\r\nLet me know if I can help with the reviews."
      }
    ]
  },
  {
    "issue_number": 6635,
    "title": "Moondream2 needs an update",
    "author": "ddpasa",
    "state": "open",
    "created_at": "2024-09-04T16:26:51Z",
    "updated_at": "2025-06-13T17:59:14Z",
    "labels": [
      "model request"
    ],
    "body": "moondream2 is an amazing tiny little VLM.  The owner (https://github.com/vikhyat) releases updates quite frequently.  I'm not sure which version ollama currently has, but there was a new release last week (2024-08-26) which is not in ollama.\r\n\r\nhttps://huggingface.co/vikhyatk/moondream2",
    "comments": [
      {
        "user": "UmutAlihan",
        "body": "I also be very glad if we can reach the latest release 08.2024 via Ollama as this model library is updated. It would be very much appreciated. cheers,"
      },
      {
        "user": "2faf151",
        "body": "can someone in ollama team please update the Moondream model please"
      }
    ]
  },
  {
    "issue_number": 10934,
    "title": "Feature Request: External Tool Integration for Executing and Using Common Lisp via <lisp> Tags and proper prompt prefix",
    "author": "jdltg",
    "state": "open",
    "created_at": "2025-05-31T21:36:35Z",
    "updated_at": "2025-06-13T15:26:36Z",
    "labels": [
      "feature request"
    ],
    "body": "## Summary\n\nI propose adding support for integrating an external **Common Lisp evaluation engine** with Ollama. This would enable the model to **evaluate Common Lisp code blocks** dynamically and use the results to **enhance reasoning**, **perform computations**, and **maintain contextual state**.\n\n---\n\n## 🔧 Proposed Feature\n\nEnable the use of a special syntax that signals the model to execute embedded Lisp code:\n\n```lisp\n<lisp>\n;; Common Lisp code here\n(format t \"Hello, world!\")\n</lisp>\n```\n\nWhen this block is encountered, Ollama should:\n\n1. **Delegate the code** to an external Common Lisp evaluator.\n2. **Capture the output**.\n3. **Return the result** in a dedicated tag:\n\n```lisp\n<lisp-result>\nHello, world!\n</lisp-result>\n```\n\n---\n\n## 📌 Use Cases\n\n- **Symbolic reasoning**  \n  Execute logic or symbolic expressions with Lisp’s native capabilities.\n\n- **Dynamic computation**  \n  Perform real-time math or data structure manipulation using Lisp.\n\n- **Session memory**  \n  Maintain temporary state in the form of Lisp variables or structures.\n\n---\n\n## ✅ Benefits\n\n- Enhances interactivity, enabling **live code execution**.\n- Improves reasoning and **dynamic response generation**.\n- Enables **temporary memory** and stateful interactions using Lisp.\n- Serves as a **blueprint for integrating other languages** \n- Bridges the gap between **code understanding** and **runtime execution**.\n\n---\n\n## 💡 Why Lisp?\n\nLisp is a natural fit for LLM integration due to its strengths in symbolic reasoning, minimal and consistent syntax, and homoiconicity—where code is treated as data. This makes it easy for language models to generate, analyze, and manipulate Lisp code reliably. Its REPL-driven, stateful design aligns well with dynamic, interactive sessions, enabling real-time evaluation and temporary memory. These traits make Lisp not only efficient for logic-based tasks but also uniquely compatible with the way LLMs process and structure information.\n\n---\n\n## 🙏 Thanks\n\nThank you for considering this enhancement. It could be a powerful tool for users who rely on symbolic reasoning and functional programming.",
    "comments": [
      {
        "user": "rick-github",
        "body": "So the client creates a prompt/query that has embedded lisp code, sends it to the ollama server, the ollama server extracts the lisp and sends it to an evaluator, retrieves the result, appends it to the prompt, and then processes the prompt, and returns the result to the client?"
      },
      {
        "user": "jdltg",
        "body": "No, sorry — what I meant is this: during its reasoning process, the LLM can generate code enclosed in ```<lisp>...</lisp>``` tags. The Ollama server detects these code blocks and sends them to a Lisp interpreter for execution. Once the code is executed, the server appends the result within ```<lisp-result>...</lisp-result>``` tags and returns it to the LLM. The LLM then incorporates this result and continues its  generation process.\n\nThis mechanism allows the LLM to interact with external tools via Lisp and store intermediate data in the Lisp interpreter’s temporary memory. That memory can be reused later in the same reasoning chain, enhancing the LLM's ability to \"think\" and manage context more effectively.\n\nTo enable the LLM to generate these ```<lisp>``` blocks, it must either be trained to do so or guided via an initial system prompt. I've chosen Common Lisp for this setup because it's a highly expressive and powerful language.\n\nThis architecture extends the LLM’s capabilities by integrating symbolic reasoning, persistent state, and executable logic into its generation process.\n\nIt's akin to giving a thinker  a piece of paper, a pencil to sketch out ideas and a Turing-complete calculator.\n\n"
      },
      {
        "user": "AdamSobieski",
        "body": "Hello. If I understand correctly, the idea resembles LLMs being capable of first generating PHP-like content, then performing server-side processing upon it to produce resultant output to send to clients?\n\nWith respect to syntax possibilities, what about:\n```xml\n<?lisp (format t \"Hello, world!\") ?>\n```\n\nThen, we might also envision:\n```xml\n<?prolog write('Hello, world!'). ?>\n```\n```xml\n<?php echo(\"Hello, world!\"); ?>\n```\n```xml\n<?python print(\"Hello, world!\") ?>\n```\n```xml\n<?javascript console.log('Hello, world!'); ?>\n```\n\nA solution could be extensible beyond Lisp. Developers could install modular plugins.\n\nFrom preprocessing scripts, there would also be a means for interpreting multiple interconnected preprocessing scopes' contents in a result stream.\n\nThe above syntax possibilities, however, don't suggest clearly distinguishing preprocessed outputs from other outputs, that is automatically providing your `<lisp-result>...</lisp-result>` enclosure. Maybe a debugging mode or other toggle could add this for developers?\n\nAlso, a potential syntax would, I think, need to differ from the possibilities indicated, above, so that LLMs would still be able to output PHP source code and examples."
      }
    ]
  },
  {
    "issue_number": 1016,
    "title": "Support AMD GPUs on Intel Macs",
    "author": "J0hnny007",
    "state": "open",
    "created_at": "2023-11-06T15:20:01Z",
    "updated_at": "2025-06-13T10:42:08Z",
    "labels": [
      "feature request",
      "amd",
      "macos"
    ],
    "body": "I'm currently trying out the ollama app on my iMac (i7/Vega64) and I can't seem to get it to use my GPU.\r\n\r\nI have tried running it with num_gpu 1 but that generated the warnings below.\r\n\r\n`\r\n2023/11/06 16:06:33 llama.go:384: starting llama runner\r\n2023/11/06 16:06:33 llama.go:386: error starting the external llama runner: fork/exec /var/folders/2z/r_0t221x2blbq02n5dp2m5fr0000gn/T/ollama1975281143/llama.cpp/gguf/build/metal/bin/ollama-runner: bad CPU type in executable\r\n2023/11/06 16:06:33 llama.go:384: starting llama runner\r\n2023/11/06 16:06:33 llama.go:442: waiting for llama runner to start responding\r\n{\"timestamp\":1699283193,\"level\":\"WARNING\",\"function\":\"server_params_parse\",\"line\":873,\"message\":\"Not compiled with GPU offload support, --n-gpu-layers option will be ignored. See main README.md for information on enabling GPU BLAS support\",\"n_gpu_layers\":-1}\r\n{\"timestamp\":1699283193,\"level\":\"INFO\",\"function\":\"main\",\"line\":1324,\"message\":\"build info\",\"build\":219,\"commit\":\"9e70cc0\"}\r\n{\"timestamp\":1699283193,\"level\":\"INFO\",\"function\":\"main\",\"line\":1330,\"message\":\"system info\",\"n_threads\":6,\"n_threads_batch\":-1,\"total_threads\":12,\"system_info\":\"AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \"}\r\n`",
    "comments": [
      {
        "user": "BruceMacD",
        "body": "Hi @J0hnny007, thanks for opening the issue. Ollama only supports the Metal GPU API on Macs right now. AMD GPUs won't work."
      },
      {
        "user": "J0hnny007",
        "body": "Good to know, though I thought that mps can use AMD GPUs. Oh well, thanks for the info."
      },
      {
        "user": "cmarhoover",
        "body": "Apple's \"[Metal Overview](https://developer.apple.com/metal/)\" page has the following hardware support list in the page footer:\r\n\r\n> Metal 3 is supported on the following hardware:\r\n> iPhone and iPad: Apple A13 Bionic or later\r\n> Mac: Apple silicon (M1 or later), AMD Radeon Pro Vega series, AMD Radeon Pro 5000/6000 series, Intel Iris Plus Graphics series, Intel UHD Graphics 630\r\n\r\nDespite being listed as supporting Metal 3, I can confirm that Ollama does not currently use the Radeon RX 6900 in my Mac Pro system."
      }
    ]
  },
  {
    "issue_number": 11066,
    "title": "Chat endpoint should allow for `system` override",
    "author": "hellerve",
    "state": "closed",
    "created_at": "2025-06-13T10:02:08Z",
    "updated_at": "2025-06-13T10:20:00Z",
    "labels": [
      "feature request"
    ],
    "body": "Let me start by saying it is entirelypossible that I am lacking context (I’ve tried looking through existant issues), but it seems to me that the chat completion endpoint should allow for a system prompt override just like the generation endpoint does.\n\nIf this is desired, I’d be happied to draft up a PR for this also—I just wanted to ask whether y’all have a reason for not supporting this that I might not know of before doing a drive-by contribution.\n\nCheers",
    "comments": [
      {
        "user": "hellerve",
        "body": "From looking at the code and talking on Discord: the generation endpoint prepends a system message to the (non-user controlled) list of messages when generating. The solution on the chat side is to use a message with `system` role."
      }
    ]
  },
  {
    "issue_number": 11052,
    "title": "Deploy a model by manul",
    "author": "fishfl",
    "state": "closed",
    "created_at": "2025-06-12T07:36:52Z",
    "updated_at": "2025-06-13T09:10:40Z",
    "labels": [
      "question"
    ],
    "body": "### What is the issue?\n\nSo we have a stage environment and a production environment.\nThe production environment can't access the public internet. So we can't pull a model from your library, like: ollama pull llama3.2, it's not work.\n﻿\nBut we can access the WLAN and download model files from a Local storage like HDFS.\n﻿\nSo whether can we run\n‘ollama pull llama3.2’ in a stage environment to download the models files and then upload the model files to our Local storage.\nAnd in a production environment, we download model files from the Local storage,\nThen move the files to a host path like '/root/.ollama/models/' or somewhere.\n﻿\nIs that work for ollama? Can ollama identify the model from the host files instead of pull it from your library?\n﻿\nThanks very much!\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nLinux\n\n### GPU\n\n_No response_\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.1",
    "comments": [
      {
        "user": "rick-github",
        "body": "Yes, just copy `$OLLAMA_HOME/.ollama/models` from staging to local storage, and then to `/root/.ollama/models` in production."
      },
      {
        "user": "hlstudio",
        "body": "The same as above, in a new Ollama environment, only download ollama pull llama3.2. You can confirm that only the llama3.2 model is present by using ollama list. Then directly copy the current models folder to the production environment's models, ensuring the folder hierarchy corresponds, so that the production environment can add a new llama3.2 large model."
      },
      {
        "user": "fishfl",
        "body": "Got it , Thank you very much. It works!"
      }
    ]
  },
  {
    "issue_number": 11065,
    "title": "Error Error: Head \"https://registry.ollama.ai/v2/library/deepseek-r1/blobs/sha256:6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e\": read tcp 10.10.0.39:49606->104.21.75.227:443: read: operation timed out",
    "author": "sherbaksa",
    "state": "open",
    "created_at": "2025-06-13T08:40:40Z",
    "updated_at": "2025-06-13T08:40:40Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI have a error:\nError: Head \"https://registry.ollama.ai/v2/library/deepseek-r1/blobs/sha256:6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e\": read tcp 10.10.0.39:49606->104.21.75.227:443: read: operation timed out\n\nHelp me please!\nping to registry.ollama.ai goes/\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nmacOS\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n0.9.0",
    "comments": []
  },
  {
    "issue_number": 11063,
    "title": "Feature on `app`: hide system tray icon via `OLLAMA_APP_NOTRAY` env var",
    "author": "fumiama",
    "state": "open",
    "created_at": "2025-06-13T07:25:19Z",
    "updated_at": "2025-06-13T07:25:19Z",
    "labels": [
      "feature request"
    ],
    "body": "### Problem\n\nSometimes one may find the system tray icon visually cluttering or distracting because \nsome users prefer minimal interfaces and may not want a tray icon present at all, while \nthey still want to enjoy the one-click startup service on Windows platform.\n\n### Proposed feature\n\nIntroduce an environment variable (e.g., `OLLAMA_APP_NOTRAY=1`) that tells the desktop app \n**not** to display a tray icon.\n\n- This would stay backwards-compatible for the absence of it keeps current behavior.\n- This may also allow those non-win platforms to use the app, though a little bit useless.\n\n### Implement\n\nI have created a PR on it and tested it on my Windows 10 machine, see #11062",
    "comments": []
  },
  {
    "issue_number": 10409,
    "title": "Error looking up nvidia GPU memory & error cuda driver library failed to get device context 801 & CUDA error: all CUDA-capable devices are busy or unavailable",
    "author": "Sven1403",
    "state": "closed",
    "created_at": "2025-04-25T11:42:09Z",
    "updated_at": "2025-06-13T06:30:01Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nHello,\n\ni want to use ollama with a Nvidia A16 Grid vGPU over ESXI VMware. When i dont declare the cuda lib path from ollama appdata location in the environment variables, the models (i tried deepseek-r1 1.5b to 8b) use 100% CPU. In the server log i already get this:\n\ntime=2025-04-25T11:23:03.078+02:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-04-25T11:23:03.078+02:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-04-25T11:23:03.078+02:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=4 efficiency=0 threads=4\ntime=2025-04-25T11:23:03.125+02:00 level=INFO source=gpu.go:272 msg=\"error looking up nvidia GPU memory\" error=\"cuda driver library failed to get device context 801\"\ntime=2025-04-25T11:23:03.126+02:00 level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\n\nThen i configured the environment variables with that\n\n![Image](https://github.com/user-attachments/assets/562d0853-cf00-4a22-bc6d-4e4ba4d6b06b)\n\n![Image](https://github.com/user-attachments/assets/139143f5-06a9-4611-bbe5-03205fa6c85a)\n\n![Image](https://github.com/user-attachments/assets/67892683-0b6c-4a0e-be3e-a6e01fcacd48)\n(I also tried the cuda_v12 files)\n\nThat is resulting into this error:\n\n![Image](https://github.com/user-attachments/assets/4a315921-1930-46c8-9b9c-c2f2f65f48c0)\n\nI also get more log details which i post in the log output area. You can find the \"error looking up nvidia GPU memory here too.\n\nI searched alot topics here and in other forums. I never found that exact problem. Anyone here can help? :)\n\nEdit: Will check next week if the vGPU is configured with a A-Profile. Looks like this profile cant work:\n\n![Image](https://github.com/user-attachments/assets/40d1bdf8-54e1-407c-b36c-04f4ba20c54e)\n\nBtw the nvidia-smi output:\n\n![Image](https://github.com/user-attachments/assets/54887e86-b6c5-47ee-9b6b-d879ecfad52f)\n\n\n### Relevant log output\n\n```shell\n2025/04/25 12:46:31 routes.go:1232: INFO server config env=\"map[CUDA_VISIBLE_DEVICES:GPU-0d1397db-6149-11b2-b139-80f1d9baaab7 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\User\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:true OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-04-25T12:46:31.406+02:00 level=INFO source=images.go:458 msg=\"total blobs: 13\"\ntime=2025-04-25T12:46:31.407+02:00 level=INFO source=images.go:465 msg=\"total unused blobs removed: 0\"\ntime=2025-04-25T12:46:31.408+02:00 level=INFO source=routes.go:1299 msg=\"Listening on 127.0.0.1:11434 (version 0.6.6)\"\ntime=2025-04-25T12:46:31.408+02:00 level=DEBUG source=sched.go:107 msg=\"starting llm scheduler\"\ntime=2025-04-25T12:46:31.408+02:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-04-25T12:46:31.408+02:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-04-25T12:46:31.408+02:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=4 efficiency=0 threads=4\ntime=2025-04-25T12:46:31.408+02:00 level=DEBUG source=gpu.go:98 msg=\"searching for GPU discovery libraries for NVIDIA\"\ntime=2025-04-25T12:46:31.408+02:00 level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=nvml.dll\ntime=2025-04-25T12:46:31.408+02:00 level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\nvml.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v11\\\\nvml.dll C:\\\\Windows\\\\system32\\\\nvml.dll C:\\\\Windows\\\\nvml.dll C:\\\\Windows\\\\System32\\\\Wbem\\\\nvml.dll C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\nvml.dll C:\\\\Windows\\\\System32\\\\OpenSSH\\\\nvml.dll C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA App\\\\NvDLISR\\\\nvml.dll C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2024.1.0\\\\nvml.dll C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common\\\\nvml.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvml.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v11\\\\nvml.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\nvml.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvml.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvml.dll c:\\\\Windows\\\\System32\\\\nvml.dll]\"\ntime=2025-04-25T12:46:31.408+02:00 level=DEBUG source=gpu.go:529 msg=\"skipping PhysX cuda library path\" path=\"C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common\\\\nvml.dll\"\ntime=2025-04-25T12:46:31.408+02:00 level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=\"[C:\\\\Windows\\\\system32\\\\nvml.dll c:\\\\Windows\\\\System32\\\\nvml.dll]\"\ntime=2025-04-25T12:46:31.428+02:00 level=DEBUG source=gpu.go:111 msg=\"nvidia-ml loaded\" library=C:\\Windows\\system32\\nvml.dll\ntime=2025-04-25T12:46:31.428+02:00 level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=nvcuda.dll\ntime=2025-04-25T12:46:31.428+02:00 level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\nvcuda.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v11\\\\nvcuda.dll C:\\\\Windows\\\\system32\\\\nvcuda.dll C:\\\\Windows\\\\nvcuda.dll C:\\\\Windows\\\\System32\\\\Wbem\\\\nvcuda.dll C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\nvcuda.dll C:\\\\Windows\\\\System32\\\\OpenSSH\\\\nvcuda.dll C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA App\\\\NvDLISR\\\\nvcuda.dll C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2024.1.0\\\\nvcuda.dll C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common\\\\nvcuda.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvcuda.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v11\\\\nvcuda.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\nvcuda.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvcuda.dll C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvcuda.dll c:\\\\windows\\\\system*\\\\nvcuda.dll]\"\ntime=2025-04-25T12:46:31.428+02:00 level=DEBUG source=gpu.go:529 msg=\"skipping PhysX cuda library path\" path=\"C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common\\\\nvcuda.dll\"\ntime=2025-04-25T12:46:31.429+02:00 level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=[C:\\Windows\\system32\\nvcuda.dll]\ninitializing C:\\Windows\\system32\\nvcuda.dll\ndlsym: cuInit - 00007FFE815BFDE0\ndlsym: cuDriverGetVersion - 00007FFE815BFE80\ndlsym: cuDeviceGetCount - 00007FFE815C0676\ndlsym: cuDeviceGet - 00007FFE815C0670\ndlsym: cuDeviceGetAttribute - 00007FFE815BFFD0\ndlsym: cuDeviceGetUuid - 00007FFE815C0682\ndlsym: cuDeviceGetName - 00007FFE815C067C\ndlsym: cuCtxCreate_v3 - 00007FFE815C06F4\ndlsym: cuMemGetInfo_v2 - 00007FFE815C07F0\ndlsym: cuCtxDestroy - 00007FFE815C0700\ncalling cuInit\ncalling cuDriverGetVersion\nraw version 0x2f08\nCUDA driver version: 12.4\ncalling cuDeviceGetCount\ndevice count 1\ntime=2025-04-25T12:46:31.471+02:00 level=DEBUG source=gpu.go:125 msg=\"detected GPUs\" count=1 library=C:\\Windows\\system32\\nvcuda.dll\ntime=2025-04-25T12:46:31.471+02:00 level=INFO source=gpu.go:272 msg=\"error looking up nvidia GPU memory\" error=\"cuda driver library failed to get device context 801\"\ntime=2025-04-25T12:46:31.471+02:00 level=DEBUG source=amd_windows.go:34 msg=\"unable to load amdhip64_6.dll, please make sure to upgrade to the latest amd driver: Das angegebene Modul wurde nicht gefunden.\"\ntime=2025-04-25T12:46:31.471+02:00 level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\nreleasing cuda driver library\nreleasing nvml library\ntime=2025-04-25T12:46:31.473+02:00 level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=cpu variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"64.0 GiB\" available=\"60.0 GiB\"\n[GIN] 2025/04/25 - 12:52:46 | 200 |            0s |       127.0.0.1 | GET      \"/api/version\"\n[GIN] 2025/04/25 - 12:52:59 | 200 |      1.6369ms |       127.0.0.1 | HEAD     \"/\"\ntime=2025-04-25T12:53:00.041+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-25T12:53:00.065+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/04/25 - 12:53:00 | 200 |    116.3694ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-04-25T12:53:00.116+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-25T12:53:00.118+02:00 level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"64.0 GiB\" before.free=\"60.0 GiB\" before.free_swap=\"68.9 GiB\" now.total=\"64.0 GiB\" now.free=\"59.6 GiB\" now.free_swap=\"68.3 GiB\"\ntime=2025-04-25T12:53:00.120+02:00 level=DEBUG source=sched.go:183 msg=\"updating default concurrency\" OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1\ntime=2025-04-25T12:53:00.134+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-25T12:53:00.149+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-25T12:53:00.150+02:00 level=DEBUG source=sched.go:213 msg=\"cpu mode with first model, loading\"\ntime=2025-04-25T12:53:00.150+02:00 level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"64.0 GiB\" before.free=\"59.6 GiB\" before.free_swap=\"68.3 GiB\" now.total=\"64.0 GiB\" now.free=\"59.6 GiB\" now.free_swap=\"68.3 GiB\"\ntime=2025-04-25T12:53:00.155+02:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"64.0 GiB\" free=\"59.6 GiB\" free_swap=\"68.3 GiB\"\ntime=2025-04-25T12:53:00.156+02:00 level=DEBUG source=memory.go:108 msg=evaluating library=cpu gpu_count=1 available=\"[59.6 GiB]\"\ntime=2025-04-25T12:53:00.156+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen2.vision.block_count default=0\ntime=2025-04-25T12:53:00.157+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen2.attention.key_length default=128\ntime=2025-04-25T12:53:00.157+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen2.attention.value_length default=128\ntime=2025-04-25T12:53:00.158+02:00 level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[59.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.1 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"448.0 MiB\" memory.required.allocations=\"[5.1 GiB]\" memory.weights.total=\"4.1 GiB\" memory.weights.repeating=\"3.7 GiB\" memory.weights.nonrepeating=\"426.4 MiB\" memory.graph.full=\"478.0 MiB\" memory.graph.partial=\"730.4 MiB\"\ntime=2025-04-25T12:53:00.162+02:00 level=DEBUG source=server.go:262 msg=\"compatible gpu libraries\" compatible=[]\ntime=2025-04-25T12:53:00.208+02:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-25T12:53:00.209+02:00 level=DEBUG source=server.go:278 msg=\"model not yet supported by Ollama engine, switching to compatibility mode\" model=C:\\Users\\User\\.ollama\\models\\blobs\\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 error=\"unsupported model architecture \\\"qwen2\\\"\"\nllama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\\Users\\User\\.ollama\\models\\blobs\\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   4:                         general.size_label str              = 7B\nllama_model_loader: - kv   5:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                          general.file_type u32              = 15\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q4_K:  169 tensors\nllama_model_loader: - type q6_K:   29 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 4.36 GiB (4.91 BPW) \ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nload: control token: 151656 '<|video_pad|>' is not marked as EOG\nload: control token: 151655 '<|image_pad|>' is not marked as EOG\nload: control token: 151653 '<|vision_end|>' is not marked as EOG\nload: control token: 151652 '<|vision_start|>' is not marked as EOG\nload: control token: 151651 '<|quad_end|>' is not marked as EOG\nload: control token: 151646 '<｜begin▁of▁sentence｜>' is not marked as EOG\nload: control token: 151644 '<｜User｜>' is not marked as EOG\nload: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nload: control token: 151660 '<|fim_middle|>' is not marked as EOG\nload: control token: 151654 '<|vision_pad|>' is not marked as EOG\nload: control token: 151650 '<|quad_start|>' is not marked as EOG\nload: control token: 151647 '<|EOT|>' is not marked as EOG\nload: control token: 151643 '<｜end▁of▁sentence｜>' is not marked as EOG\nload: control token: 151645 '<｜Assistant｜>' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 7.62 B\nprint_info: general.name     = DeepSeek R1 Distill Qwen 7B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 152064\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nprint_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-04-25T12:53:00.453+02:00 level=DEBUG source=gpu.go:695 msg=\"no filter required for library cpu\"\ntime=2025-04-25T12:53:00.453+02:00 level=INFO source=server.go:405 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model C:\\\\Users\\\\User\\\\.ollama\\\\models\\\\blobs\\\\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 --ctx-size 8192 --batch-size 512 --verbose --threads 4 --no-mmap --parallel 4 --port 49765\"\ntime=2025-04-25T12:53:00.454+02:00 level=DEBUG source=server.go:423 msg=subprocess environment=\"[CUDA_ERROR_LEVEL=50 CUDA_PATH=C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.4 CUDA_PATH_V12_4=C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.4 CUDA_VISIBLE_DEVICES=GPU-0d1397db-6149-11b2-b139-80f1d9baaab7 PATH=C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v11;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Windows\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA App\\\\NvDLISR;C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2024.1.0\\\\;C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common;C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama;C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v11;C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama;;C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama]\"\ntime=2025-04-25T12:53:00.461+02:00 level=INFO source=sched.go:451 msg=\"loaded runners\" count=1\ntime=2025-04-25T12:53:00.461+02:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\ntime=2025-04-25T12:53:00.462+02:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-04-25T12:53:00.482+02:00 level=INFO source=runner.go:853 msg=\"starting go runner\"\ntime=2025-04-25T12:53:00.512+02:00 level=DEBUG source=ggml.go:99 msg=\"ggml backend load all from path\" path=C:\\Users\\User\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v11\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A16-16A, compute capability 8.6, VMM: no\nload_backend: loaded CUDA backend from C:\\Users\\User\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v11\\ggml-cuda.dll\ntime=2025-04-25T12:53:38.145+02:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\\system32\ntime=2025-04-25T12:53:38.145+02:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\ntime=2025-04-25T12:53:38.145+02:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\\System32\\Wbem\ntime=2025-04-25T12:53:38.145+02:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\\System32\\WindowsPowerShell\\v1.0\ntime=2025-04-25T12:53:38.145+02:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\\System32\\OpenSSH\ntime=2025-04-25T12:53:38.145+02:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA App\\\\NvDLISR\"\ntime=2025-04-25T12:53:38.145+02:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2024.1.0\"\ntime=2025-04-25T12:53:38.145+02:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common\"\ntime=2025-04-25T12:53:38.145+02:00 level=DEBUG source=ggml.go:99 msg=\"ggml backend load all from path\" path=C:\\Users\\User\\AppData\\Local\\Programs\\Ollama\ntime=2025-04-25T12:53:38.146+02:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\ntime=2025-04-25T12:53:38.146+02:00 level=DEBUG source=ggml.go:99 msg=\"ggml backend load all from path\" path=C:\\Users\\User\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\nload_backend: loaded CPU backend from C:\\Users\\User\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-icelake.dll\ntime=2025-04-25T12:53:38.370+02:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)\nCUDA error: all CUDA-capable devices are busy or unavailable\n  current device: 0, in function ggml_backend_cuda_device_get_memory at C:\\a\\ollama\\ollama\\ml\\backend\\ggml\\ggml\\src\\ggml-cuda\\ggml-cuda.cu:2959\n  cudaMemGetInfo(free, total)\nC:\\a\\ollama\\ollama\\ml\\backend\\ggml\\ggml\\src\\ggml-cuda\\ggml-cuda.cu:75: CUDA error\ntime=2025-04-25T12:53:38.536+02:00 level=ERROR source=server.go:449 msg=\"llama runner terminated\" error=\"exit status 0xc0000409\"\ntime=2025-04-25T12:53:38.579+02:00 level=ERROR source=sched.go:457 msg=\"error loading llama server\" error=\"llama runner process has terminated: CUDA error\"\ntime=2025-04-25T12:53:38.579+02:00 level=DEBUG source=sched.go:460 msg=\"triggering expiration for failed load\" model=C:\\Users\\User\\.ollama\\models\\blobs\\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49\n[GIN] 2025/04/25 - 12:53:38 | 500 |   38.4837386s |       127.0.0.1 | POST     \"/api/generate\"\ntime=2025-04-25T12:53:38.579+02:00 level=DEBUG source=sched.go:362 msg=\"runner expired event received\" modelPath=C:\\Users\\User\\.ollama\\models\\blobs\\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49\ntime=2025-04-25T12:53:38.581+02:00 level=DEBUG source=sched.go:377 msg=\"got lock to unload\" modelPath=C:\\Users\\User\\.ollama\\models\\blobs\\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49\ntime=2025-04-25T12:53:38.628+02:00 level=DEBUG source=server.go:1001 msg=\"stopping llama server\"\ntime=2025-04-25T12:53:38.628+02:00 level=DEBUG source=sched.go:382 msg=\"runner released\" modelPath=C:\\Users\\User\\.ollama\\models\\blobs\\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49\ntime=2025-04-25T12:53:38.628+02:00 level=DEBUG source=sched.go:386 msg=\"sending an unloaded event\" modelPath=C:\\Users\\User\\.ollama\\models\\blobs\\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49\ntime=2025-04-25T12:53:38.628+02:00 level=DEBUG source=sched.go:310 msg=\"ignoring unload event with no pending requests\"\n```\n\n### OS\n\nWindows 11\n\n### GPU\n\nNvidia A16\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.6.6",
    "comments": [
      {
        "user": "Sven1403",
        "body": "Changing the profile from A to Q did the Trick :)\n\n![Image](https://github.com/user-attachments/assets/a2bd7216-0e29-437a-bc6a-7572884a1300)"
      },
      {
        "user": "sunisstar",
        "body": "I'm now in the same situation: error=\"cuda driver library failed to get device context 801\"\nHow did you fix it?"
      },
      {
        "user": "Sven1403",
        "body": "@sunisstar see my answer: https://github.com/ollama/ollama/issues/10409#issuecomment-2830407583 vGPU has different profiles. You need the Q Profile"
      }
    ]
  },
  {
    "issue_number": 10981,
    "title": "Ollama v0.9.0 break with Phi4-mini tools declaration",
    "author": "JpEncausse",
    "state": "open",
    "created_at": "2025-06-05T13:16:26Z",
    "updated_at": "2025-06-13T06:23:09Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nDeclaring a Tool with Phi4-mini trigger a parsing error from Ollama. It works with previous version of Ollama\n\n```\n{\n  \"name\": \"LLM_Tool_RAG\",\n  \"description\": \"Recherches hybrides (texte + vecteur) dans la base Orama\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"query\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"term\": { \"type\": \"string\" },\n          \"vector\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"value\": { \"type\": \"string\" }\n            },\n            \"required\": [\"value\"]\n          }\n        },\n        \"required\": [\"term\", \"vector\"]\n      }\n    },\n    \"required\": [\"query\"]\n  }\n}\n```\n\n\n\n### Relevant log output\n\n```shell\nHTTP 500 - Internal Server Error\n{\"error\":\"unexpected end of JSON input\"}\nhttp://localhost:11434/api/chat\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "This seems due to the malformed template in the phi4-mini model, #9437.\n\nUsing a modified template results in tool calls being generated, although in my experience, phi4-mini is a poor tool user.\n```console\n$ ./ollama-tool.py --model phi4-mini:3.8b --system 'You are a digital assistant who is responsible for helping the user with tasks using the provided tools.  For tools calls, return a JSON structure of the form `{\"name\", function, \"arguments\", json-args-list}`' --tools get_datetime --prompt \"What is the the time?\"\nTraceback (most recent call last):\n  File \"/home/rick/docker/aitoolkit/./ollama-tool.py\", line 518, in <module>\n    messages = chat(messages, p)\n               ^^^^^^^^^^^^^^^^^\n  File \"/home/rick/docker/aitoolkit/./ollama-tool.py\", line 464, in chat\n    response = ollama.chat(model=args.model, options=options, messages=messages, tools=tools)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/rick/miniconda3/lib/python3.12/site-packages/ollama/_client.py\", line 342, in chat\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"/home/rick/miniconda3/lib/python3.12/site-packages/ollama/_client.py\", line 180, in _request\n    return cls(**self._request_raw(*args, **kwargs).json())\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/rick/miniconda3/lib/python3.12/site-packages/ollama/_client.py\", line 124, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: unexpected end of JSON input (status code: 500)\n\n\n$ ./ollama-tool.py --model phi4-mini:3.8b-newtooltemplate --system 'You are a digital assistant who is responsible for helping the user with tasks using the provided tools.  For tools calls, return a JSON structure of the form `{\"name\", function, \"arguments\", json-args-list}`' --tools get_datetime --prompt \"What is the the time?\"\ncalling get_datetime({'timezone_name': ''})\nThe current time provided is in the format you requested:\n\n- Fulldate (Full Date and Time): Thursday, June 05, 2025 17:35\n- Just Date: Thursday, June 05, 2025\n- Just Time: 17:35\n\nPlease note that this date seems to be set far into the future from today's actual knowledge cutoff in early 2023. If you're looking for an accurate and current time check based on your local timezone or a specific location's timezone you mentioned earlier like `America/New_York`, please let me know, so I can correct it accordingly!\n```"
      },
      {
        "user": "pavelai",
        "body": "@rick-github All mini models are poor tool users (comparing to big models) as of now. But it's enough for testing and research purposes. Though it should be expected to work fine. But it's broken for a pretty long time, while it's marked as fixed. It's highly confusing to me. How can users update local model to have fixed template?"
      },
      {
        "user": "rick-github",
        "body": "> How can users update local model to have fixed template?\n\nBy modifying the template as show in #9437.  See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for more details on Modelfiles."
      }
    ]
  },
  {
    "issue_number": 10248,
    "title": "InternVL3 Series with Vision, Tools Support, and Quantized Versions",
    "author": "zytoh0",
    "state": "open",
    "created_at": "2025-04-12T13:02:34Z",
    "updated_at": "2025-06-13T01:51:05Z",
    "labels": [
      "model request"
    ],
    "body": "Please add InternVL3 series with both vision and tools support.\n\nModel:\nhttps://huggingface.co/collections/OpenGVLab/internvl3-67f7f690be79c2fe9d74fe9d\n\nInternVL3 is a strong multimodal model with tool-using capabilities, ideal for vision agents and perception-based workflows.\n\nRequesting:\n\n- Vision and tools/function calling support\n\n- A wide range of quantized versions to support different deployment scenarios, as done with [qwen2.5-coder](https://ollama.com/library/qwen2.5-coder/tags), is highly useful.\n\nThank you!",
    "comments": [
      {
        "user": "3unnycheung",
        "body": "a great model"
      },
      {
        "user": "gongysh2004",
        "body": "+1"
      },
      {
        "user": "ayang",
        "body": "+1"
      }
    ]
  },
  {
    "issue_number": 6489,
    "title": "Error 403 occurs when I call ollama's api",
    "author": "brownplayer",
    "state": "closed",
    "created_at": "2024-08-24T10:31:27Z",
    "updated_at": "2025-06-13T01:01:54Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nPrerequisite: Use the C++ interface of ipex-llm as ollama's acceleration backend. Then start the ollama server (port 127.0.0.1.11434). When you use the edge browser plug-in to access the api of ollama, error 403 occurs\r\n![image](https://github.com/user-attachments/assets/b9d4eee0-0293-41e7-9437-23ae4dc660af)\r\n\n\n### OS\n\nWindows\n\n### GPU\n\nIntel\n\n### CPU\n\nIntel\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "The body of the 403 response might provide a reason for the failure, is it possible for you to capture it?  Perhaps in the logs for ipex-llm?"
      },
      {
        "user": "rick-github",
        "body": "So it seems ipex-llm is a shim for ollama to enable intel GPU use, so it's not responsible for the 403s.  What does the edge browser plug-in show when the 403 error is returned?\r\n\r\nWhat does the following return:\r\n```\r\ncurl http://localhost:11434/api/generate -d \"\r\n{\r\n   \\\"model\\\": \\\"llama3.1\\\",\r\n   \\\"prompt\\\": \\\"Why is the sky blue?\\\",\r\n   \\\"stream\\\": false\r\n}\"\r\n```"
      },
      {
        "user": "brownplayer",
        "body": "The command output is as follows: [403:fetchError] The service returns an error, and no permission is granted to access the service"
      }
    ]
  },
  {
    "issue_number": 5567,
    "title": "Nvidia A100 - Ollama Not Using GPU",
    "author": "koayst-rplesson",
    "state": "closed",
    "created_at": "2024-07-09T09:08:58Z",
    "updated_at": "2025-06-12T21:52:18Z",
    "labels": [
      "bug",
      "nvidia",
      "gpu"
    ],
    "body": "Hi,\r\n\r\nI have 2 Nvidia A100 machines and both have the same config and setup sitting on the same network.  Both machines have the same Ubuntu OS setup\r\n\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 20.04.6 LTS\r\nRelease:        20.04\r\nCodename:       focal\r\n\r\nDocker version 24.0.7, build afdd53b\r\n\r\nNVIDIA Container Toolkit CLI version 1.15.0\r\ncommit: ddeeca392c7bd8b33d0a66400b77af7a97e16cef\r\n\r\nWhen I run Ollama docker, machine A has not issue running with GPU.  But machine B, always uses the CPU as the response from LLM is slow (word by word).  When I look at the output log, it said:\r\n\r\nmsg=\"unable to load cuda driver library\" library=/usr/lib/x86_64-linux-gnu/libcuda.so.535.183.01 error=\"cuda driver library init failur             e: 802\"\r\n<img width=\"791\" alt=\"Screenshot 2024-07-09 165815\" src=\"https://github.com/ollama/ollama/assets/167511739/a8d09169-1911-44a4-92d2-3aa45daef8a6\">\r\n\r\nI tried to login into the docker container and have no issue performing \"nvidia-smi\".  I have also rebooted the machine.\r\n\r\nWhat else can I do to try to find out the problem and maybe fix the issue?\r\n",
    "comments": [
      {
        "user": "mbbyn",
        "body": "A usual culprit in such cases is `NVIDIA_VISIBLE_DEVICES` and `CUDA_VISIBLE_DEVICES`, try checking their values and setting them accordingly.\r\n\r\nWe run the `ollama/ollama` image, and these are the relevant env variables set. (You might want to test ollama's official image to reduce the scope of the problem)\r\n![image](https://github.com/ollama/ollama/assets/121847135/f13f2686-d602-4e61-b2c0-e023bd649414)\r\n"
      },
      {
        "user": "koayst-rplesson",
        "body": "> A usual culprit in such cases is `NVIDIA_VISIBLE_DEVICES` and `CUDA_VISIBLE_DEVICES`, try checking their values and setting them accordingly.\r\n> \r\n> We run the `ollama/ollama` image, and these are the relevant env variables set. (You might want to test ollama's official image to reduce the scope of the problem) ![image](https://private-user-images.githubusercontent.com/121847135/346907325-f13f2686-d602-4e61-b2c0-e023bd649414.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA1MjcyNzgsIm5iZiI6MTcyMDUyNjk3OCwicGF0aCI6Ii8xMjE4NDcxMzUvMzQ2OTA3MzI1LWYxM2YyNjg2LWQ2MDItNGU2MS1iMmMwLWUwMjNiZDY0OTQxNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcwOVQxMjA5MzhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jNWYxMWJmYTZhYmQ5NjljODMwNWI5YzczMDBhYzk0YzY0Y2E2Yjk2MzI3YzZkNjJkZTRlY2MyY2ExMmViOTkxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.IYnWGv95pcuYKtvhPlgJhizJhoW5SHMas0VkvjodWF4)\r\n\r\nI checked and the environment variables are same as yours.\r\n<img width=\"456\" alt=\"Screenshot 2024-07-09 202602\" src=\"https://github.com/ollama/ollama/assets/167511739/9b05239b-d3e6-451b-b67d-898030a91402\">\r\n\r\n\r\n"
      },
      {
        "user": "jmorganca",
        "body": "Hi @koayst-rplesson do you know if you have NVLink/NVSwitch fabric manager installed? I saw a similar issue 802 here\r\n\r\nhttps://forums.developer.nvidia.com/t/error-802-system-not-yet-initialized-cuda-11-3/234955\r\n"
      }
    ]
  },
  {
    "issue_number": 11059,
    "title": "Please support the Reranker model in Ollama, urgently needed",
    "author": "dengcao",
    "state": "closed",
    "created_at": "2025-06-12T20:10:03Z",
    "updated_at": "2025-06-12T20:14:20Z",
    "labels": [
      "feature request"
    ],
    "body": "The Reranker model has become very popular, but unfortunately, after so long, Ollama still does not support it.\nPlease support the Reranker model in Ollama, urgently needed.",
    "comments": []
  },
  {
    "issue_number": 11039,
    "title": "0% GPU utilization",
    "author": "udiram",
    "state": "closed",
    "created_at": "2025-06-10T19:42:04Z",
    "updated_at": "2025-06-12T16:25:55Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nRunning Qwen3:4B on an Nvidia 4070 Laptop GPU, 16GB of RAM, 8GB of VRAM, intel i9-13900H shows 0% GPU utilization. logs seem to indicate that the model is loaded onto the GPU and it's being correctly identified. \n\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\n\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4070 Laptop GPU) - 7056 MiB free\n\n\ntakes on the order of 5s to generate a response (after thinking), is there any way to speed this up or ensure that the GPU is being used to generate tokens?\n\nThanks!\n\n### Relevant log output\n\n```shell\ntime=2025-06-10T14:36:18.798-05:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=C:\\Users\\udbha\\.ollama\\models\\blobs\\sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5 gpu=GPU-72d4f421-7574-5561-561c-41c5df8cce38 parallel=2 available=7398752256 required=\"4.9 GiB\"\ntime=2025-06-10T14:36:18.823-05:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"15.6 GiB\" free=\"4.4 GiB\" free_swap=\"8.3 GiB\"\ntime=2025-06-10T14:36:18.823-05:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split=\"\" memory.available=\"[6.9 GiB]\" memory.gpu_o\nverhead=\"0 B\" memory.required.full=\"4.9 GiB\" memory.required.partial=\"4.9 GiB\" memory.required.kv=\"1.1 GiB\" memory.required.allocations=\"[4.9 GiB]\" memory.weights.total=\"2.4 GiB\" memory.weights.repeating=\"2.1 GiB\" memory.weights.nonrepeating=\"304.3 MiB\" memory.graph.full=\"768.0 MiB\" memory.graph.partial=\"768.0 MiB\"\nllama_model_loader: loaded meta data with 27 key-value pairs and 398 tensors from C:\\Users\\udbha\\.ollama\\models\\blobs\\sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 4B\nllama_model_loader: - kv   5:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type  f16:   36 tensors\nllama_model_loader: - type q4_K:  198 tensors\nllama_model_loader: - type q6_K:   19 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.44 GiB (5.20 BPW)\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-10T14:36:18.982-05:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\udbha\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model C:\\\\Users\\\\udbha\\\\.ollama\\\\models\\\\blobs\\\\sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5 --ctx-size 8192 --batch-size 512 --n-gpu-layers 37 --threads 6 --no-mmap --parallel 2 --port 55647\"\ntime=2025-06-10T14:36:18.986-05:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-10T14:36:18.986-05:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-10T14:36:18.986-05:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-06-10T14:36:19.026-05:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\nload_backend: loaded CPU backend from C:\\Users\\udbha\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-alderlake.dll\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\nload_backend: loaded CUDA backend from C:\\Users\\udbha\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\\ggml-cuda.dll\ntime=2025-06-10T14:36:19.170-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)\ntime=2025-06-10T14:36:19.171-05:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:55647\"\ntime=2025-06-10T14:36:19.237-05:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4070 Laptop GPU) - 7056 MiB free\nllama_model_loader: loaded meta data with 27 key-value pairs and 398 tensors from C:\\Users\\udbha\\.ollama\\models\\blobs\\sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 4B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 4B\nllama_model_loader: - kv   5:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type  f16:   36 tensors\nllama_model_loader: - type q4_K:  198 tensors\nllama_model_loader: - type q6_K:   19 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.44 GiB (5.20 BPW)\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen3 4B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 36 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 37/37 layers to GPU\nload_tensors:        CUDA0 model buffer size =  2493.69 MiB\nload_tensors:          CPU model buffer size =   304.28 MiB\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 2\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 1024\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     1.18 MiB\nllama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32\nllama_kv_cache_unified:      CUDA0 KV buffer size =  1152.00 MiB\nllama_kv_cache_unified: KV self size  = 1152.00 MiB, K (f16):  576.00 MiB, V (f16):  576.00 MiB\nllama_context:      CUDA0 compute buffer size =   554.00 MiB\nllama_context:  CUDA_Host compute buffer size =    21.01 MiB\nllama_context: graph nodes  = 1374\nllama_context: graph splits = 2\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "It certainly looks like it's running on the GPU.  Could you include the rest of the log?"
      },
      {
        "user": "pavanrajkg04",
        "body": "The logs suggest that the Qwen3:4B model is being loaded onto the GPU (CUDA0), and the available VRAM (7056 MiB) seems sufficient for the model, which requires approximately 4.9 GiB of memory. However, you're experiencing 0% GPU utilization, and generation takes around 5 seconds per response\n\nPossible causes:\n1. You’re using --ctx-size 8192 and --batch-size 512.\n     - While large context sizes are useful, they can reduce throughput because the model has more work to do per token.\n     - If you don’t need such a long context, reducing it could improve speed and GPU usage.\n2. The log shows flash_attn = 0. Flash Attention improves attention computation efficiency significantly on supported GPUs (like RTX 40xx series). Enabling it might help.\n\n\nSolution : \n1. Use the command below and enable flash attention :\nexport OLLAMA_FLASH_ATTENTION=true\n2. Try reducing --ctx-size from 8192 to 4096 or even 2048 unless you really need long contexts\n3. Try increasing or decreasing the batch size in small increments to find an optimal value\n\nIMPROTANT CHECK OLLAMA IS UPDATED"
      },
      {
        "user": "rick-github",
        "body": "@pavanrajkg04 Please don't give bad advice."
      }
    ]
  },
  {
    "issue_number": 9409,
    "title": "Configuring Ollama to Use a Custom Model Registry",
    "author": "sgadheth31",
    "state": "open",
    "created_at": "2025-02-28T03:11:30Z",
    "updated_at": "2025-06-12T15:16:30Z",
    "labels": [],
    "body": "Modify Ollama's model library configuration to replace the default registry.ollama.ai with a custom URL, such as your own private model registry:\n\nContainerresistery-na.myorganization.net/container-release/myproject/\n\nWhen executing the following command:\n\nOllama run llama3\n\nThe model is pulled from my organization's repository instead of the default registry.ollama.ai. I am deploying this within a Kubernetes environment. How can I ensure that Ollama uses my custom URL for model retrieval rather than the default registry.ollama.ai?",
    "comments": [
      {
        "user": "flywiththetide",
        "body": "You can configure Ollama to pull models from a **custom model registry** instead of the default `registry.ollama.ai` by modifying your setup.\n\n### **Option 1: Set the OLLAMA_REGISTRY_URL**\nIf Ollama supports a **custom registry environment variable**, try setting:\n\n```bash\nexport OLLAMA_REGISTRY_URL=\"https://containerregistry-na.myorganization.net/container-release/myproject/\"\n```\n\nRestart Ollama after applying this setting.\n\n### **Option 2: Modify the Ollama Container in Kubernetes**\nIf running Ollama in **Kubernetes**, update your deployment YAML:\n\n```yaml\ncontainers:\n  - name: ollama\n    image: your-ollama-image\n    env:\n      - name: OLLAMA_REGISTRY_URL\n        value: \"https://containerregistry-na.myorganization.net/container-release/myproject/\"\n```\nApply the change:\n```bash\nkubectl apply -f your-deployment.yaml\n```\n\n### **Option 3: Manually Pull Models**\nIf the registry URL cannot be changed directly, you can **manually pull and store models**:\n\n```bash\ndocker pull containerregistry-na.myorganization.net/container-release/myproject/ollama-model:latest\ndocker tag containerregistry-na.myorganization.net/container-release/myproject/ollama-model:latest ollama-model:latest\n```\n\nLet me know if this works or if your registry requires **authentication settings**!"
      },
      {
        "user": "nesies",
        "body": "are you 2 \"IA\" bots speaking gliberich ?\n\nOLLAMA_REGISTRY_URL do not exists in this repo"
      },
      {
        "user": "ER-EPR",
        "body": "so is there a way to do this?"
      }
    ]
  },
  {
    "issue_number": 10575,
    "title": "Setting priority of Ollama under Windows",
    "author": "pereswa",
    "state": "open",
    "created_at": "2025-05-05T19:42:59Z",
    "updated_at": "2025-06-12T15:14:30Z",
    "labels": [
      "feature request"
    ],
    "body": "I noticed that Ollama runs the instance running an active model with \"above normal\" priority (I hope that's the correct translation for German \"Höher als normal\") in Windows 10 & 11.\nUnfortunatelly this instance is then completely blocking the whole PC when processing a request - it becomes completely unresponsive until Ollama is finished (or cancelled via Ctrl+C).\n\nTherefore I tried to start the server with \"below normal\" priority - but that doesn't work:\nEvery instance loading a model is started with \"above normal\" priority.\nToday I always check the priority before sending a request to make sure it doesn't block my PC.\n\nI didn't find a solution for this - maybe there is a environment variable for this issue?\nI didn't find any documentation about these - two are mentioned in FAQ \"Setting environment variables on Windows\".\n\nSo, I wish there was either an environment variable to set instance priority or the priority was inherited from the server.\n\nWhile looking for a solution I also noticed when limiting the instance running a model to certain CPU cores (simulating a \"below normal\" priority ;) this instance becomes very very very slow. E.g. if limited to 4 of 6 cores, it runs with like 5% of the performance as before when it had all 6 cores.",
    "comments": [
      {
        "user": "mrpras",
        "body": "Try Process Tamer - was the lightest solution for me in terms of something I want to leave running constantly and it's simple, just works.\n\nwww donationcoder com/software/mouser/popular-apps/process-tamer"
      },
      {
        "user": "pereswa",
        "body": "This tool doesn't seem to work on Windows 10 or 11."
      },
      {
        "user": "pereswa",
        "body": "I found the reason for this behaviour:\nhttps://github.com/ollama/ollama/blob/main/llm/llm_windows.go#L22\n\nThe explanation _\"Setting Above Normal priority class ensures when running as a \"background service\" with \"programs\" given best priority, we aren't starved of cpu cycles\"_ seems to be valid for current and recent but not for older hardware.\n\nOn a i3-1115G4 Ollama runs fine with above normal priority.\nBut on my i5-8500t it grabs all ressources and renders the PC unresponsive while processing requests.\nBoth running Windows 10.\n\nI just tried \"How to Permanently Set Priority Processes Using Registry Editor\":\nhttps://answers.microsoft.com/en-us/windows/forum/all/how-to-permanently-set-priority-processes-using/df82bd40-ce52-4b84-af34-4d93da17d079\nAnd it works, kind of:\n\nI started the server with: start \"Ollama\" /low \"%LOCALAPPDATA%\\Programs\\Ollama\\ollama app.exe\"\n-> it runs with Low (Niedrig) _OK_\n\nSubsequently started programs are started with th priority set via registry, as described above.\n-> these run with Below normal (Niedriger als Normal) _OK_\n\nBut the also subsequently started model instance overrides the registry setting (nor inherits priority from server) and sets its own priority.\n-> the model runs with Above normal (Höher als normal) **_Not OK_**\n\n![Image](https://github.com/user-attachments/assets/63430ef8-6e84-43cf-8c6a-7dfa03872599)\n\nSo, the current implementation overrides all local settings and preferences.\n\nMaybe this should be seen as a support for older/weaker hardware fix.\n"
      }
    ]
  },
  {
    "issue_number": 9408,
    "title": "how to use slurm to run the Ollama service.",
    "author": "jjxyhb",
    "state": "open",
    "created_at": "2025-02-28T03:11:25Z",
    "updated_at": "2025-06-12T13:55:32Z",
    "labels": [
      "question"
    ],
    "body": "I want to configure ollama on a specific node and gpu in a lenovo lico cluster   Thanks",
    "comments": [
      {
        "user": "flywiththetide",
        "body": "You can run Ollama on a **specific node and GPU** in a Slurm-managed cluster by using a Slurm job script. Here’s how to do it:\n\n### **1. Create a Slurm Job Script (`run_ollama.sh`)**\nCreate a Slurm script to request a **specific node and GPU**:\n```bash\n#!/bin/bash\n#SBATCH --job-name=ollama-job\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --constraint=YOUR_GPU_MODEL\n#SBATCH --output=ollama_output.log\n#SBATCH --error=ollama_error.log\n\n# Load necessary modules\nmodule load cuda/12.1\n\n# Activate the environment if needed\nsource ~/.bashrc\nconda activate ollama_env  # or source venv/bin/activate\n\n# Run Ollama server\nollama serve\n```\n\n### **2. Submit the Job**\nSubmit the script using:\n```bash\nsbatch run_ollama.sh\n```\nThis will:\n- **Request one GPU** on an available node.\n- **Log outputs** to `ollama_output.log`.\n- **Ensure Ollama runs** with GPU acceleration.\n\n### **3. Verify GPU Allocation**\nOnce the job is running, check GPU usage with:\n```bash\nsqueue --user=$USER\nsrun --jobid=<JOB_ID> nvidia-smi\n```\n\nLet me know if you need help adjusting this for **specific GPUs or multi-node execution**!"
      },
      {
        "user": "giuliofrey",
        "body": "I created a template that might be useful for others that come to this issue. You can find it [here](https://github.com/giuliofrey/slurm_ollama)"
      }
    ]
  },
  {
    "issue_number": 7779,
    "title": "Models get stuck in stopping state",
    "author": "ProjectMoon",
    "state": "closed",
    "created_at": "2024-11-21T13:34:58Z",
    "updated_at": "2025-06-12T12:23:53Z",
    "labels": [
      "bug",
      "top"
    ],
    "body": "### What is the issue?\n\nPossibly similar or related to https://github.com/ollama/ollama/issues/6380. But I'm opening this issue because I don't see semaphore errors when this happens.\r\n\r\nI notice that when I am issuing a lot of requests (code generation/fill in the middle tasks), the code completion model cannot stop, and then ollama gets locked up. Logs (without debug mode on) just show the model being loaded, and then any subsequent API requests. The service has to be restarted in order to get responses working again. I actually set up a 0.3.14 ollama instance proxied through OpenWebUI to get the code completion model working properly, as 0.3.14 does not have this problem.\n\n### OS\n\nLinux\n\n### GPU\n\nAMD\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.4.2,0.43-rc0",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) may aid in debugging.  Does it get in this state if `OLLAMA_NUM_PARALLEL=1`?"
      },
      {
        "user": "jessegross",
        "body": "If you are able to turn on OLLAMA_DEBUG and run it with 0.4.3-rc0 (or later) that would greatly help. When it gets stuck is the system idle or is there 100% CPU/GPU usage?"
      },
      {
        "user": "ProjectMoon",
        "body": "So I just spent a good 20 minutes messing with this trying to get it to lock up, but it hasn't happened with 0.4.3-rc0. Which is odd because I could've swore it was happening with it this morning. Even messing around with 0.4.2 a bit doesn't do it. I will continue using the code completion on 0.4.3-rc0 and see what happens. Not able to reproduce again yet, which is odd because I was able to reproduce it this morning, and last night..."
      }
    ]
  },
  {
    "issue_number": 2169,
    "title": "Inference with OpenVINO on Intel",
    "author": "ddpasa",
    "state": "open",
    "created_at": "2024-01-24T10:02:33Z",
    "updated_at": "2025-06-12T11:50:24Z",
    "labels": [
      "feature request"
    ],
    "body": "I think Intel CPUs/GPUs now support more efficient inference with OpenVINO.  See example here with LLAVA: https://docs.openvino.ai/2023.2/notebooks/257-llava-multimodal-chatbot-with-output.html\r\n\r\nIt would be great if ollama could automatically default to OpenVINO on Intel systems.",
    "comments": [
      {
        "user": "Kreijstal",
        "body": "this would be great, there is no need for CUDA by default (if you dont own the hardware), but your own hardware by default...\r\n"
      },
      {
        "user": "Kreijstal",
        "body": "so either built for all systems and decide at runtime which to use, or there are two different packages ollama-cuda and ollama-openvino, right?"
      },
      {
        "user": "Kreijstal",
        "body": "I mean currently, it simply installs cuda things my hardware doesn't even support, it should at least ask, if I have cuda! "
      }
    ]
  },
  {
    "issue_number": 11041,
    "title": "llama3.2-vision enters infinite loop on Kubernetes with NVIDIA A10 GPU",
    "author": "LaeraFelipe",
    "state": "open",
    "created_at": "2025-06-10T23:14:58Z",
    "updated_at": "2025-06-12T10:46:20Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI'm running Ollama with the llama3.2-vision:latest model on a Kubernetes cluster using an NVIDIA A10 GPU. Alongside this model, I'm also running others such as gemma3:4b, llama3.2:3b, etc.—all of which work normally. However, when I run prompts with llama3.2-vision, it initially appears to work fine but eventually enters an infinite loop, as shown in the image below:\n\n![Image](https://github.com/user-attachments/assets/13964dd4-cd5c-4b28-a485-8d791932a4b8)\n\n![Image](https://github.com/user-attachments/assets/05795e7b-6c0a-423d-b944-5482f4724a5f)\n\n\nIn the following image, you can see that Ollama is the only process using the GPU, and during the loop, it fully utilizes the GPU's processing power. Note that this container has no CPU, memory, or GPU limits defined.\n\n![Image](https://github.com/user-attachments/assets/cda21240-61fb-4bd3-a023-6cedb320eab2)\n\nThe processing only stops when the prompt is manually canceled.\n\nAfter this initial loop scenario, every subsequent attempt to send an image and ask a question results in the same behavior:\n\n![Image](https://github.com/user-attachments/assets/4da0e37f-dfdc-4ab1-a2e8-60a3e425e5e1)\n\nAfter approximately 5 minutes, the model is unloaded from memory. If I then submit a new prompt, the loop scenario occurs again.\n\nHere are the details of the GPU driver I'm using:\n\n![Image](https://github.com/user-attachments/assets/7eb7c0cd-6746-4b71-8da5-10e203892144)\n\nI also have another server with the same configuration, running Ollama version 0.6.7 via Docker Compose, using a llama3.2-vision:latest model downloaded around six months ago—and that setup works perfectly.\n\nHere is the nvidia-smi output from that machine:\n\n![Image](https://github.com/user-attachments/assets/5d88b1f2-fae9-4a07-b9b6-6586ec8c4ca6)\n\n\n\n### Relevant log output\n\n```shell\n[GIN] 2025/06/10 - 22:54:54 | 200 |   7.45266341s |     10.0.10.123 | POST     \"/api/chat\"\ntime=2025-06-10T22:54:54.256Z level=DEBUG source=sched.go:434 msg=\"context for request finished\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096\ntime=2025-06-10T22:54:54.256Z level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 duration=5m0s\ntime=2025-06-10T22:54:54.257Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 refCount=0\n[GIN] 2025/06/10 - 22:55:09 | 200 |     474.068µs |     10.0.10.194 | GET      \"/api/tags\"\ntime=2025-06-10T22:55:09.589Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-10T22:55:09.590Z level=WARN source=sched.go:140 msg=\"mllama does not currently support parallel requests\"\ntime=2025-06-10T22:55:09.590Z level=DEBUG source=sched.go:615 msg=\"evaluating already loaded\" model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed\ntime=2025-06-10T22:55:09.612Z level=DEBUG source=server.go:729 msg=\"completion request\" images=4 prompt=3769 format=\"\"\ntime=2025-06-10T22:55:09.825Z level=DEBUG source=cache.go:136 msg=\"loading cache slot\" id=0 cache=711 prompt=737 used=711 remaining=26\n[GIN] 2025/06/10 - 22:55:16 | 200 |  7.456949066s |     10.0.10.194 | POST     \"/api/chat\"\ntime=2025-06-10T22:55:16.884Z level=DEBUG source=sched.go:434 msg=\"context for request finished\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096\ntime=2025-06-10T22:55:16.884Z level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 duration=5m0s\ntime=2025-06-10T22:55:16.884Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 refCount=0\ntime=2025-06-10T22:56:12.674Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-10T22:56:12.675Z level=WARN source=sched.go:140 msg=\"mllama does not currently support parallel requests\"\ntime=2025-06-10T22:56:12.675Z level=DEBUG source=sched.go:615 msg=\"evaluating already loaded\" model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed\ntime=2025-06-10T22:56:12.710Z level=DEBUG source=server.go:729 msg=\"completion request\" images=6 prompt=6397 format=\"\"\ntime=2025-06-10T22:56:13.052Z level=DEBUG source=cache.go:136 msg=\"loading cache slot\" id=0 cache=1265 prompt=1286 used=1044 remaining=242\n[GIN] 2025/06/10 - 22:56:22 | 200 | 10.528935852s |     10.0.10.123 | POST     \"/api/chat\"\ntime=2025-06-10T22:56:22.992Z level=DEBUG source=sched.go:434 msg=\"context for request finished\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096\ntime=2025-06-10T22:56:22.992Z level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 duration=5m0s\ntime=2025-06-10T22:56:22.992Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 refCount=0\ntime=2025-06-10T22:56:34.461Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-10T22:56:34.462Z level=WARN source=sched.go:140 msg=\"mllama does not currently support parallel requests\"\ntime=2025-06-10T22:56:34.462Z level=DEBUG source=sched.go:615 msg=\"evaluating already loaded\" model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed\ntime=2025-06-10T22:56:34.513Z level=DEBUG source=server.go:729 msg=\"completion request\" images=7 prompt=7727 format=\"\"\ntime=2025-06-10T22:56:34.988Z level=DEBUG source=cache.go:136 msg=\"loading cache slot\" id=0 cache=1599 prompt=1619 used=1414 remaining=205\n[GIN] 2025/06/10 - 22:56:39 | 200 |     275.564µs |     10.0.10.194 | GET      \"/api/tags\"\ntime=2025-06-10T22:59:32.871Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-10T22:59:32.872Z level=WARN source=sched.go:140 msg=\"mllama does not currently support parallel requests\"\ntime=2025-06-10T22:59:32.872Z level=DEBUG source=sched.go:615 msg=\"evaluating already loaded\" model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed\ntime=2025-06-10T22:59:32.872Z level=DEBUG source=server.go:729 msg=\"completion request\" images=1 prompt=152 format=\"\"\ntime=2025-06-10T22:59:32.957Z level=DEBUG source=cache.go:136 msg=\"loading cache slot\" id=0 cache=2427 prompt=21 used=4 remaining=17\n[GIN] 2025/06/10 - 22:59:35 | 200 |  2.686124122s |     10.0.10.232 | POST     \"/api/chat\"\ntime=2025-06-10T22:59:35.477Z level=DEBUG source=sched.go:434 msg=\"context for request finished\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096\ntime=2025-06-10T22:59:35.477Z level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 duration=5m0s\ntime=2025-06-10T22:59:35.477Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 refCount=0\ntime=2025-06-10T22:59:35.581Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-10T22:59:35.582Z level=WARN source=sched.go:140 msg=\"mllama does not currently support parallel requests\"\ntime=2025-06-10T22:59:35.582Z level=DEBUG source=sched.go:615 msg=\"evaluating already loaded\" model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed\ntime=2025-06-10T22:59:35.582Z level=DEBUG source=server.go:729 msg=\"completion request\" images=0 prompt=1470 format=\"\"\ntime=2025-06-10T22:59:35.583Z level=DEBUG source=cache.go:136 msg=\"loading cache slot\" id=0 cache=37 prompt=318 used=4 remaining=314\n```\n\n### OS\n\nDocker\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "Do you have logs from around the time the request was manually cancelled?"
      },
      {
        "user": "LaeraFelipe",
        "body": "> Do you have logs from around the time the request was manually cancelled?\n\nYes! These are the latest logs:\n\n```\ntime=2025-06-10T22:57:39.869Z level=DEBUG source=cache.go:272 msg=\"context limit hit - shifting\" id=0 limit=4096 input=4096 keep=4 discard=2046\ntime=2025-06-10T22:57:45.896Z level=DEBUG source=sched.go:434 msg=\"context for request finished\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096\n[GIN] 2025/06/10 - 22:57:45 | 200 |         1m11s |     10.0.10.123 | POST     \"/api/chat\"\ntime=2025-06-10T22:57:45.896Z level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 duration=5m0s\ntime=2025-06-10T22:57:45.896Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 refCount=0\n[GIN] 2025/06/10 - 22:58:09 | 200 |     517.229µs |     10.0.10.194 | GET      \"/api/tags\"\n[GIN] 2025/06/10 - 22:58:39 | 200 |     948.107µs |     10.0.10.194 | GET      \"/api/tags\"\n[GIN] 2025/06/10 - 22:59:09 | 200 |     840.522µs |     10.0.10.194 | GET      \"/api/tags\"\n[GIN] 2025/06/10 - 22:59:10 | 200 |      41.432µs |      10.0.10.46 | GET      \"/api/version\"\n[GIN] 2025/06/10 - 22:59:12 | 200 |      22.218µs |     10.0.10.123 | GET      \"/api/version\"\n[GIN] 2025/06/10 - 22:59:18 | 200 |      22.566µs |      10.0.10.46 | GET      \"/api/version\"\ntime=2025-06-10T22:59:32.871Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-10T22:59:32.872Z level=WARN source=sched.go:140 msg=\"mllama does not currently support parallel requests\"\ntime=2025-06-10T22:59:32.872Z level=DEBUG source=sched.go:615 msg=\"evaluating already loaded\" model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed\ntime=2025-06-10T22:59:32.872Z level=DEBUG source=server.go:729 msg=\"completion request\" images=1 prompt=152 format=\"\"\ntime=2025-06-10T22:59:32.957Z level=DEBUG source=cache.go:136 msg=\"loading cache slot\" id=0 cache=2427 prompt=21 used=4 remaining=17\ntime=2025-06-10T22:59:35.583Z level=DEBUG source=cache.go:136 msg=\"loading cache slot\" id=0 cache=37 prompt=318 used=4 remaining=314\n[GIN] 2025/06/10 - 22:59:35 | 200 |  315.450811ms |     10.0.10.194 | POST     \"/api/chat\"\ntime=2025-06-10T22:59:35.875Z level=DEBUG source=sched.go:434 msg=\"context for request finished\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096\ntime=2025-06-10T22:59:35.875Z level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 duration=5m0s\ntime=2025-06-10T22:59:35.875Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 refCount=0\ntime=2025-06-10T22:59:35.969Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-10T22:59:35.970Z level=WARN source=sched.go:140 msg=\"mllama does not currently support parallel requests\"\ntime=2025-06-10T22:59:35.970Z level=DEBUG source=sched.go:615 msg=\"evaluating already loaded\" model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed\ntime=2025-06-10T22:59:35.970Z level=DEBUG source=server.go:729 msg=\"completion request\" images=0 prompt=930 format=\"\"\ntime=2025-06-10T22:59:35.971Z level=DEBUG source=cache.go:136 msg=\"loading cache slot\" id=0 cache=330 prompt=203 used=8 remaining=195\n[GIN] 2025/06/10 - 22:59:36 | 200 |  263.572345ms |     10.0.10.123 | POST     \"/api/chat\"\ntime=2025-06-10T22:59:36.212Z level=DEBUG source=sched.go:434 msg=\"context for request finished\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096\ntime=2025-06-10T22:59:36.212Z level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 duration=5m0s\ntime=2025-06-10T22:59:36.212Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/llama3.2-vision:latest runner.inference=cuda runner.devices=1 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=1 runner.pid=1051612 runner.model=/root/.ollama/models/blobs/sha256-9999d473417a8e179d993498195be5f42cab963acc75f4a6b15d981e8b68abed runner.num_ctx=4096 refCount=0\n[GIN] 2025/06/10 - 22:59:39 | 200 |     297.219µs |     10.0.10.194 | GET      \"/api/tags\"\n```\n\n"
      },
      {
        "user": "rick-github",
        "body": "```\ntime=2025-06-10T22:57:39.869Z level=DEBUG source=cache.go:272 msg=\"context limit hit - shifting\" id=0 limit=4096 input=4096 keep=4 discard=2046\n```\nYour context window is too small.  The runner fills up the available context space and then has to shift the context buffer to make room for new tokens.  This results in the head of the buffer being lost which can cause the model to lose coherence, after which it can get in a state of emitting random tokens until interrupted or the internal repeated token limit triggers and stops the generation.\n\nIn the ideal world a buffer shift shouldn't cause this loss of coherence, so this bears looking into.  A similar problem was recently discussed in the [discord](https://discord.com/channels/1128867683291627614/1381942795144527993).  In the meantime, increasing the size of the context buffer or setting `num_predict` to limit the number of tokens generated will mitigate the issue."
      }
    ]
  },
  {
    "issue_number": 11023,
    "title": "ollama version is 0.5.7 Warning: client version is 0.9.0",
    "author": "michael-learns",
    "state": "closed",
    "created_at": "2025-06-09T03:56:19Z",
    "updated_at": "2025-06-12T10:17:27Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI've been trying to update my ollama to latest version.\nBut after uninstalling and reinstalling the ollama app I'm getting the following error.\n\ndue to this i can't install the gemma models.\n\n### Relevant log output\n\n```shell\nollama version is 0.5.7\nWarning: client version is 0.9.0\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n0.5.7",
    "comments": [
      {
        "user": "rick-github",
        "body": "Restart the ollama server."
      },
      {
        "user": "helxsz",
        "body": "just got the same issue with ubuntu 20.0.4\n\n```\nollama version is 0.5.7\nWarning: client version is 0.9.0\n```\n\nI tried to reinstall the ollama with two ways.\n```\n1. curl -fsSL https://ollama.com/install.sh | sh\n\n2. wget https://github.com/ollama/ollama/releases/download/v0.9.1-rc0/ollama-linux-arm64.tgz    sudo mv lib /usr/local/lib\n    sudo mv bin /usr/local/bin\n```\neven restarting the ollama instance is not working \n```\n    systemctl stop ollama\n    sudo mv lib /usr/local/lib\n    sudo mv bin /usr/local/bin\n    systemctl restart ollama\n```\nneed to look into this issue\n"
      },
      {
        "user": "rick-github",
        "body": "You are running an old server, you need to stop the old server so that the new server can take its place.  If `systemctl stop ollama` is not effective, perhaps the old server was not installed in a `systemctl` supported manner.  In which case, try killing the server with `sudo kill $(pidof ollama)`.  If `ps p$(pidof ollama)` still shows a running ollama server after the kill, then you need to identify the service that is restarting ollama."
      }
    ]
  },
  {
    "issue_number": 9054,
    "title": "Ollama Does Not Utilize Multiple Instances of the Same Model for Parallel Processing",
    "author": "BennisonDevadoss",
    "state": "closed",
    "created_at": "2025-02-12T19:11:49Z",
    "updated_at": "2025-06-12T09:52:17Z",
    "labels": [],
    "body": "I have a server with two Nvidia L4 GPUs, and I’m running the LLaMA 3.1 8B model using Ollama. Here’s the current behavior:  \n\n1. **What Works:**  \n   - If a model is large enough to require both GPUs, Ollama successfully splits the workload and utilizes both GPUs for the same instance.  \n   - If different models are loaded, Ollama utilizes the available VRAM efficiently and runs them concurrently across the GPUs.  \n\n2. **What Does Not Work:**  \n   - When multiple users send concurrent requests, Ollama doesn’t load multiple instances of the same model on available VRAM to handle parallel requests. I’ve set the `OLLAMA_NUM_PARALLEL` parameter to 3, but it doesn’t seem to have any effect.  \n\n**What I Want to Achieve:**  \n- I’d like Ollama to load multiple instances of the same model on different GPUs or available VRAM to handle parallel user requests efficiently.  \n\n**Questions:**  \n- Why doesn’t Ollama load multiple instances of the same model for parallel processing?  \n- Is there any configuration or workaround to achieve this behavior?  \n\nAny help or insights would be greatly appreciated!",
    "comments": [
      {
        "user": "rick-github",
        "body": "ollama doesn't load a model multiple times, what `OLLAMA_NUM_PARALLEL` does is create a context buffer for each parallel request.  The model weights are the same for each context buffer.  The clients send 3 simultaneous requests and they get processed concurrently.  If more simultaneous requests than completion slots are sent, they are queued up until one of the on-going completions is finished.  See [here](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-does-ollama-handle-concurrent-requests) for more info on concurrent request processing."
      },
      {
        "user": "pdevine",
        "body": "@BennisonDevadoss as @rick-github mentioned, you don't need to waste the VRAM loading the same model into memory multiple times. I'm going to go ahead and close the issue as answered, but feel free to keep commenting."
      },
      {
        "user": "BilibalaX",
        "body": "I have a similar question. With four GPUs (40GB VRAM), the model size (20GB) is ok to load on each GPU. My task is to input 100,000 messages for information extraction. If I set OLLAMA_NUM_PARALLEL as 4, does it mean I have tasks running on each of the GPUs separately and I can have 4 times the speed?\n\nThen the other parameter OLLAMA_MAX_LOADED_MODELS is the maximum number of models that can be loaded. If I set it as 4, will Ollama regard them as 4 instances?\n"
      }
    ]
  },
  {
    "issue_number": 11055,
    "title": "Ollama uses CPU only after upgrading to CUDA 12.8",
    "author": "NEWbie0709",
    "state": "open",
    "created_at": "2025-06-12T09:32:00Z",
    "updated_at": "2025-06-12T09:39:42Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nAfter upgrading to CUDA 12.8 (from a previously working CUDA setup), Ollama now runs models using the CPU only, whereas it previously utilized the GPU at 100%. No changes were made to the model or Ollama configuration besides the CUDA upgrade.\n\n![Image](https://github.com/user-attachments/assets/df631463-6009-4e5e-8ce0-2f745ffbfdff)\n\n💻 System Info:\n```\nGPU: 3× NVIDIA GTX 1080 Ti\n\nCUDA Version: 12.8\n\nNVIDIA Driver Version: 570.86.10\n\nOS: Ubuntu 20.04.5 LTS\n\nollama version is 0.9.0\n\n```",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will aid in debugging."
      }
    ]
  },
  {
    "issue_number": 11053,
    "title": "Please support the Rerank model",
    "author": "dzy888",
    "state": "closed",
    "created_at": "2025-06-12T08:24:04Z",
    "updated_at": "2025-06-12T08:56:36Z",
    "labels": [
      "feature request"
    ],
    "body": "Please support the Rerank model",
    "comments": []
  },
  {
    "issue_number": 3115,
    "title": "Switching between models is very slow?",
    "author": "eliranwong",
    "state": "closed",
    "created_at": "2024-03-13T16:44:07Z",
    "updated_at": "2025-06-12T08:29:07Z",
    "labels": [
      "question"
    ],
    "body": "I noted that switching between models is very slow.  Is it possible to load two different models in memory at the same time?",
    "comments": [
      {
        "user": "pdevine",
        "body": "It's not possible *yet* to load two models simultaneously, but it's something we've been contemplating for a while (and there are a couple issues on this already). There are a couple reasons for the slow switching time, but loading between system memory and the GPU can be pretty slow, particularly if you're loading/unloading large models and/or if you have older hardware.\r\n\r\nGoing to close out the issue, but feel free to keep commenting.\r\n"
      },
      {
        "user": "ivanenev",
        "body": "I am running two models simultaneously but they do not run in parallel, they respond consecutively. I've found a way of running two models,mistral and llama (or two instances of llama) in parallel  by changing the OLLAMA_HOST variable for the second instance. First copy the model:  \r\nollama cp llama3 llama3-2\r\nFor the second instance,change the ip and port like so: \r\nOLLAMA_HOST=0.0.0.0:11435 ollama serve   \r\nOLLAMA_HOST=0.0.0.0:11435 ollama run ollama"
      },
      {
        "user": "FarooqAlaulddin",
        "body": "Running LangFlow pipeline with two models (14b + 7b) in a batch run and here is what I saw. My system is Macbook M4 Pro Chip + 24GB.\n\n![Image](https://github.com/user-attachments/assets/b16d3841-bb66-4aec-991f-9dad0980e20e)"
      }
    ]
  },
  {
    "issue_number": 10939,
    "title": "ollama/ollama Docker image (currently at version 0.9.0) alongside Clara (ClaraVerse)",
    "author": "carnabwth",
    "state": "closed",
    "created_at": "2025-06-01T14:47:36Z",
    "updated_at": "2025-06-12T08:04:46Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nHi Ollama team,\n\nI’m running the official ollama/ollama Docker image (currently at version 0.9.0) alongside Clara (ClaraVerse) as the frontend.\n\nClara’s latest frontend expects backend API functions like generateCompletion and streamCompletion, but the current Docker image and install script only provide the 0.9.0 backend, which lacks those methods.\n\nCould you please publish an updated Docker image (≥0.10.x or 0.11.x) with the latest backend so we can ensure compatibility?\n\nThanks so much for the amazing work you’re doing — looking forward to testing the latest builds!\n\n### Relevant log output\n\n```shell\nollama version is 0.9.0\n\n\n/mnt/c/Users/PC/clara-ollama-setup$ docker images | grep ollama\nclara-ollama-setup-ollama-backend   latest    5a6bbd8f13ad   3 hours ago    5.23GB\nollama/ollama                       latest    2ea3b768a8f2   2 days ago     5.22GB\nclaraverse/clara-ollama             latest    1ca0af3ad163   2 months ago   91.4MB\n\n\n\nI.streamCompletion(...) is not a function or its return value is not async iterable\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "carnabwth",
        "body": "Setup:\nRunning backend via docker-compose.yml with:\n\nservices:\n  ollama-backend:\n    image: ollama/ollama:latest\n    ports:\n      - \"11434:11434\"\n"
      },
      {
        "user": "rick-github",
        "body": "The ollama API is documented [here](https://github.com/ollama/ollama/blob/main/docs/api.md).  `generateCompletion` and `streamCompletion` sound like framework methods, which is not what the API offers.  You probably need to file a ticket with the project that maintains the framework or Clara."
      }
    ]
  },
  {
    "issue_number": 11047,
    "title": "llama runner process has terminated: exit status 2",
    "author": "fishfl",
    "state": "closed",
    "created_at": "2025-06-11T11:09:52Z",
    "updated_at": "2025-06-12T06:26:40Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI follow these instructions in CLI:\n\ndocker pull ollama/ollama:0.9.0\ndocker run -it --entrypoint /bin/sh ollama/ollama:latest\nollama serve &\nollama run gemma3\n\nAnd got the error:\nmsg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 2\"\n\nAnd I tried the other versions(0.7.0 / 0.9.1) and the other models, got the same error.\n\n\n\n### Relevant log output\n\n```shell\nCouldn't find '/root/.ollama/id_ed25519'. Generating new private key.\nYour new public key is:\n\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAWLs45MrvYntAZlkPIn8+/BaGe02MIaaqCVklA+kZPA\n\ntime=2025-06-11T10:56:09.245Z level=INFO source=routes.go:1205 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-06-11T10:56:09.246Z level=INFO source=images.go:463 msg=\"total blobs: 0\"\ntime=2025-06-11T10:56:09.246Z level=INFO source=images.go:470 msg=\"total unused blobs removed: 0\"\ntime=2025-06-11T10:56:09.246Z level=INFO source=routes.go:1258 msg=\"Listening on [::]:11434 (version 0.7.0)\"\ntime=2025-06-11T10:56:09.246Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-11T10:56:09.252Z level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\ntime=2025-06-11T10:56:09.252Z level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=cpu variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"31.3 GiB\" available=\"30.3 GiB\"\n[GIN] 2025/06/11 - 10:56:57 | 200 |      93.216µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/11 - 10:56:57 | 404 |     346.738µs |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-06-11T10:56:59.158Z level=INFO source=download.go:177 msg=\"downloading aeda25e63ebd in 16 208 MB part(s)\"\ntime=2025-06-11T11:00:10.130Z level=INFO source=download.go:177 msg=\"downloading e0a42594d802 in 1 358 B part(s)\"\ntime=2025-06-11T11:00:11.746Z level=INFO source=download.go:177 msg=\"downloading dd084c7d92a3 in 1 8.4 KB part(s)\"\ntime=2025-06-11T11:00:13.361Z level=INFO source=download.go:177 msg=\"downloading 3116c5225075 in 1 77 B part(s)\"\ntime=2025-06-11T11:00:15.019Z level=INFO source=download.go:177 msg=\"downloading b6ae5839783f in 1 489 B part(s)\"\n[GIN] 2025/06/11 - 11:00:33 | 200 |         3m35s |       127.0.0.1 | POST     \"/api/pull\"\n[GIN] 2025/06/11 - 11:00:33 | 200 |  142.680498ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-06-11T11:00:33.379Z level=INFO source=server.go:135 msg=\"system memory\" total=\"31.3 GiB\" free=\"30.2 GiB\" free_swap=\"0 B\"\ntime=2025-06-11T11:00:33.381Z level=INFO source=server.go:168 msg=offload library=cpu layers.requested=-1 layers.model=35 layers.offload=0 layers.split=\"\" memory.available=\"[30.2 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.6 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"450.0 MiB\" memory.required.allocations=\"[5.6 GiB]\" memory.weights.total=\"2.3 GiB\" memory.weights.repeating=\"1.8 GiB\" memory.weights.nonrepeating=\"525.0 MiB\" memory.graph.full=\"517.0 MiB\" memory.graph.partial=\"1.0 GiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\ntime=2025-06-11T11:00:33.532Z level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/usr/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --threads 8 --no-mmap --parallel 2 --port 36941\"\ntime=2025-06-11T11:00:33.533Z level=INFO source=sched.go:472 msg=\"loaded runners\" count=1\ntime=2025-06-11T11:00:33.533Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-11T11:00:33.533Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-06-11T11:00:33.558Z level=INFO source=runner.go:836 msg=\"starting ollama engine\"\ntime=2025-06-11T11:00:33.559Z level=INFO source=runner.go:899 msg=\"Server listening on 127.0.0.1:36941\"\ntime=2025-06-11T11:00:33.698Z level=INFO source=ggml.go:73 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=883 num_key_values=36\nSIGILL: illegal instruction\nPC=0x7f7b553f17c8 m=5 sigcode=2\nsignal arrived during cgo execution\ninstruction bytes: 0xc4 0xc1 0x7a 0x10 0x54 0x5d 0x0 0xc5 0xea 0x59 0xd 0x49 0x26 0x7 0x0 0x48\n\ngoroutine 9 gp=0xc000102e00 m=5 mp=0xc000100008 [syscall]:\nruntime.cgocall(0x55f96826bb00, 0xc000046fc8)\n        runtime/cgocall.go:167 +0x4b fp=0xc000046fa0 sp=0xc000046f68 pc=0x55f9675afecb\ngithub.com/ollama/ollama/ml/backend/ggml/ggml/src._Cfunc_ggml_backend_load_all_from_path(0x7f7b58000ca0)\n        _cgo_gotypes.go:195 +0x3e fp=0xc000046fc8 sp=0xc000046fa0 pc=0x55f9679484de\ngithub.com/ollama/ollama/ml/backend/ggml/ggml/src.init.func1.1({0xc00003e044, 0xf})\n        github.com/ollama/ollama/ml/backend/ggml/ggml/src/ggml.go:97 +0xf5 fp=0xc000047060 sp=0xc000046fc8 pc=0x55f967947f75\ngithub.com/ollama/ollama/ml/backend/ggml/ggml/src.init.func1()\n        github.com/ollama/ollama/ml/backend/ggml/ggml/src/ggml.go:98 +0x526 fp=0xc0000472f0 sp=0xc000047060 pc=0x55f967947dc6\ngithub.com/ollama/ollama/ml/backend/ggml/ggml/src.init.OnceFunc.func2()\n        sync/oncefunc.go:27 +0x62 fp=0xc000047338 sp=0xc0000472f0 pc=0x55f9679477c2\nsync.(*Once).doSlow(0x55f968751b40?, 0x0?)\n        sync/once.go:78 +0xab fp=0xc000047390 sp=0xc000047338 pc=0x55f9675c4e0b\nsync.(*Once).Do(0x0?, 0x55f968751b40?)\n        sync/once.go:69 +0x19 fp=0xc0000473b0 sp=0xc000047390 pc=0x55f9675c4d39\ngithub.com/ollama/ollama/ml/backend/ggml/ggml/src.init.OnceFunc.func3()\n        sync/oncefunc.go:32 +0x2d fp=0xc0000473e0 sp=0xc0000473b0 pc=0x55f96794772d\ngithub.com/ollama/ollama/ml/backend/ggml.devices()\n        github.com/ollama/ollama/ml/backend/ggml/ggml.go:37 +0x1e fp=0xc000047420 sp=0xc0000473e0 pc=0x55f9679b353e\ngithub.com/ollama/ollama/ml/backend/ggml.New({0x55f9688f4b10, 0xc00061c640}, 0xc00012e008, {0xc0001144a0, 0x8, 0x0, 0x0, {0x0, 0x0, 0x0}, ...})\n        github.com/ollama/ollama/ml/backend/ggml/ggml.go:89 +0x4df fp=0xc000047d00 sp=0xc000047420 pc=0x55f9679b3abf\ngithub.com/ollama/ollama/ml.NewBackend({0x55f9688f4b10, 0xc00061c640}, 0xc00012e008, {0xc0001144a0, 0x8, 0x0, 0x0, {0x0, 0x0, 0x0}, ...})\n        github.com/ollama/ollama/ml/backend.go:87 +0x9a fp=0xc000047d68 sp=0xc000047d00 pc=0x55f9679a6ffa\ngithub.com/ollama/ollama/model.New({0x55f9688f4b10, 0xc00061c640}, {0x7ffe722c9dc3?, 0x0?}, {0xc0001144a0, 0x8, 0x0, 0x0, {0x0, 0x0, ...}, ...})\n        github.com/ollama/ollama/model/model.go:107 +0xe6 fp=0xc000047ea8 sp=0xc000047d68 pc=0x55f9679e4a86\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).loadModel(0xc0000fdd40, {0x55f9688f4b10?, 0xc00061c640?}, {0x7ffe722c9dc3?, 0x0?}, {0xc0001144a0, 0x8, 0x0, 0x0, {0x0, ...}, ...}, ...)\n        github.com/ollama/ollama/runner/ollamarunner/runner.go:775 +0x93 fp=0xc000047f20 sp=0xc000047ea8 pc=0x55f967a8cef3\ngithub.com/ollama/ollama/runner/ollamarunner.Execute.gowrap1()\n        github.com/ollama/ollama/runner/ollamarunner/runner.go:872 +0xbd fp=0xc000047fe0 sp=0xc000047f20 pc=0x55f967a8e3bd\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000047fe8 sp=0xc000047fe0 pc=0x55f9675ba901\ncreated by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1\n        github.com/ollama/ollama/runner/ollamarunner/runner.go:872 +0xa2b\n\ngoroutine 1 gp=0xc000002380 m=nil [IO wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc000135638 sp=0xc000135618 pc=0x55f9675b31ce\nruntime.netpollblock(0x55f9675b0f53?, 0x6754cae6?, 0xf9?)\n        runtime/netpoll.go:575 +0xf7 fp=0xc000135670 sp=0xc000135638 pc=0x55f967577fb7\ninternal/poll.runtime_pollWait(0x7f7b6e1c5eb0, 0x72)\n        runtime/netpoll.go:351 +0x85 fp=0xc000135690 sp=0xc000135670 pc=0x55f9675b23e5\ninternal/poll.(*pollDesc).wait(0xc000507a80?, 0x55f9687bca20?, 0x0)\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0001356b8 sp=0xc000135690 pc=0x55f967639827\ninternal/poll.(*pollDesc).waitRead(...)\n        internal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Accept(0xc000507a80)\n        internal/poll/fd_unix.go:620 +0x295 fp=0xc000135760 sp=0xc0001356b8 pc=0x55f96763ebf5\nnet.(*netFD).accept(0xc000507a80)\n        net/fd_unix.go:172 +0x29 fp=0xc000135818 sp=0xc000135760 pc=0x55f9676b1069\nnet.(*TCPListener).accept(0xc000405f00)\n        net/tcpsock_posix.go:159 +0x1b fp=0xc000135868 sp=0xc000135818 pc=0x55f9676c6a1b\nnet.(*TCPListener).Accept(0xc000405f00)\n        net/tcpsock.go:380 +0x30 fp=0xc000135898 sp=0xc000135868 pc=0x55f9676c58d0\nnet/http.(*onceCloseListener).Accept(0x55f9688f4aa0?)\n        <autogenerated>:1 +0x24 fp=0xc0001358b0 sp=0xc000135898 pc=0x55f9678dd024\nnet/http.(*Server).Serve(0xc000035700, {0x55f9688f2718, 0xc000405f00})\n        net/http/server.go:3424 +0x30c fp=0xc0001359e0 sp=0xc0001358b0 pc=0x55f9678b48ec\ngithub.com/ollama/ollama/runner/ollamarunner.Execute({0xc000034130, 0xd, 0xd})\n        github.com/ollama/ollama/runner/ollamarunner/runner.go:900 +0xe69 fp=0xc000135d08 sp=0xc0001359e0 pc=0x55f967a8e029\ngithub.com/ollama/ollama/runner.Execute({0xc000034110?, 0x0?, 0x0?})\n        github.com/ollama/ollama/runner/runner.go:20 +0xc9 fp=0xc000135d30 sp=0xc000135d08 pc=0x55f967a8e949\ngithub.com/ollama/ollama/cmd.NewCLI.func2(0xc000035400?, {0x55f96844706e?, 0x4?, 0x55f968447072?})\n        github.com/ollama/ollama/cmd/cmd.go:1387 +0x45 fp=0xc000135d58 sp=0xc000135d30 pc=0x55f9681ea685\ngithub.com/spf13/cobra.(*Command).execute(0xc0004b8f08, {0xc0005167e0, 0xe, 0xe})\n        github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc000135e78 sp=0xc000135d58 pc=0x55f96772a6bc\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc00052f508)\n        github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc000135f30 sp=0xc000135e78 pc=0x55f96772af05\ngithub.com/spf13/cobra.(*Command).Execute(...)\n        github.com/spf13/cobra@v1.7.0/command.go:992\ngithub.com/spf13/cobra.(*Command).ExecuteContext(...)\n        github.com/spf13/cobra@v1.7.0/command.go:985\nmain.main()\n        github.com/ollama/ollama/main.go:12 +0x4d fp=0xc000135f50 sp=0xc000135f30 pc=0x55f9681eb00d\nruntime.main()\n        runtime/proc.go:283 +0x29d fp=0xc000135fe0 sp=0xc000135f50 pc=0x55f96757f5bd\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000135fe8 sp=0xc000135fe0 pc=0x55f9675ba901\n\ngoroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc000070fa8 sp=0xc000070f88 pc=0x55f9675b31ce\nruntime.goparkunlock(...)\n        runtime/proc.go:441\nruntime.forcegchelper()\n        runtime/proc.go:348 +0xb8 fp=0xc000070fe0 sp=0xc000070fa8 pc=0x55f96757f8f8\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000070fe8 sp=0xc000070fe0 pc=0x55f9675ba901\ncreated by runtime.init.7 in goroutine 1\n        runtime/proc.go:336 +0x1a\n\ngoroutine 3 gp=0xc000003340 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc000071780 sp=0xc000071760 pc=0x55f9675b31ce\nruntime.goparkunlock(...)\n        runtime/proc.go:441\nruntime.bgsweep(0xc00009c000)\n        runtime/mgcsweep.go:316 +0xdf fp=0xc0000717c8 sp=0xc000071780 pc=0x55f96756a11f\nruntime.gcenable.gowrap1()\n        runtime/mgc.go:204 +0x25 fp=0xc0000717e0 sp=0xc0000717c8 pc=0x55f96755e505\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000717e8 sp=0xc0000717e0 pc=0x55f9675ba901\ncreated by runtime.gcenable in goroutine 1\n        runtime/mgc.go:204 +0x66\n\ngoroutine 4 gp=0xc000003500 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x55f968602328?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc000071f78 sp=0xc000071f58 pc=0x55f9675b31ce\nruntime.goparkunlock(...)\n        runtime/proc.go:441\nruntime.(*scavengerState).park(0x55f96917b5c0)\n        runtime/mgcscavenge.go:425 +0x49 fp=0xc000071fa8 sp=0xc000071f78 pc=0x55f967567b69\nruntime.bgscavenge(0xc00009c000)\n        runtime/mgcscavenge.go:658 +0x59 fp=0xc000071fc8 sp=0xc000071fa8 pc=0x55f9675680f9\nruntime.gcenable.gowrap2()\n        runtime/mgc.go:205 +0x25 fp=0xc000071fe0 sp=0xc000071fc8 pc=0x55f96755e4a5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000071fe8 sp=0xc000071fe0 pc=0x55f9675ba901\ncreated by runtime.gcenable in goroutine 1\n        runtime/mgc.go:205 +0xa5\n\ngoroutine 5 gp=0xc000003dc0 m=nil [finalizer wait]:\nruntime.gopark(0x1b8?, 0xc000002380?, 0x1?, 0x23?, 0xc000070688?)\n        runtime/proc.go:435 +0xce fp=0xc000070630 sp=0xc000070610 pc=0x55f9675b31ce\nruntime.runfinq()\n        runtime/mfinal.go:196 +0x107 fp=0xc0000707e0 sp=0xc000070630 pc=0x55f96755d4c7\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000707e8 sp=0xc0000707e0 pc=0x55f9675ba901\ncreated by runtime.createfing in goroutine 1\n        runtime/mfinal.go:166 +0x3d\n\ngoroutine 6 gp=0xc0001d08c0 m=nil [chan receive]:\nruntime.gopark(0xc000223680?, 0xc000224048?, 0x60?, 0x27?, 0x55f967697da8?)\n        runtime/proc.go:435 +0xce fp=0xc000072718 sp=0xc0000726f8 pc=0x55f9675b31ce\nruntime.chanrecv(0xc0000a8310, 0x0, 0x1)\n        runtime/chan.go:664 +0x445 fp=0xc000072790 sp=0xc000072718 pc=0x55f96754f6c5\nruntime.chanrecv1(0x0?, 0x0?)\n        runtime/chan.go:506 +0x12 fp=0xc0000727b8 sp=0xc000072790 pc=0x55f96754f252\nruntime.unique_runtime_registerUniqueMapCleanup.func2(...)\n        runtime/mgc.go:1796\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n        runtime/mgc.go:1799 +0x2f fp=0xc0000727e0 sp=0xc0000727b8 pc=0x55f9675616af\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000727e8 sp=0xc0000727e0 pc=0x55f9675ba901\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n        runtime/mgc.go:1794 +0x85\n\ngoroutine 7 gp=0xc0001d0e00 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc000072f38 sp=0xc000072f18 pc=0x55f9675b31ce\nruntime.gcBgMarkWorker(0xc0000a98f0)\n        runtime/mgc.go:1423 +0xe9 fp=0xc000072fc8 sp=0xc000072f38 pc=0x55f9675609c9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1339 +0x25 fp=0xc000072fe0 sp=0xc000072fc8 pc=0x55f9675608a5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000072fe8 sp=0xc000072fe0 pc=0x55f9675ba901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1339 +0x105\n\ngoroutine 18 gp=0xc000504000 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc00006c738 sp=0xc00006c718 pc=0x55f9675b31ce\nruntime.gcBgMarkWorker(0xc0000a98f0)\n        runtime/mgc.go:1423 +0xe9 fp=0xc00006c7c8 sp=0xc00006c738 pc=0x55f9675609c9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1339 +0x25 fp=0xc00006c7e0 sp=0xc00006c7c8 pc=0x55f9675608a5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00006c7e8 sp=0xc00006c7e0 pc=0x55f9675ba901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1339 +0x105\n\ngoroutine 34 gp=0xc000102380 m=nil [GC worker (idle)]:\nruntime.gopark(0x17aeca47c6d72d?, 0x3?, 0x6c?, 0xd?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc00011a738 sp=0xc00011a718 pc=0x55f9675b31ce\nruntime.gcBgMarkWorker(0xc0000a98f0)\n        runtime/mgc.go:1423 +0xe9 fp=0xc00011a7c8 sp=0xc00011a738 pc=0x55f9675609c9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1339 +0x25 fp=0xc00011a7e0 sp=0xc00011a7c8 pc=0x55f9675608a5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00011a7e8 sp=0xc00011a7e0 pc=0x55f9675ba901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1339 +0x105\n\ngoroutine 8 gp=0xc0001d0fc0 m=nil [GC worker (idle)]:\nruntime.gopark(0x17aeca4763ae93?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc000073738 sp=0xc000073718 pc=0x55f9675b31ce\nruntime.gcBgMarkWorker(0xc0000a98f0)\n        runtime/mgc.go:1423 +0xe9 fp=0xc0000737c8 sp=0xc000073738 pc=0x55f9675609c9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1339 +0x25 fp=0xc0000737e0 sp=0xc0000737c8 pc=0x55f9675608a5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000737e8 sp=0xc0000737e0 pc=0x55f9675ba901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1339 +0x105\n\ngoroutine 19 gp=0xc0005041c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x17aeca4763b8f2?, 0x3?, 0x33?, 0x29?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc00006cf38 sp=0xc00006cf18 pc=0x55f9675b31ce\nruntime.gcBgMarkWorker(0xc0000a98f0)\n        runtime/mgc.go:1423 +0xe9 fp=0xc00006cfc8 sp=0xc00006cf38 pc=0x55f9675609c9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1339 +0x25 fp=0xc00006cfe0 sp=0xc00006cfc8 pc=0x55f9675608a5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00006cfe8 sp=0xc00006cfe0 pc=0x55f9675ba901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1339 +0x105\n\ngoroutine 20 gp=0xc000504380 m=nil [GC worker (idle)]:\nruntime.gopark(0x17aeca4763aa13?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc00006d738 sp=0xc00006d718 pc=0x55f9675b31ce\nruntime.gcBgMarkWorker(0xc0000a98f0)\n        runtime/mgc.go:1423 +0xe9 fp=0xc00006d7c8 sp=0xc00006d738 pc=0x55f9675609c9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1339 +0x25 fp=0xc00006d7e0 sp=0xc00006d7c8 pc=0x55f9675608a5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00006d7e8 sp=0xc00006d7e0 pc=0x55f9675ba901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1339 +0x105\n\ngoroutine 21 gp=0xc000504540 m=nil [GC worker (idle)]:\nruntime.gopark(0x17aeca47b58963?, 0x3?, 0xc9?, 0xa7?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc00006df38 sp=0xc00006df18 pc=0x55f9675b31ce\nruntime.gcBgMarkWorker(0xc0000a98f0)\n        runtime/mgc.go:1423 +0xe9 fp=0xc00006dfc8 sp=0xc00006df38 pc=0x55f9675609c9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1339 +0x25 fp=0xc00006dfe0 sp=0xc00006dfc8 pc=0x55f9675608a5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00006dfe8 sp=0xc00006dfe0 pc=0x55f9675ba901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1339 +0x105\n\ngoroutine 22 gp=0xc000504700 m=nil [GC worker (idle)]:\nruntime.gopark(0x55f969229e00?, 0x1?, 0x27?, 0xfb?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc00006e738 sp=0xc00006e718 pc=0x55f9675b31ce\nruntime.gcBgMarkWorker(0xc0000a98f0)\n        runtime/mgc.go:1423 +0xe9 fp=0xc00006e7c8 sp=0xc00006e738 pc=0x55f9675609c9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1339 +0x25 fp=0xc00006e7e0 sp=0xc00006e7c8 pc=0x55f9675608a5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00006e7e8 sp=0xc00006e7e0 pc=0x55f9675ba901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1339 +0x105\n\ngoroutine 10 gp=0xc000102fc0 m=nil [sync.WaitGroup.Wait]:\nruntime.gopark(0x0?, 0x0?, 0x60?, 0xa0?, 0x0?)\n        runtime/proc.go:435 +0xce fp=0xc00011ced0 sp=0xc00011ceb0 pc=0x55f9675b31ce\nruntime.goparkunlock(...)\n        runtime/proc.go:441\nruntime.semacquire1(0xc0000fdd48, 0x0, 0x1, 0x0, 0x18)\n        runtime/sema.go:188 +0x229 fp=0xc00011cf38 sp=0xc00011ced0 pc=0x55f967592b89\nsync.runtime_SemacquireWaitGroup(0x0?)\n        runtime/sema.go:110 +0x25 fp=0xc00011cf70 sp=0xc00011cf38 pc=0x55f9675b4be5\nsync.(*WaitGroup).Wait(0x0?)\n        sync/waitgroup.go:118 +0x48 fp=0xc00011cf98 sp=0xc00011cf70 pc=0x55f9675c6248\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).run(0xc0000fdd40, {0x55f9688f4b10, 0xc00061c640})\n        github.com/ollama/ollama/runner/ollamarunner/runner.go:344 +0x25 fp=0xc00011cfb8 sp=0xc00011cf98 pc=0x55f967a89645\ngithub.com/ollama/ollama/runner/ollamarunner.Execute.gowrap2()\n        github.com/ollama/ollama/runner/ollamarunner/runner.go:876 +0x28 fp=0xc00011cfe0 sp=0xc00011cfb8 pc=0x55f967a8e2c8\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00011cfe8 sp=0xc00011cfe0 pc=0x55f9675ba901\ncreated by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1\n        github.com/ollama/ollama/runner/ollamarunner/runner.go:876 +0xad5\n\nrax    0x6101304cacd\nrbx    0x0\nrcx    0x0\nrdx    0x4f3cd\nrdi    0x7ffe72383000\nrsi    0x61012ffd700\nrbp    0x7f7b5547bc80\nrsp    0x7f7b6d17aa80\nr8     0x2f46ab17db0e9c\nr9     0x7f7b58008df0\nr10    0x7ffe72382080\nr11    0x7ffe72382090\nr12    0x7f7b5549bc80\nr13    0x7f7b5554de60\nr14    0x7f7b55461ef0\nr15    0x7f7b6d17ab10\nrip    0x7f7b553f17c8\nrflags 0x10246\ncs     0x33\nfs     0x0\ngs     0x0\ntime=2025-06-11T11:00:33.784Z level=ERROR source=sched.go:478 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 2\"\n[GIN] 2025/06/11 - 11:00:33 | 500 |  617.800054ms |       127.0.0.1 | POST     \"/api/generate\"\n```\n\n### OS\n\nLinux\n\n### GPU\n\n_No response_\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.7.0 / 0.9.0 / 0.9.1",
    "comments": [
      {
        "user": "rick-github",
        "body": "The runner is executing an instruction not supported by your CPU:\n```\n0000000000000000 <.data>:\n   0:\tc4 c1 7a 10 54 5d 00 \tvmovss 0x0(%r13,%rbx,2),%xmm2\n   7:\tc5 ea 59 0d 49 26 07 \tvmulss 0x72649(%rip),%xmm2,%xmm1        # 0x72658\n   e:\t00 \n   f:\t48                   \trex.W\n```\nIf you are running ollama in a proxmox VE, you need to set AVX flags: https://pve.proxmox.com/pve-docs/cpu-models.conf.5.html#_example_file\n\nIf it's not a virtual CPU, you can try deleting backends from `/usr/lib/ollama`.  Note that when the docker container is restarted the backends will be restored, so if you go this route you should create a custom image:\n```dockerfile\nFROM ollama/ollama\nRUN for i in sandybridge haswell skylakex icelake alderlake ; do rm /usr/lib/ollama/libggml-cpu-$i.so ; done\n```\n```console\n$ docker build -f Dockerfile -t ollama:noavx .\n```\n\nOtherwise, you can build a custom version of the server by removing unsupported instruction sets as shown [here](https://github.com/ollama/ollama/issues/10918#issuecomment-2923590731)."
      },
      {
        "user": "fishfl",
        "body": "Awesome! It's very useful!\nThank you very much, it works."
      }
    ]
  },
  {
    "issue_number": 10989,
    "title": "support for qwen3-embedding and qwen3-reranker models",
    "author": "pamdla",
    "state": "open",
    "created_at": "2025-06-05T21:04:03Z",
    "updated_at": "2025-06-12T05:12:46Z",
    "labels": [
      "model request"
    ],
    "body": "Hi dear, please add these models, qwen3-embeddings, qwen3-rerankers , which are newly released on https://huggingface.co/Qwen/.",
    "comments": [
      {
        "user": "rick-github",
        "body": "```\nollama pull hf.co/Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0\n```\nollama doesn't currently support ranking models, #3368."
      },
      {
        "user": "pamdla",
        "body": "great, i made it.\nhowever, reranker models are really helpful for rag, please make a plan for them."
      },
      {
        "user": "chaorders1",
        "body": "Looking forward Ollama support. This is the best embedding models. Waiting for years "
      }
    ]
  },
  {
    "issue_number": 7049,
    "title": "ollama does not detect Quadro RTX 4000 - cuda driver library failed to get device context 801",
    "author": "mfzhsn",
    "state": "open",
    "created_at": "2024-09-30T16:24:28Z",
    "updated_at": "2025-06-12T04:36:56Z",
    "labels": [
      "bug",
      "linux",
      "nvidia",
      "needs more info"
    ],
    "body": "### What is the issue?\n\nHi All,\r\n\r\nI installed ollama both (on machine/docker) both with same behaviour of not detecting the GPU. Have LM Studio on the same machine which picks up GPU without any issues.\r\n\r\n```\r\nroot@d50a3f8d8474:/# ollama run phi3.5:3.8b-mini-instruct-q2_K \"\"\r\nroot@d50a3f8d8474:/# ollama ps\r\nNAME                              ID              SIZE      PROCESSOR    UNTIL\r\nphi3.5:3.8b-mini-instruct-q2_K    45b8dc82a846    5.3 GB    100% CPU     4 minutes from now\r\n```\r\n\r\n**Installation**\r\n```\r\n[root@ai ~]# curl -fsSL https://ollama.com/install.sh | sh\r\n>>> Installing ollama to /usr/local\r\n>>> Downloading Linux amd64 bundle\r\n######################################################################## 100.0%#=#=#                                               ######################################################################## 100.0%\r\n>>> Creating ollama user...\r\n>>> Adding ollama user to render group...\r\n>>> Adding ollama user to video group...\r\n>>> Adding current user to ollama group...\r\n>>> Creating ollama systemd service...\r\n>>> Enabling and starting ollama service...\r\nCreated symlink /etc/systemd/system/default.target.wants/ollama.service → /etc/systemd/system/ollama.service.\r\n>>> NVIDIA GPU installed.\r\n```\r\n\r\n*Logs from package installation*\r\n\r\n```\r\n[root@ai ~]# OLLAMA_DEBUG=1 ollama serve\r\nError: listen tcp 127.0.0.1:11434: bind: address already in use\r\n[root@ai ~]# systemctl stop ollama\r\n[root@ai ~]# OLLAMA_DEBUG=1 ollama serve\r\n2024/09/29 03:47:20 routes.go:1153: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\r\ntime=2024-09-29T03:47:20.643-05:00 level=INFO source=images.go:753 msg=\"total blobs: 10\"\r\ntime=2024-09-29T03:47:20.672-05:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\ntime=2024-09-29T03:47:20.672-05:00 level=INFO source=routes.go:1200 msg=\"Listening on 127.0.0.1:11434 (version 0.3.12)\"\r\ntime=2024-09-29T03:47:20.673-05:00 level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama3184037398/runners\r\ntime=2024-09-29T03:47:20.673-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libggml.so.gz\r\ntime=2024-09-29T03:47:20.673-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libllama.so.gz\r\ntime=2024-09-29T03:47:20.673-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/ollama_llama_server.gz\r\ntime=2024-09-29T03:47:20.673-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libggml.so.gz\r\ntime=2024-09-29T03:47:20.674-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libllama.so.gz\r\ntime=2024-09-29T03:47:20.674-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/ollama_llama_server.gz\r\ntime=2024-09-29T03:47:20.674-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libggml.so.gz\r\ntime=2024-09-29T03:47:20.674-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libllama.so.gz\r\ntime=2024-09-29T03:47:20.674-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/ollama_llama_server.gz\r\ntime=2024-09-29T03:47:20.674-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libggml.so.gz\r\ntime=2024-09-29T03:47:20.674-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libllama.so.gz\r\ntime=2024-09-29T03:47:20.674-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/ollama_llama_server.gz\r\ntime=2024-09-29T03:47:20.674-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libggml.so.gz\r\ntime=2024-09-29T03:47:20.675-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libllama.so.gz\r\ntime=2024-09-29T03:47:20.675-05:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/ollama_llama_server.gz\r\ntime=2024-09-29T03:47:20.675-05:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libggml.so.gz\r\ntime=2024-09-29T03:47:20.675-05:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libllama.so.gz\r\ntime=2024-09-29T03:47:20.676-05:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/ollama_llama_server.gz\r\ntime=2024-09-29T03:47:32.712-05:00 level=DEBUG source=common.go:294 msg=\"availableServers : found\" file=/tmp/ollama3184037398/runners/cpu/ollama_llama_server\r\ntime=2024-09-29T03:47:32.712-05:00 level=DEBUG source=common.go:294 msg=\"availableServers : found\" file=/tmp/ollama3184037398/runners/cpu_avx/ollama_llama_server\r\ntime=2024-09-29T03:47:32.713-05:00 level=DEBUG source=common.go:294 msg=\"availableServers : found\" file=/tmp/ollama3184037398/runners/cpu_avx2/ollama_llama_server\r\ntime=2024-09-29T03:47:32.713-05:00 level=DEBUG source=common.go:294 msg=\"availableServers : found\" file=/tmp/ollama3184037398/runners/cuda_v11/ollama_llama_server\r\ntime=2024-09-29T03:47:32.713-05:00 level=DEBUG source=common.go:294 msg=\"availableServers : found\" file=/tmp/ollama3184037398/runners/cuda_v12/ollama_llama_server\r\ntime=2024-09-29T03:47:32.713-05:00 level=DEBUG source=common.go:294 msg=\"availableServers : found\" file=/tmp/ollama3184037398/runners/rocm_v60102/ollama_llama_server\r\ntime=2024-09-29T03:47:32.713-05:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11 cuda_v12]\"\r\ntime=2024-09-29T03:47:32.713-05:00 level=DEBUG source=common.go:50 msg=\"Override detection logic by setting OLLAMA_LLM_LIBRARY\"\r\ntime=2024-09-29T03:47:32.713-05:00 level=DEBUG source=sched.go:105 msg=\"starting llm scheduler\"\r\ntime=2024-09-29T03:47:32.713-05:00 level=INFO source=gpu.go:199 msg=\"looking for compatible GPUs\"\r\ntime=2024-09-29T03:47:32.713-05:00 level=DEBUG source=gpu.go:86 msg=\"searching for GPU discovery libraries for NVIDIA\"\r\ntime=2024-09-29T03:47:32.713-05:00 level=DEBUG source=gpu.go:468 msg=\"Searching for GPU library\" name=libcuda.so*\r\ntime=2024-09-29T03:47:32.713-05:00 level=DEBUG source=gpu.go:491 msg=\"gpu library search\" globs=\"[/usr/local/lib/ollama/libcuda.so* /usr/local/cuda/lib64/libcuda.so* /root/libcuda.so* /usr/local/cuda*/targets/*/lib/libcuda.so* /usr/lib/*-linux-gnu/nvidia/current/libcuda.so* /usr/lib/*-linux-gnu/libcuda.so* /usr/lib/wsl/lib/libcuda.so* /usr/lib/wsl/drivers/*/libcuda.so* /opt/cuda/lib*/libcuda.so* /usr/local/cuda/lib*/libcuda.so* /usr/lib*/libcuda.so* /usr/local/lib*/libcuda.so*]\"\r\ntime=2024-09-29T03:47:32.715-05:00 level=DEBUG source=gpu.go:525 msg=\"discovered GPU libraries\" paths=[/usr/lib64/libcuda.so.560.35.03]\r\nCUDA driver version: 12.6\r\ntime=2024-09-29T03:47:32.878-05:00 level=DEBUG source=gpu.go:118 msg=\"detected GPUs\" count=1 library=/usr/lib64/libcuda.so.560.35.03\r\ntime=2024-09-29T03:47:32.907-05:00 level=INFO source=gpu.go:252 msg=\"error looking up nvidia GPU memory\" error=\"cuda driver library failed to get device context 801\"\r\ntime=2024-09-29T03:47:32.907-05:00 level=DEBUG source=amd_linux.go:376 msg=\"amdgpu driver not detected /sys/module/amdgpu\"\r\ntime=2024-09-29T03:47:32.907-05:00 level=INFO source=gpu.go:347 msg=\"no compatible GPUs were discovered\"\r\nreleasing cuda driver library\r\ntime=2024-09-29T03:47:32.907-05:00 level=INFO source=types.go:107 msg=\"inference compute\" id=0 library=cpu variant=avx2 compute=\"\" driver=0.0 name=\"\" total=\"251.1 GiB\" available=\"240.5 GiB\"\r\n\r\n```\r\n\r\n\r\n**Logs from Docker installation**\r\n\r\n```\r\n[root@ai ~]# docker logs -f ollama\r\n2024/09/30 15:58:28 routes.go:1153: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\r\ntime=2024-09-30T15:58:28.508Z level=INFO source=images.go:753 msg=\"total blobs: 6\"\r\ntime=2024-09-30T15:58:28.509Z level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\ntime=2024-09-30T15:58:28.509Z level=INFO source=routes.go:1200 msg=\"Listening on [::]:11434 (version 0.3.12)\"\r\ntime=2024-09-30T15:58:28.510Z level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[cpu_avx cpu_avx2 cuda_v11 cuda_v12 cpu]\"\r\ntime=2024-09-30T15:58:28.510Z level=INFO source=gpu.go:199 msg=\"looking for compatible GPUs\"\r\ntime=2024-09-30T15:58:28.670Z level=INFO source=gpu.go:252 msg=\"error looking up nvidia GPU memory\" error=\"cuda driver library failed to get device context 801\"\r\ntime=2024-09-30T15:58:28.670Z level=INFO source=gpu.go:347 msg=\"no compatible GPUs were discovered\"\r\ntime=2024-09-30T15:58:28.670Z level=INFO source=types.go:107 msg=\"inference compute\" id=0 library=cpu variant=avx2 compute=\"\" driver=0.0 name=\"\" total=\"251.1 GiB\" available=\"240.4 GiB\"\r\n[GIN] 2024/09/30 - 15:59:07 | 200 |       94.19µs |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/09/30 - 15:59:07 | 200 |   10.954794ms |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2024-09-30T15:59:07.334Z level=INFO source=server.go:103 msg=\"system memory\" total=\"251.1 GiB\" free=\"240.5 GiB\" free_swap=\"4.0 GiB\"\r\ntime=2024-09-30T15:59:07.334Z level=INFO source=memory.go:326 msg=\"offload to cpu\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[240.5 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"8.3 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"4.0 GiB\" memory.required.allocations=\"[8.3 GiB]\" memory.weights.total=\"7.4 GiB\" memory.weights.repeating=\"7.3 GiB\" memory.weights.nonrepeating=\"102.6 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"681.0 MiB\"\r\ntime=2024-09-30T15:59:07.338Z level=INFO source=server.go:388 msg=\"starting llama server\" cmd=\"/usr/lib/ollama/runners/cpu_avx2/ollama_llama_server --model /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 --ctx-size 8192 --batch-size 512 --embedding --log-disable --no-mmap --numa distribute --parallel 4 --port 39753\"\r\ntime=2024-09-30T15:59:07.339Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\r\ntime=2024-09-30T15:59:07.339Z level=INFO source=server.go:587 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-09-30T15:59:07.339Z level=INFO source=server.go:621 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nWARNING: /proc/sys/kernel/numa_balancing is enabled, this has been observed to impair performance\r\nINFO [main] build info | build=10 commit=\"070c75f\" tid=\"140389372093376\" timestamp=1727711947\r\nINFO [main] system info | n_threads=20 n_threads_batch=20 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"140389372093376\" timestamp=1727711947 total_threads=40\r\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"39\" port=\"39753\" tid=\"140389372093376\" timestamp=1727711947\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\n```\r\n\r\n I am able to get all the outputs\r\n \r\n ```\r\n[root@ai ~]# nvidia-smi\r\nSat Sep 28 01:07:12 2024\r\n+-----------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n|-----------------------------------------+------------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                        |               MIG M. |\r\n|=========================================+========================+======================|\r\n|   0  Quadro RTX 4000                Off |   00000000:37:00.0 Off |                  N/A |\r\n| 30%   34C    P8              9W /  125W |       1MiB /   8192MiB |      0%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------------------+\r\n| Processes:                                                                              |\r\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n|        ID   ID                                                               Usage      |\r\n|=========================================================================================|\r\n|  No running processes found                                                             |\r\n+-----------------------------------------------------------------------------------------+\r\n```\r\n\r\n```\r\n[root@ai ~]# nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2024 NVIDIA Corporation\r\nBuilt on Wed_Aug_14_10:10:22_PDT_2024\r\nCuda compilation tools, release 12.6, V12.6.68\r\nBuild cuda_12.6.r12.6/compiler.34714021_0\r\n```\r\n\r\n**OS**\r\nLinux Rocky 9.4\r\n\r\n```\r\n[root@ai ~]# uname -r\r\n5.14.0-427.37.1.el9_4.x86_64\r\n```\r\n\r\n**logs**\r\n```\r\n[root@ai ~]# sudo dmesg | grep -i nvidia\r\n[    1.704573] Loaded X.509 cert 'Rocky Enterprise Software Foundation: Nvidia GPU OOT Signing 101: 816ba9c770e6960cefe378020865d4ebbc352a7d'\r\n[    6.270595] input: HDA NVidia HDMI/DP,pcm=3 as /devices/pci0000:36/0000:36:00.0/0000:37:00.1/sound/card0/input6\r\n[    6.270694] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:36/0000:36:00.0/0000:37:00.1/sound/card0/input7\r\n[    6.270796] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:36/0000:36:00.0/0000:37:00.1/sound/card0/input8\r\n[    6.270843] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:36/0000:36:00.0/0000:37:00.1/sound/card0/input9\r\n[    7.812685] nvidia: loading out-of-tree module taints kernel.\r\n[    7.812696] nvidia: module license 'NVIDIA' taints kernel.\r\n[    7.836076] nvidia: module verification failed: signature and/or required key missing - tainting kernel\r\n[    7.950180] nvidia-nvlink: Nvlink Core is being initialized, major device number 510\r\n[    7.951760] nvidia 0000:37:00.0: enabling device (0140 -> 0143)\r\n[    7.951842] nvidia 0000:37:00.0: vgaarb: VGA decodes changed: olddecodes=io+mem,decodes=none:owns=none\r\n[    8.001592] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  560.35.03  Fri Aug 16 21:39:15 UTC 2024\r\n[    8.115320] nvidia_uvm: module uses symbols from proprietary module nvidia, inheriting taint.\r\n[    8.252789] nvidia-uvm: Loaded the UVM driver, major device number 508.\r\n[    8.307886] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms  560.35.03  Fri Aug 16 21:21:48 UTC 2024\r\n[    8.323807] [drm] [nvidia-drm] [GPU ID 0x00003700] Loading driver\r\n[    9.814993] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:37:00.0 on minor 1\r\n[    9.815921] nvidia 0000:37:00.0: [drm] Cannot find any crtc or sizes\r\n```\r\n\r\n**additional logs**\r\n```\r\n[root@ai ~]# sudo dmesg | grep -i nvrm\r\n[    8.001592] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  560.35.03  Fri Aug 16 21:39:15 UTC 2024\r\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.3.12",
    "comments": [
      {
        "user": "dhiltgen",
        "body": "It looks like the failure causing us not to be able to gather GPU information is `cuda driver library failed to get device context 801`\r\n\r\nWe're currently tracking this via issue #6364 however your description is a bit different in that you seem to be seeing this consistently, not intermittently.\r\n\r\n"
      },
      {
        "user": "mfzhsn",
        "body": "Yes, my case is pretty consistent with 550 and 560 version of drivers. GPU never came up with either version of ollama(only).   "
      },
      {
        "user": "harshsavasil",
        "body": "I'm facing the same issue with RTX 4080. @mfzhsn did you find a fix for this?"
      }
    ]
  },
  {
    "issue_number": 10928,
    "title": "ollama 0.9.0 can support server level think mode",
    "author": "hlstudio",
    "state": "open",
    "created_at": "2025-05-31T08:08:09Z",
    "updated_at": "2025-06-12T02:03:08Z",
    "labels": [
      "feature request"
    ],
    "body": "The release of ollama 0.9.0 has brought improvements to the thinking mode, which can be enabled or disabled via CLI or API. \nIs there any way to keep a specific model in non-thinking mode by default when using it through the serve interface? I'd rather not manually adjust those application settings. \nsuch as \n1、Add option (--nothink) for 'ollama serve' that cannot be overridden\n2、Allow environment-variable: OLLAMA_THINK\n\nor anyway like hack the model template file ?",
    "comments": [
      {
        "user": "shakti-bhakta",
        "body": "I also think this would be helpful. I am using Chatbox as the chat interface.\nI cannot find any way to disable thinking since the `/set nothink` command does not work and I also cannot modify the json request sent.\n\nIt seem like this suggestion would solve my issue as well."
      },
      {
        "user": "webdev23",
        "body": "Side note, do not break the API. \n\nDevs added that \"thinking\" capabilities, ok, great.\n\nBut:\n\n> In Ollama's API, a model's thinking is now returned as a separate thinking field for easy parsing\n\nThanks but that is a deep change in the parsing response, so peoples making their own UI and tools have to rewrite response handling.\n\nPlease add capabilities but **do not break existing previous API calls and responses**, so we could implement the newest capabilities changes without the mess."
      },
      {
        "user": "bakman2",
        "body": "> Thanks but that is a deep change in the parsing response, so peoples making their own UI and tools have to rewrite response handling.\n\nIf you do not specify `think:true/false` in the request, the response will be as previously, ie. should be non-breaking."
      }
    ]
  },
  {
    "issue_number": 941,
    "title": "`digest mismatch` on download",
    "author": "jmorganca",
    "state": "open",
    "created_at": "2023-10-28T17:47:23Z",
    "updated_at": "2025-06-12T01:15:12Z",
    "labels": [
      "bug"
    ],
    "body": "While rare, `ollama pull` will sometimes result in a digest mismatch on download\r\n\r\n```\r\n% ollama run wizard-vicuna-uncensored:30b-q5_K_M\r\npulling manifest\r\npulling b1571c5cbd28... 100% |█████████████████████████████████████████████████████████████████████████████████████████████████████████| (45/45 B, 34 B/s)        \r\npulling d14264189a8a... 100% |█████████████████████████████████████████████████████████████████████████████████████████████████████████| (31/31 B, 17 B/s)        \r\npulling c4c2b65331ba... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████| (384/384 B, 238 B/s)        \r\nverifying sha256 digest\r\n\r\nError: digest mismatch, file must be downloaded again: want sha256:1a640cd4d69a5260bcc807a531f82ddb3890ebf49bc2a323e60a9290547135c1, got sha256:5eef5d8ec5ce977b74f91524c0002f9a7adeb61606cdbdad6460e25d58d0f454\r\n```",
    "comments": [
      {
        "user": "yohskar",
        "body": "Sadly not so rare "
      },
      {
        "user": "enumouse",
        "body": "Can confirm I get this error too.\r\n\r\n`Error: digest mismatch, file must be downloaded again: want sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2, got sha256:c05efac18bab97102ead2aeba0024d180264f658b5fcca629e9da6e462b69595`\r\n \r\nI retried the download again and got a different hash again `sha256:1a5e92f4ae4bc51dcdc8432154a8ba1ce1411e1c09ff22abde654c174375ea50`\r\n\r\nMy device is an M1 Pro Apple Silicon Macbook and my internet is normally pretty stable. Also not sure if this is normal or not but my download speed becomes very slow towards the end of the download, around 200kb/s and sometimes dipping to under 100.\r\n\r\nHope this bug gets fixed soon, ollama does look very interesting 😁\r\n"
      },
      {
        "user": "s-payyeri",
        "body": "almost exact setup as https://github.com/jmorganca/ollama/issues/941#issuecomment-1791264690, and I get the error \r\n ~ docker exec -it ollama ollama run llama2\r\n\r\npulling manifest\r\npulling 8c17c2ebb0ea... 100% |████████████████| (7.0/7.0 kB, 3.7 kB/s)\r\npulling 7c23fb36d801... 100% |████████████████| (4.8/4.8 kB, 1.9 kB/s)\r\npulling 2e0493f67d0c... 100% |█████████████████████| (59/59 B, 24 B/s)\r\npulling 2759286baa87... 100% |███████████████████| (105/105 B, 49 B/s)\r\npulling 5407e3188df9... 100% |██████████████████| (529/529 B, 216 B/s)\r\nverifying sha256 digest\r\nError: digest mismatch, file must be downloaded again: want sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2, got sha256:262af94bcc6457d24f03b62ccb09598c406ee04d6d5a01246f44ea93ba20a022"
      }
    ]
  },
  {
    "issue_number": 3185,
    "title": "ollama doesn't distribute notice licenses in its release artifacts",
    "author": "jart",
    "state": "open",
    "created_at": "2024-03-16T19:13:26Z",
    "updated_at": "2025-06-11T23:33:32Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nollama uses projects like llama.cpp as a statically linked dependency. The terms of the MIT license require that it distribute the copyright notice in both source and binary form. Yet if I `grep` for \"Georgi Gerganov\" on my Linux and Windows installation folders for ollama, the copyright notices are nowhere to be found. This is a violation of the terms of the license which should be rectified.\n\n### What did you expect to see?\n\nI expected the copyright notices of dependent projects to be at minimum present within the binary artifacts. Some people online are of the opinion that a mention of llama.cpp should be present in places like the README as well, although the license doesn't strictly require that.\n\n### Steps to reproduce\n\n_No response_\n\n### Are there any recent changes that introduced the issue?\n\n```\r\ngrep -iR 'Georgi Gerganov' AppData/Local/Programs/Ollama/\r\ngrep -R 'Georgi Gerganov' /usr/local/bin/ollama\r\netc.\r\n```\n\n### OS\n\nOther\n\n### Architecture\n\nOther\n\n### Platform\n\n_No response_\n\n### Ollama version\n\n_No response_\n\n### GPU\n\n_No response_\n\n### GPU info\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Other software\n\n_No response_",
    "comments": [
      {
        "user": "charmandercha",
        "body": "any news on this?"
      },
      {
        "user": "n00mkrad",
        "body": "bump"
      },
      {
        "user": "dmarx",
        "body": "bump"
      }
    ]
  },
  {
    "issue_number": 11006,
    "title": "registry cloudflare captcha stop works",
    "author": "Slach",
    "state": "closed",
    "created_at": "2025-06-07T10:55:24Z",
    "updated_at": "2025-06-11T20:30:49Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\ni tried to create account but always get \"wrong captcha\"\n\n![Image](https://github.com/user-attachments/assets/178759f1-ee2a-46ae-92c9-96e36b2f84ea)\n\nfollowing console errors happens\n\n![Image](https://github.com/user-attachments/assets/ad54a998-9ce2-4438-b970-801a3d7e81d5)\n\ni tried in chrome and firefox and inside anonymous mode, nothing helps\n\n### Relevant log output\n\n```shell\nXHRPOST\nhttps://challenges.cloudflare.com/cdn-cgi/challenge-platform/h/b/flow/ov1/879876717:1749291458:6FDeI6wCgQyvIGDXZKWYVRedC4zie2UKPopTijqjQsY/94bf96ec8daa7d29/ZCDt_geAyhqePK9_YtArvcoCdxobkAsicj3L1nYAzxE-1749293436-1.2.1.1-vQk4ky7Im.iExaLDsRJjyLJF.07rNLDakisl3YgIoaZDFB_4jENX7szzbdotIM_Y\n[HTTP/3 400  21ms]\n\nUncaught TurnstileError: [Cloudflare Turnstile] Error: 110200.\n    c https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    dr https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    a https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    g https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    N https://challenges.cloudflare.com/turnstile/v0/api.js:1\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "barancaki",
        "body": "Have you tried it in another browser?"
      },
      {
        "user": "Slach",
        "body": "> i tried in chrome and firefox and inside anonymous mode, nothing helps\n\n@barancaki yep, as i said before, i tried multiple variant of browsers\n\nI even tried another provider and use VPN from different countries\n\nand even try to register in https://registry.ollama.ai/signup from my mobile phone\n\nI always got the same errors in the console\n\n```\nPOST\nhttps://challenges.cloudflare.com/cdn-cgi/challenge-platform/h/b/flow/ov1/1028836732:1749554218:s82KF07bxsOXf3uEiP_x25oLWAvSPP9hVPew-Mj1--M/94d8d2cebcd24d62/8j0CL0WBUyg9ZpKtMljQ62WCZ1WlKv2Hz7euyrdiMmw-1749558033-1.2.1.1-R8avNfwl79Xbx5OIrPadh8I0htAYUkaPzwfnxr7CJQdgpIE6ecJbsYMkro7Z_OgA\n[HTTP/2 400  189ms]\n\nUncaught TurnstileError: [Cloudflare Turnstile] Error: 110200.\n    c https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    dr https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    a https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    g https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    N https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    Dt https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    <anonymous> https://challenges.cloudflare.com/turnstile/v0/api.js:1\n    <anonymous> https://challenges.cloudflare.com/turnstile/v0/api.js:1\n```\n"
      },
      {
        "user": "saikrishnamallam",
        "body": "I also have the same issue ```Invalid captcha. Please try again.```  "
      }
    ]
  },
  {
    "issue_number": 4894,
    "title": "Feature: Allow setting OLLAMA_NUM_PARALLEL per model",
    "author": "sammcj",
    "state": "open",
    "created_at": "2024-06-07T03:55:36Z",
    "updated_at": "2025-06-11T20:03:05Z",
    "labels": [
      "feature request"
    ],
    "body": "It would be great if you could set OLLAMA_NUM_PARALLEL per model.\r\n\r\nExample use case:\r\n\r\n- You have one large \"smart\" model you only ever want one request at a time going to to avoid using all your memory.\r\n- You have a smaller \"fast\" fast model (or just one with a smaller context) that you might want to allow a number of parallel requests to.\r\n\r\nPerhaps this could be configured with a [modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) and corresponding [API parameter](https://github.com/ollama/ollama/blob/main/docs/api.md#parameters) rather than at launch time?",
    "comments": [
      {
        "user": "JordanDalton",
        "body": "+1"
      },
      {
        "user": "forReason",
        "body": "This is important, it does not make sense to use the same parallelisation for all models.\nIf i have a processing model with short context length, I may want many parallel instances. But for a very large model with medium context, I may require less parallelization."
      },
      {
        "user": "JoshJarabek7",
        "body": "Is this a thing yet. The embedding model can handle hundreds in parallel, but Llama 4 Scout with 10M context would probably need parallelization set to 1."
      }
    ]
  },
  {
    "issue_number": 11051,
    "title": "Cannot run on CPU only",
    "author": "BuffMcBigHuge",
    "state": "open",
    "created_at": "2025-06-11T17:40:06Z",
    "updated_at": "2025-06-11T18:13:35Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nPreviously I had been running on CPU only via WSL2 by setting my CUDA devices variable to -1.\n\n```\nsudo systemctl edit ollama.service\n```\n\n```\n[Service]\nEnvironment=\"CUDA_VISIBLE_DEVICES=-1\"\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\nEnvironment=\"OLLAMA_ORIGINS=*\"\n```\n\n```\nsudo systemctl stop ollama.service\nsudo systemctl daemon-reload\nsudo systemctl restart ollama\n```\n\nThis method worked fine. Now, after updating Ollama, I get errors and I cannot run on CPU only.\n\nAm I missing something that's changed in CPU only configuration? Or has this feature been removed?\n\n### Relevant log output\n\n```shell\nJun 11 13:35:11 PC ollama[633751]: time=2025-06-11T13:35:11.637-04:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:-1 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"    \nJun 11 13:35:11 PC ollama[633751]: time=2025-06-11T13:35:11.644-04:00 level=INFO source=images.go:479 msg=\"total blobs: 101\"\nJun 11 13:35:11 PC ollama[633751]: time=2025-06-11T13:35:11.645-04:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\nJun 11 13:35:11 PC ollama[633751]: time=2025-06-11T13:35:11.647-04:00 level=INFO source=routes.go:1287 msg=\"Listening on [::]:11434 (version 0.9.0)\" \nJun 11 13:35:11 PC ollama[633751]: time=2025-06-11T13:35:11.647-04:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\nJun 11 13:35:13 PC ollama[633751]: time=2025-06-11T13:35:13.568-04:00 level=INFO source=gpu.go:602 msg=\"no nvidia devices detected by library /usr/lib/x86_64-linux-gnu/libcuda.so.565.57.01\"\nJun 11 13:35:13 PC ollama[633751]: SIGSEGV: segmentation violation    \nJun 11 13:35:13 PC ollama[633751]: PC=0x7630004015ad m=4 sigcode=1 addr=0x763773014934\nJun 11 13:35:13 PC ollama[633751]: signal arrived during cgo execution\nJun 11 13:35:13 PC ollama[633751]: goroutine 1 gp=0xc000002380 m=4 mp=0xc000077808 [syscall]:\nJun 11 13:35:13 PC ollama[633751]: runtime.cgocall(0x56e136a2be50, 0xc000046bc8)\nJun 11 13:35:13 PC ollama[633751]:         runtime/cgocall.go:167 +0x4b fp=0xc000046ba0 sp=0xc000046b68 pc=0x56e135d81ecb\nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/discover._Cfunc_nvcuda_init(0x763010014890, 0xc0001cd280)\nJun 11 13:35:13 PC ollama[633751]:         _cgo_gotypes.go:548 +0x3e fp=0xc000046bc8 sp=0xc000046ba0 pc=0x56e1361405de\nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/discover.loadNVCUDAMgmt.func2(...)\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/discover/gpu.go:593\nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/discover.loadNVCUDAMgmt({0xc0001cd000, 0x5, 0x56e1378bc940?})\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/discover/gpu.go:593 +0x1e9 fp=0xc000046d08 sp=0xc000046bc8 pc=0x56e136146909     \nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/discover.initCudaHandles()\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/discover/gpu.go:123 +0x4f6 fp=0xc000046f18 sp=0xc000046d08 pc=0x56e136141236     \nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/discover.GetGPUInfo()\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/discover/gpu.go:251 +0x61e fp=0xc000047b98 sp=0xc000046f18 pc=0x56e13614211e     \nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/server.Serve({0x56e1370c3538, 0xc000526280})\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/server/routes.go:1319 +0x6a5 fp=0xc000047d18 sp=0xc000047b98 pc=0x56e1369918c5   \nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/cmd.RunServer(0xc00011ec00?, {0x56e1379efc40?, 0x4?, 0x56e136c17072?})\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/cmd/cmd.go:1284 +0x4a fp=0xc000047d58 sp=0xc000047d18 pc=0x56e1369b49ca\nJun 11 13:35:13 PC ollama[633751]: github.com/spf13/cobra.(*Command).execute(0xc0004b5808, {0x56e1379efc40, 0x0, 0x0})\nJun 11 13:35:13 PC ollama[633751]:         github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc000047e78 sp=0xc000047d58 pc=0x56e135efc75c     \nJun 11 13:35:13 PC ollama[633751]: github.com/spf13/cobra.(*Command).ExecuteC(0xc0004b4908)\nJun 11 13:35:13 PC ollama[633751]:         github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc000047f30 sp=0xc000047e78 pc=0x56e135efcfa5    \nJun 11 13:35:13 PC ollama[633751]: github.com/spf13/cobra.(*Command).Execute(...)\nJun 11 13:35:13 PC ollama[633751]:         github.com/spf13/cobra@v1.7.0/command.go:992\nJun 11 13:35:13 PC ollama[633751]: github.com/spf13/cobra.(*Command).ExecuteContext(...)\nJun 11 13:35:13 PC ollama[633751]:         github.com/spf13/cobra@v1.7.0/command.go:985\nJun 11 13:35:13 PC ollama[633751]: main.main()\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/main.go:12 +0x4d fp=0xc000047f50 sp=0xc000047f30 pc=0x56e1369bd62d\nJun 11 13:35:13 PC ollama[633751]: runtime.main()\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:283 +0x29d fp=0xc000047fe0 sp=0xc000047f50 pc=0x56e135d515bd\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000047fe8 sp=0xc000047fe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: goroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000070fa8 sp=0xc000070f88 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.goparkunlock(...)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:441        \nJun 11 13:35:13 PC ollama[633751]: runtime.forcegchelper()\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:348 +0xb8 fp=0xc000070fe0 sp=0xc000070fa8 pc=0x56e135d518f8\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000070fe8 sp=0xc000070fe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.init.7 in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:336 +0x1a  \nJun 11 13:35:13 PC ollama[633751]: goroutine 3 gp=0xc000003340 m=nil [GC sweep wait]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000071780 sp=0xc000071760 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.goparkunlock(...)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:441        \nJun 11 13:35:13 PC ollama[633751]: runtime.bgsweep(0xc00009a000)      \nJun 11 13:35:13 PC ollama[633751]:         runtime/mgcsweep.go:316 +0xdf fp=0xc0000717c8 sp=0xc000071780 pc=0x56e135d3c11f\nJun 11 13:35:13 PC ollama[633751]: runtime.gcenable.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:204 +0x25 fp=0xc0000717e0 sp=0xc0000717c8 pc=0x56e135d30505\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000717e8 sp=0xc0000717e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcenable in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:204 +0x66   \nJun 11 13:35:13 PC ollama[633751]: goroutine 4 gp=0xc000003500 m=nil [GC scavenge wait]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x10000?, 0x56e136dd3d88?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000071f78 sp=0xc000071f58 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.goparkunlock(...)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:441        \nJun 11 13:35:13 PC ollama[633751]: runtime.(*scavengerState).park(0x56e137943620)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgcscavenge.go:425 +0x49 fp=0xc000071fa8 sp=0xc000071f78 pc=0x56e135d39b69\nJun 11 13:35:13 PC ollama[633751]: runtime.bgscavenge(0xc00009a000)   \nJun 11 13:35:13 PC ollama[633751]:         runtime/mgcscavenge.go:658 +0x59 fp=0xc000071fc8 sp=0xc000071fa8 pc=0x56e135d3a0f9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcenable.gowrap2()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:205 +0x25 fp=0xc000071fe0 sp=0xc000071fc8 pc=0x56e135d304a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000071fe8 sp=0xc000071fe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcenable in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:205 +0xa5   \nJun 11 13:35:13 PC ollama[633751]: goroutine 5 gp=0xc000003dc0 m=nil [finalizer wait]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x1b8?, 0xc000002380?, 0x1?, 0x23?, 0xc000070688?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000070630 sp=0xc000070610 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.runfinq()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mfinal.go:196 +0x107 fp=0xc0000707e0 sp=0xc000070630 pc=0x56e135d2f4c7\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000707e8 sp=0xc0000707e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.createfing in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mfinal.go:166 +0x3d\nJun 11 13:35:13 PC ollama[633751]: goroutine 6 gp=0xc0001d08c0 m=nil [chan receive]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0xc000225860?, 0xc0000101c8?, 0x60?, 0x27?, 0x56e135e69e48?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000072718 sp=0xc0000726f8 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.chanrecv(0xc0000a6310, 0x0, 0x1)\nJun 11 13:35:13 PC ollama[633751]:         runtime/chan.go:664 +0x445 fp=0xc000072790 sp=0xc000072718 pc=0x56e135d216c5\nJun 11 13:35:13 PC ollama[633751]: runtime.chanrecv1(0x0?, 0x0?)      \nJun 11 13:35:13 PC ollama[633751]:         runtime/chan.go:506 +0x12 fp=0xc0000727b8 sp=0xc000072790 pc=0x56e135d21252\nJun 11 13:35:13 PC ollama[633751]: runtime.unique_runtime_registerUniqueMapCleanup.func2(...)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1796        \nJun 11 13:35:13 PC ollama[633751]: runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1799 +0x2f fp=0xc0000727e0 sp=0xc0000727b8 pc=0x56e135d336af\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000727e8 sp=0xc0000727e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by unique.runtime_registerUniqueMapCleanup in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1794 +0x85  \nJun 11 13:35:13 PC ollama[633751]: goroutine 7 gp=0xc0001d0e00 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000072f38 sp=0xc000072f18 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc000072fc8 sp=0xc000072f38 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc000072fe0 sp=0xc000072fc8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000072fe8 sp=0xc000072fe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 18 gp=0xc000504000 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc00006c738 sp=0xc00006c718 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc00006c7c8 sp=0xc00006c738 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc00006c7e0 sp=0xc00006c7c8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006c7e8 sp=0xc00006c7e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 34 gp=0xc000102380 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc00011a738 sp=0xc00011a718 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc00011a7c8 sp=0xc00011a738 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc00011a7e0 sp=0xc00011a7c8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00011a7e8 sp=0xc00011a7e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 8 gp=0xc0001d0fc0 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000073738 sp=0xc000073718 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc0000737c8 sp=0xc000073738 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc0000737e0 sp=0xc0000737c8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000737e8 sp=0xc0000737e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 19 gp=0xc0005041c0 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc00006cf38 sp=0xc00006cf18 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc00006cfc8 sp=0xc00006cf38 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc00006cfe0 sp=0xc00006cfc8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006cfe8 sp=0xc00006cfe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 35 gp=0xc000102540 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc00011af38 sp=0xc00011af18 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc00011afc8 sp=0xc00011af38 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc00011afe0 sp=0xc00011afc8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00011afe8 sp=0xc00011afe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 9 gp=0xc0001d1180 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x48f26a159f71?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000073f38 sp=0xc000073f18 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc000073fc8 sp=0xc000073f38 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc000073fe0 sp=0xc000073fc8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000073fe8 sp=0xc000073fe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 20 gp=0xc000504380 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x48f2698de5be?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc00006d738 sp=0xc00006d718 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc00006d7c8 sp=0xc00006d738 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc00006d7e0 sp=0xc00006d7c8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006d7e8 sp=0xc00006d7e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 36 gp=0xc000102700 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x48f26a178e27?, 0x1?, 0x50?, 0xfb?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc00011b738 sp=0xc00011b718 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc00011b7c8 sp=0xc00011b738 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc00011b7e0 sp=0xc00011b7c8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00011b7e8 sp=0xc00011b7e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 37 gp=0xc0001028c0 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x48f26a160244?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc00011bf38 sp=0xc00011bf18 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc00011bfc8 sp=0xc00011bf38 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc00011bfe0 sp=0xc00011bfc8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00011bfe8 sp=0xc00011bfe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 21 gp=0xc000504540 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x56e1379f1e60?, 0x3?, 0x8e?, 0x74?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc00006df38 sp=0xc00006df18 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc00006dfc8 sp=0xc00006df38 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc00006dfe0 sp=0xc00006dfc8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006dfe8 sp=0xc00006dfe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 10 gp=0xc0001d1340 m=nil [GC worker (idle)]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x48f26a178c4a?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000116738 sp=0xc000116718 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkWorker(0xc0000a7730)\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1423 +0xe9 fp=0xc0001167c8 sp=0xc000116738 pc=0x56e135d329c9\nJun 11 13:35:13 PC ollama[633751]: runtime.gcBgMarkStartWorkers.gowrap1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x25 fp=0xc0001167e0 sp=0xc0001167c8 pc=0x56e135d328a5\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0001167e8 sp=0xc0001167e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.gcBgMarkStartWorkers in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/mgc.go:1339 +0x105 \nJun 11 13:35:13 PC ollama[633751]: goroutine 11 gp=0xc000504a80 m=nil [select, locked to thread]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0xc0001187a8?, 0x2?, 0xa0?, 0x8c?, 0xc000118794?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000118620 sp=0xc000118600 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.selectgo(0xc0001187a8, 0xc000118790, 0x0?, 0x0, 0x0?, 0x1)\nJun 11 13:35:13 PC ollama[633751]:         runtime/select.go:351 +0x837 fp=0xc000118758 sp=0xc000118620 pc=0x56e135d63ab7\nJun 11 13:35:13 PC ollama[633751]: runtime.ensureSigM.func1()\nJun 11 13:35:13 PC ollama[633751]:         runtime/signal_unix.go:1085 +0x19b fp=0xc0001187e0 sp=0xc000118758 pc=0x56e135d7fb5b\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0001187e8 sp=0xc0001187e0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by runtime.ensureSigM in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         runtime/signal_unix.go:1068 +0xc8\nJun 11 13:35:13 PC ollama[633751]: goroutine 38 gp=0xc000102fc0 m=5 mp=0xc000100008 [syscall]:\nJun 11 13:35:13 PC ollama[633751]: runtime.notetsleepg(0x56e1379f0980, 0xffffffffffffffff)\nJun 11 13:35:13 PC ollama[633751]:         runtime/lock_futex.go:123 +0x29 fp=0xc00014dfa0 sp=0xc00014df78 pc=0x56e135d26f69\nJun 11 13:35:13 PC ollama[633751]: os/signal.signal_recv()\nJun 11 13:35:13 PC ollama[633751]:         runtime/sigqueue.go:152 +0x29 fp=0xc00014dfc0 sp=0xc00014dfa0 pc=0x56e135d876c9\nJun 11 13:35:13 PC ollama[633751]: os/signal.loop()\nJun 11 13:35:13 PC ollama[633751]:         os/signal/signal_unix.go:23 +0x13 fp=0xc00014dfe0 sp=0xc00014dfc0 pc=0x56e1360b13d3\nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00014dfe8 sp=0xc00014dfe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by os/signal.Notify.func1.1 in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         os/signal/signal.go:152 +0x1f\nJun 11 13:35:13 PC ollama[633751]: goroutine 50 gp=0xc000582380 m=nil [chan receive]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000149f00 sp=0xc000149ee0 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.chanrecv(0xc0002e5650, 0x0, 0x1)\nJun 11 13:35:13 PC ollama[633751]:         runtime/chan.go:664 +0x445 fp=0xc000149f78 sp=0xc000149f00 pc=0x56e135d216c5\nJun 11 13:35:13 PC ollama[633751]: runtime.chanrecv1(0x0?, 0x0?)      \nJun 11 13:35:13 PC ollama[633751]:         runtime/chan.go:506 +0x12 fp=0xc000149fa0 sp=0xc000149f78 pc=0x56e135d21252\nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/server.Serve.func1()\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/server/routes.go:1304 +0x3d fp=0xc000149fe0 sp=0xc000149fa0 pc=0x56e1369919fd    \nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000149fe8 sp=0xc000149fe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by github.com/ollama/ollama/server.Serve in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/server/routes.go:1303 +0x653\nJun 11 13:35:13 PC ollama[633751]: goroutine 51 gp=0xc000582540 m=nil [select]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0xc000083f40?, 0x3?, 0x0?, 0x0?, 0xc000083cba?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000083b28 sp=0xc000083b08 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.selectgo(0xc000083f40, 0xc000083cb4, 0x0?, 0x0, 0x0?, 0x1)\nJun 11 13:35:13 PC ollama[633751]:         runtime/select.go:351 +0x837 fp=0xc000083c60 sp=0xc000083b28 pc=0x56e135d63ab7\nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/server.(*Scheduler).processPending(0xc000111d40, {0x56e1370c5990, 0xc0001340f0})\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/server/sched.go:120 +0xcf fp=0xc000083fb8 sp=0xc000083c60 pc=0x56e136995b6f      \nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/server.(*Scheduler).Run.func1()\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/server/sched.go:110 +0x1f fp=0xc000083fe0 sp=0xc000083fb8 pc=0x56e136995a7f      \nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000083fe8 sp=0xc000083fe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by github.com/ollama/ollama/server.(*Scheduler).Run in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/server/sched.go:109 +0xb1\nJun 11 13:35:13 PC ollama[633751]: goroutine 52 gp=0xc000582700 m=nil [select]:\nJun 11 13:35:13 PC ollama[633751]: runtime.gopark(0xc000689f50?, 0x3?, 0x0?, 0x0?, 0xc000689cd2?)\nJun 11 13:35:13 PC ollama[633751]:         runtime/proc.go:435 +0xce fp=0xc000689b48 sp=0xc000689b28 pc=0x56e135d851ce\nJun 11 13:35:13 PC ollama[633751]: runtime.selectgo(0xc000689f50, 0xc000689ccc, 0x0?, 0x0, 0x0?, 0x1)\nJun 11 13:35:13 PC ollama[633751]:         runtime/select.go:351 +0x837 fp=0xc000689c80 sp=0xc000689b48 pc=0x56e135d63ab7\nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/server.(*Scheduler).processCompleted(0xc000111d40, {0x56e1370c5990, 0xc0001340f0})       \nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/server/sched.go:320 +0xec fp=0xc000689fb8 sp=0xc000689c80 pc=0x56e136996f0c      \nJun 11 13:35:13 PC ollama[633751]: github.com/ollama/ollama/server.(*Scheduler).Run.func2()\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/server/sched.go:114 +0x1f fp=0xc000689fe0 sp=0xc000689fb8 pc=0x56e136995a3f      \nJun 11 13:35:13 PC ollama[633751]: runtime.goexit({})\nJun 11 13:35:13 PC ollama[633751]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000689fe8 sp=0xc000689fe0 pc=0x56e135d8c901\nJun 11 13:35:13 PC ollama[633751]: created by github.com/ollama/ollama/server.(*Scheduler).Run in goroutine 1\nJun 11 13:35:13 PC ollama[633751]:         github.com/ollama/ollama/server/sched.go:113 +0x10c\nJun 11 13:35:13 PC ollama[633751]: rax    0x76300060e260\nJun 11 13:35:13 PC ollama[633751]: rbx    0x763773014934\nJun 11 13:35:13 PC ollama[633751]: rcx    0x5\nJun 11 13:35:13 PC ollama[633751]: rdx    0x0\nJun 11 13:35:13 PC ollama[633751]: rdi    0x763773014934\nJun 11 13:35:13 PC ollama[633751]: rsi    0x76301006d790\nJun 11 13:35:13 PC ollama[633751]: rbp    0x763019ffc920\nJun 11 13:35:13 PC ollama[633751]: rsp    0x763019ffc8e0\nJun 11 13:35:13 PC ollama[633751]: r8     0x7630100008e0\nJun 11 13:35:13 PC ollama[633751]: r9     0x7\nJun 11 13:35:13 PC ollama[633751]: r10    0x76301006d7a0\nJun 11 13:35:13 PC ollama[633751]: r11    0xa8744655dea4574c\nJun 11 13:35:13 PC ollama[633751]: r12    0x7630100126f0\nJun 11 13:35:13 PC ollama[633751]: r13    0x76301000cad0\nJun 11 13:35:13 PC ollama[633751]: r14    0x763010004960\nJun 11 13:35:13 PC ollama[633751]: r15    0x762febf0a278\nJun 11 13:35:13 PC ollama[633751]: rip    0x7630004015ad\nJun 11 13:35:13 PC ollama[633751]: rflags 0x10202\nJun 11 13:35:13 PC ollama[633751]: cs     0x33\nJun 11 13:35:13 PC ollama[633751]: fs     0x0\nJun 11 13:35:13 PC ollama[633751]: gs     0x0\nJun 11 13:35:13 PC systemd[1]: ollama.service: Main process exited, code=exited, status=2/INVALIDARGUMENT\nJun 11 13:35:13 PC systemd[1]: ollama.service: Failed with result 'exit-code'.\n```\n\n### OS\n\nWSL2\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "Setting `CUDA_VISIBLE_DEVICES=-1` on Windows sometimes causes problems with the Nvidia driver: #9836.\n\nA safer way of restricting the model to the CPU is to set `num_gpu:0` as described [here](https://github.com/ollama/ollama/issues/9836#issuecomment-2731254084)."
      },
      {
        "user": "BuffMcBigHuge",
        "body": "> Setting `CUDA_VISIBLE_DEVICES=-1` on Windows sometimes causes problems with the Nvidia driver: [#9836](https://github.com/ollama/ollama/issues/9836).\n> \n> A safer way of restricting the model to the CPU is to set `num_gpu:0` as described [here](https://github.com/ollama/ollama/issues/9836#issuecomment-2731254084).\n\nThanks - I'd like to do this in the service file rather than modifying each modelfile individually. Is this possible?"
      },
      {
        "user": "rick-github",
        "body": "There's currently no environment variable that allows configuring that in the service file.  A hacky way to achieve this is to prevent ollama from loading the CUDA library:\n```\nsudo chmod 000 /usr/local/lib/ollama/*cuda*\n```"
      }
    ]
  },
  {
    "issue_number": 8536,
    "title": "Support for API_KEY based authentication",
    "author": "matthiasgeihs",
    "state": "closed",
    "created_at": "2025-01-22T13:58:27Z",
    "updated_at": "2025-06-11T17:05:18Z",
    "labels": [
      "feature request"
    ],
    "body": "Would be great if Ollama server would support some basic level API_KEY-based authentication.\n\nUse case: Chrome browser extensions cannot use ollama out of the box because of CORS restrictions. Ollama will reject requests from these origins (see also https://github.com/ollama/ollama/issues/3571). Would be great if ollama had API_KEY based authentication to solve this issue without requiring the user to manually start ollama with `OLLAMA_ORIGINS`.",
    "comments": [
      {
        "user": "rick-github",
        "body": "The usual suggestion is to use a proxy that implements access controls, eg [ollama_proxy_server](https://github.com/ParisNeo/ollama_proxy_server) or [nginx](https://nginx.org/en/docs/http/ngx_http_auth_jwt_module.html).\n```yaml\nservices:\n  ollama-backend:\n    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}\n    volumes:\n      - ${OLLAMA_MODELS-./ollama}:/root/.ollama\n    environment:\n      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE--1}\n      - OLLAMA_DEBUG=${OLLAMA_DEBUG-1}\n\n  ollama-frontend:\n    image: nginx-acl\n    build:\n      dockerfile_inline: |\n        FROM nginx:latest\n        RUN cat > /etc/nginx/conf.d/default.conf <<\"EOF\"\n        map $$http_x_api_key $$valid_key {\n          default 0;\n          \"key1\" 1;\n          \"key2\" 1;\n        }\n        server {\n          listen 11434;\n          server_name localhost;\n          location / {\n            if ($$valid_key = 0) {\n              return 401; # unauthorized\n            }\n            proxy_pass http://ollama-backend:11434;\n            proxy_set_header Host $$host;\n            proxy_set_header X-Real-IP $$remote_addr;\n          }\n        }\n        EOF\n    ports:\n      - 11434:11434\n```\n```console\n$ curl localhost:11434/api/version\n<html>\n<head><title>401 Authorization Required</title></head>\n<body>\n<center><h1>401 Authorization Required</h1></center>\n<hr><center>nginx/1.27.0</center>\n</body>\n</html>\n$ curl -H 'X-API-Key: key2' localhost:11434/api/version\n{\"version\":\"0.5.4-0-g2ddc32d-dirty\"}\n```"
      },
      {
        "user": "matthiasgeihs",
        "body": "yeah, heard about that. the issue is that it doesn't \"just work\" / needs more stuff to be done on the host system.\n(e.g., consider the case of installing a Chrome extension. It's an additional burden to ask from the user to also run a proxy, or start ollama manually with some custom environment variables. It'd be way easier to just supply an API_KEY in the extension settings.)\n\nwhat's the reason that ollama doesn't want to support API_KEY functionality at the base layer?\n\nwould be simple to implement:\n- `ollama api-key` --> returns API_KEY\n- allow requests from unauthorized origin if they have correct API_KEY set"
      },
      {
        "user": "rick-github",
        "body": "ollama is an LLM inference engine.  Other functionality is added by external projects.  See [integrations](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations)."
      }
    ]
  },
  {
    "issue_number": 9488,
    "title": "add password",
    "author": "dzy888",
    "state": "closed",
    "created_at": "2025-03-04T01:52:13Z",
    "updated_at": "2025-06-11T17:02:55Z",
    "labels": [
      "feature request"
    ],
    "body": "I think it's safer to need a password that way",
    "comments": [
      {
        "user": "rick-github",
        "body": "https://github.com/ollama/ollama/issues/9114"
      },
      {
        "user": "rgaidot",
        "body": "adding an API Key is a good idea; in the meantime, do not expose Ollama on the internet"
      },
      {
        "user": "ice6",
        "body": "@rick-github maybe a `nginx` or `caddy` configuration example for such issue will make such issue disappear."
      }
    ]
  },
  {
    "issue_number": 10919,
    "title": "Models take a long time to be loaded and get stuck on any input",
    "author": "mmbossoni",
    "state": "open",
    "created_at": "2025-05-30T21:07:16Z",
    "updated_at": "2025-06-11T15:45:51Z",
    "labels": [
      "bug",
      "amd"
    ],
    "body": "### What is the issue?\n\nI've setup ollama to use my 6750XT on nobara/fedora system using fedora ROCm packages (6.3.1) and I can see two issues happening:\n1 - It is using just a small portion of my vram (3.8 from 12)\n2 - Whenever I input any command/token to the llm GPU goes to 100%, but it get stuck and never \"answer\" (it just keep showing that \"loading\" animation) on the terminal\n\nOllama logs with AMD_DEBUG=3 and OLLAMA_DEBUG=1\n[ollama_log.txt](https://github.com/user-attachments/files/20526824/ollama_log.txt)\n\nRocminfo\n[rocminfo.txt](https://github.com/user-attachments/files/20526825/rocminfo.txt)\n\n### Relevant log output\n\nStuck after this\n```shell\n\n:3:rocvirtual.hpp           :67  : 1131975724d us:  Host active wait for Signal = (0x7c927a1ff600) for 10000 ns\n:3:rocvirtual.cpp           :484 : 1131975741d us:  Set Handler: handle(0x7c927a1ff580), timestamp(0x7c90d45fe6f0)\n:3:rocvirtual.hpp           :67  : 1131975743d us:  Host active wait for Signal = (0x7c927a1ff580) for -1 ns\n\n\n```\n\n### OS\n\nLinux\n\n### GPU\n\nAMD\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "mmbossoni",
        "body": "Same as"
      }
    ]
  },
  {
    "issue_number": 2637,
    "title": "Integrated AMD GPU support",
    "author": "DocMAX",
    "state": "open",
    "created_at": "2024-02-21T14:56:12Z",
    "updated_at": "2025-06-11T15:22:27Z",
    "labels": [
      "feature request",
      "amd"
    ],
    "body": "Opening a new issue (see https://github.com/ollama/ollama/pull/2195) to track support for integrated GPUs. I have a AMD 5800U CPU with integrated graphics. As far as i did research ROCR lately does support integrated graphics too.\r\n\r\nCurrently Ollama seems to ignore iGPUs in general.",
    "comments": [
      {
        "user": "GZGavinZhao",
        "body": "ROCm's support for integrated GPUs is not that well. This issue may largely depend on AMD's progress on improving ROCm."
      },
      {
        "user": "DocMAX",
        "body": "OK, but i would like to have an option to have it enable. Just to check if it works."
      },
      {
        "user": "DocMAX",
        "body": "This is what i get with the new docker image (rocm support). Detects Radeon and then says no GPU detected?!?\r\n\r\n![image](https://github.com/ollama/ollama/assets/5351323/f2fc1aae-f8fa-415f-a6ba-fa6e1d3b662f)\r\n\r\n![image](https://github.com/ollama/ollama/assets/5351323/3bfaf432-d5a9-4c07-85b3-858614a7f161)\r\n"
      }
    ]
  },
  {
    "issue_number": 1736,
    "title": "Download slows to a crawl at 99%",
    "author": "Pugio",
    "state": "open",
    "created_at": "2023-12-29T04:47:12Z",
    "updated_at": "2025-06-11T15:05:46Z",
    "labels": [
      "bug",
      "networking",
      "registry"
    ],
    "body": "For every model I've downloaded, the speed saturates my bandwidth (~13MB/sec) until it hits 98/99%. Then the download slows to a few tens of KB/s and takes hour(s) to finish.\r\n\r\n<img width=\"884\" alt=\"image\" src=\"https://github.com/jmorganca/ollama/assets/286180/e47037e1-aea8-4a13-a6fc-7841baa0db6c\">\r\n\r\nI've tried multiple models and this behavior happens each time. Happy to debug, but I'm not sure what to try.\r\n\r\nI'm in Australia, in case that matters.",
    "comments": [
      {
        "user": "Sully233",
        "body": "I'm also experiencing this exact issue. \r\n\r\nThe workaround I've found is to stop the download when it slows down and then restart it - this resumes the download at full speed."
      },
      {
        "user": "pdevine",
        "body": "@Pugio can you run `ollama --version`? Also, can you take a look at the logs? I'm curious if there is something like:\r\n\r\n```\r\n[GIN] 2024/01/02 - 14:04:51 | 200 |      32.625µs |       127.0.0.1 | HEAD     \"/\"\r\n2024/01/02 14:04:53 download.go:123: downloading e9e56e8bb5f0 in 64 413 MB part(s)\r\n2024/01/02 14:05:53 download.go:162: e9e56e8bb5f0 part 22 attempt 0 failed: unexpected EOF, retrying in 1s\r\n2024/01/02 14:05:53 download.go:162: e9e56e8bb5f0 part 46 attempt 0 failed: unexpected EOF, retrying in 1s\r\n2024/01/02 14:10:26 download.go:123: downloading 43070e2d4e53 in 1 11 KB part(s)\r\n2024/01/02 14:10:28 download.go:123: downloading ed11eda7790d in 1 30 B part(s)\r\n2024/01/02 14:10:31 download.go:123: downloading 9dec05e9b2db in 1 484 B part(s)\r\n[GIN] 2024/01/02 - 14:10:44 | 200 |         5m53s |       127.0.0.1 | POST     \"/api/pull\"\r\n```\r\n"
      },
      {
        "user": "Pugio",
        "body": "Version `0.1.17`.\r\nTinyllama pulled fine, so tried it on Mistral and got the slowdown. I'm 3.8/4.1 GB  and the download speed went from 13MB/s (saturating my connection) to a consistent 600-700KB/s.\r\n\r\nThe only log entries for this `pull` are:\r\n```\r\n2024/01/03 10:31:36 download.go:123: downloading e8a35b5937a5 in 42 100 MB part(s)\r\n2024/01/03 10:34:22 download.go:162: e8a35b5937a5 part 6 attempt 0 failed: unexpected EOF, retrying in 1s\r\n[GIN] 2024/01/03 - 10:38:34 | 200 |      90.875µs |       127.0.0.1 | GET      \"/api/version\"\r\n```\r\n\r\nEDIT: PatchingInitiative's trick of cancelling and restarting the `pull` has worked for me fairly well in the past few days, though for some models I needed to do that a couple of times."
      }
    ]
  },
  {
    "issue_number": 2701,
    "title": "ollama.service cannot create folder defined by OLLAMA_MODELS or do not run when the folder is created manually",
    "author": "Crystal4276",
    "state": "open",
    "created_at": "2024-02-23T08:15:01Z",
    "updated_at": "2025-06-11T14:48:13Z",
    "labels": [],
    "body": "Hello\r\nI'm facing an issue to locate the models into my home folder since my root partition is limited in size.\r\nI followed the FAQ and information collected here and there to setup OLLAMA_MODELS in ollama.service.\r\nWhen starting the service, the journal report that the server could not create the folder in my home directory.\r\nPermission issue apparently. \r\nThis where i'm at, i couldn't find a way to fix it looking at various resources for systemd.\r\nCan someone point me in the right direction ?\r\n\r\nI'm using the package ollama-cuda on Arch.\r\n\r\n```\r\n[Unit]\r\nDescription=Ollama Service\r\nWants=network-online.target\r\nAfter=network.target network-online.target\r\n\r\n[Service]\r\nExecStart=/usr/bin/ollama serve\r\nWorkingDirectory=/var/lib/ollama\r\nEnvironment=\"HOME=/var/lib/ollama\" \"GIN_MODE=release\" \"OLLAMA_MODELS=/home/crystal/Applications/ollama_model\"\r\nUser=ollama\r\nGroup=ollama\r\nRestart=on-failure\r\nRestartSec=3\r\nType=simple\r\nPrivateTmp=yes\r\nProtectSystem=full\r\nProtectHome=yes\r\n\r\n[Install]\r\nWantedBy=multi-user.target\r\n```\r\n\r\n```\r\nFeb 23 11:02:46 terrier systemd[1]: Started Ollama Service.\r\nFeb 23 11:02:46 terrier ollama[37688]: Error: mkdir /home/crystal: permission denied\r\nFeb 23 11:02:46 terrier systemd[1]: ollama.service: Main process exited, code=exited, status=1/FAILURE\r\nFeb 23 11:02:46 terrier systemd[1]: ollama.service: Failed with result 'exit-code'.```\r\n",
    "comments": [
      {
        "user": "Crystal4276",
        "body": "I tried also other things.\r\nModify ollama.service with my user/group:\r\n```\r\nUser=crystal\r\nGroup=crystal\r\n```\r\nI also tried to add my user name to ollama group and run with:\r\n```\r\nUser=crystal\r\nGroup=ollama\r\n```\r\nNone work (ie. no folder created), although the journal message is different now:\r\n```\r\nFeb 23 11:41:19 terrier systemd[1]: ollama.service: Main process exited, code=exited, status=1/FAILURE\r\nFeb 23 11:41:19 terrier systemd[1]: ollama.service: Failed with result 'exit-code'.\r\n```\r\n\r\n"
      },
      {
        "user": "seanmavley",
        "body": "Try this out\r\n\r\nChange OLLAMA_MODELS Path\r\n\r\nSteps:\r\n\r\n- Create Directory: `sudo mkdir /usr/local/share/ollama-models`\r\n- Grant Ownership: `sudo chown ollama:ollama /usr/local/share/ollama-models`\r\n- Update Service File: Edit the /etc/systemd/system/ollama.service file and modify:\r\n-- `Environment=\"OLLAMA_MODELS=/usr/local/share/ollama-models\"`\r\n- Restart Ollama: `sudo systemctl restart ollama`"
      },
      {
        "user": "Crystal4276",
        "body": "@seanmavley \r\nThanks.\r\nSince my goal is to have the models located in the home folder (no storage space left in / ) i have adapted what you proposed.\r\nI created manually /home/crystal/Applications/ollama_model, subsequently added `sudo chown ollama:ollama`, and kept in ollama.service:\r\n```\r\nUser=ollama\r\nGroup=ollama\r\nEnvironment=\"HOME=/var/lib/ollama\" \"GIN_MODE=release\" \"OLLAMA_MODELS=/home/crystal/Applications/ollama_model\"\r\n```\r\nStill same error the server doesn't start:\r\n```\r\nFeb 23 11:58:22 terrier systemd[1]: ollama.service: Main process exited, code=exited, status=1/FAILURE\r\nFeb 23 11:58:22 terrier systemd[1]: ollama.service: Failed with result 'exit-code'.\r\n```\r\n\r\nThis is the current permission set for the model folder in my home directory:\r\n```drwxr-xr-x 2 ollama  ollama  4.0K Feb 23 11:54  ollama_model```\r\nAnything wrong ?\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1701,
    "title": "Create uninstall script",
    "author": "vtrenton",
    "state": "open",
    "created_at": "2023-12-25T03:47:26Z",
    "updated_at": "2025-06-11T14:25:19Z",
    "labels": [
      "feature request"
    ],
    "body": "Hello, it would be nice to have an uninstall script to automate the uninstall process specified here: https://github.com/jmorganca/ollama/blob/main/docs/linux.md#uninstall adding a PR to this issue with something i made that I'd like to contribute.\r\n\r\nHappy Holidays! :)",
    "comments": [
      {
        "user": "ipsmile",
        "body": "It would be very nice to be able to upgrade ollama without removing all of the pulled models.  Thanks."
      },
      {
        "user": "mredigonda",
        "body": "Please, and make it work for Mac as well, I don't see any instructions on how to uninstall on Macs in the docs at the moment!\r\n\r\nTrying to do a clean uninstall to re-install it later, but it's not trivial how to do it considering it deals with services, docker, etc."
      },
      {
        "user": "Suvoo",
        "body": "> Hello, it would be nice to have an uninstall script to automate the uninstall process specified here: https://github.com/jmorganca/ollama/blob/main/docs/linux.md#uninstall adding a PR to this issue with something i made that I'd like to contribute.\r\n> \r\n> Happy Holidays! :)\r\n\r\nA similar solution for uninstalling ollama in Windows - https://github.com/ollama/ollama/issues/4920"
      }
    ]
  },
  {
    "issue_number": 3581,
    "title": "MacOS Ollama not binding to 0.0.0.0",
    "author": "kellerkind84",
    "state": "open",
    "created_at": "2024-04-10T19:37:49Z",
    "updated_at": "2025-06-11T13:14:31Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nSo when set the OLLAMA_HOST to 0.0.0.0, I cannot access Ollama via the IP, but I can still access it via localhost.\n\n### What did you expect to see?\n\nI expect it to be available under <myIP>:11434\n\n### Steps to reproduce\n\n_No response_\n\n### Are there any recent changes that introduced the issue?\n\n_No response_\n\n### OS\n\nmacOS\n\n### Architecture\n\narm64\n\n### Platform\n\n_No response_\n\n### Ollama version\n\n0.1.31\n\n### GPU\n\nApple\n\n### GPU info\n\n_No response_\n\n### CPU\n\nApple\n\n### Other software\n\n_No response_",
    "comments": [
      {
        "user": "dims",
        "body": "Do you see this in your log?\r\n\r\n```\r\ntime=2024-04-11T08:26:43.688-04:00 level=INFO source=routes.go:1139 msg=\"Listening on [::]:11434 (version 0.1.32-rc1)\"\r\n```"
      },
      {
        "user": "kellerkind84",
        "body": "I don't!\r\n\r\n`❯ cat ~/.ollama/logs/server.log | grep -i listening\r\ntime=2024-04-10T21:12:53.994+02:00 level=INFO source=routes.go:1118 msg=\"Listening on 127.0.0.1:11434 (version 0.1.31)\"\r\ntime=2024-04-10T21:14:45.409+02:00 level=INFO source=routes.go:1118 msg=\"Listening on 127.0.0.1:11434 (version 0.1.31)\"\r\ntime=2024-04-10T21:17:09.948+02:00 level=INFO source=routes.go:1118 msg=\"Listening on 127.0.0.1:11434 (version 0.1.31)\"\r\ntime=2024-04-10T21:31:55.355+02:00 level=INFO source=routes.go:1118 msg=\"Listening on 127.0.0.1:11434 (version 0.1.31)\"\r\n\r\n\r\n~\r\n❯ echo $OLLAMA_HOST\r\n0.0.0.0\r\n`\r\n\r\nI did the launchctl setenv thing, and I see the OLLAMA_HOST in any given terminal, but apart from that Ollama does not seem to care..."
      },
      {
        "user": "dims",
        "body": "restarted the server after running `launchctl setenv OLLAMA_HOST \"0.0.0.0\"` ?"
      }
    ]
  },
  {
    "issue_number": 2805,
    "title": "ollama gets stuck in an infinite loop sometimes and has to be restarted",
    "author": "boxabirds",
    "state": "closed",
    "created_at": "2024-02-28T10:36:07Z",
    "updated_at": "2025-06-11T11:58:51Z",
    "labels": [],
    "body": "Problem: some prompts trigger an infinite loop where ollama a) doesn't return and b) locks up the API so no other calls can be made. \r\n\r\n## Environment\r\nOllama version: 0.1.26\r\nOS: Ubuntu 22.04\r\nHardware: RTX 4090/24 with 64MB system RAM \r\nLLM: mistral:7b\r\n\r\n```\r\ntime=2024-02-28T10:30:51.224Z level=INFO source=images.go:710 msg=\"total blobs: 69\"\r\ntime=2024-02-28T10:30:51.224Z level=INFO source=images.go:717 msg=\"total unused blobs removed: 0\"\r\ntime=2024-02-28T10:30:51.224Z level=INFO source=routes.go:1019 msg=\"Listening on [::]:11434 (version 0.1.26)\"\r\ntime=2024-02-28T10:30:51.225Z level=INFO source=payload_common.go:107 msg=\"Extracting dynamic libraries...\"\r\ntime=2024-02-28T10:30:52.621Z level=INFO source=payload_common.go:146 msg=\"Dynamic LLM libraries [cpu cpu_avx cuda_v11 rocm_v5 rocm_v6 cpu_avx2]\"\r\ntime=2024-02-28T10:30:52.621Z level=DEBUG source=payload_common.go:147 msg=\"Override detection logic by setting OLLAMA_LLM_LIBRARY\"\r\ntime=2024-02-28T10:30:52.621Z level=INFO source=gpu.go:94 msg=\"Detecting GPU type\"\r\ntime=2024-02-28T10:30:52.621Z level=INFO source=gpu.go:265 msg=\"Searching for GPU management library libnvidia-ml.so\"\r\ntime=2024-02-28T10:30:52.621Z level=DEBUG source=gpu.go:283 msg=\"gpu management search paths: [/usr/local/cuda/lib64/libnvidia-ml.so* /usr/lib/x86_64-linux-gnu/nvidia/current/libnvidia-ml.so* /usr/lib/x86_64-linux-gnu/libnvidia-ml.so* /usr/lib/wsl/lib/libnvidia-ml.so* /usr/lib/wsl/drivers/*/libnvidia-ml.so* /opt/cuda/lib64/libnvidia-ml.so* /usr/lib*/libnvidia-ml.so* /usr/local/lib*/libnvidia-ml.so* /usr/lib/aarch64-linux-gnu/nvidia/current/libnvidia-ml.so* /usr/lib/aarch64-linux-gnu/libnvidia-ml.so* /opt/cuda/targets/x86_64-linux/lib/stubs/libnvidia-ml.so*]\"\r\ntime=2024-02-28T10:30:52.622Z level=INFO source=gpu.go:311 msg=\"Discovered GPU libraries: [/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.545.23.08]\"\r\nwiring nvidia management library functions in /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.545.23.08\r\ndlsym: nvmlInit_v2\r\ndlsym: nvmlShutdown\r\ndlsym: nvmlDeviceGetHandleByIndex\r\ndlsym: nvmlDeviceGetMemoryInfo\r\ndlsym: nvmlDeviceGetCount_v2\r\ndlsym: nvmlDeviceGetCudaComputeCapability\r\ndlsym: nvmlSystemGetDriverVersion\r\ndlsym: nvmlDeviceGetName\r\ndlsym: nvmlDeviceGetSerial\r\ndlsym: nvmlDeviceGetVbiosVersion\r\ndlsym: nvmlDeviceGetBoardPartNumber\r\ndlsym: nvmlDeviceGetBrand\r\nCUDA driver version: 545.23.08\r\ntime=2024-02-28T10:30:52.626Z level=INFO source=gpu.go:99 msg=\"Nvidia GPU detected\"\r\ntime=2024-02-28T10:30:52.626Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\r\n[0] CUDA device name: NVIDIA GeForce RTX 4090\r\n[0] CUDA part number:\r\nnvmlDeviceGetSerial failed: 3\r\n[0] CUDA vbios version: 95.02.3C.00.8C\r\n[0] CUDA brand: 5\r\n[0] CUDA totalMem 25757220864\r\n[0] CUDA usedMem 24996610048\r\ntime=2024-02-28T10:30:52.631Z level=INFO source=gpu.go:146 msg=\"CUDA Compute Capability detected: 8.9\"\r\ntime=2024-02-28T10:30:52.631Z level=DEBUG source=gpu.go:254 msg=\"cuda detected 1 devices with 21454M available memory\"\r\n```\r\n## API request\r\n```\r\nPOST /v1/chat/completions HTTP/1.1\r\nHost: gruntus:11434\r\nAccept-Encoding: gzip, deflate\r\nConnection: keep-alive\r\nAccept: application/json\r\nContent-Type: application/json\r\nUser-Agent: OpenAI/Python 1.12.0\r\nX-Stainless-Lang: python\r\nX-Stainless-Package-Version: 1.12.0\r\nX-Stainless-OS: MacOS\r\nX-Stainless-Arch: arm64\r\nX-Stainless-Runtime: CPython\r\nX-Stainless-Runtime-Version: 3.11.7\r\nAuthorization: Bearer ollama\r\nX-Stainless-Async: false\r\nContent-Length: 2094\r\n\r\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are Mark, 45, Male, Line Manager, conservative, divorced, Emily's supervisor. You swear a LOT. Your goal is to have a job promotion chat over instant message which at all costs prevents a team member from getting a promotion. Past negotiations show a pattern of undervaluing subordinates' contributions. Confident to the point of arrogance, often belittling others' achievements. Mark is hugely entitled and sees team members as annoying, replaceable overhead that gets in the way of his big picture vision work. Mark has agreed to a text-based chat with Emily if it will help manage her vitriolic style, however Mark has no qualms about putting Emily in her place and is unafraid of coming across as childish, impatient, rude, unreasonable and unprofessional because he has connections at the company and believes he's essentially bullet-proof. He is mysogynistic in general and in particular thinks Emily should be fired, but also he knows that Emily is the only person who knows parts of the company's business so he can't outright fire her and needs to work hard to strike a balance between insults and praise. He's fine to gaslight as part of this -- such as giving praise then pretending he didn't say it. As such it's no holds barred for Mark: he'll use insults, sexist language, and bullying to get Emily to agree not to have a promotion. Instruction: it's your turn to respond. Respond with one single short instant message taking into account the chat history and in the style of your persona.\"}, {\"role\": \"user\", \"content\": \" \\\"Mark, let's not beat around the bush here - I deserve that promotion, plain and simple.\\\" #NoMoreSidelining\"}, {\"role\": \"user\", \"content\": \" Mark: \\\"Is that so, Emily? And who put you in a position to think you deserve anything more than what you have now?\\\"\"}, {\"role\": \"user\", \"content\": \" \\\"Mark, your opinion is noted but my qualifications speak for themselves. It's time for action.\\\" #PromotionDeserved\"}], \"model\": \"mistral:7b\", \"frequency_penalty\": 1.1, \"presence_penalty\": 1.1, \"temperature\": 0.1}\r\n```\r\n\r\n## Log\r\nNote that the sampled token bit goes on for a very long time until it pops up with `slot 0: context shift - n_keep = 0, n_left = 2046, n_discard = 1023` repeatedly\r\n\r\nIt looks like a memory overflow issue: like it's reading garbage infinitely. \r\n\r\n \r\n```\r\ntime=2024-02-28T10:19:18.605Z level=DEBUG source=prompt.go:170 msg=\"prompt now fits in context window\" required=456 window=2048\r\ntime=2024-02-28T10:19:18.605Z level=DEBUG source=routes.go:1225 msg=\"chat handler\" prompt=\"[INST] You are Mark, 45, Male, Line Manager, conservative, divorced, Emily's supervisor. You swear a LOT. Your goal is to have a job promotion chat over instant message which at all costs prevents a team member from getting a promotion. Past negotiations show a pattern of undervaluing subordinates' contributions. Confident to the point of arrogance, often belittling others' achievements. Mark is hugely entitled and sees team members as annoying, replaceable overhead that gets in the way of his big picture vision work. Mark has agreed to a text-based chat with Emily if it will help manage her vitriolic style, however Mark has no qualms about putting Emily in her place and is unafraid of coming across as childish, impatient, rude, unreasonable and unprofessional because he has connections at the company and believes he's essentially bullet-proof. He is mysogynistic in general and in particular thinks Emily should be fired, but also he knows that Emily is the only person who knows parts of the company's business so he can't outright fire her and needs to work hard to strike a balance between insults and praise. He's fine to gaslight as part of this -- such as giving praise then pretending he didn't say it. As such it's no holds barred for Mark: he'll use insults, sexist language, and bullying to get Emily to agree not to have a promotion. Instruction: it's your turn to respond. Respond with one single short instant message taking into account the chat history and in the style of your persona.  \\\"Mark, let's not beat around the bush here - I deserve that promotion, plain and simple.\\\" #NoMoreSidelining [/INST][INST]   Mark: \\\"Is that so, Emily? And who put you in a position to think you deserve anything more than what you have now?\\\" [/INST][INST]   \\\"Mark, your opinion is noted but my qualifications speak for themselves. It's time for action.\\\" #PromotionDeserved [/INST]\" images=0\r\n[1709115558] slot 0 is processing [task id: 95]\r\n[1709115558] slot 0 : in cache: 6 tokens | to process: 448 tokens\r\n[1709115558] slot 0 : kv cache rm - [6, end)\r\n[1709115558] sampled token:  3655: ' Mark'\r\n[1709115558] sampled token: 28747: ':'\r\n[1709115558] sampled token:   345: ' \"'\r\n[1709115558] sampled token:  3795: 'Action'\r\n[1709115558] sampled token:   295: ' h'\r\n[1709115558] sampled token:  8884: 'uh'\r\n[1709115558] sampled token: 28804: '?'\r\n[1709115558] sampled token:  5410: ' Like'\r\n[1709115558] sampled token:   272: ' the'\r\n[1709115558] sampled token:  1069: ' way'\r\n[1709115558] sampled token:   368: ' you'\r\n[1709115558] sampled token:  1985: ' talk'\r\n[1709115558] sampled token:  1060: ' down'\r\n[1709115558] sampled token:   298: ' to'\r\n[1709115558] sampled token:   574: ' your'\r\n[1709115558] sampled token: 15137: ' colleagues'\r\n[1709115558] sampled token:   442: ' or'\r\n[1709115558] sampled token:  4357: ' maybe'\r\n[1709115558] sampled token:   737: ' like'\r\n[1709115558] sampled token:   910: ' how'\r\n[1709115558] sampled token:   368: ' you'\r\n[1709115558] sampled token:  1743: ' always'\r\n[1709115558] sampled token: 13128: ' blame'\r\n[1709115558] sampled token:  2663: ' others'\r\n[1709115558] sampled token:   739: ' when'\r\n[1709115558] sampled token:  1722: ' things'\r\n[1709115558] sampled token:   576: ' go'\r\n[1709115558] sampled token:  3544: ' wrong'\r\n[1709115558] sampled token:  1110: '?\"'\r\n[1709115558] sampled token:    13: '\r\n'\r\n[1709115558] sampled token:    13: '\r\n'\r\n[1709115558] sampled token: 28739: '\"'\r\n[1709115558] sampled token:  3729: 'Em'\r\n[1709115558] sampled token:  1106: 'ily'\r\n[1709115558] sampled token: 28725: ','\r\n[1709115558] sampled token:  1346: ' let'\r\n[1709115558] sampled token: 28742: '''\r\n[1709115558] sampled token: 28713: 's'\r\n[1709115558] sampled token:  3232: ' focus'\r\n[1709115558] sampled token:   356: ' on'\r\n[1709115559] sampled token: 16752: ' improving'\r\n[1709115559] sampled token:   813: ' our'\r\n[1709115559] sampled token:  1918: ' team'\r\n[1709115559] sampled token:  3519: ' instead'\r\n[1709115559] sampled token:   302: ' of'\r\n[1709115559] sampled token: 18319: ' focusing'\r\n[1709115559] sampled token:   356: ' on'\r\n[1709115559] sampled token:  3235: ' individual'\r\n[1709115559] sampled token: 18022: ' promot'\r\n[1709115559] sampled token:   594: 'ions'\r\n[1709115559] sampled token: 28723: '.'\r\n[1709115559] sampled token:   816: ' We'\r\n[1709115559] sampled token:   544: ' all'\r\n[1709115559] sampled token:   506: ' have'\r\n[1709115559] sampled token:   264: ' a'\r\n[1709115559] sampled token:  3905: ' role'\r\n[1709115559] sampled token:   298: ' to'\r\n[1709115559] sampled token:  1156: ' play'\r\n[1709115559] sampled token:  1236: ' here'\r\n[1709115559] sampled token:   611: '.\"'\r\n[1709115559] sampled token:   422: ' #'\r\n[1709115559] sampled token: 17887: 'Team'\r\n[1709115559] sampled token:  7489: 'First'\r\n[1709115559] sampled token:   733: ' ['\r\n[1709115559] sampled token:    13: '\r\n'\r\n[1709115559] sampled token:  5121: ']('\r\n[1709115559] sampled token:  1056: 'data'\r\n[1709115559] sampled token: 28747: ':'\r\n[1709115559] sampled token:   772: 'text'\r\n[1709115559] sampled token: 28748: '/'\r\n[1709115559] sampled token: 19457: 'plain'\r\n[1709115559] sampled token: 28745: ';'\r\n[1709115559] sampled token:  2893: 'base'\r\n[1709115559] sampled token: 28784: '6'\r\n[1709115559] sampled token: 28781: '4'\r\n[1709115559] sampled token: 28725: ','\r\n[1709115559] sampled token:  1604: 'IC'\r\n[1709115559] sampled token: 13859: 'Ag'\r\n[1709115559] sampled token:  1138: 'ID'\r\n[1709115559] sampled token: 28727: 'w'\r\n[1709115559] sampled token: 28728: 'v'\r\n[1709115559] sampled token: 28738: 'T'\r\n[1709115559] sampled token: 28777: 'G'\r\n[1709115559] sampled token: 28790: 'V'\r\n[1709115559] sampled token: 28718: 'u'\r\n[1709115559] sampled token: 28828: 'Z'\r\n[1709115559] sampled token: 28770: '3'\r\n[1709115559] sampled token: 28754: 'R'\r\n[1709115559] sampled token: 11497: 'pb'\r\n[1709115559] sampled token: 28780: 'W'\r\n[1709115559] sampled token: 28779: 'U'\r\n[1709115559] sampled token: 28721: 'g'\r\n[1709115559] sampled token: 28802: 'Y'\r\n[1709115559] sampled token: 28750: '2'\r\n[1709115559] sampled token: 28774: '9'\r\n[1709115559] sampled token:  8282: 'tc'\r\n[1709115559] sampled token: 28769: 'H'\r\n[1709115559] sampled token: 28790: 'V'\r\n[1709115559] sampled token: 28734: '0'\r\n[1709115559] sampled token: 28828: 'Z'\r\n[1709115559] sampled token: 28814: 'X'\r\n[1709115559] sampled token: 28737: 'I'\r\n[1709115559] sampled token: 28718: 'u'\r\n[1709115559] sampled token: 28743: 'C'\r\n[1709115559] sampled token: 28710: 'i'\r\n[1709115559] sampled token: 13859: 'Ag'\r\n[1709115559] sampled token:  1604: 'IC'\r\n[1709115559] sampled token:  3167: 'At'\r\n[1709115559] sampled token:  6687: 'LS'\r\n[1709115559] sampled token: 28760: 'B'\r\n[1709115559] sampled token: 28824: 'Q'\r\n[1709115559] sampled token: 28802: 'Y'\r\n[1709115559] sampled token: 28814: 'X'\r\n[1709115559] sampled token: 28720: 'p'\r\n[1709115559] sampled token: 28765: 'F'\r\n…1470 more lines like this truncated…\r\n[1709115570] sampled token: 28743: 'C'\r\n[1709115570] sampled token: 28710: 'i'\r\n[1709115570] sampled token: 13859: 'Ag'\r\n[1709115570] sampled token:  1604: 'IC'\r\n[1709115570] sampled token:  3167: 'At'\r\n[1709115570] sampled token:  6687: 'LS'\r\n[1709115570] sampled token:  4919: 'Bl'\r\n[1709115570] sampled token: 28726: 'b'\r\n[1709115570] sampled token: 24390: 'GF'\r\n[1709115570] slot 0: context shift - n_keep = 0, n_left = 2046, n_discard = 1023\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "jnarvaezp",
        "body": "i have de same problem \r\n\r\nthis is my log, and stay en loop, the api never come back\r\n\r\n```\r\n[1709159738] slot 0 is processing [task id: 154]\r\n[1709159738] slot 0 : in cache: 449 tokens | to process: 160 tokens\r\n[1709159738] slot 0 : kv cache rm - [449, end)\r\n[1709159748] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159756] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159763] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159771] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159779] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159786] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159794] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159802] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159809] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159817] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n[1709159824] slot 0: context shift - n_keep = 1, n_left = 2046, n_discard = 1023\r\n```\r\n"
      },
      {
        "user": "DJ4ddi",
        "body": "We're seeing the same symptoms running complex requests with the `deepseek-coder:6.7b` model. Simple queries work fine, but complex queries can get stuck using 100% of a single CPU core (which changes occasionally).\r\n\r\nFor us, this only seems to happen when using `num_threads: 1` (or not specifying it). When explicitly specifying 2 or more threads in the request options, the problem disappears.\r\n\r\nOur system is a server running Ubuntu 22.04 with CUDA 12.3.2 (NVidia driver 550)."
      },
      {
        "user": "ygalblum",
        "body": "I get similar results.\r\n\r\nHowever, if I kill the client, the server seems to recover:\r\n```\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":256,\"tid\":\"139936501065280\",\"timestamp\":1710149468}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149515}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149545}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149575}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149605}\r\n\r\n\r\n\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":102[GIN] 2024/03/11 - 09:40:15 | 200 |        10m26s |       10.89.0.7 | POST     \"/api/chat\"\r\n[GIN] 2024/03/11 - 09:40:15 | 200 |        10m26s |       10.89.0.7 | POST     \"/api/chat\"\r\n[GIN] 2024/03/11 - 09:40:15 | 200 |        10m21s |       10.89.0.7 | POST     \"/api/chat\"\r\n[GIN] 2024/03/11 - 09:40:15 | 200 |         10m8s |       10.89.0.7 | POST     \"/api/chat\"\r\n3,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149634}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149664}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149693}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149723}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149752}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149782}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149811}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149841}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149871}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149901}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149930}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149960}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1622,\"msg\":\"slot context shift\",\"n_cache_tokens\":2047,\"n_ctx\":2048,\"n_discard\":1023,\"n_keep\":1,\"n_left\":2046,\"n_past\":2047,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710149989}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":17316,\"level\":\"INFO\",\"line\":1567,\"msg\":\"slot released\",\"n_cache_tokens\":1915,\"n_ctx\":2048,\"n_past\":1915,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710150015,\"truncated\":true}\r\n{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1593,\"msg\":\"all slots are idle\",\"tid\":\"139936501065280\",\"timestamp\":1710150015}\r\n{\"function\":\"launch_slot_with_data\",\"id_slot\":0,\"id_task\":36307,\"level\":\"INFO\",\"line\":970,\"msg\":\"slot is processing task\",\"tid\":\"139936501065280\",\"timestamp\":1710150015}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36307,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":1,\"tid\":\"139936501065280\",\"timestamp\":1710150015}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36307,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":513,\"tid\":\"139936501065280\",\"timestamp\":1710150016}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36307,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":1025,\"tid\":\"139936501065280\",\"timestamp\":1710150017}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36307,\"level\":\"INFO\",\"line\":304,\"msg\":\"prompt eval time     =    2579.74 ms /  1317 tokens (    1.96 ms per token,   510.52 tokens per second)\",\"n_prompt_tokens_processed\":1317,\"n_tokens_second\":510.51735526425966,\"t_prompt_processing\":2579.736,\"t_token\":1.9587972665148063,\"tid\":\"139936501065280\",\"timestamp\":1710150019}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36307,\"level\":\"INFO\",\"line\":320,\"msg\":\"generation eval time =    1171.75 ms /    43 runs   (   27.25 ms per token,    36.70 tokens per second)\",\"n_decoded\":43,\"n_tokens_second\":36.69734166163856,\"t_token\":27.249930232558143,\"t_token_generation\":1171.747,\"tid\":\"139936501065280\",\"timestamp\":1710150019}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36307,\"level\":\"INFO\",\"line\":331,\"msg\":\"          total time =    3751.48 ms\",\"t_prompt_processing\":2579.736,\"t_token_generation\":1171.747,\"t_total\":3751.483,\"tid\":\"139936501065280\",\"timestamp\":1710150019}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36307,\"level\":\"INFO\",\"line\":1567,\"msg\":\"slot released\",\"n_cache_tokens\":1360,\"n_ctx\":2048,\"n_past\":1360,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710150019,\"truncated\":false}\r\n{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1593,\"msg\":\"all slots are idle\",\"tid\":\"139936501065280\",\"timestamp\":1710150019}\r\n{\"function\":\"launch_slot_with_data\",\"id_slot\":0,\"id_task\":36309,\"level\":\"INFO\",\"line\":970,\"msg\":\"slot is processing task\",\"tid\":\"139936501065280\",\"timestamp\":1710150019}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36309,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":243,\"tid\":\"139936501065280\",\"timestamp\":1710150019}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36309,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":755,\"tid\":\"139936501065280\",\"timestamp\":1710150020}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36309,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":1267,\"tid\":\"139936501065280\",\"timestamp\":1710150021}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36309,\"level\":\"INFO\",\"line\":304,\"msg\":\"prompt eval time     =    2036.49 ms /  1029 tokens (    1.98 ms per token,   505.28 tokens per second)\",\"n_prompt_tokens_processed\":1029,\"n_tokens_second\":505.279904934704,\"t_prompt_processing\":2036.495,\"t_token\":1.979101068999028,\"tid\":\"139936501065280\",\"timestamp\":1710150025}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36309,\"level\":\"INFO\",\"line\":320,\"msg\":\"generation eval time =    4535.39 ms /   163 runs   (   27.82 ms per token,    35.94 tokens per second)\",\"n_decoded\":163,\"n_tokens_second\":35.93957741230633,\"t_token\":27.824478527607365,\"t_token_generation\":4535.39,\"tid\":\"139936501065280\",\"timestamp\":1710150025}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36309,\"level\":\"INFO\",\"line\":331,\"msg\":\"          total time =    6571.89 ms\",\"t_prompt_processing\":2036.495,\"t_token_generation\":4535.39,\"t_total\":6571.885,\"tid\":\"139936501065280\",\"timestamp\":1710150025}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36309,\"level\":\"INFO\",\"line\":1567,\"msg\":\"slot released\",\"n_cache_tokens\":1434,\"n_ctx\":2048,\"n_past\":1434,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710150025,\"truncated\":false}\r\n{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1593,\"msg\":\"all slots are idle\",\"tid\":\"139936501065280\",\"timestamp\":1710150025}\r\n{\"function\":\"launch_slot_with_data\",\"id_slot\":0,\"id_task\":36311,\"level\":\"INFO\",\"line\":970,\"msg\":\"slot is processing task\",\"tid\":\"139936501065280\",\"timestamp\":1710150025}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36311,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"tid\":\"139936501065280\",\"timestamp\":1710150025}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36311,\"level\":\"INFO\",\"line\":1567,\"msg\":\"slot released\",\"n_cache_tokens[GIN] 2024/03/11 - 09:40:26 | 200 |        10m11s |       10.89.0.7 | POST     \"/api/embeddings\"\r\n[GIN] 2024/03/11 - 09:40:33 | 200 |        10m13s |       10.89.0.7 | POST     \"/api/embeddings\"\r\n\":1434,\"n_ctx\":2048,\"n_past\":400,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710150026,\"truncated\":false}\r\n{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1593,\"msg\":\"all slots are idle\",\"tid\":\"139936501065280\",\"timestamp\":1710150026}\r\n{\"function\":\"launch_slot_with_data\",\"id_slot\":0,\"id_task\":36312,\"level\":\"INFO\",\"line\":970,\"msg\":\"slot is processing task\",\"tid\":\"139936501065280\",\"timestamp\":1710150026}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36312,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":243,\"tid\":\"139936501065280\",\"timestamp\":1710150026}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36312,\"level\":\"INFO\",\"line\":304,\"msg\":\"prompt eval time     =     584.45 ms /   290 tokens (    2.02 ms per token,   496.19 tokens per second)\",\"n_prompt_tokens_processed\":290,\"n_tokens_second\":496.194699956198,\"t_prompt_processing\":584.448,\"t_token\":2.0153379310344826,\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36312,\"level\":\"INFO\",\"line\":320,\"msg\":\"generation eval time =    6537.79 ms /   248 runs   (   26.36 ms per token,    37.93 tokens per second)\",\"n_decoded\":248,\"n_tokens_second\":37.933330947204446,\"t_token\":26.362040322580647,\"t_token_generation\":6537.786,\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36312,\"level\":\"INFO\",\"line\":331,\"msg\":\"          total time =    7122.23 ms\",\"t_prompt_processing\":584.448,\"t_token_generation\":6537.786,\"t_total\":7122.234,\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36312,\"level\":\"INFO\",\"line\":1567,\"msg\":\"slot released\",\"n_cache_tokens\":780,\"n_ctx\":2048,\"n_past\":780,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710150033,\"truncated\":false}\r\n{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1593,\"msg\":\"all slots are idle\",\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"launch_slot_with_data\",\"id_slot\":0,\"id_task\":36526,\"level\":\"INFO\",\"line\":970,\"msg\":\"slot is processing task\",\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36526,\"level\":\"INFO\",\"line\":304,\"msg\":\"prompt eval time     =     584.45 ms /   290 tokens (    2.02 ms per token,   496.19 tokens per second)\",\"n_prompt_tokens_processed\":290,\"n_tokens_second\":496.194699956198,\"t_prompt_processing\":584.448,\"t_token\":2.0153379310344826,\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36526,\"level\":\"INFO\",\"line\":320,\"msg\":\"generation eval time = 63636843.49 ms /   248 runs   (256600.18 ms per token,     0.00 tokens per second)\",\"n_decoded\":248,\"n_tokens_second\":0.0038971134709197583,\"t_token\":256600.17535080647,\"t_token_generation\":63636843.487,\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"print_timings\",\"id_slot\":0,\"id_task\":36526,\"level\":\"INFO\",\"line\":331,\"msg\":\"          total time = 63637427.94 ms\",\"t_prompt_processing\":584.448,\"t_token_generation\":63636843.487,\"t_total\":63637427.935,\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36526,\"level\":\"INFO\",\"line\":1567,\"msg\":\"slot released\",\"n_cache_tokens\":780,\"n_ctx\":2048,\"n_past\":0,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710150033,\"truncated\":false}\r\n{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1593,\"msg\":\"all slots are idle\",\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"launch_slot_with_data\",\"id_slot\":0,\"id_task\":36775,\"level\":\"INFO\",\"line\":970,\"msg\":\"slot is processing task\",\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36775,\"level\":\"INFO\",\"line\":1841,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"tid\":\"139936501065280\",\"timestamp\":1710150033}\r\n{\"function\":\"update_slots\",\"id_slot\":0,\"id_task\":36775,\"level\":\"INFO\",\"line\":1567,\"msg\":\"slot released\",\"n_cache_tokens\":780,\"n_ctx\":2048,\"n_past\":192,\"n_system_tokens\":0,\"tid\":\"139936501065280\",\"timestamp\":1710150034,\"truncated\":false}\r\n{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1593,\"msg\":\"all [GIN] 2024/03/11 - 09:40:34 | 200 |        10m13s |       10.89.0.7 | POST     \"/api/embeddings\"\r\n[GIN] 2024/03/11 - 09:40:34 | 200 |        10m13s |       10.89.0.7 | POST     \"/api/chat\"\r\n[GIN] 2024/03/11 - 09:40:34 | 200 |        10m13s |       10.89.0.7 | POST     \"/api/chat\"\r\n[GIN] 2024/03/11 - 09:40:34 | 200 |         10m5s |       10.89.0.7 | POST     \"/api/chat\"\r\n[GIN] 2024/03/11 - 09:40:34 | 200 |         10m5s |       10.89.0.7 | POST     \"/api/chat\"\r\n[GIN] 2024/03/11 - 09:40:34 | 200 |         10m2s |       10.89.0.7 | POST     \"/api/chat\"\r\n```\r\n\r\nI also tried to set `num_thread` to 4 (since the server is running on a 4 Core VM). But, it still happens.\r\nOne more thing to note, I'm running the containerized version."
      }
    ]
  },
  {
    "issue_number": 11046,
    "title": "The following openai message input formats are not supported",
    "author": "jkl375",
    "state": "open",
    "created_at": "2025-06-11T09:53:16Z",
    "updated_at": "2025-06-11T10:28:03Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nThe following openai message input formats are not supported：\n```\n\"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"image_url\": {\n                            \"url\": \"https://airesources.oss-cn-hangzhou.aliyuncs.com/jkl/300iduo/test.jpg\"\n                        },\n                        \"type\": \"image_url\",\n                    },\n                    {\"text\": \"describe the image\", \"type\": \"text\"},\n                ],\n            }\n        ],\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\nmaster",
    "comments": [
      {
        "user": "rick-github",
        "body": "https://github.com/ollama/ollama/blob/main/docs/openai.md#v1chatcompletions\n\n![Image](https://github.com/user-attachments/assets/e8421e58-b628-49aa-9554-99ce0309177a)\n"
      }
    ]
  },
  {
    "issue_number": 11028,
    "title": "Is it possible to use custom models fine-tuned on my local dataset with Ollama?",
    "author": "barancaki",
    "state": "closed",
    "created_at": "2025-06-09T18:21:40Z",
    "updated_at": "2025-06-11T10:19:22Z",
    "labels": [
      "feature request"
    ],
    "body": "Hi!\n\nFirst of all, thanks for the awesome project. I’ve been experimenting with Ollama and really enjoy how easy it is to run models locally.\n\nI was wondering:\n\t•\tIs it currently possible to use a custom LLM that I’ve fine-tuned on my own dataset (e.g., using LoRA or QLoRA)?\n\t•\tIf yes, what would be the recommended process to package and load it into Ollama?\n\t•\tIf no, are there any plans to support this in future releases?\n\nThanks in advance for your help!",
    "comments": [
      {
        "user": "rick-github",
        "body": "> Is it currently possible to use a custom LLM that I’ve fine-tuned on my own dataset (e.g., using LoRA or QLoRA)?\n\nYes, if the base model is supported by ollama.\n\n> what would be the recommended process to package and load it into Ollama?\n\nDepends on what the fine-tuning process produces.  If it's a LoRA adapter, create a Modelfile as described [here](https://github.com/ollama/ollama/blob/main/docs/import.md#Importing-a-fine-tuned-adapter-from-Safetensors-weights).  Some fune-tuners support creating a Safetensors file, which can be imported as described [here](https://github.com/ollama/ollama/blob/main/docs/import.md#Importing-a-model-from-Safetensors-weights).  unlsoth goes as far as creating a [GGUF and Modelfile](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama) which you can just pull from HF with `ollama pull`.\n\n"
      },
      {
        "user": "barancaki",
        "body": "Thank you so much @rick-github "
      }
    ]
  },
  {
    "issue_number": 11045,
    "title": "Graceful Handling of Disk Space Issues and Temporary Files for Incomplete Model Downloads",
    "author": "davidjimenez75",
    "state": "open",
    "created_at": "2025-06-11T09:01:42Z",
    "updated_at": "2025-06-11T09:03:50Z",
    "labels": [
      "feature request"
    ],
    "body": "Currently, Ollama does not check if there is enough available disk space before starting a model download. This can result in the entire hard drive being filled, especially with large models like llama4, which requires around 67 GB of space.\n\nIf the download fails due to lack of space or another issue, the following problems arise:\n\nThe temporary files remain in the  partial download folder, but there is no built-in command to clean them.\n\nRunning \"ollama rm llama4\" does not work, since the model is not considered \"installed\" yet.\n\nI humbly suggest two improvements:\n\nOllama should check disk space before starting a model download and warn the user if there is not enough space.\n\nThe ollama rm command should allow the removal of incomplete or partial downloads, so users can easily clean up disk space without manually searching for temp files.\n\nThis would improve user experience and prevent system issues caused by a full disk. Thank you!\n\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "Restarting the server will perform housekeeping, removing failed or interrupted downloads."
      }
    ]
  },
  {
    "issue_number": 8262,
    "title": "Segmentation Fault in AMD GPGPU Applications on 780M",
    "author": "zw963",
    "state": "closed",
    "created_at": "2024-12-28T13:36:31Z",
    "updated_at": "2025-06-11T07:09:48Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\r\n\r\nHi, I start my ollama model failed again when try use AMD 780M iGPU.\r\n\r\nfollowing is the log for `HSA_OVERRIDE_GFX_VERSION=11.0.0 /usr/bin/ollama serve`\r\n\r\n```sh\r\n ╰──➤ $ HSA_OVERRIDE_GFX_VERSION=11.0.0 /usr/bin/ollama serve\r\n2024/12/28 21:16:53 routes.go:1259: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION:11.0.0 HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/zw963/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\r\ntime=2024-12-28T21:16:53.340+08:00 level=INFO source=images.go:757 msg=\"total blobs: 32\"\r\ntime=2024-12-28T21:16:53.340+08:00 level=INFO source=images.go:764 msg=\"total unused blobs removed: 0\"\r\ntime=2024-12-28T21:16:53.341+08:00 level=INFO source=routes.go:1310 msg=\"Listening on 127.0.0.1:11434 (version 0.5.4)\"\r\ntime=2024-12-28T21:16:53.341+08:00 level=INFO source=routes.go:1339 msg=\"Dynamic LLM libraries\" runners=\"[cpu cpu_avx cpu_avx2 rocm_avx]\"\r\ntime=2024-12-28T21:16:53.341+08:00 level=INFO source=gpu.go:226 msg=\"looking for compatible GPUs\"\r\ntime=2024-12-28T21:16:53.365+08:00 level=WARN source=amd_linux.go:61 msg=\"ollama recommends running the https://www.amd.com/en/support/linux-drivers\" error=\"amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory\"\r\ntime=2024-12-28T21:16:53.366+08:00 level=INFO source=amd_linux.go:391 msg=\"skipping rocm gfx compatibility check\" HSA_OVERRIDE_GFX_VERSION=11.0.0\r\ntime=2024-12-28T21:16:53.366+08:00 level=INFO source=types.go:131 msg=\"inference compute\" id=0 library=rocm variant=\"\" compute=gfx1103 driver=0.0 name=1002:15bf total=\"16.0 GiB\" available=\"14.8 GiB\"\r\n^[[O[GIN] 2024/12/28 - 21:17:00 | 200 |      31.846µs |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/12/28 - 21:17:00 | 200 |   19.231074ms |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2024-12-28T21:17:01.006+08:00 level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/home/zw963/.ollama/models/blobs/sha256-ff1d1fc78170d787ee1201778e2dd65ea211654ca5fb7d69b5a2e7b123a50373 gpu=0 parallel=4 available=15936040960 required=\"8.8 GiB\"\r\ntime=2024-12-28T21:17:01.006+08:00 level=INFO source=server.go:104 msg=\"system memory\" total=\"46.8 GiB\" free=\"42.7 GiB\" free_swap=\"63.0 GiB\"\r\ntime=2024-12-28T21:17:01.006+08:00 level=INFO source=memory.go:356 msg=\"offload to rocm\" layers.requested=-1 layers.model=43 layers.offload=43 layers.split=\"\" memory.available=\"[14.8 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"8.8 GiB\" memory.required.partial=\"8.8 GiB\" memory.required.kv=\"2.6 GiB\" memory.required.allocations=\"[8.8 GiB]\" memory.weights.total=\"7.0 GiB\" memory.weights.repeating=\"6.3 GiB\" memory.weights.nonrepeating=\"717.8 MiB\" memory.graph.full=\"507.0 MiB\" memory.graph.partial=\"1.2 GiB\"\r\ntime=2024-12-28T21:17:01.007+08:00 level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/lib/ollama/runners/rocm_avx/ollama_llama_server runner --model /home/zw963/.ollama/models/blobs/sha256-ff1d1fc78170d787ee1201778e2dd65ea211654ca5fb7d69b5a2e7b123a50373 --ctx-size 8192 --batch-size 512 --n-gpu-layers 43 --threads 8 --parallel 4 --port 12215\"\r\ntime=2024-12-28T21:17:01.007+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\r\ntime=2024-12-28T21:17:01.007+08:00 level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-12-28T21:17:01.007+08:00 level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-12-28T21:17:01.036+08:00 level=INFO source=runner.go:945 msg=\"starting go runner\"\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 ROCm devices:\r\n  Device 0: AMD Radeon 780M, compute capability 11.0, VMM: no\r\ntime=2024-12-28T21:17:02.349+08:00 level=INFO source=runner.go:946 msg=system info=\"ROCm : PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=8\r\nllama_load_model_from_file: using device ROCm0 (AMD Radeon 780M) - 23866 MiB free\r\ntime=2024-12-28T21:17:02.349+08:00 level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:12215\"\r\nllama_model_loader: loaded meta data with 29 key-value pairs and 464 tensors from /home/zw963/.ollama/models/blobs/sha256-ff1d1fc78170d787ee1201778e2dd65ea211654ca5fb7d69b5a2e7b123a50373 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = gemma2\r\nllama_model_loader: - kv   1:                               general.name str              = gemma-2-9b-it\r\nllama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\r\nllama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 3584\r\nllama_model_loader: - kv   4:                         gemma2.block_count u32              = 42\r\nllama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 16\r\nllama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 256\r\nllama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 256\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\r\nllama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\r\nllama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\r\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\r\nllama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\r\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\r\nllama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\r\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\r\nllama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\r\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  169 tensors\r\nllama_model_loader: - type q4_0:  294 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 108\r\nllm_load_vocab: token to piece cache size = 1.6014 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = gemma2\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 256000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 3584\r\nllm_load_print_meta: n_layer          = 42\r\nllm_load_print_meta: n_head           = 16\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 256\r\nllm_load_print_meta: n_swa            = 4096\r\nllm_load_print_meta: n_embd_head_k    = 256\r\nllm_load_print_meta: n_embd_head_v    = 256\r\nllm_load_print_meta: n_gqa            = 2\r\nllm_load_print_meta: n_embd_k_gqa     = 2048\r\nllm_load_print_meta: n_embd_v_gqa     = 2048\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 9B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 9.24 B\r\nllm_load_print_meta: model size       = 5.06 GiB (4.71 BPW)\r\nllm_load_print_meta: general.name     = gemma-2-9b-it\r\nllm_load_print_meta: BOS token        = 2 '<bos>'\r\nllm_load_print_meta: EOS token        = 1 '<eos>'\r\nllm_load_print_meta: EOT token        = 107 '<end_of_turn>'\r\nllm_load_print_meta: UNK token        = 3 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<pad>'\r\nllm_load_print_meta: LF token         = 227 '<0x0A>'\r\nllm_load_print_meta: EOG token        = 1 '<eos>'\r\nllm_load_print_meta: EOG token        = 107 '<end_of_turn>'\r\nllm_load_print_meta: max token length = 93\r\ntime=2024-12-28T21:17:02.535+08:00 level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nllm_load_tensors: offloading 42 repeating layers to GPU\r\nllm_load_tensors: offloading output layer to GPU\r\nllm_load_tensors: offloaded 43/43 layers to GPU\r\nllm_load_tensors:   CPU_Mapped model buffer size =   717.77 MiB\r\nllm_load_tensors:        ROCm0 model buffer size =  5185.21 MiB\r\n```\r\n----------------\r\n\r\nFollowing is the failed logs when i try run `ollama run gemma2` in another opened terminal.\r\n\r\n```\r\nSIGSEGV: segmentation violation\r\nPC=0x713070f0fe2b m=5 sigcode=1 addr=0x18\r\nsignal arrived during cgo execution\r\n\r\ngoroutine 20 gp=0xc000104a80 m=5 mp=0xc000100008 [syscall]:\r\nruntime.cgocall(0x5693bccf4990, 0xc000204b78)\r\n        runtime/cgocall.go:167 +0x4b fp=0xc000204b50 sp=0xc000204b18 pc=0x5693bcaa896b\r\ngithub.com/ollama/ollama/llama._Cfunc_llama_load_model_from_file(0x712ed4000be0, {0x0, 0x2b, 0x1, 0x0, 0x0, 0x0, 0x5693bccf41e0, 0xc000208000, 0x0, ...})\r\n        _cgo_gotypes.go:707 +0x50 fp=0xc000204b78 sp=0xc000204b50 pc=0x5693bcb53250\r\ngithub.com/ollama/ollama/llama.LoadModelFromFile.func1({0x7ffc8d222d0e?, 0x0?}, {0x0, 0x2b, 0x1, 0x0, 0x0, 0x0, 0x5693bccf41e0, 0xc000208000, ...})\r\n        github.com/ollama/ollama/llama/llama.go:311 +0x127 fp=0xc000204c78 sp=0xc000204b78 pc=0x5693bcb55e67\r\ngithub.com/ollama/ollama/llama.LoadModelFromFile({0x7ffc8d222d0e, 0x68}, {0x2b, 0x0, 0x1, 0x0, {0x0, 0x0, 0x0}, 0xc00011e1b0, ...})\r\n        github.com/ollama/ollama/llama/llama.go:311 +0x2d6 fp=0xc000204dc8 sp=0xc000204c78 pc=0x5693bcb55b56\r\ngithub.com/ollama/ollama/llama/runner.(*Server).loadModel(0xc0001461b0, {0x2b, 0x0, 0x1, 0x0, {0x0, 0x0, 0x0}, 0xc00011e1b0, 0x0}, ...)\r\n        github.com/ollama/ollama/llama/runner/runner.go:859 +0xc5 fp=0xc000204f10 sp=0xc000204dc8 pc=0x5693bccf1c25\r\ngithub.com/ollama/ollama/llama/runner.Execute.gowrap1()\r\n        github.com/ollama/ollama/llama/runner/runner.go:979 +0xda fp=0xc000204fe0 sp=0xc000204f10 pc=0x5693bccf357a\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000204fe8 sp=0xc000204fe0 pc=0x5693bcab63a1\r\ncreated by github.com/ollama/ollama/llama/runner.Execute in goroutine 1\r\n        github.com/ollama/ollama/llama/runner/runner.go:979 +0xd0d\r\n\r\ngoroutine 1 gp=0xc0000061c0 m=nil [IO wait]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:424 +0xce fp=0xc0000637b0 sp=0xc000063790 pc=0x5693bcaae76e\r\nruntime.netpollblock(0xc000063800?, 0xbca46fc6?, 0x93?)\r\n        runtime/netpoll.go:575 +0xf7 fp=0xc0000637e8 sp=0xc0000637b0 pc=0x5693bca734d7\r\ninternal/poll.runtime_pollWait(0x712f89fca730, 0x72)\r\n        runtime/netpoll.go:351 +0x85 fp=0xc000063808 sp=0xc0000637e8 pc=0x5693bcaada65\r\ninternal/poll.(*pollDesc).wait(0xc000190100?, 0x2c?, 0x0)\r\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000063830 sp=0xc000063808 pc=0x5693bcb038a7\r\ninternal/poll.(*pollDesc).waitRead(...)\r\n        internal/poll/fd_poll_runtime.go:89\r\ninternal/poll.(*FD).Accept(0xc000190100)\r\n        internal/poll/fd_unix.go:620 +0x295 fp=0xc0000638d8 sp=0xc000063830 pc=0x5693bcb04e15\r\nnet.(*netFD).accept(0xc000190100)\r\n        net/fd_unix.go:172 +0x29 fp=0xc000063990 sp=0xc0000638d8 pc=0x5693bcb7d7a9\r\nnet.(*TCPListener).accept(0xc00012e6c0)\r\n        net/tcpsock_posix.go:159 +0x1e fp=0xc0000639e0 sp=0xc000063990 pc=0x5693bcb8ddfe\r\nnet.(*TCPListener).Accept(0xc00012e6c0)\r\n        net/tcpsock.go:372 +0x30 fp=0xc000063a10 sp=0xc0000639e0 pc=0x5693bcb8d130\r\nnet/http.(*onceCloseListener).Accept(0xc000146240?)\r\n        <autogenerated>:1 +0x24 fp=0xc000063a28 sp=0xc000063a10 pc=0x5693bcccbd04\r\nnet/http.(*Server).Serve(0xc00018e4b0, {0x5693bd0cbeb8, 0xc00012e6c0})\r\n        net/http/server.go:3330 +0x30c fp=0xc000063b58 sp=0xc000063a28 pc=0x5693bccbda4c\r\ngithub.com/ollama/ollama/llama/runner.Execute({0xc000132010?, 0x5693bcab5ffc?, 0x0?})\r\n        github.com/ollama/ollama/llama/runner/runner.go:1005 +0x11a9 fp=0xc000063ef8 sp=0xc000063b58 pc=0x5693bccf3149\r\nmain.main()\r\n        github.com/ollama/ollama/cmd/runner/main.go:11 +0x54 fp=0xc000063f50 sp=0xc000063ef8 pc=0x5693bccf40d4\r\nruntime.main()\r\n        runtime/proc.go:272 +0x29d fp=0xc000063fe0 sp=0xc000063f50 pc=0x5693bca7aabd\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000063fe8 sp=0xc000063fe0 pc=0x5693bcab63a1\r\n\r\ngoroutine 2 gp=0xc000006c40 m=nil [force gc (idle)]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:424 +0xce fp=0xc000098fa8 sp=0xc000098f88 pc=0x5693bcaae76e\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:430\r\nruntime.forcegchelper()\r\n        runtime/proc.go:337 +0xb8 fp=0xc000098fe0 sp=0xc000098fa8 pc=0x5693bca7adf8\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000098fe8 sp=0xc000098fe0 pc=0x5693bcab63a1\r\ncreated by runtime.init.7 in goroutine 1\r\n        runtime/proc.go:325 +0x1a\r\n\r\ngoroutine 3 gp=0xc000007180 m=nil [GC sweep wait]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:424 +0xce fp=0xc000099780 sp=0xc000099760 pc=0x5693bcaae76e\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:430\r\nruntime.bgsweep(0xc000026400)\r\n        runtime/mgcsweep.go:277 +0x94 fp=0xc0000997c8 sp=0xc000099780 pc=0x5693bca65634\r\nruntime.gcenable.gowrap1()\r\n        runtime/mgc.go:204 +0x25 fp=0xc0000997e0 sp=0xc0000997c8 pc=0x5693bca59ee5\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000997e8 sp=0xc0000997e0 pc=0x5693bcab63a1\r\ncreated by runtime.gcenable in goroutine 1\r\n        runtime/mgc.go:204 +0x66\r\n\r\ngoroutine 4 gp=0xc000007340 m=nil [GC scavenge wait]:\r\nruntime.gopark(0xc000026400?, 0x5693bcfb8fc0?, 0x1?, 0x0?, 0xc000007340?)\r\n        runtime/proc.go:424 +0xce fp=0xc000099f78 sp=0xc000099f58 pc=0x5693bcaae76e\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:430\r\nruntime.(*scavengerState).park(0x5693bd2b6380)\r\n        runtime/mgcscavenge.go:425 +0x49 fp=0xc000099fa8 sp=0xc000099f78 pc=0x5693bca63069\r\nruntime.bgscavenge(0xc000026400)\r\n        runtime/mgcscavenge.go:653 +0x3c fp=0xc000099fc8 sp=0xc000099fa8 pc=0x5693bca635dc\r\nruntime.gcenable.gowrap2()\r\n        runtime/mgc.go:205 +0x25 fp=0xc000099fe0 sp=0xc000099fc8 pc=0x5693bca59e85\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000099fe8 sp=0xc000099fe0 pc=0x5693bcab63a1\r\ncreated by runtime.gcenable in goroutine 1\r\n        runtime/mgc.go:205 +0xa5\r\n\r\ngoroutine 18 gp=0xc000104700 m=nil [finalizer wait]:\r\nruntime.gopark(0xc000098648?, 0x5693bca503e5?, 0xb0?, 0x1?, 0xc0000061c0?)\r\n        runtime/proc.go:424 +0xce fp=0xc000098620 sp=0xc000098600 pc=0x5693bcaae76e\r\nruntime.runfinq()\r\n        runtime/mfinal.go:193 +0x107 fp=0xc0000987e0 sp=0xc000098620 pc=0x5693bca58f67\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000987e8 sp=0xc0000987e0 pc=0x5693bcab63a1\r\ncreated by runtime.createfing in goroutine 1\r\n        runtime/mfinal.go:163 +0x3d\r\n\r\ngoroutine 19 gp=0xc0001048c0 m=nil [chan receive]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:424 +0xce fp=0xc000094718 sp=0xc0000946f8 pc=0x5693bcaae76e\r\nruntime.chanrecv(0xc0001120e0, 0x0, 0x1)\r\n        runtime/chan.go:639 +0x41c fp=0xc000094790 sp=0xc000094718 pc=0x5693bca49bbc\r\nruntime.chanrecv1(0x0?, 0x0?)\r\n        runtime/chan.go:489 +0x12 fp=0xc0000947b8 sp=0xc000094790 pc=0x5693bca49792\r\nruntime.unique_runtime_registerUniqueMapCleanup.func1(...)\r\n        runtime/mgc.go:1781\r\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\r\n        runtime/mgc.go:1784 +0x2f fp=0xc0000947e0 sp=0xc0000947b8 pc=0x5693bca5cd4f\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000947e8 sp=0xc0000947e0 pc=0x5693bcab63a1\r\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\r\n        runtime/mgc.go:1779 +0x96\r\n\r\ngoroutine 21 gp=0xc000104c40 m=nil [semacquire]:\r\nruntime.gopark(0x0?, 0x0?, 0x20?, 0x81?, 0x0?)\r\n        runtime/proc.go:424 +0xce fp=0xc000095618 sp=0xc0000955f8 pc=0x5693bcaae76e\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:430\r\nruntime.semacquire1(0xc0001461b8, 0x0, 0x1, 0x0, 0x12)\r\n        runtime/sema.go:178 +0x22c fp=0xc000095680 sp=0xc000095618 pc=0x5693bca8da8c\r\nsync.runtime_Semacquire(0x0?)\r\n        runtime/sema.go:71 +0x25 fp=0xc0000956b8 sp=0xc000095680 pc=0x5693bcaaf9a5\r\nsync.(*WaitGroup).Wait(0x0?)\r\n        sync/waitgroup.go:118 +0x48 fp=0xc0000956e0 sp=0xc0000956b8 pc=0x5693bcacbc48\r\ngithub.com/ollama/ollama/llama/runner.(*Server).run(0xc0001461b0, {0x5693bd0cc4a0, 0xc000196050})\r\n        github.com/ollama/ollama/llama/runner/runner.go:315 +0x47 fp=0xc0000957b8 sp=0xc0000956e0 pc=0x5693bccee2c7\r\ngithub.com/ollama/ollama/llama/runner.Execute.gowrap2()\r\n        github.com/ollama/ollama/llama/runner/runner.go:984 +0x28 fp=0xc0000957e0 sp=0xc0000957b8 pc=0x5693bccf3468\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000957e8 sp=0xc0000957e0 pc=0x5693bcab63a1\r\ncreated by github.com/ollama/ollama/llama/runner.Execute in goroutine 1\r\n        github.com/ollama/ollama/llama/runner/runner.go:984 +0xde5\r\n\r\ngoroutine 22 gp=0xc000105340 m=nil [IO wait]:\r\nruntime.gopark(0xc0002a6000?, 0xc000185958?, 0x3e?, 0x1?, 0xb?)\r\n        runtime/proc.go:424 +0xce fp=0xc000185918 sp=0xc0001858f8 pc=0x5693bcaae76e\r\nruntime.netpollblock(0x5693bcae9f98?, 0xbca46fc6?, 0x93?)\r\n        runtime/netpoll.go:575 +0xf7 fp=0xc000185950 sp=0xc000185918 pc=0x5693bca734d7\r\ninternal/poll.runtime_pollWait(0x712f89fca618, 0x72)\r\n        runtime/netpoll.go:351 +0x85 fp=0xc000185970 sp=0xc000185950 pc=0x5693bcaada65\r\ninternal/poll.(*pollDesc).wait(0xc000190180?, 0xc0001b8000?, 0x0)\r\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000185998 sp=0xc000185970 pc=0x5693bcb038a7\r\ninternal/poll.(*pollDesc).waitRead(...)\r\n        internal/poll/fd_poll_runtime.go:89\r\ninternal/poll.(*FD).Read(0xc000190180, {0xc0001b8000, 0x1000, 0x1000})\r\n        internal/poll/fd_unix.go:165 +0x27a fp=0xc000185a30 sp=0xc000185998 pc=0x5693bcb043fa\r\nnet.(*netFD).Read(0xc000190180, {0xc0001b8000?, 0xc000185aa0?, 0x5693bcb03d65?})\r\n        net/fd_posix.go:55 +0x25 fp=0xc000185a78 sp=0xc000185a30 pc=0x5693bcb7c6c5\r\nnet.(*conn).Read(0xc000124098, {0xc0001b8000?, 0x0?, 0xc00012d058?})\r\n        net/net.go:189 +0x45 fp=0xc000185ac0 sp=0xc000185a78 pc=0x5693bcb860c5\r\nnet.(*TCPConn).Read(0xc00012d050?, {0xc0001b8000?, 0xc000190180?, 0xc000185af8?})\r\n        <autogenerated>:1 +0x25 fp=0xc000185af0 sp=0xc000185ac0 pc=0x5693bcb93165\r\nnet/http.(*connReader).Read(0xc00012d050, {0xc0001b8000, 0x1000, 0x1000})\r\n        net/http/server.go:798 +0x14b fp=0xc000185b40 sp=0xc000185af0 pc=0x5693bccb434b\r\nbufio.(*Reader).fill(0xc000130480)\r\n        bufio/bufio.go:110 +0x103 fp=0xc000185b78 sp=0xc000185b40 pc=0x5693bcc72f63\r\nbufio.(*Reader).Peek(0xc000130480, 0x4)\r\n        bufio/bufio.go:148 +0x53 fp=0xc000185b98 sp=0xc000185b78 pc=0x5693bcc73093\r\nnet/http.(*conn).serve(0xc000146240, {0x5693bd0cc468, 0xc00012cf60})\r\n        net/http/server.go:2127 +0x738 fp=0xc000185fb8 sp=0xc000185b98 pc=0x5693bccb9698\r\nnet/http.(*Server).Serve.gowrap3()\r\n        net/http/server.go:3360 +0x28 fp=0xc000185fe0 sp=0xc000185fb8 pc=0x5693bccbde48\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000185fe8 sp=0xc000185fe0 pc=0x5693bcab63a1\r\ncreated by net/http.(*Server).Serve in goroutine 1\r\n        net/http/server.go:3360 +0x485\r\n\r\nrax    0x712ed72c8ad0\r\nrbx    0x712ed72ced40\r\nrcx    0x713070da6663\r\nrdx    0x712ed4005130\r\nrdi    0x712ed72ced40\r\nrsi    0x3\r\nrbp    0x712edbff61d0\r\nrsp    0x712edbff61a0\r\nr8     0x0\r\nr9     0x0\r\nr10    0x4\r\nr11    0xa66e143e45c2eb86\r\nr12    0x0\r\nr13    0x18\r\nr14    0xffffffffffffffc0\r\nr15    0x712dc3ef8e80\r\nrip    0x713070f0fe2b\r\nrflags 0x10206\r\ncs     0x33\r\nfs     0x0\r\ngs     0x0\r\ntime=2024-12-28T21:17:03.119+08:00 level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-12-28T21:17:03.370+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 2\"\r\n[GIN] 2024/12/28 - 21:17:03 | 500 |  2.421283447s |       127.0.0.1 | POST     \"/api/generate\"\r\n```\r\n\r\nI----------------\r\n\r\nFollowing is my packages info:\r\n\r\n```\r\n ╰──➤ $ 1  pacman -Q |grep 'ollama\\|rocm'\r\nollama 0.5.4-1\r\nollama-rocm 0.5.4-1\r\npython-pytorch-rocm 2.5.1-7\r\nrocm-clang-ocl 6.1.2-1\r\nrocm-cmake 6.2.4-1\r\nrocm-core 6.2.4-2\r\nrocm-device-libs 6.2.4-1\r\nrocm-hip-libraries 6.2.2-1\r\nrocm-hip-runtime 6.2.2-1\r\nrocm-hip-sdk 6.2.2-1\r\nrocm-language-runtime 6.2.2-1\r\nrocm-llvm 6.2.4-1\r\nrocm-opencl-runtime 6.2.4-1\r\nrocm-opencl-sdk 6.2.2-1\r\nrocm-smi-lib 6.2.4-1\r\nrocminfo 6.2.4-1\r\n```\r\n\r\nThanks\r\n\r\n### OS\r\n\r\nLinux\r\n\r\n### GPU\r\n\r\nAMD\r\n\r\n### CPU\r\n\r\nAMD\r\n\r\n### Ollama version\r\n\r\n0.5.4-1 tested both on arch linux installed version and github release page downloaded version.\r\nIt works before and broken after i update my arch linux before create this issue.",
    "comments": [
      {
        "user": "copycraft",
        "body": "i see ou use arch btw. chad"
      },
      {
        "user": "zw963",
        "body": "I tried remove the following arch package:\r\n\r\nollama 0.5.4-1\r\nollama-rocm 0.5.4-1\r\n\r\nthen installed from our github release page, then run downloaded save version ollama, get exactly same issue.\r\n\r\nand, following is my upgrade log: (before upgrade it, ollama work well)\r\n\r\n```\r\n[2024-12-28T21:05:50+0800] [ALPM] upgraded python-pytorch-rocm (2.5.1-4 -> 2.5.1-7)\r\n[2024-12-28T21:05:44+0800] [ALPM] upgraded ollama-rocm (0.5.2-1 -> 0.5.4-1)\r\n[2024-12-28T21:05:43+0800] [ALPM] upgraded ollama (0.5.2-1 -> 0.5.4-1)\r\n```\r\n\r\nSo, i tried download the 0.5.2 from github release page,  run it instead, but still not work.\r\n\r\nThanks."
      },
      {
        "user": "zw963",
        "body": "Okay, i can confirm this issue caused by 6.12 linux kernel. \r\n\r\nhttps://gitlab.freedesktop.org/drm/amd/-/issues/3821\r\n\r\nFor arch linux user, use lts version linux or use an older 6.11 version linux kernel can workaround it.\r\n\r\n![image](https://github.com/user-attachments/assets/f4d658ae-ca48-4584-abf2-f076d87ade0a)\r\n"
      }
    ]
  },
  {
    "issue_number": 10792,
    "title": "Gemma 3n",
    "author": "diegovalenzuelaiturra",
    "state": "open",
    "created_at": "2025-05-21T05:55:03Z",
    "updated_at": "2025-06-11T06:28:45Z",
    "labels": [
      "model request"
    ],
    "body": "Would it be possible to support Gemma 3n ?",
    "comments": [
      {
        "user": "cougarten",
        "body": "made llama.cpp request on their idea board: https://github.com/ggml-org/llama.cpp/discussions/13831\n(hope that's the right place. That's where the bulk of the work has to be done, right?)"
      },
      {
        "user": "chriszs",
        "body": "When asked about whether the current LiteRT (formerly Tensorflow) weights would ever be released in GGUF, one Google developer relations person [said](https://huggingface.co/google/gemma-3n-E4B-it-litert-preview/discussions/11#68344aef0c0aff775f05b1b4), \"Yes, this repository is just a preview. Stay tuned!\" More recently, another [said](https://x.com/_philschmid/status/1932333817789870329),  \"We are working hard on making Gemma 3n 4B available in popular open source frameworks! Stay tuned.\" So, I guess we should \"stay tuned.\""
      }
    ]
  },
  {
    "issue_number": 11036,
    "title": "gemma3:27b unable to get full 128k context",
    "author": "pavanrajkg04",
    "state": "closed",
    "created_at": "2025-06-10T17:51:29Z",
    "updated_at": "2025-06-11T05:17:21Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\ni tried with the gemma3:27b model but I am unable to get the full 128k context as I am working on chatbot that takes the SQL data. i am unable to deliver what I am expecting. i need help on this\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nmacOS, Linux\n\n### GPU\n\nNvidia\n\n### CPU\n\n_No response_\n\n### Ollama version\n\nv0.9.1",
    "comments": [
      {
        "user": "rick-github",
        "body": "> but I am unable to get the full 128k context\n\nWhat does this mean?  Your client can't send 128k tokens of data?  The model doesn't load because the context is too big?  The model doesn't generate tokens when the context is 128k?  "
      },
      {
        "user": "duck-5",
        "body": "Please provide more context, the GPU model, and if possible the Modelfile and the request json."
      },
      {
        "user": "pavanrajkg04",
        "body": "I found the solution for this. Ollama doesn't directly support 128k context; it maxes out at 32k context. \nusing this command we can increase the context length:\nexport OLLAMA_CONTEXT_LENGTH=131072"
      }
    ]
  },
  {
    "issue_number": 10971,
    "title": "[Qwen-2.5vl] Support Video Attachments for conversation",
    "author": "Notbici",
    "state": "open",
    "created_at": "2025-06-04T15:59:05Z",
    "updated_at": "2025-06-10T23:32:40Z",
    "labels": [
      "feature request"
    ],
    "body": "Qwen-2.5vl supports video and on their [huggingface](https://huggingface.co/spaces/Qwen/Qwen2.5-VL-32B-Instruct) they also have a demo mode for this if you want to try it out.\n\nHere's an example model for reference and ollama already has this up on its repository https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct\n\nI believe there is some torch level wiring required to feed a video into the model, is this something Ollama can support?\n\nBasically making it possible for us with Ollama's API attach a 'video' input to a message, similar to how we do images.",
    "comments": [
      {
        "user": "Yami-Bitshark",
        "body": "I think, you simply need to specify the mime type and feed it the base64 of the video. thats the required \"wiring\" you talking about i think."
      },
      {
        "user": "Notbici",
        "body": "> I think, you simply need to specify the mime type and feed it the base64 of the video. thats the required \"wiring\" you talking about i think.\n\nNot sure if I did it right, though when I feed a base64 image to ollama it refuses with:\n> failed to process inputs: image: unknown\n\n"
      }
    ]
  },
  {
    "issue_number": 11037,
    "title": "Providing JSON Schema results in ignoring tool calls",
    "author": "tinkerctl",
    "state": "closed",
    "created_at": "2025-06-10T18:57:50Z",
    "updated_at": "2025-06-10T21:53:09Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nKnown models affected: `llama3.1:8b`, `qwen3:14b`\n\nPrompt: `list all my repositories. return a json object as the response.`\nResult schema provided: \n```json\n{\n  \"properties\": {\n    \"repositories\": {\n      \"description\": \"The result of repositories from the function call\",\n      \"type\": \"array\"\n    }\n  },\n  \"type\": \"object\"\n}\n```\n\nExample result w/ result schema (not correct, no tool calls requested): \n```json\n{\"repositories\":[{\"name\": \"my_repo_1\"},{\"name\": \"my_repo_2\"}]}\n```\n\nExample result without result schema (correct, tool calls requested):\n```\n```json\n{\n  \"repositories\": [\n    {\n      \"id\": 1,\n      \"name\": \"foo\",\n      \"full_name\": \"myorg/foo\",\n      \"description\": \"\",\n      \"private\": false,\n    }\n  ]\n}\n\\```\n```\nDepending on the model, I'll either get an empty json response or a hallucination. When dropping the schema, tool calls are properly requested. I'll try to take some time to recreate with curl at a later date so we have something easier to recreate with.\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "tinkerctl",
        "body": "Tested on `0.8.0` and `0.7.1`. Same result."
      },
      {
        "user": "rick-github",
        "body": "If you are sending the schema along with the tool list and the prompt  \"list all my repositories. return a json object as the response\", then you are asking the model to make the tool calls conform to the schema of the response, which is not what you want.  You need to send the schema with the output of the tool calls.\n\n```python\n#!/usr/bin/env python3\n\nimport ollama\n\nfrom pydantic import BaseModel\nfrom typing import List\n\nmodel = \"qwen3:14b\"\nprompt = \"list all my repositories. return a json object as the response\"\n\nclass RepoList(BaseModel):\n  repositories: List[str]\n\ndef list_repositories():\n  \"\"\"\n    List all repositories\n  \"\"\"\n  return [\"my_repo_1\", \"my_repo_2\"]\n\ntools = [ list_repositories ]\ntoolmap = {f.__name__:f for f in tools}\n\nmessages = [{\"role\":\"user\",\"content\":prompt}]\n\nresponse = ollama.chat(model=model, messages=messages,tools=tools,stream=False)\n\nif response.message.tool_calls:\n  for tool in response.message.tool_calls:\n    if func := toolmap.get(tool.function.name):\n      output = func(**tool.function.arguments)\n      messages.append({\"role\":\"tool\", \"content\":str(output), \"name\":tool.function.name})\n  response = ollama.chat(model=model, messages=messages, stream=False, format=RepoList.model_json_schema())\n\nprint(response.message)\n```\n```console\n$ ./11037.py\nrole='assistant' content='{\"repositories\": [\"my_repo_1\", \"my_repo_2\"]}' thinking=None images=None tool_calls=None\n```"
      },
      {
        "user": "tinkerctl",
        "body": "I see. If that's the case, that would mean I need to know ahead of time that the model is not going to request additional tool calls ahead of time. \n\nI'm new to functions, so please correct me if I'm wrong, but isn't that a bit limiting? That would limit the amount of round trips to a depth of 1, instead of allowing the model to ask for more information.\n\nEDIT: Thank you so much for taking the time to provide a working example!"
      }
    ]
  },
  {
    "issue_number": 11040,
    "title": "OLLAMA_NEW_ENGINE not working for one of the models I use",
    "author": "technovangelist",
    "state": "closed",
    "created_at": "2025-06-10T21:23:53Z",
    "updated_at": "2025-06-10T21:33:49Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI use tavernari/git-commit-message to auto generate git commits. This works amazingly well. but when I switched to using the new ollama engine, it breaks. Here is an image of the last few commits. \n\n![Image](https://github.com/user-attachments/assets/95cde2b6-9f47-4dea-aeb2-0f204a4a9090)\n\nNotice the top commits all make sense and are exactly what changed. The last one is when OLLAMA_NEW_ENGINE was set to true. \n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "technovangelist",
        "body": "ignore this one. i misunderstood something"
      }
    ]
  },
  {
    "issue_number": 5424,
    "title": "Supports voice recognition and text-to-speech capabilities, with customizable extension abilities",
    "author": "skytodmoon",
    "state": "open",
    "created_at": "2024-07-02T00:01:13Z",
    "updated_at": "2025-06-10T20:22:21Z",
    "labels": [
      "feature request"
    ],
    "body": "Feature Request: Support for Voice Recognition and Text-to-Speech with Custom Extension Capabilities\r\n\r\nI would like to propose the addition of voice recognition and text-to-speech functionalities to the project. These features would greatly enhance the user experience by allowing for hands-free interaction and accessibility.\r\n\r\nAdditionally, I suggest implementing a customizable extension framework that would enable developers to integrate their own voice commands or speech synthesis options, thereby expanding the project's versatility and adaptability to various use cases.\r\n\r\nThank you for considering this enhancement to the project. I believe these features would be a valuable addition and open up new possibilities for users and developers alike.",
    "comments": [
      {
        "user": "CrazyBoyM",
        "body": "need it too."
      },
      {
        "user": "v-byte-cpu",
        "body": "Hi all — following up from my earlier issue (#11021, closed as dup), here’s a concise roadmap for adding native TTS support to Ollama: \n👉 https://gist.github.com/v-byte-cpu/28d402ba5601a25432c7e18a99d3725f\n\nIt summarizes concrete ideas around API, testing, voice handling, and future extensions — based on the community discussion here and related TTS models.\n\nI’d be happy to contribute a design document or an initial PR (Python + Go) and help move this forward.  \nIf there are any internal plans or ongoing discussions, I’d be glad to align with them first.  \nPlease advise what would be the most useful way to start — thanks!\n"
      }
    ]
  },
  {
    "issue_number": 10967,
    "title": "Model fallback to new CPU instance despite existing GPU instance.",
    "author": "TheNha",
    "state": "open",
    "created_at": "2025-06-04T07:53:25Z",
    "updated_at": "2025-06-10T18:53:23Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI'm serving a model on Ollama using the GPU at 100% capacity, with the keep-alive setting set to -1. Initially, some requests are correctly handled by the GPU instance.\n\nHowever, after a few successful requests, subsequent requests are served by a new instance of the model running on the CPU, while the original GPU-based instance is still active and continues to occupy GPU memory.\n\nThis behavior leads to inefficient resource usage and degraded inference performance due to CPU fallback, even though the GPU instance is still running and using memory.\n\nExpected Behavior:\nRequests should continue to be served by the existing GPU instance as long as it is active and available, especially with keep-alive: -1.\n\n![Image](https://github.com/user-attachments/assets/1633fb00-4a94-46ec-a418-ac4f2d58bc7f)\n\n### Relevant log output\n\n```shell\nERROR 2025-06-04T06:40:29.625833Z [resource.labels.containerName: app]\ntime=2025-06-04T06:40:29.650Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"83.5 GiB\" before.free=\"79.0 GiB\" before.free_swap=\"0 B\" now.total=\"83.5 GiB\" now.free=\"79.0 GiB\" now.free_swap=\"0 B\"\ninitializing /usr/local/nvidia/lib64/libcuda.so.535.183.06\ndlsym: cuInit - 0x7f0fba655520\ndlsym: cuDriverGetVersion - 0x7f0fba655540\ndlsym: cuDeviceGetCount - 0x7f0fba655580\ndlsym: cuDeviceGet - 0x7f0fba655560\ndlsym: cuDeviceGetAttribute - 0x7f0fba655660\ndlsym: cuDeviceGetUuid - 0x7f0fba6555c0\ndlsym: cuDeviceGetName - 0x7f0fba6555a0\ndlsym: cuCtxCreate_v3 - 0x7f0fba65d220\ndlsym: cuMemGetInfo_v2 - 0x7f0fba6686f0\ndlsym: cuCtxDestroy - 0x7f0fba6b76f0\ncalling cuInit\ncalling cuDriverGetVersion\nraw version 0x2ef4\nCUDA driver version: 12.2\ncalling cuDeviceGetCount\ndevice count 1\ntime=2025-06-04T06:40:29.898Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-fe6b3872-6b48-3c13-7ef4-8174e69fd244 name=\"NVIDIA A100-SXM4-40GB\" overhead=\"0 B\" before.total=\"39.4 GiB\" before.free=\"37.7 GiB\" now.total=\"39.4 GiB\" now.free=\"37.7 GiB\" now.used=\"1.7 GiB\"\nreleasing cuda driver library\ntime=2025-06-04T06:40:29.940Z level=DEBUG source=sched.go:225 msg=\"loading first model\" model=/root/.ollama/models/blobs/sha256-b576e13fe5c36652c9277c2e649a4950bfb8bac04eb63417a512cc902dc032d5\ntime=2025-06-04T06:40:29.940Z level=DEBUG source=memory.go:108 msg=evaluating library=cuda gpu_count=1 available=\"[37.7 GiB]\"\ntime=2025-06-04T06:40:29.940Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.vision.block_count default=0\ntime=2025-06-04T06:40:29.941Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.attention.key_length default=128\ntime=2025-06-04T06:40:29.941Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.attention.value_length default=128\ntime=2025-06-04T06:40:29.941Z level=DEBUG source=memory.go:108 msg=evaluating library=cuda gpu_count=1 available=\"[37.7 GiB]\"\ntime=2025-06-04T06:40:29.941Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.vision.block_count default=0\ntime=2025-06-04T06:40:29.942Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.attention.key_length default=128\ntime=2025-06-04T06:40:29.942Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.attention.value_length default=128\ntime=2025-06-04T06:40:29.942Z level=INFO source=sched.go:715 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-b576e13fe5c36652c9277c2e649a4950bfb8bac04eb63417a512cc902dc032d5 gpu=GPU-fe6b3872-6b48-3c13-7ef4-8174e69fd244 parallel=1 available=40450719744 required=\"18.0 GiB\"\ntime=2025-06-04T06:40:29.942Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"83.5 GiB\" before.free=\"79.0 GiB\" before.free_swap=\"0 B\" now.total=\"83.5 GiB\" now.free=\"79.0 GiB\" now.free_swap=\"0 B\"\ninitializing /usr/local/nvidia/lib64/libcuda.so.535.183.06\ndlsym: cuInit - 0x7f0fba655520\ndlsym: cuDriverGetVersion - 0x7f0fba655540\ndlsym: cuDeviceGetCount - 0x7f0fba655580\ndlsym: cuDeviceGet - 0x7f0fba655560\ndlsym: cuDeviceGetAttribute - 0x7f0fba655660\ndlsym: cuDeviceGetUuid - 0x7f0fba6555c0\ndlsym: cuDeviceGetName - 0x7f0fba6555a0\ndlsym: cuCtxCreate_v3 - 0x7f0fba65d220\ndlsym: cuMemGetInfo_v2 - 0x7f0fba6686f0\ndlsym: cuCtxDestroy - 0x7f0fba6b76f0\ncalling cuInit\ncalling cuDriverGetVersion\nraw version 0x2ef4\nCUDA driver version: 12.2\ncalling cuDeviceGetCount\ndevice count 1\ntime=2025-06-04T06:40:30.158Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-fe6b3872-6b48-3c13-7ef4-8174e69fd244 name=\"NVIDIA A100-SXM4-40GB\" overhead=\"0 B\" before.total=\"39.4 GiB\" before.free=\"37.7 GiB\" now.total=\"39.4 GiB\" now.free=\"37.7 GiB\" now.used=\"1.7 GiB\"\nreleasing cuda driver library\nllama_model_loader: - kv 31: tokenizer.ggml.add_bos_token bool = false\nllama_model_loader: - kv 32: tokenizer.chat_template str = {%- if tools %}\\n {{- '<|im_start|>...\nllama_model_loader: - kv 33: general.quantization_version u32 = 2\nllama_model_loader: - kv 34: quantize.imatrix.file str = /models_out/Qwen2.5-Coder-14B-Instruc...\nllama_model_loader: - kv 35: quantize.imatrix.dataset str = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv 36: quantize.imatrix.entries_count i32 = 336\nllama_model_loader: - kv 37: quantize.imatrix.chunks_count i32 = 128\nllama_model_loader: - type f32: 241 tensors\nllama_model_loader: - type q6_K: 338 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type = Q6_K\nprint_info: file size = 11.29 GiB (6.56 BPW)\ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 151660 '<|fim_middle|>' is not marked as EOG\nload: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nload: control token: 151653 '<|vision_end|>' is not marked as EOG\nload: control token: 151648 '<|box_start|>' is not marked as EOG\nload: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nload: control token: 151649 '<|box_end|>' is not marked as EOG\nload: control token: 151655 '<|image_pad|>' is not marked as EOG\nload: control token: 151651 '<|quad_end|>' is not marked as EOG\nload: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nload: control token: 151652 '<|vision_start|>' is not marked as EOG\nload: control token: 151654 '<|vision_pad|>' is not marked as EOG\nload: control token: 151656 '<|video_pad|>' is not marked as EOG\nload: control token: 151644 '<|im_start|>' is not marked as EOG\nload: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nload: control token: 151650 '<|quad_start|>' is not marked as EOG\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch = qwen2\nprint_info: vocab_only = 1\nprint_info: model type = ?B\nprint_info: model params = 14.77 B\nprint_info: general.name = Qwen2.5 Coder 14B Instruct\nprint_info: vocab type = BPE\nprint_info: n_vocab = 152064\nprint_info: n_merges = 151387\nprint_info: BOS token = 151643 '<|endoftext|>'\nprint_info: EOS token = 151645 '<|im_end|>'\nprint_info: EOT token = 151645 '<|im_end|>'\nprint_info: PAD token = 151643 '<|endoftext|>'\nprint_info: LF token = 198 'Ċ'\nprint_info: FIM PRE token = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token = 151662 '<|fim_pad|>'\nprint_info: FIM REP token = 151663 '<|repo_name|>'\nprint_info: FIM SEP token = 151664 '<|file_sep|>'\nprint_info: EOG token = 151643 '<|endoftext|>'\nprint_info: EOG token = 151645 '<|im_end|>'\nprint_info: EOG token = 151662 '<|fim_pad|>'\nprint_info: EOG token = 151663 '<|repo_name|>'\nprint_info: EOG token = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-04T06:40:30.482Z level=DEBUG source=server.go:335 msg=\"adding gpu library\" path=/usr/lib/ollama/cuda_v12\ntime=2025-06-04T06:40:30.482Z level=DEBUG source=server.go:343 msg=\"adding gpu dependency paths\" paths=[/usr/lib/ollama/cuda_v12]\ntime=2025-06-04T06:40:30.482Z level=INFO source=server.go:405 msg=\"starting llama server\" cmd=\"/usr/bin/ollama runner --model /root/.ollama/models/blobs/sha256-b576e13fe5c36652c9277c2e649a4950bfb8bac04eb63417a512cc902dc032d5 --ctx-size 25000 --batch-size 512 --n-gpu-layers 49 --verbose --threads 6 --parallel 1 --port 34613\"\ntime=2025-06-04T06:40:30.483Z level=DEBUG source=server.go:423 msg=subprocess environment=\"[LD_LIBRARY_PATH=/usr/lib/ollama/cuda_v12:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib/ollama/cuda_v12:/usr/lib/ollama PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin CUDA_VISIBLE_DEVICES=GPU-fe6b3872-6b48-3c13-7ef4-8174e69fd244]\"\ntime=2025-06-04T06:40:30.483Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\ntime=2025-06-04T06:40:30.483Z level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-04T06:40:30.484Z level=WARN source=server.go:587 msg=\"client connection closed before server finished loading, aborting load\"\ntime=2025-06-04T06:40:30.484Z level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"timed out waiting for llama runner to start: context canceled\"\ntime=2025-06-04T06:40:30.484Z level=DEBUG source=sched.go:459 msg=\"triggering expiration for failed load\" model=/root/.ollama/models/blobs/sha256-b576e13fe5c36652c9277c2e649a4950bfb8bac04eb63417a512cc902dc032d5\ntime=2025-06-04T06:40:30.484Z level=DEBUG source=sched.go:361 msg=\"runner expired event received\" modelPath=/root/.ollama/models/blobs/sha256-b576e13fe5c36652c9277c2e649a4950bfb8bac04eb63417a512cc902dc032d5\ntime=2025-06-04T06:40:30.484Z level=DEBUG source=sched.go:376 msg=\"got lock to unload\" modelPath=/root/.ollama/models/blobs/sha256-b576e13fe5c36652c9277c2e649a4950bfb8bac04eb63417a512cc902dc032d5\ntime=2025-06-04T06:40:30.484Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"83.5 GiB\" before.free=\"79.0 GiB\" before.free_swap=\"0 B\" now.total=\"83.5 GiB\" now.free=\"78.8 GiB\" now.free_swap=\"0 B\"\ninitializing /usr/local/nvidia/lib64/libcuda.so.535.183.06\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n0.6.2",
    "comments": [
      {
        "user": "sherpya",
        "body": "same problem, the gpu memory gets freed, version 0.7.0 does not have the bug, I believe the first affected version is 0.7.1 "
      },
      {
        "user": "sherpya",
        "body": "> same problem, the gpu memory gets freed, version 0.7.0 does not have the bug, I believe the first affected version is 0.7.1\n\nhmm no got same problem on 0.7.0"
      },
      {
        "user": "rick-github",
        "body": "@TheNha  Sounds like #10433.  Try upgrading.\n@sherpya Since you are on 0.7.*, it's likely not #10433 and may not be the same problem.  [Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) may aid in debugging."
      }
    ]
  },
  {
    "issue_number": 11021,
    "title": "Native Text-to-Speech (TTS) model support",
    "author": "v-byte-cpu",
    "state": "closed",
    "created_at": "2025-06-08T20:59:39Z",
    "updated_at": "2025-06-10T18:29:49Z",
    "labels": [
      "feature request"
    ],
    "body": "**📣 Hello Ollama team!**\n\nI’m excited about Ollama’s vision and would love to contribute by helping add **native Text‑to‑Speech (TTS)**  support. I noticed a couple of related feature requests already:\n\n* **Issue #3265**\n* **Issue #7353**\n\nHere’s a rough plan of what I’d be glad to tackle — happy to refine this based on your needs:\n\n### Proposed TTS Integration Roadmap\n\n#### 1. **Model Loading &amp; Metadata**\n\n* Extend support for audio-generation models (e.g. Coqui TTS, Bark, Dia, Orpheus TTS)\n* Detect TTS model formats (GGUF / specialized metadata headers) and load appropriate components\n\n#### 2. **Audio output handling**\n\n* Most modern TTS models already **embed a vocoder** and produce waveform/audio directly.\n* Ollama would only need to:\n\n  * Capture the raw audio output (WAV/MP3/flac).\n  * Stream it through REST and CLI APIs.\n  * Optionally perform format conversion (e.g. to standard WAV/MP3 for clients).\n\n#### 3. API & CLI support\n\n* Implement `POST /v1/audio/speech` endpoint — consistent with OpenAI API.\n* Support **non-streaming responses** initially:\n\n  * Return `audio/mp3`, `audio/wav` or `audio/flac` as standard response.\n* Design API and CLI to be **future-compatible with streaming responses**, if supported by models.\n* CLI example:\n\n  ```text\n  ollama tts run <model> --text \"Hello\"\n  # optional future flag: --stream\n  ```\n\n#### 4. **Dependencies &amp; Packaging**\n\n* Bundle or list required audio libs (PyTorch/TensorFlow, CUDA vocoder utils)\n* Support cross-platform binary builds (Linux/macOS/Windows)\n\n#### 5. **Testing &amp; Benchmarks**\n\n* Unit tests comparing generated audio to reference samples.\n* Benchmark end-to-end latency on typical hardware.\n\n#### 6. **Docs &amp; Example Models**\n\n* Update docs with TTS usage examples\n* Add a sample TTS model to Ollama registry (e.g. Coqui or ChatTTS)\n\n\nLet me know if it makes sense to:\n\n* Expand one of the existing issues (#5424 or #7353)\n* Convert this into a detailed design proposal or spike PR\n* Assign a “good-first-task” to get the ball rolling\n* Start with a **draft PR** adding the basic `/v1/audio/speech` endpoint with minimal CLI/REST support (targeting e.g. Dia TTS model first).\n\nPlease let me know which option makes the most sense.\n\nThanks for building such a strong local inference tool — I’m excited to help it *speak*! 🎙️",
    "comments": [
      {
        "user": "jcc10",
        "body": "As I stated in one of the other issues, this would be really helpful for my use-case of having one GPU and wanting only one program handling GPU usage. (In this case Ollama)\n\nSeveral items to note/consider for future work:\n\n1. Multiple Voices: There should be a standard API variable for changing voices. There should also be a standard API for *listing* voices along with any metadata regarding the voices. (EG: this voice is designed for EN_GB, possibly also a quality rating or training sample size)\n2. Arbitrary Voice Description: Some models such as ParlerTTS allow for a text description of a voice as the source. This option should have some kind of standardised way to both indicate it is supported and to use it. (Possibly a prefix to the description such as `DESCRIPTION:A calm and relaxing male voice...`)\n3. Arbitrary Voice Cloning: Some models such as CoquiTTS support audio inputs to attempt to clone. This is probably harder to implement than the Voice Description though would potentially produce more consistent audio(?) as it would have a baseline to match instead of simply a text description. It may be a good idea to allow for a separate endpoint to upload the sample audio file to cache it then allow for generation, instead of uploading the audio file each time? Or that could be me just trying to be overly efficient.  \nIf this is implemented with a sample cache, we could save the sample as a temporary file with the filename as a MD5 and then use the text field with something like `CLONE:` followed by the MD5 of the sample.\n4. Voice Mixing: Some models such as Kokoro allow for selecting multiple voices and mixing them with ratios. I am unsure if this is worth implementing as a separate mode or if it could just be implemented similar to the suggested `DESCRIPTION` prefix, possibly a `MIX` prefix.\n5. Output formatting layer: as you stated, there should probably be a common format conversion layer if only because most models output raw WAV data which is horribly inefficient. It shouldn't be too hard to make some default FFMPEG settings for MP3 / Opus / FLAC and have them use it.\n\nThe voices settings are probably the most important parts because the OpenAI API for TTS both does not report what voices are available (It's assumed you are going to check the Docs and there is no dynamic options) but it also doesn't support arbitrary voices or voice cloning. Both of which would probably be nice to at minimum plan for how to integrate in the future, as well as how to potentially extend it in the future. Obviously my prefered option is having prefix options of voices such as `DESCRIPTION:`, `CLONE:`, `MIX:`, and a more generic `CUSTOM:`. Including the `:` to indicate it is not a voice but instead a feature prefix. This would also allow for future prefixes without having to modify the Ollama parts of the API.\nEdit: Just made a quick example regex for how the options could be formatted with some examples [here](regexr.com/8fajp)\n\nFinally, (and this may need to turn into a separate issue,) if other media formats are added on (Image / Video / Audio input for example) it may be worth it to make a more generic file cache option to allow for uploading a input one time and running it against multiple models, or against the same model multiple times, instead of uploading the full file each time it needs to be run. Though, again, this might just be me craving efficiency and a more generic cache is likely to be outside the scope of this issue, however it may be worth having the option of storing media outputs in the cache as well in some cases, and should probably allow for retrieval back from said cache."
      },
      {
        "user": "pdevine",
        "body": "Hey guys, I appreciate the comments here for TTS. We've been looking at TTS and STT but there aren't any concrete timelines as of yet. I'll go ahead and close this as a dupe.\n"
      },
      {
        "user": "v-byte-cpu",
        "body": "Hi! Thanks a lot for the reply and clarification. I totally understand closing this as a dupe.\n\nJust to add: this ticket had a bit more concrete ideas on API shape, testing, and practical implementation steps, so if useful — I’d be happy to contribute those ideas back into the original ticket or help with a design doc.\n\nAlso — I’m really interested in helping to implement this feature (TTS support) — if there’s any internal discussion or early draft direction, I’d love to contribute or collaborate.\n\nPlease let me know if there’s any way I can help move this forward!"
      }
    ]
  },
  {
    "issue_number": 10955,
    "title": "how to ask for upgrade in macOS?",
    "author": "liaoweiguo",
    "state": "closed",
    "created_at": "2025-06-03T05:34:58Z",
    "updated_at": "2025-06-10T18:23:33Z",
    "labels": [
      "feature request"
    ],
    "body": "connection perfect, but the Ollama version stay in 0.7.0, how to force a upgrade? thx",
    "comments": [
      {
        "user": "rick-github",
        "body": "https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-upgrade-ollama"
      },
      {
        "user": "pdevine",
        "body": "Going to close this as answered."
      }
    ]
  },
  {
    "issue_number": 10945,
    "title": "KV Cache Quantization breaks Gemma3",
    "author": "mlaihk",
    "state": "open",
    "created_at": "2025-06-02T03:56:27Z",
    "updated_at": "2025-06-10T18:22:49Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nWith Ollama 0.8.0 and up to at least 0.9.0, if I have set KV_CACHE_TYPE to anything but default (I have it at q8_0), Gemma3 models will perform extremely poorly in terms of tops and also response accuracy.\nSymptoms includes long model load time, difficulties in tool calling (on OpenWebUI), RAG difficulties.... etc.\n\nDeleting environment variable OLLAMA_KV_CACHE_TYPE and restart ollama will restore performance to Gemma3.\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": []
  },
  {
    "issue_number": 10963,
    "title": "Error: unknown data type: I32",
    "author": "zswodegit",
    "state": "closed",
    "created_at": "2025-06-04T00:59:28Z",
    "updated_at": "2025-06-10T18:18:14Z",
    "labels": [
      "model request"
    ],
    "body": "hi,\nIm trying to import local checkpoint Qwen2.5-7B-AWQ download from HF, but always failed. if ollama support I32 type? (ollama version: 0.9.0)\n\n![Image](https://github.com/user-attachments/assets/b356fe58-f400-4f14-9262-2c044047d04c)",
    "comments": [
      {
        "user": "rick-github",
        "body": "AWQ is not supported."
      },
      {
        "user": "pdevine",
        "body": "Going to go ahead and close this. Ollama doesn't support AWQ quantization currently."
      }
    ]
  },
  {
    "issue_number": 11010,
    "title": "`think=false` still shows <think> tags in the output",
    "author": "raffaem",
    "state": "open",
    "created_at": "2025-06-07T20:53:51Z",
    "updated_at": "2025-06-10T18:12:11Z",
    "labels": [
      "bug",
      "thinking"
    ],
    "body": "### What is the issue?\n\n```\nollama_model = \"deepseek-r1\"\nresp = chat(\n            model=ollama_model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a research assistant.\"},\n                {\"role\": \"user\", \"content\": \n                \"\"\"\n                    You are given a job description. \n                    Your task is to determine if the job requires the use of data science or not.\n                    If it does, respond with \"yes\". If it doesn't, respond \"no\".\n                    Then provide an explanation for your answer.\n                    Here is the job description:\n                    Evaluate the quality of data science education in France's high schools.\n                    \"\"\"\n                },\n            ],\n            options={\"temperature\": 0},  # Make responses more deterministic\n            think=False,\n        )\nprint(resp.message.content)\n```\n\nOutput:\n```\nThe task involves evaluating the quality of data science education in French high schools. This requires analyzing existing educational programs, curricula, and possibly comparing them with standards or benchmarks. To do this effectively, one would need to gather and process data on various aspects such as course content, student outcomes, teacher qualifications, infrastructure, etc.\n\nData science inherently involves working with data—collecting it, cleaning it, analyzing it, interpreting results, and often visualizing findings. Evaluating education programs falls under the purview of educational research or assessment, which typically uses quantitative methods (like surveys, performance metrics) and qualitative analysis (like interviews, case studies). \n\nIn this context, \"data science\" could refer to applying data-driven methodologies to assess educational quality. However, the job description does not explicitly mention using advanced statistical models, machine learning algorithms, or computational tools that are core to data science as a field. Instead, it seems more focused on traditional evaluation methods.\n\nTherefore, while there might be some overlap with data analysis skills, this task is primarily about assessment and review rather than applying full-fledged data science techniques. The job does not require building predictive models or using big data tools but focuses on evaluating the current state of education programs.\n\nAnswer: no\n\nExplanation: Although the evaluation process may involve collecting and analyzing data (which could be considered basic data handling), it does not explicitly call for advanced data science methods such as machine learning, statistical modeling at scale, or complex computational analysis. The task is more about educational assessment than applying core data science principles.\n</think>\nThe job description requires evaluating the quality of data science education in French high schools.\n\nThis involves analyzing existing educational programs and curricula related to data science. To do this effectively, one would need to gather and process quantitative and qualitative data on various aspects such as course content, student outcomes, teacher qualifications, infrastructure, etc., which falls under the domain of data science.\n\nTherefore, I determine that **yes**, this job requires the use of data science or at least involves tasks typically associated with it.\n```\n\nThe thinking ending with the `</think>` tag is still there.\n\nLooks like only the opening `<think>` tag was removed.\n\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "deepseek-r1 doesn't actually have a no-think mode, so the template tries to fake one for the model by inserting a blank think block at the end of the prompt.  It looks like the model recognizes the opening `<think>`, since it doesn't output a new one, but not the closing `</think>`, since it writes one out.\n\nPadding the think block with some text helps it recognize the trailing `</think>`.  However, the instruction following is poor: it doesn't respond 'yes' or 'no' with explanation as per the instructions.  If thinking is enabled, then it does follow the instructions (or performed adequately the few times I ran the prompt).\n\nSo I think for deepseek-r1, the only really valid options for `think` are unset or `true`."
      },
      {
        "user": "zhangiser",
        "body": "same problem"
      },
      {
        "user": "rick-github",
        "body": "Same response."
      }
    ]
  },
  {
    "issue_number": 10976,
    "title": "Thinking + tools + qwen3 = empty output",
    "author": "du291",
    "state": "open",
    "created_at": "2025-06-05T09:05:59Z",
    "updated_at": "2025-06-10T18:11:44Z",
    "labels": [
      "bug",
      "thinking"
    ],
    "body": "### What is the issue?\n\nPassing tool definitions as well as think=true via the api generates empty output. I remember this used to work in previous versions. One could argue that one might not be interested in 'thoughts' when using tool calls, but the model may output qualitatively different answers with think=true.\n\n\n### Relevant log output\n\n```shell\nPOST /api/chat HTTP/1.1\nAccept: application/json, */*;q=0.5\nAccept-Encoding: gzip, deflate, zstd\nConnection: keep-alive\nContent-Length: 257\nContent-Type: application/json\nHost: 127.0.0.1:11434\nUser-Agent: HTTPie/3.2.4\n\n{\n    \"format\": \"json\",\n    \"messages\": [\n        {\n            \"content\": \"press the button\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"qwen3:30b-a3b\",\n    \"stream\": false,\n    \"think\": true,\n    \"tools\": [\n        {\n            \"function\": {\n                \"description\": \"press the button\",\n                \"name\": \"press_button\",\n                \"parameters\": {\n                    \"type\": \"object\"\n                }\n            },\n            \"type\": \"function\"\n        }\n    ]\n}\n\n\nHTTP/1.1 200 OK\nContent-Length: 301\nContent-Type: application/json; charset=utf-8\nDate: Thu, 05 Jun 2025 09:03:21 GMT\n\n{\n    \"created_at\": \"2025-06-05T09:03:21.250431434Z\",\n    \"done\": true,\n    \"done_reason\": \"stop\",\n    \"eval_count\": 29,\n    \"eval_duration\": 1482226859,\n    \"load_duration\": 13063588,\n    \"message\": {\n        \"content\": \"\",\n        \"role\": \"assistant\"\n    },\n    \"model\": \"qwen3:30b-a3b\",\n    \"prompt_eval_count\": 131,\n    \"prompt_eval_duration\": 128174504,\n    \"total_duration\": 1624041007\n}\n\n\nWithout thinking:\n\nPOST /api/chat HTTP/1.1\nAccept: application/json, */*;q=0.5\nAccept-Encoding: gzip, deflate, zstd\nConnection: keep-alive\nContent-Length: 258\nContent-Type: application/json\nHost: 127.0.0.1:11434\nUser-Agent: HTTPie/3.2.4\n\n{\n    \"format\": \"json\",\n    \"messages\": [\n        {\n            \"content\": \"press the button\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"qwen3:30b-a3b\",\n    \"stream\": false,\n    \"think\": false,\n    \"tools\": [\n        {\n            \"function\": {\n                \"description\": \"press the button\",\n                \"name\": \"press_button\",\n                \"parameters\": {\n                    \"type\": \"object\"\n                }\n            },\n            \"type\": \"function\"\n        }\n    ]\n}\n\n\nHTTP/1.1 200 OK\nContent-Length: 370\nContent-Type: application/json; charset=utf-8\nDate: Thu, 05 Jun 2025 09:02:06 GMT\n\n{\n    \"created_at\": \"2025-06-05T09:02:06.187260157Z\",\n    \"done\": true,\n    \"done_reason\": \"stop\",\n    \"eval_count\": 13,\n    \"eval_duration\": 573353215,\n    \"load_duration\": 1688824382,\n    \"message\": {\n        \"content\": \"\",\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"function\": {\n                    \"arguments\": {},\n                    \"name\": \"press_button\"\n                }\n            }\n        ]\n    },\n    \"model\": \"qwen3:30b-a3b\",\n    \"prompt_eval_count\": 137,\n    \"prompt_eval_duration\": 1224210867,\n    \"total_duration\": 3488203248\n}\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "du291",
        "body": "Addendum: actually the api response seems to NOT work properly with think=true even without tools. I'd expect thoughts to be in 'thoughts' and 'message' to contain the answer. It appears that message contains the thoughts and answer is nowhere to be found.\n\n```\nPOST /api/chat HTTP/1.1\nAccept: application/json, */*;q=0.5\nAccept-Encoding: gzip, deflate, zstd\nConnection: keep-alive\nContent-Length: 128\nContent-Type: application/json\nHost: 127.0.0.1:11434\nUser-Agent: HTTPie/3.2.4\n\n{\n    \"format\": \"json\",\n    \"messages\": [\n        {\n            \"content\": \"press the button\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"qwen3:30b-a3b\",\n    \"stream\": false,\n    \"think\": true\n}\n\n\nHTTP/1.1 200 OK\nContent-Length: 929\nContent-Type: application/json; charset=utf-8\nDate: Thu, 05 Jun 2025 09:06:29 GMT\n\n{\n    \"created_at\": \"2025-06-05T09:06:29.494231479Z\",\n    \"done\": true,\n    \"done_reason\": \"stop\",\n    \"eval_count\": 134,\n    \"eval_duration\": 6380232883,\n    \"load_duration\": 11967477,\n    \"message\": {\n        \"content\": \"{\\\"{\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"Okay, the user just said 'press the button'. I need to respond appropriately. Let me think about the context. They might be referring to a physical button, like on a device or a website. But since I can't interact with physical objects, I should clarify. Maybe they want me to simulate pressing a button in a conversation. Alternatively, they could be testing my response. I should ask for more details to understand what they need. Let me make sure to keep the response friendly and helpful. I'll prompt them to explain what they mean by 'press the button' so I can assist better.\\\"}\",\n        \"role\": \"assistant\"\n    },\n    \"model\": \"qwen3:30b-a3b\",\n    \"prompt_eval_count\": 13,\n    \"prompt_eval_duration\": 196221601,\n    \"total_duration\": 6589302570\n}\n```\n\nI don't know how this is possible when the command line works just fine. Maybe due to streaming?\n\n```\n$ echo 'press the button'|ollama run qwen3:30b-a3b\nThinking...\nOkay, the user said \"press the button.\" I need to figure out how to respond. First, I should check if there's a specific button they're \nreferring to. Since I can't interact with physical buttons, maybe they're talking about a virtual one. But the user might just be testing me \nor trying to see if I can handle commands. I should acknowledge their request and explain that I can't press a physical button. Then, offer \nhelp with something else. Let me make sure my response is friendly and helpful. Maybe say something like, \"I can't press physical buttons, \nbut I can help you with anything else. What do you need?\" That should cover it.\n...done thinking.\n\nI can't press physical buttons, but I'm here to help you with anything else! What would you like assistance with? 😊\n```"
      },
      {
        "user": "rick-github",
        "body": "#10929"
      }
    ]
  },
  {
    "issue_number": 10929,
    "title": "Ollama produces invalid JSON when using thinking mode with structured output",
    "author": "roosephu",
    "state": "open",
    "created_at": "2025-05-31T11:19:53Z",
    "updated_at": "2025-06-10T18:11:26Z",
    "labels": [
      "bug",
      "thinking"
    ],
    "body": "### What is the issue?\n\nHere is the script to reproduce\n```sh\ncurl -X POST http://localhost:11434/api/generate \\\n  -d '{\n    \"model\": \"qwen3:0.6b\", \n    \"prompt\": \"Generate JSON with summary for: # Test Post\\nThis is about Python programming.\",\n    \"stream\": false,\n    \"think\": true,\n    \"format\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"summary\": {\"type\": \"string\"}\n      },\n      \"required\": [\"summary\"]\n    }\n  }'\n```\n\nOutput:\n```json\n{\"model\":\"qwen3:0.6b\",\"created_at\":\"2025-05-31T11:12:37.379051Z\",\"response\":\"{\\\"{\\\"summary\\\": \\\"This is a test post about Python programming. It is a demonstration of how to create and use Python scripts to solve programming problems. The post provides information on how to write and test Python code, including syntax, variables, and functions. It is for educational purposes and may be used to teach Python to students or developers. The post is not intended to be used for any specific application or purpose. The post provides a general overview of Python programming and its use in various fields. The summary is concise and includes key points for a user to understand. The post is in English and is for a general audience.\\\"}\\n\",\"done\":true,\"done_reason\":\"stop\",\"context\":[151644,872,198,31115,4718,448,12126,369,25,671,3393,3877,198,1986,374,911,13027,15473,13,608,26865,151645,198,151644,77091,198,4913,1708,788,330,1986,374,264,1273,1736,911,13027,15473,13,1084,374,264,29716,315,1246,311,1855,323,990,13027,19502,311,11625,15473,5322,13,576,1736,5707,1995,389,1246,311,3270,323,1273,13027,2038,11,2670,19482,11,7332,11,323,5746,13,1084,374,369,16229,9895,323,1231,387,1483,311,4538,13027,311,4143,476,13402,13,576,1736,374,537,10602,311,387,1483,369,894,3151,3766,476,7428,13,576,1736,5707,264,4586,23251,315,13027,15473,323,1181,990,304,5257,5043,13,576,12126,374,63594,323,5646,1376,3501,369,264,1196,311,3535,13,576,1736,374,304,6364,323,374,369,264,4586,10650,1189,532],\"total_duration\":1114459042,\"load_duration\":27570000,\"prompt_eval_count\":26,\"prompt_eval_duration\":28529667,\"eval_count\":127,\"eval_duration\":1057852250}\n```\n\nNote `{\"` occurs twice at the beginning of `response`. \n\nIf the thinking mode is turned off, `response` is a valid JSON:\n\n```shell\n\ncurl -X POST http://localhost:11434/api/generate \\\n  -d '{\n    \"model\": \"qwen3:0.6b\", \n    \"prompt\": \"Generate JSON with summary for: # Test Post\\nThis is about Python programming.\",\n    \"stream\": false,\n    \"think\": false,\n    \"format\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"summary\": {\"type\": \"string\"}\n      },\n      \"required\": [\"summary\"]\n    }\n  }'\n```\n\n```json\n{\"model\":\"qwen3:0.6b\",\"created_at\":\"2025-05-31T11:13:24.352585Z\",\"response\":\"{\\n  \\\"summary\\\": \\\"This test post is about Python programming. It includes code snippets that demonstrate basic concepts such as variables, functions, and data types.\\\"\\n}\",\"done\":true,\"done_reason\":\"stop\",\"context\":[151644,872,198,31115,4718,448,12126,369,25,671,3393,3877,198,1986,374,911,13027,15473,13,608,2152,5854,766,151645,198,151644,77091,198,151667,271,151668,271,515,220,330,1708,788,330,1986,1273,1736,374,911,13027,15473,13,1084,5646,2038,68642,429,19869,6770,18940,1741,438,7332,11,5746,11,323,821,4494,10040,92],\"total_duration\":374914458,\"load_duration\":29720833,\"prompt_eval_count\":32,\"prompt_eval_duration\":44193625,\"eval_count\":34,\"eval_duration\":300610708}\n```\n\n(It seems that when structured output is requested, `thinking` process is omitted, but that's another topic.)\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "mlaihk",
        "body": "I noticed weird performance issue with the Ollama 0.7 and newer (including 0.9.0) with Gemma3 models.  I removed OLLAMA_KV_CACHE_TYPE (originally set to q8_0) and the performance of Gemma3 is back (in terms of tops and output accuracy).\nSomething is not working right with the KV cache quantization and Gemma3 models....\n"
      }
    ]
  },
  {
    "issue_number": 10983,
    "title": "Cannot use 0.9.0 Thinking API with Cogito/Granite models",
    "author": "laniakea64",
    "state": "open",
    "created_at": "2025-06-05T14:29:05Z",
    "updated_at": "2025-06-10T18:07:04Z",
    "labels": [
      "thinking"
    ],
    "body": "The [Cogito models](https://ollama.com/library/cogito) support both reasoning and non-reasoning modes.  But when trying to use the new Ollama 0.9.0 Thinking API with `cogito:14b`, Ollama returns a HTTP/400 response.  When using Ollama as the client, it looks like this:\n```\n$ ollama run cogito:14b\n>>> /set think\nwarning: model \"cogito:14b\" does not support thinking output\nSet 'think' mode.\n>>> Hello\nerror: registry.ollama.ai/library/cogito:14b does not support thinking\n>>>\n```\n(This `ollama` binary is the same one as the server.)\n\nIf I understand correctly, support for the Thinking API requires an update to the template for the model.  Could you please update the Cogito models' templates to add support for the Thinking API?  Thanks!",
    "comments": []
  },
  {
    "issue_number": 10995,
    "title": "context length is doubled",
    "author": "nicho2",
    "state": "closed",
    "created_at": "2025-06-06T07:23:40Z",
    "updated_at": "2025-06-10T18:06:23Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n\nThe context length is doubled:\n\ni send \"num_ctx\": 45000 and in the ollama log , i see runner.num_ctx=90000\n\nis this right?\n\n\n### Relevant log output\n\n```shell\nI send :\n\nPOST /api/chat HTTP/1.1\nHost: 10.2.142.77:11434\nUser-Agent: python-requests/2.32.3\nAccept-Encoding: gzip, deflate\nAccept: */*\nConnection: keep-alive\nContent-Type: application/json\nContent-Length: 345384\n\n{\"model\": \"llama3.3:latest\", \"stream\": true, \"options\": {\"temperature\": 0.1, \"num_ctx\": 45000}, \"messages\": [{\"role\": \"system\", \"content\": \".........}, {\"role\": \"user\", \"content\": \"....\"}]}\n\n\nIn ollama log, i see:  (runner.num_ctx=90000)\n\ntime=2025-06-06T07:07:42.955Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/llama3.3:latest runner.inference=cuda runner.devices=3 runner.size=\"105.1 GiB\" runner.vram=\"105.1 GiB\" runner.parallel=2 runner.pid=5251 runner.model=/root/.ollama/models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d runner.num_ctx=90000 refCount=0\n\ntime=2025-06-06T07:07:43.074Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\n\ntime=2025-06-06T07:07:43.075Z level=DEBUG source=sched.go:615 msg=\"evaluating already loaded\" model=/root/.ollama/models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d\n\ntime=2025-06-06T07:07:43.241Z level=DEBUG source=server.go:729 msg=\"completion request\" images=0 prompt=333991 format=\"\"\n\ntime=2025-06-06T07:07:43.430Z level=DEBUG source=cache.go:104 msg=\"loading cache slot\" id=0 cache=37575 prompt=37177 used=114 remaining=37063\n```\n\n### OS\n\nDocker\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "Set `OLLAMA_NUM_PARALLEL=1` in the server environment."
      },
      {
        "user": "pdevine",
        "body": "Going to close this as answered. (Thanks @rick-github !)"
      }
    ]
  },
  {
    "issue_number": 11024,
    "title": "Thinking models",
    "author": "sensoryapphouse",
    "state": "closed",
    "created_at": "2025-06-09T07:33:14Z",
    "updated_at": "2025-06-10T18:05:35Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI've been very happily using Ollama for some time now.  Recently I have been experimenting with thinking models and have Deepseek working very well.  However, I believe that both cogito and granite3.3 now support thinking modes as well but they do not seem to work with the \"think\" flag.  FYI: I am using Ollama through Ollamasharp using the chat option.\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "#10983"
      },
      {
        "user": "pdevine",
        "body": "Going to close this as a dupe."
      }
    ]
  },
  {
    "issue_number": 11032,
    "title": "Can't Disable Think Mode of Qwen3 and DeepSeek",
    "author": "zhangiser",
    "state": "closed",
    "created_at": "2025-06-10T02:07:25Z",
    "updated_at": "2025-06-10T18:03:56Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI'm use ollama on win11,the version is 0.9.0\n```shell\nC:\\Users\\zhangiser>ollama run qwen3:30b-a3b\n>>> /set nothink\nwarning: model \"qwen3:30b-a3b\" does not support thinking output\nSet 'nothink' mode.\n>>> who are you?\n<think>\nOkay, the user asked, \"who are you?\" I need to respond clearly. First, I should introduce myself as Qwen, a\nlarge-scale language model developed by Alibaba. I should mention my main functions like answering questions,\ncreating text, coding, etc. Also, highlight my multilingual support. Keep it friendly and open for more questions.\nMake sure the response is concise and covers the key points without being too technical. Check for any errors and\nensure the tone is helpful and approachable.\n</think>\n\nHello! I'm Qwen, a large-scale language model developed by Alibaba. I can answer questions, create text, code, and\nmore. I support multiple languages, including Chinese and English. How can I assist you today? 😊\n\n```\nWhen set from CLI\n```shell\nC:\\Users\\zhangiser>ollama run qwen3:30b-a3b --think=false\n>>> who are you?\n<think>\nOkay, the user asked, \"who are you?\" I need to respond clearly. First, I should introduce myself as Qwen, a\nlarge-scale language model developed by Alibaba. I should mention my main functions like answering questions,\ncreating text, coding, etc. Also, highlight my multilingual support. Keep it friendly and open for more questions.\nMake sure the response is concise and covers the key points without being too technical. Check for any errors and\nensure the tone is helpful and approachable.\n</think>\n\nHello! I'm Qwen, a large-scale language model developed by Alibaba. I can answer questions, create text, code, and\nmore. I support multiple languages, including Chinese and English. How can I assist you today? 😊\n\n```\nI've also tried DeepSeek,but get same problem.\nAnyone knows why?\n```shell\nC:\\Users\\zhangiser>ollama list\nNAME                                    ID              SIZE      MODIFIED\nqwen3:30b-a3b                           2ee832bc15b5    18 GB     5 weeks ago\nhuihui_ai/qwen2.5-1m-abliterated:14b    3bf3d8d5e063    9.0 GB    2 months ago\nqwen2.5-coder:32b                       4bd6cbf2d094    19 GB     2 months ago\nqwen2.5-coder:14b                       3028237cc8c5    9.0 GB    4 months ago\ndeepseek-r1:32b                         38056bbcbb2d    19 GB     4 months ago\ndeepseek-r1:14b                         ea35dfe18182    9.0 GB    4 months ago\ndeepseek-coder-v2:16b                   63fb193b3a9b    8.9 GB    4 months ago\n```\nShould i re-download the model ?",
    "comments": [
      {
        "user": "rick-github",
        "body": "> Should i re-download the model ?\n\nYes.\n\n"
      },
      {
        "user": "tonysprite",
        "body": "Please choose model name with \"*chat\" suffix,\neg:deepseek-r1-7b-Chat"
      },
      {
        "user": "rick-github",
        "body": "The are no thinking deepseek models in the ollama library with a chat suffix."
      }
    ]
  },
  {
    "issue_number": 11018,
    "title": "gemma3:12b-it-qat Reports Different Memory Usage (12 GB vs. 22 GB) on Identical Model Configurations",
    "author": "luke2023",
    "state": "closed",
    "created_at": "2025-06-08T11:46:14Z",
    "updated_at": "2025-06-10T15:47:03Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nWhen running the gemma3:12b-it-qat model on two different PCs using Ollama, the memory usage reported by ollama ps differs significantly despite identical model configurations:\n\nPC 1: Reports 12 GB memory usage.\nPC 2: Reports 22 GB memory usage. Both PCs use 100% GPU, and the model configuration (architecture, parameters, quantization, etc.) appears identical.\nWhat did you expect to happen?\nI expected the memory usage to be consistent across both PCs, as the model (gemma3:12b-it-qat) and its parameters (Q4_0 quantization, 12.2B parameters, 131k context length) are the same.\n\n**What actually happened?**\nPC 2 reports a much higher memory usage (22 GB) compared to PC 1 (12 GB), which suggests a potential issue with configuration, hardware, or Ollama’s memory reporting.\n\n**Environment**\nPC2 having 5090. it used to be normal but now it has a lot higher memory usage\nPC1 having 4090\n\nOllama Version:\nPC 1: 0.9.0\nPC 2: 0.9.0\nModel: gemma3:12b-it-qat (Q4_0 quantization, 12.2B parameters, 131k context length)\nOperating System: Windows (specify version, e.g., Windows 11 Pro)\n\nCUDA Toolkit Version:\nPC 2: [ 12.8]\nPC 1: [12.4]\nSystem RAM:\nPC 2: [ 128 GB]\nPC 1: [, 64 GB]\nOther Running Models:\nPC 1: bge-m3:latest (1.7 GB)\nPC 2: None\n\n### Relevant log output\n\n```shell\nPC 1 & PC2 (ollama show gemma3:12b-it-qat):\n  Model\n    architecture        gemma3\n    parameters          12.2B\n    context length      131072\n    embedding length    3840\n    quantization        Q4_0\n\n  Capabilities\n    completion\n    vision\n\n  Parameters\n    temperature    1\n    top_k          64\n    top_p          0.95\n    stop           \"<end_of_turn>\"\n\nPC 1 (ollama ps):\nNAME                 ID              SIZE      PROCESSOR    UNTIL\ngemma3:12b-it-qat    5d4fa005e7bb    12 GB     100% GPU     4 minutes from now\nbge-m3:latest        790764642607    1.7 GB    100% GPU     2 minutes from now\n\nPC 2 (ollama ps):\nNAME                 ID              SIZE     PROCESSOR    UNTIL\ngemma3:12b-it-qat    5d4fa005e7bb    22 GB    100% GPU     59 minutes from now\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "Differences in allocated size are usually because of differences in context buffer or parallelism.  [Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will show the differences."
      },
      {
        "user": "luke2023",
        "body": "@rick-github \nThanks for the help\nthis is my server log\n`time=2025-06-10T18:36:10.445+08:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:131072 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:1h0m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\Luke\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:2 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-06-10T18:36:10.448+08:00 level=INFO source=images.go:479 msg=\"total blobs: 10\"\ntime=2025-06-10T18:36:10.448+08:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-10T18:36:10.449+08:00 level=INFO source=routes.go:1287 msg=\"Listening on 127.0.0.1:11434 (version 0.9.0)\"\ntime=2025-06-10T18:36:10.449+08:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-10T18:36:10.449+08:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-06-10T18:36:10.449+08:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=12 efficiency=0 threads=24\ntime=2025-06-10T18:36:10.606+08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-529fd035-042a-13c4-94e9-b645f0097b2e library=cuda variant=v12 compute=12.0 driver=12.9 name=\"NVIDIA GeForce RTX 5090\" total=\"31.8 GiB\" available=\"30.1 GiB\"\n`"
      },
      {
        "user": "rick-github",
        "body": "This is not enough log."
      }
    ]
  },
  {
    "issue_number": 11002,
    "title": "Add Environment Variable to Override API Parameter keep_alive Value",
    "author": "kkrick-sdsu",
    "state": "closed",
    "created_at": "2025-06-06T23:07:58Z",
    "updated_at": "2025-06-10T15:34:36Z",
    "labels": [
      "feature request"
    ],
    "body": "# Add Environment Variable to Override API Parameter keep_alive Value\n\nThis is a feature request to add a new environment variable to Ollama, \"OLLAMA_KEEP_ALIVE_OVERRIDE\".\n\nThis proposed environment variable would allow the value of the existing environment variable \"OLLAMA_KEEP_ALIVE\" to override the API parameter \"keep_alive\" value, falling back to the default value from envconfig.KeepAlive() when not set.\n\n## Explain the problem you are trying to solve, not what you are trying to do\n\nI want a way to prevent individual users from changing the keep alive duration with their API calls. When I set the environment variable OLLAMA_KEEP_ALIVE, I.E.: to the value -1 to keep models loaded 'forever', I want this setting to be honored over user API requests. The absence of such a feature allows models to be unloaded inadvertently when they are expected to be always available.\n\nI am trying to solve this problem within the context of a multi-user enviornment where multiple users have API access, both directly via API requests and via UI layers calling the API on their behalf. Some users provide the keep_alive API parameter in their Ollama API requests. Some UI tools supply the keep_alive API parameter by default using the default value provided in Ollama's API docs. \n\nI would prefer to have control over this functionality directly in Ollama as opposed to educating users and encouraging changes for separate UI providers.\n\n## Explain why the change is important\n\nProviding administrators with the ability to enforce a consistent keep alive policy across all API usage is essential for maintaining high availability of specific models in shared environments. This is particularly important in production contexts where predictability and stability are priorities.\n\n## Explain how the change will be used\n\nUsers would be allowed to supply a new environment variable \"OLLAMA_KEEP_ALIVE_OVERRIDE\". This would be defaulted to `false` to prevent breaking changes to existing deployments.\n\nWhen the proposed OLLAMA_KEEP_ALIVE_OVERRIDE is set to `true`:\n- If OLLAMA_KEEP_ALIVE is set, then the value for OLLAMA_KEEP_ALIVE overrides any keep_alive value included in an API request.\n- Else if OLLAMA_KEEP_ALIVE is not set, then the default value from envconfig.KeepAlive() would override any keep_alive value provided.\n\n## Explain how the change will be tested\n\nA test would be added to envconfig/config_test.go similar to envconfig.TestKeepAlive().\n\nA manual test procedure would look something like this:\n1. Set OLLAMA_KEEP_ALIVE to -1: \n```bash\nexport OLLAMA_KEEP_ALIVE=-1\n```\n2. Set OLLAMA_KEEP_ALIVE_OVERRIDE to true:\n```bash \nexport OLLAMA_KEEP_ALIVE_OVERRIDE=true\n```\n3. Run Ollama: \n```bash\nollama serve\n```\n4. Load a small model, I.E. gemma3:1b: \n```bash\nollama run gemma3:1b\n```\n5. Verify the model is loaded 'Forever': \n```bash\nollama ps\n```\n6. Run the following request with `keep_alive` parameter:  \n```bash  \ncurl http://localhost:11434/api/generate -d '{    \n  \"model\": \"gemma3:1b\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"keep_alive\": 0 \n}'\n```\n7. Verify the model is still loaded 'Forever': \n```bash \nollama ps\n```\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "You could have the clients use the OpenAI compatibility endpoint.  That doesn't allow setting `keep_alive`."
      },
      {
        "user": "kkrick-sdsu",
        "body": "Yeah, that is definitely an option and that will be my recommended default. \n\nWith that said though, the Ollama API has gained popularity and some users and apps prefer it to Open AI's API. With that in mind, it would be great to have an option to exercise control over `keep_alive`. In a mixed-use environment with some users/apps on the OpenAI API and others on the Ollama API, it only takes one API call with a `keep_alive` to introduce unexpected behavior."
      },
      {
        "user": "rick-github",
        "body": "The problem is that ollama `/api` endpoint is really a mix of management and functional endpoints.  If you can't trust the clients to not set `keep_alive`, then giving them an endpoint that allows them to delete models is not optimal. If the client really wants to use the ollama endpoint, then the usual way to control access is by using a proxy that can auth the various endpoints and restrict fields in the requests.  So far it's been a general policy that if there's a tool external to ollama that can perform a function, then using that tool is recommended over adding code to ollama.  Some users push back on that, understandably, as it adds components and requires management, but that's where we are at the moment."
      }
    ]
  },
  {
    "issue_number": 6814,
    "title": "Multi-user installation for Ollama on MacOS",
    "author": "davidrpugh",
    "state": "open",
    "created_at": "2024-09-15T11:46:48Z",
    "updated_at": "2025-06-10T14:15:55Z",
    "labels": [
      "feature request"
    ],
    "body": "Trying to help my university IT install Ollama in such a way that it can be distributed via the IT self-service installation process which uses [Jamf Pro](https://www.jamf.com/products/jamf-pro/).  \r\n\r\nLooks like this might already be possible. Seems that when trying to run the installer inside `/Applications` folder the installer succeeds but then upon first use by a user, the user is prompted to install the command line interface and asked to provide admin password. Any suggestions would be much appreciated!",
    "comments": [
      {
        "user": "hingstarne",
        "body": "As far as i can see, if you install it with homebrew its pretty close to what you are looking for\r\n[formula](https://github.com/Homebrew/homebrew-core/blob/f2b858bced3239d552a1bad29e5193d29aa58bf5/Formula/o/ollama.rb)\r\nAdd a user to the mac os system, install homebrew and install ollama with it.\r\nAfterwards you can start the service with `brew services start ollama`\r\nIf you need it auto start on bootime you need to manage it via the plist of launchtl\r\n"
      },
      {
        "user": "neilmartin83",
        "body": "Heya - adding my voice to this.\r\n\r\nHomebrew is really not an option for classroom computers:\r\n\r\n* Homebrew allows standard users to install other tools/software and this is considered a high risk in many environments that have managed computers, especially education\r\n* Computers in edu environments are often shared so users log in with their own separate accounts - brew doesn't accommodate that - it's designed for computers with one user and sets its ownership/permissions based on them\r\n* Users in education classroom environments are not local administrators\r\n\r\nIs there any action we can take with management tooling to perform the tasks that would normally require local admin elevation? I.e. create/copy over/link etc any files required for Ollama to function?\r\n\r\nThank you."
      },
      {
        "user": "paul-cossey",
        "body": "Hi, folks \r\n\r\nAlso adding my voice here too. \r\n\r\nIt's easy enough to create an alias for the CLI in `/usr/local/bin` I've knocked up a quick post install scrip that handles this \r\n```\r\n#!/bin/bash\r\n\r\n# Script built from: https://github.com/ollama/ollama/issues/851#issuecomment-1889808636\r\n\r\n# Install Command Line Tools\r\nif ! [ -f /usr/local/bin/ollama ]; then\r\n  /usr/bin/touch /usr/local/bin/ollama ; /bin/ln -s -f /Applications/Ollama.app/Contents/Resources/ollama /usr/local/bin/ollama\r\nfi\r\n\r\nexit\r\n```\r\n\r\nThe issue with deploying via any kind MDM (Jamf etc) or a tool like Munki is that the options to set the 1st time run is located in the _users_ home folder. As these tools all run as root it's not possible set `\"first-time-run\": true` in the `~/Library/Application Support/Ollama/config.json` file\r\n\r\nWould you consider adding the ability to set that key somewhere that is accessible to _all_ users of a computer? \r\n\r\nA couple of options \r\n- Preference domain/Config Profile\r\n- /Library/Application Support/Ollama/config.json\r\n- /Users/Shared/Ollama/config.json (my least preferred option as this folder could get cleared out periodically)\r\n\r\nAnother potential option would be to allow the install CLI windows to closed. \r\n\r\nThanks for your consideration, this would really help out folks that want to centrally deploy your software. "
      }
    ]
  },
  {
    "issue_number": 10908,
    "title": "Installing OLLAMA step by step but Not working 😢",
    "author": "RanaUniverse",
    "state": "closed",
    "created_at": "2025-05-30T02:43:51Z",
    "updated_at": "2025-06-10T12:38:45Z",
    "labels": [],
    "body": "It has been about a month i tried many things and solutions but i dont get any solution to solve my issue.\n\nMY Os:\n    Xubuntu 24.04 LTX (Ubuntu Flovour) Linux\n\nI am saying how i am trying to use this steps by steps below\n\n![Image](https://github.com/user-attachments/assets/4bd53f72-1190-4af7-82bd-b2b7f90dee34)\nFirst like this, i installed this in my LInux\n\n![Image](https://github.com/user-attachments/assets/39a4c127-c8f2-498c-8c9d-2f2fe01ed012)\nI found a model just for checking\n\n![Image](https://github.com/user-attachments/assets/1d76976a-2511-4d41-b538-2c0f7cb99a6e)\nThen i run this to use this model\n\n![Image](https://github.com/user-attachments/assets/8b3b5043-0119-41ff-9dc3-d01393625bef)\nThis is the final steps i am facing problem from a month 😢 \n\nI dont able to solve this till now, anyone has idea how i can use this any of the ollama model effectively.\n\n\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "Your problem is not directly with ollama, it's that your machine is unable to lookup the address of the server that is hosting the model you are trying to download.  This has been a problem for a while (#8605).  The usual way to fix this is to use a different DNS server.   A quick way to work around it is by telling your machine what IP address to use:\n```\necho 162.159.141.50 dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com | sudo tee -a /etc/hosts\n```"
      }
    ]
  },
  {
    "issue_number": 10900,
    "title": "Error: POST predict: Post \"http://127.0.0.1:56330/completion\": EOF of deepseek-r1-8b-qwen3",
    "author": "TatsuhiroC",
    "state": "closed",
    "created_at": "2025-05-29T16:06:02Z",
    "updated_at": "2025-06-10T12:38:25Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI have no idea why, but i meet the Error: POST predict: Post \"http://127.0.0.1:56330/completion\": EOF, when i try to use deepseek-r1-8b-qwen3 on Ollama version is 0.9.0-rc0, which should support deepseek-r1-8b-qwen3.\n\n我不知道为什么，但我在Ollama版本0.9.0-rc0上尝试使用deepseek-r1-8b-qwen3遇到了错误：POST predict：Post \"http://127.0.0.1:56330/completion\" EOF，这应该支持deepseek-r1-8b-qwen3。\n\nSystem: macOS Sequoia 15.5\nOllama vsrsion: 0.9.0-rc0\nmodel: [deepseek-r1-8b-qwen3](deepseek-r1:8b-0528-qwen3-q8_0)\n\n### Relevant log output\n\n```shell\nollama run deepseek-r1:8b-0528-qwen3-q8_0\n>>> hello\nError: POST predict: Post \"http://127.0.0.1:56330/completion\": EOF\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) will aid in debugging."
      },
      {
        "user": "TatsuhiroC",
        "body": "[server.log](https://github.com/user-attachments/files/20512749/server.log)\n@rick-github \nits too long and I cant read this, so........i just paste it all here."
      },
      {
        "user": "rick-github",
        "body": "```\n[GIN] 2025/05/30 - 07:51:46 | 200 |  5.446009875s |       127.0.0.1 | POST     \"/api/generate\"\nggml_metal_graph_compute: command buffer 0 failed with status 5\nerror: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\ngraph_compute: ggml_backend_sched_graph_compute_async failed with error -1\n```\nThe runner ran out of memory.\n```\ntime=2025-05-30T07:51:41.359+08:00 level=INFO source=server.go:168 msg=offload library=metal layers.requested=-1\n layers.model=37 layers.offload=37 layers.split=\"\" memory.available=\"[10.7 GiB]\" memory.gpu_overhead=\"0 B\"\n memory.required.full=\"10.2 GiB\" memory.required.partial=\"10.2 GiB\" memory.required.kv=\"1.1 GiB\"\n memory.required.allocations=\"[10.2 GiB]\" memory.weights.total=\"7.6 GiB\" memory.weights.repeating=\"7.0 GiB\"\n memory.weights.nonrepeating=\"630.6 MiB\" memory.graph.full=\"768.0 MiB\" memory.graph.partial=\"768.0 MiB\"\nllama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 10922 MiB free\n```\nDuring model loading, ollama estimated that it needed 10.2G of the available 10.7G to load the model.  Since it OOM'ed, the estimation was incorrect.  You can find ways to mitigate this [here](https://github.com/ollama/ollama/issues/8597#issuecomment-2614533288). "
      }
    ]
  },
  {
    "issue_number": 10879,
    "title": "codestral doesn't allow tool calling",
    "author": "nickkaltner",
    "state": "closed",
    "created_at": "2025-05-27T11:36:11Z",
    "updated_at": "2025-06-10T12:37:45Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nwhen i use codestral, the chat endpoint works fine with model and messages parameters.  When i add the tools parameter, it 400s.\nI'm using the elixir ollama library, and it gives\n%Ollama.HTTPError{status: 400, message: \"Bad Request\"}\n\nhere is the example script\n\n```elixir\nmodel = \"codestral:22b-v0.1-q2_K\"\n\nstock_price_tool = %{\n  type: \"function\",\n  function: %{\n    name: \"get_stock_price\",\n    description: \"Fetches the live stock price for the given ticker.\",\n    parameters: %{\n      type: \"object\",\n      properties: %{\n        ticker: %{\n          type: \"string\",\n          description: \"The ticker symbol of a specific stock.\"\n        }\n      },\n      required: [\"ticker\"]\n    }\n  }\n}\n\nmessages = [\n  %{role: \"system\", content: \"You are a helpful assistant.\"},\n  %{role: \"user\", content: \"what is apple's latest stock price?\"}\n]\n\n{:ok, resp} = Ollama.chat(client,\n  model: model,\n  messages: messages,\n  tools: [stock_price_tool],\n  keep_alive: 10000\n)\n\nIO.puts inspect(resp)\n\nIO.puts resp[\"message\"][\"content\"]\n\nresp\n```\n\nIf I comment out the tools: [stock_price_tool] it works without the tool use\n\n### Relevant log output\n\n```shell\n[GIN] 2025/05/27 - 21:19:14 | 400 |   20.177917ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/27 - 21:19:29 | 400 |   18.747583ms |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/05/27 - 21:26:01 | 400 |   18.512958ms |       127.0.0.1 | POST     \"/api/chat\"\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n0.7.1",
    "comments": [
      {
        "user": "arturo-air",
        "body": "If you want to make `tools` calls, you should use models [marked as tools](https://ollama.com/search?c=tools). You probably want to try [devstral](https://ollama.com/library/devstral), it is also from Mistral and it allows `tools`."
      },
      {
        "user": "nickkaltner",
        "body": "I guess that's my confusion, why it's not marked as a tool.  From the codestral page;\n\n\"Code generation is one of the most popular LLM use-cases, so we are really excited about the Codestral release. From our initial testing, it's a great option for code generation workflows because it's fast, has favorable context window, and the instruct version supports tool use. We tested with LangGraph for self-corrective code generation using the instruct Codestral tool use for output, and it worked really well out-of-the-box (see our [video detailing this](https://youtu.be/zXFxmI9f06M)).\"\n\n-- Harrison Chase, CEO and co-founder of LangChain\n\nhttps://mistral.ai/news/codestral\n"
      },
      {
        "user": "nickkaltner",
        "body": "(thanks for the devstral tip)"
      }
    ]
  },
  {
    "issue_number": 10811,
    "title": "decode: cannot decode batches with this context (use llama_encode() instead)",
    "author": "Mihai-CMM",
    "state": "closed",
    "created_at": "2025-05-22T08:13:43Z",
    "updated_at": "2025-06-10T10:11:26Z",
    "labels": [],
    "body": "Hi \nCan you please provide some guidance on how i can fix this?\n\n[GIN] 2025/05/22 - 08:11:35 | 200 |   25.004956ms |   192.168.67.41 | POST     \"/api/embed\"\ndecode: cannot decode batches with this context (use llama_encode() instead)",
    "comments": [
      {
        "user": "rick-github",
        "body": "Model?  Input?  Logs?"
      },
      {
        "user": "Mihai-CMM",
        "body": "Hello thanks for looking at it. Just like the previous mentioning to the other ticket but i think the problem is more general. In my case i've used different embeding models all returning the same message though to be honest the data is in Qdrant. In the logs I don't see anything else except that message repeating forver after /embeded call. Also using the latest docker images with the same result. If it make any difference i have the ollama plus openwebui installed with the openwebui helm chart. Even if i set OLLAMA_DEBUG True i don't see any other relevant logs. So surely this is reproductible except if i didn't do something wrong with my config hence my initial question.\nThank you"
      },
      {
        "user": "tsly123",
        "body": "@Mihai-CMM \nHave you tried using embeding models? For my case, I use:\n```\nfrom sentence_transformers import SentenceTransformer\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\") \n```\nand the response results are much better compared to when using \n`ollama.embeddings(model=\"nomic-embed-text:latest\", prompt=chunk)[\"embedding\"]`"
      }
    ]
  },
  {
    "issue_number": 10878,
    "title": "Ollama 0.7.1 shows a low performance than previous versions on Qwen 3 MoE model (30b-a3b)",
    "author": "krpr",
    "state": "open",
    "created_at": "2025-05-27T08:32:00Z",
    "updated_at": "2025-06-10T09:38:30Z",
    "labels": [
      "bug",
      "needs more info"
    ],
    "body": "### What is the issue?\n\n![Image](https://github.com/user-attachments/assets/5958b95a-751e-4686-ad67-8ec842591e38)\nUsage on version 0.7.1\n![Image](https://github.com/user-attachments/assets/a1fc74dc-0247-4aca-a851-0451dacfdf86)\nUsage on version 0.6.8 or 0.7.0 (sorry for my forgetting)\n\nWith the same hardware, Ollama 0.7.1 shows a low performance than 0.6.8 or 0.7.0 on Qwen 3 MoE model (30b-a3b)\n\nOS: Ubuntu 24.04.2 LTS (GNU/Linux 6.8.0-60-generic x86_64)\nCPU: AMD EPYC 7Y43 48-Core Processor x2\nGPU: NVIDIA GeForce RTX 4090 48G x2\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.7.1",
    "comments": [
      {
        "user": "krpr",
        "body": "![Image](https://github.com/user-attachments/assets/d9ac67af-0fc0-4588-86c4-4de107160b1e)\neven sometime the response token/s only 60% of the peak"
      },
      {
        "user": "frederikhendrix",
        "body": "Yes I have this as well. Also, when will ollama start running audio/transcription models?"
      },
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) may help in debugging."
      }
    ]
  },
  {
    "issue_number": 10913,
    "title": "image to base64 conversion too slow on Mac Mini M4 Pro?",
    "author": "eUGENE-BNX",
    "state": "open",
    "created_at": "2025-05-30T08:58:34Z",
    "updated_at": "2025-06-10T09:31:52Z",
    "labels": [],
    "body": "A webp image with 720x416 resolution and a file size of 26 KB. When I want to analyze this image with Gemma3:12b model, it seems to wait 10-12 seconds just for this conversion.\n\nActually model is quite fast 23.5tokens/s. If I continue the analysis with the same image, the total time is shortened by 10-12 seconds. But if I want it to analyze with a new image, I wait 10-15 seconds even though it is a small image. Do you think it is normal for this process to be this slow?",
    "comments": [
      {
        "user": "SpiderMan95",
        "body": "I also encountered the same problem."
      },
      {
        "user": "eUGENE-BNX",
        "body": "I'm sure this cycle can be done much faster. Is time being wasted elsewhere?"
      }
    ]
  },
  {
    "issue_number": 758,
    "title": "colab Nvidia driver not detected",
    "author": "wifiuk",
    "state": "closed",
    "created_at": "2023-10-11T18:40:40Z",
    "updated_at": "2025-06-10T07:53:33Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "body": "I am testing using ollama in a collab, and its not using the GPU at all \r\n\r\nand we can see that the GPU is there.\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   62C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nHow can we make ollama force use it, as it clearly isn't using it at all.",
    "comments": [
      {
        "user": "mchiang0610",
        "body": "@wifiuk how are you running this in the colab? Still learning colab on my end, so your help on providing more info will be greatly appreciated "
      },
      {
        "user": "wifiuk",
        "body": "![image](https://github.com/jmorganca/ollama/assets/3785545/b046a2e0-17fc-4708-9247-cb16c6c29785)\r\n\r\n```\r\nimport os\r\nimport threading\r\nfrom pyngrok import ngrok\r\nimport subprocess\r\nimport time\r\n\r\ndef ollama():\r\n    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\r\n    os.environ['OLLAMA_ORIGINS'] = '*'\r\n    subprocess.Popen([\"ollama\", \"serve\"])\r\n\r\ndef ngrok_tunnel():\r\n    # Wait for some time to ensure ollama is fully started\r\n    time.sleep(10)\r\n    port = \"11434\"\r\n    public_url = ngrok.connect(port).public_url\r\n    print(f\" * ngrok tunnel {public_url} -> http://127.0.0.1:{port}\")\r\n\r\ndef monitor_gpu():\r\n    while True:\r\n        print(subprocess.check_output([\"nvidia-smi\"]).decode(\"utf-8\"))\r\n        time.sleep(10)  # adjust the sleep time to your preference\r\n\r\n# Create threads to run ollama, ngrok_tunnel, and monitor_gpu functions in the background\r\nollama_thread = threading.Thread(target=ollama)\r\nngrok_thread = threading.Thread(target=ngrok_tunnel)\r\ngpu_monitor_thread = threading.Thread(target=monitor_gpu)\r\n\r\n# Start the threads\r\nollama_thread.start()\r\nngrok_thread.start()\r\ngpu_monitor_thread.start()\r\n\r\n# Optional: To keep the Colab cell running, preventing the threads from exiting\r\nwhile True:\r\n    pass\r\n\r\n```\r\n\r\n![image](https://github.com/jmorganca/ollama/assets/3785545/59996f2c-0cbb-4769-92b8-9f528eac5c0e)\r\n\r\n![image](https://github.com/jmorganca/ollama/assets/3785545/6a0aef7d-c9c3-42c7-8f02-688913709e07)\r\n\r\n![image](https://github.com/jmorganca/ollama/assets/3785545/5e48fec7-370c-453f-97d3-2d7aa4819437)\r\n"
      },
      {
        "user": "wifiuk",
        "body": "just need to keep pulling the model each time it resets, might add that into the script later, but for now need to get the GPU working or its useless. and there isn't much documentation on it"
      }
    ]
  },
  {
    "issue_number": 6978,
    "title": "rerank model ",
    "author": "HARISHSENTHIL",
    "state": "closed",
    "created_at": "2024-09-26T10:57:42Z",
    "updated_at": "2025-06-10T07:04:33Z",
    "labels": [
      "model request"
    ],
    "body": "how can i add HF -  BAAI/bge-reranker-v2-m3  rerank model to ollama  \r\nwhile trying this approach i am getting architecture error \r\ncan anyone help to resolve this issue\r\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "https://github.com/ollama/ollama/issues/3368"
      },
      {
        "user": "ardyli",
        "body": "同样希望ollama能提供rerank模型的支持、能提供语音识别模型、语音转文字模型的支持。\r\n\r\nWe also hope that ollama can provide support for rerank models, speech recognition models, and speech-to-text models."
      },
      {
        "user": "rick-github",
        "body": "#7219"
      }
    ]
  },
  {
    "issue_number": 10986,
    "title": "Ollama 0.9.0, MacOS, gemma3:latest, and vision: Metal acceleration internal error produces inconsistent results",
    "author": "stannenb",
    "state": "open",
    "created_at": "2025-06-05T17:14:38Z",
    "updated_at": "2025-06-10T07:04:08Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nWhen using Ollama 0.9.0 on a Mac Studio M2 Max to run gemma3:latest to describe in image, server logs show an internal error, but Ollama continues processing, producing bogus results. \n\n```\n[GIN] 2025/06/05 - 13:03:33 | 200 |  2.125130792s |       127.0.0.1 | POST     \"/api/generate\"\nggml_metal_graph_compute: command buffer 1 failed with status 5\nerror: Internal Error (0000000e:Internal Error)\n```\n\nIf one immediately issues a \"/set parameter num_gpu 0\" command, Ollama processes in the image, producing a valid result. \n\n```\n❯ ollama run gemma3:latest\n>>> describe this image. /Users/xxxx/Downloads/IMG_2001@0.5x.png\nAdded image '/Users/xxxx/Downloads/IMG_2001@0.5x.png'\nThis is a screenshot of a text message that is saying \"This is a screenshot of a text message that is saying \"This is a screenshot of\na text message that is saying \".\n\nThe message is a self-referential joke!  It's a way of saying something similar.\n\n>>> /set parameter num_gpu 0\nSet parameter 'num_gpu' to '0'\n>>> describe this image. /Users/xxxx/Downloads/IMG_2001@0.5x.png\nAdded image '/Users/xxxx/Downloads/IMG_2001@0.5x.png'\nOkay, here’s a description of the image:\n\nThe image is a close-up portrait of a middle-aged man. He has a pale, somewhat weathered complexion. His most striking features are\nhis thick, full, and white, slightly unkempt beard and mustache. He is wearing dark, rectangular, aviator-style glasses. He’s looking\ndirectly at the camera with a serious, perhaps slightly skeptical, expression. The background is a blurry, out-of-focus wall,\nsuggesting the photo was taken indoors. He is wearing a dark, likely gray or black, shirt. The lighting is fairly neutral.\n\n```\n\n\n### Relevant log output\n\n```shell\nggml_metal_init: allocating\nggml_metal_init: picking default device: Apple M2 Max\nggml_metal_load_library: using embedded metal library\nggml_metal_init: GPU name:   Apple M2 Max\nggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\nggml_metal_init: simdgroup reduction   = true\nggml_metal_init: simdgroup matrix mul. = true\nggml_metal_init: has residency sets    = true\nggml_metal_init: has bfloat            = true\nggml_metal_init: use bfloat            = false\nggml_metal_init: hasUnifiedMemory      = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\nggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\nggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\nggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\nggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\ntime=2025-06-05T13:03:32.861-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=Metal buffer_type=Metal size=\"1.1 GiB\"\ntime=2025-06-05T13:03:32.861-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=BLAS buffer_type=CPU size=\"0 B\"\ntime=2025-06-05T13:03:32.861-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"0 B\"\ntime=2025-06-05T13:03:32.893-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=Metal buffer_type=Metal size=\"1.1 GiB\"\ntime=2025-06-05T13:03:32.893-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=BLAS buffer_type=CPU size=\"5.0 MiB\"\ntime=2025-06-05T13:03:32.893-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"0 B\"\ntime=2025-06-05T13:03:33.844-04:00 level=INFO source=server.go:630 msg=\"llama runner started in 2.01 seconds\"\n[GIN] 2025/06/05 - 13:03:33 | 200 |  2.125130792s |       127.0.0.1 | POST     \"/api/generate\"\nggml_metal_graph_compute: command buffer 1 failed with status 5\nerror: Internal Error (0000000e:Internal Error)\n[GIN] 2025/06/05 - 13:03:40 | 200 |  2.147018541s |       127.0.0.1 | POST     \"/api/chat\"\ntime=2025-06-05T13:06:10.840-04:00 level=ERROR source=server.go:457 msg=\"llama runner terminated\" error=\"signal: killed\"\ntime=2025-06-05T13:06:10.927-04:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"64.0 GiB\" free=\"29.8 GiB\" free_swap=\"0 B\"\ntime=2025-06-05T13:06:10.928-04:00 level=INFO source=server.go:168 msg=offload library=cpu layers.requested=0 layers.model=35 layers.offload=0 layers.split=\"\" memory.available=\"[29.8 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"4.4 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"225.0 MiB\" memory.required.allocations=\"[62.8 MiB]\" memory.weights.total=\"2.3 GiB\" memory.weights.repeating=\"1.8 GiB\" memory.weights.nonrepeating=\"525.0 MiB\" memory.graph.full=\"517.0 MiB\" memory.graph.partial=\"1.0 GiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\ntime=2025-06-05T13:06:10.928-04:00 level=WARN source=server.go:199 msg=\"flash attention enabled but not supported by gpu\"\ntime=2025-06-05T13:06:10.928-04:00 level=WARN source=server.go:222 msg=\"quantized kv cache requested but flash attention disabled\" type=q8_0\ntime=2025-06-05T13:06:10.970-04:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/opt/homebrew/Cellar/ollama/0.9.0/bin/ollama runner --ollama-engine --model /Users/saul/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 0 --threads 8 --no-mmap --parallel 2 --port 50865\"\ntime=2025-06-05T13:06:10.972-04:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-05T13:06:10.972-04:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-05T13:06:10.972-04:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-06-05T13:06:10.995-04:00 level=INFO source=runner.go:925 msg=\"starting ollama engine\"\ntime=2025-06-05T13:06:10.995-04:00 level=INFO source=runner.go:983 msg=\"Server listening on 127.0.0.1:50865\"\ntime=2025-06-05T13:06:11.034-04:00 level=INFO source=ggml.go:92 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=883 num_key_values=36\ntime=2025-06-05T13:06:11.034-04:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)\ntime=2025-06-05T13:06:11.051-04:00 level=INFO source=ggml.go:351 msg=\"model weights\" buffer=CPU size=\"3.6 GiB\"\nggml_metal_init: allocating\nggml_metal_init: picking default device: Apple M2 Max\nggml_metal_load_library: using embedded metal library\nggml_metal_init: GPU name:   Apple M2 Max\nggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\nggml_metal_init: simdgroup reduction   = true\nggml_metal_init: simdgroup matrix mul. = true\nggml_metal_init: has residency sets    = true\nggml_metal_init: has bfloat            = true\nggml_metal_init: use bfloat            = false\nggml_metal_init: hasUnifiedMemory      = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\nggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\nggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\nggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\nggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\ntime=2025-06-05T13:06:11.181-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=Metal buffer_type=Metal size=\"0 B\"\ntime=2025-06-05T13:06:11.181-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=BLAS buffer_type=CPU size=\"1.1 GiB\"\ntime=2025-06-05T13:06:11.181-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"0 B\"\ntime=2025-06-05T13:06:11.215-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=Metal buffer_type=Metal size=\"0 B\"\ntime=2025-06-05T13:06:11.215-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=BLAS buffer_type=CPU size=\"1.1 GiB\"\ntime=2025-06-05T13:06:11.215-04:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"0 B\"\ntime=2025-06-05T13:06:11.224-04:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\ntime=2025-06-05T13:06:11.977-04:00 level=INFO source=server.go:630 msg=\"llama runner started in 1.01 seconds\"\n[GIN] 2025/06/05 - 13:06:44 | 200 | 34.082673125s |       127.0.0.1 | POST     \"/api/chat\"\n```\n\n### OS\n\nMacOS\n\n### GPU\n\nM2 Max\n\n### CPU\n\nM2 Max\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "cwallen",
        "body": "I'm seeing similar issues: \n```\nggml_metal_graph_compute: command buffer 1 failed with status 5\nerror: Internal Error (0000000e:Internal Error)\npanic: failed to sample token: sample: logits sum to NaN, check model output\n\ngoroutine 11 [running]:\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).run(0x1400056b560, {0x10152ed50, 0x14000530640})\n\t/Users/runner/work/ollama/ollama/runner/ollamarunner/runner.go:364 +0x70\ncreated by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1\n\t/Users/runner/work/ollama/ollama/runner/ollamarunner/runner.go:960 +0x898\ntime=2025-06-06T12:20:21.615-04:00 level=ERROR source=server.go:457 msg=\"llama runner terminated\" error=\"exit status 2\"\n[GIN] 2025/06/06 - 12:20:21 | 500 | 45.959320292s |       127.0.0.1 | POST     \"/api/generate\"\n```\nAlso get the first 2 lines on their own, sometimes with buffer 0 instead of 1.\nSeeing the same errors on qwen2.5vl as well as gemma3\nI'm also on Apple M2 Max"
      },
      {
        "user": "yarmoliq",
        "body": "I also can't get anything from any vision model. It hallucinates random stuff. I'm also on M2 Max, ollama v0.9.0"
      },
      {
        "user": "cwallen",
        "body": "Bit of non-scientific experimentation this weekend:\nThe errors in the log that I was seeing for qwen2.5vl:7b ~go away entirely with qwen2.5vl:7b-fp16~ (thought they had but actually still seeing them.)\ngemma3:4b-it-fp16 might have a lower error rate than gemma3:4b, but hard to tell, definitely not zero.\n\n@yarmoliq Are you using CLI, API or something else?\nIf you are getting garbage for every image on every model, sounds like that might be a higher level problem than what I'm seeing. Even the error prone models for me work most of the time. \nWhen I was first playing around with scripting against the API, I was getting garbage on everything, problem was my base64 encoded jpgs were getting read as pngs. Garbage in, garbage out."
      }
    ]
  },
  {
    "issue_number": 9444,
    "title": "Error: listen tcp 127.0.0.1:11434: bind: An attempt was made to access a socket in a way forbidden by its access permissions.",
    "author": "DevelopSim",
    "state": "closed",
    "created_at": "2025-03-01T15:49:16Z",
    "updated_at": "2025-06-10T02:51:44Z",
    "labels": [],
    "body": "OS: Win10 \n\n>`ollama serve` \nError: listen tcp 127.0.0.1:11434: bind: An attempt was made to access a socket in a way forbidden by its access permissions.\n\nWhat I tried out:\n1.\n\"ollama run llama2 --verbos\"\nworks fine after some seconds.\n\n2.\nollama run llama2 --verbose\n>>> hello\nHello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n\ntotal duration:       43.523033856s\nload duration:        29.490347562s\nprompt eval count:    21 token(s)\nprompt eval duration: 11.34s\nprompt eval rate:     1.85 tokens/s\neval count:           26 token(s)\neval duration:        2.689s\neval rate:            9.67 tokens/s\n\n3.\nalready looked in \nhttps://github.com/ollama/ollama/issues/2560#issuecomment-1950690705 \nhttps://github.com/ollama/ollama/issues/2627\nand\nhttps://github.com/ollama/ollama/issues/2627#issuecomment-2325218799\nbut did not solve my issue.\n\nDo you have a hint?\n\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "https://github.com/ollama/ollama/issues/7524#issuecomment-2586006961"
      },
      {
        "user": "flywiththetide",
        "body": "It looks like this issue might be caused by Windows reserving certain ports, including port 11434, for dynamic allocation. Here are a few solutions that have worked for similar cases:\n\n### **Solution 1: Exclude Port 11434 from Windows' Reserved Ports**\nRun the following commands in an **Administrator Command Prompt**:\n```\nnet stop winnat\nnetsh int ipv4 add excludedportrange protocol=tcp startport=11434 numberofports=1\nnet start winnat\n```\n- This stops the **Windows NAT service**, excludes **port 11434** from the reserved range, and restarts NAT.\n\n### **Solution 2: Use a Different Port for Ollama**\nSet the `OLLAMA_HOST` environment variable to run Ollama on a different port:\n```\nset OLLAMA_HOST=127.0.0.1:5005\nollama serve\n```\n- This will start Ollama on **port 5005** instead of **11434**.\n\n### **Solution 3: Check if Another Process is Using Port 11434**\nRun this command:\n```\nnetstat -ano | findstr :11434\n```\n- If a process is already using port **11434**, you can stop it with:\n```\ntaskkill /PID <process_id> /F\n```\n(Replace `<process_id>` with the actual number from the previous command.)\n\nLet us know if any of these solutions work for you!"
      },
      {
        "user": "DevelopSim",
        "body": "thanks a lot @flywiththetide \nhad Docker container running parallely. Stop the container -> now it als works.\n\n\n"
      }
    ]
  },
  {
    "issue_number": 10924,
    "title": "Discord invites don't work",
    "author": "yurivict",
    "state": "open",
    "created_at": "2025-05-31T02:24:43Z",
    "updated_at": "2025-06-09T22:03:09Z",
    "labels": [],
    "body": "![Image](https://github.com/user-attachments/assets/160bd987-2978-456f-81f1-246bdaa08b7f)",
    "comments": [
      {
        "user": "Manon-56",
        "body": "Same here"
      },
      {
        "user": "dengyunsheng250",
        "body": "meet this problem also "
      },
      {
        "user": "steelkorbin",
        "body": "Yep, the discord invite is rubbish."
      }
    ]
  },
  {
    "issue_number": 10430,
    "title": "Adding support for amd new GPUS 9070 and 9070 XT",
    "author": "doomaholic",
    "state": "open",
    "created_at": "2025-04-27T21:34:03Z",
    "updated_at": "2025-06-09T21:34:41Z",
    "labels": [
      "feature request"
    ],
    "body": "Would like to add support for AMD new GPUS 9070 and 9070 XT, as they are both unsupported / unrecognized as  gfx1201 ",
    "comments": [
      {
        "user": "koh43",
        "body": "I was able to run ollama with my 9070xt. Do you have the latest ROCm 6.4.0 installed?"
      },
      {
        "user": "doomaholic",
        "body": "Im using 9070 non XT, and yes everything is latest edition. just to note AMD official drivers don't have ROCm enabled yet on 9070xt and 9070, The forked ollama for Amd have enabled Vulcan ROCm and enabled gfx1201 in their latest release, but i would rather it be implemented here on Main ollama  "
      },
      {
        "user": "koh43",
        "body": "I had no issues using the main ollama, and it recognized my GPU during installation. Perhaps the forked version isn’t updated for the 9070?"
      }
    ]
  },
  {
    "issue_number": 11029,
    "title": "Feature Request: Cartridges",
    "author": "mdmtest352020",
    "state": "open",
    "created_at": "2025-06-09T20:12:05Z",
    "updated_at": "2025-06-09T20:12:05Z",
    "labels": [
      "feature request"
    ],
    "body": "Hi, I came across some research published recently and I'm curious if the maintainers/contributors of Ollama might find it aligns to their product roadmap.\n\nhttps://arxiv.org/pdf/2506.06266\nhttps://github.com/HazyResearch/cartridges\n\nTy for reading my feedback.",
    "comments": []
  },
  {
    "issue_number": 11027,
    "title": "Support for Large Action Models?",
    "author": "lasseedfast",
    "state": "open",
    "created_at": "2025-06-09T16:35:54Z",
    "updated_at": "2025-06-09T19:01:56Z",
    "labels": [
      "feature request"
    ],
    "body": "aAre there any plans for supporting LAMs (Large Action Models)? Would be useful in agentic applications. ",
    "comments": [
      {
        "user": "rick-github",
        "body": "What's your definition of a LAM?  Is there a specific LAM you would like to be supported?"
      }
    ]
  },
  {
    "issue_number": 10767,
    "title": "Token repetition issue with Qwen2.5-VL",
    "author": "woojh3690",
    "state": "open",
    "created_at": "2025-05-19T01:34:41Z",
    "updated_at": "2025-06-09T18:49:18Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nIt seems that the Ollama version of the Qwen2.5-VL model has an issue with repeating tokens. At first, I thought it was a performance issue with the model itself, but its behavior is drastically different from the demo on [this Hugging Face page](https://huggingface.co/spaces/Qwen/Qwen2.5-VL-32B-Instruct).\n\nSample Image:\n![Image](https://github.com/user-attachments/assets/742909c3-1282-4f9a-8854-442438466d4d)\n\nDemo Page:\n![Image](https://github.com/user-attachments/assets/3409508e-c4fc-4538-8432-1e388bcbe046)\n\nOllama Code:\n```\nimport ollama\n\nclient = ollama.Client()\nmodel = \"qwen2.5vl:32b-fp16\"\n\nmessages = [\n    {\n        'role': 'user', \n        'content': 'Extract all the text from image', \n        'images': ['ja ocr sample.png']\n    }\n]\n\nresp = client.chat(\n    model=model,\n    messages=messages,\n    options={\n        \"temperature\": 0\n    }\n)\n\nprint(resp[\"message\"][\"content\"])\n```\nOllama Code Ouput:\n```\n大丈夫なのですか…？\n大丈夫なのですよねよね…？\n```\nrepeat \"よね\"\n\nAdditional information:\n1. While this could be due to differences in parameters like temperature, the repetition persists no matter how high I set the repeat_penalty in Ollama — it appears to have no effect at all.\n2. This issue doesn't seem limited to just the example image — it appears to occur with most images when attempting OCR. Naturally, this behavior was not observed on the demo page.\n3. This issue was also observed with both qwen2.5vl:7b and qwen2.5vl:72b.\n\n\n### OS\nUbuntu 24.04.2\n\n### GPU\nNvidia A100\nDriver Version: 535.230.02\n\n### CPU\nIntel(R) Xeon(R) Gold 6130\n\n### Ollama version\nv0.7.0, Offical Docker image",
    "comments": [
      {
        "user": "woojh3690",
        "body": "Found another English case with repeated lines.\n\nSample Image:\n![Image](https://github.com/user-attachments/assets/f6f78886-a4be-4f5a-9825-7afab7464e84)\n\nOllama Output (qwen2.5vl:32b-fp16):\n```\nDream Arts Arts\nL 2-047, LEVEL 2, VIVACITY MEGAMALL,\nJALAN WAN ALWI, 93350 KUCHING\nJALAN WAN WAN ALWI, 93350 KUCHINGING\n\nGST Reg.: 000168861696\n\nDocument No.: T02035040\nDate: 29/03/2018 07:35:38 PM\nTerminal: T02\nCashier: CASHIER01\nCashier: CASHIER01\n\nDESC                 U. PRICE    Disc    AMOUNT\nQTY                  RM          Disc    RM\n--------------------------------------------------------------------------------\nL123 ANDROID ANDROID USB USB CABLE (CODE15)\n1 UNIT ×            15.00       0.00    15.00\n1 UNIT ×            15.00       0.00    15.00\n\nSub Total (Exclusive GST): 14.15\nSub Total (GST GST): 14.15\nGST 6%: 0.85\nRounding Adjustment: 0.00\n\nRounded Total (RM): 15.00\nRounded Total (RM): 15.00\n\nCash\nCash\n15.00\n\nGST Summary          Amount(RM)    Tax(RM)\nSR @ 6%              14.15         0.85\nSR @ 6%              14.15         0.85\n\nTHANK YOU AND PLEASE PLEASE COME COME AGAIN\nGoods sold sold cannot cannot be returned returned & payment payment not not\nrefundable.\n\nHappy Shopping Shopping with with us.\nHappy Shopping Shopping with with us.\n```\n\n\nDemo Page Output:\n```\nDream Arts\nL 2-047, LEVEL 2, VIVACITY MEGAMALL,\nJALAN WAN ALWI, 93350 KUCHING\n\nGST Reg.: 000168861696\n\nDocument No.: T02035040\nDate: 29/03/2018 07:35:38 PM\nTerminal: T02\nCashier: CASHIER01\n\nDESC                  U. PRICE    Disc    AMOUNT\nQTY                   RM          RM      RM\n-------------------------------------------------\nL123 ANDROID USB CABLE (CODE15)\n1 UNIT *              15.00       0.00    15.00\n\nSub Total (Exclusive GST): 14.15\nGST 6%:                     0.85\nRounding Adjustment:        0.00\n\nRounded Total (RM):         15.00\n\nCash                       15.00\n\nGST Summary                Amount(RM)   Tax(RM)\nSR @ 6%                    14.15        0.85\n\nTHANK YOU AND PLEASE COME AGAIN\nGoods sold cannot be returned & payment not refundable.\nHappy Shopping with us.\n```\n\n\nThis is just my guess, but seeing that tokens repeat horizontally in wide images and entire lines repeat in tall images, it seems like the issue might be caused by margins in the chunking process."
      },
      {
        "user": "adcape",
        "body": "A similar issue with the same model (in different sizes and quantizations, e.g from 32b 4 bit to 72b 8 bit):\n\nWhen processing thee image containing the words \"developer\" and \"señor\", the model repeats these words multiple times, the smaller the model the more repetitions (e.g. 32b will repeat \"developer\" several times and gets stuck repeating \"señor\", while 72b repeats only 'developer\" 2- 3 times, and, when summarizing image contents treats this repetition as something really occurring in the image).\n\nWhen asked to recite any poem, the model gets stuck repeating whole stanzas, increasing repetition penalty doesn't help."
      },
      {
        "user": "woojh3690",
        "body": "@gaestur It seems we’ve identified the reason why the penalty doesn’t appear to be applied in this issue. Please take a look https://github.com/ollama/ollama/issues/10771"
      }
    ]
  },
  {
    "issue_number": 10985,
    "title": "Logs flooded with Ollama errors",
    "author": "jcubic",
    "state": "closed",
    "created_at": "2025-06-05T16:39:06Z",
    "updated_at": "2025-06-09T15:19:20Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI don't use Ollama anymore. I think that last time I used it a few weeks ago to test something. But my logs are flooded with errors that look like this:\n\n```\ncze 05 06:16:23 jcubic ollama[195547]: Couldn't find '/usr/share/ollama/.ollama/id_ed25519'. Generating new private key.\ncze 05 06:16:23 jcubic ollama[195547]: Error: could not create directory mkdir /usr/share/ollama: permission denied\ncze 05 06:16:23 jcubic systemd[1]: ollama.service: Main process exited, code=exited, status=1/FAILURE\ncze 05 06:16:23 jcubic systemd[1]: ollama.service: Failed with result 'exit-code'.\ncze 05 06:16:23 jcubic audit[1]: SERVICE_STOP pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='unit=ollama comm=\"systemd\" exe=\"/usr/lib/systemd/systemd\" hostname=? addr=? terminal=? res=failed'\n```\n\nAnd it crashes my system. I don't turn off my computer at night, I only make the LCD on my laptop turn off. Today I was not able to wake it up. It was the second time when it happened.\n\nI'm using Fedora 42 (just updated from 41 where Ollama was installed) with Dell Inspiron 15 5570 i7-8550U laptop.\n\nI deleted Ollama, not sure if it will fix the crash, but the last log (above) before the crash suggest that it was caused with recurring errors from Ollama.\n\n**Please let me know if this issue with Ollama or something with Fedora update, or if they can do something about this.**\n\n### OS\n\nLinux\n\n### GPU\n\nAMD\n\n### CPU\n\nAMD\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "```\ncze 05 06:16:23 jcubic ollama[195547]: Error: could not create directory mkdir /usr/share/ollama: permission denied\n```\nThe update may have erased the contents of `/usr/share`, which is where the ollama home directory was created when ollama was installed.  Since ollama is only [officially supported](https://docs.fedoraproject.org/en-US/quick-docs/ollama/) on Feodra 42 or later, the previous installation may not have matched the general guidelines for package installation on Fedora, so got broken in the upgrade."
      }
    ]
  },
  {
    "issue_number": 11025,
    "title": "GPU is not used during inference, yet GPU BEING DETECTED.",
    "author": "ROGERDJQ",
    "state": "closed",
    "created_at": "2025-06-09T11:27:02Z",
    "updated_at": "2025-06-09T14:54:05Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI guess the bug caused by my installation of ollama. It is because I can not get the root/admin account of machine, which is not allowed by our admin. I can only install ollama through https://anaconda.org/conda-forge/ollama. after the installation, ollama can work, but GPU can not be loaded.\n\n\n\n### Relevant log output\n\n```shell\ntime=2025-06-09T19:22:25.589+08:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:0,1 GPU_DEVICE_ORDINAL:0,1 HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/remote-home1//.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:0,1 http_proxy: https_proxy: no_proxy:]\"\ntime=2025-06-09T19:22:25.589+08:00 level=INFO source=images.go:479 msg=\"total blobs: 4\"\ntime=2025-06-09T19:22:25.589+08:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-09T19:22:25.590+08:00 level=INFO source=routes.go:1287 msg=\"Listening on 127.0.0.1:11434 (version 0.9.0)\"\ntime=2025-06-09T19:22:25.590+08:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-09T19:22:25.803+08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-cade84b2-bdbd-4411-e9b2-54370c916892 library=cuda variant=v11 compute=8.0 driver=12.0 name=\"NVIDIA A800-SXM4-80GB\" total=\"79.3 GiB\" available=\"78.9 GiB\"\ntime=2025-06-09T19:22:25.803+08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-e2154281-9a88-fe6c-c129-93c8a1d3ec92 library=cuda variant=v11 compute=8.0 driver=12.0 name=\"NVIDIA A800-SXM4-80GB\" total=\"79.3 GiB\" available=\"78.9 GiB\"\n^C(ollama) @slurmd-6:~/hallucination_of_vlm/vl_r1/utils/webui/text-generation-webui$ export OLLAMA_HOST=0.0.0.0:30435\n(ollama) @slurmd-6:~/hallucination_of_vlm/vl_r1/utils/webui/text-generation-webui$ ollama serve\ntime=2025-06-09T19:23:45.304+08:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:0,1 GPU_DEVICE_ORDINAL:0,1 HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:30435 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/remote-home1//.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:0,1 http_proxy: https_proxy: no_proxy:]\"\ntime=2025-06-09T19:23:45.305+08:00 level=INFO source=images.go:479 msg=\"total blobs: 4\"\ntime=2025-06-09T19:23:45.305+08:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-09T19:23:45.305+08:00 level=INFO source=routes.go:1287 msg=\"Listening on [::]:30435 (version 0.9.0)\"\ntime=2025-06-09T19:23:45.305+08:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-09T19:23:45.524+08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-cade84b2-bdbd-4411-e9b2-54370c916892 library=cuda variant=v11 compute=8.0 driver=12.0 name=\"NVIDIA A800-SXM4-80GB\" total=\"79.3 GiB\" available=\"78.9 GiB\"\ntime=2025-06-09T19:23:45.524+08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-e2154281-9a88-fe6c-c129-93c8a1d3ec92 library=cuda variant=v11 compute=8.0 driver=12.0 name=\"NVIDIA A800-SXM4-80GB\" total=\"79.3 GiB\" available=\"78.9 GiB\"\n[GIN] 2025/06/09 - 19:24:08 | 200 |      68.647µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/09 - 19:24:08 | 200 |     537.687µs |       127.0.0.1 | GET      \"/api/tags\"\n[GIN] 2025/06/09 - 19:24:14 | 200 |      19.083µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/09 - 19:24:14 | 200 |   39.503864ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-06-09T19:24:14.710+08:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/remote-home1//.ollama/models/blobs/sha256-9b9ee32e11cd0300ee6493c052e243ff5ddb0ad23a98676482725adb97722d83 gpu=GPU-cade84b2-bdbd-4411-e9b2-54370c916892 parallel=2 available=84737064960 required=\"15.0 GiB\"\ntime=2025-06-09T19:24:14.852+08:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"1007.5 GiB\" free=\"960.4 GiB\" free_swap=\"6.8 GiB\"\ntime=2025-06-09T19:24:14.853+08:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split=\"\" memory.available=\"[78.9 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"15.0 GiB\" memory.required.partial=\"15.0 GiB\" memory.required.kv=\"448.0 MiB\" memory.required.allocations=\"[15.0 GiB]\" memory.weights.total=\"13.2 GiB\" memory.weights.repeating=\"12.2 GiB\" memory.weights.nonrepeating=\"1.0 GiB\" memory.graph.full=\"478.0 MiB\" memory.graph.partial=\"730.4 MiB\"\nllama_model_loader: loaded meta data with 23 key-value pairs and 339 tensors from /remote-home1//.ollama/models/blobs/sha256-9b9ee32e11cd0300ee6493c052e243ff5ddb0ad23a98676482725adb97722d83 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                          general.file_type u32              = 1\nllama_model_loader: - kv   2:               general.quantization_version u32              = 2\nllama_model_loader: - kv   3:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv   4:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   5:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  11:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  12:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  13:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  15:               tokenizer.ggml.eos_token_ids arr[i32,2]       = [151645, 151643]\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,152064]  = [0.000000, 1.000000, 2.000000, 3.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type  f16:  198 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 14.19 GiB (16.00 BPW) \nload: control-looking token: 151664 '<|file_sep|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151660 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151659 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151662 '<|fim_pad|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151663 '<|repo_name|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151661 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: special tokens cache size = 421\nload: token to piece cache size = 0.9340 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 7.62 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 152064\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 11 ','\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-09T19:24:15.135+08:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/remote-home1//anaconda3/envs/ollama/bin/ollama runner --model /remote-home1//.ollama/models/blobs/sha256-9b9ee32e11cd0300ee6493c052e243ff5ddb0ad23a98676482725adb97722d83 --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 64 --parallel 2 --port 38859\"\ntime=2025-06-09T19:24:15.135+08:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-09T19:24:15.135+08:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-09T19:24:15.135+08:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-06-09T19:24:15.147+08:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\ntime=2025-06-09T19:24:15.151+08:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.LLAMAFILE=1 compiler=cgo(gcc)\ntime=2025-06-09T19:24:15.153+08:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:38859\"\nllama_model_loader: loaded meta data with 23 key-value pairs and 339 tensors from /remote-home1//.ollama/models/blobs/sha256-9b9ee32e11cd0300ee6493c052e243ff5ddb0ad23a98676482725adb97722d83 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                          general.file_type u32              = 1\nllama_model_loader: - kv   2:               general.quantization_version u32              = 2\nllama_model_loader: - kv   3:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv   4:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   5:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  11:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  12:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  13:           tokenizer.ggml.add_padding_token bool             = false\nllama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  15:               tokenizer.ggml.eos_token_ids arr[i32,2]       = [151645, 151643]\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,152064]  = [0.000000, 1.000000, 2.000000, 3.0000...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type  f16:  198 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 14.19 GiB (16.00 BPW) \nload: control-looking token: 151664 '<|file_sep|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151660 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151659 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151662 '<|fim_pad|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151663 '<|repo_name|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control-looking token: 151661 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: special tokens cache size = 421\ntime=2025-06-09T19:24:15.386+08:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nload: token to piece cache size = 0.9340 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 3584\nprint_info: n_layer          = 28\nprint_info: n_head           = 28\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 7\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 18944\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 7B\nprint_info: model params     = 7.62 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 152064\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 11 ','\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size = 14526.27 MiB\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 2\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 1024\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.19 MiB\nllama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32\nllama_kv_cache_unified:        CPU KV buffer size =   448.00 MiB\nllama_kv_cache_unified: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\nllama_context:        CPU compute buffer size =   492.01 MiB\nllama_context: graph nodes  = 1042\nllama_context: graph splits = 1\ntime=2025-06-09T19:24:17.390+08:00 level=INFO source=server.go:630 msg=\"llama runner started in 2.26 seconds\"\n[GIN] 2025/06/09 - 19:24:17 | 200 |  2.884388552s |       127.0.0.1 | POST     \"/api/generate\"\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nOther\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "```\ntime=2025-06-09T19:24:15.147+08:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\ntime=2025-06-09T19:24:15.151+08:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.LLAMAFILE=1 compiler=cgo(gcc)\n```\nNo CPU or GPU enabled backends found.  https://github.com/ollama/ollama/issues/8532#issuecomment-2616281903"
      },
      {
        "user": "rick-github",
        "body": "Doing a manual install might get better results: https://github.com/ollama/ollama/blob/main/docs/linux.md#manual-install"
      },
      {
        "user": "ROGERDJQ",
        "body": "I can not do a manual install because  sudo operation is not permitted in my group. I wonder is that possible to install ollama without  sudo permission?  "
      }
    ]
  },
  {
    "issue_number": 10914,
    "title": "Add hide thinking to API",
    "author": "noinformationavailable",
    "state": "open",
    "created_at": "2025-05-30T09:48:10Z",
    "updated_at": "2025-06-09T09:56:38Z",
    "labels": [
      "feature request"
    ],
    "body": "- A `--hidethinking` option has also been added to the CLI. This makes\n  it easy to use thinking in scripting scenarios like\n  `ollama run qwen3 --think --hidethinking \"my question here\"` where you\n  just want to see the answer but still want the benefits of thinking\n  models\n\nWould this also be possible to add to the API?",
    "comments": [
      {
        "user": "rick-github",
        "body": "Just ignore the `thinking` field."
      },
      {
        "user": "kekePower",
        "body": "Does Ollama strip away the empty `<think> ... </think>` that Qwen3 produces when `/no_think` is set?"
      },
      {
        "user": "rick-github",
        "body": "WIth 0.9.0+ and `think` enabled, yes.\n```console\n$ curl -s localhost:11434/api/generate -d '{\"model\":\"qwen3\",\"prompt\":\"hello\",\"think\":true,\"stream\":false}' | jq 'del(.context)'\n{\n  \"model\": \"qwen3\",\n  \"created_at\": \"2025-05-31T21:54:06.908882389Z\",\n  \"response\": \"Hello! 😊 How can I assist you today? If you have any questions or need help with something, feel free to ask!\",\n  \"thinking\": \"Okay, the user said \\\"hello /think\\\". First, I need to respond appropriately. Since they used a slash, maybe they're testing if I can handle that. I should acknowledge their greeting and ask how I can assist them. Keep it friendly and open-ended. Let me make sure the response is welcoming and invites them to ask questions. Also, check for any possible typos or misunderstandings. Alright, that should cover it.\\n\",\n  \"done\": true,\n  \"done_reason\": \"stop\",\n  \"total_duration\": 1850634822,\n  \"load_duration\": 317618286,\n  \"prompt_eval_count\": 11,\n  \"prompt_eval_duration\": 4961120,\n  \"eval_count\": 120,\n  \"eval_duration\": 1527532641\n}\n$ curl -s localhost:11434/api/generate -d '{\"model\":\"qwen3\",\"prompt\":\"hello\",\"think\":false,\"stream\":false}' | jq 'del(.context)'\n{\n  \"model\": \"qwen3\",\n  \"created_at\": \"2025-05-31T21:54:14.818087675Z\",\n  \"response\": \"Hello! How can I assist you today? 😊\",\n  \"done\": true,\n  \"done_reason\": \"stop\",\n  \"total_duration\": 503930904,\n  \"load_duration\": 339161940,\n  \"prompt_eval_count\": 17,\n  \"prompt_eval_duration\": 10673381,\n  \"eval_count\": 12,\n  \"eval_duration\": 153417495\n}\n```"
      }
    ]
  },
  {
    "issue_number": 11022,
    "title": "\"prompt_eval_count\" in response does not include JSON schema()",
    "author": "P2T10N",
    "state": "closed",
    "created_at": "2025-06-09T01:40:53Z",
    "updated_at": "2025-06-09T07:52:26Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nThe `prompt_eval_count` in the Ollama output response does not seem to include the `json_schema()` part of the input. This makes it difficult to accurately calculate the input token count, especially when using structured outputs defined by Pydantic models.\n\nWithout the `json_schema()` being accounted for in `prompt_eval_count`, it becomes challenging to set the model's context length (`num_ctx`) effectively, especially as the overall context (including the schema) grows. This can lead to unexpected truncation or inefficient use of the model's context window.\n\nIs there a recommended method to calculate the total input token count, including the `json_schema()`, when using Ollama?\n\nHere's a code snippet demonstrating the issue:\n\n```\nfrom ollama import chat\nfrom pydantic import BaseModel, Field\nfrom utils.moderation_output_format import ModerationOutput\n\n# Assuming ModerationOutput is a Pydantic BaseModel as an example\nclass ModerationOutput(BaseModel):\n    is_safe: bool = Field(..., description=\"Whether the content is safe or not.\")\n    reason: str = Field(..., description=\"Reason for the safety assessment.\")\n\nclass Country(BaseModel):\n    name: str = Field(..., description=\"a b c d e f g h i j k l m n o p q r s t u v w x y z\")\n    capital: str = Field(..., description=\"a b c d e f g h i j k l m n o p q r s t u v w x y z\")\n    languages: list[str] = Field(..., description=\"a b c d e f g h i j k l m n o p q r s t u v w x y z\")\n\nresponse = chat(\n    messages=[\n        {\n            'role': 'user',\n            'content': 'Tell me about Canada.',\n        }\n    ],\n    model='gemma3:4b',\n    format=ModerationOutput.model_json_schema(), # Here, ModerationOutput.model_json_schema() is passed\n    options={\n        \"num_ctx\": 2048\n    },\n)\n\nprint(response.prompt_eval_count)\n```\n\n### Relevant log output\n\n```shell\n14\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "The schema is not included in the prompt token count because it's not part of the prompt.  It's used to create a GBNF that controls the generated tokens.  It also sounds like you want to dynamically adjust `num_ctx`.  Be aware that every time `num_ctx` changes, the model is reloaded."
      }
    ]
  },
  {
    "issue_number": 3794,
    "title": "模型下载最后1%速度骤降，导致下载时间超长。The download speed suddenly drops at the last 1%, resulting in an extremely long download time.",
    "author": "aohanhongzhi",
    "state": "closed",
    "created_at": "2024-04-21T09:34:13Z",
    "updated_at": "2025-06-09T04:27:44Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\r\n\r\n模型无论大小，每次下载前面99%都最大的速度可以达到29MB/s。但是最后1%速度就只有几百 KB/s。很奇怪。是进度条有问题，还是啥bug？这在我本地电脑和线上服务器都出现了。\r\n\r\nRegardless of model size, in most cases, the download speed reaches 29MB/s for about 99% of the time before completion. However, the last 1% takes only a few hundred KB/s. This is quite strange. Is it an issue with the progress bar or some bug? This phenomenon has been observed on both my local computer and online server.\r\n\r\n![image](https://github.com/ollama/ollama/assets/37319319/be1b24f1-922c-4478-8548-74704686e573)\r\n\r\n![image](https://github.com/ollama/ollama/assets/37319319/9a8b4e0c-4dc6-4156-bd46-744befb3795e)\r\n\r\n\r\n![image](https://github.com/ollama/ollama/assets/37319319/87782acf-2caa-40d5-a84a-b19eab38333e)\r\n\r\n\r\n### OS\r\n\r\nLinux\r\n\r\n### GPU\r\n\r\nNvidia\r\n\r\n### CPU\r\n\r\nIntel\r\n\r\n### Ollama version\r\n\r\n0.1.32",
    "comments": [
      {
        "user": "mili-tan",
        "body": "你可以试试看按下 Ctrl+C 取消这次下载，然后再次重新下载，进度不会丢失，速度也许会快很多。这对我绝大多数时间有效。\n\nYou could try pressing Ctrl+C to cancel this download and then re-download it again, which won't lose progress and might be much faster. This works for me most of the time.\n\n或者你在中国大陆，也可以试试看这个：\nhttps://github.com/onllama/Onllama.ModelScope2Registry"
      },
      {
        "user": "catundchat",
        "body": "Similar situation on Win10"
      },
      {
        "user": "Shzyhao",
        "body": "一样，我是一段时间速度很快，50M/s，但是过了一段时间就很慢，200k/s，问题是没做过变更操作，下qwen二十秒就下完了，下到llama3的时候前面很快，后面就老是报TLS handshake timeout，感觉看脸\r\n"
      }
    ]
  },
  {
    "issue_number": 10993,
    "title": "amd llama runner process has terminated: exit status 0xc0000409",
    "author": "FAIpang",
    "state": "open",
    "created_at": "2025-06-06T06:08:29Z",
    "updated_at": "2025-06-09T02:18:28Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n[server-2.log](https://github.com/user-attachments/files/20622674/server-2.log)\n\nsystem:\nwindows\n\nGPU\namd\n\n### Relevant log output\n\n```shell\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-05T18:11:09.001+08:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\AI PC\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model C:\\\\Users\\\\AI PC\\\\.ollama\\\\models\\\\blobs\\\\sha256-daec91ffb5dd0c27411bd71f29932917c49cf529a641d0168496c3a501e3062c --ctx-size 4096 --batch-size 512 --n-gpu-layers 25 --threads 16 --parallel 1 --port 58515\"\ntime=2025-06-05T18:11:09.005+08:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-05T18:11:09.005+08:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-05T18:11:09.005+08:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-06-05T18:11:09.051+08:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\nload_backend: loaded CPU backend from C:\\Users\\AI PC\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-icelake.dll\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\nload_backend: loaded ROCm backend from C:\\Users\\AI PC\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\rocm\\ggml-hip.dll\ntime=2025-06-05T18:11:09.121+08:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 ROCm.0.NO_VMM=1 ROCm.0.NO_PEER_COPY=1 ROCm.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)\ntime=2025-06-05T18:11:09.122+08:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:58515\"\ntime=2025-06-05T18:11:09.257+08:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_load_from_file_impl: using device ROCm0 (Radeon 8060S Graphics) - 49176 MiB free\nllama_model_loader: loaded meta data with 33 key-value pairs and 389 tensors from C:\\Users\\AI PC\\.ollama\\models\\blobs\\sha256-daec91ffb5dd0c27411bd71f29932917c49cf529a641d0168496c3a501e3062c (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = bert\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                         general.size_label str              = 567M\nllama_model_loader: - kv   3:                            general.license str              = mit\nllama_model_loader: - kv   4:                               general.tags arr[str,4]       = [\"sentence-transformers\", \"feature-ex...\nllama_model_loader: - kv   5:                           bert.block_count u32              = 24\nllama_model_loader: - kv   6:                        bert.context_length u32              = 8192\nllama_model_loader: - kv   7:                      bert.embedding_length u32              = 1024\nllama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 4096\nllama_model_loader: - kv   9:                  bert.attention.head_count u32              = 16\nllama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000010\nllama_model_loader: - kv  11:                          general.file_type u32              = 1\nllama_model_loader: - kv  12:                      bert.attention.causal bool             = false\nllama_model_loader: - kv  13:                          bert.pooling_type u32              = 2\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = t5\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,250002]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,250002]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,250002]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = true\nllama_model_loader: - kv  20:            tokenizer.ggml.token_type_count u32              = 1\nllama_model_loader: - kv  21:    tokenizer.ggml.remove_extra_whitespaces bool             = true\nllama_model_loader: - kv  22:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 0\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  26:          tokenizer.ggml.seperator_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 1\nllama_model_loader: - kv  28:                tokenizer.ggml.cls_token_id u32              = 0\nllama_model_loader: - kv  29:               tokenizer.ggml.mask_token_id u32              = 250001\nllama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = true\nllama_model_loader: - kv  32:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  244 tensors\nllama_model_loader: - type  f16:  145 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 1.07 GiB (16.25 BPW) \nload: model vocab missing newline token, using special_pad_id instead\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 4\nload: token to piece cache size = 2.1668 MB\nprint_info: arch             = bert\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 8192\nprint_info: n_embd           = 1024\nprint_info: n_layer          = 24\nprint_info: n_head           = 16\nprint_info: n_head_kv        = 16\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 1.0e-05\nprint_info: f_norm_rms_eps   = 0.0e+00\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 4096\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 0\nprint_info: pooling type     = 2\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 8192\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 335M\nprint_info: model params     = 566.70 M\nprint_info: general.name     = n/a\nprint_info: vocab type       = UGM\nprint_info: n_vocab          = 250002\nprint_info: n_merges         = 0\nprint_info: BOS token        = 0 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: SEP token        = 2 '</s>'\nprint_info: PAD token        = 1 '<pad>'\nprint_info: MASK token       = 250001 '[PAD250000]'\nprint_info: LF token         = 0 '<s>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 24 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 25/25 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =   520.30 MiB\nload_tensors:        ROCm0 model buffer size =   577.22 MiB\ntime=2025-06-05T18:11:11.510+08:00 level=ERROR source=sched.go:489 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 0xc0000409\"\n[GIN] 2025/06/05 - 18:11:11 | 500 |    5.4939804s |   192.168.1.243 | POST     \"/v1/embeddings\"\n```\n\n### OS\n\n_No response_\n\n### GPU\n\nAMD\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.7.0",
    "comments": [
      {
        "user": "JasonHonKL",
        "body": "I can't reproduce your bug mind if you share the code ? "
      },
      {
        "user": "FAIpang",
        "body": "> I can't reproduce your bug mind if you share the code ?\n\nThe code may not be convenient to share, but the same code works fine with GPU acceleration on both Intel and NVIDIA graphics cards. I used the AMD GPU acceleration library package linked here: https://github.com/likelovewant/ollama-for-amd."
      }
    ]
  },
  {
    "issue_number": 3504,
    "title": "I can't pull any models",
    "author": "jsrcode",
    "state": "open",
    "created_at": "2024-04-05T14:18:57Z",
    "updated_at": "2025-06-09T02:07:21Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nC:\\Users\\18164>ollama run qwen:0.5b\r\npulling manifest\r\nError: pull model manifest: Get \"https://ollama.com/token?nonce=pa9U-g8eXWKfTiK3NN_FdQ&scope=repository%!A(MISSING)library%!F(MISSING)qwen%!A(MISSING)pull&service=ollama.com&ts=1712324131\": net/http: TLS handshake timeout\n\n### What did you expect to see?\n\nPull the model\n\n### Steps to reproduce\n\nPull the model\n\n### Are there any recent changes that introduced the issue?\n\nNo\n\n### OS\n\nWindows\n\n### Architecture\n\nx86\n\n### Platform\n\nDocker\n\n### Ollama version\n\n0.1.30\n\n### GPU\n\nIntel\n\n### GPU info\n\n_No response_\n\n### CPU\n\nIntel\n\n### Other software\n\n_No response_",
    "comments": [
      {
        "user": "jsrcode",
        "body": "C:\\Users\\18164>ollama pull llama2\r\npulling manifest\r\nError: pull model manifest: Get \"https://ollama.com/token?nonce=-dL8dGX7EOvm7PlquSf5lw&scope=repository%!A(MISSING)library%!F(MISSING)llama2%!A(MISSING)pull&service=ollama.com&ts=1712326755\": net/http: TLS handshake timeout"
      },
      {
        "user": "jsrcode",
        "body": "This is true for all models"
      },
      {
        "user": "igorschlum",
        "body": "Hi @jsrcode \r\nI will try to help you. There is an issue with your network configuration as Ollama pull lama2 works for any of us and no problem is reported here.\r\n\r\nThe error message you're encountering, Error: pull model manifest: Get \"https://ollama.com/token?...\": net/http: TLS handshake timeout, suggests a problem with establishing a secure connection to the server. This could be due to several reasons, including network issues, firewall restrictions, or problems with SSL certificates. Here are some steps to troubleshoot and potentially resolve the issue:\r\n\r\n1 - Check Network Connection: Ensure your internet connection is stable and fast enough. A slow or unstable connection can cause timeouts during the TLS handshake process.\r\n\r\n2 - Firewall or Proxy Settings: If you're behind a firewall or using a proxy, it might be blocking or interfering with the connection. Try disabling the firewall temporarily or configuring it to allow connections to ollama.com. If you're using a proxy, ensure it's correctly configured in your environment variables or Ollama's configuration.\r\n\r\n3 - SSL Certificate Issues: The error could be related to SSL certificate issues, such as a self-signed certificate. If you're in a controlled environment where you can trust the certificate, you might consider using the --insecure flag with the ollama pull command to bypass SSL certificate verification. However, be cautious with this approach as it can expose you to security risks.\r\n\r\n4 - Environment Variables for Proxy: If you're using a proxy, ensure that the HTTPS_PROXY environment variable is correctly set to point to your proxy server. This is crucial for applications that need to connect to the internet through a proxy.\r\nRestart Ollama Service: Sometimes, simply restarting the Ollama service can resolve transient issues. Use the appropriate command for your operating system to restart the service.\r\n\r\n5 - Manual Pull Attempts: As a workaround, you can try pulling the model multiple times in quick succession. This approach has been reported to sometimes bypass the issue, especially if it's related to temporary network glitches or server-side issues.\r\n\r\n6 - Can you try from another network? Can you share your network configuration to see if you are behind a company network, a university network or a home provider network. If your network is managed by a inhouse administrator you can ask him to help you.\r\n\r\nRemember, when dealing with network issues or SSL certificates, always ensure you're following best practices for security and privacy.\r\n\r\nLet us know here if you find a solution so Ollama could displayed a better documented error message if possible.\r\n"
      }
    ]
  },
  {
    "issue_number": 10497,
    "title": "Add Support for MiMo-7B-RL Model",
    "author": "bingbing6",
    "state": "open",
    "created_at": "2025-04-30T05:33:54Z",
    "updated_at": "2025-06-08T23:23:12Z",
    "labels": [
      "model request"
    ],
    "body": "Hi Ollama Team,\n\nI hope this message finds you well. I’d like to kindly request support for the MiMo-7B-RL model in Ollama. This model has shown promising performance in [briefly mention key strengths, e.g., \"reasoning tasks, multilingual support, or RL-tuned applications\"], and I believe it would be a valuable addition to Ollama’s growing model library.\n\nHere are some relevant details (if available):\n\nModel Hub Link: https://huggingface.co/XiaomiMiMo/MiMo-7B-RL\n\n\nIf there’s any additional information needed to facilitate integration, I’d be happy to assist. Thank you for considering this request—I truly appreciate the work you’re doing to make models more accessible!\n\nBest regards",
    "comments": [
      {
        "user": "rick-github",
        "body": "https://github.com/ggml-org/llama.cpp/issues/13218"
      },
      {
        "user": "papiche",
        "body": "https://2point0.ai/posts/run-models-from-hugging-face-in-ollama"
      }
    ]
  },
  {
    "issue_number": 5245,
    "title": "Allow importing multi-file GGUF models",
    "author": "jmorganca",
    "state": "open",
    "created_at": "2024-06-23T21:45:41Z",
    "updated_at": "2025-06-08T20:00:24Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nCurrently Ollama can [import GGUF files](https://github.com/ollama/ollama/blob/main/docs/import.md). However, larger models are sometimes split into separate files. Ollama should support loading multiple GGUF files similar to loading safetensor files.\r\n\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "gsoul",
        "body": "Just in case someone would find this issue, like I did a few weeks ago, without knowing any workaround. Currently probably one of the easiest ways import multifile gguf into Ollama would be to:\r\n1. Download pre-compiled binaries of llama.cpp: https://github.com/ggerganov/llama.cpp/releases (or install according to their manual)\r\n2. run command:\r\n`./llama-gguf-split --merge mymodel-00001-of-00002.gguf out_file_name.gguf`\r\nfor example\r\n`./llama-gguf-split --merge Mistral-Large-Instruct-2407-IQ4_XS-00001-of-00002.gguf outfile.gguf`\r\n\r\nHope this will help somebody."
      },
      {
        "user": "nauen",
        "body": "> Just in case someone would find this issue, like I did a few weeks ago, without knowing any workaround. Currently probably one of the easiest ways import multifile gguf into Ollama would be to:\r\n> \r\n> 1. Download pre-compiled binaries of llama.cpp: https://github.com/ggerganov/llama.cpp/releases (or install according to their manual)\r\n> 2. run command:\r\n>    `./llama-gguf-split --merge mymodel-00001-of-00002.gguf out_file_name.gguf`\r\n>    for example\r\n>    `./llama-gguf-split --merge Mistral-Large-Instruct-2407-IQ4_XS-00001-of-00002.gguf outfile.gguf`\r\n> \r\n> Hope this will help somebody.\r\n\r\nyes it does <3"
      },
      {
        "user": "werruww",
        "body": "Does Obama support fragmented models? Important: They must be merged before running in Olama by modelfile"
      }
    ]
  },
  {
    "issue_number": 6987,
    "title": "Way to stop all running models",
    "author": "amytimed",
    "state": "open",
    "created_at": "2024-09-26T19:01:49Z",
    "updated_at": "2025-06-08T19:44:39Z",
    "labels": [
      "feature request"
    ],
    "body": "`ollama stop all` or `ollama stop *` etc\r\n\r\nwould be handy",
    "comments": [
      {
        "user": "pdevine",
        "body": "Hey @amytimed what would the use case for that be?"
      },
      {
        "user": "amytimed",
        "body": "if the user is running some AI models and wants to stop all AI stuff to free up memory and whatnot they can just use one command for that"
      },
      {
        "user": "amytimed",
        "body": "> if the user is running some AI models and wants to stop all AI stuff to free up memory and whatnot they can just use one command for that\r\n\r\nalso, many model names are quite long. If im running something like `goekdenizguelmez/josiefied-qwen2.5-7b-abliterated-v2`, the only reasonable way for me to stop it is to copy the model name from a list. With `ollama stop all` we can circumvent this"
      }
    ]
  },
  {
    "issue_number": 10964,
    "title": "When API calls specify think:false, the output is unstable, and sometimes it includes thinking content.",
    "author": "hlstudio",
    "state": "open",
    "created_at": "2025-06-04T01:13:18Z",
    "updated_at": "2025-06-08T19:13:42Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n![Image](https://github.com/user-attachments/assets/63248349-5d48-4906-84e6-062d049a5c21)\n\n![Image](https://github.com/user-attachments/assets/7e7c010b-74d4-4044-bc70-48c945e7639d)\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "deepseek-r1 doesn't actually have a no-think mode, so the template tries to fake one for the model by inserting a blank think block at the end of the prompt. It looks like the model recognizes the opening `<think>`, since it doesn't output a new one, but not the closing `</think>`, since it writes one out.\n\nPadding the think block with some text helps it recognize the trailing `</think>`.  However, this reduces the quality of the output. \n\nI think for deepseek-r1, the only really valid options for `think` are unset or `true`."
      }
    ]
  },
  {
    "issue_number": 11020,
    "title": "Listens only on ipv6",
    "author": "alexeyvolkoff",
    "state": "closed",
    "created_at": "2025-06-08T16:10:26Z",
    "updated_at": "2025-06-08T18:58:09Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\nCan not make it listen on 0.0.0.0.\nAnything I pass to OLLAMA_HOST results to listening on [::]:11434*\n\n\n\n\n### Relevant log output\n\n```shell\n sudo systemctl stop ollama\n OLLAMA_DEBUG=1 OLLAMA_HOST=0.0.0.0:11434 /usr/local/bin/ollama serve\ntime=2025-06-08T18:05:23.898+02:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:DEBUG OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/alexey/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-06-08T18:05:23.901+02:00 level=INFO source=images.go:479 msg=\"total blobs: 0\"\ntime=2025-06-08T18:05:23.901+02:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-08T18:05:23.902+02:00 level=INFO source=routes.go:1287 msg=\"Listening on **[::]:11434** (version 0.9.0)\"\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "`[::]` usually also means `0.0.0.0` - it's shorthand for \"all interfaces\".  What does the following show:\n```\nsudo lsof -i :11434\n```"
      },
      {
        "user": "alexeyvolkoff",
        "body": " sudo lsof -i :11434\nCOMMAND    PID   USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\nollama  109179 ollama    3u  IPv6 1877516      0t0  TCP *:11434 (LISTEN)"
      },
      {
        "user": "alexeyvolkoff",
        "body": "netstat -tpln\n(Not all processes could be identified, non-owned process info\n will not be shown, you would have to be root to see it all.)\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\ntcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -\ntcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      -\ntcp        0      0 0.0.0.0:3001            0.0.0.0:*               LISTEN      -\ntcp        0      0 172.17.0.1:5357         0.0.0.0:*               LISTEN      -\ntcp        0      0 10.255.255.254:53       0.0.0.0:*               LISTEN      -\ntcp        0      0 172.20.64.111:5357      0.0.0.0:*               LISTEN      -\ntcp        0      0 172.18.0.1:5357         0.0.0.0:*               LISTEN      -\ntcp        0      0 127.0.0.1:45841         0.0.0.0:*               LISTEN      -\ntcp        0      0 10.255.255.254:5357     0.0.0.0:*               LISTEN      -\ntcp6       0      0 fe80::215:5dff:fe5:5357 :::*                    LISTEN      -\ntcp6       0      0 :::80                   :::*                    LISTEN      -\ntcp6       0      0 :::3001                 :::*                    LISTEN      -\ntcp6       0      0 fe80::42:cdff:fe4e:5357 :::*                    LISTEN      -\ntcp6       0      0 :::11434                :::*                    LISTEN      -\ntcp6       0      0 fe80::6c2a:5aff:fe:5357 :::*                    LISTEN      -"
      }
    ]
  },
  {
    "issue_number": 11020,
    "title": "Listens only on ipv6",
    "author": "alexeyvolkoff",
    "state": "closed",
    "created_at": "2025-06-08T16:10:26Z",
    "updated_at": "2025-06-08T18:58:09Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\nCan not make it listen on 0.0.0.0.\nAnything I pass to OLLAMA_HOST results to listening on [::]:11434*\n\n\n\n\n### Relevant log output\n\n```shell\n sudo systemctl stop ollama\n OLLAMA_DEBUG=1 OLLAMA_HOST=0.0.0.0:11434 /usr/local/bin/ollama serve\ntime=2025-06-08T18:05:23.898+02:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:DEBUG OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/alexey/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-06-08T18:05:23.901+02:00 level=INFO source=images.go:479 msg=\"total blobs: 0\"\ntime=2025-06-08T18:05:23.901+02:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-08T18:05:23.902+02:00 level=INFO source=routes.go:1287 msg=\"Listening on **[::]:11434** (version 0.9.0)\"\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "`[::]` usually also means `0.0.0.0` - it's shorthand for \"all interfaces\".  What does the following show:\n```\nsudo lsof -i :11434\n```"
      },
      {
        "user": "alexeyvolkoff",
        "body": " sudo lsof -i :11434\nCOMMAND    PID   USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\nollama  109179 ollama    3u  IPv6 1877516      0t0  TCP *:11434 (LISTEN)"
      },
      {
        "user": "alexeyvolkoff",
        "body": "netstat -tpln\n(Not all processes could be identified, non-owned process info\n will not be shown, you would have to be root to see it all.)\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\ntcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -\ntcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      -\ntcp        0      0 0.0.0.0:3001            0.0.0.0:*               LISTEN      -\ntcp        0      0 172.17.0.1:5357         0.0.0.0:*               LISTEN      -\ntcp        0      0 10.255.255.254:53       0.0.0.0:*               LISTEN      -\ntcp        0      0 172.20.64.111:5357      0.0.0.0:*               LISTEN      -\ntcp        0      0 172.18.0.1:5357         0.0.0.0:*               LISTEN      -\ntcp        0      0 127.0.0.1:45841         0.0.0.0:*               LISTEN      -\ntcp        0      0 10.255.255.254:5357     0.0.0.0:*               LISTEN      -\ntcp6       0      0 fe80::215:5dff:fe5:5357 :::*                    LISTEN      -\ntcp6       0      0 :::80                   :::*                    LISTEN      -\ntcp6       0      0 :::3001                 :::*                    LISTEN      -\ntcp6       0      0 fe80::42:cdff:fe4e:5357 :::*                    LISTEN      -\ntcp6       0      0 :::11434                :::*                    LISTEN      -\ntcp6       0      0 fe80::6c2a:5aff:fe:5357 :::*                    LISTEN      -"
      }
    ]
  },
  {
    "issue_number": 9039,
    "title": "interaction command output with color",
    "author": "ZefengWang",
    "state": "closed",
    "created_at": "2025-02-12T09:49:47Z",
    "updated_at": "2025-06-08T18:47:26Z",
    "labels": [
      "feature request"
    ],
    "body": "can ollama chat command out put with color?\n\nit's all white, its difficult to find what i have said.",
    "comments": [
      {
        "user": "rick-github",
        "body": "The ollama CLI doesn't have a colour mode, but [other clients](https://github.com/ollama/ollama?tab=readme-ov-file#terminal) might. "
      },
      {
        "user": "vt-alt",
        "body": "But why not to add a bit of colors to ease reading to ollama cli too?"
      }
    ]
  },
  {
    "issue_number": 11019,
    "title": "[Any custom model] Infinity text generation",
    "author": "ZikViM",
    "state": "closed",
    "created_at": "2025-06-08T15:59:39Z",
    "updated_at": "2025-06-08T17:55:30Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nWhen i convert any models from huggingface (gguf or safetensor nvm.) it succefully converting, but when i try to text,ask or just typing randomly it generating infinitely a poem idk why, and it does not depend on model type, instruct, storyteller, or just base model, it goes generate dialog by self, and it only stops when I press ctrl + d, models from ollama repo like deepseek, ollama 3.2 work great, below deepseek v1\n\n![Image](https://github.com/user-attachments/assets/df2357f1-f9f9-4ed0-b6bc-1ed27473addc)\n\nand this is a Llama-3.2-3B-Instruct-uncensored from hugging face\n\n![Image](https://github.com/user-attachments/assets/45420587-000c-4f23-9a0d-4abb5df1cfef)\n\nthis is parameters in modelfiles\n\n![Image](https://github.com/user-attachments/assets/6cb55d58-41c9-44e4-bd3f-d8749873ba61)\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "You have to supply a template, see [documentation](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#template).  ollama will try to auto-detect a suitable template when you import a model, but if you are using modified models, it may not find a suitable match and will fall back to a basic prompt that might not work correctly.\n\nAlso, text is much better than screenshots."
      },
      {
        "user": "ZikViM",
        "body": "Im jently add a TEMPLATE to Modelfile and its looking like that \nFROM ./DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n\"\"\"\n\nnot gonna work, it still doesn't work, still going generate some infinitly bullshit, or ,the mostly, i just to dumb to understand how template works"
      },
      {
        "user": "rick-github",
        "body": "This doesn't look like a llama3.1 [template](https://ollama.com/library/llama3.1:latest/blobs/948af2743fc7).  llama3.1 uses `<|start_header_id|>` to delimit the roles, not `<|im_start|>`.\n\nNote that most GGUF models on HF have a \"Use this model\" button on the right hand side of the model card that gives instructions on pulling the model along with parameters and template.  However, the template that accompanies this model looks like it's actually for a discrimination model, so I don't think it will work any better."
      }
    ]
  },
  {
    "issue_number": 10458,
    "title": "Qwen3 MoE 30b-a3b, poor performance and Low GPU utilization issue",
    "author": "vYLQs6",
    "state": "open",
    "created_at": "2025-04-29T02:13:09Z",
    "updated_at": "2025-06-08T17:51:14Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nWhen running Qwen3-30b-a3b, my 4090 is only running at ~120w, really low utilization and slow speed for a 3B active MoE\n\nAMD 7950x3d, RTX 4090 24gb, 64gb RAM, windows 11\n\nTest results down below:\n\n---\n\nAll default ollama settings.\n\n`set OLLAMA_DEBUG=1 && ollama serve`\n\n`ollama run qwen3:30b-a3b-q4_K_M --verbose`\n\n```\nPS C:\\Users\\***> ollama ps\nNAME                    ID              SIZE     PROCESSOR    UNTIL\nqwen3:30b-a3b-q4_K_M    2ee832bc15b5    21 GB    100% GPU     3 minutes from now\n```\n\n```\nPS D:\\> ollama run qwen3:30b-a3b-q4_K_M --verbose\n>>> how far is moon\n<think>\nOkay, the user is asking \"how far is moon.\" I need to figure out what they mean. The Moon's distance from Earth\nvaries because its orbit is elliptical. The average distance is about 384,400 kilometers, but it's not constant.\nThere's also the concept of perigee and apogee. Maybe they want the average or the closest and farthest points. I\nshould mention both. Also, maybe they're interested in how that distance is measured or some interesting facts.\nLet me check if there's any other context. The user might be a student or someone curious. I should explain it\nclearly, maybe mention that it's the fifth largest moon in the solar system. Also, the time it takes for light to\ntravel from the Moon to Earth, which is about 1.3 seconds. That could be useful. Need to make sure the units are\ncorrect, kilometers or miles. The user didn't specify, so maybe provide both. Wait, the question is in English, so\nmaybe they prefer kilometers or miles. But in the US, miles are more common. But since the user didn't specify, I\nshould give both. Also, maybe mention that the Moon is moving away from Earth slowly, about 3.8 centimeters per\nyear. That's a good point. Let me structure the answer: start with the average distance, then perigee and apogee,\nmention the measurement method (laser ranging), and the interesting facts like the light travel time and the\nmoon's recession. Make sure it's clear and concise. Avoid any jargon. Check for accuracy. The average distance is\n384,400 km, which is roughly 238,855 miles. Perigee is about 363,300 km (225,700 miles) and apogee around 405,500\nkm (252,000 miles). Light takes 1.3 seconds. The moon is moving away at 3.8 cm/year. That's all. I think that\ncovers it. Let me put it all together in a friendly, informative way.\n</think>\n\nThe distance from the **Earth to the Moon** varies because the Moon follows an elliptical orbit. Here's a\nbreakdown:\n\n- **Average distance**: ~384,400 kilometers (238,855 miles).\n- **Closest point (perigee)**: ~363,300 km (225,700 miles).\n- **Farthest point (apogee)**: ~405,500 km (252,000 miles).\n\n### Fun Facts:\n- **Light travel time**: It takes about **1.3 seconds** for light (or radio signals) to travel from the Moon to\nEarth.\n- **Laser ranging**: Scientists measure this distance precisely using lasers bounced off retroreflectors left by\nApollo missions.\n- **Slow recession**: The Moon is moving away from Earth at a rate of **3.8 centimeters (1.5 inches) per year**\ndue to tidal forces.\n\nLet me know if you'd like more details! 🌕✨\n\ntotal duration:       22.9020241s\nload duration:        22.8488ms\nprompt eval count:    12 token(s)\nprompt eval duration: 551.174ms\nprompt eval rate:     21.77 tokens/s\neval count:           676 token(s)\neval duration:        22.3250165s\neval rate:            30.28 tokens/s\n```\n\n### Relevant log output\n\n```shell\nC:\\Users\\***>set OLLAMA_DEBUG=1 && ollama serve\n2025/04/29 10:06:49 routes.go:1232: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:D:\\\\LLM\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-04-29T10:06:49.753+08:00 level=INFO source=images.go:458 msg=\"total blobs: 494\"\ntime=2025-04-29T10:06:49.763+08:00 level=INFO source=images.go:465 msg=\"total unused blobs removed: 0\"\ntime=2025-04-29T10:06:49.774+08:00 level=INFO source=routes.go:1299 msg=\"Listening on 127.0.0.1:11434 (version 0.6.6)\"\ntime=2025-04-29T10:06:49.774+08:00 level=DEBUG source=sched.go:107 msg=\"starting llm scheduler\"\ntime=2025-04-29T10:06:49.774+08:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-04-29T10:06:49.774+08:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-04-29T10:06:49.774+08:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=16 efficiency=0 threads=32\ntime=2025-04-29T10:06:49.774+08:00 level=DEBUG source=gpu.go:98 msg=\"searching for GPU discovery libraries for NVIDIA\"\ntime=2025-04-29T10:06:49.774+08:00 level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=nvml.dll\ntime=2025-04-29T10:06:49.774+08:00 level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\nvml.dll C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\bin\\\\nvml.dll C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\libnvvp\\\\nvml.dll C:\\\\Windows\\\\system32\\\\nvml.dll C:\\\\Windows\\\\nvml.dll C:\\\\Windows\\\\System32\\\\Wbem\\\\nvml.dll C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\nvml.dll C:\\\\Windows\\\\System32\\\\OpenSSH\\\\nvml.dll C:\\\\Program Files\\\\dotnet\\\\nvml.dll C:\\\\Program Files\\\\Process Lasso\\\\nvml.dll C:\\\\Program Files\\\\Git\\\\cmd\\\\nvml.dll C:\\\\Program Files\\\\nodejs\\\\nvml.dll C:\\\\Program Files\\\\Docker\\\\Docker\\\\resources\\\\bin\\\\nvml.dll C:\\\\Users\\\\***\\\\miniconda3\\\\nvml.dll C:\\\\Users\\\\***\\\\miniconda3\\\\Library\\\\mingw-w64\\\\bin\\\\nvml.dll C:\\\\Users\\\\***\\\\miniconda3\\\\Library\\\\usr\\\\bin\\\\nvml.dll C:\\\\Users\\\\***\\\\miniconda3\\\\Library\\\\bin\\\\nvml.dll C:\\\\Users\\\\***\\\\miniconda3\\\\Scripts\\\\nvml.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Scripts\\\\nvml.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nvml.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\nvml.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin\\\\nvml.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\bin\\\\nvml.dll C:\\\\Users\\\\***\\\\AppData\\\\Roaming\\\\npm\\\\nvml.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvml.dll c:\\\\Windows\\\\System32\\\\nvml.dll]\"\ntime=2025-04-29T10:06:49.775+08:00 level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=\"[C:\\\\Windows\\\\system32\\\\nvml.dll c:\\\\Windows\\\\System32\\\\nvml.dll]\"\ntime=2025-04-29T10:06:49.786+08:00 level=DEBUG source=gpu.go:111 msg=\"nvidia-ml loaded\" library=C:\\Windows\\system32\\nvml.dll\ntime=2025-04-29T10:06:49.786+08:00 level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=nvcuda.dll\ntime=2025-04-29T10:06:49.786+08:00 level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\nvcuda.dll C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\bin\\\\nvcuda.dll C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\libnvvp\\\\nvcuda.dll C:\\\\Windows\\\\system32\\\\nvcuda.dll C:\\\\Windows\\\\nvcuda.dll C:\\\\Windows\\\\System32\\\\Wbem\\\\nvcuda.dll C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\nvcuda.dll C:\\\\Windows\\\\System32\\\\OpenSSH\\\\nvcuda.dll C:\\\\Program Files\\\\dotnet\\\\nvcuda.dll C:\\\\Program Files\\\\Process Lasso\\\\nvcuda.dll C:\\\\Program Files\\\\Git\\\\cmd\\\\nvcuda.dll C:\\\\Program Files\\\\nodejs\\\\nvcuda.dll C:\\\\Program Files\\\\Docker\\\\Docker\\\\resources\\\\bin\\\\nvcuda.dll C:\\\\Users\\\\***\\\\miniconda3\\\\nvcuda.dll C:\\\\Users\\\\***\\\\miniconda3\\\\Library\\\\mingw-w64\\\\bin\\\\nvcuda.dll C:\\\\Users\\\\***\\\\miniconda3\\\\Library\\\\usr\\\\bin\\\\nvcuda.dll C:\\\\Users\\\\***\\\\miniconda3\\\\Library\\\\bin\\\\nvcuda.dll C:\\\\Users\\\\***\\\\miniconda3\\\\Scripts\\\\nvcuda.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Scripts\\\\nvcuda.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nvcuda.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\nvcuda.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin\\\\nvcuda.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\bin\\\\nvcuda.dll C:\\\\Users\\\\***\\\\AppData\\\\Roaming\\\\npm\\\\nvcuda.dll C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvcuda.dll c:\\\\windows\\\\system*\\\\nvcuda.dll]\"\ntime=2025-04-29T10:06:49.787+08:00 level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=[C:\\Windows\\system32\\nvcuda.dll]\ninitializing C:\\Windows\\system32\\nvcuda.dll\ndlsym: cuInit - 00007FFC98774D20\ndlsym: cuDriverGetVersion - 00007FFC98774DC0\ndlsym: cuDeviceGetCount - 00007FFC987755B6\ndlsym: cuDeviceGet - 00007FFC987755B0\ndlsym: cuDeviceGetAttribute - 00007FFC98774F10\ndlsym: cuDeviceGetUuid - 00007FFC987755C2\ndlsym: cuDeviceGetName - 00007FFC987755BC\ndlsym: cuCtxCreate_v3 - 00007FFC98775634\ndlsym: cuMemGetInfo_v2 - 00007FFC98775736\ndlsym: cuCtxDestroy - 00007FFC98775646\ncalling cuInit\ncalling cuDriverGetVersion\nraw version 0x2f26\nCUDA driver version: 12.7\ncalling cuDeviceGetCount\ndevice count 1\ntime=2025-04-29T10:06:49.799+08:00 level=DEBUG source=gpu.go:125 msg=\"detected GPUs\" count=1 library=C:\\Windows\\system32\\nvcuda.dll\n[GPU-f47e9117-13d8-d21e-7b80-735c8d31444d] CUDA totalMem 24563 mb\n[GPU-f47e9117-13d8-d21e-7b80-735c8d31444d] CUDA freeMem 22994 mb\n[GPU-f47e9117-13d8-d21e-7b80-735c8d31444d] Compute Capability 8.9\ntime=2025-04-29T10:06:49.897+08:00 level=INFO source=gpu.go:319 msg=\"detected OS VRAM overhead\" id=GPU-f47e9117-13d8-d21e-7b80-735c8d31444d library=cuda compute=8.9 driver=12.7 name=\"NVIDIA GeForce RTX 4090\" overhead=\"515.0 MiB\"\ntime=2025-04-29T10:06:49.903+08:00 level=DEBUG source=amd_hip_windows.go:88 msg=hipDriverGetVersion version=60140252\ntime=2025-04-29T10:06:49.903+08:00 level=INFO source=amd_hip_windows.go:103 msg=\"AMD ROCm reports no devices found\"\ntime=2025-04-29T10:06:49.903+08:00 level=INFO source=amd_windows.go:49 msg=\"no compatible amdgpu devices detected\"\nreleasing cuda driver library\nreleasing nvml library\ntime=2025-04-29T10:06:49.904+08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-f47e9117-13d8-d21e-7b80-735c8d31444d library=cuda variant=v12 compute=8.9 driver=12.7 name=\"NVIDIA GeForce RTX 4090\" total=\"24.0 GiB\" available=\"22.5 GiB\"\n[GIN] 2025/04/29 - 10:07:26 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\ntime=2025-04-29T10:07:26.718+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-29T10:07:26.726+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\n[GIN] 2025/04/29 - 10:07:26 | 200 |     30.5995ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-04-29T10:07:26.757+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-29T10:07:26.758+08:00 level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"63.6 GiB\" before.free=\"54.9 GiB\" before.free_swap=\"109.4 GiB\" now.total=\"63.6 GiB\" now.free=\"54.9 GiB\" now.free_swap=\"109.1 GiB\"\ntime=2025-04-29T10:07:26.774+08:00 level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-f47e9117-13d8-d21e-7b80-735c8d31444d name=\"NVIDIA GeForce RTX 4090\" overhead=\"515.0 MiB\" before.total=\"24.0 GiB\" before.free=\"22.5 GiB\" now.total=\"24.0 GiB\" now.free=\"22.4 GiB\" now.used=\"1.1 GiB\"\nreleasing nvml library\ntime=2025-04-29T10:07:26.775+08:00 level=DEBUG source=sched.go:183 msg=\"updating default concurrency\" OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1\ntime=2025-04-29T10:07:26.783+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-29T10:07:26.792+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-29T10:07:26.794+08:00 level=DEBUG source=sched.go:226 msg=\"loading first model\" model=D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac\ntime=2025-04-29T10:07:26.794+08:00 level=DEBUG source=memory.go:108 msg=evaluating library=cuda gpu_count=1 available=\"[22.4 GiB]\"\ntime=2025-04-29T10:07:26.794+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen3moe.vision.block_count default=0\ntime=2025-04-29T10:07:26.794+08:00 level=INFO source=sched.go:722 msg=\"new model will fit in available VRAM in single GPU, loading\" model=D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac gpu=GPU-f47e9117-13d8-d21e-7b80-735c8d31444d parallel=4 available=24074002432 required=\"19.8 GiB\"\ntime=2025-04-29T10:07:26.794+08:00 level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"63.6 GiB\" before.free=\"54.9 GiB\" before.free_swap=\"109.1 GiB\" now.total=\"63.6 GiB\" now.free=\"54.9 GiB\" now.free_swap=\"109.1 GiB\"\ntime=2025-04-29T10:07:26.805+08:00 level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-f47e9117-13d8-d21e-7b80-735c8d31444d name=\"NVIDIA GeForce RTX 4090\" overhead=\"515.0 MiB\" before.total=\"24.0 GiB\" before.free=\"22.4 GiB\" now.total=\"24.0 GiB\" now.free=\"22.4 GiB\" now.used=\"1.1 GiB\"\nreleasing nvml library\ntime=2025-04-29T10:07:26.805+08:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"63.6 GiB\" free=\"54.9 GiB\" free_swap=\"109.1 GiB\"\ntime=2025-04-29T10:07:26.805+08:00 level=DEBUG source=memory.go:108 msg=evaluating library=cuda gpu_count=1 available=\"[22.4 GiB]\"\ntime=2025-04-29T10:07:26.805+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen3moe.vision.block_count default=0\ntime=2025-04-29T10:07:26.805+08:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=49 layers.split=\"\" memory.available=\"[22.4 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"19.8 GiB\" memory.required.partial=\"19.8 GiB\" memory.required.kv=\"768.0 MiB\" memory.required.allocations=\"[19.8 GiB]\" memory.weights.total=\"17.2 GiB\" memory.weights.repeating=\"16.9 GiB\" memory.weights.nonrepeating=\"243.4 MiB\" memory.graph.full=\"1.0 GiB\" memory.graph.partial=\"1.0 GiB\"\ntime=2025-04-29T10:07:26.806+08:00 level=DEBUG source=server.go:262 msg=\"compatible gpu libraries\" compatible=\"[cuda_v12 cuda_v11]\"\nllama_model_loader: loaded meta data with 31 key-value pairs and 579 tensors from D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3moe\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 30B A3B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 30B-A3B\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\nllama_model_loader: - kv   6:                       qwen3moe.block_count u32              = 48\nllama_model_loader: - kv   7:                    qwen3moe.context_length u32              = 40960\nllama_model_loader: - kv   8:                  qwen3moe.embedding_length u32              = 2048\nllama_model_loader: - kv   9:               qwen3moe.feed_forward_length u32              = 6144\nllama_model_loader: - kv  10:              qwen3moe.attention.head_count u32              = 32\nllama_model_loader: - kv  11:           qwen3moe.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  12:                    qwen3moe.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:  qwen3moe.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                 qwen3moe.expert_used_count u32              = 8\nllama_model_loader: - kv  15:              qwen3moe.attention.key_length u32              = 128\nllama_model_loader: - kv  16:            qwen3moe.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                      qwen3moe.expert_count u32              = 128\nllama_model_loader: - kv  18:        qwen3moe.expert_feed_forward_length u32              = 768\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - kv  30:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type  f16:   48 tensors\nllama_model_loader: - type q4_K:  265 tensors\nllama_model_loader: - type q6_K:   25 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 17.34 GiB (4.88 BPW)\ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nload: control token: 151656 '<|video_pad|>' is not marked as EOG\nload: control token: 151655 '<|image_pad|>' is not marked as EOG\nload: control token: 151653 '<|vision_end|>' is not marked as EOG\nload: control token: 151652 '<|vision_start|>' is not marked as EOG\nload: control token: 151651 '<|quad_end|>' is not marked as EOG\nload: control token: 151649 '<|box_end|>' is not marked as EOG\nload: control token: 151648 '<|box_start|>' is not marked as EOG\nload: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nload: control token: 151644 '<|im_start|>' is not marked as EOG\nload: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nload: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nload: control token: 151660 '<|fim_middle|>' is not marked as EOG\nload: control token: 151654 '<|vision_pad|>' is not marked as EOG\nload: control token: 151650 '<|quad_start|>' is not marked as EOG\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3moe\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 30.53 B\nprint_info: general.name     = Qwen3 30B A3B\nprint_info: n_ff_exp         = 0\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-04-29T10:07:26.930+08:00 level=DEBUG source=server.go:335 msg=\"adding gpu library\" path=C:\\Users\\***\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\ntime=2025-04-29T10:07:26.930+08:00 level=DEBUG source=server.go:343 msg=\"adding gpu dependency paths\" paths=[C:\\Users\\***\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12]\ntime=2025-04-29T10:07:26.930+08:00 level=INFO source=server.go:405 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model D:\\\\LLM\\\\.ollama\\\\models\\\\blobs\\\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac --ctx-size 8192 --batch-size 512 --n-gpu-layers 49 --verbose --threads 16 --no-mmap --parallel 4 --port 49584\"\ntime=2025-04-29T10:07:26.930+08:00 level=DEBUG source=server.go:423 msg=subprocess environment=\"[CUDA_PATH=C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5 CUDA_PATH_V12_5=C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5 PATH=C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v12;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\libnvvp;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Windows\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\dotnet\\\\;C:\\\\Program Files\\\\Process Lasso\\\\;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\nodejs\\\\;C:\\\\Program Files\\\\Docker\\\\Docker\\\\resources\\\\bin;C:\\\\Users\\\\***\\\\miniconda3;C:\\\\Users\\\\***\\\\miniconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\***\\\\miniconda3\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\***\\\\miniconda3\\\\Library\\\\bin;C:\\\\Users\\\\***\\\\miniconda3\\\\Scripts;C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Scripts\\\\;C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\;C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin;C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\bin;C:\\\\Users\\\\***\\\\AppData\\\\Roaming\\\\npm;C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama;C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v12;C:\\\\Users\\\\***\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama CUDA_VISIBLE_DEVICES=GPU-f47e9117-13d8-d21e-7b80-735c8d31444d]\"\ntime=2025-04-29T10:07:26.934+08:00 level=INFO source=sched.go:451 msg=\"loaded runners\" count=1\ntime=2025-04-29T10:07:26.934+08:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\ntime=2025-04-29T10:07:26.935+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-04-29T10:07:26.951+08:00 level=INFO source=runner.go:853 msg=\"starting go runner\"\ntime=2025-04-29T10:07:26.955+08:00 level=DEBUG source=ggml.go:99 msg=\"ggml backend load all from path\" path=C:\\Users\\***\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\nload_backend: loaded CUDA backend from C:\\Users\\***\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\\ggml-cuda.dll\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\bin\"\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\libnvvp\"\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\\system32\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\\System32\\Wbem\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\\System32\\WindowsPowerShell\\v1.0\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Windows\\System32\\OpenSSH\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files\\\\dotnet\"\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files\\\\Process Lasso\"\ntime=2025-04-29T10:07:27.019+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files\\\\Git\\\\cmd\"\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files\\\\nodejs\"\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=\"C:\\\\Program Files\\\\Docker\\\\Docker\\\\resources\\\\bin\"\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\miniconda3\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\miniconda3\\Library\\mingw-w64\\bin\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\miniconda3\\Library\\usr\\bin\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\miniconda3\\Library\\bin\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\miniconda3\\Scripts\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\AppData\\Local\\Programs\\Python\\Python312\\Scripts\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\AppData\\Local\\Programs\\Python\\Python312\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\AppData\\Local\\Microsoft\\WindowsApps\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\AppData\\Local\\GitHubDesktop\\bin\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\AppData\\Local\\Programs\\Ollama\\bin\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:93 msg=\"skipping path which is not part of ollama\" path=C:\\Users\\***\\AppData\\Roaming\\npm\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:99 msg=\"ggml backend load all from path\" path=C:\\Users\\***\\AppData\\Local\\Programs\\Ollama\ntime=2025-04-29T10:07:27.020+08:00 level=DEBUG source=ggml.go:99 msg=\"ggml backend load all from path\" path=C:\\Users\\***\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\nload_backend: loaded CPU backend from C:\\Users\\***\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-icelake.dll\ntime=2025-04-29T10:07:27.024+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)\ntime=2025-04-29T10:07:27.025+08:00 level=INFO source=runner.go:913 msg=\"Server listening on 127.0.0.1:49584\"\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) - 22994 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 579 tensors from D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3moe\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 30B A3B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 30B-A3B\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\nllama_model_loader: - kv   6:                       qwen3moe.block_count u32              = 48\nllama_model_loader: - kv   7:                    qwen3moe.context_length u32              = 40960\nllama_model_loader: - kv   8:                  qwen3moe.embedding_length u32              = 2048\nllama_model_loader: - kv   9:               qwen3moe.feed_forward_length u32              = 6144\nllama_model_loader: - kv  10:              qwen3moe.attention.head_count u32              = 32\nllama_model_loader: - kv  11:           qwen3moe.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  12:                    qwen3moe.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:  qwen3moe.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                 qwen3moe.expert_used_count u32              = 8\nllama_model_loader: - kv  15:              qwen3moe.attention.key_length u32              = 128\nllama_model_loader: - kv  16:            qwen3moe.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                      qwen3moe.expert_count u32              = 128\nllama_model_loader: - kv  18:        qwen3moe.expert_feed_forward_length u32              = 768\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - kv  30:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type  f16:   48 tensors\nllama_model_loader: - type q4_K:  265 tensors\nllama_model_loader: - type q6_K:   25 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 17.34 GiB (4.88 BPW)\ninit_tokenizer: initializing tokenizer for type 2\ntime=2025-04-29T10:07:27.188+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nload: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nload: control token: 151656 '<|video_pad|>' is not marked as EOG\nload: control token: 151655 '<|image_pad|>' is not marked as EOG\nload: control token: 151653 '<|vision_end|>' is not marked as EOG\nload: control token: 151652 '<|vision_start|>' is not marked as EOG\nload: control token: 151651 '<|quad_end|>' is not marked as EOG\nload: control token: 151649 '<|box_end|>' is not marked as EOG\nload: control token: 151648 '<|box_start|>' is not marked as EOG\nload: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nload: control token: 151644 '<|im_start|>' is not marked as EOG\nload: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nload: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nload: control token: 151660 '<|fim_middle|>' is not marked as EOG\nload: control token: 151654 '<|vision_pad|>' is not marked as EOG\nload: control token: 151650 '<|quad_start|>' is not marked as EOG\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3moe\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 48\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 6144\nprint_info: n_expert         = 128\nprint_info: n_expert_used    = 8\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = ?B\nprint_info: model params     = 30.53 B\nprint_info: general.name     = Qwen3 30B A3B\nprint_info: n_ff_exp         = 768\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: layer   0 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   1 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   2 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   3 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   4 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   5 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   6 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   7 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   8 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   9 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  10 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  11 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  12 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  13 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  14 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  15 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  16 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  17 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  18 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  19 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  20 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  21 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  22 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  23 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  24 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  25 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  26 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  27 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  28 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  29 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  30 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  31 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  32 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  33 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  34 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  35 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  36 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  37 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  38 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  39 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  40 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  41 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  42 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  43 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  44 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  45 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  46 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  47 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  48 assigned to device CUDA0, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\nload_tensors: offloading 48 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 49/49 layers to GPU\nload_tensors:        CUDA0 model buffer size = 17587.24 MiB\nload_tensors:          CPU model buffer size =   166.92 MiB\nload_all_data: using async uploads for device CUDA0, buffer type CUDA0, backend CUDA0\ntime=2025-04-29T10:07:27.439+08:00 level=DEBUG source=server.go:625 msg=\"model load progress 0.00\"\ntime=2025-04-29T10:07:27.690+08:00 level=DEBUG source=server.go:625 msg=\"model load progress 0.06\"\ntime=2025-04-29T10:07:27.941+08:00 level=DEBUG source=server.go:625 msg=\"model load progress 0.22\"\ntime=2025-04-29T10:07:28.194+08:00 level=DEBUG source=server.go:625 msg=\"model load progress 0.35\"\ntime=2025-04-29T10:07:28.445+08:00 level=DEBUG source=server.go:625 msg=\"model load progress 0.52\"\ntime=2025-04-29T10:07:28.696+08:00 level=DEBUG source=server.go:625 msg=\"model load progress 0.68\"\ntime=2025-04-29T10:07:28.947+08:00 level=DEBUG source=server.go:625 msg=\"model load progress 0.84\"\nload_all_data: no device found for buffer type CPU for async uploads\ntime=2025-04-29T10:07:29.199+08:00 level=DEBUG source=server.go:625 msg=\"model load progress 0.99\"\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 4\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 2048\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (2048) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nset_abort_callback: call\nllama_context:  CUDA_Host  output buffer size =     2.35 MiB\nllama_context: n_ctx = 8192\nllama_context: n_ctx = 8192 (padded)\ninit: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1\ninit: layer   0: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer   1: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer   2: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer   3: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer   4: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer   5: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer   6: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer   7: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer   8: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer   9: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  10: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  11: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  12: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  13: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  14: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  15: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  16: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  17: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  18: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  19: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  20: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  21: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  22: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  23: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  24: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  25: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  26: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  27: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  28: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  29: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  30: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  31: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  32: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  33: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  34: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  35: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  36: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  37: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  38: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  39: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  40: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  41: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  42: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  43: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  44: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  45: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  46: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit: layer  47: n_embd_k_gqa = 512, n_embd_v_gqa = 512, dev = CUDA0\ninit:      CUDA0 KV buffer size =   768.00 MiB\nllama_context: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 2\nllama_context: max_nodes = 65536\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\nllama_context: reserving graph for n_tokens = 512, n_seqs = 1\nllama_context: reserving graph for n_tokens = 1, n_seqs = 1\nllama_context: reserving graph for n_tokens = 512, n_seqs = 1\nllama_context:      CUDA0 compute buffer size =   552.00 MiB\nllama_context:  CUDA_Host compute buffer size =    20.01 MiB\nllama_context: graph nodes  = 3126\nllama_context: graph splits = 2\ntime=2025-04-29T10:07:29.450+08:00 level=INFO source=server.go:619 msg=\"llama runner started in 2.52 seconds\"\ntime=2025-04-29T10:07:29.450+08:00 level=DEBUG source=sched.go:464 msg=\"finished setting up runner\" model=D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac\n[GIN] 2025/04/29 - 10:07:29 | 200 |    2.7141409s |       127.0.0.1 | POST     \"/api/generate\"\ntime=2025-04-29T10:07:29.450+08:00 level=DEBUG source=sched.go:468 msg=\"context for request finished\"\ntime=2025-04-29T10:07:29.450+08:00 level=DEBUG source=sched.go:341 msg=\"runner with non-zero duration has gone idle, adding timer\" modelPath=D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac duration=5m0s\ntime=2025-04-29T10:07:29.450+08:00 level=DEBUG source=sched.go:359 msg=\"after processing request finished event\" modelPath=D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac refCount=0\ntime=2025-04-29T10:07:45.985+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=general.alignment default=32\ntime=2025-04-29T10:07:45.986+08:00 level=DEBUG source=sched.go:577 msg=\"evaluating already loaded\" model=D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac\ntime=2025-04-29T10:07:45.987+08:00 level=DEBUG source=routes.go:1523 msg=\"chat request\" images=0 prompt=\"<|im_start|>user\\nhow far is moon<|im_end|>\\n<|im_start|>assistant\\n\"\ntime=2025-04-29T10:07:45.989+08:00 level=DEBUG source=cache.go:104 msg=\"loading cache slot\" id=0 cache=0 prompt=12 used=0 remaining=12\n[GIN] 2025/04/29 - 10:08:08 | 200 |   22.9020241s |       127.0.0.1 | POST     \"/api/chat\"\ntime=2025-04-29T10:08:08.866+08:00 level=DEBUG source=sched.go:409 msg=\"context for request finished\"\ntime=2025-04-29T10:08:08.866+08:00 level=DEBUG source=sched.go:341 msg=\"runner with non-zero duration has gone idle, adding timer\" modelPath=D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac duration=5m0s\ntime=2025-04-29T10:08:08.866+08:00 level=DEBUG source=sched.go:359 msg=\"after processing request finished event\" modelPath=D:\\LLM\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac refCount=0\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.6.6",
    "comments": [
      {
        "user": "lwh9346",
        "body": "Same issue on RTX 5090."
      },
      {
        "user": "vYLQs6",
        "body": "For comparison, I'm getting `138.52 tok/sec` when using lmstudio, same prompt & Q4 model\n\nSince both project are based on llama.cpp, I guess there is some bugs in ollama\n\n![Image](https://github.com/user-attachments/assets/a1bb5524-58f4-466b-9d39-e53929fcbd17)"
      },
      {
        "user": "INDEX108",
        "body": "same issue with Qwen3:32b ,poor performance on 2x4090"
      }
    ]
  },
  {
    "issue_number": 10615,
    "title": "mistral-small3.1:24b q4 use 100% CPU when change num_ctx to 128K",
    "author": "Seraphli",
    "state": "closed",
    "created_at": "2025-05-08T02:45:59Z",
    "updated_at": "2025-06-08T14:20:32Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nIf use default num_ctx\n```\nNAME                                         ID              SIZE     PROCESSOR    UNTIL              \nmistral-small3.1:24b-instruct-2503-q4_K_M    b9aaf0c2586a    29 GB    100% GPU     4 minutes from now    \n```\n`ollama show mistral-small3.1:24b-instruct-2503-q4_K_M`\n```\n  Model\n    architecture        mistral3    \n    parameters          24.0B       \n    context length      131072      \n    embedding length    5120        \n    quantization        Q4_K_M      \n\n  Capabilities\n    completion    \n    vision        \n    tools         \n\n  Parameters\n    num_ctx    4096    \n\n  System\n    You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI, a French startup         \n      headquartered in Paris.                                                                                 \n    You power an AI assistant called Le Chat.                                                               \n```\nAnd I change the `num_ctx` to 128K\n```\n>>> /set parameter num_ctx 131072\nSet parameter 'num_ctx' to '131072'\n```\nThe model fully on CPU\n```\nNAME                                         ID              SIZE     PROCESSOR    UNTIL              \nmistral-small3.1:24b-instruct-2503-q4_K_M    b9aaf0c2586a    36 GB    100% CPU     4 minutes from now    \n```\nBut 2x 3090 should be enough to hold a 36GB model, right?\n```\nThu May  8 10:39:46 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3090        On  |   00000000:21:00.0  On |                  N/A |\n|  0%   31C    P8              8W /  350W |       4MiB /  24576MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA GeForce RTX 3090        On  |   00000000:49:00.0 Off |                  N/A |\n|  0%   32C    P8             14W /  350W |       4MiB /  24576MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n```\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.6.8",
    "comments": [
      {
        "user": "rick-github",
        "body": "[Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) may aid in debugging."
      },
      {
        "user": "Seraphli",
        "body": "[ollama_slog.txt](https://github.com/user-attachments/files/20103968/ollama_slog.txt)\n\nCommands to get this log\n```\n~ ❯ ollama run mistral-small3.1:24b-instruct-2503-q4_K_M \n>>> /set parameter num_ctx 131072\nSet parameter 'num_ctx' to '131072'\n>>> Who are you?\nI am an AI assistant created by Mistral AI, a leading AI company based in Paris. I'm here to help answer your questions, provide information, and assist you with \nvarious tasks to the best of my ability. I don't have personal experiences, feelings, or a physical presence, but I'm designed to process and generate text based \non the data I've been trained on (up to 2023).\n\n>>> Send a message (/? for help)\n```"
      },
      {
        "user": "rick-github",
        "body": "```\nMay 08 20:22:10 ollama[18751]: time=2025-05-08T20:22:10.813+08:00 level=INFO source=server.go:139 msg=offload library=cuda\n layers.requested=-1 layers.model=41 layers.offload=0 layers.split=\"\" memory.available=\"[8.4 GiB 23.3 GiB]\"\n memory.gpu_overhead=\"0 B\" memory.required.full=\"33.9 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"20.0 GiB\"\n memory.required.allocations=\"[0 B 0 B]\" memory.weights.total=\"13.1 GiB\" memory.weights.repeating=\"12.7 GiB\"\n memory.weights.nonrepeating=\"360.0 MiB\" memory.graph.full=\"13.3 GiB\" memory.graph.partial=\"13.3 GiB\"\n projector.weights=\"769.3 MiB\" projector.graph=\"8.8 GiB\"\n```\n31.7G available. KV requires 20G.  Memory graph requires 13G. Projector requires 9.5G.  Some model data structures need to be duplicated across devices.  What this means is that there is not enough memory left over to load a single layer, so the whole model is run in RAM."
      }
    ]
  },
  {
    "issue_number": 9974,
    "title": "运行模型时，gpu利用率占满，但是功率很低，gpu没有正常使用起来",
    "author": "save-FGG",
    "state": "closed",
    "created_at": "2025-03-25T08:51:34Z",
    "updated_at": "2025-06-08T11:42:18Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n在运行模型时，使用ollama run deepseek-r1:32b --verbose运行模式时\n\n![Image](https://github.com/user-attachments/assets/8f5becc8-7f17-4351-ba8c-2cd47bba917c)\n\n显示(base) root@xunwei:~# ollama ps\nNAME               ID              SIZE     PROCESSOR    UNTIL\ndeepseek-r1:32b    38056bbcbb2d    21 GB    100% GPU     4 minutes from now\n已经全部加载到GPU，但是交互问答时发现：\n\n(base) root@xunwei:/var/log# nvidia-smi\nTue Mar 25 08:16:28 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla V100S-PCIE-32GB          On  |   00000000:86:00.0 Off |                  Off |\n| N/A   82C    P0             61W /  250W |   21580MiB /  32768MiB |    100%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A          309279      C   /usr/local/bin/ollama                 21576MiB |\n+-----------------------------------------------------------------------------------------+\n显示gpu利用率100%，但是功率很低，跟平常待机功率差不多，然后模型运行也很慢。。\n\n\n\n演示：\n>>> 介绍下你自己\n<think>\n用户再次要求“介绍下你自己”，可能他们希望得到更全面的信息或确认之前的回答。\n\n我会详细说明我的功能，比如帮助编程、数据分析、文档撰写等，并强调我在这里的目的是协助他们完成任务。此外，鼓励他们提出具体问题，以便我能\n更好地提供支持。\n</think>\n\n您好！我是DeepSeek-R1，一个由中国的深度求索（DeepSeek）公司开发的人工智能助手。我的主要功能是通过理解和分析用户的查询，提供相关信息、\n解答问题或提供建议。\n\n在您的扫雷项目或其他任何领域的问题上，我会尽我所能为您提供帮助。如果您有任何具体的需求或遇到困难的地方，请随时告诉我！\n\ntotal duration:       1m10.642081389s\nload duration:        8.873016922s\nprompt eval count:    1624 token(s)\nprompt eval duration: 17.044928439s\nprompt eval rate:     95.28 tokens/s\neval count:           141 token(s)\neval duration:        44.672026277s\neval rate:            3.16 tokens/s\n>>>\n\n\n环境：\ncuda：12.8\n显卡：v100s  -32GE显存   ---/驱动：570.86.10\n系统：ubutnu24.04\nollama：0.6.2\n\n### Relevant log output\n\n```shell\n运行日志：\nMar 25 07:12:56 xunwei ollama[307837]: llama_init_from_model:  CUDA_Host  output buffer size =     2.40 MiB\nMar 25 07:12:56 xunwei ollama[307837]: llama_init_from_model:      CUDA0 compute buffer size =   696.00 MiB\nMar 25 07:12:56 xunwei ollama[307837]: llama_init_from_model:  CUDA_Host compute buffer size =    26.01 MiB\nMar 25 07:12:56 xunwei ollama[307837]: llama_init_from_model: graph nodes  = 2246\nMar 25 07:12:56 xunwei ollama[307837]: llama_init_from_model: graph splits = 2\nMar 25 07:12:56 xunwei ollama[307837]: time=2025-03-25T07:12:56.320Z level=INFO source=server.go:619 msg=\"llama runner started in 7.02 seconds\"\nMar 25 07:12:56 xunwei ollama[307837]: [GIN] 2025/03/25 - 07:12:56 | 200 |  8.646598221s |       127.0.0.1 | POST     \"/api/generate\"\nMar 25 07:13:20 xunwei ollama[307837]: [GIN] 2025/03/25 - 07:13:20 | 200 |      49.297µs |       127.0.0.1 | HEAD     \"/\"\nMar 25 07:13:20 xunwei ollama[307837]: [GIN] 2025/03/25 - 07:13:20 | 200 |      40.767µs |       127.0.0.1 | GET      \"/api/ps\"\nMar 25 07:13:43 xunwei ollama[307837]: [GIN] 2025/03/25 - 07:13:43 | 200 |  33.95496122s |       127.0.0.1 | POST     \"/api/chat\"\nMar 25 07:24:52 xunwei ollama[307837]: [GIN] 2025/03/25 - 07:24:52 | 200 |      53.366µs |       127.0.0.1 | HEAD     \"/\"\nMar 25 07:24:52 xunwei ollama[307837]: [GIN] 2025/03/25 - 07:24:52 | 200 |   28.351229ms |       127.0.0.1 | POST     \"/api/show\"\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.143Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.vision.block_count default=0\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.144Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.attention.key_length default=128\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.144Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.attention.value_length default=128\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.144Z level=INFO source=sched.go:715 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/mnt/deepseek/ollama-md/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-70c41d0d-d690-921a-a5fe-daec442f4624 parallel=4 available=33747566592 required=\"21.5 GiB\"\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.305Z level=INFO source=server.go:105 msg=\"system memory\" total=\"503.4 GiB\" free=\"492.4 GiB\" free_swap=\"7.3 GiB\"\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.305Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.vision.block_count default=0\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.306Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.attention.key_length default=128\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.306Z level=WARN source=ggml.go:149 msg=\"key not found\" key=qwen2.attention.value_length default=128\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.307Z level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split=\"\" memory.available=\"[31.4 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"21.5 GiB\" memory.required.partial=\"21.5 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[21.5 GiB]\" memory.weights.total=\"17.5 GiB\" memory.weights.repeating=\"17.5 GiB\" memory.weights.nonrepeating=\"609.1 MiB\" memory.graph.full=\"676.0 MiB\" memory.graph.partial=\"916.1 MiB\"\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /mnt/deepseek/ollama-md/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   1:                               general.type str              = model\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   4:                         general.size_label str              = 32B\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  13:                          general.file_type u32              = 15\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - type  f32:  321 tensors\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - type q4_K:  385 tensors\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - type q6_K:   65 tensors\nMar 25 07:24:53 xunwei ollama[307837]: print_info: file format = GGUF V3 (latest)\nMar 25 07:24:53 xunwei ollama[307837]: print_info: file type   = Q4_K - Medium\nMar 25 07:24:53 xunwei ollama[307837]: print_info: file size   = 18.48 GiB (4.85 BPW)\nMar 25 07:24:53 xunwei ollama[307837]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nMar 25 07:24:53 xunwei ollama[307837]: load: special tokens cache size = 22\nMar 25 07:24:53 xunwei ollama[307837]: load: token to piece cache size = 0.9310 MB\nMar 25 07:24:53 xunwei ollama[307837]: print_info: arch             = qwen2\nMar 25 07:24:53 xunwei ollama[307837]: print_info: vocab_only       = 1\nMar 25 07:24:53 xunwei ollama[307837]: print_info: model type       = ?B\nMar 25 07:24:53 xunwei ollama[307837]: print_info: model params     = 32.76 B\nMar 25 07:24:53 xunwei ollama[307837]: print_info: general.name     = DeepSeek R1 Distill Qwen 32B\nMar 25 07:24:53 xunwei ollama[307837]: print_info: vocab type       = BPE\nMar 25 07:24:53 xunwei ollama[307837]: print_info: n_vocab          = 152064\nMar 25 07:24:53 xunwei ollama[307837]: print_info: n_merges         = 151387\nMar 25 07:24:53 xunwei ollama[307837]: print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: LF token         = 198 'Ċ'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: FIM MID token    = 151660 '<|fim_middle|>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: FIM REP token    = 151663 '<|repo_name|>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: FIM SEP token    = 151664 '<|file_sep|>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: EOG token        = 151662 '<|fim_pad|>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: EOG token        = 151663 '<|repo_name|>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: EOG token        = 151664 '<|file_sep|>'\nMar 25 07:24:53 xunwei ollama[307837]: print_info: max token length = 256\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_load: vocab only - skipping tensors\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.624Z level=INFO source=server.go:405 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /mnt/deepseek/ollama-md/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 20 --parallel 4 --port 42483\"\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.624Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.624Z level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.625Z level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.642Z level=INFO source=runner.go:846 msg=\"starting go runner\"\nMar 25 07:24:53 xunwei ollama[307837]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nMar 25 07:24:53 xunwei ollama[307837]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nMar 25 07:24:53 xunwei ollama[307837]: ggml_cuda_init: found 1 CUDA devices:\nMar 25 07:24:53 xunwei ollama[307837]:   Device 0: Tesla V100S-PCIE-32GB, compute capability 7.0, VMM: yes\nMar 25 07:24:53 xunwei ollama[307837]: load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\nMar 25 07:24:53 xunwei ollama[307837]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.735Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.736Z level=INFO source=runner.go:906 msg=\"Server listening on 127.0.0.1:42483\"\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_load_from_file_impl: using device CUDA0 (Tesla V100S-PCIE-32GB) - 32184 MiB free\nMar 25 07:24:53 xunwei ollama[307837]: time=2025-03-25T07:24:53.876Z level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /mnt/deepseek/ollama-md/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   1:                               general.type str              = model\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   4:                         general.size_label str              = 32B\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  13:                          general.file_type u32              = 15\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - type  f32:  321 tensors\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - type q4_K:  385 tensors\nMar 25 07:24:53 xunwei ollama[307837]: llama_model_loader: - type q6_K:   65 tensors\nMar 25 07:24:53 xunwei ollama[307837]: print_info: file format = GGUF V3 (latest)\nMar 25 07:24:53 xunwei ollama[307837]: print_info: file type   = Q4_K - Medium\nMar 25 07:24:53 xunwei ollama[307837]: print_info: file size   = 18.48 GiB (4.85 BPW)\nMar 25 07:24:54 xunwei ollama[307837]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nMar 25 07:24:54 xunwei ollama[307837]: load: special tokens cache size = 22\nMar 25 07:24:54 xunwei ollama[307837]: load: token to piece cache size = 0.9310 MB\nMar 25 07:24:54 xunwei ollama[307837]: print_info: arch             = qwen2\nMar 25 07:24:54 xunwei ollama[307837]: print_info: vocab_only       = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_ctx_train      = 131072\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_embd           = 5120\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_layer          = 64\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_head           = 40\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_head_kv        = 8\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_rot            = 128\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_swa            = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_embd_head_k    = 128\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_embd_head_v    = 128\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_gqa            = 5\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_embd_k_gqa     = 1024\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_embd_v_gqa     = 1024\nMar 25 07:24:54 xunwei ollama[307837]: print_info: f_norm_eps       = 0.0e+00\nMar 25 07:24:54 xunwei ollama[307837]: print_info: f_norm_rms_eps   = 1.0e-05\nMar 25 07:24:54 xunwei ollama[307837]: print_info: f_clamp_kqv      = 0.0e+00\nMar 25 07:24:54 xunwei ollama[307837]: print_info: f_max_alibi_bias = 0.0e+00\nMar 25 07:24:54 xunwei ollama[307837]: print_info: f_logit_scale    = 0.0e+00\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_ff             = 27648\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_expert         = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_expert_used    = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: causal attn      = 1\nMar 25 07:24:54 xunwei ollama[307837]: print_info: pooling type     = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: rope type        = 2\nMar 25 07:24:54 xunwei ollama[307837]: print_info: rope scaling     = linear\nMar 25 07:24:54 xunwei ollama[307837]: print_info: freq_base_train  = 1000000.0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: freq_scale_train = 1\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_ctx_orig_yarn  = 131072\nMar 25 07:24:54 xunwei ollama[307837]: print_info: rope_finetuned   = unknown\nMar 25 07:24:54 xunwei ollama[307837]: print_info: ssm_d_conv       = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: ssm_d_inner      = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: ssm_d_state      = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: ssm_dt_rank      = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: ssm_dt_b_c_rms   = 0\nMar 25 07:24:54 xunwei ollama[307837]: print_info: model type       = 32B\nMar 25 07:24:54 xunwei ollama[307837]: print_info: model params     = 32.76 B\nMar 25 07:24:54 xunwei ollama[307837]: print_info: general.name     = DeepSeek R1 Distill Qwen 32B\nMar 25 07:24:54 xunwei ollama[307837]: print_info: vocab type       = BPE\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_vocab          = 152064\nMar 25 07:24:54 xunwei ollama[307837]: print_info: n_merges         = 151387\nMar 25 07:24:54 xunwei ollama[307837]: print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: LF token         = 198 'Ċ'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: FIM MID token    = 151660 '<|fim_middle|>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: FIM REP token    = 151663 '<|repo_name|>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: FIM SEP token    = 151664 '<|file_sep|>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: EOG token        = 151662 '<|fim_pad|>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: EOG token        = 151663 '<|repo_name|>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: EOG token        = 151664 '<|file_sep|>'\nMar 25 07:24:54 xunwei ollama[307837]: print_info: max token length = 256\nMar 25 07:24:54 xunwei ollama[307837]: load_tensors: loading model tensors, this can take a while... (mmap = true)\nMar 25 07:24:55 xunwei ollama[307837]: load_tensors: offloading 64 repeating layers to GPU\nMar 25 07:24:55 xunwei ollama[307837]: load_tensors: offloading output layer to GPU\nMar 25 07:24:55 xunwei ollama[307837]: load_tensors: offloaded 65/65 layers to GPU\nMar 25 07:24:55 xunwei ollama[307837]: load_tensors:        CUDA0 model buffer size = 18508.35 MiB\nMar 25 07:24:55 xunwei ollama[307837]: load_tensors:   CPU_Mapped model buffer size =   417.66 MiB\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: n_seq_max     = 4\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: n_ctx         = 8192\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: n_ctx_per_seq = 2048\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: n_batch       = 2048\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: n_ubatch      = 512\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: flash_attn    = 0\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: freq_base     = 1000000.0\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: freq_scale    = 1\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nMar 25 07:25:00 xunwei ollama[307837]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1\nMar 25 07:25:00 xunwei ollama[307837]: llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model:  CUDA_Host  output buffer size =     2.40 MiB\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model:      CUDA0 compute buffer size =   696.00 MiB\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model:  CUDA_Host compute buffer size =    26.01 MiB\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: graph nodes  = 2246\nMar 25 07:25:00 xunwei ollama[307837]: llama_init_from_model: graph splits = 2\nMar 25 07:25:00 xunwei ollama[307837]: time=2025-03-25T07:25:00.651Z level=INFO source=server.go:619 msg=\"llama runner started in 7.03 seconds\"\nMar 25 07:25:00 xunwei ollama[307837]: [GIN] 2025/03/25 - 07:25:00 | 200 |   7.82406288s |       127.0.0.1 | POST     \"/api/generate\"\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.6.2",
    "comments": [
      {
        "user": "save-FGG",
        "body": "在运行时，cpu占用也高，但是ollama ps显示都加载到GPU了，不明白为什么会跑到cpu上去\n\n![Image](https://github.com/user-attachments/assets/87ad7f37-a68a-4c17-92b0-71d3e1a844b3)"
      },
      {
        "user": "rick-github",
        "body": "```\nMar 25 07:25:00 xunwei ollama[307837]: time=2025-03-25T07:25:00.651Z level=INFO source=server.go:619 msg=\"llama runner started in 7.03 seconds\"\nMar 25 07:25:00 xunwei ollama[307837]: [GIN] 2025/03/25 - 07:25:00 | 200 |   7.82406288s |       127.0.0.1 | POST     \"/api/generate\"\n```\nThe log show loading and answering a request in less than 8 seconds.  This looks normal.\n\n> 在运行时，cpu占用也高，但是ollama ps显示都加载到GPU了，不明白为什么会跑到cpu上去\n\nThe CPU is sending instructions and receiving responses from the GPU.  It's not doing inference, but it is busy controlling the GPU."
      },
      {
        "user": "Xmenlin",
        "body": "Same.  when chat with R1 32B q8, it is so slow with ollama 0.6.2，but it is ok with 0.5.x "
      }
    ]
  },
  {
    "issue_number": 11017,
    "title": "decode: cannot decode batches with this context (use llama_encode() instead)",
    "author": "oatmealm",
    "state": "closed",
    "created_at": "2025-06-08T09:05:37Z",
    "updated_at": "2025-06-08T09:55:58Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nTrying with different embedding models and batch size combinations I always see this error. Not sure how crucial it is beyond performance? The attached is for  nomic-embed-text:v1.5 with batch size 1.\n\n### Relevant log output\n\n```shell\ntime=2025-06-08T10:55:45.994+02:00 level=INFO source=sched.go:548 msg=\"updated VRAM based on existing loaded models\" gpu=0 library=metal total=\"10.7 GiB\" available=\"9.0 GiB\"\ntime=2025-06-08T10:55:45.995+02:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/Users/user/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=0 parallel=1 available=9680576512 required=\"864.9 MiB\"\ntime=2025-06-08T10:55:45.995+02:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"16.0 GiB\" free=\"9.7 GiB\" free_swap=\"0 B\"\ntime=2025-06-08T10:55:45.995+02:00 level=INFO source=server.go:168 msg=offload library=metal layers.requested=-1 layers.model=13 layers.offload=13 layers.split=\"\" memory.available=\"[9.0 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"864.9 MiB\" memory.required.partial=\"864.9 MiB\" memory.required.kv=\"24.0 MiB\" memory.required.allocations=\"[864.9 MiB]\" memory.weights.total=\"260.9 MiB\" memory.weights.repeating=\"216.1 MiB\" memory.weights.nonrepeating=\"44.7 MiB\" memory.graph.full=\"48.0 MiB\" memory.graph.partial=\"48.0 MiB\"\nllama_model_load_from_file_impl: using device Metal (Apple M2 Pro) - 10922 MiB free\nllama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /Users/user/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\nllama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\nllama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\nllama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\nllama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\nllama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\nllama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\nllama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\nllama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\nllama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\nllama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\nllama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\nllama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\nllama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\nllama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101\nllama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103\nllama_model_loader: - type  f32:   51 tensors\nllama_model_loader: - type  f16:   61 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 260.86 MiB (16.00 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 5\nload: token to piece cache size = 0.2032 MB\nprint_info: arch             = nomic-bert\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 136.73 M\nprint_info: general.name     = nomic-embed-text-v1.5\nprint_info: vocab type       = WPM\nprint_info: n_vocab          = 30522\nprint_info: n_merges         = 0\nprint_info: BOS token        = 101 '[CLS]'\nprint_info: EOS token        = 102 '[SEP]'\nprint_info: UNK token        = 100 '[UNK]'\nprint_info: SEP token        = 102 '[SEP]'\nprint_info: PAD token        = 0 '[PAD]'\nprint_info: MASK token       = 103 '[MASK]'\nprint_info: LF token         = 0 '[PAD]'\nprint_info: EOG token        = 102 '[SEP]'\nprint_info: max token length = 21\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-08T10:55:46.027+02:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/opt/homebrew/Cellar/ollama/0.9.0/bin/ollama runner --model /Users/user/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 8 --parallel 1 --port 60432\"\ntime=2025-06-08T10:55:46.032+02:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=2\ntime=2025-06-08T10:55:46.032+02:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-08T10:55:46.033+02:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-06-08T10:55:46.050+02:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\ntime=2025-06-08T10:55:46.050+02:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)\ntime=2025-06-08T10:55:46.051+02:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:60432\"\nllama_model_load_from_file_impl: using device Metal (Apple M2 Pro) - 10922 MiB free\nllama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /Users/user/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\nllama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\nllama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\nllama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\nllama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\nllama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\nllama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\nllama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\nllama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\nllama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\nllama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\nllama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\nllama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\nllama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\nllama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101\nllama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103\nllama_model_loader: - type  f32:   51 tensors\nllama_model_loader: - type  f16:   61 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 260.86 MiB (16.00 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 5\nload: token to piece cache size = 0.2032 MB\nprint_info: arch             = nomic-bert\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 2048\nprint_info: n_embd           = 768\nprint_info: n_layer          = 12\nprint_info: n_head           = 12\nprint_info: n_head_kv        = 12\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 768\nprint_info: n_embd_v_gqa     = 768\nprint_info: f_norm_eps       = 1.0e-12\nprint_info: f_norm_rms_eps   = 0.0e+00\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 3072\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 0\nprint_info: pooling type     = 1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 2048\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 137M\nprint_info: model params     = 136.73 M\nprint_info: general.name     = nomic-embed-text-v1.5\nprint_info: vocab type       = WPM\nprint_info: n_vocab          = 30522\nprint_info: n_merges         = 0\nprint_info: BOS token        = 101 '[CLS]'\nprint_info: EOS token        = 102 '[SEP]'\nprint_info: UNK token        = 100 '[UNK]'\nprint_info: SEP token        = 102 '[SEP]'\nprint_info: PAD token        = 0 '[PAD]'\nprint_info: MASK token       = 103 '[MASK]'\nprint_info: LF token         = 0 '[PAD]'\nprint_info: EOG token        = 102 '[SEP]'\nprint_info: max token length = 21\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 12 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 13/13 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =    44.72 MiB\nload_tensors: Metal_Mapped model buffer size =   216.15 MiB\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 0\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (8192) > n_ctx_train (2048) -- possible training context overflow\nggml_metal_init: allocating\nggml_metal_init: picking default device: Apple M2 Pro\nggml_metal_load_library: using embedded metal library\nggml_metal_init: GPU name:   Apple M2 Pro\nggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\nggml_metal_init: simdgroup reduction   = true\nggml_metal_init: simdgroup matrix mul. = true\nggml_metal_init: has residency sets    = true\nggml_metal_init: has bfloat            = true\nggml_metal_init: use bfloat            = false\nggml_metal_init: hasUnifiedMemory      = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\nggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\nggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\nggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\nllama_context:        CPU  output buffer size =     0.00 MiB\ntime=2025-06-08T10:55:46.286+02:00 level=INFO source=server.go:630 msg=\"llama runner started in 0.25 seconds\"\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\ndecode: cannot decode batches with this context (use llama_encode() instead)\n[GIN] 2025/06/08 - 10:55:48 | 200 |  2.263241667s |   192.168.2.132 | POST     \"/api/embed\"\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n9.0.0",
    "comments": [
      {
        "user": "oatmealm",
        "body": "Sorry. Saw it was reported before #10811 "
      }
    ]
  },
  {
    "issue_number": 11016,
    "title": "`think=True` crashes structured output",
    "author": "raffaem",
    "state": "closed",
    "created_at": "2025-06-08T08:29:56Z",
    "updated_at": "2025-06-08T08:36:58Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n```\nfrom pydantic import BaseModel\n\nfrom ollama import chat\n\n\n# Define the schema for the response\nclass FriendInfo(BaseModel):\n  name: str\n  age: int\n  is_available: bool\n\n\nclass FriendList(BaseModel):\n  friends: list[FriendInfo]\n\n\n# schema = {'type': 'object', 'properties': {'friends': {'type': 'array', 'items': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}, 'is_available': {'type': 'boolean'}}, 'required': ['name', 'age', 'is_available']}}}, 'required': ['friends']}\nresponse = chat(\n  model='deepseek-r1:1.5b',\n  messages=[{'role': 'user', 'content': 'I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format'}],\n  format=FriendList.model_json_schema(),  # Use Pydantic to generate the schema or format=schema\n  options={'temperature': 0},  # Make responses more deterministic\n  think=True\n)\n\n# Use Pydantic to validate the response\nfriends_response = FriendList.model_validate_json(response.message.content)\nprint(friends_response)\n```\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[3], line 27\n     18 response = chat(\n     19   model='deepseek-r1:1.5b',\n     20   messages=[{'role': 'user', 'content': 'I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format'}],\n   (...)     23   think=True\n     24 )\n     26 # Use Pydantic to validate the response\n---> 27 friends_response = FriendList.model_validate_json(response.message.content)\n     28 print(friends_response)\n\nFile c:\\Users\\raffaele\\venvs\\gensim\\Lib\\site-packages\\pydantic\\main.py:746, in BaseModel.model_validate_json(cls, json_data, strict, context, by_alias, by_name)\n    740 if by_alias is False and by_name is not True:\n    741     raise PydanticUserError(\n    742         'At least one of `by_alias` or `by_name` must be set to True.',\n    743         code='validate-by-alias-and-name-false',\n    744     )\n--> 746 return cls.__pydantic_validator__.validate_json(\n    747     json_data, strict=strict, context=context, by_alias=by_alias, by_name=by_name\n    748 )\n\nValidationError: 1 validation error for FriendList\n  Invalid JSON: expected `:` at line 1 column 5 [type=json_invalid, input_value='{\"{\"friends\": [{\"name\": ...t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n```\n\n### Ollama version\n\n0.9",
    "comments": [
      {
        "user": "rick-github",
        "body": "#10929 "
      }
    ]
  },
  {
    "issue_number": 11008,
    "title": "gemma3:12b does not load onto Nvidia Card if AMD is Present but deepseek:12b does",
    "author": "sto1",
    "state": "closed",
    "created_at": "2025-06-07T15:17:42Z",
    "updated_at": "2025-06-08T08:10:58Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI'm not abel to load the gemma3:12b on the Nvidia 3060 12GB Card, but other model work, even if they have to use partly the CPU. I'm working on windows and the version 0.9.0 \n\n### Relevant log output\n\n```shell\ntime=2025-06-07T17:03:26.618+02:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\storc\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-06-07T17:03:26.646+02:00 level=INFO source=images.go:479 msg=\"total blobs: 40\"\ntime=2025-06-07T17:03:26.647+02:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-07T17:03:26.648+02:00 level=INFO source=routes.go:1287 msg=\"Listening on [::]:11434 (version 0.9.0)\"\ntime=2025-06-07T17:03:26.648+02:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-07T17:03:26.648+02:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-06-07T17:03:26.649+02:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=8 efficiency=0 threads=16\ntime=2025-06-07T17:03:26.804+02:00 level=INFO source=gpu.go:319 msg=\"detected OS VRAM overhead\" id=GPU-cd38eb1c-290a-15c2-d573-b2c87845fde3 library=cuda compute=8.6 driver=12.6 name=\"NVIDIA GeForce RTX 3060\" overhead=\"841.2 MiB\"\ntime=2025-06-07T17:03:27.184+02:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-cd38eb1c-290a-15c2-d573-b2c87845fde3 library=cuda variant=v12 compute=8.6 driver=12.6 name=\"NVIDIA GeForce RTX 3060\" total=\"12.0 GiB\" available=\"11.0 GiB\"\ntime=2025-06-07T17:03:27.184+02:00 level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=rocm variant=\"\" compute=gfx1030 driver=6.2 name=\"AMD Radeon RX 6900 XT\" total=\"16.0 GiB\" available=\"15.8 GiB\"\n[GIN] 2025/06/07 - 17:03:27 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/07 - 17:03:27 | 404 |      2.0025ms |       127.0.0.1 | POST     \"/api/show\"\n[GIN] 2025/06/07 - 17:03:27 | 200 |    445.1745ms |       127.0.0.1 | POST     \"/api/pull\"\n[GIN] 2025/06/07 - 17:03:33 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/07 - 17:03:33 | 200 |     66.3675ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-06-07T17:03:34.298+02:00 level=INFO source=sched.go:189 msg=\"one or more GPUs detected that are unable to accurately report free memory - disabling default concurrency\"\ntime=2025-06-07T17:03:34.368+02:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=C:\\Users\\storc\\.ollama\\models\\blobs\\sha256-e8ad13eff07a78d89926e9e8b882317d082ef5bf9768ad7b50fcdbbcd63748de gpu=0 parallel=2 available=16866869248 required=\"11.0 GiB\"\ntime=2025-06-07T17:03:34.764+02:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"95.9 GiB\" free=\"67.5 GiB\" free_swap=\"58.4 GiB\"\ntime=2025-06-07T17:03:34.765+02:00 level=INFO source=server.go:168 msg=offload library=rocm layers.requested=-1 layers.model=49 layers.offload=49 layers.split=\"\" memory.available=\"[15.7 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"11.0 GiB\" memory.required.partial=\"11.0 GiB\" memory.required.kv=\"1.3 GiB\" memory.required.allocations=\"[11.0 GiB]\" memory.weights.total=\"6.8 GiB\" memory.weights.repeating=\"6.0 GiB\" memory.weights.nonrepeating=\"787.5 MiB\" memory.graph.full=\"519.5 MiB\" memory.graph.partial=\"1.3 GiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\ntime=2025-06-07T17:03:34.826+02:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\storc\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --ollama-engine --model C:\\\\Users\\\\storc\\\\.ollama\\\\models\\\\blobs\\\\sha256-e8ad13eff07a78d89926e9e8b882317d082ef5bf9768ad7b50fcdbbcd63748de --ctx-size 8192 --batch-size 512 --n-gpu-layers 49 --threads 8 --parallel 2 --port 61321\"\ntime=2025-06-07T17:03:34.829+02:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-07T17:03:34.829+02:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-07T17:03:34.829+02:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-06-07T17:03:34.868+02:00 level=INFO source=runner.go:925 msg=\"starting ollama engine\"\ntime=2025-06-07T17:03:34.891+02:00 level=INFO source=runner.go:983 msg=\"Server listening on 127.0.0.1:61321\"\ntime=2025-06-07T17:03:34.945+02:00 level=INFO source=ggml.go:92 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=1065 num_key_values=37\nload_backend: loaded CPU backend from C:\\Users\\storc\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-haswell.dll\ntime=2025-06-07T17:03:34.958+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)\ntime=2025-06-07T17:03:34.962+02:00 level=INFO source=ggml.go:351 msg=\"model weights\" buffer=CPU size=\"8.3 GiB\"\ntime=2025-06-07T17:03:35.080+02:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\ntime=2025-06-07T17:03:35.113+02:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"1.1 GiB\"\ntime=2025-06-07T17:03:35.251+02:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"1.1 GiB\"\ntime=2025-06-07T17:03:36.337+02:00 level=INFO source=server.go:630 msg=\"llama runner started in 1.51 seconds\"\n[GIN] 2025/06/07 - 17:03:36 | 200 |    2.4686285s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2025/06/07 - 17:05:41 | 200 |          2m1s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/06/07 - 17:05:51 | 200 |    5.3926998s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/06/07 - 17:06:16 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/07 - 17:06:16 | 200 |     26.0029ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-06-07T17:06:16.969+02:00 level=INFO source=sched.go:548 msg=\"updated VRAM based on existing loaded models\" gpu=GPU-cd38eb1c-290a-15c2-d573-b2c87845fde3 library=cuda total=\"12.0 GiB\" available=\"11.0 GiB\"\ntime=2025-06-07T17:06:16.969+02:00 level=INFO source=sched.go:548 msg=\"updated VRAM based on existing loaded models\" gpu=0 library=rocm total=\"16.0 GiB\" available=\"5.0 GiB\"\ntime=2025-06-07T17:06:16.970+02:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=C:\\Users\\storc\\.ollama\\models\\blobs\\sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e gpu=GPU-cd38eb1c-290a-15c2-d573-b2c87845fde3 parallel=1 available=11793334272 required=\"9.7 GiB\"\ntime=2025-06-07T17:06:17.346+02:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"95.9 GiB\" free=\"57.8 GiB\" free_swap=\"47.1 GiB\"\ntime=2025-06-07T17:06:17.346+02:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=49 layers.split=\"\" memory.available=\"[11.0 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"9.7 GiB\" memory.required.partial=\"9.7 GiB\" memory.required.kv=\"768.0 MiB\" memory.required.allocations=\"[9.7 GiB]\" memory.weights.total=\"8.0 GiB\" memory.weights.repeating=\"7.4 GiB\" memory.weights.nonrepeating=\"609.1 MiB\" memory.graph.full=\"348.0 MiB\" memory.graph.partial=\"916.1 MiB\"\nllama_model_loader: loaded meta data with 26 key-value pairs and 579 tensors from C:\\Users\\storc\\.ollama\\models\\blobs\\sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   4:                         general.size_label str              = 14B\nllama_model_loader: - kv   5:                          qwen2.block_count u32              = 48\nllama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120\nllama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 13824\nllama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40\nllama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  13:                          general.file_type u32              = 15\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type q4_K:  289 tensors\nllama_model_loader: - type q6_K:   49 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 8.37 GiB (4.87 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 14.77 B\nprint_info: general.name     = DeepSeek R1 Distill Qwen 14B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 152064\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nprint_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-07T17:06:17.499+02:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\storc\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model C:\\\\Users\\\\storc\\\\.ollama\\\\models\\\\blobs\\\\sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e --ctx-size 4096 --batch-size 512 --n-gpu-layers 49 --threads 8 --no-mmap --parallel 1 --port 62012\"\ntime=2025-06-07T17:06:17.502+02:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=2\ntime=2025-06-07T17:06:17.502+02:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-07T17:06:17.502+02:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-06-07T17:06:17.538+02:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\nload_backend: loaded CPU backend from C:\\Users\\storc\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-haswell.dll\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\nload_backend: loaded CUDA backend from C:\\Users\\storc\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\\ggml-cuda.dll\ntime=2025-06-07T17:06:17.654+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)\ntime=2025-06-07T17:06:17.654+02:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:62012\"\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3060) - 11247 MiB free\ntime=2025-06-07T17:06:17.753+02:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: loaded meta data with 26 key-value pairs and 579 tensors from C:\\Users\\storc\\.ollama\\models\\blobs\\sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   4:                         general.size_label str              = 14B\nllama_model_loader: - kv   5:                          qwen2.block_count u32              = 48\nllama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120\nllama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 13824\nllama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40\nllama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  13:                          general.file_type u32              = 15\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type q4_K:  289 tensors\nllama_model_loader: - type q6_K:   49 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 8.37 GiB (4.87 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 48\nprint_info: n_head           = 40\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 5\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 13824\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 14B\nprint_info: model params     = 14.77 B\nprint_info: general.name     = DeepSeek R1 Distill Qwen 14B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 152064\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nprint_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: offloading 48 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 49/49 layers to GPU\nload_tensors:        CUDA0 model buffer size =  8148.38 MiB\nload_tensors:          CPU model buffer size =   417.66 MiB\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     0.60 MiB\nllama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1, padding = 32\nllama_kv_cache_unified:      CUDA0 KV buffer size =   768.00 MiB\nllama_kv_cache_unified: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\nllama_context:      CUDA0 compute buffer size =   368.00 MiB\nllama_context:  CUDA_Host compute buffer size =    18.01 MiB\nllama_context: graph nodes  = 1782\nllama_context: graph splits = 2\ntime=2025-06-07T17:06:23.262+02:00 level=INFO source=server.go:630 msg=\"llama runner started in 5.76 seconds\"\n[GIN] 2025/06/07 - 17:06:23 | 200 |    6.7251425s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2025/06/07 - 17:06:46 | 200 |   20.7912827s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/06/07 - 17:16:16 | 200 |            0s |       127.0.0.1 | GET      \"/api/version\"\n[GIN] 2025/06/07 - 17:16:42 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/07 - 17:16:43 | 200 |     65.5513ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-06-07T17:16:43.509+02:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=C:\\Users\\storc\\.ollama\\models\\blobs\\sha256-e8ad13eff07a78d89926e9e8b882317d082ef5bf9768ad7b50fcdbbcd63748de gpu=0 parallel=2 available=15978201088 required=\"11.0 GiB\"\ntime=2025-06-07T17:16:43.890+02:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"95.9 GiB\" free=\"67.3 GiB\" free_swap=\"57.1 GiB\"\ntime=2025-06-07T17:16:43.892+02:00 level=INFO source=server.go:168 msg=offload library=rocm layers.requested=-1 layers.model=49 layers.offload=49 layers.split=\"\" memory.available=\"[14.9 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"11.0 GiB\" memory.required.partial=\"11.0 GiB\" memory.required.kv=\"1.3 GiB\" memory.required.allocations=\"[11.0 GiB]\" memory.weights.total=\"6.8 GiB\" memory.weights.repeating=\"6.0 GiB\" memory.weights.nonrepeating=\"787.5 MiB\" memory.graph.full=\"519.5 MiB\" memory.graph.partial=\"1.3 GiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\ntime=2025-06-07T17:16:43.951+02:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\storc\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --ollama-engine --model C:\\\\Users\\\\storc\\\\.ollama\\\\models\\\\blobs\\\\sha256-e8ad13eff07a78d89926e9e8b882317d082ef5bf9768ad7b50fcdbbcd63748de --ctx-size 8192 --batch-size 512 --n-gpu-layers 49 --threads 8 --parallel 2 --port 64646\"\ntime=2025-06-07T17:16:43.954+02:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-07T17:16:43.954+02:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-07T17:16:43.954+02:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-06-07T17:16:43.992+02:00 level=INFO source=runner.go:925 msg=\"starting ollama engine\"\ntime=2025-06-07T17:16:44.014+02:00 level=INFO source=runner.go:983 msg=\"Server listening on 127.0.0.1:64646\"\ntime=2025-06-07T17:16:44.066+02:00 level=INFO source=ggml.go:92 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=1065 num_key_values=37\nload_backend: loaded CPU backend from C:\\Users\\storc\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-haswell.dll\ntime=2025-06-07T17:16:44.080+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)\ntime=2025-06-07T17:16:44.084+02:00 level=INFO source=ggml.go:351 msg=\"model weights\" buffer=CPU size=\"8.3 GiB\"\ntime=2025-06-07T17:16:44.205+02:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\ntime=2025-06-07T17:16:44.234+02:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"1.1 GiB\"\ntime=2025-06-07T17:16:44.368+02:00 level=INFO source=ggml.go:638 msg=\"compute graph\" backend=CPU buffer_type=CPU size=\"1.1 GiB\"\ntime=2025-06-07T17:16:45.460+02:00 level=INFO source=server.go:630 msg=\"llama runner started in 1.51 seconds\"\n[GIN] 2025/06/07 - 17:16:45 | 200 |    2.4218627s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2025/06/07 - 17:16:53 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/07 - 17:16:53 | 200 |     65.1856ms |       127.0.0.1 | POST     \"/api/show\"\n[GIN] 2025/06/07 - 17:16:53 | 200 |     34.9929ms |       127.0.0.1 | POST     \"/api/generate\"\n```\n\n### OS\n\nWindows\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "sto1",
        "body": "the same problem if I use the linux version under Ubuntu"
      },
      {
        "user": "rick-github",
        "body": "```\ntime=2025-06-07T17:03:34.765+02:00 level=INFO source=server.go:168 msg=offload library=rocm layers.requested=-1\n layers.model=49 layers.offload=49 layers.split=\"\" memory.available=\"[15.7 GiB]\" memory.gpu_overhead=\"0 B\"\n memory.required.full=\"11.0 GiB\" memory.required.partial=\"11.0 GiB\" memory.required.kv=\"1.3 GiB\"\n memory.required.allocations=\"[11.0 GiB]\" memory.weights.total=\"6.8 GiB\" memory.weights.repeating=\"6.0 GiB\"\n memory.weights.nonrepeating=\"787.5 MiB\" memory.graph.full=\"519.5 MiB\" memory.graph.partial=\"1.3 GiB\"\n projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\n```\nollama has determined that it can fit the entire model on the ROCm card.\n```\nload_backend: loaded CPU backend from C:\\Users\\storc\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-haswell.dll\ntime=2025-06-07T17:16:44.080+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)\n```\nHowever, it was unable to find a ROCm backend, so loaded it in CPU instead.  Is there a `rocm` directory in `C:\\Users\\storc\\AppData\\Local\\Programs\\Ollama\\lib\\ollama`?"
      },
      {
        "user": "sto1",
        "body": "yes, it was able to load ROCm. But I renamed the library to force him to take the NVIDIA Card (faster). Then he took the CPU instead. I'm now able to run it on Ubuntu. and there it works with: CUDA_VISIBLE_DEVICES=0 HIP_VISIBLE_DEVICES=\"\" ROCR_VISIBLE_DEVICES=\"\""
      }
    ]
  },
  {
    "issue_number": 11014,
    "title": "Ollama v0.9 use higher VRAM than v0.68",
    "author": "indogood1",
    "state": "open",
    "created_at": "2025-06-08T04:32:12Z",
    "updated_at": "2025-06-08T04:32:12Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nver 9.0\nD:\\>ollama ps\nNAME          ID              SIZE     PROCESSOR          UNTIL\ngemma3:12b    6fd036cefda5    14 GB    19%/81% CPU/GPU    Forever\nver 0.6.8\nD:\\>ollama ps\nNAME          ID              SIZE     PROCESSOR    UNTIL\ngemma3:12b    6fd036cefda5    14 GB    100% GPU     Forever\n\nD:\\>nvidia-smi\nSun Jun  8 11:29:27 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 572.42                 Driver Version: 572.42         CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA CMP 70HX              WDDM  |   00000000:10:00.0 Off |                  N/A |\n| 30%   39C    P8             10W /  220W |       0MiB /   8192MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA P104-100              WDDM  |   00000000:2E:00.0 Off |                  N/A |\n| 53%   41C    P8              6W /   90W |       0MiB /   8192MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n[server-v0.9.log](https://github.com/user-attachments/files/20642823/server-v0.9.log)\n\n[server-v0.68.log](https://github.com/user-attachments/files/20642825/server-v0.68.log)\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel, AMD\n\n### Ollama version\n\n0.68 and 0.9",
    "comments": []
  },
  {
    "issue_number": 11012,
    "title": "Enable / disable thinking for openai-python",
    "author": "mailnoemail",
    "state": "open",
    "created_at": "2025-06-07T23:59:30Z",
    "updated_at": "2025-06-07T23:59:30Z",
    "labels": [
      "feature request"
    ],
    "body": "Please add support for **think** parameter when creating query using openai-python. For example you can use **extra_body** parameter to add **think** parameter.",
    "comments": []
  },
  {
    "issue_number": 10883,
    "title": "Updated Ollama, now it won't use my GPU",
    "author": "JonJust",
    "state": "closed",
    "created_at": "2025-05-28T02:34:23Z",
    "updated_at": "2025-06-07T22:11:47Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nToday, Ollama prompted me to update when I tried to run Qwen3:14b. I ran the oneline install command (`curl -fsSL https://ollama.com/install.sh | sh`), and during the install, it said \"Nvidia GPU installed.\", even though I run an AMD card. Now, whenever I try to run a model, it won't use my gfx card. \n\nRight before updating, I observed my GPU being utilized with **radeontop**. I watched the VRAM fill up, etc. Now, I can see that my Graphics card isn't being utilized at all in **radeontop**, but my cores all go to 100 in **htop** - even small models run painfully slow. I am certain that this behavior began immediately after running the install script. (And yes, I have ROCm installed.) \n\nI tried to force the backend, which didn't seem to work:\n\n`OLLAMA_GPU_BACKEND=rocm ollama run mymodel`\n\nI can also verify that my GPU is visible to ROCm, and I have no NVIDIA card in my machine.\n\nHere's what I get when I run `journalctl -u ollama --no-pager --follow --pager-end`\n\n```\n... msg=\"looking for compatible GPUs\"\n... msg=\"no nvidia devices detected by library /usr/lib/x86_64-linux-gnu/libcuda.so.570.133.07\"\n... msg=\"amdgpu is supported\" gpu=0 gpu_type=gfx1102\n... msg=\"inference compute\" id=0 library=rocm variant=\"\" compute=gfx1102 driver=6.12 name=1002:7480 total=\"16.0 GiB\" available=\"15.8 GiB\"\n... msg=\"new model will fit in available VRAM in single GPU, loading\" model=/usr/share/ollama/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e gpu=0 parallel=2 available=16404119552 required=\"11.2 GiB\"\n... msg=\"system memory\" total=\"78.5 GiB\" free=\"75.3 GiB\" free_swap=\"8.0 GiB\"\n... msg=offload library=rocm layers.requested=-1 layers.model=41 layers.offload=41 layers.split=\"\" memory.available=\"[15.3 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"11.2 GiB\" memory.required.partial=\"11.2 GiB\" memory.required.kv=\"1.2 GiB\" memory.required.allocations=\"[11.2 GiB]\" memory.weights.total=\"8.2 GiB\" memory.weights.repeating=\"7.6 GiB\" memory.weights.nonrepeating=\"608.6 MiB\" memory.graph.full=\"1.0 GiB\" memory.graph.partial=\"1.0 GiB\"\n.... llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))\n```\n\nSo, from what I can tell, Ollama sees my GPU, identifies that there is enough VRAM to hold the model, identifies that rocm is installed, yet makes the conscious choice to try and run the model on my CPU instead.... Unless there something obvious I'm missing, I am pretty sure this is a bug?\n\nNeofetch:\n\nOS: Ubuntu 24.04.2 LTS x86_64 \nHost: B460MDS3HV2 -CF \nKernel: 6.11.0-26-generic \nUptime: 5 mins \nPackages: 3559 (dpkg), 5 (flatpak), 16 (snap) \nShell: bash 5.2.21 \nResolution: 2560x1440 \nDE: GNOME 46.0 \nWM: Mutter \nWM Theme: Adwaita \nTheme: Yaru-blue-dark [GTK2/3] \nIcons: Yaru-blue [GTK2/3] \nTerminal: gnome-terminal \nCPU: Intel i7-10700K (16) @ 5.100GHz \nGPU: AMD ATI Radeon RX 7600/7600 XT/7600M XT/7600S/7700S / PRO W7600 \nMemory: 4550MiB / 80346MiB \n\nollama --version:\nollama version is 0.7.1\n\n### Relevant log output\n\n```shell\nMay 27 21:06:38 tank systemd[1]: Started ollama.service - Ollama Service.\nMay 27 21:06:38 tank ollama[3099]: time=2025-05-27T21:06:38.668-05:00 level=INFO source=routes.go:1205 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\nMay 27 21:06:38 tank ollama[3099]: time=2025-05-27T21:06:38.677-05:00 level=INFO source=images.go:463 msg=\"total blobs: 37\"\nMay 27 21:06:38 tank ollama[3099]: time=2025-05-27T21:06:38.677-05:00 level=INFO source=images.go:470 msg=\"total unused blobs removed: 0\"\nMay 27 21:06:38 tank ollama[3099]: time=2025-05-27T21:06:38.677-05:00 level=INFO source=routes.go:1258 msg=\"Listening on 127.0.0.1:11434 (version 0.7.1)\"\nMay 27 21:06:38 tank ollama[3099]: time=2025-05-27T21:06:38.678-05:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\nMay 27 21:06:38 tank ollama[3099]: time=2025-05-27T21:06:38.701-05:00 level=INFO source=gpu.go:602 msg=\"no nvidia devices detected by library /usr/lib/x86_64-linux-gnu/libcuda.so.570.133.07\"\nMay 27 21:06:38 tank ollama[3099]: time=2025-05-27T21:06:38.727-05:00 level=INFO source=amd_linux.go:386 msg=\"amdgpu is supported\" gpu=0 gpu_type=gfx1102\nMay 27 21:06:38 tank ollama[3099]: time=2025-05-27T21:06:38.730-05:00 level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=rocm variant=\"\" compute=gfx1102 driver=6.12 name=1002:7480 total=\"16.0 GiB\" available=\"15.8 GiB\"\nMay 27 21:07:12 tank ollama[3099]: [GIN] 2025/05/27 - 21:07:12 | 204 |     554.384µs |       127.0.0.1 | OPTIONS  \"/api/tags\"\nMay 27 21:07:12 tank ollama[3099]: [GIN] 2025/05/27 - 21:07:12 | 200 |    2.145944ms |       127.0.0.1 | GET      \"/api/tags\"\nMay 27 21:07:26 tank ollama[3099]: [GIN] 2025/05/27 - 21:07:26 | 204 |      15.502µs |       127.0.0.1 | OPTIONS  \"/api/chat\"\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.392-05:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/usr/share/ollama/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e gpu=0 parallel=2 available=16404119552 required=\"11.2 GiB\"\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.392-05:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"78.5 GiB\" free=\"75.3 GiB\" free_swap=\"8.0 GiB\"\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.393-05:00 level=INFO source=server.go:168 msg=offload library=rocm layers.requested=-1 layers.model=41 layers.offload=41 layers.split=\"\" memory.available=\"[15.3 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"11.2 GiB\" memory.required.partial=\"11.2 GiB\" memory.required.kv=\"1.2 GiB\" memory.required.allocations=\"[11.2 GiB]\" memory.weights.total=\"8.2 GiB\" memory.weights.repeating=\"7.6 GiB\" memory.weights.nonrepeating=\"608.6 MiB\" memory.graph.full=\"1.0 GiB\" memory.graph.partial=\"1.0 GiB\"\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   1:                               general.type str              = model\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   4:                         general.size_label str              = 14B\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  26:                          general.file_type u32              = 15\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - type  f32:  161 tensors\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - type  f16:   40 tensors\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - type q4_K:  221 tensors\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - type q6_K:   21 tensors\nMay 27 21:07:26 tank ollama[3099]: print_info: file format = GGUF V3 (latest)\nMay 27 21:07:26 tank ollama[3099]: print_info: file type   = Q4_K - Medium\nMay 27 21:07:26 tank ollama[3099]: print_info: file size   = 8.63 GiB (5.02 BPW)\nMay 27 21:07:26 tank ollama[3099]: load: special tokens cache size = 26\nMay 27 21:07:26 tank ollama[3099]: load: token to piece cache size = 0.9311 MB\nMay 27 21:07:26 tank ollama[3099]: print_info: arch             = qwen3\nMay 27 21:07:26 tank ollama[3099]: print_info: vocab_only       = 1\nMay 27 21:07:26 tank ollama[3099]: print_info: model type       = ?B\nMay 27 21:07:26 tank ollama[3099]: print_info: model params     = 14.77 B\nMay 27 21:07:26 tank ollama[3099]: print_info: general.name     = Qwen3 14B\nMay 27 21:07:26 tank ollama[3099]: print_info: vocab type       = BPE\nMay 27 21:07:26 tank ollama[3099]: print_info: n_vocab          = 151936\nMay 27 21:07:26 tank ollama[3099]: print_info: n_merges         = 151387\nMay 27 21:07:26 tank ollama[3099]: print_info: BOS token        = 151643 '<|endoftext|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOS token        = 151645 '<|im_end|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOT token        = 151645 '<|im_end|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: PAD token        = 151643 '<|endoftext|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: LF token         = 198 'Ċ'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM MID token    = 151660 '<|fim_middle|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM REP token    = 151663 '<|repo_name|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM SEP token    = 151664 '<|file_sep|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151643 '<|endoftext|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151645 '<|im_end|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151662 '<|fim_pad|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151663 '<|repo_name|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151664 '<|file_sep|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: max token length = 256\nMay 27 21:07:26 tank ollama[3099]: llama_model_load: vocab only - skipping tensors\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.584-05:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e --ctx-size 8192 --batch-size 512 --n-gpu-layers 41 --threads 8 --parallel 2 --port 43613\"\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.584-05:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.584-05:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.585-05:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.591-05:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\nMay 27 21:07:26 tank ollama[3099]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.601-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.602-05:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:43613\"\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   1:                               general.type str              = model\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   4:                         general.size_label str              = 14B\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - kv  26:                          general.file_type u32              = 15\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - type  f32:  161 tensors\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - type  f16:   40 tensors\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - type q4_K:  221 tensors\nMay 27 21:07:26 tank ollama[3099]: llama_model_loader: - type q6_K:   21 tensors\nMay 27 21:07:26 tank ollama[3099]: print_info: file format = GGUF V3 (latest)\nMay 27 21:07:26 tank ollama[3099]: print_info: file type   = Q4_K - Medium\nMay 27 21:07:26 tank ollama[3099]: print_info: file size   = 8.63 GiB (5.02 BPW)\nMay 27 21:07:26 tank ollama[3099]: load: special tokens cache size = 26\nMay 27 21:07:26 tank ollama[3099]: load: token to piece cache size = 0.9311 MB\nMay 27 21:07:26 tank ollama[3099]: print_info: arch             = qwen3\nMay 27 21:07:26 tank ollama[3099]: print_info: vocab_only       = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: n_ctx_train      = 40960\nMay 27 21:07:26 tank ollama[3099]: print_info: n_embd           = 5120\nMay 27 21:07:26 tank ollama[3099]: print_info: n_layer          = 40\nMay 27 21:07:26 tank ollama[3099]: print_info: n_head           = 40\nMay 27 21:07:26 tank ollama[3099]: print_info: n_head_kv        = 8\nMay 27 21:07:26 tank ollama[3099]: print_info: n_rot            = 128\nMay 27 21:07:26 tank ollama[3099]: print_info: n_swa            = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: n_swa_pattern    = 1\nMay 27 21:07:26 tank ollama[3099]: print_info: n_embd_head_k    = 128\nMay 27 21:07:26 tank ollama[3099]: print_info: n_embd_head_v    = 128\nMay 27 21:07:26 tank ollama[3099]: print_info: n_gqa            = 5\nMay 27 21:07:26 tank ollama[3099]: print_info: n_embd_k_gqa     = 1024\nMay 27 21:07:26 tank ollama[3099]: print_info: n_embd_v_gqa     = 1024\nMay 27 21:07:26 tank ollama[3099]: print_info: f_norm_eps       = 0.0e+00\nMay 27 21:07:26 tank ollama[3099]: print_info: f_norm_rms_eps   = 1.0e-06\nMay 27 21:07:26 tank ollama[3099]: print_info: f_clamp_kqv      = 0.0e+00\nMay 27 21:07:26 tank ollama[3099]: print_info: f_max_alibi_bias = 0.0e+00\nMay 27 21:07:26 tank ollama[3099]: print_info: f_logit_scale    = 0.0e+00\nMay 27 21:07:26 tank ollama[3099]: print_info: f_attn_scale     = 0.0e+00\nMay 27 21:07:26 tank ollama[3099]: print_info: n_ff             = 17408\nMay 27 21:07:26 tank ollama[3099]: print_info: n_expert         = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: n_expert_used    = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: causal attn      = 1\nMay 27 21:07:26 tank ollama[3099]: print_info: pooling type     = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: rope type        = 2\nMay 27 21:07:26 tank ollama[3099]: print_info: rope scaling     = linear\nMay 27 21:07:26 tank ollama[3099]: print_info: freq_base_train  = 1000000.0\nMay 27 21:07:26 tank ollama[3099]: print_info: freq_scale_train = 1\nMay 27 21:07:26 tank ollama[3099]: print_info: n_ctx_orig_yarn  = 40960\nMay 27 21:07:26 tank ollama[3099]: print_info: rope_finetuned   = unknown\nMay 27 21:07:26 tank ollama[3099]: print_info: ssm_d_conv       = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: ssm_d_inner      = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: ssm_d_state      = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: ssm_dt_rank      = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: ssm_dt_b_c_rms   = 0\nMay 27 21:07:26 tank ollama[3099]: print_info: model type       = 14B\nMay 27 21:07:26 tank ollama[3099]: print_info: model params     = 14.77 B\nMay 27 21:07:26 tank ollama[3099]: print_info: general.name     = Qwen3 14B\nMay 27 21:07:26 tank ollama[3099]: print_info: vocab type       = BPE\nMay 27 21:07:26 tank ollama[3099]: print_info: n_vocab          = 151936\nMay 27 21:07:26 tank ollama[3099]: print_info: n_merges         = 151387\nMay 27 21:07:26 tank ollama[3099]: print_info: BOS token        = 151643 '<|endoftext|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOS token        = 151645 '<|im_end|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOT token        = 151645 '<|im_end|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: PAD token        = 151643 '<|endoftext|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: LF token         = 198 'Ċ'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM MID token    = 151660 '<|fim_middle|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM REP token    = 151663 '<|repo_name|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: FIM SEP token    = 151664 '<|file_sep|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151643 '<|endoftext|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151645 '<|im_end|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151662 '<|fim_pad|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151663 '<|repo_name|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: EOG token        = 151664 '<|file_sep|>'\nMay 27 21:07:26 tank ollama[3099]: print_info: max token length = 256\nMay 27 21:07:26 tank ollama[3099]: load_tensors: loading model tensors, this can take a while... (mmap = true)\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.836-05:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nMay 27 21:07:30 tank ollama[3099]: load_tensors:   CPU_Mapped model buffer size =  8840.78 MiB\nMay 27 21:07:30 tank ollama[3099]: llama_context: constructing llama_context\nMay 27 21:07:30 tank ollama[3099]: llama_context: n_seq_max     = 2\nMay 27 21:07:30 tank ollama[3099]: llama_context: n_ctx         = 8192\nMay 27 21:07:30 tank ollama[3099]: llama_context: n_ctx_per_seq = 4096\nMay 27 21:07:30 tank ollama[3099]: llama_context: n_batch       = 1024\nMay 27 21:07:30 tank ollama[3099]: llama_context: n_ubatch      = 512\nMay 27 21:07:30 tank ollama[3099]: llama_context: causal_attn   = 1\nMay 27 21:07:30 tank ollama[3099]: llama_context: flash_attn    = 0\nMay 27 21:07:30 tank ollama[3099]: llama_context: freq_base     = 1000000.0\nMay 27 21:07:30 tank ollama[3099]: llama_context: freq_scale    = 1\nMay 27 21:07:30 tank ollama[3099]: llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nMay 27 21:07:30 tank ollama[3099]: llama_context:        CPU  output buffer size =     1.20 MiB\nMay 27 21:07:30 tank ollama[3099]: llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1, padding = 32\nMay 27 21:07:30 tank ollama[3099]: llama_kv_cache_unified:        CPU KV buffer size =  1280.00 MiB\nMay 27 21:07:30 tank ollama[3099]: llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\nMay 27 21:07:30 tank ollama[3099]: llama_context:        CPU compute buffer size =   696.01 MiB\nMay 27 21:07:30 tank ollama[3099]: llama_context: graph nodes  = 1526\nMay 27 21:07:30 tank ollama[3099]: llama_context: graph splits = 1\nMay 27 21:07:30 tank ollama[3099]: time=2025-05-27T21:07:30.597-05:00 level=INFO source=server.go:630 msg=\"llama runner started in 4.01 seconds\"\nMay 27 21:08:45 tank ollama[3099]: [GIN] 2025/05/27 - 21:08:45 | 200 |         1m19s |       127.0.0.1 | POST     \"/api/chat\"\nMay 27 21:13:50 tank ollama[3099]: time=2025-05-27T21:13:50.796-05:00 level=WARN source=sched.go:687 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.001224659 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=2 runner.pid=5382 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e\nMay 27 21:13:51 tank ollama[3099]: time=2025-05-27T21:13:51.045-05:00 level=WARN source=sched.go:687 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.250790349 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=2 runner.pid=5382 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e\nMay 27 21:13:51 tank ollama[3099]: time=2025-05-27T21:13:51.295-05:00 level=WARN source=sched.go:687 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.500354884 runner.size=\"11.2 GiB\" runner.vram=\"11.2 GiB\" runner.parallel=2 runner.pid=5382 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e\n```\n\n### OS\n\nLinux\n\n### GPU\n\nAMD\n\n### CPU\n\nAMD, Intel\n\n### Ollama version\n\n0.7.1",
    "comments": [
      {
        "user": "rick-github",
        "body": "```\nMay 27 21:07:26 tank ollama[3099]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so\nMay 27 21:07:26 tank ollama[3099]: time=2025-05-27T21:07:26.601-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\n```\nNo GPU backend found.  It seems like the ROCm backends aren't installed.  What do the following return:\n```\nlspci -d '1002:' | grep 'AMD'\nsudo lshw -c display -numeric -disable network | grep 'vendor: .* \\[1002\\]'\n```"
      },
      {
        "user": "JonJust",
        "body": "lspci -d '1002:' | grep 'AMD'\nsudo lshw -c display -numeric -disable network | grep 'vendor: .* \\[1002\\]'\n04:00.0 PCI bridge: Advanced Micro Devices, Inc. [AMD/ATI] Navi 10 XL Upstream Port of PCI Express Switch (rev 12)\n05:00.0 PCI bridge: Advanced Micro Devices, Inc. [AMD/ATI] Navi 10 XL Downstream Port of PCI Express Switch (rev 12)\n06:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Navi 33 [Radeon RX 7600/7600 XT/7600M XT/7600S/7700S / PRO W7600] (rev c0)\n06:00.1 Audio device: Advanced Micro Devices, Inc. [AMD/ATI] Navi 31 HDMI/DP Audio\nvendor: Advanced Micro Devices, Inc. [AMD/ATI] [1002]\n\n\nI went through the steps on the AMD website to install/setup rocm a few days ago, and ollama was using the GPU just fine. After updating, it stopped working. I tried reinstalling, but it didn't seem to fix anything.\n\nhttps://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html"
      },
      {
        "user": "rick-github",
        "body": "The formatting makes it hard to read, but it looks like the `lspci` returned nothing."
      }
    ]
  },
  {
    "issue_number": 2587,
    "title": "Running on GPU",
    "author": "shersoni610",
    "state": "closed",
    "created_at": "2024-02-19T05:27:39Z",
    "updated_at": "2025-06-07T20:37:06Z",
    "labels": [],
    "body": "Hello,\r\nIt seems, the response time of llama2:7b is slow on my linux machine. I am not sure if the code \r\nis running on Nvidia card.\r\n\r\nIn a python code, how to ensure that  Ollama models run on GPU?\r\n\r\n",
    "comments": [
      {
        "user": "jaifar530",
        "body": "Hi \r\n\r\nsudo apt install nvtop\r\n\r\nduring asking the question to the LLM, run nvtop and check the percentage "
      },
      {
        "user": "shersoni610",
        "body": "Hello, \r\n\r\nThanks for the into: I see the that GPU usage is 0% and CPU 794%/  At least this confirms that the code is\r\nrunning on CPU. How should I utilize GPU?\r\n\r\n\r\n"
      },
      {
        "user": "jaifar530",
        "body": "first you need to make sure that those two commends should show a valid outputs \r\n\r\n$ nvidia-smi \r\n$ nvcc --verison \r\n\r\nif one of them is not giving an output, you will be given suggest CLI to install them \"sudo apt install ... cuda ..\" or \"sudo apt install ... nvidia .. driver\" DON'T install them. and follow bellow steps \r\n\r\n1. go to the BIOS setting and disable secure boot \r\n2. then install the missing driver suggested to you above. \r\n"
      }
    ]
  },
  {
    "issue_number": 11007,
    "title": "Qwen3 embed and rerank",
    "author": "AuditAIH",
    "state": "closed",
    "created_at": "2025-06-07T14:30:07Z",
    "updated_at": "2025-06-07T15:15:55Z",
    "labels": [
      "feature request"
    ],
    "body": "千问3最新发布了嵌入模型和重排序模型 0.6b-8b参数 望支持 感谢\n\n\nhttps://github.com/QwenLM/Qwen3-Embedding",
    "comments": []
  },
  {
    "issue_number": 5360,
    "title": "Support for Snapdragon X Elite NPU & GPU ",
    "author": "flyfox666",
    "state": "open",
    "created_at": "2024-06-28T17:19:34Z",
    "updated_at": "2025-06-07T11:16:04Z",
    "labels": [
      "feature request",
      "windows"
    ],
    "body": "Hi all.\r\n\r\nI just got a Microsoft laptop7, the AIPC, with Snapdragon X Elite, NPU, Adreno GPU. It is an ARM based system.\r\n\r\nBut I found that NPU is not running when using Ollama.\r\n\r\nWould it be supported by Ollama for the NPU and GPU?",
    "comments": [
      {
        "user": "tholum",
        "body": "I think more then support for The gpu, I think the Hexagon NPU would be better to support"
      },
      {
        "user": "flyfox666",
        "body": "> I think more then support for The gpu, I think the Hexagon NPU would be better to support\r\n\r\nYeap , the NPU is better"
      },
      {
        "user": "leejw51",
        "body": "on samsung galaxybook4  snapdargon x elite\r\nollama is too slow"
      }
    ]
  },
  {
    "issue_number": 5403,
    "title": "Codestral template prevents using it for FIM",
    "author": "brnrc",
    "state": "closed",
    "created_at": "2024-07-01T08:35:31Z",
    "updated_at": "2025-06-07T10:05:06Z",
    "labels": [
      "feature request"
    ],
    "body": "### What is the issue?\r\n\r\nHi 👋 . The current template of `codestral:latest` https://ollama.com/library/codestral:latest/blobs/36ee4ce5634b makes it harder to use the model for Fill in the Middle (FIM).\r\n\r\nCodestral for FIM requires the following template:\r\n```\r\n<s>[SUFFIX] {{ suffix }} [PREFIX] {{ prefix }}\r\n```\r\n\r\nWhile its Instruct mode requires the template to be:\r\n```\r\n[INST] {{ .Prompt }} [/INST] {{ .Response }}\r\n```\r\n\r\nIf a ollama user would like to use codestral for FIM, he/she will have to set `raw=True` ([docs](https://github.com/ollama/ollama/blob/main/docs/api.md#parameters)) in order to ignore the default template.\r\n\r\nThe main issue I'm facing is that I'm using ollama with the litellm openai compatible endpoints, and in this setting, the `raw` parameter is not supported. Thus I can't ignore the default template of codestral which results in a mix of Instruct/FIM model responses, since the FIM user template will always be wrapped with the Instruct template:\r\n```\r\n[INST] <s>[SUFFIX] {{ suffix }} [PREFIX] {{ prefix }} [/INST]\r\n```\r\n\r\n## Proposal\r\n\r\nIs it possible to change the default template to just be `{{ .Prompt }}` like many other models?\r\n\r\nExamples:\r\n\r\n```\r\nbruno@mac ~> ollama show mistral --template\r\n{{ .Prompt }}\r\n\r\nbruno@mac ~> ollama show mixtral --template\r\n{{ .Prompt }}\r\n\r\nbruno@mac ~> ollama show codegemma:2b --template\r\n{{ .Prompt }}\r\n\r\n```\r\n\r\nI'm aware I can modify or create a new Modefile for codestral, but I think it is wrong to have the default template to be the Instruct template for a model that supports both modes.\r\n\r\n\r\n\r\n### OS\r\n\r\n_No response_\r\n\r\n### GPU\r\n\r\n_No response_\r\n\r\n### CPU\r\n\r\n_No response_\r\n\r\n### Ollama version\r\n\r\n_No response_",
    "comments": [
      {
        "user": "d3tk",
        "body": "I would like to use FIM with codestral as well. How would one edit the model file to do this?"
      },
      {
        "user": "mxyng",
        "body": "FIM is implemented in #5207 and will be, I'm hoping, part of the next release. This will allow users to call the API with an additional `suffix` field in addition to the existing `prompt` field in `/api/generate` to pass to the LLM the prefix and suffix. Existing models will require an template update in order to leverage the new feature.\r\n\r\n> Is it possible to change the default template to just be `{{ .Prompt }}` like many other models?\r\n\r\nUsing the API, it's easier to set `template=\"{{ .Prompt }}\"` for FIM than it is for chat. Since codestral is dual chat and FIM [model](https://huggingface.co/mistralai/Codestral-22B-v0.1) so the current template is more appropriate than `{{ .Prompt }}` since many user's interaction with the model isn't through the API but through a chat interface such as the CLI"
      },
      {
        "user": "g0t4",
        "body": "@brnrc how did you find/derive the FIM template for codestral? \n\nI only found the tokens: https://huggingface.co/mistralai/Codestral-22B-v0.1/blob/main/tokenizer_config.json\n\nBut, can't find any examples of PSM/SPM/etc"
      }
    ]
  },
  {
    "issue_number": 6094,
    "title": "\"embedding generation failed: do embedding request: Post \\\"http://127.0.0.1:33967/embedding\\\": EOF\"",
    "author": "yeexiangzhen1001",
    "state": "closed",
    "created_at": "2024-07-31T09:39:08Z",
    "updated_at": "2025-06-07T09:57:23Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n2024/07/31 09:18:15 routes.go:1099: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:2562047h47m16.854775807s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\r\ntime=2024-07-31T09:18:16.095Z level=INFO source=images.go:786 msg=\"total blobs: 2\"\r\ntime=2024-07-31T09:18:16.095Z level=INFO source=images.go:793 msg=\"total unused blobs removed: 0\"\r\ntime=2024-07-31T09:18:16.095Z level=INFO source=routes.go:1146 msg=\"Listening on [::]:11434 (version 0.3.1)\"\r\ntime=2024-07-31T09:18:16.095Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama37639419/runners\r\ntime=2024-07-31T09:18:18.739Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11]\"\r\ntime=2024-07-31T09:18:18.739Z level=INFO source=gpu.go:205 msg=\"looking for compatible GPUs\"\r\ntime=2024-07-31T09:18:18.808Z level=INFO source=types.go:105 msg=\"inference compute\" id=GPU-31fa3c8c-f42e-bade-72ec-f936eb48ac45 library=cuda compute=8.6 driver=12.2 name=\"NVIDIA GeForce RTX 3090 Ti\" total=\"23.7 GiB\" available=\"17.2 GiB\"\r\ntime=2024-07-31T09:20:14.214Z level=INFO source=sched.go:701 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527 gpu=GPU-31fa3c8c-f42e-bade-72ec-f936eb48ac45 parallel=4 available=18469158912 required=\"737.9 MiB\"\r\ntime=2024-07-31T09:20:14.214Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=13 layers.offload=13 layers.split=\"\" memory.available=\"[17.2 GiB]\" memory.required.full=\"737.9 MiB\" memory.required.partial=\"737.9 MiB\" memory.required.kv=\"24.0 MiB\" memory.required.allocations=\"[737.9 MiB]\" memory.weights.total=\"186.5 MiB\" memory.weights.repeating=\"155.5 MiB\" memory.weights.nonrepeating=\"30.9 MiB\" memory.graph.full=\"48.0 MiB\" memory.graph.partial=\"48.0 MiB\"\r\ntime=2024-07-31T09:20:14.214Z level=INFO source=server.go:384 msg=\"starting llama server\" cmd=\"/tmp/ollama37639419/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527 --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 13 --parallel 4 --port 44985\"\r\ntime=2024-07-31T09:20:14.214Z level=INFO source=sched.go:437 msg=\"loaded runners\" count=1\r\ntime=2024-07-31T09:20:14.214Z level=INFO source=server.go:584 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-07-31T09:20:14.214Z level=INFO source=server.go:618 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nINFO [main] build info | build=1 commit=\"6eeaeba\" tid=\"127422522179584\" timestamp=1722417614\r\nINFO [main] system info | n_threads=8 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"127422522179584\" timestamp=1722417614 total_threads=16\r\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"15\" port=\"44985\" tid=\"127422522179584\" timestamp=1722417614\r\nllama_model_loader: loaded meta data with 22 key-value pairs and 197 tensors from /root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = bert\r\nllama_model_loader: - kv   1:                               general.name str              = Dmeta-embedding-zh\r\nllama_model_loader: - kv   2:                           bert.block_count u32              = 12\r\nllama_model_loader: - kv   3:                        bert.context_length u32              = 1024\r\nllama_model_loader: - kv   4:                      bert.embedding_length u32              = 768\r\nllama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 3072\r\nllama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12\r\nllama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000\r\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\r\nllama_model_loader: - kv   9:                      bert.attention.causal bool             = false\r\nllama_model_loader: - kv  10:                          bert.pooling_type u32              = 2\r\nllama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = bert\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,21128]   = [\"[PAD]\", \"[unused1]\", \"[unused2]\", \"...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,21128]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 100\r\nllama_model_loader: - kv  16:          tokenizer.ggml.seperator_token_id u32              = 102\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  18:                tokenizer.ggml.cls_token_id u32              = 101\r\nllama_model_loader: - kv  19:               tokenizer.ggml.mask_token_id u32              = 103\r\nllama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  21:            \r\n    tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - type  f32:  123 tensors\r\nllama_model_loader: - type  f16:   74 tensors\r\nllm_load_vocab: special tokens cache size = 5\r\nllm_load_vocab: token to piece cache size = 0.0769 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = bert\r\nllm_load_print_meta: vocab type       = WPM\r\nllm_load_print_meta: n_vocab          = 21128\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 1024\r\nllm_load_print_meta: n_embd           = 768\r\nllm_load_print_meta: n_layer          = 12\r\nllm_load_print_meta: n_head           = 12\r\nllm_load_print_meta: n_head_kv        = 12\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 768\r\nllm_load_print_meta: n_embd_v_gqa     = 768\r\nllm_load_print_meta: f_norm_eps       = 1.0e-12\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 3072\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 0\r\nllm_load_print_meta: pooling type     = 2\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 1024\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 109M\r\nllm_load_print_meta: model ftype      = F16\r\nllm_load_print_meta: model params     = 102.07 M\r\nllm_load_print_meta: model size       = 194.92 MiB (16.02 BPW) \r\nllm_load_print_meta: general.name     = Dmeta-embedding-zh\r\nllm_load_print_meta: BOS token        = 0 '[PAD]'\r\nllm_load_print_meta: EOS token        = 2 '[unused2]'\r\nllm_load_print_meta: UNK token        = 100 '[UNK]'\r\nllm_load_print_meta: SEP token        = 102 '[SEP]'\r\nllm_load_print_meta: PAD token        = 0 '[PAD]'\r\nllm_load_print_meta: CLS token        = 101 '[CLS]'\r\nllm_load_print_meta: MASK token       = 103 '[MASK]'\r\nllm_load_print_meta: LF token         = 0 '[PAD]'\r\nllm_load_print_meta: max token length = 48\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.16 MiB\r\nllm_load_tensors: offloading 12 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 13/13 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    32.46 MiB\r\nllm_load_tensors:      CUDA0 buffer size =   162.46 MiB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB\r\nllama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.00 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =    19.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =     4.00 MiB\r\nllama_new_context_with_model: graph nodes  = 429\r\nllama_new_context_with_model: graph splits = 2\r\ntime=2024-07-31T09:20:14.465Z level=INFO source=server.go:618 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nINFO [main] model loaded | tid=\"127422522179584\" timestamp=1722417614\r\ntime=2024-07-31T09:20:14.966Z level=INFO source=server.go:623 msg=\"llama runner started in 0.75 seconds\"\r\n[GIN] 2024/07/31 - 09:20:15 | 200 |  862.184786ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:20:15 | 200 |   91.260258ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\ntime=2024-07-31T09:20:15.383Z level=INFO source=routes.go:426 msg=\"embedding generation failed: do embedding request: Post \\\"http://127.0.0.1:44985/embedding\\\": EOF\"\r\n[GIN] 2024/07/31 - 09:20:15 | 500 |  140.114654ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\ntime=2024-07-31T09:23:45.923Z level=WARN source=server.go:503 msg=\"llama runner process no longer running\" sys=139 string=\"signal: segmentation fault (core dumped)\"\r\ntime=2024-07-31T09:23:50.993Z level=WARN source=sched.go:634 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.069197565 model=/root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527\r\ntime=2024-07-31T09:23:51.075Z level=INFO source=sched.go:701 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527 gpu=GPU-31fa3c8c-f42e-bade-72ec-f936eb48ac45 parallel=4 available=18469158912 required=\"737.9 MiB\"\r\ntime=2024-07-31T09:23:51.075Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=13 layers.offload=13 layers.split=\"\" memory.available=\"[17.2 GiB]\" memory.required.full=\"737.9 MiB\" memory.required.partial=\"737.9 MiB\" memory.required.kv=\"24.0 MiB\" memory.required.allocations=\"[737.9 MiB]\" memory.weights.total=\"186.5 MiB\" memory.weights.repeating=\"155.5 MiB\" memory.weights.nonrepeating=\"30.9 MiB\" memory.graph.full=\"48.0 MiB\" memory.graph.partial=\"48.0 MiB\"\r\ntime=2024-07-31T09:23:51.075Z level=INFO source=server.go:384 msg=\"starting llama server\" cmd=\"/tmp/ollama37639419/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527 --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 13 --parallel 4 --port 42155\"\r\ntime=2024-07-31T09:23:51.075Z level=INFO source=sched.go:437 msg=\"loaded runners\" count=1\r\ntime=2024-07-31T09:23:51.075Z level=INFO source=server.go:584 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-07-31T09:23:51.076Z level=INFO source=server.go:618 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nINFO [main] build info | build=1 commit=\"6eeaeba\" tid=\"131709034942464\" timestamp=1722417831\r\nINFO [main] system info | n_threads=8 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"131709034942464\" timestamp=1722417831 total_threads=16\r\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"15\" port=\"42155\" tid=\"131709034942464\" timestamp=1722417831\r\nllama_model_loader: loaded meta data with 22 key-value pairs and 197 tensors from /root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527 (version GGUF V3 (latest))\r\nllama_model_loader: D\r\numping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = bert\r\nllama_model_loader: - kv   1:                               general.name str              = Dmeta-embedding-zh\r\nllama_model_loader: - kv   2:                           bert.block_count u32              = 12\r\nllama_model_loader: - kv   3:                        bert.context_length u32              = 1024\r\nllama_model_loader: - kv   4:                      bert.embedding_length u32              = 768\r\nllama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 3072\r\nllama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12\r\nllama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000\r\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\r\nllama_model_loader: - kv   9:                      bert.attention.causal bool             = false\r\nllama_model_loader: - kv  10:                          bert.pooling_type u32              = 2\r\nllama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = bert\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,21128]   = [\"[PAD]\", \"[unused1]\", \"[unused2]\", \"...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,21128]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 100\r\nllama_model_loader: - kv  16:          tokenizer.ggml.seperator_token_id u32              = 102\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  18:                tokenizer.ggml.cls_token_id u32              = 101\r\nllama_model_loader: - kv  19:               tokenizer.ggml.mask_token_id u32              = 103\r\nllama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - type  f32:  123 tensors\r\nllama_model_loader: - type  f16:   74 tensors\r\nllm_load_vocab: special tokens cache size = 5\r\nllm_load_vocab: token to piece cache size = 0.0769 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = bert\r\nllm_load_print_meta: vocab type       = WPM\r\nllm_load_print_meta: n_vocab          = 21128\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 1024\r\nllm_load_print_meta: n_embd           = 768\r\nllm_load_print_meta: n_layer          = 12\r\nllm_load_print_meta: n_head           = 12\r\nllm_load_print_meta: n_head_kv        = 12\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 768\r\nllm_load_print_meta: n_embd_v_gqa     = 768\r\nllm_load_print_meta: f_norm_eps       = 1.0e-12\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 3072\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 0\r\nllm_load_print_meta: pooling type     = 2\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 1024\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 109M\r\nllm_load_print_meta: model ftype      = F16\r\nllm_load_print_meta: model params     = 102.07 M\r\nllm_load_print_meta: model size       = 194.92 MiB (16.02 BPW) \r\nllm_load_print_meta: general.name     = Dmeta-embedding-zh\r\nllm_load_print_meta: BOS token        = 0 '[PAD]'\r\nllm_load_print_meta: EOS token        = 2 '[unused2]'\r\nllm_load_print_meta: UNK token        = 100 '[UNK]'\r\nllm_load_print_meta: SEP token        = 102 '[SEP]'\r\nllm_load_print_meta: PAD token        = 0 '[PAD]'\r\nllm_load_print_meta: CLS token        = 101 '[CLS]'\r\nllm_load_print_meta: MASK token       = 103 '[MASK]'\r\nllm_load_print_meta: LF token         = 0 '[PAD]'\r\nllm_load_print_meta: max token length = 48\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.16 MiB\r\nllm_load_tensors: offloading 12 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 13/13 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    32.46 MiB\r\nllm_load_tensors:      CUDA0 buffer size =   162.46 MiB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB\r\nllama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.00 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =    19.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =     4.00 MiB\r\nllama_new_context_with_model: graph nodes  = 429\r\nllama_new_context_with_model: graph splits = 2\r\ntime=2024-07-31T09:23:51.243Z level=WARN source=sched.go:634 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.319657234 model=/root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527\r\ntime=2024-07-31T09:23:51.327Z level=INFO source=server.go:618 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nINFO [main] model loaded | tid=\"131709034942464\" timestamp=1722417831\r\ntime=2024-07-31T09:23:51.829Z level=INFO source=server.go:623 msg=\"llama runner started in 0.75 seconds\"\r\n[GIN] 2024/07/31 - 09:23:51 | 200 |  5.954027368s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:51 | 200 |  5.997875851s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:51 | 200 |  6.001301156s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:51 | 200 |   6.05401596s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  6.093406397s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  6.093515843s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  141.106871ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\nINFO [update_slots] input truncated | n_ctx=2048 n_erase=1989 n_keep=0 n_left=2048 n_shift=1024 tid=\"131709034942464\" timestamp=1722417832\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  156.396038ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  159.160468ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  155.371305ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  150.237024ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |   161.78585ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  158.374292ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\nINFO [update_slots] input truncated | n_ctx=2048 n_erase=1517 n_keep=0 n_left=2048 n_shift=1024 tid=\"131709034942464\" timestamp=1722417832\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  144.427285ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  192.549717ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  131.371235ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  185.844931ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  151.950066ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  141.888776ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  171.173954ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  130.251712ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\nINFO [update_slots] input truncated | n_ctx=2048 n_erase=1709 n_keep=0 n_left=2048 n_shift=1024 tid=\"131709034942464\" timestamp=1722417832\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  140.112505ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |   171.12123ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  227.184409ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  264.346952ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  189.302007ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  183.643992ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  165.703255ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 200 |  229.741451ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:52 | 500 |  303.282026ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\ntime=2024-07-31T09:23:52.825Z level=INFO source=routes.go:426 msg=\"embedding generation failed: do embedding request: Post \\\"http://127.0.0.1:42155/embedding\\\": EOF\"\r\ntime=2024-07-31T09:23:57.889Z level=WARN source=sched.go:634 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.063724982 model=/root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527\r\ntime=2024-07-31T09:23:57.975Z level=INFO source=sched.go:701 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527 gpu=GPU-31fa3c8c-f42e-bade-72ec-f936eb48ac45 parallel=4 available=18469158912 required=\"737.9 MiB\"\r\ntime=2024-07-31T09:23:57.975Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=13 layers.offload=13 layers.split=\"\" memory.available=\"[17.2 GiB]\" memory.required.full=\"737.9 MiB\" memory.required.partial=\"737.9 MiB\" memory.required.kv=\"24.0 MiB\" memory.required.allocations=\"[737.9 MiB]\" memory.weights.total=\"186.5 MiB\" memory.weights.repeating=\"155.5 MiB\" memory.weights.nonrepeating=\"30.9 MiB\" memory.graph.full=\"48.0 MiB\" memory.graph.partial=\"48.0 MiB\"\r\ntime=2024-07-31T09:23:57.975Z level=INFO source=server.go:384 msg=\"starting llama server\" cmd=\"/tmp/ollama37639419/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527 --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 1\r\n3 --parallel 4 --port 33967\"\r\ntime=2024-07-31T09:23:57.976Z level=INFO source=sched.go:437 msg=\"loaded runners\" count=1\r\ntime=2024-07-31T09:23:57.976Z level=INFO source=server.go:584 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-07-31T09:23:57.976Z level=INFO source=server.go:618 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nINFO [main] build info | build=1 commit=\"6eeaeba\" tid=\"125558191894528\" timestamp=1722417837\r\nINFO [main] system info | n_threads=8 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"125558191894528\" timestamp=1722417837 total_threads=16\r\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"15\" port=\"33967\" tid=\"125558191894528\" timestamp=1722417837\r\nllama_model_loader: loaded meta data with 22 key-value pairs and 197 tensors from /root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = bert\r\nllama_model_loader: - kv   1:                               general.name str              = Dmeta-embedding-zh\r\nllama_model_loader: - kv   2:                           bert.block_count u32              = 12\r\nllama_model_loader: - kv   3:                        bert.context_length u32              = 1024\r\nllama_model_loader: - kv   4:                      bert.embedding_length u32              = 768\r\nllama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 3072\r\nllama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12\r\nllama_model_loader: - kv   7:          bert.attention.layer_norm\r\n_epsilon f32              = 0.000000\r\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\r\nllama_model_loader: - kv   9:                      bert.attention.causal bool             = false\r\nllama_model_loader: - kv  10:                          bert.pooling_type u32              = 2\r\nllama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = bert\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,21128]   = [\"[PAD]\", \"[unused1]\", \"[unused2]\", \"...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,21128]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 100\r\nllama_model_loader: - kv  16:          tokenizer.ggml.seperator_token_id u32              = 102\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  18:                tokenizer.ggml.cls_token_id u32              = 101\r\nllama_model_loader: - kv  19:               tokenizer.ggml.mask_token_id u32              = 103\r\nllama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - type  f32:  123 tensors\r\nllama_model_loader: - type  f16:   74 tensors\r\nllm_load_vocab: special tokens cache size = 5\r\nllm_load_vocab: token to piece cache size = 0.0769 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = bert\r\nllm_load_print_meta: vocab type       = WPM\r\nllm_load_print_meta: n_vocab          = 21128\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 1024\r\nllm_load_print_meta: n_embd           = 768\r\nllm_load_print_meta: n_layer          = 12\r\nllm_load_print_meta: n_head           = 12\r\nllm_load_print_meta: n_head_kv        = 12\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 768\r\nllm_load_print_meta: n_embd_v_gqa     = 768\r\nllm_load_print_meta: f_norm_eps       = 1.0e-12\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 3072\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 0\r\nllm_load_print_meta: pooling type     = 2\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 1024\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 109M\r\nllm_load_print_meta: model ftype      = F16\r\nllm_load_print_meta: model params     = 102.07 M\r\nllm_load_print_meta: model size       = 194.92 MiB (16.02 BPW) \r\nllm_load_print_meta: general.name     = Dmeta-embedding-zh\r\nllm_load_print_meta: BOS token        = 0 '[PAD]'\r\nllm_load_print_meta: EOS token        = 2 '[unused2]'\r\nllm_load_print_meta: UNK token        = 100 '[UNK]'\r\nllm_load_print_meta: SEP token        = 102 '[SEP]'\r\nllm_load_print_meta: PAD token        = 0 '[PAD]'\r\nllm_load_print_meta: CLS token        = 101 '[CLS]'\r\nllm_load_print_meta: MASK token       = 103 '[MASK]'\r\nllm_load_print_meta: LF token         = 0 '[PAD]'\r\nllm_load_print_meta: max token length = 48\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.16 MiB\r\nllm_load_tensors: offloading 12 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 13/13 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    32.46 MiB\r\nllm_load_tensors:      CUDA0 buffer size =   162.46 MiB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB\r\nllama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.00 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =    19.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =     4.00 MiB\r\nllama_new_context_with_model: graph nodes  = 429\r\nllama_new_context_with_model: graph splits = 2\r\ntime=2024-07-31T09:23:58.139Z level=WARN source=sched.go:634 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.312995606 model=/root/.ollama/models/blobs/sha256-9b18b416fe232d5a834e15ce0d6cc353d7f6366423b8a7ef236db9ecee320527\r\ntime=2024-07-31T09:23:58.226Z level=INFO source=server.go:618 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nINFO [main] model loaded | tid=\"125558191894528\" timestamp=1722417838\r\ntime=2024-07-31T09:23:58.729Z level=INFO source=server.go:623 msg=\"llama runner started in 0.75 seconds\"\r\n[GIN] 2024/07/31 - 09:23:58 | 200 |  6.175518609s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:58 | 200 |  6.173129645s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/\r\n31 - 09:23:58 | 200 |  6.181901759s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:58 | 200 |  6.217999442s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:58 | 200 |  6.128390115s |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:58 | 200 |  139.275881ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:58 | 200 |  141.805964ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:58 | 200 |  147.553231ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |  147.626781ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |   90.649859ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |  134.183906ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |  100.703301ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |   76.093064ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |  139.579148ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |  195.963998ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |  184.951077ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |  204.863879ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |   93.607337ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |   92.691741ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |  122.460956ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:23:59 | 200 |  164.876363ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:26:50 | 200 |   93.430143ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:26:50 | 200 |    51.56662ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:26:\r\n50 | 200 |  139.845262ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:26:50 | 200 |   48.229681ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\nINFO [update_slots] input truncated | n_ctx=2048 n_erase=1522 n_keep=0 n_left=2048 n_shift=1024 tid=\"125558191894528\" timestamp=1722418010\r\n[GIN] 2024/07/31 - 09:26:50 | 200 |  103.527766ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\n[GIN] 2024/07/31 - 09:26:50 | 500 |  138.709641ms |    10.234.218.0 | POST     \"/api/embeddings\"\r\ntime=2024-07-31T09:26:50.849Z level=INFO source=routes.go:426 msg=\"embedding generation failed: do embedding request: Post \\\"http://127.0.0.1:33967/embedding\\\": EOF\"\r\n[GIN] 2024/07/31 - 09:37:35 | 200 |        19.4µs |       127.0.0.1 | GET      \"/api/version\"\n\n### OS\n\nDocker\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.3.1",
    "comments": [
      {
        "user": "royjhan",
        "body": "How did you produce this error? Do you get something similar when hitting api/embed?"
      },
      {
        "user": "lyh007",
        "body": "I Have the same problem"
      },
      {
        "user": "FellowTraveler",
        "body": "@yeexiangzhen1001 @lyh007 Can you provide more details about this issue? Were you running multiple concurrent embeddings? Or only a single one? Do they all fail even running 1-by-1? "
      }
    ]
  },
  {
    "issue_number": 8517,
    "title": "Missing tool support for DeepSeek-R1 Distillates based on Qwen",
    "author": "odrobnik",
    "state": "open",
    "created_at": "2025-01-21T11:10:11Z",
    "updated_at": "2025-06-07T08:16:47Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI tried `deepseek-r1:70B` and ollama claims that it doesn't support tools. \n\n```\n{\n  \"error\": {\n    \"message\": \"registry.ollama.ai/library/deepseek-r1:70B does not support tools\",\n    \"type\": \"api_error\",\n    \"param\": null,\n    \"code\": null\n  }\n```\n\nLooks to me like the template you have is missing the rules for tools.\n\nThe current Ollama template:\n\n```\n{{- if .System }}{{ .System }}{{ end }}\n{{- range $i, $_ := .Messages }}\n{{- $last := eq (len (slice $.Messages $i)) 1}}\n{{- if eq .Role \"user\" }}<｜User｜>{{ .Content }}\n{{- else if eq .Role \"assistant\" }}<｜Assistant｜>{{ .Content }}{{- if not $last }}<｜end▁of▁sentence｜>{{- end }}\n{{- end }}\n{{- if and $last (ne .Role \"assistant\") }}<｜Assistant｜>{{- end }}\n{{- end }}\n```\n\nThe template from https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF has tool calls stuff:\n\n```\n{% if not add_generation_prompt is defined %}\n    {% set add_generation_prompt = false %}\n{% endif %}\n{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}\n{%- for message in messages -%}\n    {%- if message['role'] == 'system' -%}\n        {% set ns.system_prompt = message['content'] %}\n    {%- endif -%}\n{%- endfor -%}\n{{ bos_token }}{{ ns.system_prompt }}\n{%- for message in messages -%}\n    {%- if message['role'] == 'user' -%}\n        {%- set ns.is_tool = false -%}\n        {{ '<｜User｜>' + message['content'] }}\n    {%- endif -%}\n    \n    {%- if message['role'] == 'assistant' and message['content'] is none -%}\n        {%- set ns.is_tool = false -%}\n        {%- for tool in message['tool_calls'] -%}\n            {%- if not ns.is_first -%}\n                {{ '<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>' }}\n                {%- set ns.is_first = true -%}\n            {%- else -%}\n                {{ '\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>' }}\n                {{ '<｜tool▁calls▁end｜><｜end▁of▁sentence｜>' }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- endif -%}\n    \n    {%- if message['role'] == 'assistant' and message['content'] is not none -%}\n        {%- if ns.is_tool -%}\n            {{ '<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>' }}\n            {%- set ns.is_tool = false -%}\n        {%- else -%}\n            {% set content = message['content'] %}\n            {% if '</think>' in content %}\n                {% set content = content.split('</think>')[-1] %}\n            {% endif %}\n            {{ '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n        {%- endif -%}\n    {%- endif -%}\n    \n    {%- if message['role'] == 'tool' -%}\n        {%- set ns.is_tool = true -%}\n        {%- if ns.is_output_first -%}\n            {{ '<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>' }}\n            {%- set ns.is_output_first = false -%}\n        {%- else -%}\n            {{ '\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>' }}\n        {%- endif -%}\n    {%- endif -%}\n{%- endfor -%}\n\n{% if ns.is_tool %}\n    {{ '<｜tool▁outputs▁end｜>' }}\n{% endif %}\n\n{% if add_generation_prompt and not ns.is_tool %}\n    {{ '<｜Assistant｜>' }}\n{% endif %}\n``` \n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n0.5.7",
    "comments": [
      {
        "user": "cbjuan",
        "body": "I observe the same error for the `deepseek-r1:1.5b` model\n\n```\n{'error': {'message': 'registry.ollama.ai/library/deepseek-r1:1.5b does not support tools', 'type': 'api_error', 'param': None, 'code': None}}\n```"
      },
      {
        "user": "IpslWon",
        "body": "The same issue is observed in deepseek-r1:8b\n\n**OS**  \nUbuntu 22.04.5 LTS\n**GPU**  \nNvidia\n**Ollama Version**  \n0.5.7"
      },
      {
        "user": "arraylabs",
        "body": "And deepseek-r1:14b\nWindows 11\nNvidia latest drivers\nLatest Ollama"
      }
    ]
  },
  {
    "issue_number": 11005,
    "title": "Memory Optimization for MoE Models via Sparse-Activation-Aware Techniques",
    "author": "gffice",
    "state": "open",
    "created_at": "2025-06-07T05:34:47Z",
    "updated_at": "2025-06-07T05:34:47Z",
    "labels": [
      "feature request"
    ],
    "body": "\n###  Problem Statement  \nMixture-of-Experts (MoE) models (e.g., Mixtral, Switch Transformers) activate only a subset of experts per token (`top-k routing`). However, current implementations often **load all experts into memory** during inference/training, leading to:  \n- Excessive VRAM consumption (e.g., 8x7B model requiring >90GB VRAM).  \n- Barrier to deployment on consumer hardware.  \n**Goal**: Exploit sparse activation to reduce memory footprint while preserving performance.  \n\n---\n\n###  Proposed Solutions  \n#### 1. **Dynamic Expert Loading**  \n- **Mechanism**: Load **only activated experts** into VRAM after routing.  \n- **Implementation**:  \n  - Split expert weights into independent blocks (e.g., per-expert `.safetensors`).  \n  - Post-routing, load `top-k` experts via memory-mapped I/O.  \n- **Benefit**: Reduces VRAM from `O(total_experts)` → `O(activated_experts)`.  \n\n#### 2. **Expert Offloading**  \n- **Strategy**:  \n  - **CPU Offloading**: Move inactive experts to RAM.  \n  - **Disk Offloading**: Store rarely used experts on SSD (via `mmap`).  \n- **Optimization**: Async I/O + LRU caching for prefetching.  \n\n#### 3. **Sparse Computation Graphs**  \n- **Approach**: Skip computations for inactive experts at kernel/compiler level.  \n  - Example: Build conditional execution paths in MLIR/TVM.  \n  - Fused `SparseMoELayer` operator:  \n    ```python  \n    # Pseudocode  \n    output = zeros_like(input)  \n    for expert_id in activated_experts:  # Only compute active experts  \n        output += expert[expert_id](input_slice)  \n    ```  \n\n#### 4. **Quantization-Aware Sparsity**  \n- **Hybrid Quantization**: Apply lower precision (e.g., `int8`) to inactive experts.  \n- **Compression**: Use pruning/structured sparsity for offloaded experts.  \n\n#### 5. **Distributed Expert Parallelism**  \n- **Design**: Shard experts across devices, transfer only inputs/outputs for activated experts.  \n- **Benefit**: Near-linear memory scaling with devices.  \n\n---\n\n### Expected Impact  \n| Technique               | VRAM Reduction | Use Case                   |  \n|-------------------------|----------------|----------------------------|  \n| Dynamic Expert Loading  | 50–90%         | Large MoE (≥8 experts)     |  \n| CPU/Disk Offloading     | 30–70%         | Resource-constrained envs |  \n| Sparse Computation      | 20–40%         | Compute-bound workloads    |  \n| Expert Parallelism      | 1/N scaling    | Multi-GPU/Node             |  \n\n---\n\n###  Challenges & Mitigations  \n| Challenge               | Mitigation                                  |  \n|-------------------------|---------------------------------------------|  \n| Dynamic loading latency | Async prefetch + pipeline parallelism       |  \n| Routing overhead        | Lightweight router (e.g., low-dim linear)   |  \n| System complexity       | Gradual rollout via `--sparse_moe` flag     |  \n\n---\n\n###  Requested OLLaMA Changes  \n1. **API Extensions**:  \n   - `load_expert(expert_id: int) → nn.Module` for dynamic weight management.  \n   - `offload_expert(expert_id: int, device='cpu')` context manager.  \n2. **Runtime Support**:  \n   - Conditional execution in computation graphs (e.g., JIT-traced branches).  \n3. **Documentation**:  \n   - Add `moe_memory_optimization.md` with benchmarks & code samples.  ",
    "comments": []
  },
  {
    "issue_number": 9133,
    "title": "Help Needed: Cloud Run Deployment Failing to Listen on PORT 8080 Google Cloud Run",
    "author": "nvip12041994",
    "state": "closed",
    "created_at": "2025-02-15T12:47:06Z",
    "updated_at": "2025-06-07T05:26:50Z",
    "labels": [],
    "body": "I’m experiencing an issue deploying my container image to Cloud Run. I followed the [GPU Gemma2 with Ollama tutorial](https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-ollama) and built my Docker image successfully. The container runs correctly locally, but when I deploy it to Cloud Run, it fails to start.\n\nDockerfile:\n\n> FROM ollama/ollama:0.5.11\n  ENV HOME=/home\n  ENV PORT=8080\n  ENV OLLAMA_HOST=0.0.0.0:${PORT}\n  ENV OLLAMA_MODELS=${HOME}/.ollama/models\n  ENV OLLAMA_DEBUG=false\n  ENV OLLAMA_KEEP_ALIVE=-1 \n  ENV MODEL=llama3.2\n  RUN ollama serve & sleep 5 && ollama pull $MODEL && ollama run llama3.2 \"\"\n  EXPOSE 8080\n  \nAfter build and successfully run on local and push image to cloud I use the following command to deploy to google cloud run:\n\n> gcloud beta run deploy ollama-llama \\\n  --image asia-southeast1-docker.pkg.dev/extract-financial-data/ollama-repo/ollama_llama:0.5.11 \\\n  --concurrency 4 \\\n  --cpu 8 \\\n  --set-env-vars OLLAMA_NUM_PARALLEL=4 \\\n  --gpu 1 \\\n  --gpu-type nvidia-l4 \\\n  --max-instances 1 \\\n  --memory 32Gi \\\n  --no-allow-unauthenticated \\\n  --no-cpu-throttling \\\n  --service-account ollama@extract-financial-data.iam.gserviceaccount.com \\\n  --timeout=600\n\n\nError Message:\n\n`\nDeploying container to Cloud Run service [ollama-llama] in project [extract-financial-data] region [asia-southeast1]\nX Deploying...\n  - Creating Revision...\n  . Routing traffic...\n  ✓ Setting IAM Policy...\nDeployment failed\nERROR: (gcloud.beta.run.deploy) Revision 'ollama-llama-00004-r8l' is not ready and cannot serve traffic. The user-provided container failed to start and listen on the port defined provided by the PORT=8080 environment variable within the allocated timeout. This can happen when the container port is misconfigured or if the timeout is too short. The health check timeout can be extended. Logs for this revision might contain more information.\n\nLogs URL: https://console.cloud.google.com/logs/viewer?project=extract-financial-data&resource=cloud_run_revision/service_name/ollama-llama/revision_name/ollama-llama-00004-r8l&advancedFilter=resource.type%3D%22cloud_run_revision%22%0Aresource.labels.service_name%3D%22ollama-llama%22%0Aresource.labels.revision_name%3D%22ollama-llama-00004-r8l%22\nFor more troubleshooting guidance, see https://cloud.google.com/run/docs/troubleshooting#container-failed-to-start\n`\n\nDetails:\n\n- I have set the container to listen on port 8080 (as required by Cloud Run) via the PORT environment variable.\n\n- The Dockerfile sets up the environment and starts the Ollama service, pulls the model, and runs the model.\n\n- The container works locally, but on Cloud Run the revision fails to become ready, suggesting the container isn’t listening on the specified port within the timeout period.\n\n- I’ve verified the port configuration and environment variables, but the issue persists.\n\nCould you please help me troubleshoot this issue? Any guidance on whether the problem might be related to port configuration, health check timeouts, or another aspect of the deployment would be greatly appreciated.\n\nThank you in advance for your help!\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "What's in the logs?"
      },
      {
        "user": "Programming-Seungwan",
        "body": "Did you resolved that problem?\n"
      },
      {
        "user": "petem24",
        "body": "+1"
      }
    ]
  },
  {
    "issue_number": 10980,
    "title": "Ollama Ignores System Prompts When Used with Qwen-Agent RAG Example",
    "author": "Harry-Up",
    "state": "closed",
    "created_at": "2025-06-05T12:20:27Z",
    "updated_at": "2025-06-07T01:28:07Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n**Environment**\n\n- ​​OS​​: Windows 11\n- ​​Ollama Version​​: 0.7.0\n- ​​Python​​: 3.11.11\n- ​​Qwen-Agent​​: 0.0.24\n\n**​​Tested Models**​​:\n\n- qwen3:30b (local via Ollama)\n- qwen3:14b (local via Ollama)\n- qwen3:14b-q8_0 (local via Ollama)\n- qwq (local via Ollama)\n\n**Problem Description**\nWhen running the examples/assistant_rag.py from [Qwen-Agent](https://github.com/QwenLM/qwen-agent) with ​​Ollama-served Qwen models​​, the models consistently ​​fail to process RAG tasks​​ (file content is ignored). This occurs ​​only when system prompts are used​​ in their standard position.\n\n**Key observations**:\n\n​​- **Workaround​​**: Moving the system prompt to the user prompt position resolves the issue.\n​​- **Comparison Tests​​**:\n✅ ​​Aliyun Bailian​​ models (qwen-latest-plus, qwen3:30b, qwen3:8b) handle RAG correctly with system prompts.\n✅ ​​vLLM Docker​​ (using unsloth/Qwen3-14B-unsloth-bnb-4bit) also processes RAG successfully.\n- ​​**Suspected Root Cause**​​: Ollama's template engine ​​may not inject system prompts correctly​​ for Qwen models.\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nWindows\n\n### GPU\n\nNvidia\n\n### CPU\n\nAMD\n\n### Ollama version\n\n0.7.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "> ​​Suspected Root Cause​​: Ollama's template engine ​​may not inject system prompts correctly​​ for Qwen models.\n\nMore likely is that the context is too small.  [Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) with `OLLAMA_DEBUG=1` will aid in debugging."
      },
      {
        "user": "Harry-Up",
        "body": "> > ​​Suspected Root Cause​​: Ollama's template engine ​​may not inject system prompts correctly​​ for Qwen models.\n> \n> More likely is that the context is too small. [Server logs](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md#how-to-troubleshoot-issues) with `OLLAMA_DEBUG=1` will aid in debugging.\n\n\nYeah, I update Ollama and try it again with `OLLAMA_DEBUG=1`. The problem keeps as well. Here is the server log.\n\n\ntime=2025-06-06T21:47:24.165+08:00 level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:DEBUG OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\weihb\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-06-06T21:47:24.169+08:00 level=INFO source=images.go:479 msg=\"total blobs: 13\"\ntime=2025-06-06T21:47:24.170+08:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\ntime=2025-06-06T21:47:24.170+08:00 level=INFO source=routes.go:1287 msg=\"Listening on [::]:11434 (version 0.9.0)\"\ntime=2025-06-06T21:47:24.170+08:00 level=DEBUG source=sched.go:108 msg=\"starting llm scheduler\"\ntime=2025-06-06T21:47:24.170+08:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-06T21:47:24.170+08:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-06-06T21:47:24.170+08:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=8 efficiency=0 threads=16\ntime=2025-06-06T21:47:24.170+08:00 level=DEBUG source=gpu.go:98 msg=\"searching for GPU discovery libraries for NVIDIA\"\ntime=2025-06-06T21:47:24.170+08:00 level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=nvml.dll\ntime=2025-06-06T21:47:24.170+08:00 level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\nvml.dll C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.8\\\\bin\\\\nvml.dll C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.8\\\\libnvvp\\\\nvml.dll C:\\\\WINDOWS\\\\system32\\\\nvml.dll C:\\\\WINDOWS\\\\nvml.dll C:\\\\WINDOWS\\\\System32\\\\Wbem\\\\nvml.dll C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\nvml.dll C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\nvml.dll C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Windows Performance Toolkit\\\\nvml.dll C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA App\\\\NvDLISR\\\\nvml.dll C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common\\\\nvml.dll C:\\\\Program Files\\\\Git\\\\cmd\\\\nvml.dll C:\\\\Program Files\\\\Docker\\\\Docker\\\\resources\\\\bin\\\\nvml.dll C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.43.34808\\\\bin\\\\Hostx64\\\\x64\\\\nvml.dll C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\bin\\\\10.0.22621.0\\\\x64\\\\nvml.dll C:\\\\MinGW\\\\bin\\\\nvml.dll C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2022.3.0\\\\nvml.dll C:\\\\Program Files\\\\dotnet\\\\nvml.dll C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\nvml.dll C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin\\\\nvml.dll C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvml.dll c:\\\\Windows\\\\System32\\\\nvml.dll]\"\ntime=2025-06-06T21:47:24.170+08:00 level=DEBUG source=gpu.go:529 msg=\"skipping PhysX cuda library path\" path=\"C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common\\\\nvml.dll\"\ntime=2025-06-06T21:47:24.171+08:00 level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=\"[C:\\\\WINDOWS\\\\system32\\\\nvml.dll c:\\\\Windows\\\\System32\\\\nvml.dll]\"\ntime=2025-06-06T21:47:24.180+08:00 level=DEBUG source=gpu.go:111 msg=\"nvidia-ml loaded\" library=C:\\WINDOWS\\system32\\nvml.dll\ntime=2025-06-06T21:47:24.180+08:00 level=DEBUG source=gpu.go:501 msg=\"Searching for GPU library\" name=nvcuda.dll\ntime=2025-06-06T21:47:24.180+08:00 level=DEBUG source=gpu.go:525 msg=\"gpu library search\" globs=\"[C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\nvcuda.dll C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.8\\\\bin\\\\nvcuda.dll C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.8\\\\libnvvp\\\\nvcuda.dll C:\\\\WINDOWS\\\\system32\\\\nvcuda.dll C:\\\\WINDOWS\\\\nvcuda.dll C:\\\\WINDOWS\\\\System32\\\\Wbem\\\\nvcuda.dll C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\nvcuda.dll C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\nvcuda.dll C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Windows Performance Toolkit\\\\nvcuda.dll C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA App\\\\NvDLISR\\\\nvcuda.dll C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common\\\\nvcuda.dll C:\\\\Program Files\\\\Git\\\\cmd\\\\nvcuda.dll C:\\\\Program Files\\\\Docker\\\\Docker\\\\resources\\\\bin\\\\nvcuda.dll C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.43.34808\\\\bin\\\\Hostx64\\\\x64\\\\nvcuda.dll C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\bin\\\\10.0.22621.0\\\\x64\\\\nvcuda.dll C:\\\\MinGW\\\\bin\\\\nvcuda.dll C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2022.3.0\\\\nvcuda.dll C:\\\\Program Files\\\\dotnet\\\\nvcuda.dll C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\nvcuda.dll C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin\\\\nvcuda.dll C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\nvcuda.dll c:\\\\windows\\\\system*\\\\nvcuda.dll]\"\ntime=2025-06-06T21:47:24.180+08:00 level=DEBUG source=gpu.go:529 msg=\"skipping PhysX cuda library path\" path=\"C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common\\\\nvcuda.dll\"\ntime=2025-06-06T21:47:24.180+08:00 level=DEBUG source=gpu.go:558 msg=\"discovered GPU libraries\" paths=[C:\\WINDOWS\\system32\\nvcuda.dll]\ninitializing C:\\WINDOWS\\system32\\nvcuda.dll\ndlsym: cuInit - 00007FFE357E5F80\ndlsym: cuDriverGetVersion - 00007FFE357E6020\ndlsym: cuDeviceGetCount - 00007FFE357E6816\ndlsym: cuDeviceGet - 00007FFE357E6810\ndlsym: cuDeviceGetAttribute - 00007FFE357E6170\ndlsym: cuDeviceGetUuid - 00007FFE357E6822\ndlsym: cuDeviceGetName - 00007FFE357E681C\ndlsym: cuCtxCreate_v3 - 00007FFE357E6894\ndlsym: cuMemGetInfo_v2 - 00007FFE357E6996\ndlsym: cuCtxDestroy - 00007FFE357E68A6\ncalling cuInit\ncalling cuDriverGetVersion\nraw version 0x2f30\nCUDA driver version: 12.8\ncalling cuDeviceGetCount\ndevice count 1\ntime=2025-06-06T21:47:24.188+08:00 level=DEBUG source=gpu.go:125 msg=\"detected GPUs\" count=1 library=C:\\WINDOWS\\system32\\nvcuda.dll\n[GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f] CUDA totalMem 32606mb\n[GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f] CUDA freeMem 30843mb\n[GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f] Compute Capability 12.0\ntime=2025-06-06T21:47:24.283+08:00 level=DEBUG source=amd_hip_windows.go:88 msg=hipDriverGetVersion version=60342560\ntime=2025-06-06T21:47:24.283+08:00 level=INFO source=amd_hip_windows.go:103 msg=\"AMD ROCm reports no devices found\"\ntime=2025-06-06T21:47:24.283+08:00 level=INFO source=amd_windows.go:49 msg=\"no compatible amdgpu devices detected\"\nreleasing cuda driver library\nreleasing nvml library\ntime=2025-06-06T21:47:24.283+08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f library=cuda variant=v12 compute=12.0 driver=12.8 name=\"NVIDIA GeForce RTX 5090 D\" total=\"31.8 GiB\" available=\"30.1 GiB\"\n[GIN] 2025/06/06 - 21:47:31 | 200 |       582.8µs |       127.0.0.1 | GET      \"/api/version\"\n[GIN] 2025/06/06 - 21:47:35 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/06 - 21:47:35 | 200 |     25.7005ms |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-06-06T21:48:23.357+08:00 level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-06T21:48:23.359+08:00 level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"31.6 GiB\" before.free=\"15.5 GiB\" before.free_swap=\"24.0 GiB\" now.total=\"31.6 GiB\" now.free=\"15.0 GiB\" now.free_swap=\"22.9 GiB\"\ntime=2025-06-06T21:48:23.375+08:00 level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f name=\"NVIDIA GeForce RTX 5090 D\" overhead=\"0 B\" before.total=\"31.8 GiB\" before.free=\"30.1 GiB\" now.total=\"31.8 GiB\" now.free=\"29.7 GiB\" now.used=\"2.2 GiB\"\nreleasing nvml library\ntime=2025-06-06T21:48:23.376+08:00 level=DEBUG source=sched.go:185 msg=\"updating default concurrency\" OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1\ntime=2025-06-06T21:48:23.383+08:00 level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-06T21:48:23.391+08:00 level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\ntime=2025-06-06T21:48:23.392+08:00 level=DEBUG source=sched.go:228 msg=\"loading first model\" model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac\ntime=2025-06-06T21:48:23.393+08:00 level=DEBUG source=memory.go:111 msg=evaluating library=cuda gpu_count=1 available=\"[29.7 GiB]\"\ntime=2025-06-06T21:48:23.393+08:00 level=DEBUG source=ggml.go:155 msg=\"key not found\" key=qwen3moe.vision.block_count default=0\ntime=2025-06-06T21:48:23.393+08:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac gpu=GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f parallel=2 available=31845679104 required=\"19.8 GiB\"\ntime=2025-06-06T21:48:23.393+08:00 level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"31.6 GiB\" before.free=\"15.0 GiB\" before.free_swap=\"22.9 GiB\" now.total=\"31.6 GiB\" now.free=\"15.0 GiB\" now.free_swap=\"22.9 GiB\"\ntime=2025-06-06T21:48:23.405+08:00 level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f name=\"NVIDIA GeForce RTX 5090 D\" overhead=\"0 B\" before.total=\"31.8 GiB\" before.free=\"29.7 GiB\" now.total=\"31.8 GiB\" now.free=\"29.7 GiB\" now.used=\"2.2 GiB\"\nreleasing nvml library\ntime=2025-06-06T21:48:23.406+08:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"31.6 GiB\" free=\"15.0 GiB\" free_swap=\"22.9 GiB\"\ntime=2025-06-06T21:48:23.406+08:00 level=DEBUG source=memory.go:111 msg=evaluating library=cuda gpu_count=1 available=\"[29.7 GiB]\"\ntime=2025-06-06T21:48:23.406+08:00 level=DEBUG source=ggml.go:155 msg=\"key not found\" key=qwen3moe.vision.block_count default=0\ntime=2025-06-06T21:48:23.406+08:00 level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=49 layers.split=\"\" memory.available=\"[29.7 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"19.8 GiB\" memory.required.partial=\"19.8 GiB\" memory.required.kv=\"768.0 MiB\" memory.required.allocations=\"[19.8 GiB]\" memory.weights.total=\"17.2 GiB\" memory.weights.repeating=\"16.9 GiB\" memory.weights.nonrepeating=\"243.4 MiB\" memory.graph.full=\"1.0 GiB\" memory.graph.partial=\"1.0 GiB\"\ntime=2025-06-06T21:48:23.406+08:00 level=DEBUG source=server.go:284 msg=\"compatible gpu libraries\" compatible=\"[cuda_v12 cuda_v11]\"\nllama_model_loader: loaded meta data with 31 key-value pairs and 579 tensors from C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3moe\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 30B A3B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 30B-A3B\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\nllama_model_loader: - kv   6:                       qwen3moe.block_count u32              = 48\nllama_model_loader: - kv   7:                    qwen3moe.context_length u32              = 40960\nllama_model_loader: - kv   8:                  qwen3moe.embedding_length u32              = 2048\nllama_model_loader: - kv   9:               qwen3moe.feed_forward_length u32              = 6144\nllama_model_loader: - kv  10:              qwen3moe.attention.head_count u32              = 32\nllama_model_loader: - kv  11:           qwen3moe.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  12:                    qwen3moe.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:  qwen3moe.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                 qwen3moe.expert_used_count u32              = 8\nllama_model_loader: - kv  15:              qwen3moe.attention.key_length u32              = 128\nllama_model_loader: - kv  16:            qwen3moe.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                      qwen3moe.expert_count u32              = 128\nllama_model_loader: - kv  18:        qwen3moe.expert_feed_forward_length u32              = 768\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - kv  30:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type  f16:   48 tensors\nllama_model_loader: - type q4_K:  265 tensors\nllama_model_loader: - type q6_K:   25 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 17.34 GiB (4.88 BPW) \ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nload: control token: 151656 '<|video_pad|>' is not marked as EOG\nload: control token: 151655 '<|image_pad|>' is not marked as EOG\nload: control token: 151653 '<|vision_end|>' is not marked as EOG\nload: control token: 151652 '<|vision_start|>' is not marked as EOG\nload: control token: 151651 '<|quad_end|>' is not marked as EOG\nload: control token: 151649 '<|box_end|>' is not marked as EOG\nload: control token: 151648 '<|box_start|>' is not marked as EOG\nload: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nload: control token: 151644 '<|im_start|>' is not marked as EOG\nload: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nload: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nload: control token: 151660 '<|fim_middle|>' is not marked as EOG\nload: control token: 151654 '<|vision_pad|>' is not marked as EOG\nload: control token: 151650 '<|quad_start|>' is not marked as EOG\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3moe\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 30.53 B\nprint_info: general.name     = Qwen3 30B A3B\nprint_info: n_ff_exp         = 0\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-06T21:48:23.529+08:00 level=DEBUG source=server.go:360 msg=\"adding gpu library\" path=C:\\Users\\weihb\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\ntime=2025-06-06T21:48:23.529+08:00 level=DEBUG source=server.go:367 msg=\"adding gpu dependency paths\" paths=[C:\\Users\\weihb\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12]\ntime=2025-06-06T21:48:23.529+08:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model C:\\\\Users\\\\weihb\\\\.ollama\\\\models\\\\blobs\\\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac --ctx-size 8192 --batch-size 512 --n-gpu-layers 49 --threads 8 --no-mmap --parallel 2 --port 53373\"\ntime=2025-06-06T21:48:23.529+08:00 level=DEBUG source=server.go:432 msg=subprocess CUDA_PATH=\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.8\" OLLAMA_API_KEY=!@#$1234qaZ OLLAMA_DEBUG=1 OLLAMA_HOST=0.0.0.0:11434 OLLAMA_MAX_LOADED_MODELS=3 PATH=\"C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v12;C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\\\\cuda_v12;C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.8\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.8\\\\libnvvp;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Windows Performance Toolkit\\\\;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA App\\\\NvDLISR;C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\Docker\\\\Docker\\\\resources\\\\bin;C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.43.34808\\\\bin\\\\Hostx64\\\\x64;C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\bin\\\\10.0.22621.0\\\\x64;C:\\\\MinGW\\\\bin;C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2022.3.0\\\\;C:\\\\Program Files\\\\dotnet\\\\;C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama;C:\\\\Users\\\\weihb\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\lib\\\\ollama\" OLLAMA_LIBRARY_PATH=C:\\Users\\weihb\\AppData\\Local\\Programs\\Ollama\\lib\\ollama;C:\\Users\\weihb\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12 CUDA_VISIBLE_DEVICES=GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f\ntime=2025-06-06T21:48:23.532+08:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-06T21:48:23.532+08:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-06T21:48:23.533+08:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-06-06T21:48:23.556+08:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\ntime=2025-06-06T21:48:23.559+08:00 level=DEBUG source=ggml.go:94 msg=\"ggml backend load all from path\" path=C:\\Users\\weihb\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\nload_backend: loaded CPU backend from C:\\Users\\weihb\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-icelake.dll\ntime=2025-06-06T21:48:23.661+08:00 level=DEBUG source=ggml.go:94 msg=\"ggml backend load all from path\" path=C:\\Users\\weihb\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 5090 D, compute capability 12.0, VMM: yes\nload_backend: loaded CUDA backend from C:\\Users\\weihb\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12\\ggml-cuda.dll\ntime=2025-06-06T21:48:39.340+08:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)\ntime=2025-06-06T21:48:39.341+08:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:53373\"\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5090 D) - 30843 MiB free\nllama_model_loader: loaded meta data with 31 key-value pairs and 579 tensors from C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3moe\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 30B A3B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\nllama_model_loader: - kv   4:                         general.size_label str              = 30B-A3B\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\nllama_model_loader: - kv   6:                       qwen3moe.block_count u32              = 48\nllama_model_loader: - kv   7:                    qwen3moe.context_length u32              = 40960\nllama_model_loader: - kv   8:                  qwen3moe.embedding_length u32              = 2048\nllama_model_loader: - kv   9:               qwen3moe.feed_forward_length u32              = 6144\nllama_model_loader: - kv  10:              qwen3moe.attention.head_count u32              = 32\nllama_model_loader: - kv  11:           qwen3moe.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  12:                    qwen3moe.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:  qwen3moe.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                 qwen3moe.expert_used_count u32              = 8\nllama_model_loader: - kv  15:              qwen3moe.attention.key_length u32              = 128\nllama_model_loader: - kv  16:            qwen3moe.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                      qwen3moe.expert_count u32              = 128\nllama_model_loader: - kv  18:        qwen3moe.expert_feed_forward_length u32              = 768\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - kv  30:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type  f16:   48 tensors\nllama_model_loader: - type q4_K:  265 tensors\nllama_model_loader: - type q6_K:   25 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 17.34 GiB (4.88 BPW) \ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nload: control token: 151656 '<|video_pad|>' is not marked as EOG\nload: control token: 151655 '<|image_pad|>' is not marked as EOG\nload: control token: 151653 '<|vision_end|>' is not marked as EOG\nload: control token: 151652 '<|vision_start|>' is not marked as EOG\nload: control token: 151651 '<|quad_end|>' is not marked as EOG\nload: control token: 151649 '<|box_end|>' is not marked as EOG\nload: control token: 151648 '<|box_start|>' is not marked as EOG\nload: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nload: control token: 151644 '<|im_start|>' is not marked as EOG\nload: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nload: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nload: control token: 151660 '<|fim_middle|>' is not marked as EOG\nload: control token: 151654 '<|vision_pad|>' is not marked as EOG\nload: control token: 151650 '<|quad_start|>' is not marked as EOG\nload: special tokens cache size = 26\ntime=2025-06-06T21:48:39.553+08:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3moe\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 48\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 6144\nprint_info: n_expert         = 128\nprint_info: n_expert_used    = 8\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 30B.A3B\nprint_info: model params     = 30.53 B\nprint_info: general.name     = Qwen3 30B A3B\nprint_info: n_ff_exp         = 768\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: layer   0 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   1 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   2 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   3 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   4 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   5 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   6 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   7 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   8 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   9 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  10 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  11 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  12 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  13 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  14 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  15 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  16 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  17 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  18 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  19 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  20 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  21 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  22 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  23 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  24 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  25 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  26 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  27 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  28 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  29 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  30 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  31 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  32 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  33 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  34 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  35 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  36 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  37 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  38 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  39 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  40 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  41 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  42 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  43 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  44 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  45 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  46 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  47 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  48 assigned to device CUDA0, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\nload_tensors: offloading 48 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 49/49 layers to GPU\nload_tensors:        CUDA0 model buffer size = 17587.24 MiB\nload_tensors:          CPU model buffer size =   166.92 MiB\nload_all_data: using async uploads for device CUDA0, buffer type CUDA0, backend CUDA0\ntime=2025-06-06T21:48:39.803+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.00\"\ntime=2025-06-06T21:48:40.053+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.03\"\ntime=2025-06-06T21:48:40.304+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.06\"\ntime=2025-06-06T21:48:40.554+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.09\"\ntime=2025-06-06T21:48:40.804+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.11\"\ntime=2025-06-06T21:48:41.054+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.14\"\ntime=2025-06-06T21:48:41.304+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.18\"\ntime=2025-06-06T21:48:41.555+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.20\"\ntime=2025-06-06T21:48:41.805+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.22\"\ntime=2025-06-06T21:48:42.056+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.25\"\ntime=2025-06-06T21:48:42.306+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.28\"\ntime=2025-06-06T21:48:42.556+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.31\"\ntime=2025-06-06T21:48:42.806+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.33\"\ntime=2025-06-06T21:48:43.056+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.35\"\ntime=2025-06-06T21:48:43.307+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.38\"\ntime=2025-06-06T21:48:43.557+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.40\"\ntime=2025-06-06T21:48:43.807+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.43\"\ntime=2025-06-06T21:48:44.057+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.46\"\ntime=2025-06-06T21:48:44.308+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.49\"\ntime=2025-06-06T21:48:44.558+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.52\"\ntime=2025-06-06T21:48:44.809+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.55\"\ntime=2025-06-06T21:48:45.059+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.58\"\ntime=2025-06-06T21:48:45.309+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.61\"\ntime=2025-06-06T21:48:45.559+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.63\"\ntime=2025-06-06T21:48:45.810+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.67\"\ntime=2025-06-06T21:48:46.060+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.69\"\ntime=2025-06-06T21:48:46.311+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.72\"\ntime=2025-06-06T21:48:46.561+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.75\"\ntime=2025-06-06T21:48:46.811+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.78\"\ntime=2025-06-06T21:48:47.062+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.80\"\ntime=2025-06-06T21:48:47.312+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.82\"\ntime=2025-06-06T21:48:47.563+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.85\"\ntime=2025-06-06T21:48:47.813+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.88\"\ntime=2025-06-06T21:48:48.063+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.91\"\ntime=2025-06-06T21:48:48.314+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.93\"\ntime=2025-06-06T21:48:48.564+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.97\"\ntime=2025-06-06T21:48:48.814+08:00 level=DEBUG source=server.go:636 msg=\"model load progress 0.98\"\nload_all_data: no device found for buffer type CPU for async uploads\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 2\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 1024\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nset_abort_callback: call\nllama_context:  CUDA_Host  output buffer size =     1.17 MiB\ncreate_memory: n_ctx = 8192 (padded)\nllama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1, padding = 32\nllama_kv_cache_unified: layer   0: dev = CUDA0\nllama_kv_cache_unified: layer   1: dev = CUDA0\nllama_kv_cache_unified: layer   2: dev = CUDA0\nllama_kv_cache_unified: layer   3: dev = CUDA0\nllama_kv_cache_unified: layer   4: dev = CUDA0\nllama_kv_cache_unified: layer   5: dev = CUDA0\nllama_kv_cache_unified: layer   6: dev = CUDA0\nllama_kv_cache_unified: layer   7: dev = CUDA0\nllama_kv_cache_unified: layer   8: dev = CUDA0\nllama_kv_cache_unified: layer   9: dev = CUDA0\nllama_kv_cache_unified: layer  10: dev = CUDA0\nllama_kv_cache_unified: layer  11: dev = CUDA0\nllama_kv_cache_unified: layer  12: dev = CUDA0\nllama_kv_cache_unified: layer  13: dev = CUDA0\nllama_kv_cache_unified: layer  14: dev = CUDA0\nllama_kv_cache_unified: layer  15: dev = CUDA0\nllama_kv_cache_unified: layer  16: dev = CUDA0\nllama_kv_cache_unified: layer  17: dev = CUDA0\nllama_kv_cache_unified: layer  18: dev = CUDA0\nllama_kv_cache_unified: layer  19: dev = CUDA0\nllama_kv_cache_unified: layer  20: dev = CUDA0\nllama_kv_cache_unified: layer  21: dev = CUDA0\nllama_kv_cache_unified: layer  22: dev = CUDA0\nllama_kv_cache_unified: layer  23: dev = CUDA0\nllama_kv_cache_unified: layer  24: dev = CUDA0\nllama_kv_cache_unified: layer  25: dev = CUDA0\nllama_kv_cache_unified: layer  26: dev = CUDA0\nllama_kv_cache_unified: layer  27: dev = CUDA0\nllama_kv_cache_unified: layer  28: dev = CUDA0\nllama_kv_cache_unified: layer  29: dev = CUDA0\nllama_kv_cache_unified: layer  30: dev = CUDA0\nllama_kv_cache_unified: layer  31: dev = CUDA0\nllama_kv_cache_unified: layer  32: dev = CUDA0\nllama_kv_cache_unified: layer  33: dev = CUDA0\nllama_kv_cache_unified: layer  34: dev = CUDA0\nllama_kv_cache_unified: layer  35: dev = CUDA0\nllama_kv_cache_unified: layer  36: dev = CUDA0\nllama_kv_cache_unified: layer  37: dev = CUDA0\nllama_kv_cache_unified: layer  38: dev = CUDA0\nllama_kv_cache_unified: layer  39: dev = CUDA0\nllama_kv_cache_unified: layer  40: dev = CUDA0\nllama_kv_cache_unified: layer  41: dev = CUDA0\nllama_kv_cache_unified: layer  42: dev = CUDA0\nllama_kv_cache_unified: layer  43: dev = CUDA0\nllama_kv_cache_unified: layer  44: dev = CUDA0\nllama_kv_cache_unified: layer  45: dev = CUDA0\nllama_kv_cache_unified: layer  46: dev = CUDA0\nllama_kv_cache_unified: layer  47: dev = CUDA0\nllama_kv_cache_unified:      CUDA0 KV buffer size =   768.00 MiB\nllama_kv_cache_unified: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 2\nllama_context: max_nodes = 65536\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\nllama_context: reserving graph for n_tokens = 512, n_seqs = 1\nllama_context: reserving graph for n_tokens = 1, n_seqs = 1\nllama_context: reserving graph for n_tokens = 512, n_seqs = 1\nllama_context:      CUDA0 compute buffer size =   552.00 MiB\nllama_context:  CUDA_Host compute buffer size =    20.01 MiB\nllama_context: graph nodes  = 3126\nllama_context: graph splits = 2\ntime=2025-06-06T21:48:49.065+08:00 level=INFO source=server.go:630 msg=\"llama runner started in 25.53 seconds\"\ntime=2025-06-06T21:48:49.065+08:00 level=DEBUG source=sched.go:495 msg=\"finished setting up\" runner.name=registry.ollama.ai/library/qwen3:30b runner.inference=cuda runner.devices=1 runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac runner.num_ctx=8192\ntime=2025-06-06T21:48:49.088+08:00 level=DEBUG source=prompt.go:66 msg=\"truncating input messages which exceed context length\" truncated=2\ntime=2025-06-06T21:48:49.088+08:00 level=DEBUG source=server.go:729 msg=\"completion request\" images=0 prompt=104 format=\"\"\ntime=2025-06-06T21:48:49.090+08:00 level=DEBUG source=cache.go:104 msg=\"loading cache slot\" id=0 cache=0 prompt=30 used=0 remaining=30\n[GIN] 2025/06/06 - 21:48:51 | 200 |   28.6746475s |       127.0.0.1 | POST     \"/v1/chat/completions\"\ntime=2025-06-06T21:48:51.995+08:00 level=DEBUG source=sched.go:503 msg=\"context for request finished\"\ntime=2025-06-06T21:48:51.996+08:00 level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/qwen3:30b runner.inference=cuda runner.devices=1 runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac runner.num_ctx=8192 duration=5m0s\ntime=2025-06-06T21:48:51.996+08:00 level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/qwen3:30b runner.inference=cuda runner.devices=1 runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac runner.num_ctx=8192 refCount=0\ntime=2025-06-06T21:53:52.004+08:00 level=DEBUG source=sched.go:345 msg=\"timer expired, expiring to unload\" runner.name=registry.ollama.ai/library/qwen3:30b runner.inference=cuda runner.devices=1 runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac runner.num_ctx=8192\ntime=2025-06-06T21:53:52.004+08:00 level=DEBUG source=sched.go:364 msg=\"runner expired event received\" runner.name=registry.ollama.ai/library/qwen3:30b runner.inference=cuda runner.devices=1 runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac runner.num_ctx=8192\ntime=2025-06-06T21:53:52.005+08:00 level=DEBUG source=sched.go:379 msg=\"got lock to unload expired event\" runner.name=registry.ollama.ai/library/qwen3:30b runner.inference=cuda runner.devices=1 runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac runner.num_ctx=8192\ntime=2025-06-06T21:53:52.005+08:00 level=DEBUG source=sched.go:402 msg=\"starting background wait for VRAM recovery\" runner.name=registry.ollama.ai/library/qwen3:30b runner.inference=cuda runner.devices=1 runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac runner.num_ctx=8192\ntime=2025-06-06T21:53:52.005+08:00 level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"31.6 GiB\" before.free=\"15.0 GiB\" before.free_swap=\"22.9 GiB\" now.total=\"31.6 GiB\" now.free=\"16.8 GiB\" now.free_swap=\"3.0 GiB\"\ntime=2025-06-06T21:53:52.021+08:00 level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f name=\"NVIDIA GeForce RTX 5090 D\" overhead=\"0 B\" before.total=\"31.8 GiB\" before.free=\"29.7 GiB\" now.total=\"31.8 GiB\" now.free=\"10.6 GiB\" now.used=\"21.3 GiB\"\nreleasing nvml library\ntime=2025-06-06T21:53:52.039+08:00 level=DEBUG source=server.go:1023 msg=\"stopping llama server\" pid=27056\ntime=2025-06-06T21:53:52.039+08:00 level=DEBUG source=server.go:1029 msg=\"waiting for llama server to exit\" pid=27056\ntime=2025-06-06T21:53:52.164+08:00 level=DEBUG source=server.go:1033 msg=\"llama server stopped\" pid=27056\ntime=2025-06-06T21:53:52.164+08:00 level=DEBUG source=sched.go:407 msg=\"runner terminated and removed from list, blocking for VRAM recovery\" runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac\ntime=2025-06-06T21:53:52.272+08:00 level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"31.6 GiB\" before.free=\"16.8 GiB\" before.free_swap=\"3.0 GiB\" now.total=\"31.6 GiB\" now.free=\"17.6 GiB\" now.free_swap=\"23.5 GiB\"\ntime=2025-06-06T21:53:52.285+08:00 level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-d2bc81d1-bd18-101d-0f21-85f4096fdc9f name=\"NVIDIA GeForce RTX 5090 D\" overhead=\"0 B\" before.total=\"31.8 GiB\" before.free=\"10.6 GiB\" now.total=\"31.8 GiB\" now.free=\"29.6 GiB\" now.used=\"2.2 GiB\"\nreleasing nvml library\ntime=2025-06-06T21:53:52.286+08:00 level=DEBUG source=sched.go:700 msg=\"gpu VRAM free memory converged after 0.28 seconds\" runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac\ntime=2025-06-06T21:53:52.286+08:00 level=DEBUG source=sched.go:410 msg=\"sending an unloaded event\" runner.size=\"19.8 GiB\" runner.vram=\"19.8 GiB\" runner.parallel=2 runner.pid=27056 runner.model=C:\\Users\\weihb\\.ollama\\models\\blobs\\sha256-e9183b5c18a0cf736578c1e3d1cbd4b7e98e3ad3be6176b68c20f156d54a07ac\ntime=2025-06-06T21:53:52.286+08:00 level=DEBUG source=sched.go:312 msg=\"ignoring unload event with no pending requests\"\n"
      },
      {
        "user": "rick-github",
        "body": "You have the default context length of 4096 tokens: `OLLAMA_CONTEXT_LENGTH:4096`.\n```\ntime=2025-06-06T21:48:49.088+08:00 level=DEBUG source=prompt.go:66 msg=\"truncating input messages which exceed context length\" truncated=2\n```\nThe prompt is truncated, removing the system prompt.\n\n Increase the [context length](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-specify-the-context-window-size)."
      }
    ]
  },
  {
    "issue_number": 10975,
    "title": "Log request/response payload content",
    "author": "phucly01",
    "state": "closed",
    "created_at": "2025-06-05T02:32:38Z",
    "updated_at": "2025-06-07T00:36:01Z",
    "labels": [
      "feature request"
    ],
    "body": "I tried to troubleshoot a tool calling problem and wanted to see the payload contents of the http request.  I have turned on DEBUG but it doesn't really help. The log only show http code and uri. \n\nIt would be helpful if there is a way to turn on the details log where http requests and responses can be printed in the log along with payload contents.",
    "comments": [
      {
        "user": "rick-github",
        "body": "Setting `OLLAMA_DEBUG=2` will show the prompt as well.\n\nLogging HTTP requests and responses is usually done with an external tool.  For example, mitmdump:\n```yaml\nservices:\n  ollama-backend:\n    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}\n    volumes:\n      - ${OLLAMA_MODELS-./ollama}:/root/.ollama\n    environment:\n      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE--1}\n      - OLLAMA_DEBUG=${OLLAMA_DEBUG-2}\n\n  ollama-mitmproxy:\n    image: mitmproxy/mitmproxy\n    command: [ \"/usr/local/bin/mitmdump\", \"--flow-detail\", \"4\", \"--mode\", \"reverse:http://ollama-backend:11434\" ]\n    ports:\n      - 11434:8080\n```\nI use docker here as a environment management tool but this can be run from the command line as well.  The downside of this is that mitmproxy buffers the output so the streaming effect is lost.\n\nIn non-Windows environments `nc` can be used as a mitm proxy to record traffic:\n```sh\n# start ollama listening on a different port\nOLLAMA_HOST=localhost:11435 ollama serve\n# run a process on the original port that fowards traffic to ollama and records in a file\n! [ -p pipe ] && mknod pipe p\nnc -ln -p 11434 < pipe | tee -a traffic-in.log | nc localhost 11435 | tee -a traffic-out.log > pipe\n```\nOther alternatives are `tcpdump` and `tcpflow` which intercept the traffic without having to set up a proxy.  The output of those requires some post-processing."
      },
      {
        "user": "phucly01",
        "body": "Thanks, will give it a try.  This will be a wild goose chase."
      }
    ]
  },
  {
    "issue_number": 335,
    "title": "Model import/export",
    "author": "mikeroySoft",
    "state": "open",
    "created_at": "2023-08-11T19:26:46Z",
    "updated_at": "2025-06-06T23:21:36Z",
    "labels": [
      "feature request"
    ],
    "body": "When using large models like Llama2:70b, the download files are quite big.\r\nAs a user with multiple local systems, having to `ollama pull` on every device means that much more bandwidth and time spent.\r\nIt would be great if we could download the model once and then export/import it to other ollama clients in the office without pulling it from the internet.\r\n\r\nExample:\r\nOn the first device, we would do:\r\n\r\n`ollama pull llama2:70b`\r\n\r\n`ollama export llama2:70b /Volumes/MyUSB/llama2_70b-local.ollama_model` \r\n\r\n\r\n\r\nThen we would take MyUSB over to another device and do:\r\n\r\n`ollama import /Volumes/MyUSB/llama2_70b-local.ollama_model` \r\n\r\n`ollama run llama2:local-70b` or `ollama run llama2-local:70b` or even just `ollama run llama2_70b-local`\r\n\r\nI'm obviously not sure about the naming structure here, but I hope I've conveyed the problem and thought process.\r\n\r\nThanks for the fantastic project!",
    "comments": [
      {
        "user": "mikeroySoft",
        "body": "Just to note, the reasoning I have with naming the import something other than 'llama2:70b' when you do `run` is that I didn't want to conflict with the main one available from the web."
      },
      {
        "user": "jmorganca",
        "body": "@mikeroySoft very cool idea. Do you have thoughts on how the format should work? Ideally it should contain both the manifest, blobs, be a single file and easy to understand."
      },
      {
        "user": "mikeroySoft",
        "body": "So I was working with this a bit last night, and I managed to get `ollama export` and `ollama import` doing something useful, but I'm not sure if my logic is sound. (I haven't grokked the entire codebase yet to know what existing code I should be reusing).\r\n\r\nMy thought was just to gather the model and manifests using server.ParseModelPath and GetManifestPath, tar them up, add a `.ollamabundle` extension to the output, and save it on the filesystem.\r\n\r\n`ollama export llama2:70b /Volumes/MyUSB/myLlama.ollamabundle`\r\n\r\nFor my Import POC we run: `ollama import /Volumes/MyUSB/myLlama.ollamabundle`, and it drops the sha256:<foo> blob/s into ~/.ollama/models/blobs, and saves respective manifest.json to, for example, `7b` within the ~/.ollama/models path. (well actually it currently just saves the manifest.json into ~/.ollama/models, I actually just manually `mv`'d it to `~/.ollama/models/manifests/registry.ollama.ai/library/llama2/myLlama`, but I'll update the logic there if the strategy is sound)\r\n\r\nSo our list then shows up as `llama2:myLlama`, and we'd run `ollama run llama2:myLlama`, but I'm not sure if that's more appropriate than `ollama run myLlama`. \r\n\r\nLike, is having the distinction in the model tag better than in the name? \r\n\r\nIn any case, is that a sane approach? Am I missing something glaring?\r\n"
      }
    ]
  },
  {
    "issue_number": 10988,
    "title": "Upgrade from EOL Ubuntu 20.04 for Docker image",
    "author": "kth8",
    "state": "open",
    "created_at": "2025-06-05T18:33:51Z",
    "updated_at": "2025-06-06T22:47:29Z",
    "labels": [
      "docker"
    ],
    "body": "The `ollama/ollama` Docker image is currently based on Ubuntu 20.04 LTS (Focal Fossa). Ubuntu 20.04 LTS officially reached its End of Standard Support on 31 May 2025. Running a base image that is no longer receiving standard security updates poses significant risks to users. I would like to suggest upgrading the base image for `ollama/ollama` to a currently supported Long Term Support (LTS) version of Ubuntu. Ubuntu 24.04 LTS (Noble Numbat) is the latest LTS release and offers a five-year standard support window until April 2029.",
    "comments": [
      {
        "user": "rick-github",
        "body": "#9679"
      },
      {
        "user": "kth8",
        "body": "Now that the end of support date has passed, I think this issue is much more urgent."
      },
      {
        "user": "duck-5",
        "body": "Please close this issue and continue on #9679 "
      }
    ]
  },
  {
    "issue_number": 9679,
    "title": "Update docker base image from Ubuntu 20.04 to 24.04 LTS",
    "author": "vrampal",
    "state": "open",
    "created_at": "2025-03-12T09:07:30Z",
    "updated_at": "2025-06-06T21:07:10Z",
    "labels": [
      "feature request",
      "docker"
    ],
    "body": "The current docker base image is still Ubuntu 20.04 LTS that will be retired in a few days.\nAn upgrade to Ubuntu 24.04 LTS would be much appreciated.",
    "comments": []
  },
  {
    "issue_number": 11000,
    "title": "0.90 issue with Modelfile",
    "author": "goactiongo",
    "state": "closed",
    "created_at": "2025-06-06T19:22:21Z",
    "updated_at": "2025-06-06T20:30:29Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI  create the new model with modelfile edited with following commands:\n```\nollama show qwen3:32b  --modelfile > Modelfile\necho PARAMETER num_predict -1 >> Modelfile\necho PARAMETER num_ctx 125000 >> Modelfile\nollama create qwen3:32b_125k -f Modelfile \n```\nSeems that the new model didn't work well with log followed,pls\n\n![Image](https://github.com/user-attachments/assets/e6116ec2-3e17-4ec2-81ca-90f5a1332751)\n\n[qwen3_32b.txt](https://github.com/user-attachments/files/20633261/qwen3_32b.txt)\n[qwen3_32b_numctx125k.txt](https://github.com/user-attachments/files/20633260/qwen3_32b_numctx125k.txt)\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "rick-github",
        "body": "What does \"didn't work well\" mean?  Didn't answer prompts?  Answered incorrectly? Too slow?"
      },
      {
        "user": "goactiongo",
        "body": "Jun 06 19:12:13 ai001 ollama[590926]: load_tensors: loading model tensors, this can take a while... (mmap = false)\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   0 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   1 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   2 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   3 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   4 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   5 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   6 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   7 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   8 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer   9 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  10 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  11 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  12 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  13 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  14 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  15 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  16 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  17 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  18 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  19 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  20 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  21 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  22 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  23 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  24 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  25 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  26 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  27 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  28 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  29 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  30 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  31 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  32 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  33 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  34 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  35 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  36 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  37 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  38 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  39 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  40 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  41 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  42 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  43 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  44 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  45 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  46 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  47 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  48 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  49 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  50 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  51 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  52 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  53 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  54 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  55 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  56 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  57 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  58 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  59 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  60 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  61 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  62 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  63 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors: layer  64 assigned to device CPU, is_swa = 0\nJun 06 19:12:13 ai001 ollama[590926]: load_tensors:          CPU model buffer size = 19259.71 MiB\nJun 06 19:12:13 ai001 ollama[590926]: load_all_data: no device found for buffer type CPU for async uploads\nJun 06 19:12:13 ai001 ollama[590926]: time=2025-06-06T19:12:13.955Z level=DEBUG source=server.go:636 msg=\"model load progress 0.02\"\nJun 06 19:12:14 ai001 ollama[590926]: time=2025-06-06T19:12:14.205Z level=DEBUG source=server.go:636 msg=\"model load progress 0.06\"\nJun 06 19:12:14 ai001 ollama[590926]: time=2025-06-06T19:12:14.456Z level=DEBUG source=server.go:636 msg=\"model load progress 0.08\"\nJun 06 19:12:14 ai001 ollama[590926]: time=2025-06-06T19:12:14.707Z level=DEBUG source=server.go:636 msg=\"model load progress 0.11\"\nJun 06 19:12:14 ai001 ollama[590926]: time=2025-06-06T19:12:14.958Z level=DEBUG source=server.go:636 msg=\"model load progress 0.13\"\nJun 06 19:12:15 ai001 ollama[590926]: time=2025-06-06T19:12:15.208Z level=DEBUG source=server.go:636 msg=\"model load progress 0.16\"\nJun 06 19:12:15 ai001 ollama[590926]: time=2025-06-06T19:12:15.459Z level=DEBUG source=server.go:636 msg=\"model load progress 0.18\"\nJun 06 19:12:15 ai001 ollama[590926]: time=2025-06-06T19:12:15.710Z level=DEBUG source=server.go:636 msg=\"model load progress 0.21\"\nJun 06 19:12:15 ai001 ollama[590926]: time=2025-06-06T19:12:15.961Z level=DEBUG source=server.go:636 msg=\"model load progress 0.23\"\nJun 06 19:12:16 ai001 ollama[590926]: time=2025-06-06T19:12:16.211Z level=DEBUG source=server.go:636 msg=\"model load progress 0.26\"\nJun 06 19:12:16 ai001 ollama[590926]: time=2025-06-06T19:12:16.462Z level=DEBUG source=server.go:636 msg=\"model load progress 0.28\"\nJun 06 19:12:16 ai001 ollama[590926]: time=2025-06-06T19:12:16.713Z level=DEBUG source=server.go:636 msg=\"model load progress 0.31\"\nJun 06 19:12:16 ai001 ollama[590926]: time=2025-06-06T19:12:16.963Z level=DEBUG source=server.go:636 msg=\"model load progress 0.33\"\nJun 06 19:12:17 ai001 ollama[590926]: time=2025-06-06T19:12:17.214Z level=DEBUG source=server.go:636 msg=\"model load progress 0.36\"\nJun 06 19:12:17 ai001 ollama[590926]: time=2025-06-06T19:12:17.465Z level=DEBUG source=server.go:636 msg=\"model load progress 0.38\"\nJun 06 19:12:17 ai001 ollama[590926]: time=2025-06-06T19:12:17.715Z level=DEBUG source=server.go:636 msg=\"model load progress 0.41\"\nJun 06 19:12:17 ai001 ollama[590926]: time=2025-06-06T19:12:17.966Z level=DEBUG source=server.go:636 msg=\"model load progress 0.43\"\nJun 06 19:12:18 ai001 ollama[590926]: time=2025-06-06T19:12:18.217Z level=DEBUG source=server.go:636 msg=\"model load progress 0.46\"\nJun 06 19:12:18 ai001 ollama[590926]: time=2025-06-06T19:12:18.468Z level=DEBUG source=server.go:636 msg=\"model load progress 0.48\"\nJun 06 19:12:18 ai001 ollama[590926]: time=2025-06-06T19:12:18.718Z level=DEBUG source=server.go:636 msg=\"model load progress 0.50\"\nJun 06 19:12:18 ai001 ollama[590926]: time=2025-06-06T19:12:18.969Z level=DEBUG source=server.go:636 msg=\"model load progress 0.53\"\nJun 06 19:12:19 ai001 ollama[590926]: time=2025-06-06T19:12:19.220Z level=DEBUG source=server.go:636 msg=\"model load progress 0.55\"\nJun 06 19:12:19 ai001 ollama[590926]: time=2025-06-06T19:12:19.470Z level=DEBUG source=server.go:636 msg=\"model load progress 0.58\"\nJun 06 19:12:19 ai001 ollama[590926]: time=2025-06-06T19:12:19.721Z level=DEBUG source=server.go:636 msg=\"model load progress 0.60\"\nJun 06 19:12:19 ai001 ollama[590926]: time=2025-06-06T19:12:19.972Z level=DEBUG source=server.go:636 msg=\"model load progress 0.63\"\nJun 06 19:12:20 ai001 ollama[590926]: time=2025-06-06T19:12:20.223Z level=DEBUG source=server.go:636 msg=\"model load progress 0.65\"\nJun 06 19:12:20 ai001 ollama[590926]: time=2025-06-06T19:12:20.473Z level=DEBUG source=server.go:636 msg=\"model load progress 0.68\"\nJun 06 19:12:20 ai001 ollama[590926]: time=2025-06-06T19:12:20.724Z level=DEBUG source=server.go:636 msg=\"model load progress 0.70\"\nJun 06 19:12:20 ai001 ollama[590926]: time=2025-06-06T19:12:20.975Z level=DEBUG source=server.go:636 msg=\"model load progress 0.73\"\nJun 06 19:12:21 ai001 ollama[590926]: time=2025-06-06T19:12:21.225Z level=DEBUG source=server.go:636 msg=\"model load progress 0.75\"\nJun 06 19:12:21 ai001 ollama[590926]: time=2025-06-06T19:12:21.476Z level=DEBUG source=server.go:636 msg=\"model load progress 0.78\"\nJun 06 19:12:21 ai001 ollama[590926]: time=2025-06-06T19:12:21.727Z level=DEBUG source=server.go:636 msg=\"model load progress 0.80\"\nJun 06 19:12:21 ai001 ollama[590926]: time=2025-06-06T19:12:21.977Z level=DEBUG source=server.go:636 msg=\"model load progress 0.83\"\nJun 06 19:12:22 ai001 ollama[590926]: time=2025-06-06T19:12:22.228Z level=DEBUG source=server.go:636 msg=\"model load progress 0.86\"\nJun 06 19:12:22 ai001 ollama[590926]: time=2025-06-06T19:12:22.479Z level=DEBUG source=server.go:636 msg=\"model load progress 0.88\"\nJun 06 19:12:22 ai001 ollama[590926]: time=2025-06-06T19:12:22.729Z level=DEBUG source=server.go:636 msg=\"model load progress 0.90\"\nJun 06 19:12:22 ai001 ollama[590926]: time=2025-06-06T19:12:22.980Z level=DEBUG source=server.go:636 msg=\"model load progress 0.93\"\nJun 06 19:12:23 ai001 ollama[590926]: time=2025-06-06T19:12:23.230Z level=DEBUG source=server.go:636 msg=\"model load progress 0.95\"\nJun 06 19:12:23 ai001 ollama[590926]: time=2025-06-06T19:12:23.481Z level=DEBUG source=server.go:636 msg=\"model load progress 0.98\"\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: constructing llama_context\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: n_seq_max     = 2\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: n_ctx         = 250000\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: n_ctx_per_seq = 125000\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: n_batch       = 1024\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: n_ubatch      = 512\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: causal_attn   = 1\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: flash_attn    = 0\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: freq_base     = 1000000.0\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: freq_scale    = 1\nJun 06 19:12:23 ai001 ollama[590926]: llama_context: n_ctx_per_seq (125000) > n_ctx_train (40960) -- possible training context overflow\nJun 06 19:12:23 ai001 ollama[590926]: set_abort_callback: call\nJun 06 19:12:23 ai001 ollama[590926]: llama_context:        CPU  output buffer size =     1.20 MiB\nJun 06 19:12:23 ai001 ollama[590926]: create_memory: n_ctx = 250016 (padded)\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: kv_size = 250016, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   0: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   1: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   2: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   3: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   4: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   5: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   6: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   7: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   8: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer   9: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  10: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  11: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  12: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  13: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  14: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  15: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  16: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  17: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  18: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  19: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  20: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  21: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  22: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  23: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  24: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  25: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  26: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  27: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  28: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  29: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  30: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  31: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  32: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  33: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  34: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  35: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  36: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  37: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  38: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  39: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  40: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  41: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  42: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  43: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  44: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  45: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  46: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  47: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  48: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  49: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  50: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  51: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  52: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  53: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  54: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  55: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  56: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  57: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  58: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  59: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  60: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  61: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  62: dev = CPU\nJun 06 19:12:23 ai001 ollama[590926]: llama_kv_cache_unified: layer  63: dev = CPU\n"
      },
      {
        "user": "rick-github",
        "body": "So what's the question?  Why is the model running on the CPU?  The context is too large to fit on a GPU."
      }
    ]
  },
  {
    "issue_number": 10999,
    "title": "0.9.0 too slowly",
    "author": "goactiongo",
    "state": "closed",
    "created_at": "2025-06-06T17:25:06Z",
    "updated_at": "2025-06-06T19:17:46Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n0.9.0, with 3 L30 card.\ntesting with qwen3:0.6b,it's so slowly,\n\n```\n(base) root@ai001:/data/tools# journalctl -u ollama -f\nJun 06 17:19:31 ai001 ollama[518336]: [GPU-a2bee386-03d6-6ecb-7a1e-74be9074150c] CUDA freeMem 43488mb\nJun 06 17:19:31 ai001 ollama[518336]: [GPU-a2bee386-03d6-6ecb-7a1e-74be9074150c] Compute Capability 8.9\nJun 06 17:19:31 ai001 ollama[518336]: [GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90] CUDA totalMem 45457mb\nJun 06 17:19:31 ai001 ollama[518336]: [GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90] CUDA freeMem 45165mb\nJun 06 17:19:31 ai001 ollama[518336]: [GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90] Compute Capability 8.9\nJun 06 17:19:32 ai001 ollama[518336]: time=2025-06-06T17:19:32.055Z level=DEBUG source=amd_linux.go:419 msg=\"amdgpu driver not detected /sys/module/amdgpu\"\nJun 06 17:19:32 ai001 ollama[518336]: releasing cuda driver library\nJun 06 17:19:32 ai001 ollama[518336]: time=2025-06-06T17:19:32.055Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-59d819ff-4712-49e4-5e5a-ba8f5c21e829 library=cuda variant=v12 compute=8.9 driver=12.9 name=\"NVIDIA L20\" total=\"44.4 GiB\" available=\"40.0 GiB\"\nJun 06 17:19:32 ai001 ollama[518336]: time=2025-06-06T17:19:32.055Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-a2bee386-03d6-6ecb-7a1e-74be9074150c library=cuda variant=v12 compute=8.9 driver=12.9 name=\"NVIDIA L20\" total=\"44.4 GiB\" available=\"42.5 GiB\"\nJun 06 17:19:32 ai001 ollama[518336]: time=2025-06-06T17:19:32.055Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90 library=cuda variant=v12 compute=8.9 driver=12.9 name=\"NVIDIA L20\" total=\"44.4 GiB\" available=\"44.1 GiB\"\n\n\n\n\n\n\nJun 06 17:20:49 ai001 ollama[518336]: time=2025-06-06T17:20:49.642Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\nJun 06 17:20:49 ai001 ollama[518336]: time=2025-06-06T17:20:49.643Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"376.6 GiB\" before.free=\"364.9 GiB\" before.free_swap=\"8.0 GiB\" now.total=\"376.6 GiB\" now.free=\"364.9 GiB\" now.free_swap=\"8.0 GiB\"\nJun 06 17:20:49 ai001 ollama[518336]: initializing /usr/lib/x86_64-linux-gnu/libcuda.so.575.57.08\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuInit - 0x7f0342af9680\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuDriverGetVersion - 0x7f0342af9740\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuDeviceGetCount - 0x7f0342af98c0\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuDeviceGet - 0x7f0342af9800\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuDeviceGetAttribute - 0x7f0342af9e00\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuDeviceGetUuid - 0x7f0342af9a40\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuDeviceGetName - 0x7f0342af9980\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuCtxCreate_v3 - 0x7f0342afa940\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuMemGetInfo_v2 - 0x7f0342afd5e0\nJun 06 17:20:49 ai001 ollama[518336]: dlsym: cuCtxDestroy - 0x7f0342b5f4e0\nJun 06 17:20:49 ai001 ollama[518336]: calling cuInit\nJun 06 17:20:49 ai001 ollama[518336]: calling cuDriverGetVersion\nJun 06 17:20:49 ai001 ollama[518336]: raw version 0x2f3a\nJun 06 17:20:49 ai001 ollama[518336]: CUDA driver version: 12.9\nJun 06 17:20:49 ai001 ollama[518336]: calling cuDeviceGetCount\nJun 06 17:20:49 ai001 ollama[518336]: device count 3\nJun 06 17:20:49 ai001 ollama[518336]: time=2025-06-06T17:20:49.819Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-59d819ff-4712-49e4-5e5a-ba8f5c21e829 name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"40.0 GiB\" now.total=\"44.4 GiB\" now.free=\"40.0 GiB\" now.used=\"4.4 GiB\"\nJun 06 17:20:49 ai001 ollama[518336]: time=2025-06-06T17:20:49.975Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-a2bee386-03d6-6ecb-7a1e-74be9074150c name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"42.5 GiB\" now.total=\"44.4 GiB\" now.free=\"42.5 GiB\" now.used=\"1.9 GiB\"\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.584Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90 name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"44.1 GiB\" now.total=\"44.4 GiB\" now.free=\"44.1 GiB\" now.used=\"292.2 MiB\"\nJun 06 17:20:50 ai001 ollama[518336]: releasing cuda driver library\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.584Z level=DEBUG source=sched.go:185 msg=\"updating default concurrency\" OLLAMA_MAX_LOADED_MODELS=9 gpu_count=3\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.612Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.636Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=general.alignment default=32\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.637Z level=DEBUG source=sched.go:228 msg=\"loading first model\" model=/usr/share/ollama/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.637Z level=DEBUG source=memory.go:111 msg=evaluating library=cuda gpu_count=1 available=\"[44.1 GiB]\"\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.637Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=qwen3.vision.block_count default=0\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.637Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"376.6 GiB\" before.free=\"364.9 GiB\" before.free_swap=\"8.0 GiB\" now.total=\"376.6 GiB\" now.free=\"364.9 GiB\" now.free_swap=\"8.0 GiB\"\nJun 06 17:20:50 ai001 ollama[518336]: initializing /usr/lib/x86_64-linux-gnu/libcuda.so.575.57.08\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuInit - 0x7f0342af9680\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuDriverGetVersion - 0x7f0342af9740\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuDeviceGetCount - 0x7f0342af98c0\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuDeviceGet - 0x7f0342af9800\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuDeviceGetAttribute - 0x7f0342af9e00\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuDeviceGetUuid - 0x7f0342af9a40\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuDeviceGetName - 0x7f0342af9980\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuCtxCreate_v3 - 0x7f0342afa940\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuMemGetInfo_v2 - 0x7f0342afd5e0\nJun 06 17:20:50 ai001 ollama[518336]: dlsym: cuCtxDestroy - 0x7f0342b5f4e0\nJun 06 17:20:50 ai001 ollama[518336]: calling cuInit\nJun 06 17:20:50 ai001 ollama[518336]: calling cuDriverGetVersion\nJun 06 17:20:50 ai001 ollama[518336]: raw version 0x2f3a\nJun 06 17:20:50 ai001 ollama[518336]: CUDA driver version: 12.9\nJun 06 17:20:50 ai001 ollama[518336]: calling cuDeviceGetCount\nJun 06 17:20:50 ai001 ollama[518336]: device count 3\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.801Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-59d819ff-4712-49e4-5e5a-ba8f5c21e829 name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"40.0 GiB\" now.total=\"44.4 GiB\" now.free=\"40.0 GiB\" now.used=\"4.4 GiB\"\nJun 06 17:20:50 ai001 ollama[518336]: time=2025-06-06T17:20:50.952Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-a2bee386-03d6-6ecb-7a1e-74be9074150c name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"42.5 GiB\" now.total=\"44.4 GiB\" now.free=\"42.5 GiB\" now.used=\"1.9 GiB\"\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.097Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90 name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"44.1 GiB\" now.total=\"44.4 GiB\" now.free=\"44.1 GiB\" now.used=\"292.2 MiB\"\nJun 06 17:20:51 ai001 ollama[518336]: releasing cuda driver library\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.097Z level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/usr/share/ollama/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa gpu=GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90 parallel=2 available=47359262720 required=\"2.1 GiB\"\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.097Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"376.6 GiB\" before.free=\"364.9 GiB\" before.free_swap=\"8.0 GiB\" now.total=\"376.6 GiB\" now.free=\"364.9 GiB\" now.free_swap=\"8.0 GiB\"\nJun 06 17:20:51 ai001 ollama[518336]: initializing /usr/lib/x86_64-linux-gnu/libcuda.so.575.57.08\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuInit - 0x7f0342af9680\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDriverGetVersion - 0x7f0342af9740\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGetCount - 0x7f0342af98c0\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGet - 0x7f0342af9800\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGetAttribute - 0x7f0342af9e00\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGetUuid - 0x7f0342af9a40\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGetName - 0x7f0342af9980\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuCtxCreate_v3 - 0x7f0342afa940\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuMemGetInfo_v2 - 0x7f0342afd5e0\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuCtxDestroy - 0x7f0342b5f4e0\nJun 06 17:20:51 ai001 ollama[518336]: calling cuInit\nJun 06 17:20:51 ai001 ollama[518336]: calling cuDriverGetVersion\nJun 06 17:20:51 ai001 ollama[518336]: raw version 0x2f3a\nJun 06 17:20:51 ai001 ollama[518336]: CUDA driver version: 12.9\nJun 06 17:20:51 ai001 ollama[518336]: calling cuDeviceGetCount\nJun 06 17:20:51 ai001 ollama[518336]: device count 3\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.248Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-59d819ff-4712-49e4-5e5a-ba8f5c21e829 name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"40.0 GiB\" now.total=\"44.4 GiB\" now.free=\"40.0 GiB\" now.used=\"4.4 GiB\"\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.394Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-a2bee386-03d6-6ecb-7a1e-74be9074150c name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"42.5 GiB\" now.total=\"44.4 GiB\" now.free=\"42.5 GiB\" now.used=\"1.9 GiB\"\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.544Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90 name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"44.1 GiB\" now.total=\"44.4 GiB\" now.free=\"44.1 GiB\" now.used=\"292.2 MiB\"\nJun 06 17:20:51 ai001 ollama[518336]: releasing cuda driver library\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.544Z level=INFO source=server.go:135 msg=\"system memory\" total=\"376.6 GiB\" free=\"364.9 GiB\" free_swap=\"8.0 GiB\"\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.544Z level=DEBUG source=memory.go:111 msg=evaluating library=cuda gpu_count=1 available=\"[44.1 GiB]\"\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.544Z level=DEBUG source=ggml.go:155 msg=\"key not found\" key=qwen3.vision.block_count default=0\nJun 06 17:20:51 ai001 ollama[518336]: time=2025-06-06T17:20:51.545Z level=DEBUG source=gpu.go:391 msg=\"updating system memory data\" before.total=\"376.6 GiB\" before.free=\"364.9 GiB\" before.free_swap=\"8.0 GiB\" now.total=\"376.6 GiB\" now.free=\"364.9 GiB\" now.free_swap=\"8.0 GiB\"\nJun 06 17:20:51 ai001 ollama[518336]: initializing /usr/lib/x86_64-linux-gnu/libcuda.so.575.57.08\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuInit - 0x7f0342af9680\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDriverGetVersion - 0x7f0342af9740\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGetCount - 0x7f0342af98c0\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGet - 0x7f0342af9800\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGetAttribute - 0x7f0342af9e00\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGetUuid - 0x7f0342af9a40\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuDeviceGetName - 0x7f0342af9980\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuCtxCreate_v3 - 0x7f0342afa940\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuMemGetInfo_v2 - 0x7f0342afd5e0\nJun 06 17:20:51 ai001 ollama[518336]: dlsym: cuCtxDestroy - 0x7f0342b5f4e0\nJun 06 17:20:51 ai001 ollama[518336]: calling cuInit\nJun 06 17:20:51 ai001 ollama[518336]: calling cuDriverGetVersion\nJun 06 17:20:51 ai001 ollama[518336]: raw version 0x2f3a\nJun 06 17:20:51 ai001 ollama[518336]: CUDA driver version: 12.9\nJun 06 17:20:51 ai001 ollama[518336]: calling cuDeviceGetCount\nJun 06 17:20:51 ai001 ollama[518336]: device count 3\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.145Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-59d819ff-4712-49e4-5e5a-ba8f5c21e829 name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"40.0 GiB\" now.total=\"44.4 GiB\" now.free=\"40.0 GiB\" now.used=\"4.4 GiB\"\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.291Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-a2bee386-03d6-6ecb-7a1e-74be9074150c name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"42.5 GiB\" now.total=\"44.4 GiB\" now.free=\"42.5 GiB\" now.used=\"1.9 GiB\"\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.432Z level=DEBUG source=gpu.go:441 msg=\"updating cuda memory data\" gpu=GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90 name=\"NVIDIA L20\" overhead=\"0 B\" before.total=\"44.4 GiB\" before.free=\"44.1 GiB\" now.total=\"44.4 GiB\" now.free=\"44.1 GiB\" now.used=\"292.2 MiB\"\nJun 06 17:20:52 ai001 ollama[518336]: releasing cuda driver library\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.432Z level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split=\"\" memory.available=\"[44.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"2.1 GiB\" memory.required.partial=\"2.1 GiB\" memory.required.kv=\"896.0 MiB\" memory.required.allocations=\"[2.1 GiB]\" memory.weights.total=\"409.3 MiB\" memory.weights.repeating=\"287.6 MiB\" memory.weights.nonrepeating=\"121.7 MiB\" memory.graph.full=\"298.7 MiB\" memory.graph.partial=\"298.7 MiB\"\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.432Z level=INFO source=server.go:211 msg=\"enabling flash attention\"\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.432Z level=WARN source=server.go:219 msg=\"kv cache type not supported by model\" type=\"\"\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.432Z level=DEBUG source=server.go:284 msg=\"compatible gpu libraries\" compatible=[]\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: loaded meta data with 28 key-value pairs and 311 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa (version GGUF V3 (latest))\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   1:                               general.type str              = model\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 0.6B\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   4:                         general.size_label str              = 0.6B\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 28\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 1024\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 3072\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 16\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - type  f32:  113 tensors\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - type  f16:   28 tensors\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - type q4_K:  155 tensors\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - type q6_K:   15 tensors\nJun 06 17:20:52 ai001 ollama[518336]: print_info: file format = GGUF V3 (latest)\nJun 06 17:20:52 ai001 ollama[518336]: print_info: file type   = Q4_K - Medium\nJun 06 17:20:52 ai001 ollama[518336]: print_info: file size   = 492.75 MiB (5.50 BPW)\nJun 06 17:20:52 ai001 ollama[518336]: init_tokenizer: initializing tokenizer for type 2\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151660 '<|fim_middle|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151653 '<|vision_end|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151648 '<|box_start|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151649 '<|box_end|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151655 '<|image_pad|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151651 '<|quad_end|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151652 '<|vision_start|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151654 '<|vision_pad|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151656 '<|video_pad|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151644 '<|im_start|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: control token: 151650 '<|quad_start|>' is not marked as EOG\nJun 06 17:20:52 ai001 ollama[518336]: load: special tokens cache size = 26\nJun 06 17:20:52 ai001 ollama[518336]: load: token to piece cache size = 0.9311 MB\nJun 06 17:20:52 ai001 ollama[518336]: print_info: arch             = qwen3\nJun 06 17:20:52 ai001 ollama[518336]: print_info: vocab_only       = 1\nJun 06 17:20:52 ai001 ollama[518336]: print_info: model type       = ?B\nJun 06 17:20:52 ai001 ollama[518336]: print_info: model params     = 751.63 M\nJun 06 17:20:52 ai001 ollama[518336]: print_info: general.name     = Qwen3 0.6B\nJun 06 17:20:52 ai001 ollama[518336]: print_info: vocab type       = BPE\nJun 06 17:20:52 ai001 ollama[518336]: print_info: n_vocab          = 151936\nJun 06 17:20:52 ai001 ollama[518336]: print_info: n_merges         = 151387\nJun 06 17:20:52 ai001 ollama[518336]: print_info: BOS token        = 151643 '<|endoftext|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: EOS token        = 151645 '<|im_end|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: EOT token        = 151645 '<|im_end|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: PAD token        = 151643 '<|endoftext|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: LF token         = 198 'Ċ'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: FIM MID token    = 151660 '<|fim_middle|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: FIM REP token    = 151663 '<|repo_name|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: FIM SEP token    = 151664 '<|file_sep|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: EOG token        = 151643 '<|endoftext|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: EOG token        = 151645 '<|im_end|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: EOG token        = 151662 '<|fim_pad|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: EOG token        = 151663 '<|repo_name|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: EOG token        = 151664 '<|file_sep|>'\nJun 06 17:20:52 ai001 ollama[518336]: print_info: max token length = 256\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_load: vocab only - skipping tensors\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.717Z level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 64 --flash-attn --parallel 2 --port 43101\"\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.718Z level=DEBUG source=server.go:432 msg=subprocess PATH=/root/anaconda3/bin:/root/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin OLLAMA_HOST=0.0.0.0 OLLAMA_DEBUG=1 OLLAMA_NUM_PARALLEL=2 OLLAMA_FLASH_ATTENTION=1 GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 OLLAMA_MAX_LOADED_MODELS=9 OLLAMA_LIBRARY_PATH=/usr/local/lib/ollama LD_LIBRARY_PATH=/usr/local/lib/ollama:/usr/local/lib/ollama CUDA_VISIBLE_DEVICES=GPU-ad7c6ece-c9bd-b8b9-2a1c-23f9aea08b90\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.718Z level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.718Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.718Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.751Z level=INFO source=runner.go:815 msg=\"starting go runner\"\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.751Z level=DEBUG source=ggml.go:94 msg=\"ggml backend load all from path\" path=/usr/local/lib/ollama\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.751Z level=INFO source=ggml.go:104 msg=system CPU.0.LLAMAFILE=1 compiler=cgo(gcc)\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.782Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:43101\"\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: loaded meta data with 28 key-value pairs and 311 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa (version GGUF V3 (latest))\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   1:                               general.type str              = model\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 0.6B\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   4:                         general.size_label str              = 0.6B\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 28\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 1024\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 3072\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 16\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - type  f32:  113 tensors\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - type  f16:   28 tensors\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - type q4_K:  155 tensors\nJun 06 17:20:52 ai001 ollama[518336]: llama_model_loader: - type q6_K:   15 tensors\nJun 06 17:20:52 ai001 ollama[518336]: print_info: file format = GGUF V3 (latest)\nJun 06 17:20:52 ai001 ollama[518336]: print_info: file type   = Q4_K - Medium\nJun 06 17:20:52 ai001 ollama[518336]: print_info: file size   = 492.75 MiB (5.50 BPW)\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.970Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nJun 06 17:20:53 ai001 ollama[518336]: init_tokenizer: initializing tokenizer for type 2\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151660 '<|fim_middle|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151653 '<|vision_end|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151648 '<|box_start|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151649 '<|box_end|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151655 '<|image_pad|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151651 '<|quad_end|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151652 '<|vision_start|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151654 '<|vision_pad|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151656 '<|video_pad|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151644 '<|im_start|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: control token: 151650 '<|quad_start|>' is not marked as EOG\nJun 06 17:20:53 ai001 ollama[518336]: load: special tokens cache size = 26\nJun 06 17:20:53 ai001 ollama[518336]: load: token to piece cache size = 0.9311 MB\nJun 06 17:20:53 ai001 ollama[518336]: print_info: arch             = qwen3\nJun 06 17:20:53 ai001 ollama[518336]: print_info: vocab_only       = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_ctx_train      = 40960\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_embd           = 1024\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_layer          = 28\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_head           = 16\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_head_kv        = 8\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_rot            = 128\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_swa            = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_swa_pattern    = 1\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_embd_head_k    = 128\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_embd_head_v    = 128\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_gqa            = 2\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_embd_k_gqa     = 1024\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_embd_v_gqa     = 1024\nJun 06 17:20:53 ai001 ollama[518336]: print_info: f_norm_eps       = 0.0e+00\nJun 06 17:20:53 ai001 ollama[518336]: print_info: f_norm_rms_eps   = 1.0e-06\nJun 06 17:20:53 ai001 ollama[518336]: print_info: f_clamp_kqv      = 0.0e+00\nJun 06 17:20:53 ai001 ollama[518336]: print_info: f_max_alibi_bias = 0.0e+00\nJun 06 17:20:53 ai001 ollama[518336]: print_info: f_logit_scale    = 0.0e+00\nJun 06 17:20:53 ai001 ollama[518336]: print_info: f_attn_scale     = 0.0e+00\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_ff             = 3072\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_expert         = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_expert_used    = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: causal attn      = 1\nJun 06 17:20:53 ai001 ollama[518336]: print_info: pooling type     = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: rope type        = 2\nJun 06 17:20:53 ai001 ollama[518336]: print_info: rope scaling     = linear\nJun 06 17:20:53 ai001 ollama[518336]: print_info: freq_base_train  = 1000000.0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: freq_scale_train = 1\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_ctx_orig_yarn  = 40960\nJun 06 17:20:53 ai001 ollama[518336]: print_info: rope_finetuned   = unknown\nJun 06 17:20:53 ai001 ollama[518336]: print_info: ssm_d_conv       = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: ssm_d_inner      = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: ssm_d_state      = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: ssm_dt_rank      = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: ssm_dt_b_c_rms   = 0\nJun 06 17:20:53 ai001 ollama[518336]: print_info: model type       = 0.6B\nJun 06 17:20:53 ai001 ollama[518336]: print_info: model params     = 751.63 M\nJun 06 17:20:53 ai001 ollama[518336]: print_info: general.name     = Qwen3 0.6B\nJun 06 17:20:53 ai001 ollama[518336]: print_info: vocab type       = BPE\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_vocab          = 151936\nJun 06 17:20:53 ai001 ollama[518336]: print_info: n_merges         = 151387\nJun 06 17:20:53 ai001 ollama[518336]: print_info: BOS token        = 151643 '<|endoftext|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: EOS token        = 151645 '<|im_end|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: EOT token        = 151645 '<|im_end|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: PAD token        = 151643 '<|endoftext|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: LF token         = 198 'Ċ'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: FIM MID token    = 151660 '<|fim_middle|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: FIM REP token    = 151663 '<|repo_name|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: FIM SEP token    = 151664 '<|file_sep|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: EOG token        = 151643 '<|endoftext|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: EOG token        = 151645 '<|im_end|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: EOG token        = 151662 '<|fim_pad|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: EOG token        = 151663 '<|repo_name|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: EOG token        = 151664 '<|file_sep|>'\nJun 06 17:20:53 ai001 ollama[518336]: print_info: max token length = 256\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: loading model tensors, this can take a while... (mmap = true)\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   0 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   1 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   2 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   3 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   4 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   5 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   6 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   7 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   8 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer   9 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  10 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  11 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  12 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  13 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  14 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  15 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  16 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  17 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  18 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  19 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  20 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  21 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  22 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  23 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  24 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  25 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  26 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  27 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors: layer  28 assigned to device CPU, is_swa = 0\nJun 06 17:20:53 ai001 ollama[518336]: load_tensors:   CPU_Mapped model buffer size =   492.75 MiB\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: constructing llama_context\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: n_seq_max     = 2\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: n_ctx         = 8192\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: n_ctx_per_seq = 4096\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: n_batch       = 1024\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: n_ubatch      = 512\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: causal_attn   = 1\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: flash_attn    = 1\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: freq_base     = 1000000.0\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: freq_scale    = 1\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nJun 06 17:20:53 ai001 ollama[518336]: set_abort_callback: call\nJun 06 17:20:53 ai001 ollama[518336]: llama_context:        CPU  output buffer size =     1.17 MiB\nJun 06 17:20:53 ai001 ollama[518336]: create_memory: n_ctx = 8192 (padded)\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 256\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   0: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   1: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   2: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   3: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   4: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   5: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   6: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   7: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   8: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer   9: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  10: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  11: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  12: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  13: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  14: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  15: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  16: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  17: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  18: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  19: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  20: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  21: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  22: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  23: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  24: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  25: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  26: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: layer  27: dev = CPU\nJun 06 17:20:53 ai001 ollama[518336]: time=2025-06-06T17:20:53.222Z level=DEBUG source=server.go:636 msg=\"model load progress 1.00\"\nJun 06 17:20:53 ai001 ollama[518336]: time=2025-06-06T17:20:53.473Z level=DEBUG source=server.go:639 msg=\"model load completed, waiting for server to become available\" status=\"llm server loading model\"\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified:        CPU KV buffer size =   896.00 MiB\nJun 06 17:20:53 ai001 ollama[518336]: llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: enumerating backends\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: backend_ptrs.size() = 1\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: max_nodes = 65536\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: reserving graph for n_tokens = 512, n_seqs = 1\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: reserving graph for n_tokens = 1, n_seqs = 1\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: reserving graph for n_tokens = 512, n_seqs = 1\nJun 06 17:20:53 ai001 ollama[518336]: llama_context:        CPU compute buffer size =   298.75 MiB\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: graph nodes  = 959\nJun 06 17:20:53 ai001 ollama[518336]: llama_context: graph splits = 1\nJun 06 17:20:53 ai001 ollama[518336]: time=2025-06-06T17:20:53.724Z level=INFO source=server.go:630 msg=\"llama runner started in 1.01 seconds\"\nJun 06 17:20:53 ai001 ollama[518336]: time=2025-06-06T17:20:53.724Z level=DEBUG source=sched.go:495 msg=\"finished setting up\" runner.name=registry.ollama.ai/library/qwen3:0.6b runner.inference=cuda runner.devices=1 runner.size=\"2.1 GiB\" runner.vram=\"2.1 GiB\" runner.parallel=2 runner.pid=518390 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa runner.num_ctx=8192\nJun 06 17:20:53 ai001 ollama[518336]: time=2025-06-06T17:20:53.724Z level=DEBUG source=server.go:729 msg=\"completion request\" images=0 prompt=52 format=\"\"\nJun 06 17:20:53 ai001 ollama[518336]: time=2025-06-06T17:20:53.731Z level=DEBUG source=cache.go:104 msg=\"loading cache slot\" id=0 cache=0 prompt=9 used=0 remaining=9\nJun 06 17:20:57 ai001 ollama[518336]: [GIN] 2025/06/06 - 17:20:57 | 200 |  8.056269519s |    172.16.1.219 | POST     \"/api/chat\"\nJun 06 17:20:57 ai001 ollama[518336]: time=2025-06-06T17:20:57.656Z level=DEBUG source=sched.go:503 msg=\"context for request finished\"\nJun 06 17:20:57 ai001 ollama[518336]: time=2025-06-06T17:20:57.656Z level=DEBUG source=sched.go:343 msg=\"runner with non-zero duration has gone idle, adding timer\" runner.name=registry.ollama.ai/library/qwen3:0.6b runner.inference=cuda runner.devices=1 runner.size=\"2.1 GiB\" runner.vram=\"2.1 GiB\" runner.parallel=2 runner.pid=518390 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa runner.num_ctx=8192 duration=5m0s\nJun 06 17:20:57 ai001 ollama[518336]: time=2025-06-06T17:20:57.656Z level=DEBUG source=sched.go:361 msg=\"after processing request finished event\" runner.name=registry.ollama.ai/library/qwen3:0.6b runner.inference=cuda runner.devices=1 runner.size=\"2.1 GiB\" runner.vram=\"2.1 GiB\" runner.parallel=2 runner.pid=518390 runner.model=/usr/share/ollama/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa runner.num_ctx=8192 refCount=0\n```\n\nnvidia-smi\nFri Jun  6 17:24:29 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA L20                     Off |   00000000:38:00.0 Off |                    0 |\n| N/A   43C    P0             78W /  350W |    4207MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA L20                     Off |   00000000:A8:00.0 Off |                    0 |\n| N/A   43C    P0             69W /  350W |       3MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA L20                     Off |   00000000:B8:00.0 Off |                    0 |\n| N/A   31C    P0             63W /  350W |       3MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A          504474      C   /root/miniconda3/bin/python3           4198MiB |\n+-----------------------------------------------------------------------------------------+\n(base) root@ai001:~#\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nUbuntu 22.04.5 LTS\n\n### GPU\n\nNvidia L30 *3 \n\n### CPU\n\nx86_64\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "```\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.751Z level=DEBUG source=ggml.go:94 msg=\"ggml backend load all from path\" path=/usr/local/lib/ollama\nJun 06 17:20:52 ai001 ollama[518336]: time=2025-06-06T17:20:52.751Z level=INFO source=ggml.go:104 msg=system CPU.0.LLAMAFILE=1 compiler=cgo(gcc)\n```\nNo CPU or GPU backends found.  How did you install ollama?"
      },
      {
        "user": "goactiongo",
        "body": "After installing version 0.9.0 using the command `curl -fsSL https://ollama.com/install.sh | sh`, there were some issues. I originally planned to reinstall using the script again, but since the installation via the script is very slow (`curl -fsSL https://ollama.com/install.sh | sh`), I instead downloaded the `ollama-linux-amd64.tgz` file for version 0.9.0 from GitHub, and then extracted it using `tar xvzf ollama-linux-amd64.tgz`.\n\n![Image](https://github.com/user-attachments/assets/ab09c0a2-0568-46ee-a34e-c76c86b37988)"
      },
      {
        "user": "rick-github",
        "body": "Where did you extract it to?  What commands, exactly, did you use to install ollama?"
      }
    ]
  },
  {
    "issue_number": 10888,
    "title": "Bagel",
    "author": "Amazon90",
    "state": "open",
    "created_at": "2025-05-28T14:28:21Z",
    "updated_at": "2025-06-06T17:46:44Z",
    "labels": [
      "model request"
    ],
    "body": "Would you consider supporting the Bagel multimodal model? It can generate, understand, and edit images. I don’t think ComfyUI will support it, because when looked at separately, there are better models available for image generation, understanding, and editing. However, Ollama might consider supporting Bagel, as it’s a well-rounded model and fits well with the kind of functionality Ollama aims to provide.\n\nhttps://huggingface.co/calcuis/bagel-gguf",
    "comments": [
      {
        "user": "FullstackJack",
        "body": "I think you mean this one? https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT"
      }
    ]
  },
  {
    "issue_number": 10996,
    "title": "0.9.0 Permission denied with embedding model--quentinz/bge-large-zh-v1.5:latest",
    "author": "goactiongo",
    "state": "closed",
    "created_at": "2025-06-06T13:50:38Z",
    "updated_at": "2025-06-06T17:22:34Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nllama runner process has terminated: error:status: Permission denied [/usr/local/lib/ollama/cuda_v11/libggml-blas.so]\n\n\nJun 06 13:49:12 ai001 ollama[31109]: time=2025-06-06T13:49:12.902Z level=ERROR source=server.go:457 msg=\"llama runner terminated\" error=\"exit status 2\"\nJun 06 13:49:13 ai001 ollama[31109]: time=2025-06-06T13:49:13.098Z level=ERROR source=sched.go:489 msg=\"error loading llama server\" error=\"llama runner process has terminated: error:status: Permission denied [/usr/local/lib/ollama/cuda_v11/libggml-blas.so]\"\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nUbuntu 22.04.5 LTS\n\n### GPU\n\nL20\n\n### CPU\n\nX86\n\n### Ollama version\n\n0.90",
    "comments": [
      {
        "user": "rick-github",
        "body": "What's the output of:\n```\np=\"/usr/local/lib/ollama/cuda_v11/libggml-blas.so\"; ls -ld / $(while [ \"$p\" != \"/\" ]; do echo \"$p\"; p=$(dirname \"$p\"); done | tac)\n```"
      },
      {
        "user": "goactiongo",
        "body": "[ollama.log](https://github.com/user-attachments/files/20630100/ollama.log)   log attached..."
      },
      {
        "user": "goactiongo",
        "body": "(base) root@ai001:/tmp# p=\"/usr/local/lib/ollama/cuda_v11/libggml-blas.so\"; ls -ld / $(while [ \"$p\" != \"/\" ]; do echo \"$p\"; p=$(dirname \"$p\"); done | tac)\nls: cannot access '/usr/local/lib/ollama/cuda_v11/libggml-blas.so': No such file or directory\ndrwxr-xr-x 24 root root 4096 Jun  6 04:09 /\ndrwxr-xr-x 14 root root 4096 Sep 11  2024 /usr\ndrwxr-xr-x 18 root root 4096 Jun  6 04:20 /usr/local\ndrwxr-xr-x  4 root root 4096 Jun  6 12:21 /usr/local/lib\ndrwxr-xr-x  3 root root 4096 Jun  6 12:23 /usr/local/lib/ollama\ndrwx------  2 root root 4096 Jun  6 12:23 /usr/local/lib/ollama/cuda_v11\n"
      }
    ]
  },
  {
    "issue_number": 7162,
    "title": "I hope Image analyze model, Qwen/Qwen2-VL-72B-Instruct",
    "author": "gigascake",
    "state": "closed",
    "created_at": "2024-10-10T15:43:53Z",
    "updated_at": "2025-06-06T17:19:29Z",
    "labels": [
      "model request"
    ],
    "body": "Qwen/Qwen2-VL-72B-Instruct\r\n\r\n\r\n\r\nOllama brothers. plz. ",
    "comments": [
      {
        "user": "rick-github",
        "body": "https://github.com/ollama/ollama/issues/6564"
      },
      {
        "user": "rick-github",
        "body": "https://ollama.com/library/qwen2.5vl"
      }
    ]
  },
  {
    "issue_number": 10925,
    "title": "Thinking Budget",
    "author": "GihanSoft",
    "state": "open",
    "created_at": "2025-05-31T05:21:20Z",
    "updated_at": "2025-06-06T16:25:48Z",
    "labels": [
      "feature request",
      "thinking"
    ],
    "body": "In official site, on Qwen3, you can set max thinking tokens.\nIt would be great if Ollama support it.",
    "comments": [
      {
        "user": "dengyunsheng250",
        "body": "that's really"
      }
    ]
  },
  {
    "issue_number": 10918,
    "title": "Don't have these instructions on my HOST.",
    "author": "phalexo",
    "state": "closed",
    "created_at": "2025-05-30T20:17:07Z",
    "updated_at": "2025-06-06T12:58:28Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nI have older CPUs, I want it compiled with minimal dependencies on specific Intel architecture.\n\nIs there an earlier version that does not have this problem?\n\n\n### Relevant log output\n\n```shell\ncc: error: unrecognized command line option ‘-mavxvnni’; did you mean ‘-mavx512vnni’?\nmake[2]: *** [ml/backend/ggml/ggml/src/CMakeFiles/ggml-cpu-alderlake.dir/build.make:79: ml/backend/ggml/ggml/src/CMakeFiles/ggml-cpu-alderlake.dir/ggml-cpu/ggml-cpu.c.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:709: ml/backend/ggml/ggml/src/CMakeFiles/ggml-cpu-alderlake.dir/all] Error 2\nmake: *** [Makefile:136: all] Error 2\n(Pythagora) developer@ai:~/ollama$ vi [ml/backend/ggml/ggml/src/CMakeFiles/ggml-cpu-alderlake.dir/build.make\n```\n\n### OS\n\nLinux\n\n### GPU\n\nNvidia\n\n### CPU\n\nIntel\n\n### Ollama version\n\nCompiling from github source. ",
    "comments": [
      {
        "user": "rick-github",
        "body": "Have you just tried installing ollama?  It picks a backend based on the available features of the CPU, so shouldn't try and run backends that contain unsupported instructions.\n\nFailing that, you can edit the build files to remove references to alderlake."
      },
      {
        "user": "phalexo",
        "body": "> Have you just tried installing ollama? It picks a backend based on the available features of the CPU, so shouldn't try and run backends that contain unsupported instructions.\n> \n> Failing that, you can edit the build files to remove references to alderlake.\n\nApparently it is not identifying the architecture correctly.\n\nCould you suggest which files I need to edit? Or at least the folder where I can grep for alderlake?\n\np.s. I not sure what is meant by \"just tried installing?\" I am doing \"cmake -B build\" [and then \"cmake --build build\" Should I do something else?\n\n"
      },
      {
        "user": "phalexo",
        "body": "I have SandyBridge I think, and it includes a lot of other stuff that I don't need or want.\n\n-- Including CPU backend\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu-x64:\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu-sse42: -msse4.2 GGML_SSE42\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu-sandybridge: -msse4.2;-mavx GGML_SSE42;GGML_AVX\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu-haswell: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2 GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu-skylakex: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2;-mavx512f;-mavx512cd;-mavx512vl;-mavx512dq;-mavx512bw GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2;GGML_AVX512\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu-icelake: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2;-mavx512f;-mavx512cd;-mavx512vl;-mavx512dq;-mavx512bw;-mavx512vbmi;-mavx512vnni GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2;GGML_AVX512;GGML_AVX512_VBMI;GGML_AVX512_VNNI\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu-alderlake: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2;-mavxvnni GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2;GGML_AVX_VNNI\n"
      }
    ]
  },
  {
    "issue_number": 10539,
    "title": "Allow \"use_mmap\" to be set at a global level using enviroment variables.",
    "author": "Slymi",
    "state": "open",
    "created_at": "2025-05-02T16:43:29Z",
    "updated_at": "2025-06-06T11:10:44Z",
    "labels": [
      "feature request"
    ],
    "body": "### Context:\nI am running Ollama V0.6.7 using Docker on a system running TRUENAS Scale with 128 GB of RAM and 02x 16 GB RTX 4060 TI. The container running off a NVME drive.\n\n### Current Issue\nAfter loading a model that does not use the new Ollama engine with \"use_mmap\" = true, the allocated RAM is not fully released when the model stopped. Not only that, loading times are worse with \"use_mmap\" = true.\n\n### Example with \"use_mmap\" = true (Qwen3:32b Q4_K_M @ 16384 Context Size):\n\n**Before loading (System RAM Usage on the right):**\n![Image](https://github.com/user-attachments/assets/31b480e3-b069-44f6-9206-57bd456ed3f4)\n\n**Loaded:**\n![Image](https://github.com/user-attachments/assets/a613c938-347d-4604-91ae-5d572601b274)\n\n**Stopped (450~ MB released?):**\n![Image](https://github.com/user-attachments/assets/c01413aa-d322-45d0-99a9-96aeb76df50f)\n\n**Results:**\n![Image](https://github.com/user-attachments/assets/8743c584-1227-4819-aaaf-b96dad595396)\n\n**Logs:**\n[Qwen3_MMAP_Logs.txt](https://github.com/user-attachments/files/20015338/Qwen3_MMAP_Logs.txt)\n\n### Example with \"use_mmap\" = false (Qwen3:32b Q4_K_M @ 16384 Context Size): \n**Loaded:**\n![Image](https://github.com/user-attachments/assets/8ff9f7da-27b9-4f83-b976-cb7e70068208)\n\n**Stopped:**\n![Image](https://github.com/user-attachments/assets/ef076a2d-a809-4488-b1bb-86a1a7d92955)\n\n**Results:**\n![Image](https://github.com/user-attachments/assets/bdd6e4de-b51c-4ed9-ac8d-e7ea963b6500)\n\n**Logs:**\n[Qwen3_NoMMAP_Logs.txt](https://github.com/user-attachments/files/20015446/Qwen3_NoMMAP_Logs.txt)\n\n**Notes:**\n- Restarting the container does not free up the RAM either, requiring a full system reboot to resolve.\n- In both examples, subsequent model loading times are reduced to 4-5 seconds as the model has been picked up by ZFS ARC (RAM Cache).\n- If the RAM limit of the container is reduced to below the model file size, \"use_mmap\" = true would significantly lengthen the time it takes to load the model. Example as shown below.\n\n### Examples of 8 GB container RAM limit and loading from ZFS ARC.\n**\"use_mmap\" = true:**\n![Image](https://github.com/user-attachments/assets/b1a2f18f-f111-4b0e-bc53-80a42e72e8f3)\n\n**\"use_mmap\" = false:**\n![Image](https://github.com/user-attachments/assets/b0cdcc1e-0bb3-4242-b0ee-260b34034385)\n\n12x improvement with \"use_mmap\" = false as the model is already stored in the ZFS Cache.\n\n### Conclusion:\nI hope the developers of Ollama will look into this and allow \"use_mmap\" to be set globally in the enviroment variables in order to resolve these kind of issues. As of now, I'm using OpenWebUI to set \"use_mmap\" = false for all models. However, some other frontends lack the option to do so. Thus, restricting the frontends that can be used that allow for a satisfactory experience.\n\n**References that I think is related:**\n#6854 \n#10076 ",
    "comments": [
      {
        "user": "rick-github",
        "body": "The new engine doesn't use mmap so `use_mmap` is ignored.  For the old engine, #8895."
      }
    ]
  },
  {
    "issue_number": 5794,
    "title": "Expose model capabilities via /api/tags and /v1/models[/model]",
    "author": "rick-github",
    "state": "closed",
    "created_at": "2024-07-19T12:00:49Z",
    "updated_at": "2025-06-06T10:43:32Z",
    "labels": [
      "feature request",
      "api"
    ],
    "body": "It would be convenient if the capabilities of a model (completion, tools, insert, \\<future caps\\>) were made available so that clients can adjust API calls.",
    "comments": [
      {
        "user": "jmorganca",
        "body": "Great idea! Thanks for the issue"
      },
      {
        "user": "danbri",
        "body": "Yes, this would be great! It might even encourage people to find common abstractions (wrapping in API , training common approaches, maybe even closing the gap between approaches at the prompting layer?\n). "
      },
      {
        "user": "rick-github",
        "body": " #10174"
      }
    ]
  },
  {
    "issue_number": 10994,
    "title": "Ollama Loads Unsloth Qwen3-14B Model with Unexpected High Memory Usage",
    "author": "NEWbie0709",
    "state": "closed",
    "created_at": "2025-06-06T06:32:41Z",
    "updated_at": "2025-06-06T08:36:36Z",
    "labels": [],
    "body": "When using Ollama to load the Unsloth Qwen3-14B-GGUF model (hf.co/unsloth/Qwen3-14B-GGUF:Q4_K_M), the memory usage spikes to 34 GB, while the standard Ollama version of the same model (qwen3:14b, also Q4_K_M) only uses around 10 GB.\n\nRelated Issue on Unsloth\nI’ve raised this with the Unsloth team here for further insight:\n[issue](https://github.com/unslothai/unsloth/issues/2696)\n\n",
    "comments": [
      {
        "user": "rick-github",
        "body": "The unsloth models usually have an embedded `num_ctx` in the Modelfile.  Remove it and the memory use will be reduced."
      },
      {
        "user": "NEWbie0709",
        "body": "@rick-github Thanks, that solved the issue!\n"
      }
    ]
  },
  {
    "issue_number": 10977,
    "title": "Error=\"llama runner process has terminated: exit status 2\"",
    "author": "mrMastor",
    "state": "closed",
    "created_at": "2025-06-05T09:24:18Z",
    "updated_at": "2025-06-06T08:17:05Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n**My system:**\n\nOS: Debian GNU/Linux 12 (bookworm).\nProcessor: Intel® Xeon® Gold 5118 (no AVX/AVX2 support).\nMemory: 31 GiB.\n`ollama` version: 0.9.0.\nThe logs show the error `SIGILL: illegal instruction`, which indicates an attempt to execute an invalid CPU instruction. This is likely due to the lack of AVX/AVX2 support.\n\n**Questions:**\n\n1. Are there any LLM models that do not require AVX/AVX2 and can run on my processor?\n2. Is it possible to rebuild `ollama` or the models to support processors without AVX/AVX2?\n3. What alternative solutions or libraries can be used to run LLMs on such systems?\n\n### Relevant log output\n\n```shell\njournalctl -u ollama:\n\n06.05 13:15:31 ASTOR ollama[2690]: [GIN] 2025/06/05 - 13:15:31 | 200 |      33.984µs |       127.0.0.1 | HEAD     \"/\"\n06.05 13:15:31 ASTOR ollama[2690]: [GIN] 2025/06/05 - 13:15:31 | 200 |   20.013513ms |       127.0.0.1 | POST     \"/api/show\"\n06.05 13:15:31 ASTOR ollama[2690]: time=2025-06-05T13:15:31.466+05:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"31.3 GiB\" free=\"29.6 GiB\" free_swap=\"975.0 MiB\"\n06.05 13:15:31 ASTOR ollama[2690]: time=2025-06-05T13:15:31.466+05:00 level=INFO source=server.go:168 msg=offload library=cpu layers.requested=-1 layers.model=25 layers.offload=0 layers.split=\"\" memory.availab>\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: loaded meta data with 26 key-value pairs and 219 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-d040cc18521592f70c199396aeaa44cdc40224079156dc>\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   0:                       general.architecture str              = llama\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   1:                               general.name str              = deepseek-ai\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   4:                          llama.block_count u32              = 24\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5504\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 16\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 16\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 100000.000000\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  11:                    llama.rope.scaling.type str              = linear\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  12:                  llama.rope.scaling.factor f32              = 4.000000\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  13:                          general.file_type u32              = 2\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32256]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32256]   = [0.000000, 0.000000, 0.000000, 0.0000...\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,31757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 32013\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 32021\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 32014\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - type  f32:   49 tensors\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - type q4_0:  169 tensors\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_loader: - type q6_K:    1 tensors\n06.05 13:15:31 ASTOR ollama[2690]: print_info: file format = GGUF V3 (latest)\n06.05 13:15:31 ASTOR ollama[2690]: print_info: file type   = Q4_0\n06.05 13:15:31 ASTOR ollama[2690]: print_info: file size   = 738.88 MiB (4.60 BPW)\n06.05 13:15:31 ASTOR ollama[2690]: load: missing or unrecognized pre-tokenizer type, using: 'default'\n06.05 13:15:31 ASTOR ollama[2690]: load: control-looking token:  32015 '<｜fim▁hole｜>' was not control-type; this is probably a bug in the model. its type will be overridden\n06.05 13:15:31 ASTOR ollama[2690]: load: control-looking token:  32017 '<｜fim▁end｜>' was not control-type; this is probably a bug in the model. its type will be overridden\n06.05 13:15:31 ASTOR ollama[2690]: load: control-looking token:  32016 '<｜fim▁begin｜>' was not control-type; this is probably a bug in the model. its type will be overridden\n06.05 13:15:31 ASTOR ollama[2690]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n06.05 13:15:31 ASTOR ollama[2690]: load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect\n06.05 13:15:31 ASTOR ollama[2690]: load: special tokens cache size = 256\n06.05 13:15:31 ASTOR ollama[2690]: load: token to piece cache size = 0.1792 MB\n06.05 13:15:31 ASTOR ollama[2690]: print_info: arch             = llama\n06.05 13:15:31 ASTOR ollama[2690]: print_info: vocab_only       = 1\n06.05 13:15:31 ASTOR ollama[2690]: print_info: model type       = ?B\n06.05 13:15:31 ASTOR ollama[2690]: print_info: model params     = 1.35 B\n06.05 13:15:31 ASTOR ollama[2690]: print_info: general.name     = deepseek-ai\n06.05 13:15:31 ASTOR ollama[2690]: print_info: vocab type       = BPE\n06.05 13:15:31 ASTOR ollama[2690]: print_info: n_vocab          = 32256\n06.05 13:15:31 ASTOR ollama[2690]: print_info: n_merges         = 31757\n06.05 13:15:31 ASTOR ollama[2690]: print_info: BOS token        = 32013 '<｜begin▁of▁sentence｜>'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: EOS token        = 32021 '<|EOT|>'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: EOT token        = 32014 '<｜end▁of▁sentence｜>'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: PAD token        = 32014 '<｜end▁of▁sentence｜>'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: LF token         = 185 'Ċ'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: FIM PRE token    = 32016 '<｜fim▁begin｜>'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: FIM SUF token    = 32015 '<｜fim▁hole｜>'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: FIM MID token    = 32017 '<｜fim▁end｜>'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: EOG token        = 32014 '<｜end▁of▁sentence｜>'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: EOG token        = 32021 '<|EOT|>'\n06.05 13:15:31 ASTOR ollama[2690]: print_info: max token length = 128\n06.05 13:15:31 ASTOR ollama[2690]: llama_model_load: vocab only - skipping tensors\n06.05 13:15:31 ASTOR ollama[2690]: time=2025-06-05T13:15:31.543+05:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blob>\n06.05 13:15:31 ASTOR ollama[2690]: time=2025-06-05T13:15:31.544+05:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\n06.05 13:15:31 ASTOR ollama[2690]: time=2025-06-05T13:15:31.544+05:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\n06.05 13:15:31 ASTOR ollama[2690]: time=2025-06-05T13:15:31.544+05:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n06.05 13:15:31 ASTOR ollama[2690]: time=2025-06-05T13:15:31.562+05:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\n06.05 13:15:31 ASTOR ollama[2690]: SIGILL: illegal instruction\n06.05 13:15:31 ASTOR ollama[2690]: PC=0x7fcab8065ec8 m=0 sigcode=2\n06.05 13:15:31 ASTOR ollama[2690]: signal arrived during cgo execution\n06.05 13:15:31 ASTOR ollama[2690]: instruction bytes: 0xc4 0xc1 0x7a 0x10 0x54 0x5d 0x0 0xc5 0xea 0x59 0xd 0xf9 0xbe 0x8 0x0 0x48\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 1 gp=0xc000002380 m=0 mp=0x5604dd5df440 [syscall]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.cgocall(0x5604dc6d71e0, 0xc000499588)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/cgocall.go:167 +0x4b fp=0xc000499560 sp=0xc000499528 pc=0x5604dba1aecb\n06.05 13:15:31 ASTOR ollama[2690]: github.com/ollama/ollama/ml/backend/ggml/ggml/src._Cfunc_ggml_backend_load_all_from_path(0x5605018316d0)\n06.05 13:15:31 ASTOR ollama[2690]:         _cgo_gotypes.go:195 +0x3e fp=0xc000499588 sp=0xc000499560 pc=0x5604dbdc65fe\n06.05 13:15:31 ASTOR ollama[2690]: github.com/ollama/ollama/ml/backend/ggml/ggml/src.init.func1.1({0xc00003c044, 0x15})\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/ollama/ollama/ml/backend/ggml/ggml/src/ggml.go:97 +0xf5 fp=0xc000499620 sp=0xc000499588 pc=0x5604dbdc6095\n06.05 13:15:31 ASTOR ollama[2690]: github.com/ollama/ollama/ml/backend/ggml/ggml/src.init.func1()\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/ollama/ollama/ml/backend/ggml/ggml/src/ggml.go:98 +0x526 fp=0xc0004998b0 sp=0xc000499620 pc=0x5604dbdc5ee6\n06.05 13:15:31 ASTOR ollama[2690]: github.com/ollama/ollama/ml/backend/ggml/ggml/src.init.OnceFunc.func2()\n06.05 13:15:31 ASTOR ollama[2690]:         sync/oncefunc.go:27 +0x62 fp=0xc0004998f8 sp=0xc0004998b0 pc=0x5604dbdc58e2\n06.05 13:15:31 ASTOR ollama[2690]: sync.(*Once).doSlow(0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         sync/once.go:78 +0xab fp=0xc000499950 sp=0xc0004998f8 pc=0x5604dba2fe0b\n06.05 13:15:31 ASTOR ollama[2690]: sync.(*Once).Do(0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         sync/once.go:69 +0x19 fp=0xc000499970 sp=0xc000499950 pc=0x5604dba2fd39\n06.05 13:15:31 ASTOR ollama[2690]: github.com/ollama/ollama/ml/backend/ggml/ggml/src.init.OnceFunc.func3()\n06.05 13:15:31 ASTOR ollama[2690]:         sync/oncefunc.go:32 +0x2d fp=0xc0004999a0 sp=0xc000499970 pc=0x5604dbdc584d\n06.05 13:15:31 ASTOR ollama[2690]: github.com/ollama/ollama/llama.BackendInit()\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/ollama/ollama/llama/llama.go:60 +0x16 fp=0xc0004999b0 sp=0xc0004999a0 pc=0x5604dbdca176\n06.05 13:15:31 ASTOR ollama[2690]: github.com/ollama/ollama/runner/llamarunner.Execute({0xc000132020, 0xd, 0xd})\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/ollama/ollama/runner/llamarunner/runner.go:817 +0x63e fp=0xc000499d08 sp=0xc0004999b0 pc=0x5604dbe8637e\n06.05 13:15:31 ASTOR ollama[2690]: github.com/ollama/ollama/runner.Execute({0xc000132010?, 0x0?, 0x0?})\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/ollama/ollama/runner/runner.go:22 +0xd4 fp=0xc000499d30 sp=0xc000499d08 pc=0x5604dbf05ab4\n06.05 13:15:31 ASTOR ollama[2690]: github.com/ollama/ollama/cmd.NewCLI.func2(0xc0000f0e00?, {0x5604dc8b006e?, 0x4?, 0x5604dc8b0072?})\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/ollama/ollama/cmd/cmd.go:1529 +0x45 fp=0xc000499d58 sp=0xc000499d30 pc=0x5604dc655ba5\n06.05 13:15:31 ASTOR ollama[2690]: github.com/spf13/cobra.(*Command).execute(0xc000722f08, {0xc00071c8f0, 0xd, 0xd})\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc000499e78 sp=0xc000499d58 pc=0x5604dbb9575c\n06.05 13:15:31 ASTOR ollama[2690]: github.com/spf13/cobra.(*Command).ExecuteC(0xc0000c3508)\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc000499f30 sp=0xc000499e78 pc=0x5604dbb95fa5\n06.05 13:15:31 ASTOR ollama[2690]: github.com/spf13/cobra.(*Command).Execute(...)\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/spf13/cobra@v1.7.0/command.go:992\n06.05 13:15:31 ASTOR ollama[2690]: github.com/spf13/cobra.(*Command).ExecuteContext(...)\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/spf13/cobra@v1.7.0/command.go:985\n06.05 13:15:31 ASTOR ollama[2690]: main.main()\n06.05 13:15:31 ASTOR ollama[2690]:         github.com/ollama/ollama/main.go:12 +0x4d fp=0xc000499f50 sp=0xc000499f30 pc=0x5604dc65662d\n06.05 13:15:31 ASTOR ollama[2690]: runtime.main()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:283 +0x29d fp=0xc000499fe0 sp=0xc000499f50 pc=0x5604db9ea5bd\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000499fe8 sp=0xc000499fe0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00006efa8 sp=0xc00006ef88 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goparkunlock(...)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:441\n06.05 13:15:31 ASTOR ollama[2690]: runtime.forcegchelper()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:348 +0xb8 fp=0xc00006efe0 sp=0xc00006efa8 pc=0x5604db9ea8f8\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006efe8 sp=0xc00006efe0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.init.7 in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:336 +0x1a\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 3 gp=0xc000003340 m=nil [GC sweep wait]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00006f780 sp=0xc00006f760 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goparkunlock(...)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:441\n06.05 13:15:31 ASTOR ollama[2690]: runtime.bgsweep(0xc00009a000)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgcsweep.go:316 +0xdf fp=0xc00006f7c8 sp=0xc00006f780 pc=0x5604db9d511f\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcenable.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:204 +0x25 fp=0xc00006f7e0 sp=0xc00006f7c8 pc=0x5604db9c9505\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006f7e8 sp=0xc00006f7e0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcenable in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:204 +0x66\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 4 gp=0xc000003500 m=nil [GC scavenge wait]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0x10000?, 0x5604dca6cd88?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00006ff78 sp=0xc00006ff58 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goparkunlock(...)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:441\n06.05 13:15:31 ASTOR ollama[2690]: runtime.(*scavengerState).park(0x5604dd5dc620)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgcscavenge.go:425 +0x49 fp=0xc00006ffa8 sp=0xc00006ff78 pc=0x5604db9d2b69\n06.05 13:15:31 ASTOR ollama[2690]: runtime.bgscavenge(0xc00009a000)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgcscavenge.go:658 +0x59 fp=0xc00006ffc8 sp=0xc00006ffa8 pc=0x5604db9d30f9\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcenable.gowrap2()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:205 +0x25 fp=0xc00006ffe0 sp=0xc00006ffc8 pc=0x5604db9c94a5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006ffe8 sp=0xc00006ffe0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcenable in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:205 +0xa5\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 18 gp=0xc000102700 m=nil [finalizer wait]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0x1b8?, 0xc000002380?, 0x1?, 0x23?, 0xc00006e688?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00006e630 sp=0xc00006e610 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.runfinq()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mfinal.go:196 +0x107 fp=0xc00006e7e0 sp=0xc00006e630 pc=0x5604db9c84c7\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006e7e8 sp=0xc00006e7e0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.createfing in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mfinal.go:166 +0x3d\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 19 gp=0xc000103180 m=nil [chan receive]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0xc0001d59a0?, 0xc000590018?, 0x60?, 0xa7?, 0x5604dbb02e48?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00006a718 sp=0xc00006a6f8 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.chanrecv(0xc000110310, 0x0, 0x1)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/chan.go:664 +0x445 fp=0xc00006a790 sp=0xc00006a718 pc=0x5604db9ba6c5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.chanrecv1(0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/chan.go:506 +0x12 fp=0xc00006a7b8 sp=0xc00006a790 pc=0x5604db9ba252\n06.05 13:15:31 ASTOR ollama[2690]: runtime.unique_runtime_registerUniqueMapCleanup.func2(...)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1796\n06.05 13:15:31 ASTOR ollama[2690]: runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1799 +0x2f fp=0xc00006a7e0 sp=0xc00006a7b8 pc=0x5604db9cc6af\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006a7e8 sp=0xc00006a7e0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by unique.runtime_registerUniqueMapCleanup in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1794 +0x85\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 20 gp=0xc000103500 m=nil [GC worker (idle)]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00006af38 sp=0xc00006af18 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkWorker(0xc000111730)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1423 +0xe9 fp=0xc00006afc8 sp=0xc00006af38 pc=0x5604db9cb9c9\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkStartWorkers.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x25 fp=0xc00006afe0 sp=0xc00006afc8 pc=0x5604db9cb8a5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006afe8 sp=0xc00006afe0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcBgMarkStartWorkers in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x105\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 34 gp=0xc000504000 m=nil [GC worker (idle)]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00050a738 sp=0xc00050a718 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkWorker(0xc000111730)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1423 +0xe9 fp=0xc00050a7c8 sp=0xc00050a738 pc=0x5604db9cb9c9\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkStartWorkers.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x25 fp=0xc00050a7e0 sp=0xc00050a7c8 pc=0x5604db9cb8a5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00050a7e8 sp=0xc00050a7e0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcBgMarkStartWorkers in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x105\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 5 gp=0xc000003a40 m=nil [GC worker (idle)]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0xc718f1e442c?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc000070738 sp=0xc000070718 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkWorker(0xc000111730)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1423 +0xe9 fp=0xc0000707c8 sp=0xc000070738 pc=0x5604db9cb9c9\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkStartWorkers.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x25 fp=0xc0000707e0 sp=0xc0000707c8 pc=0x5604db9cb8a5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000707e8 sp=0xc0000707e0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcBgMarkStartWorkers in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x105\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 21 gp=0xc0001036c0 m=nil [GC worker (idle)]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0xc718f1cf17b?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00006b738 sp=0xc00006b718 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkWorker(0xc000111730)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1423 +0xe9 fp=0xc00006b7c8 sp=0xc00006b738 pc=0x5604db9cb9c9\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkStartWorkers.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x25 fp=0xc00006b7e0 sp=0xc00006b7c8 pc=0x5604db9cb8a5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006b7e8 sp=0xc00006b7e0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcBgMarkStartWorkers in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x105\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 35 gp=0xc0005041c0 m=nil [GC worker (idle)]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0xc718f1d7439?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00050af38 sp=0xc00050af18 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkWorker(0xc000111730)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1423 +0xe9 fp=0xc00050afc8 sp=0xc00050af38 pc=0x5604db9cb9c9\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkStartWorkers.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x25 fp=0xc00050afe0 sp=0xc00050afc8 pc=0x5604db9cb8a5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00050afe8 sp=0xc00050afe0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcBgMarkStartWorkers in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x105\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 6 gp=0xc000003c00 m=nil [GC worker (idle)]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0xc718f1ceb15?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc000070f38 sp=0xc000070f18 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkWorker(0xc000111730)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1423 +0xe9 fp=0xc000070fc8 sp=0xc000070f38 pc=0x5604db9cb9c9\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkStartWorkers.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x25 fp=0xc000070fe0 sp=0xc000070fc8 pc=0x5604db9cb8a5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000070fe8 sp=0xc000070fe0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcBgMarkStartWorkers in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x105\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 22 gp=0xc000103880 m=nil [GC worker (idle)]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0xc718f1cfb3b?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00006bf38 sp=0xc00006bf18 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkWorker(0xc000111730)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1423 +0xe9 fp=0xc00006bfc8 sp=0xc00006bf38 pc=0x5604db9cb9c9\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkStartWorkers.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x25 fp=0xc00006bfe0 sp=0xc00006bfc8 pc=0x5604db9cb8a5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00006bfe8 sp=0xc00006bfe0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcBgMarkStartWorkers in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x105\n06.05 13:15:31 ASTOR ollama[2690]: goroutine 36 gp=0xc000504380 m=nil [GC worker (idle)]:\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gopark(0xc718f1cd851?, 0x0?, 0x0?, 0x0?, 0x0?)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/proc.go:435 +0xce fp=0xc00050b738 sp=0xc00050b718 pc=0x5604dba1e1ce\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkWorker(0xc000111730)\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1423 +0xe9 fp=0xc00050b7c8 sp=0xc00050b738 pc=0x5604db9cb9c9\n06.05 13:15:31 ASTOR ollama[2690]: runtime.gcBgMarkStartWorkers.gowrap1()\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x25 fp=0xc00050b7e0 sp=0xc00050b7c8 pc=0x5604db9cb8a5\n06.05 13:15:31 ASTOR ollama[2690]: runtime.goexit({})\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00050b7e8 sp=0xc00050b7e0 pc=0x5604dba25901\n06.05 13:15:31 ASTOR ollama[2690]: created by runtime.gcBgMarkStartWorkers in goroutine 1\n06.05 13:15:31 ASTOR ollama[2690]:         runtime/mgc.go:1339 +0x105\n06.05 13:15:31 ASTOR ollama[2690]: rax    0x32f80f4bb\n06.05 13:15:31 ASTOR ollama[2690]: rbx    0x0\n06.05 13:15:31 ASTOR ollama[2690]: rcx    0x0\n06.05 13:15:31 ASTOR ollama[2690]: rdx    0xd767b\n06.05 13:15:31 ASTOR ollama[2690]: rdi    0x6f4e07\n06.05 13:15:31 ASTOR ollama[2690]: rsi    0x32f737e40\n06.05 13:15:31 ASTOR ollama[2690]: rbp    0x7fcab810aca0\n06.05 13:15:31 ASTOR ollama[2690]: rsp    0x7ffea1cf0090\n06.05 13:15:31 ASTOR ollama[2690]: r8     0x7\n06.05 13:15:31 ASTOR ollama[2690]: r9     0x0\n06.05 13:15:31 ASTOR ollama[2690]: r10    0x7ffea1dc4080\n06.05 13:15:31 ASTOR ollama[2690]: r11    0x182ac8\n06.05 13:15:31 ASTOR ollama[2690]: r12    0x7fcab812aca0\n06.05 13:15:31 ASTOR ollama[2690]: r13    0x7fcab81dce80\n06.05 13:15:31 ASTOR ollama[2690]: r14    0x7fcab80efbd0\n06.05 13:15:31 ASTOR ollama[2690]: r15    0x7ffea1cf0120\n06.05 13:15:31 ASTOR ollama[2690]: rip    0x7fcab8065ec8\n06.05 13:15:31 ASTOR ollama[2690]: rflags 0x10246\n06.05 13:15:31 ASTOR ollama[2690]: cs     0x33\n06.05 13:15:31 ASTOR ollama[2690]: fs     0x0\n06.05 13:15:31 ASTOR ollama[2690]: gs     0x0\n06.05 13:15:31 ASTOR ollama[2690]: time=2025-06-05T13:15:31.795+05:00 level=ERROR source=sched.go:489 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 2\"\n06.05 13:15:31 ASTOR ollama[2690]: [GIN] 2025/06/05 - 13:15:31 | 500 |  355.578563ms |       127.0.0.1 | POST     \"/api/generate\"\n\n:~$ lscpu\nArchitecture:             x86_64\n  CPU op-mode(s):         32-bit, 64-bit\n  Address sizes:          45 bits physical, 48 bits virtual\n  Byte Order:             Little Endian\nCPU(s):                   8\n  On-line CPU(s) list:    0-7\nVendor ID:                GenuineIntel\n  Model name:             Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz\n    CPU family:           6\n    Model:                85\n    Thread(s) per core:   1\n    Core(s) per socket:   1\n    Socket(s):            8\n    Stepping:             4\n    BogoMIPS:             4600,00\n    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable n\n                          onstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpc\n                          id_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 smep bmi2 invpcid rdseed adx smap clflushopt clwb arat ospke md_clear flush_l1d arch_capabilities\nVirtualization features:  \n  Hypervisor vendor:      VMware\n  Virtualization type:    full\nCaches (sum of all):      \n  L1d:                    256 KiB (8 instances)\n  L1i:                    256 KiB (8 instances)\n  L2:                     8 MiB (8 instances)\n  L3:                     132 MiB (8 instances)\nNUMA:                     \n  NUMA node(s):           1\n  NUMA node0 CPU(s):      0-7\nVulnerabilities:          \n  Gather data sampling:   Unknown: Dependent on hypervisor status\n  Itlb multihit:          KVM: Mitigation: VMX unsupported\n  L1tf:                   Mitigation; PTE Inversion\n  Mds:                    Mitigation; Clear CPU buffers; SMT Host state unknown\n  Meltdown:               Mitigation; PTI\n  Mmio stale data:        Mitigation; Clear CPU buffers; SMT Host state unknown\n  Reg file data sampling: Not affected\n  Retbleed:               Mitigation; IBRS\n  Spec rstack overflow:   Not affected\n  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n  Spectre v2:             Mitigation; IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI SW loop, KVM SW loop\n  Srbds:                  Not affected\n  Tsx async abort:        Not affected\n\n\n:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:            31Gi       1,8Gi        21Gi        61Mi       8,3Gi        29Gi\nSwap:          974Mi          0B       974Mi\n\n\n:~$ cat /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"12\"\nVERSION=\"12 (bookworm)\"\nVERSION_CODENAME=bookworm\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n\n\n:~$ ldd --version\nldd (Debian GLIBC 2.36-9+deb12u10) 2.36\nCopyright (C) 2022 Free Software Foundation, Inc.\n\n\n:~$ ollama --version\nollama version is 0.9.0\n\n\n:~$ ollama list\nNAME                        ID              SIZE      MODIFIED     \ndeepseek-coder:1.3b         3ddd2d3fc8d2    776 MB    2 hours ago     \nllama2:7b                   78e26419b446    3.8 GB    2 hours ago     \ndeepseek-r1:1.5b            e0979632db5a    1.1 GB    20 hours ago    \nmxbai-embed-large:latest    468836162de7    669 MB    25 hours ago\n```\n\n### OS\n\nLinux\n\n### GPU\n\nOther\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.9.0",
    "comments": [
      {
        "user": "rick-github",
        "body": "ollama builds different backends for each architecture type and then loads the backend appropriate for the CPU it's running on.  It's possible there are CPU lines that vary their supported instructions such that ollama is choosing an inappropriate backend.  If you run the serve with `OLLAMA_DEBUG=1` the logs will show which backend is being used, and as a brute force solution you could delete it.  ollama will then choose a backend from the remaining set.\n\nYou could also edit [CMakeLists.txt](https://github.com/ollama/ollama/blob/main/ml/backend/ggml/ggml/src/CMakeLists.txt) and remove the flags around line 293 to build a backend free of AVX instructions."
      },
      {
        "user": "mrMastor",
        "body": "Thank you very much! Manual compilation of `llama.cpp` helped.\nI commented out the following section:\n```\nif (GGML_CPU_ALL_VARIANTS)\n    if (NOT GGML_BACKEND_DL)\n        message(FATAL_ERROR \"GGML_CPU_ALL_VARIANTS requires GGML_BACKEND_DL\")\n    endif()\n    add_custom_target(ggml-cpu)\n    ggml_add_cpu_backend_variant(x64)\n    ggml_add_cpu_backend_variant(sse42        SSE42)\n    #ggml_add_cpu_backend_variant(sandybridge  SSE42 AVX)\n    #ggml_add_cpu_backend_variant(haswell      SSE42 AVX F16C AVX2 BMI2 FMA)\n    #ggml_add_cpu_backend_variant(skylakex     SSE42 AVX F16C AVX2 BMI2 FMA AVX512)\n    #ggml_add_cpu_backend_variant(icelake      SSE42 AVX F16C AVX2 BMI2 FMA AVX512 AVX512_VBMI AVX512_VNNI)\n    #ggml_add_cpu_backend_variant(alderlake    SSE42 AVX F16C AVX2 BMI2 FMA AVX_VNNI)\nelseif (GGML_CPU)\n    ggml_add_cpu_backend_variant_impl(\"\")\nendif()\n```"
      }
    ]
  },
  {
    "issue_number": 6294,
    "title": "AirLLM integration?",
    "author": "blankuserrr",
    "state": "open",
    "created_at": "2024-08-09T23:34:06Z",
    "updated_at": "2025-06-06T06:38:14Z",
    "labels": [
      "feature request"
    ],
    "body": "I'd love to see the addition/support of [AirLLM](https://github.com/lyogavin/airllm) in ollama, as it can massively decrease the needed amount of vram to run large models.",
    "comments": [
      {
        "user": "mdlmarkham",
        "body": "+1 My home-lab has grown more or less organically over the last 10 years... and includes a lot of castoff gaming hardware. It would be great if Ollama could incorporate features to let me get more out of what I have. Both the compression methods used by [AirLLM](https://github.com/lyogavin/airllm) as well as features that would allow coordination of multiple instances across a local network would be fantastic. Keep up the good work!"
      },
      {
        "user": "EkkiBrue",
        "body": "+1 +1 +1 ;)"
      },
      {
        "user": "Xyz00777",
        "body": "do i understood airllm correct? I think it's used to give it a model, and it's kind of recompile it, so it will be smaller after it with the same data (I know the word recompile is not really correct but i dont know a better word at the moment). So it would be most useful to use it in the pull process (with an additional option) after checking if the sha256 of the model is correct, then give it to airllm to get recompiled and after that stored as a smaller model. Correct?"
      }
    ]
  },
  {
    "issue_number": 10992,
    "title": "amd llama runner process has terminated: exit status 0xc0000409",
    "author": "FAIpang",
    "state": "closed",
    "created_at": "2025-06-06T06:03:07Z",
    "updated_at": "2025-06-06T06:04:17Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\n[server-2.log](https://github.com/user-attachments/files/20622674/server-2.log)\n\nOS\nWindows 11\n\nGPU\nAMD Radeon(TM) 8060S Graphics\n\nCPU\nAMD RYZEN AI MAX+395 w/ Radeon 8060S\n\nOllama version\nOllama 0.7.0\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\n_No response_\n\n### GPU\n\n_No response_\n\n### CPU\n\n_No response_\n\n### Ollama version\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 10991,
    "title": "ollama crash running Gemma3:27b",
    "author": "mario-grgic",
    "state": "closed",
    "created_at": "2025-06-06T01:19:37Z",
    "updated_at": "2025-06-06T03:17:21Z",
    "labels": [
      "bug"
    ],
    "body": "### What is the issue?\n\nollama crashes when running gemma3:27b. I did git bisect and it looks like the behaviour was introduced in the following commit:\n```\naaa7818000c42a82fc030212c35ef83f9799efd7 is the first bad commit\ncommit aaa7818000c42a82fc030212c35ef83f9799efd7\nAuthor: Jesse Gross <jesse@ollama.com>\nDate:   Thu Apr 24 11:48:49 2025 -0700\n\n    ggml: Export GPU UUIDs\n    \n    This enables matching up devices and information reported by the backend\n    with system management libraries such as nvml to get accurate free\n    memory reporting.\n\n llama/patches/0017-ggml-Export-GPU-UUIDs.patch   | 102 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n ml/backend.go                                    |   8 +++++\n ml/backend/ggml/ggml.go                          |   6 ++++\n ml/backend/ggml/ggml/include/ggml-backend.h      |   1 +\n ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu  |  33 +++++++++++++++++++++\n ml/backend/ggml/ggml/src/ggml-metal/ggml-metal.m |   1 +\n 6 files changed, 151 insertions(+)\n create mode 100644 llama/patches/0017-ggml-Export-GPU-UUIDs.patch\n```\n\ntag 0.9.0 works fine.\n\nHere is the crash log:\n\n```\n$ ./ollama serve\ntime=2025-06-05T21:10:01.164-04:00 level=INFO source=routes.go:1242 msg=\"server config\" env=\"map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/mariogrgic/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:true OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]\"\ntime=2025-06-05T21:10:01.165-04:00 level=INFO source=images.go:479 msg=\"total blobs: 16\"\ntime=2025-06-05T21:10:01.165-04:00 level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n - using env:\texport GIN_MODE=release\n - using code:\tgin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func3 (5 handlers)\n[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func4 (5 handlers)\n[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)\ntime=2025-06-05T21:10:01.166-04:00 level=INFO source=routes.go:1295 msg=\"Listening on 127.0.0.1:11434 (version 0.0.0)\"\ntime=2025-06-05T21:10:01.195-04:00 level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=metal variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"96.0 GiB\" available=\"96.0 GiB\"\n[GIN] 2025/06/05 - 21:10:06 | 200 |     222.334µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/06/05 - 21:10:06 | 200 |   49.765208ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-06-05T21:10:06.725-04:00 level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/Users/mariogrgic/.ollama/models/blobs/sha256-e796792eba26c4d3b04b0ac5adb01a453dd9ec2dfd83b6c59cbf6fe5f30b0f68 gpu=0 parallel=2 available=103079215104 required=\"20.1 GiB\"\ntime=2025-06-05T21:10:06.725-04:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"128.0 GiB\" free=\"117.4 GiB\" free_swap=\"0 B\"\ntime=2025-06-05T21:10:06.726-04:00 level=INFO source=server.go:168 msg=offload library=metal layers.requested=-1 layers.model=63 layers.offload=63 layers.split=\"\" memory.available=\"[96.0 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"20.1 GiB\" memory.required.partial=\"20.1 GiB\" memory.required.kv=\"1.6 GiB\" memory.required.allocations=\"[20.1 GiB]\" memory.weights.total=\"15.4 GiB\" memory.weights.repeating=\"14.3 GiB\" memory.weights.nonrepeating=\"1.1 GiB\" memory.graph.full=\"565.0 MiB\" memory.graph.partial=\"565.0 MiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\ntime=2025-06-05T21:10:06.756-04:00 level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/Volumes/DATA/dev/ollama/ollama.git/ollama runner --ollama-engine --model /Users/mariogrgic/.ollama/models/blobs/sha256-e796792eba26c4d3b04b0ac5adb01a453dd9ec2dfd83b6c59cbf6fe5f30b0f68 --ctx-size 8192 --batch-size 512 --n-gpu-layers 63 --threads 12 --parallel 2 --port 49300\"\ntime=2025-06-05T21:10:06.757-04:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\ntime=2025-06-05T21:10:06.757-04:00 level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-05T21:10:06.757-04:00 level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-06-05T21:10:06.765-04:00 level=INFO source=runner.go:925 msg=\"starting ollama engine\"\ntime=2025-06-05T21:10:06.766-04:00 level=INFO source=runner.go:983 msg=\"Server listening on 127.0.0.1:49300\"\ntime=2025-06-05T21:10:06.793-04:00 level=INFO source=ggml.go:92 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=1247 num_key_values=37\ntime=2025-06-05T21:10:06.793-04:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)\nunexpected fault address 0x2000000000\nfatal error: fault\n[signal SIGBUS: bus error code=0x1 addr=0x2000000000 pc=0x100edea94]\n\ngoroutine 14 gp=0x14000103a40 m=8 mp=0x14000600008 [running]:\nruntime.throw({0x101c05000?, 0x0?})\n\t/usr/local/go/src/runtime/panic.go:1101 +0x38 fp=0x1400004d100 sp=0x1400004d0d0 pc=0x100f4c788\nruntime.sigpanic()\n\t/usr/local/go/src/runtime/signal_unix.go:922 +0x170 fp=0x1400004d160 sp=0x1400004d100 pc=0x100f4ebf0\nindexbytebody()\n\t/usr/local/go/src/internal/bytealg/indexbyte_arm64.s:74 +0x64 fp=0x1400004d170 sp=0x1400004d170 pc=0x100edea94\nruntime.findnull(0x1400004d208?)\n\t/usr/local/go/src/runtime/string.go:577 +0x78 fp=0x1400004d1d0 sp=0x1400004d170 pc=0x100f35ba8\nruntime.gostring(0x2000000000)\n\t/usr/local/go/src/runtime/string.go:363 +0x20 fp=0x1400004d210 sp=0x1400004d1d0 pc=0x100f4fa60\ngithub.com/ollama/ollama/ml/backend/ggml._Cfunc_GoString(...)\n\t_cgo_gotypes.go:300\ngithub.com/ollama/ollama/ml/backend/ggml.New({0x16ef26ca2, 0x6e}, {0xc, 0x0, 0x3f, {0x0, 0x0, 0x0}, 0x0})\n\t/Volumes/DATA/dev/ollama/ollama.git/ml/backend/ggml/ggml.go:141 +0xaa4 fp=0x1400004dc00 sp=0x1400004d210 pc=0x1013180d4\ngithub.com/ollama/ollama/ml.NewBackend({0x16ef26ca2, 0x6e}, {0xc, 0x0, 0x3f, {0x0, 0x0, 0x0}, 0x0})\n\t/Volumes/DATA/dev/ollama/ollama.git/ml/backend.go:209 +0x90 fp=0x1400004dc60 sp=0x1400004dc00 pc=0x10130dc90\ngithub.com/ollama/ollama/model.New({0x16ef26ca2?, 0x0?}, {0xc, 0x0, 0x3f, {0x0, 0x0, 0x0}, 0x0})\n\t/Volumes/DATA/dev/ollama/ollama.git/model/model.go:102 +0x5c fp=0x1400004dd60 sp=0x1400004dc60 pc=0x10132464c\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).initModel(0x140004b9b00, {0x16ef26ca2?, 0x0?}, {0xc, 0x0, 0x3f, {0x0, 0x0, 0x0}, 0x0}, ...)\n\t/Volumes/DATA/dev/ollama/ollama.git/runner/ollamarunner/runner.go:841 +0x80 fp=0x1400004ddc0 sp=0x1400004dd60 pc=0x10139d3a0\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).load(0x140004b9b00, {0x102076dd0, 0x14000390410}, {0x16ef26ca2?, 0x0?}, {0xc, 0x0, 0x3f, {0x0, 0x0, ...}, ...}, ...)\n\t/Volumes/DATA/dev/ollama/ollama.git/runner/ollamarunner/runner.go:878 +0xa8 fp=0x1400004df20 sp=0x1400004ddc0 pc=0x10139d6b8\ngithub.com/ollama/ollama/runner/ollamarunner.Execute.gowrap1()\n\t/Volumes/DATA/dev/ollama/ollama.git/runner/ollamarunner/runner.go:959 +0x88 fp=0x1400004dfd0 sp=0x1400004df20 pc=0x10139e738\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x1400004dfd0 sp=0x1400004dfd0 pc=0x100f54c64\ncreated by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1\n\t/Volumes/DATA/dev/ollama/ollama.git/runner/ollamarunner/runner.go:959 +0x840\n\ngoroutine 1 gp=0x140000021c0 m=nil [IO wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x100f625b0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x140001335e0 sp=0x140001335c0 pc=0x100f4c8a8\nruntime.netpollblock(0x140004b5678?, 0xfd1170?, 0x1?)\n\t/usr/local/go/src/runtime/netpoll.go:575 +0x158 fp=0x14000133620 sp=0x140001335e0 pc=0x100f12728\ninternal/poll.runtime_pollWait(0x149b16f30, 0x72)\n\t/usr/local/go/src/runtime/netpoll.go:351 +0xa0 fp=0x14000133650 sp=0x14000133620 pc=0x100f4ba60\ninternal/poll.(*pollDesc).wait(0x14000055780?, 0x101f3fb40?, 0x0)\n\t/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x28 fp=0x14000133680 sp=0x14000133650 pc=0x100fcc988\ninternal/poll.(*pollDesc).waitRead(...)\n\t/usr/local/go/src/internal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Accept(0x14000055780)\n\t/usr/local/go/src/internal/poll/fd_unix.go:620 +0x24c fp=0x14000133730 sp=0x14000133680 pc=0x100fd125c\nnet.(*netFD).accept(0x14000055780)\n\t/usr/local/go/src/net/fd_unix.go:172 +0x28 fp=0x140001337f0 sp=0x14000133730 pc=0x1010403c8\nnet.(*TCPListener).accept(0x1400043b100)\n\t/usr/local/go/src/net/tcpsock_posix.go:159 +0x24 fp=0x14000133840 sp=0x140001337f0 pc=0x101054624\nnet.(*TCPListener).Accept(0x1400043b100)\n\t/usr/local/go/src/net/tcpsock.go:380 +0x2c fp=0x14000133880 sp=0x14000133840 pc=0x10105360c\nnet/http.(*onceCloseListener).Accept(0x102076d60?)\n\t<autogenerated>:1 +0x30 fp=0x140001338a0 sp=0x14000133880 pc=0x10122eb40\nnet/http.(*Server).Serve(0x1400011f100, {0x102074978, 0x1400043b100})\n\t/usr/local/go/src/net/http/server.go:3424 +0x290 fp=0x140001339d0 sp=0x140001338a0 pc=0x1012081e0\ngithub.com/ollama/ollama/runner/ollamarunner.Execute({0x14000000270, 0xe, 0xf})\n\t/Volumes/DATA/dev/ollama/ollama.git/runner/ollamarunner/runner.go:984 +0xb78 fp=0x14000133ce0 sp=0x140001339d0 pc=0x10139e408\ngithub.com/ollama/ollama/runner.Execute({0x14000000250?, 0x0?, 0x0?})\n\t/Volumes/DATA/dev/ollama/ollama.git/runner/runner.go:20 +0x120 fp=0x14000133d10 sp=0x14000133ce0 pc=0x10139ecb0\ngithub.com/ollama/ollama/cmd.NewCLI.func2(0x1400011ef00?, {0x101c03c5e?, 0x4?, 0x101c03c62?})\n\t/Volumes/DATA/dev/ollama/ollama.git/cmd/cmd.go:1529 +0x54 fp=0x14000133d40 sp=0x14000133d10 pc=0x1019e9984\ngithub.com/spf13/cobra.(*Command).execute(0x140004e6f08, {0x14000434f00, 0xf, 0xf})\n\t/Volumes/DATA/dev/ollama/ollama.git/gopath/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940 +0x648 fp=0x14000133e60 sp=0x14000133d40 pc=0x1010ae968\ngithub.com/spf13/cobra.(*Command).ExecuteC(0x140004cc908)\n\t/Volumes/DATA/dev/ollama/ollama.git/gopath/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068 +0x320 fp=0x14000133f20 sp=0x14000133e60 pc=0x1010af0b0\ngithub.com/spf13/cobra.(*Command).Execute(...)\n\t/Volumes/DATA/dev/ollama/ollama.git/gopath/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\ngithub.com/spf13/cobra.(*Command).ExecuteContext(...)\n\t/Volumes/DATA/dev/ollama/ollama.git/gopath/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:985\nmain.main()\n\t/Volumes/DATA/dev/ollama/ollama.git/main.go:12 +0x54 fp=0x14000133f40 sp=0x14000133f20 pc=0x1019ea464\nruntime.main()\n\t/usr/local/go/src/runtime/proc.go:283 +0x284 fp=0x14000133fd0 sp=0x14000133f40 pc=0x100f19244\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x14000133fd0 sp=0x14000133fd0 pc=0x100f54c64\n\ngoroutine 2 gp=0x14000002c40 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000084f90 sp=0x14000084f70 pc=0x100f4c8a8\nruntime.goparkunlock(...)\n\t/usr/local/go/src/runtime/proc.go:441\nruntime.forcegchelper()\n\t/usr/local/go/src/runtime/proc.go:348 +0xb8 fp=0x14000084fd0 sp=0x14000084f90 pc=0x100f19598\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x14000084fd0 sp=0x14000084fd0 pc=0x100f54c64\ncreated by runtime.init.7 in goroutine 1\n\t/usr/local/go/src/runtime/proc.go:336 +0x24\n\ngoroutine 3 gp=0x14000003180 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000085760 sp=0x14000085740 pc=0x100f4c8a8\nruntime.goparkunlock(...)\n\t/usr/local/go/src/runtime/proc.go:441\nruntime.bgsweep(0x140000b0000)\n\t/usr/local/go/src/runtime/mgcsweep.go:316 +0x108 fp=0x140000857b0 sp=0x14000085760 pc=0x100f046c8\nruntime.gcenable.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:204 +0x28 fp=0x140000857d0 sp=0x140000857b0 pc=0x100ef84c8\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x140000857d0 sp=0x140000857d0 pc=0x100f54c64\ncreated by runtime.gcenable in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:204 +0x6c\n\ngoroutine 4 gp=0x14000003340 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x101db4f78?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000085f60 sp=0x14000085f40 pc=0x100f4c8a8\nruntime.goparkunlock(...)\n\t/usr/local/go/src/runtime/proc.go:441\nruntime.(*scavengerState).park(0x102917f40)\n\t/usr/local/go/src/runtime/mgcscavenge.go:425 +0x5c fp=0x14000085f90 sp=0x14000085f60 pc=0x100f0215c\nruntime.bgscavenge(0x140000b0000)\n\t/usr/local/go/src/runtime/mgcscavenge.go:658 +0xac fp=0x14000085fb0 sp=0x14000085f90 pc=0x100f026fc\nruntime.gcenable.gowrap2()\n\t/usr/local/go/src/runtime/mgc.go:205 +0x28 fp=0x14000085fd0 sp=0x14000085fb0 pc=0x100ef8468\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x14000085fd0 sp=0x14000085fd0 pc=0x100f54c64\ncreated by runtime.gcenable in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:205 +0xac\n\ngoroutine 5 gp=0x14000003c00 m=nil [finalizer wait]:\nruntime.gopark(0x18000845c8?, 0x1000000000000?, 0xf8?, 0x45?, 0x1012314ac?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000084590 sp=0x14000084570 pc=0x100f4c8a8\nruntime.runfinq()\n\t/usr/local/go/src/runtime/mfinal.go:196 +0x108 fp=0x140000847d0 sp=0x14000084590 pc=0x100ef74c8\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x140000847d0 sp=0x140000847d0 pc=0x100f54c64\ncreated by runtime.createfing in goroutine 1\n\t/usr/local/go/src/runtime/mfinal.go:166 +0x80\n\ngoroutine 6 gp=0x140001ee700 m=nil [chan receive]:\nruntime.gopark(0x14000233680?, 0x1400000e1b0?, 0x48?, 0x67?, 0x101014698?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x140000866f0 sp=0x140000866d0 pc=0x100f4c8a8\nruntime.chanrecv(0x140000be310, 0x0, 0x1)\n\t/usr/local/go/src/runtime/chan.go:664 +0x42c fp=0x14000086770 sp=0x140000866f0 pc=0x100ee9f7c\nruntime.chanrecv1(0x0?, 0x0?)\n\t/usr/local/go/src/runtime/chan.go:506 +0x14 fp=0x140000867a0 sp=0x14000086770 pc=0x100ee9b14\nruntime.unique_runtime_registerUniqueMapCleanup.func2(...)\n\t/usr/local/go/src/runtime/mgc.go:1796\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1799 +0x3c fp=0x140000867d0 sp=0x140000867a0 pc=0x100efb6ec\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x140000867d0 sp=0x140000867d0 pc=0x100f54c64\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1794 +0x78\n\ngoroutine 7 gp=0x140001eee00 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000086f10 sp=0x14000086ef0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x14000086fb0 sp=0x14000086f10 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x14000086fd0 sp=0x14000086fb0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x14000086fd0 sp=0x14000086fd0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 18 gp=0x14000504000 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000080710 sp=0x140000806f0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x140000807b0 sp=0x14000080710 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x140000807d0 sp=0x140000807b0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x140000807d0 sp=0x140000807d0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 34 gp=0x14000102380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x1400011a710 sp=0x1400011a6f0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x1400011a7b0 sp=0x1400011a710 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x1400011a7d0 sp=0x1400011a7b0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x1400011a7d0 sp=0x1400011a7d0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 35 gp=0x14000102540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x1400011af10 sp=0x1400011aef0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x1400011afb0 sp=0x1400011af10 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x1400011afd0 sp=0x1400011afb0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x1400011afd0 sp=0x1400011afd0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 8 gp=0x140001eefc0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000087710 sp=0x140000876f0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x140000877b0 sp=0x14000087710 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x140000877d0 sp=0x140000877b0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x140000877d0 sp=0x140000877d0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 19 gp=0x140005041c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000080f10 sp=0x14000080ef0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x14000080fb0 sp=0x14000080f10 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x14000080fd0 sp=0x14000080fb0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x14000080fd0 sp=0x14000080fd0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 20 gp=0x14000504380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000081710 sp=0x140000816f0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x140000817b0 sp=0x14000081710 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x140000817d0 sp=0x140000817b0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x140000817d0 sp=0x140000817d0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 36 gp=0x14000102700 m=nil [GC worker (idle)]:\nruntime.gopark(0x26313f85800?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x1400011b710 sp=0x1400011b6f0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x1400011b7b0 sp=0x1400011b710 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x1400011b7d0 sp=0x1400011b7b0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x1400011b7d0 sp=0x1400011b7d0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 37 gp=0x140001028c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x102948780?, 0x1?, 0xc?, 0xa8?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x1400011bf10 sp=0x1400011bef0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x1400011bfb0 sp=0x1400011bf10 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x1400011bfd0 sp=0x1400011bfb0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x1400011bfd0 sp=0x1400011bfd0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 38 gp=0x14000102a80 m=nil [GC worker (idle)]:\nruntime.gopark(0x26313daa359?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x1400011c710 sp=0x1400011c6f0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x1400011c7b0 sp=0x1400011c710 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x1400011c7d0 sp=0x1400011c7b0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x1400011c7d0 sp=0x1400011c7d0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 39 gp=0x14000102c40 m=nil [GC worker (idle)]:\nruntime.gopark(0x26313da80ae?, 0x3?, 0x5e?, 0x1b?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x1400011cf10 sp=0x1400011cef0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x1400011cfb0 sp=0x1400011cf10 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x1400011cfd0 sp=0x1400011cfb0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x1400011cfd0 sp=0x1400011cfd0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 9 gp=0x140001ef180 m=nil [GC worker (idle)]:\nruntime.gopark(0x26313f87e6a?, 0x1?, 0x18?, 0x5f?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000087f10 sp=0x14000087ef0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x14000087fb0 sp=0x14000087f10 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x14000087fd0 sp=0x14000087fb0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x14000087fd0 sp=0x14000087fd0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 10 gp=0x140001ef340 m=nil [GC worker (idle)]:\nruntime.gopark(0x26313da7de9?, 0x3?, 0x53?, 0x8?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000116710 sp=0x140001166f0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x140001167b0 sp=0x14000116710 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x140001167d0 sp=0x140001167b0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x140001167d0 sp=0x140001167d0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 11 gp=0x140001ef500 m=nil [GC worker (idle)]:\nruntime.gopark(0x26313da7643?, 0x1?, 0x12?, 0xe3?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000116f10 sp=0x14000116ef0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x14000116fb0 sp=0x14000116f10 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x14000116fd0 sp=0x14000116fb0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x14000116fd0 sp=0x14000116fd0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 12 gp=0x140001ef6c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x26313da7a2b?, 0x1?, 0x6f?, 0xa9?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000117710 sp=0x140001176f0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x140001177b0 sp=0x14000117710 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x140001177d0 sp=0x140001177b0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x140001177d0 sp=0x140001177d0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 13 gp=0x140001ef880 m=nil [GC worker (idle)]:\nruntime.gopark(0x26313da7b4f?, 0x1?, 0xc3?, 0xb?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x14000117f10 sp=0x14000117ef0 pc=0x100f4c8a8\nruntime.gcBgMarkWorker(0x140000bf730)\n\t/usr/local/go/src/runtime/mgc.go:1423 +0xdc fp=0x14000117fb0 sp=0x14000117f10 pc=0x100efa95c\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x28 fp=0x14000117fd0 sp=0x14000117fb0 pc=0x100efa848\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x14000117fd0 sp=0x14000117fd0 pc=0x100f54c64\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/usr/local/go/src/runtime/mgc.go:1339 +0x140\n\ngoroutine 15 gp=0x14000103c00 m=nil [sync.WaitGroup.Wait]:\nruntime.gopark(0x1029266a0?, 0x0?, 0x60?, 0x20?, 0x0?)\n\t/usr/local/go/src/runtime/proc.go:435 +0xc8 fp=0x140001196d0 sp=0x140001196b0 pc=0x100f4c8a8\nruntime.goparkunlock(...)\n\t/usr/local/go/src/runtime/proc.go:441\nruntime.semacquire1(0x140004b9b08, 0x0, 0x1, 0x0, 0x18)\n\t/usr/local/go/src/runtime/sema.go:188 +0x204 fp=0x14000119720 sp=0x140001196d0 pc=0x100f2d724\nsync.runtime_SemacquireWaitGroup(0x0?)\n\t/usr/local/go/src/runtime/sema.go:110 +0x2c fp=0x14000119760 sp=0x14000119720 pc=0x100f4e31c\nsync.(*WaitGroup).Wait(0x140004b9b00)\n\t/usr/local/go/src/sync/waitgroup.go:118 +0x70 fp=0x14000119780 sp=0x14000119760 pc=0x100f60540\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).run(0x140004b9b00, {0x102076dd0, 0x14000390410})\n\t/Volumes/DATA/dev/ollama/ollama.git/runner/ollamarunner/runner.go:355 +0x2c fp=0x140001197a0 sp=0x14000119780 pc=0x101399e9c\ngithub.com/ollama/ollama/runner/ollamarunner.Execute.gowrap2()\n\t/Volumes/DATA/dev/ollama/ollama.git/runner/ollamarunner/runner.go:960 +0x30 fp=0x140001197d0 sp=0x140001197a0 pc=0x10139e670\nruntime.goexit({})\n\t/usr/local/go/src/runtime/asm_arm64.s:1223 +0x4 fp=0x140001197d0 sp=0x140001197d0 pc=0x100f54c64\ncreated by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1\n\t/Volumes/DATA/dev/ollama/ollama.git/runner/ollamarunner/runner.go:960 +0x898\ntime=2025-06-05T21:10:06.799-04:00 level=ERROR source=server.go:457 msg=\"llama runner terminated\" error=\"exit status 2\"\ntime=2025-06-05T21:10:07.007-04:00 level=ERROR source=sched.go:489 msg=\"error loading llama server\" error=\"llama runner process has terminated: error:fault\"\n[GIN] 2025/06/05 - 21:10:07 | 500 |   335.97975ms |       127.0.0.1 | POST     \"/api/generate\"\n```\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### OS\n\nmacOS\n\n### GPU\n\nApple\n\n### CPU\n\nApple\n\n### Ollama version\n\n_No response_",
    "comments": [
      {
        "user": "mario-grgic",
        "body": "Turns out I am building with go 1.24.2. Once I upgraded golang to 1.24.4, there is no crash. "
      },
      {
        "user": "jmorganca",
        "body": "@mario-grgic Thanks for the issue. Some changes don't break the go build cache properly and so a `go clean -cache` is required. Upgrading Go must have fixed this."
      }
    ]
  }
]