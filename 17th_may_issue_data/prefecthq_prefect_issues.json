[
  {
    "issue_number": 18326,
    "title": "[2.x] flow always restarts from beginning when unpaused",
    "author": "zzstoatzz",
    "state": "open",
    "created_at": "2025-06-17T13:51:19Z",
    "updated_at": "2025-06-17T14:11:23Z",
    "labels": [
      "bug",
      "cloud",
      "2.x"
    ],
    "body": "### Bug summary\n\n> One of our weekly flow runs pauses in the middle of the run to allow for manual inspection of a file before it gets sent, and last week it would pause and restart with subsequent tasks fine. This week though it seems that the entire flow run is being started from the beginning when the flow run is restarted; tasks prior to the pause are cached and marked as completed immediately, before hitting the pause call again, meaning that we can never get the flow to progress past the pause.\n\n```python\n\"\"\"\nReproduction for pause_flow_run issue where flow restarts from beginning\nafter resuming from pause, causing an infinite loop.\n\n2.x\n\"\"\"\n\nimport time\n\nfrom prefect import flow, pause_flow_run, task\n\n\n@task\ndef task_1():\n    print(\"Task 1: Starting initial work\")\n    time.sleep(1)\n    print(\"Task 1: Completed\")\n    return \"task_1_result\"\n\n\n@task\ndef task_2():\n    print(\"Task 2: Preparing data for manual inspection\")\n    time.sleep(1)\n    print(\"Task 2: Data ready for inspection\")\n    return \"task_2_result\"\n\n\n@task\ndef task_3():\n    print(\"Task 3: Processing after manual inspection\")\n    time.sleep(1)\n    print(\"Task 3: Final processing complete\")\n    return \"task_3_result\"\n\n\n@flow(name=\"pause-flow-repro\", persist_result=True)\ndef flow_run():\n    print(\"\\n=== FLOW RUN STARTED ===\")\n\n    result_1 = task_1()\n    print(f\"Task 1 returned: {result_1}\")\n\n    result_2 = task_2()\n    print(f\"Task 2 returned: {result_2}\")\n\n    print(\"\\n‚è∏Ô∏è  PAUSING FLOW for manual inspection...\")\n    print(\"Flow will pause for 30 seconds (or until manually resumed)\")\n\n    pause_flow_run(timeout=30, reschedule=True)\n\n    print(\"\\n‚ñ∂Ô∏è  FLOW RESUMED - continuing with remaining tasks\")\n\n    result_3 = task_3()\n    print(f\"Task 3 returned: {result_3}\")\n\n    print(\"\\n=== FLOW RUN COMPLETED ===\")\n    return {\"task_1\": result_1, \"task_2\": result_2, \"task_3\": result_3}\n\n\nif __name__ == \"__main__\":\n    print(\"Testing pause_flow_run behavior\")\n    print(\"This should pause once after task_2, then continue to task_3\")\n    print(\"If the bug is present, it will restart from the beginning after resume\\n\")\n\n    flow_run.serve()\n```\n\n### Version info\n\n```Text\n2.x\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18323,
    "title": "Prefect flow stay suck in PENDING status and is never picked up by workers",
    "author": "arihantaraza",
    "state": "open",
    "created_at": "2025-06-17T06:50:56Z",
    "updated_at": "2025-06-17T14:11:13Z",
    "labels": [
      "bug",
      "cloud"
    ],
    "body": "### Bug summary\n\nThe prefect flow stays stuck in PENDING status and is never transitioned to RUNNING or even LATE.\nWe are using ecs:push workpools.\nThe ECS task is not launch (it is not failing, it was never launch in AWS)\n\nThere is no logs in cloud UI\n\nprefect.flow-run.Scheduled 12:31:41 PM\nprefect.flow-run.Pending 09:30:04 PM\n\n\n### Version info\n\n```Text\nVersion:             3.1.15\nAPI version:         0.8.4\nPython version:      3.10.6\nGit commit:          3ac3d548\nBuilt:               Thu, Jan 30, 2025 11:31 AM\nOS/Arch:             linux/x86_64\nProfile:             production\nServer type:         cloud\nPydantic version:    2.10.6\nIntegrations:\n  prefect-aws:       0.5.7\n```\n\n### Additional context\n\nThe deployment  is scheduled in prefect cloud each 3 hours.\nThe previous and next flows were running without issue. ",
    "comments": []
  },
  {
    "issue_number": 17913,
    "title": "Manually retrying flows gets stuck in `AwaitingRetry` state",
    "author": "desertaxle",
    "state": "open",
    "created_at": "2025-04-25T16:16:33Z",
    "updated_at": "2025-06-17T14:09:43Z",
    "labels": [],
    "body": "\n### Discussed in https://github.com/PrefectHQ/prefect/discussions/17912\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **MartinEmilJakobsen** April 25, 2025</sup>\nI am experiencing some weird behavior of manual retry attempts getting stuck in AwaitingRetry state.\nThis happens consistently in the following example but it only occurs when retry is initiated from the UI of a failed flow that has been deployed with retries > 0.\nMinimal reproducible example:\n```\nfrom prefect import flow\nfrom prefect.docker import DockerImage\n@flow(retries=3, retry_delay_seconds=5, cache_result_in_memory=False, persist_result=False)\ndef flow_of_ray_tasks():\n    raise ValueError(\"This is an error\")\n\nif __name__ == \"__main__\":\n    flow_of_ray_tasks.deploy(\n        name=\"test_flow_retry\",\n        image=DockerImage(\n            name=\"<<my_docker_registry>>/docker/test\",\n            tag=\"latest\",\n        ),\n        push=True,\n        work_pool_name=\"docker-pool\",\n        cron=\"5 * * * *\",\n        tags=[\"docker-pool\"],\n    )\n```\nMy prefect version:\n```\nVersion:             3.3.5\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          db4b7a33\nBuilt:               Thu, Apr 17, 2025 09:25 PM\nOS/Arch:             linux/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.10.3\nIntegrations:\n  prefect-aws:       0.5.10\n  prefect-ray:       0.4.4\n  prefect-docker:    0.6.3\n```\nFlow-run inspect when stuck in AwaitingRetry:\n```\nFlowRun(\n    id='64d8462a-88e1-4b3c-9b83-32793726ef09',\n    created=DateTime(2025, 4, 25, 15, 42, 23, 268262, tzinfo=Timezone('UTC')),\n    updated=DateTime(2025, 4, 25, 15, 42, 58, 610000, tzinfo=Timezone('UTC')),\n    name='thundering-guppy',\n    flow_id='de7b6093-e012-417d-bb7a-8c1dfff96a33',\n    state_id=UUID('8b13221b-91d1-407e-a848-33f3839a7571'),\n    deployment_id=UUID('8ceab248-8cd8-45cb-8fc5-9c3293d1d0e2'),\n    deployment_version='a9d49177bb38517847ef51a9f02624e4',\n    work_queue_name='default',\n    flow_version='a9d49177bb38517847ef51a9f02624e4',\n    parameters={},\n    context={},\n    empirical_policy=FlowRunPolicy(retries=3, retry_delay=5, pause_keys=set(), retry_type='in_process'),\n    tags=['docker-pool'],\n    labels={\n        'prefect.flow.id': 'de7b6093-e012-417d-bb7a-8c1dfff96a33',\n        'prefect.deployment.id': '8ceab248-8cd8-45cb-8fc5-9c3293d1d0e2',\n        'prefect.worker.name': 'DockerWorker d83d79e8-3ba9-48bb-9628-922d50bbbf82',\n        'prefect.worker.type': 'docker',\n        'prefect.work-pool.name': 'docker-pool',\n        'prefect.work-pool.id': 'c1a5ba4a-96ac-4eca-abc3-8e9015cb0d3b'\n    },\n    run_count=4,\n    expected_start_time=DateTime(2025, 4, 25, 15, 42, 23, 234210, tzinfo=Timezone('UTC')),\n    next_scheduled_start_time=DateTime(2025, 4, 25, 15, 42, 58, 600442, tzinfo=Timezone('UTC')),\n    start_time=DateTime(2025, 4, 25, 15, 42, 31, 607274, tzinfo=Timezone('UTC')),\n    total_run_time=datetime.timedelta(microseconds=182368),\n    estimated_run_time=datetime.timedelta(microseconds=182368),\n    estimated_start_time_delta=datetime.timedelta(seconds=8, microseconds=373064),\n    infrastructure_pid='http+docker://localhost:9eaa9a7d72c791c79a616fbcbb88c4442f008e27e021c5046b062591149641d4',\n    work_queue_id=UUID('8a9a1012-f5e8-4aa4-8051-549f8e4d0cc7'),\n    work_pool_id=UUID('c1a5ba4a-96ac-4eca-abc3-8e9015cb0d3b'),\n    work_pool_name='docker-pool',\n    state=State(\n        id='8b13221b-91d1-407e-a848-33f3839a7571',\n        type=StateType.SCHEDULED,\n        name='AwaitingRetry',\n        timestamp=datetime.datetime(2025, 4, 25, 15, 42, 58, 600771, tzinfo=TzInfo(UTC)),\n        message='Retry from the UI',\n        state_details=StateDetails(flow_run_id=UUID('64d8462a-88e1-4b3c-9b83-32793726ef09'), scheduled_time=DateTime(2025, 4, 25, 15, 42, 58, 600442, tzinfo=Timezone('UTC')), deferred=False)\n    ),\n    job_variables={},\n    state_type=StateType.SCHEDULED,\n    state_name='AwaitingRetry'\n)\n```",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Copying my response from the dicussion:\n\n> The root cause appears to be that the retry_type is set to in_process when the flow retires, but the retry_type isn't updated when the retry is initiated from the UI. Workers ignore scheduled runs with the retry_type set to in_process, so that's why the run gets stuck in AwaitingRetry."
      },
      {
        "user": "Vict0r7",
        "body": "Hello, we‚Äôve encountered this issue. Is there a workaround we can use while we wait for a fix?"
      },
      {
        "user": "Vict0r7",
        "body": "Have there been any developments regarding that? üò¢"
      }
    ]
  },
  {
    "issue_number": 18322,
    "title": "Unexpected task retry behavior",
    "author": "jim-ngoo",
    "state": "closed",
    "created_at": "2025-06-17T06:12:50Z",
    "updated_at": "2025-06-17T14:08:40Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI had this code snippet:\n```python\nfrom prefect import flow, task, State\nfrom prefect.states import Completed, Failed\n\n@flow\ndef my_flow() -> State:\n    future = my_task.submit()\n    return Completed(message=\"my_flow completed\")\n\n@task(retries=3, retry_delay_seconds=10)\ndef my_task() -> None:\n    raise Exception(\"my_task exception\")\n\nif __name__ == \"__main__\":\n    my_flow()\n```\n\nI expect prefect could retry my flow in case of exception, but it turns into crash state and failed to retry then.\n\nTerminal log:\n```console\n% python run_prefect_job.py                  \n14:01:41.890 | INFO    | prefect - Starting temporary server on http://127.0.0.1:8986\nSee https://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server for more information on running a dedicated Prefect server.\n14:01:43.973 | INFO    | Flow run 'diligent-orangutan' - Beginning flow run 'diligent-orangutan' for flow 'my-flow'\n14:01:44.019 | INFO    | Task run 'my_task-9d7' - Task run failed with exception: Exception('my_task exception') - Retry 1/3 will start 10 second(s) from now\n14:01:53.996 | ERROR   | Task run 'my_task-9d7' - Crash detected! Execution was cancelled by the runtime environment.\n14:01:54.000 | ERROR   | Task run 'my_task-9d7' - Finished in state Crashed('Execution was cancelled by the runtime environment.')\n14:01:54.006 | INFO    | Flow run 'diligent-orangutan' - Finished in state Completed('my_flow completed')\n14:01:54.043 | INFO    | prefect - Stopping temporary server on http://127.0.0.1:8986\n```\n\nBut if I do like this, then it works (failure with what I expected)\n```python\nfrom prefect import flow, task, State\nfrom prefect.states import Completed, Failed\n\n@flow\ndef my_flow() -> State:\n    future = my_task.submit()\n    future.wait()\n    if future.state.is_completed():\n        return Completed(message=\"my_flow completed\")\n    else:\n        return Failed(message=\"my_flow failed\")\n\n@task(retries=3, retry_delay_seconds=10)\ndef my_task() -> None:\n    raise Exception(\"my_task exception\")\n\nif __name__ == \"__main__\":\n    my_flow()\n```\n\nTerminal log:\n```console\n% python run_prefect_job_ok.py\n14:08:47.159 | INFO    | prefect - Starting temporary server on http://127.0.0.1:8515\nSee https://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server for more information on running a dedicated Prefect server.\n14:08:49.212 | INFO    | Flow run 'independent-boa' - Beginning flow run 'independent-boa' for flow 'my-flow'\n14:08:49.255 | INFO    | Task run 'my_task-c79' - Task run failed with exception: Exception('my_task exception') - Retry 1/3 will start 10 second(s) from now\n14:08:59.234 | INFO    | Task run 'my_task-c79' - Task run failed with exception: Exception('my_task exception') - Retry 2/3 will start 10 second(s) from now\n14:09:09.242 | INFO    | Task run 'my_task-c79' - Task run failed with exception: Exception('my_task exception') - Retry 3/3 will start 10 second(s) from now\n14:09:19.251 | ERROR   | Task run 'my_task-c79' - Task run failed with exception: Exception('my_task exception') - Retries are exhausted\n```\n\nis it a prefect bug from my first code snippet?\n\n### Version info\n\n```Text\nVersion:             3.4.4\nAPI version:         0.8.4\nPython version:      3.9.6\nGit commit:          0367d7aa\nBuilt:               Thu, May 29, 2025 09:37 PM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         ephemeral\nPydantic version:    2.10.6\nServer:\n  Database:          sqlite\n  SQLite version:    3.43.2\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @jim-ngoo - thanks for the issue.\n\nin your first snippet, you're not [resolving your future](https://docs.prefect.io/v3/concepts/task-runners#submit-tasks-to-a-task-runner), so you're prone to non-deterministic behavior where your future might be garbage collected early. In this case, your task fails and then when you exit the scope of the flow (which will _not_ wait for unresolved futures by default), the task is cancelled and you get crashed.\n\nin your second example, you correctly call `.wait()` on the future before trying to `return`.\n\nso to fix your first example, call `.wait()` on your future\n```python\nfrom prefect import State, flow, task\nfrom prefect.states import Completed\n\n\n@flow\ndef my_flow() -> State:\n    my_task.submit().wait()\n    return Completed(message=\"my_flow completed\")\n\n\n@task(retries=3, retry_delay_seconds=3)\ndef my_task() -> None:\n    raise Exception(\"my_task exception\")\n\n\nif __name__ == \"__main__\":\n    my_flow()\n```\n\n"
      }
    ]
  },
  {
    "issue_number": 18325,
    "title": "PrefectDbtRunner not callable from an Async flow?",
    "author": "steveh-101",
    "state": "open",
    "created_at": "2025-06-17T11:41:57Z",
    "updated_at": "2025-06-17T13:37:04Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI'm not sure what's happening but if I try to run this example code - it fails. \n\n```python\n\nfrom pathlib import Path\n\nfrom prefect import flow, task\nfrom prefect_dbt import PrefectDbtRunner, PrefectDbtSettings\nfrom pydantic import BaseModel, Field\n\n\nclass DbtConfig(BaseModel):\n    \"\"\"Represents a configuration for running dbt commands.\"\"\"\n\n    project_dir: str = Field(\n        ...,\n        description=\"The path to the dbt project directory.\",\n    )\n    commands: list[str] = Field(\n        [\"debug\"],\n        description=\"The dbt commands to run. Defaults to ['debug']. Omit the dbt keyword.\",\n    )\n\n\n# @task\ndef invoke_dbt(dbt_config: DbtConfig) -> None:\n    \"\"\"Invoke dbt commands for a given dbt project. Optionally git clone the project running dbt commands.\"\"\"\n\n    settings = PrefectDbtSettings(\n        project_dir=Path(dbt_config.project_dir),\n        profiles_dir=Path(dbt_config.project_dir) / \".prefect\",\n    )\n\n    args = dbt_config.commands\n    PrefectDbtRunner(settings=settings).invoke(args=args)\n\n\n@flow(log_prints=True)\nasync def main() -> None:\n    \"\"\"Main function to run the test flow.\"\"\"\n\n    invoke_dbt(\n        DbtConfig(\n            project_dir=\"path_to_my_dbt_project\",\n            commands=[\"debug\"],\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    # Run the main function in an event loop\n    asyncio.run(main())\n```\n\n```\nTraceback (most recent call last):\n  File \"/flows/test.py\", line 50, in <module>\n    asyncio.run(main())\n    ~~~~~~~~~~~^^^^^^^^\n  File \"/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ~~~~~~~~~~^^^^^^\n  File \"/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/asyncio/base_events.py\", line 725, in run_until_complete\n    return future.result()\n           ~~~~~~~~~~~~~^^\n  File \"/.venv/lib/python3.13/site-packages/prefect/flow_engine.py\", line 1396, in run_flow_async\n    return engine.state if return_type == \"state\" else await engine.result()\n                                                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/prefect/flow_engine.py\", line 915, in result\n    raise self._raised\n  File \"/.venv/lib/python3.13/site-packages/prefect/flow_engine.py\", line 1332, in run_context\n    yield self\n  File \"/.venv/lib/python3.13/site-packages/prefect/flow_engine.py\", line 1394, in run_flow_async\n    await engine.call_flow_fn()\n  File \"/.venv/lib/python3.13/site-packages/prefect/flow_engine.py\", line 1346, in call_flow_fn\n    result = await call_with_parameters(self.flow.fn, self.parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/flows/test.py\", line 38, in main\n    invoke_dbt(\n    ~~~~~~~~~~^\n        DbtConfig(\n        ^^^^^^^^^^\n    ...<2 lines>...\n        )\n        ^\n    )\n    ^\n  File \"/flows/test.py\", line 31, in invoke_dbt\n    PrefectDbtRunner(settings=settings).invoke(args=args)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/prefect_dbt/core/runner.py\", line 403, in invoke\n    related_prefect_context = run_coro_as_sync(\n        related_resources_from_run_context(self.client),\n    )\n  File \"/.venv/lib/python3.13/site-packages/prefect/utilities/asyncutils.py\", line 207, in run_coro_as_sync\n    return call.result()\n           ~~~~~~~~~~~^^\n  File \"/.venv/lib/python3.13/site-packages/prefect/_internal/concurrency/calls.py\", line 365, in result\n    return self.future.result(timeout=timeout)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/prefect/_internal/concurrency/calls.py\", line 192, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/.venv/lib/python3.13/site-packages/prefect/_internal/concurrency/calls.py\", line 441, in _run_async\n    result = await coro\n             ^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/prefect/utilities/asyncutils.py\", line 188, in coroutine_wrapper\n    return await task\n           ^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/prefect/events/related.py\", line 119, in related_resources_from_run_context\n    await asyncio.gather(\n    ...<40 lines>...\n    )\n  File \"/.venv/lib/python3.13/site-packages/prefect/events/related.py\", line 206, in _get_and_cache_related_object\n    obj_ = await client_method(obj_id)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/prefect/client/orchestration/_flows/client.py\", line 242, in read_flow\n    response = await self.request(\"GET\", \"/flows/{id}\", path_params={\"id\": flow_id})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/prefect/client/orchestration/base.py\", line 53, in request\n    return await self._client.send(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/prefect/client/base.py\", line 330, in send\n    response = await self._send_with_retry(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<24 lines>...\n    )\n    ^\n  File \"/.venv/lib/python3.13/site-packages/prefect/client/base.py\", line 250, in _send_with_retry\n    response = await send(request, *send_args, **send_kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        pool_request.request\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/.venv/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py\", line 136, in handle_async_request\n    raise exc\n  File \"/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py\", line 106, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py\", line 177, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py\", line 217, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.READ_NUM_BYTES, timeout=timeout\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/.venv/lib/python3.13/site-packages/httpcore/_backends/anyio.py\", line 35, in read\n    return await self._stream.receive(max_bytes=max_bytes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/anyio/streams/tls.py\", line 219, in receive\n    data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/anyio/streams/tls.py\", line 162, in _call_sslobject_method\n    data = await self.transport_stream.receive()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 1254, in receive\n    await self._protocol.read_event.wait()\n  File \"/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/asyncio/locks.py\", line 210, in wait\n    fut = self._get_loop().create_future()\n          ~~~~~~~~~~~~~~^^\n  File \"/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/asyncio/mixins.py\", line 20, in _get_loop\n    raise RuntimeError(f'{self!r} is bound to a different event loop')\n```\n\nthe only thing that works is if I make the parent flow a synchronous one. \n\nHowever dummy async flows called within async worked ok\n\n\n### Version info\n\n```Text\nVersion:             3.4.6\nAPI version:         0.8.4\nPython version:      3.13.2\nGit commit:          d10c6e6a\nBuilt:               Wed, Jun 11, 2025 08:01 PM\nOS/Arch:             darwin/arm64\nProfile:             dev\nServer type:         cloud\nPydantic version:    2.11.7\nIntegrations:\n  prefect-dbt:       0.7.0rc1\n  prefect-slack:     0.3.1\n  prefect-aws:       0.5.10\n  prefect-github:    0.3.1\n  prefect-snowflake: 0.28.5\n  prefect-shell:     0.3.1\n```\n\n### Additional context\n\nusing ` \"prefect-dbt[snowflake]>=0.7.0rc1\"`",
    "comments": []
  },
  {
    "issue_number": 18324,
    "title": "Flow runs remain stuck in pending state after server restart in non-Kubernetes environment",
    "author": "lelouvincx",
    "state": "open",
    "created_at": "2025-06-17T08:25:49Z",
    "updated_at": "2025-06-17T08:27:38Z",
    "labels": [],
    "body": "### Bug summary\n\n**Description:**\n\nFlow runs remain permanently stuck in pending state after server restarts in a non-Kubernetes Linux environment. This issue is similar to #17955 but occurs outside of Kubernetes deployments, indicating the problem may be more widespread than initially reported.\n\nWhen flows are in pending state and the Prefect server is restarted, those flows never resume execution and remain stuck indefinitely. With worker concurrency limits set to 1, this effectively blocks the entire queue, preventing any new flows from executing.\n\n**Steps to Reproduce:**\n\n1. Set up Prefect 3.3.4 in a Linux environment (non-Kubernetes)\n2. Configure a worker with concurrency limit of 1\n3. Start a flow run and let it enter pending state\n4. Restart the Prefect server\n5. Observe that the flow run remains stuck in pending state\n6. Attempt to run new flows - they will be blocked due to the stuck flow and concurrency limit\n\n**Expected Behavior:**\n\nAfter server restart, flows in pending state should either:\n- Resume execution automatically\n- Be marked as failed and allow retry\n- Be handled gracefully to not block the queue indefinitely\n\n**Actual Behavior:**\n\n- Flow runs remain permanently stuck in pending state\n- Worker concurrency limit prevents any new flows from starting\n- Manual intervention required to cancel stuck flows and restore queue functionality\n\n### Version info\n\n```\nVersion:             3.3.4\nAPI version:         0.8.4\nPython version:      3.11.10\nGit commit:          7ca03553\nBuilt:               Fri, Apr 11, 2025 12:27 AM\nOS/Arch:             linux/amd64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.11.3\nIntegrations:\n  prefect-gcp:       0.6.5\n  prefect-dbt:       0.7.0rc1\n  prefect-shell:     0.3.1\n```\n\n### Environment Details\n\n- **Deployment**: Linux server (non-Kubernetes)\n- **Work Queue Configuration**: Concurrency limit of 1\n- **Database**: PostgreSQL\n\n### Additional context\n\nThis issue appears to be related to #17955, which reported similar behavior in Kubernetes environments. The fact that this also occurs in non-Kubernetes Linux deployments suggests the root cause may be in Prefect's core state management logic rather than being specific to Kubernetes infrastructure.\n\nThe problem significantly impacts production workflows as:\n1. Stuck flows prevent new executions due to concurrency limits\n2. No automatic recovery mechanism exists\n3. Manual intervention is required to identify and cancel stuck flows\n4. This can lead to extended downtime of data pipelines\n\n### Workaround\n\nCurrently using manual monitoring to identify stuck flows and cancel them via Prefect CLI/API, but this is not sustainable for production environments.\n\n### Related Issues\n\n- #17955 - Similar issue in Kubernetes environments",
    "comments": []
  },
  {
    "issue_number": 17955,
    "title": "Flow runs get stuck after server reboot in Kubernetes setup with Prefect 3",
    "author": "maitlandmarshall",
    "state": "closed",
    "created_at": "2025-04-30T22:10:16Z",
    "updated_at": "2025-06-17T08:13:46Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n**Description:**  \nI am experiencing an issue in Prefect 3 with a setup consisting of a Kubernetes worker, Kubernetes self-hosted API, and Postgres database. When a flow run is in a running or pending state and the servers are rebooted (e.g., for weekly updates), the flow run remains stuck in that state permanently. This issue is particularly problematic when combined with concurrency limits set to cancel mode, as it causes all subsequent flows to be cancelled indefinitely.\n\n**Steps to Reproduce:**  \n1. Set up a Prefect 3 environment with a Kubernetes worker, Kubernetes self-hosted API, and Postgres database.  \n2. Create a flow with concurrency limit 1 and set to cancel mode.  \n3. Start a flow run.  \n4. Reboot the servers while the flow run is in a running or pending state.  \n5. Observe that the flow run remains stuck in its current state, and subsequent flows are cancelled indefinitely due to the concurrency limits.\n\n**Expected Behavior:**  \nAfter the servers are rebooted, the flow run should handle the disruption gracefully. It should either resume, be retried, or be marked as failed, but not remain stuck indefinitely.\n\n**Actual Behavior:**  \nThe flow run remains stuck in the running or pending state. Additionally, when concurrency limits are set to cancel mode, all subsequent flows are cancelled indefinitely.\n\n### Version info\n\n```Text\n- Prefect Version: 3.2.7\n- Python Version: 3.11 \n- OS/Arch: azure linux\n- Kubernetes Version: v1.30.11 \n- Postgres Version: 12.11.1\n```\n\n```yaml\nchart: prefect/prefect-worker\nversion: 2025.2.21193831\n```\n\n```bash\npip show prefect-kubernetes\nName: prefect-kubernetes\nVersion: 0.5.3\nSummary: Prefect integrations for interacting with Kubernetes.\nHome-page:\nAuthor:\nAuthor-email: \"Prefect Technologies, Inc.\" <help@prefect.io>\nLicense: Apache License 2.0\nLocation: /usr/local/lib/python3.11/site-packages\nRequires: exceptiongroup, kubernetes-asyncio, prefect, pyopenssl, tenacity\nRequired-by:\n```\n\n### Additional context\n\n- This issue might be related to past issues in Prefect 2, such as [PrefectHQ/prefect#8409](https://github.com/PrefectHQ/prefect/issues/8409), which reported similar problems with Kubernetes flow runs remaining stuck after pod restarts. However, I could not find a specific fix for this issue in Prefect 3.  ",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @maitlandmarshall - thanks for the issue! can you share your `prefect-kubernetes` version used by your worker?\n\nI believe this may be fixed in the [alpha version ](https://pypi.org/project/prefect-kubernetes/0.6.0a2/) of `prefect-kubernetes` "
      },
      {
        "user": "maitlandmarshall",
        "body": "> hi [@maitlandmarshall](https://github.com/maitlandmarshall) - thanks for the issue! can you share your `prefect-kubernetes` version used by your worker?\n\nof course!\n\n```yaml\nchart: prefect/prefect-worker\nversion: 2025.2.21193831\n```\n\n```bash\npip show prefect-kubernetes\nName: prefect-kubernetes\nVersion: 0.5.3\nSummary: Prefect integrations for interacting with Kubernetes.\nHome-page:\nAuthor:\nAuthor-email: \"Prefect Technologies, Inc.\" <help@prefect.io>\nLicense: Apache License 2.0\nLocation: /usr/local/lib/python3.11/site-packages\nRequires: exceptiongroup, kubernetes-asyncio, prefect, pyopenssl, tenacity\nRequired-by:\n```"
      },
      {
        "user": "desertaxle",
        "body": "Hey @maitlandmarshall, what does your Kubernetes worker do when the server is restarted? Does it crash? Because if the worker crashes while the flow run is in `PENDING` then that could be the root cause of the issue."
      }
    ]
  },
  {
    "issue_number": 18294,
    "title": "DockerImage build_args invalid argument",
    "author": "TheUkrainian1991",
    "state": "closed",
    "created_at": "2025-06-13T11:17:55Z",
    "updated_at": "2025-06-17T02:38:44Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nbuildargs is unexpected argument for BuildApiMixin\nThis is caused when trying to pass arguments for the dockerfile using the build_args/buildkwargs\n\n```python\ndbt_image = DockerImage(\n        name=\"dbt-repo-image\",\n        dockerfile=\"Dockerfile\",\n        tag=\"latest\", # do not change this\n        build_args={\n            \"DBT_REPO\" : f\"https://github.com/org/myrepo.git\"\n        },\n    )\n```\n\nRemoving the underscore in the buildargs fixes this, because BuildApiMixin.build takes buildargs not build_args\n\n```python\ndbt_image = DockerImage(\n        name=\"dbt-repo-image\",\n        dockerfile=\"Dockerfile\",\n        tag=\"latest\", # do not change this\n        buildargs={\n            \"DBT_REPO\" : f\"https://github.com/org/myrepo.git\"\n        },\n    )\n```\n\n### Version info\n\n```Text\nVersion:             3.4.6\nAPI version:         0.8.4\nPython version:      3.12.3\nOS/Arch:             macos/arm\nIntegrations:\n  prefect-docker:    0.6.6\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @TheUkrainian1991 -[ we're just passing through **kwargs to the underlying docker client implementation](https://github.com/prefecthq/prefect/blob/performance-improvements-json-typeadapter/src/prefect/docker/docker_image.py#L40), the prefect SDK does not need to know all the kwargs that the underlying SDK can accept\n\nso for detail on what's accepted I recommend checking the [upstream docs](https://docker-py.readthedocs.io/en/stable/)"
      }
    ]
  },
  {
    "issue_number": 18320,
    "title": "‚ÄúCannot connect to Server API at http://localhost:4200/api‚Äù in remote Docker deployment ‚Äî PREFECT_UI_API_URL ignored",
    "author": "zs856",
    "state": "closed",
    "created_at": "2025-06-17T00:48:23Z",
    "updated_at": "2025-06-17T02:37:24Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen deploying Prefect¬†via Docker Compose on a remote Ubuntu¬†22.04 server, the Web UI fails to connect to the API‚Äîalways pointing to localhost, even though PREFECT_UI_API_URL is set:\n\n```Can't connect to Server API at http://localhost:4200/api. Check that it's accessible from your machine.```\n\n\n### Version info\n\n```Text\nprefect version\nError processing line 1 of /opt/conda/envs/prefect/lib/python3.12/site-packages/distutils-precedence.pth:\n\n  Traceback (most recent call last):\n    File \"<frozen site>\", line 206, in addpackage\n    File \"<string>\", line 1, in <module>\n  ModuleNotFoundError: No module named '_distutils_hack'\n\nRemainder of file ignored\n\n\nVersion:             3.4.6\nAPI version:         0.8.4\nPython version:      3.12.11\nGit commit:          d10c6e6a\nBuilt:               Wed, Jun 11, 2025 08:03 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.11.5\nServer:\n  Database:          postgresql\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\nDocker compose file:\n```\nversion: '3'\nservices:\n  postgres:\n    image: docker.1ms.run/library/postgres:16\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: passw0rd123\n      POSTGRES_DB: prefect\n    volumes:\n      - prefectdb:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n\n  prefect:\n    image: docker.1ms.run/prefecthq/prefect:3-python3.12-conda\n    command: prefect server start --host 0.0.0.0\n    ports:\n      - \"4200:4200\"\n    environment:\n      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://postgres:passw0rd123@postgres:5432/prefect\n      - PREFECT_SERVER_API_HOST=0.0.0.0                # ‚ù§Ô∏è listen on all interfaces\n      - PREFECT_UI_API_URL=http://localhost:4200/api   # üß≠ tell UI where API lives\n    depends_on:\n      - postgres\n\nvolumes:\n  prefectdb:\n\n```",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @zs856 \n\n>   `PREFECT_UI_API_URL=http://localhost:4200/api   # üß≠ tell UI where API lives`\n\nthe value you're using here seems like the issue, should this not be something like this?\n```\n- PREFECT_UI_API_URL=http://<YOUR_UBUNTU_SERVER_IP>:4200/api   # Points to your server\n```"
      },
      {
        "user": "zs856",
        "body": "> hi [@zs856](https://github.com/zs856)\n> \n> > `PREFECT_UI_API_URL=http://localhost:4200/api   # üß≠ tell UI where API lives`\n> \n> the value you're using here seems like the issue, should this not be something like this?\n> \n> ```\n> - PREFECT_UI_API_URL=http://<YOUR_UBUNTU_SERVER_IP>:4200/api   # Points to your server\n> ```\n\nThanks for your reply! I tried replacing it with the server's IP, and it worked. But if the network changes and the IP changes, I may have to reconfigure it here. Is there any good way to solve the problem once and for all?"
      },
      {
        "user": "zzstoatzz",
        "body": "generally i'd say pointing at your server is the standard move here (if you need to set it explicitly), I'd consider getting yourself a static IP via your hosting provider and a custom domain etc if necessary\n\nbut you may not have to set that explicitly, see [examples](https://github.com/zzstoatzz/prefect-pack/tree/main/examples/run_a_prefect_server/docker_compose)"
      }
    ]
  },
  {
    "issue_number": 18319,
    "title": "Use the Websocket custom SSL and proxy support for the prefect.client.Subscription websockets",
    "author": "chrisguidry",
    "state": "open",
    "created_at": "2025-06-16T20:11:22Z",
    "updated_at": "2025-06-16T20:11:22Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nWe currently support custom server SSL certificate and proxy servers for our events (and soon-to-be logs) websockets, but not for the socket that drives our task run workers.\n\n### Describe the proposed behavior\n\nUse the new common websockets utilities introduced in  https://github.com/PrefectHQ/prefect/pull/18318 for uniform support\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18317,
    "title": "PydanticUserError when initializing client schema data models for flow run creation",
    "author": "jlwhelan28",
    "state": "closed",
    "created_at": "2025-06-16T18:48:44Z",
    "updated_at": "2025-06-16T19:47:10Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nLooking to use the client schema data models but running into a Pydantic error when initializing the following data models. Unsure if this is intentional and these models normally can't be used outside of the server context or something. \n- `FlowRunCreate`\n- `TaskRunCreate`\n- `DeploymentFlowRunCreate`\n\n```python\nfrom prefect.client.schemas.actions import FlowRunCreate\nfrom prefect.client.schemas.actions import FlowCreate\n\nFlowCreate(\"hello-world\")\n>>> FlowCreate(name='hello', tags=[], labels={})\n\nFlowRunCreate()\n>>> PydanticUserError: `FlowRunCreate` is not fully defined; you should define `ResultRecordMetadata`, then call `FlowRunCreate.model_rebuild()`.\n```\n\nThis is preventing me from running a flow using the prefect client\n```python\nasync with get_client() as client:\n    await client.create_flow_run_from_deployment(\"my-deployment-uid\", **{\"parameters\": {\"hello\": \"world\"}})\n```\n\n### Version info\n\n```Text\nName: pydantic\nVersion: 2.11.7\n\n\nName: prefect\nVersion: 3.4.6\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @jlwhelan28 - this feels related to https://github.com/PrefectHQ/prefect/issues/15957\n\ncan you try `import prefect.main # noqa` before you import the actions schemas?"
      },
      {
        "user": "jlwhelan28",
        "body": "> hi [@jlwhelan28](https://github.com/jlwhelan28) - this feels related to [#15957](https://github.com/PrefectHQ/prefect/issues/15957)\n> \n> can you try `import prefect.main # noqa` before you import the actions schemas?\n\nYep, that's it. \n\n```python\nimport prefect.main\nfrom prefect.client.schemas.actions import DeploymentFlowRunCreate\nDeploymentFlowRunCreate()\n>>> DeploymentFlowRunCreate(state=None, name=None, parameters={}, enforce_parameter_schema=None, context={}, infrastructure_document_id=None, empirical_policy=FlowRunPolicy(max_retries=0, retry_delay_seconds=0, retries=None, retry_delay=None, pause_keys=set(), resuming=False, retry_type=None), tags=[], idempotency_key=None, parent_task_run_id=None, work_queue_name=None, job_variables=None, labels={})\n```\n\nI see now you're tracking this here: https://github.com/PrefectHQ/prefect/discussions/16387\n\nApologies for the extraneous report and thanks once again for the quick answer"
      },
      {
        "user": "zzstoatzz",
        "body": "no worries! the situation today is not ideal so it makes sense that you opened it\n\nbut yep, lets track this over at https://github.com/PrefectHQ/prefect/discussions/16387\n\nthanks!"
      }
    ]
  },
  {
    "issue_number": 18298,
    "title": "Cancel individual task run inside flow run",
    "author": "zkurtz",
    "state": "closed",
    "created_at": "2025-06-13T17:22:23Z",
    "updated_at": "2025-06-16T15:55:00Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nSummary: I wonder if a \"cancel run\" option could be added for an individual task inside a run.\n\nMotivation: One of the tasks in my flow sometimes hangs. The rest of the flow run is fine, so I'd like it to run to completion, but since the one task is hanging, I end up having to cancel the entire flow run, resulting in failure to complete some wrap-up steps at the end of my flow.\n\nAttempted solution: I tried running something like\n```\nfrom prefect import get_client\nfrom prefect.states import Failed\nimport asyncio\n\nasync def fail_hanging_task(task_run_id: str):\n    async with get_client() as client:\n        await client.set_task_run_state(\n            task_run_id=task_run_id,\n            state=Failed(message=\"Manually failed due to hanging\")\n        )\n\nasyncio.run(fail_hanging_task(\"your-task-run-id\"))\n```\nThis effectively cancels a task run, but\n- fails to prevent retries\n- even without retries, the flow run doesn't seem to get the message and just keeps running even after all other tasks have completed\n\n### Describe the proposed behavior\n\nIdeally, here's how I'd like a \"cancel task run\" functionality to behave:\n- The cancelled run is marked as \"failed\" or \"cancelled\"\n- By default, the cancellation overrides and prevents any remaining retries\n- Upon cancellation, control immediately is passed back to the flow run to terminate as usual.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @zkurtz - have you tried something like this? that way if `hanging_task` hangs, you can selectively retry and then continue on in the parent flow\n```python\nimport time\nfrom prefect import flow, get_run_logger, task\n\ndef no_retry_on_timeout(task, task_run, state):\n    \"\"\"Prevent retries when task times out\"\"\"\n    return not isinstance(state.data, TimeoutError)\n\n@task(retries=3, timeout_seconds=5, retry_condition_fn=no_retry_on_timeout)\ndef hanging_task():\n    \"\"\"This task will \"hang\", timeout after 5s, and NOT retry\"\"\"\n    get_run_logger().info(\"Starting task that hangs...\")\n    time.sleep(60)\n    return \"Never reached\"\n\n@flow\ndef demo():\n    try:\n        hanging_task()\n    except Exception as e:\n        get_run_logger().info(f\"Task cancelled: {type(e).__name__}\")\n\nif __name__ == \"__main__\":\n    demo()\n```"
      },
      {
        "user": "zkurtz",
        "body": "Thanks @zzstoatzz, this looks useful; both `timeout_seconds` and `retry_condition_fn` are arguments I had not looked at before."
      },
      {
        "user": "zzstoatzz",
        "body": "cool! I'll convert this to a discussion for now since I'm not sure there's an enhancement request expressed here then"
      }
    ]
  },
  {
    "issue_number": 16771,
    "title": "Add support for tags to Automation",
    "author": "dgarros",
    "state": "closed",
    "created_at": "2025-01-19T16:20:42Z",
    "updated_at": "2025-06-16T15:28:42Z",
    "labels": [],
    "body": "### Describe the current behavior\n\nCurrently it's not possible to add tags to an Automation object\n\n### Describe the proposed behavior\n\nAs the number of Automation grow on a project it would be very useful to be able to attach tags to Automation, similar to what is possible on flow and flow_run today.\n\n\n\n### Example Use\n\nOur use case is that we have different types of Automation, related to different features, and we would like to manage them programmatically independently of each other. Today we have to maintain a complex naming convention in order to capture additional information about each Automation.\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18306,
    "title": "ECS Work Pool match_latest_revision_in_family unreliable when task definition has environment Variables",
    "author": "MarMar-1",
    "state": "closed",
    "created_at": "2025-06-14T19:52:22Z",
    "updated_at": "2025-06-16T15:21:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen using an ECS Work Pool with a task definition which contains multiple environment variables, the match_latest_revision_in_family functionality will often not work.\n\n### Impact\n\nAs discussed at length in previous issues, match_latest_revision_in_family is necessary to prevent ECS Work Pools from hitting AWS rate limits.\nSee:\n- #15865 \n- #4402\n- #10102\n\n### Root Cause\nWhen ECS stores task definition environment variables it stores them in an unordered data structure. Therefore, when you retrieve that task definition from the API the environment variables are likely to be in a different order than when you defined them. AWS provides no guarantees about this order.\n\nThis means that when comparing task definitions for equality, the order of the environment variables list should not be considered. However, when the [ECS worker compares task definitions](https://github.com/PrefectHQ/prefect/blob/5486631396a72284b1035fd98e50dc6c837d2cd0/src/integrations/prefect-aws/prefect_aws/workers/ecs_worker.py#L1760 ) it uses `==` without first ensuring the environment variables lists are in a consistent order.\n\n### Version info\n\n```Text\nVersion:             3.4.2\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          c3c1c119\nBuilt:               Mon, May 19, 2025 04:06 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         cloud\nPydantic version:    2.11.4\nIntegrations:\n  prefect-aws:       0.5.10\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\n### Suggested Fix\nDuring the pre-processing in [_task_definitions_equal](https://github.com/PrefectHQ/prefect/blob/5486631396a72284b1035fd98e50dc6c837d2cd0/src/integrations/prefect-aws/prefect_aws/workers/ecs_worker.py#L1699), the environment variables lists in the container definitions should be sorted in a consistent order (e.g. alphabetically by name).\nThe same should be done for any other list in the task definition which ECS considers to be unordered, but I'm not sure which fields that applies to. I've confirmed that it applies to `environment`; I suspect it also applies to `secrets` and `environmentFiles`.\n\n### Workaround\nAlthough AWS does not provide any guarantees about the variable ordering, the order does appear to stay consistent. Therefore, you can re-order the environment variables in your task definition to match the order AWS puts them in.\nHowever, this may break at any time and I haven't tested whether the ordering stays consistent across regions/accounts.",
    "comments": []
  },
  {
    "issue_number": 18310,
    "title": "The task missing from the flow.",
    "author": "jcppython",
    "state": "closed",
    "created_at": "2025-06-16T13:44:48Z",
    "updated_at": "2025-06-16T15:01:38Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\ngrep 897e9abd-57a7-40f3-b0ec-8f2b17b210b7 log/prefect/server.log \n\n// „Äê20:42:57„Äëflow run \n\n20:42:57.096 | DEBUG   | prefect.server.events.messaging - Publishing event: prefect.**task-run.Pending** with id: 1702e22e-f0cc-41ab-b8d6-227ec0a73528 for resource: prefect.task-run.897e9abd-57a7-40f3-b0ec-8f2b17b210b7\n20:42:57.098 | DEBUG   | prefect.server.events.messaging - Publishing event: prefect.**task-run.Running** with id: dfbd5af3-27f7-42de-8543-913e0df05d58 for resource: prefect.task-run.897e9abd-57a7-40f3-b0ec-8f2b17b210b7\n20:43:02.496 | DEBUG   | prefect.server.events.messaging - Publishing event: prefect.**task-run.Completed** with id: 839f8884-4192-43cf-a51f-17fe5e94b6f8 for resource: prefect.task-run.897e9abd-57a7-40f3-b0ec-8f2b17b210b7\n\n// „Äê21:25:36„Äërestart worker only !!!\n\n21:25:36.057 | DEBUG   | prefect.server.events.services.event_persister - Received event: prefect.**task-run.Pending** with id: 1702e22e-f0cc-41ab-b8d6-227ec0a73528 for resource: prefect.task-run.897e9abd-57a7-40f3-b0ec-8f2b17b210b7\n21:25:36.058 | DEBUG   | prefect.server.services.task_run_recorder - Received event: prefect.**task-run.Pending with** id: 1702e22e-f0cc-41ab-b8d6-227ec0a73528 for resource: prefect.task-run.897e9abd-57a7-40f3-b0ec-8f2b17b210b7\n21:25:36.072 | DEBUG   | prefect.server.events.services.event_persister - Received event: **prefect.task-run.Running** with id: dfbd5af3-27f7-42de-8543-913e0df05d58 for resource: prefect.task-run.897e9abd-57a7-40f3-b0ec-8f2b17b210b7\n21:25:36.089 | DEBUG   | prefect.server.events.services.event_persister - Received event: **prefect.task-run.Completed** with id: 839f8884-4192-43cf-a51f-17fe5e94b6f8 for resource: prefect.task-run.897e9abd-57a7-40f3-b0ec-8f2b17b210b7\n\n\n\nWhen I executed the flow, **the task was missing from the dashboard**. After a while, I **restarted the worker** and found the task was in a pending state. Additionally, **two sets of task states were found in the server**.\n\n\n### Version info\n\n```Text\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:28 PM\nOS/Arch:             linux/x86_64\nProfile:             s1sexpt\nServer type:         server\nPydantic version:    2.10.3\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @jcppython \n\nthis is not enough specific information to understand what your problem is or how to reproduce it. \n\n> When I executed the flow, the task was missing from the dashboard. After a while, I restarted the worker and found the task was in a pending state.\n\nits not obvious to me that you're demonstrating an issue here from the above description\n\n\n> Additionally, two sets of task states were found in the server.\n\n> 21:25:36.057 | DEBUG | prefect.server.events.services.event_persister - Received event: prefect.task-run.Pending with id: 1702e22e-f0cc-41ab-b8d6-227ec0a73528 for resource: prefect.task-run.897e9abd-57a7-40f3-b0ec-8f2b17b210b7\n21:25:36.058 | DEBUG | prefect.server.services.task_run_recorder - Received event: prefect.task-run.Pending with id: 1702e22e-f0cc-41ab-b8d6-227ec0a73528 for resource: prefect.task-run.897e9abd-57a7-40f3-b0ec-8f2b17b210b7\n\nis this what you're referring to? this is 2 different services processing the same event, not 2 sets of task states"
      },
      {
        "user": "jcppython",
        "body": "<img width=\"1245\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/923bf35d-d887-448b-87f6-e2ee9598a9e3\" />\n\n<img width=\"1238\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/374b9b2a-9bf1-4ae0-bf4c-5d2bceffb81a\" />\n\n\n> this is 2 different services processing the same event, not 2 sets of task states\n\nNoÔºåIt is only one service\n\nSorry, I didn't describe it clearly.\n\nFrom the logs, it can be seen that my task was executed successfully.\n\nPerhaps it would be better to use this diagram. You can see that tasks are missing from the graph (this is a reproduction, so the timestamps don't exactly match). @zzstoatzz \n\n\n"
      },
      {
        "user": "jcppython",
        "body": "export PREFECT_MESSAGING_BROKER='prefect.server.utilities.messaging.memory'\nexport PREFECT_MESSAGING_CACHE='prefect.server.utilities.messaging.memory'\n\nAfter removing the above two items of **prefect_redis.messaging** configuration and switching to the default memory broker, the issue was resolved. \nWas my prefect_redis configuration incomplete or incorrect? Apart from tasks missing from the dashboard, no other impacts were observed.\n"
      }
    ]
  },
  {
    "issue_number": 17767,
    "title": "Prefect server experiencing deadlocks with Postgres database",
    "author": "ashrielbrian",
    "state": "open",
    "created_at": "2025-04-08T09:42:51Z",
    "updated_at": "2025-06-16T13:11:21Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWe deploy Prefect server using helm (`2024.6.24152434`) into EKS, with RDS Postgres 14.13 as the backing database.\n\nLately, we've been experiencing frequent deadlocks after upgrading to Prefect 3.3.1. \n\n```\n09:14:35.587 | ERROR   | prefect.server.utilities.messaging.memory - Failed in consume_loop\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 545, in _prepare_and_execute\n    self._rows = deque(await prepared_stmt.fetch(*parameters))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/asyncpg/prepared_stmt.py\", line 176, in fetch\n    data = await self.__bind_execute(args, 0, timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/asyncpg/prepared_stmt.py\", line 267, in __bind_execute\n    data, status, _ = await self.__do_execute(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/asyncpg/prepared_stmt.py\", line 256, in __do_execute\n    return await executor(protocol)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"asyncpg/protocol/protocol.pyx\", line 206, in bind_execute\nasyncpg.exceptions.DeadlockDetectedError: deadlock detected\nDETAIL:  Process 6676 waits for ShareLock on transaction 47526832; blocked by process 6679.\nProcess 6679 waits for ShareLock on speculative token 1 of transaction 47526833; blocked by process 6676.\nHINT:  See server log for query details.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n    self.dialect.do_execute(\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n    cursor.execute(statement, parameters)\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 580, in execute\n    self._adapt_connection.await_(\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n    value = await result\n            ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 558, in _prepare_and_execute\n    self._handle_exception(error)\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 508, in _handle_exception\n    self._adapt_connection._handle_exception(error)\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 792, in _handle_exception\n    raise translated_error from error\nsqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_dbapi.Error: <class 'asyncpg.exceptions.DeadlockDetectedError'>: deadlock detected\nDETAIL:  Process 6676 waits for ShareLock on transaction 47526832; blocked by process 6679.\nProcess 6679 waits for ShareLock on speculative token 1 of transaction 47526833; blocked by process 6676.\nHINT:  See server log for query details.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/prefect/server/utilities/messaging/memory.py\", line 356, in _consume_loop\n    await handler(message)\n  File \"/usr/local/lib/python3.12/site-packages/prefect/server/services/task_run_recorder.py\", line 210, in message_handler\n    await record_task_run_event(event)\n  File \"/usr/local/lib/python3.12/site-packages/prefect/server/services/task_run_recorder.py\", line 169, in record_task_run_event\n    await _insert_task_run(session, task_run, task_run_attributes)\n  File \"/usr/local/lib/python3.12/site-packages/prefect/server/services/task_run_recorder.py\", line 54, in _insert_task_run\n    await session.execute(\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/ext/asyncio/session.py\", line 463, in execute\n    result = await greenlet_spawn(\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\n    result = context.throw(*sys.exc_info())\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 2365, in execute\n    return self._execute_internal(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 2251, in _execute_internal\n    result: Result[Any] = compile_state_cls.orm_execute_statement(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/orm/bulk_persistence.py\", line 1294, in orm_execute_statement\n    result = conn.execute(\n             ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1416, in execute\n    return meth(\n           ^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py\", line 523, in _execute_on_connection\n    return connection._execute_clauseelement(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1638, in _execute_clauseelement\n    ret = self._execute_context(\n          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1843, in _execute_context\n    return self._exec_single_context(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1983, in _exec_single_context\n    self._handle_dbapi_exception(\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 2352, in _handle_dbapi_exception\n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n    self.dialect.do_execute(\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n    cursor.execute(statement, parameters)\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 580, in execute\n    self._adapt_connection.await_(\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n    value = await result\n            ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 558, in _prepare_and_execute\n    self._handle_exception(error)\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 508, in _handle_exception\n    self._adapt_connection._handle_exception(error)\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 792, in _handle_exception\n    raise translated_error from error\nsqlalchemy.exc.DBAPIError: (sqlalchemy.dialects.postgresql.asyncpg.Error) <class 'asyncpg.exceptions.DeadlockDetectedError'>: deadlock detected\nDETAIL:  Process 6676 waits for ShareLock on transaction 47526832; blocked by process 6679.\nProcess 6679 waits for ShareLock on speculative token 1 of transaction 47526833; blocked by process 6676.\nHINT:  See server log for query details.\n[SQL: INSERT INTO task_run (flow_run_id, task_key, dynamic_key, flow_run_run_count, empirical_policy, task_inputs, tags, name, run_count, expected_start_time, start_time, total_run_time, id, created, updated) VALUES ($1::UUID, $2::VARCHAR, $3::VARCHAR, $4::INTEGER, $5, $6, $7::JSONB, $8::VARCHAR, $9::INTEGER, $10::TIMESTAMP WITH TIME ZONE, $11::TIMESTAMP WITH TIME ZONE, $12::INTERVAL, $13::UUID, $14::TIMESTAMP WITH TIME ZONE, $15::TIMESTAMP WITH TIME ZONE) ON CONFLICT (id) DO UPDATE SET flow_run_id = $16::UUID, task_key = $17::VARCHAR, dynamic_key = $18::VARCHAR, flow_run_run_count = $19::INTEGER, empirical_policy = $20, task_inputs = $21, tags = $22::JSONB, name = $23::VARCHAR, run_count = $24::INTEGER, expected_start_time = $25::TIMESTAMP WITH TIME ZONE, start_time = $26::TIMESTAMP WITH TIME ZONE, total_run_time = $27::INTERVAL, id = $28::UUID, updated = $29::TIMESTAMP WITH TIME ZONE WHERE task_run.state_timestamp < $30::TIMESTAMP WITH TIME ZONE]\n```\n\n\n\n### Version info\n\n```Text\nVersion:             3.3.1\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          6a164620\nBuilt:               Tue, Apr 01, 2025 2:41 AM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.11.1\nServer:\n  Database:          sqlite\n  SQLite version:    3.40.1\nIntegrations:\n  prefect-redis:     0.2.2\n  prefect-kubernetes: 0.5.9\n```\n\n### Additional context\n\nI'm not sure if it's related to this https://github.com/PrefectHQ/prefect/issues/16299. In terms of load, we have around 30-40 pipelines running, but only around ~5-10 at any one time, so not particularly heavy workloads. We use Kubernetes workers for the workloads.\n\nWe also get warnings of the `CancellationCleanup` taking longer than the loop interval:\n\n```\n09:15:04.236 | WARNING | prefect.server.services.cancellationcleanup - CancellationCleanup took 53.716827 seconds to run, which is longer than its loop interval of 20.0 seconds.\n09:16:00.380 | WARNING | prefect.server.services.cancellationcleanup - CancellationCleanup took 56.141906 seconds to run, which is longer than its loop interval of 20.0 seconds.\n09:16:56.335 | WARNING | prefect.server.services.cancellationcleanup - CancellationCleanup took 55.95355 seconds to run, which is longer than its loop interval of 20.0 seconds.\n09:17:49.045 | WARNING | prefect.server.services.cancellationcleanup - CancellationCleanup took 52.707539 seconds to run, which is longer than its loop interval of 20.0 seconds.\n09:18:55.992 | WARNING | prefect.server.services.cancellationcleanup - CancellationCleanup took 66.945765 seconds to run, which is longer than its loop interval of 20.0 seconds.\n09:20:02.635 | WARNING | prefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 5.264522 seconds to run, which is longer than its loop interval of 5 seconds.\n09:20:04.380 | WARNING | prefect.server.services.cancellationcleanup - CancellationCleanup took 68.379368 seconds to run, which is longer than its loop interval of 20.0 seconds.\n09:20:33.036 | WARNING | prefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 5.393272 seconds to run, which is longer than its loop interval of 5 seconds.\n09:20:43.139 | WARNING | prefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 5.096183 seconds to run, which is longer than its loop interval of 5 seconds.\n```",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Thanks for the issue @ashrielbrian! I just saw this issue in our [CI pipeline](https://github.com/PrefectHQ/prefect/actions/runs/14337099017/job/40186903397) as well:\n```\nERROR tests/client/test_prefect_client.py::TestClientWorkQueues::test_get_runs_from_queue_respects_limit - sqlalchemy.exc.DBAPIError: (sqlalchemy.dialects.postgresql.asyncpg.Error) <class 'asyncpg.exceptions.DeadlockDetectedError'>: deadlock detected\nDETAIL:  Process 102 waits for ShareLock on transaction 5699; blocked by process 187.\nProcess 187 waits for ShareLock on transaction 5700; blocked by process 102.\nHINT:  See server log for query details.\n[SQL: DELETE FROM work_pool]\n(Background on this error at: https://sqlalche.me/e/20/dbapi)\n```\nso it seems intermittent but persistent."
      },
      {
        "user": "svank",
        "body": "I've been seeing similar sporadic deadlocks, but with prefect 3.2.15. Just now it occurred with a very light load on the server (<5 simultaneous flows on a 128-core server). We're running prefect on a local server which is also running Postgres 17.1 in podman. Happy to help collect more info!\n\n```\n$ prefect server start \n                                                                                                                                    \n ___ ___ ___ ___ ___ ___ _____    \n| _ \\ _ \\ __| __| __/ __|_   _|   \n|  _/   / _|| _|| _| (__  | |                                                                                                                                                                                                                                                     \n|_| |_|_\\___|_| |___\\___| |_|              \n                                           \nConfigure Prefect to communicate with the server with:                                                                       \n                                  \n    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n                                                                                                                                       \nView the API reference documentation at http://127.0.0.1:4200/docs\n                                                                  \nCheck out the dashboard at http://127.0.0.1:4200                                                                                                                                                                                                                                  \n                          \n                          \n                                                                                                                             \n21:17:02.450 | WARNING | prefect.server.services.marklateruns - MarkLateRuns took 11.880714 seconds to run, which is longer than its loop interval of 5.0 seconds.\n21:18:56.865 | ERROR   | prefect.server.utilities.messaging.memory - Failed in consume_loop\nTraceback (most recent call last):                                                                                                                                                                                                                                                \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 545, in _prepare_and_execute\n    self._rows = deque(await prepared_stmt.fetch(*parameters))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                     \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/asyncpg/prepared_stmt.py\", line 176, in fetch\n    data = await self.__bind_execute(args, 0, timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/asyncpg/prepared_stmt.py\", line 267, in __bind_execute\n    data, status, _ = await self.__do_execute(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                    \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/asyncpg/prepared_stmt.py\", line 256, in __do_execute\n    return await executor(protocol)                                                                                                                                                                                                                                               \n           ^^^^^^^^^^^^^^^^^^^^^^^^                              \n  File \"asyncpg/protocol/protocol.pyx\", line 206, in bind_execute                                                                                                                                                                                                                 \nasyncpg.exceptions.DeadlockDetectedError: deadlock detected\nDETAIL:  Process 42 waits for ShareLock on transaction 126812486; blocked by process 43.                                          \nProcess 43 waits for ShareLock on speculative token 4 of transaction 126812487; blocked by process 42.\nHINT:  See server log for query details.                                                                                                                                                                                                                                          \n                                  \nThe above exception was the direct cause of the following exception:                                                                                                                                                                                                              \n                                                                                                    \nTraceback (most recent call last):         \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context                                                                                                                                        \n    self.dialect.do_execute(\n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/engine/default.py\", line 942, in do_execute\n    cursor.execute(statement, parameters)                                                                                                                                                                                                                                         \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 580, in execute\n    self._adapt_connection.await_(                                                                                                                                                                                                                                                \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501                                                                                                                                                                              \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn                                                                                                                                    \n    value = await result                                                                \n            ^^^^^^^^^^^^                                                                              \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 558, in _prepare_and_execute\n    self._handle_exception(error)                                                                                                                                                                                                                                                    File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 508, in _handle_exception                                                                                                                                 self._adapt_connection._handle_exception(error)                                                                                                                                                                                                                                  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 792, in _handle_exception                                                                                                                            \n    raise translated_error from error                                                                                                                                                                                                                                              sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_dbapi.Error: <class 'asyncpg.exceptions.DeadlockDetectedError'>: deadlock detected                                                                                                                                      \nDETAIL:  Process 42 waits for ShareLock on transaction 126812486; blocked by process 43.                                                                                                                                                                                           Process 43 waits for ShareLock on speculative token 4 of transaction 126812487; blocked by process 42.                                                                                                                                                                             HINT:  See server log for query details.                                                                                                                                                                                                                                          \n                                                             \nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/prefect/server/utilities/messaging/memory.py\", line 356, in _consume_loop\n    await handler(message)\n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/prefect/server/services/task_run_recorder.py\", line 210, in message_handler\n    await record_task_run_event(event)\n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/prefect/server/services/task_run_recorder.py\", line 169, in record_task_run_event\n    await _insert_task_run(session, task_run, task_run_attributes)\n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/prefect/server/services/task_run_recorder.py\", line 54, in _insert_task_run\n    await session.execute(\n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/ext/asyncio/session.py\", line 463, in execute\n    result = await greenlet_spawn(                                                                                                                                                                                                                                                \n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\n    result = context.throw(*sys.exc_info())                                                                                  \n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                       \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 2365, in execute\n    return self._execute_internal(                                                                                                                                                                                                                                                \n           ^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                       \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/orm/session.py\", line 2251, in _execute_internal\n    result: Result[Any] = compile_state_cls.orm_execute_statement(                                                                                                                                                                                                                \n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                       \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/orm/bulk_persistence.py\", line 1294, in orm_execute_statement\n    result = conn.execute(                                                                                                            \n             ^^^^^^^^^^^^^                                                                                                        \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1416, in execute\n    return meth(                                                                                                                                                                                                                                                                  \n           ^^^^^                                                                                                                \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/sql/elements.py\", line 523, in _execute_on_connection                                                                                                                                      \n    return connection._execute_clauseelement(                    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                     \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1638, in _execute_clauseelement\n    ret = self._execute_context(                                                                                                  \n          ^^^^^^^^^^^^^^^^^^^^^^                                                                      \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1843, in _execute_context                                                                                                                                            \n    return self._exec_single_context(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                             \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1983, in _exec_single_context\n    self._handle_dbapi_exception(          \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 2352, in _handle_dbapi_exception                                                                                                                                     \n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n    self.dialect.do_execute(                                                                                                                                                                                                                                                      \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/engine/default.py\", line 942, in do_execute          \n    cursor.execute(statement, parameters)                                                                                                                                                                                                                                         \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 580, in execute\n    self._adapt_connection.await_(                                                                                                                                                                                                                                                \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501                                                                                                                                                                              \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                             \n  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n    value = await result                                                                                                                                 \n            ^^^^^^^^^^^^                                                                                                                                                                                                                                                             File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 558, in _prepare_and_execute                                                                                                                              self._handle_exception(error)                                                                                                                                                                                                                                                    File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 508, in _handle_exception                                                                                                                            \n    self._adapt_connection._handle_exception(error)                                                                                                                                                                                                                                  File \"/home/samuel.vankooten/venvs/punchpipe/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 792, in _handle_exception                                                                                                                            \n    raise translated_error from error                                                                                                                                                                                                                                              sqlalchemy.exc.DBAPIError: (sqlalchemy.dialects.postgresql.asyncpg.Error) <class 'asyncpg.exceptions.DeadlockDetectedError'>: deadlock detected                                                                                                                                    DETAIL:  Process 42 waits for ShareLock on transaction 126812486; blocked by process 43.                                                                                                                                                                                          \nProcess 43 waits for ShareLock on speculative token 4 of transaction 126812487; blocked by process 42.\nHINT:  See server log for query details.                            \n[SQL: INSERT INTO task_run (flow_run_id, task_key, dynamic_key, flow_run_run_count, empirical_policy, task_inputs, tags, labels, name, run_count, expected_start_time, start_time, total_run_time, id, created, updated) VALUES ($1::UUID, $2::VARCHAR, $3::VARCHAR, $4::INTEGER, $\n5, $6, $7::JSONB, $8::JSONB, $9::VARCHAR, $10::INTEGER, $11::TIMESTAMP WITH TIME ZONE, $12::TIMESTAMP WITH TIME ZONE, $13::INTERVAL, $14::UUID, $15::TIMESTAMP WITH TIME ZONE, $16::TIMESTAMP WITH TIME ZONE) ON CONFLICT (id) DO UPDATE SET flow_run_id = $17::UUID, task_key = $1\n8::VARCHAR, dynamic_key = $19::VARCHAR, flow_run_run_count = $20::INTEGER, empirical_policy = $21, task_inputs = $22, tags = $23::JSONB, labels = $24::JSONB, name = $25::VARCHAR, run_count = $26::INTEGER, expected_start_time = $27::TIMESTAMP WITH TIME ZONE, start_time = $28:\n:TIMESTAMP WITH TIME ZONE, total_run_time = $29::INTERVAL, id = $30::UUID, updated = $31::TIMESTAMP WITH TIME ZONE WHERE task_run.state_timestamp < $32::TIMESTAMP WITH TIME ZONE]\n[parameters: ('156c8162-38be-4c32-abc6-a021e239dfc6', 'load_pipeline_configuration-d22fa713', 'e9486952-ef9e-437e-96f0-a5071314ab6a', 1, '{\"max_retries\": 0, \"retry_delay_seconds\": 0.0, \"retries\": 0, \"retry_delay\": 0, \"retry_jitter_factor\": null}', '{\"path\": []}', '[]', '{}',\n 'load_pipeline_configuration-e94', 1, DateTime(2025, 4, 9, 21, 18, 55, 606924, tzinfo=Timezone('UTC')), DateTime(2025, 4, 9, 21, 18, 55, 616573, tzinfo=Timezone('UTC')), datetime.timedelta(0), '0c7980ad-70c0-43d1-bc3e-544e6340f1f7', DateTime(2025, 4, 9, 21, 18, 55, 641555,\ntzinfo=Timezone('UTC')), DateTime(2025, 4, 9, 21, 18, 55, 646784, tzinfo=Timezone('UTC')), '156c8162-38be-4c32-abc6-a021e239dfc6', 'load_pipeline_configuration-d22fa713', 'e9486952-ef9e-437e-96f0-a5071314ab6a', 1, '{\"max_retries\": 0, \"retry_delay_seconds\": 0.0, \"retries\": 0$\n \"retry_delay\": 0, \"retry_jitter_factor\": null}', '{\"path\": []}', '[]', '{}', 'load_pipeline_configuration-e94', 1, DateTime(2025, 4, 9, 21, 18, 55, 606924, tzinfo=Timezone('UTC')), DateTime(2025, 4, 9, 21, 18, 55, 616573, tzinfo=Timezone('UTC')), datetime.timedelta(0), '0c7\n980ad-70c0-43d1-bc3e-544e6340f1f7', DateTime(2025, 4, 9, 21, 18, 55, 641746, tzinfo=Timezone('UTC')), DateTime(2025, 4, 9, 21, 18, 55, 616573, tzinfo=Timezone('UTC')))]\n(Background on this error at: https://sqlalche.me/e/20/dbapi)\n```\n\nThe Postgres server log  at the time:\n```\n2025-04-07 22:48:46.260 UTC [1] LOG:  database system is ready to accept connections\n2025-04-09 21:18:56.862 UTC [42] ERROR:  deadlock detected\n2025-04-09 21:18:56.862 UTC [42] DETAIL:  Process 42 waits for ShareLock on transaction 126812486; blocked by process 43.\n        Process 43 waits for ShareLock on speculative token 4 of transaction 126812487; blocked by process 42.\n        Process 42: INSERT INTO task_run (flow_run_id, task_key, dynamic_key, flow_run_run_count, empirical_policy, task_inputs, tags, labels, name, run_count, expected_start_time, start_time, total_run_time, id, created, updated) VALUES ($1::UUID, $2::VARCHAR, $3::VARCHAR, $4::INTEGER, $5, $6, $7::JSONB, $8::JSONB, $9::VARCHAR, $10::INTEGER, $11::TIMESTAMP WITH TIME ZONE, $12::TIMESTAMP WITH TIME ZONE, $13::INTERVAL, $14::UUID, $15::TIMESTAMP WITH TIME ZONE, $16::TIMESTAMP WITH TIME ZONE) ON CONFLICT (id) DO UPDATE SET flow_run_id = $17::UUID, task_key = $18::VARCHAR, dynamic_key = $19::VARCHAR, flow_run_run_count = $20::INTEGER, empirical_policy = $21, task_inputs = $22, tags = $23::JSONB, labels = $24::JSONB, name = $25::VARCHAR, run_count = $26::INTEGER, expected_start_time = $27::TIMESTAMP WITH TIME ZONE, start_time = $28::TIMESTAMP WITH TIME ZONE, total_run_time = $29::INTERVAL, id = $30::UUID, updated = $31::TIMESTAMP WITH TIME ZONE WHERE task_run.state_timestamp < $32::TIMESTAMP WITH TIME ZONE\n        Process 43: UPDATE task_run SET state_id=$1::UUID, state_type=$2::state_type, state_name=$3::VARCHAR, state_timestamp=$4::TIMESTAMP WITH TIME ZONE, updated=now() WHERE task_run.id = $5::UUID AND (task_run.state_timestamp IS NULL OR task_run.state_timestamp < $6::TIMESTAMP WITH TIME ZONE)\n2025-04-09 21:18:56.862 UTC [42] HINT:  See server log for query details.\n2025-04-09 21:18:56.862 UTC [42] CONTEXT:  while inserting index tuple (68844,1) in relation \"uq_task_run__flow_run_id_task_key_dynamic_key\"\n2025-04-09 21:18:56.862 UTC [42] STATEMENT:  INSERT INTO task_run (flow_run_id, task_key, dynamic_key, flow_run_run_count, empirical_policy, task_inputs, tags, labels, name, run_count, expected_start_time, start_time, total_run_time, id, created, updated) VALUES ($1::UUID, $2::VARCHAR, $3::VARCHAR, $4::INTEGER, $5, $6, $7::JSONB, $8::JSONB, $9::VARCHAR, $10::INTEGER, $11::TIMESTAMP WITH TIME ZONE, $12::TIMESTAMP WITH TIME ZONE, $13::INTERVAL, $14::UUID, $15::TIMESTAMP WITH TIME ZONE, $16::TIMESTAMP WITH TIME ZONE) ON CONFLICT (id) DO UPDATE SET flow_run_id = $17::UUID, task_key = $18::VARCHAR, dynamic_key = $19::VARCHAR, flow_run_run_count = $20::INTEGER, empirical_policy = $21, task_inputs = $22, tags = $23::JSONB, labels = $24::JSONB, name = $25::VARCHAR, run_count = $26::INTEGER, expected_start_time = $27::TIMESTAMP WITH TIME ZONE, start_time = $28::TIMESTAMP WITH TIME ZONE, total_run_time = $29::INTERVAL, id = $30::UUID, updated = $31::TIMESTAMP WITH TIME ZONE WHERE task_run.state_timestamp < $32::TIMESTAMP WITH TIME ZONE\n2025-04-09 21:19:30.416 UTC [27] LOG:  checkpoint starting: time\n2025-04-09 21:22:11.553 UTC [27] LOG:  checkpoint complete: wrote 1585 buffers (9.7%); 0 WAL file(s) added, 1 removed, 0 recycled; write=160.809 s, sync=0.168 s, total=161.138 s; sync files=112, longest=0.040 s, average=0.002 s; distance=17168 kB, estimate=17168 kB; lsn=33A/A748A3B0, redo lsn=33A/A736F168\n2025-04-09 21:24:30.655 UTC [27] LOG:  checkpoint starting: time\n2025-04-09 21:25:12.231 UTC [27] LOG:  checkpoint complete: wrote 413 buffers (2.5%); 0 WAL file(s) added, 0 removed, 0 recycled; write=41.248 s, sync=0.181 s, total=41.576 s; sync files=63, longest=0.043 s, average=0.003 s; distance=2002 kB, estimate=15651 kB; lsn=33A/A75AF760, redo lsn=33A/A7563C38\n```\n\n```\n$ prefect version\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:28 PM\nOS/Arch:             linux/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-dask:      0.3.3\n  prefect-sqlalchemy: 0.5.2\n```"
      },
      {
        "user": "ashrielbrian",
        "body": "We upgraded from `3.0.3` so the regression could have been introduced between `3.0.3` and `3.2.15`?"
      }
    ]
  },
  {
    "issue_number": 18307,
    "title": "Lost the ability to view task summary in the new UI",
    "author": "jeneoten",
    "state": "open",
    "created_at": "2025-06-16T00:35:39Z",
    "updated_at": "2025-06-16T03:52:49Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI have thousands of tasks in a flow and if one of them failed, I relied on the old UI to filter for failed tasks.\n\nIn the new UI I have to wait 10 minutes to slowly load all the tasks, making it very difficult to debug errors.\n\nPlease give an option to use the old UI, or add the task summary back to the new UI.\n\n\n### Version info\n\n```Text\nVersion:             2.20.18\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          b7059e89\nBuilt:               Wed, Apr 30, 2025 2:37 PM\nOS/Arch:             win32/AMD64\nProfile:             default\nServer type:         ephemeral\nServer:\n  Database:          sqlite\n  SQLite version:    3.49.1\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "jeneoten",
        "body": "https://prefect-community.slack.com/archives/C0192RWGJQH/p1750034153200199?thread_ts=1748383037.447659&cid=C0192RWGJQH"
      },
      {
        "user": "zzstoatzz",
        "body": "hi @jeneoten - thank you for the issue! I think this is a duplicate of https://github.com/PrefectHQ/prefect/issues/18142, feel free to let me know if you think you have a distinct concern! see https://github.com/PrefectHQ/prefect/issues/18142#issuecomment-2968230207"
      },
      {
        "user": "jeneoten",
        "body": "@zzstoatzz My key issue is that I could immediately find the 1 failed task out of 10,000 in the old UI, but in the new UI, I have to slowly load 100 at a time and it takes forever to find the ID of the failed task."
      }
    ]
  },
  {
    "issue_number": 17194,
    "title": "Enable submission of tasks and flows to remote infrastructure without previous registration",
    "author": "desertaxle",
    "state": "open",
    "created_at": "2025-02-19T17:14:13Z",
    "updated_at": "2025-06-15T19:33:21Z",
    "labels": [
      "enhancement"
    ],
    "body": "## Describe the current behavior\n\nToday, when users write workflows with Prefect that need to run on multiple pieces of infrastructure, they are forced to create a deployment for each workflow and use run_deployment to invoke the created deployment. This can lead to a sprawl of deployments that may hold little business meaning but are listed alongside their workflows with business meaning to conform to our current operating model.\n\nHere‚Äôs what a multi-infra workflow would look like today with Prefect:\n\n```python\nfrom prefect import flow\nfrom prefect.deployments import run_deployment\n\n@flow\ndef parent_flow()\n    ... # do some stuff\n    flow_run = run_deployment(\n        name=\"my-vertex-flow/my-vertex-deployment\",\n        parameters={\"foo\": \"bar\"},\n    )\n    vertex_result = flow_run.state.result()\n    ... # do some more stuff\n    run_deployment(\n       name=\"my-k8s-high-cpu-flow/my-k8s-high-cpu-deployment\",\n       parameters={\"foo\": vertex_result},\n    )\n    k8s_high_cpu_result = flow_run.state.result()\n    ... # finish up\n   \n    \n@flow\ndef my_vertex_flow():\n    ... # does things that need GPUs\n    \n\n@flow\ndef my_high_cpu_k8s_flow()\n    ... # really works those CPUs\n```\n\nThis code is hard to grok and easy to mess up.\n\nOutside of this code, the author would also need to create the necessary work pools, create deployments for each flow, and run a worker for each work pool. The parent flow‚Äôs relationship to the child infra-dependent flows is very decoupled making execution of the parent flow brittle to changes in server-side configuration (e.g. deployment name changes).\n\nAdditionally, it‚Äôs highly likely that `my_vertex_flow` and `my_high_cpu_k8s_flow` will fail if called directly because they rely on specialized infrastructure, but the flow author cannot enforce that dependency in Prefect today.\n\n## Describe the proposed behavior\n\nWe want to enable users to run workflows on ad-hoc infrastructure without needing to first create a deployment or run a persistent worker so that they can write multi-infrastructure workflows that are portable, easy to understand, and resilient.\n\nNote that some setup may still be required outside of Prefect to support workflows written to take advantage of the SDK proposed in this document (e.g. a Kubernetes cluster, a GCP account with APIs enabled). There is a possibility to extend this design further to take on responsibility for infra setup, but that is considered out of scope for this design.\n\nImplementing this would involve introducing two new APIs:\n1. Flow submission to workers\n2. Infrastructure decorators for tasks\n\nThe first API would be more low-level and allow greater control over the submission process. The second API would allow decorating functions to bind them to execution in a given environment.\n\n## Example Use\n\n### Bind to infrastructure via a decorator\n\n```python\nfrom prefect import flow\nfrom prefect_kubernetes import kubernetes\n\n@kubernetes(\n    work_pool_name=\"olympic\", \n    memory=64000,\n)\n@flow\ndef my_k8s_flow():\n    return \"hello\"\n\nfuture = my_k8s_flow() # Runs in the \"olympic\" work pool\nprint(future.result())  # prints \"hello\"\n```\n\nBecause `my_k8s_flow` is decorated with `@kubernetes`, it will run via a Kubernetes job each time it is invoked. For this API, the worker creation and flow submission are handled under the hood. Any additional kwargs (like memory in the above example) will be used as job variables to customize the infrastructure the flow runs on.\n\n### Submit to a worker\n\n```python\nfrom prefect import flow\nfrom prefect_kubernetes import KubernetesWorker\n\n@flow\ndef my_k8s_flow():\n    return \"hello\"\n\nwith KubernetesWorker(\n    work_pool_name=\"olympic\"\n) as worker:\n    future = worker.submit(my_k8s_flow, job_variables={\"memory\": 64000})\n    print(future.result())  # prints \"hello\"\n```\n\nWhen this code is executed, a Kubernetes worker will start up locally, query the configured work pool for its configured bundle storage location, bundle the submitted flow, and create a Kubernetes job that will be responsible for executing the flow. The `bundle_storage` configured on the work pool defines a storage location where the worker will upload the bundle for the submitted flow so it can be downloaded and executed in the execution environment (i.e. the Kubernetes job).\n\nUpon submission, the user gets back a PrefectFuture object that can be used to track the execution of that flow and get the result once it is completed. This pattern tracks very closely to the one used by task runners.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Heads up to everyone following this issue: direct submission of flows to dynamic infrastructure is now available in beta!\n\nYou can check out the [docs](https://docs.prefect.io/v3/deploy/submit-flows-directly-to-dynamic-infrastructure) to get started. If you run into any issues or have feedback, feel free to comment here or open a new issue!\n"
      },
      {
        "user": "Rahlir",
        "body": "Thank you for this work, this is a great feature. I have one question: is there a reason that s3 work-pool storage cannot be configured with minio authentication? Is that by design? Or is there some way to configure minio s3 storage for a work pool that I might have missed?\n\nThanks!"
      },
      {
        "user": "desertaxle",
        "body": "Hey @Rahlir! We can definitely support MinIO for work pool storage. If you open an issue for that, I think I can implement it before the next release."
      }
    ]
  },
  {
    "issue_number": 6430,
    "title": "ui not loading icons/swagger on startup without internet",
    "author": "bralbral",
    "state": "closed",
    "created_at": "2022-08-16T18:25:31Z",
    "updated_at": "2025-06-15T14:06:40Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### First check\r\n\r\n- [X] I added a descriptive title to this issue.\r\n- [X] I used the GitHub search to find a similar issue and didn't find it.\r\n- [X] I refreshed the page and this issue still occurred.\r\n- [X] I checked if this issue was specific to the browser I was using by testing with a different browser.\r\n\r\n### Bug summary\r\n\r\n\r\nOn a private network without Internet access, the user interface misses some icons (for example: the key image in \"Secret Block\").\r\n\r\nSimilarly with swagger, but this, I think, is related to fastapi:\r\n\r\nhttps://github.com/tiangolo/fastapi/issues/608\r\n\r\n### Reproduction\r\n\r\nJust start server without internet connection:\r\n\r\n`prefect orion start`\r\n\r\n### Error\r\n\r\n\r\nEmpty swagger page:\r\n![Screenshot from 2022-08-16 21-23-08](https://user-images.githubusercontent.com/54455457/184951624-11782674-9149-42dc-8733-37513c2eaccf.png)\r\n\r\nMissing icons:\r\n![Screenshot from 2022-08-16 21-22-48](https://user-images.githubusercontent.com/54455457/184951625-ab2453f6-e098-435c-93fe-064ac6ec4305.png)\r\n\r\n\r\n### Browers\r\n\r\n- [X] Chrome\r\n- [X] Firefox\r\n- [x] Safari\r\n- [x] Edge\r\n\r\n### Prefect version\r\n\r\n```\r\n\r\n# Copy output of `prefect version` here if hosting your own UI\r\nVersion:             2.0.4\r\nAPI version:         0.8.0\r\nPython version:      3.9.2\r\nGit commit:          39db6fb1\r\nBuilt:               Wed, Aug 10, 2022 1:19 PM\r\nOS/Arch:             linux/x86_64\r\nProfile:             default\r\nServer type:         ephemeral\r\nServer:\r\n  Database:          sqlite\r\n  SQLite version:    3.34.1\r\n\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "zhen0",
        "body": "Thanks for the feedback. For the ui, is there any functionality missing or is the only issue the missing icons? \n\nInteresting point about the swagger docs. I'll leave the issue open to see if there's any further requests/interest in offline access. "
      },
      {
        "user": "bralbral",
        "body": "> Thanks for the feedback. For the ui, is there any functionality missing or is the only issue the missing icons?\r\n\r\n\r\nthis only applies to icons. buttons functionality is not broken.\r\n\r\n"
      },
      {
        "user": "chaoticefx",
        "body": "would definitely be interested in offline access to the swagger docs too!"
      }
    ]
  },
  {
    "issue_number": 12958,
    "title": "Issues with outputs capture (AssertionError: feed_data after feed_eof)",
    "author": "AntoninG",
    "state": "open",
    "created_at": "2023-05-23T15:36:19Z",
    "updated_at": "2025-06-15T08:22:27Z",
    "labels": [
      "bug"
    ],
    "body": "# Bug summary\r\nWhen using `prefect_shell.ShellOperation` or `prefect_dbt.DbtCoreOperation` to run dbt commands, the subprocess seems unable to capture properly the command outputs. \r\n\r\nThis is non blocking as the dbt commands seems to run properly up to the end, but it generates a lot of ERROR-level logs and it seems to prevent me from getting the output as the `fetch_result()` always returns an empty list even though the ERROR logs generated make the dbt logs recognizable as you will see further down this issue. \r\n\r\n# Reproduction\r\n## Dependencies\r\n```\r\nUsing python 3.8.10\r\n\r\n[tool.poetry.dependencies]\r\naeye = \"0.158.0.dev1\"\r\nalembic = \"^1.8.1\"\r\ndbt-bigquery = \"1.5.0\"\r\ndbt-core = \"1.5.0\"\r\nprefect = \"2.10.9\"\r\npsycopg2-binary = \"~=2.9.2\"\r\nprefect-dbt = {version = \"~=0.3\", extras = [\"cli\", \"bigquery\"]}\r\npython-slugify = \"^8.0.1\"\r\n```\r\n\r\n## Task snippet\r\n```\r\ndbt_operation_kwargs = dict(\r\n    commands=[\"dbt --no-use-colors run --target ci\"],\r\n    stream_output=True,\r\n    env={\r\n        \"DBT_PROFILES_DIR\": \".\",\r\n        \"GOOGLE_APPLICATION_CREDENTIALS\": GOOGLE_APPLICATION_CREDENTIALS,\r\n        \"DBT_CI_DATASET\": TESTS_DATASET_NAME,\r\n    },\r\n)\r\n\r\n@task(name=\"tests_create_dataset\", persist_result=True)\r\ndef dbt_task() -> ShellProcess:\r\n    with DbtCoreOperation(**dbt_operation_kwargs) as dbt_operation:\r\n        dbt_process: ShellProcess = dbt_operation.trigger()\r\n        try:\r\n            dbt_process.wait_for_completion()\r\n        except RuntimeError as exc:\r\n            raise RuntimeError(dbt_process.fetch_result()) from exc\r\n        return dbt_process\r\n\r\n@flow\r\ndef main_flow():\r\n    return dbt_task().submit()\r\n```\r\n\r\n## Results\r\n```\r\n...\r\n17:23:15.522 | ERROR   | asyncio - Exception in callback SubprocessStreamProtocol.pipe_data_received(1, b'15:23:15   ...n15:23:15  \\n')\r\nhandle: <Handle SubprocessStreamProtocol.pipe_data_received(1, b'15:23:15   ...n15:23:15  \\n')>\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/events.py\", line 95, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/subprocess.py\", line 69, in pipe_data_received\r\n    reader.feed_data(data)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/streams.py\", line 472, in feed_data\r\n    assert not self._eof, 'feed_data after feed_eof'\r\nAssertionError: feed_data after feed_eof\r\n17:23:15.525 | ERROR   | asyncio - Exception in callback SubprocessStreamProtocol.pipe_data_received(1, b'15:23:15  D...P=0 TOTAL=8\\n')\r\nhandle: <Handle SubprocessStreamProtocol.pipe_data_received(1, b'15:23:15  D...P=0 TOTAL=8\\n')>\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/events.py\", line 95, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/subprocess.py\", line 69, in pipe_data_received\r\n    reader.feed_data(data)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/streams.py\", line 472, in feed_data\r\n    assert not self._eof, 'feed_data after feed_eof'\r\nAssertionError: feed_data after feed_eof\r\n...\r\n```\r\n\r\n# Workaround tested\r\n- Use `dbt-core~=1.3` and `dbt-bigquery~=1.3` as said in this issue: PrefectHQ/prefect-dbt#120 => did not work\r\n- Remove `colorama` package as proposed in this issue: https://github.com/PrefectHQ/prefect/issues/6335 => did not work as this was a dependency of dbt-core\r\n\r\nIt seems other people on PrefectHQ/prefect-dbt#120 are encountering the same problem.\r\n\r\nThanks for your help!\r\n\r\n- [ ] I would like to [help contribute](https://PrefectHQ.github.io/prefect-dbt/#contributing) a pull request to resolve this!\r\n",
    "comments": [
      {
        "user": "giacomochiarella",
        "body": "same issue here"
      },
      {
        "user": "desertaxle",
        "body": "We are investigating a fix for this issue, but we've observed that this bug only affects `DbtCoreOperation` and `ShellOperation` when used in a synchronous context when using `.trigger()`, `.wait_for_completion()`, and `.fetch_results()` in succession. Using the `.run()` method will allow you to retrieve the output of shell operations while we continue to investigate this issue."
      },
      {
        "user": "AntoninG",
        "body": "Hmmm, I will have an issue with this solution. If I am reading correctly, by using run() I have no way to get the outputs if the `return_code != 0` as the `wait_for_completion` is not caught and the `ShellProcess` object is not returned, am I not?"
      }
    ]
  },
  {
    "issue_number": 18264,
    "title": "Invalid username-password pair or user is disabled",
    "author": "jcppython",
    "state": "closed",
    "created_at": "2025-06-10T01:29:54Z",
    "updated_at": "2025-06-14T16:30:24Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n```\nroot@c2970f8cdd1c:/opt/prefect# prefect work-pool ls\n01:10:15.585 | INFO    | prefect - Starting temporary server on http://127.0.0.1:8098\nSee https://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server for more information on running a dedicated Prefect server.\n               Work Pools               \n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Name ‚îÉ Type ‚îÉ ID ‚îÉ Concurrency Limit ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       (**) denotes a paused pool       \n01:10:20.136 | INFO    | prefect - Stopping temporary server on http://127.0.0.1:8098\n01:10:20.315 | ERROR   | uvicorn.error - Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/starlette/routing.py\", line 692, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 217, in __aexit__\n    await anext(self.gen)\n  File \"/usr/local/lib/python3.12/site-packages/prefect/server/api/server.py\", line 639, in lifespan\n    async with Services.running():\n               ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 217, in __aexit__\n    await anext(self.gen)\n  File \"/usr/local/lib/python3.12/site-packages/prefect/server/services/base.py\", line 119, in running\n    await asyncio.gather(*[service.stop() for service in service_tasks])\n  File \"/usr/local/lib/python3.12/site-packages/prefect/server/services/task_run_recorder.py\", line 266, in stop\n    await self.consumer_task\nredis.exceptions.AuthenticationError: invalid username-password pair or user is disabled.\n\n01:10:20.318 | ERROR   | uvicorn.error - Application shutdown failed. Exiting.\n```\n\n\n```\nroot@c2970f8cdd1c:/opt/prefect# env | grep REDIS\nPREFECT_REDIS_MESSAGING_PASSWORD=xxxx  # password is correct\nPREFECT_REDIS_MESSAGING_DB=1\nPREFECT_REDIS_MESSAGING_PORT=6379\nPREFECT_REDIS_MESSAGING_HOST=xxxx\n```\n\n### Version info\n\n```Text\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:30 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.11.0\nServer:\n  Database:          postgresql\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "jcppython",
        "body": "Note the Redis username (PREFECT_REDIS_MESSAGING_USERNAME). Prefect uses \"default\" as the default USERNAME instead of an empty string (\"\")."
      }
    ]
  },
  {
    "issue_number": 18303,
    "title": "\"Change Task Run State\" in UI does not update persisted result",
    "author": "sophiaponte",
    "state": "open",
    "created_at": "2025-06-14T02:24:45Z",
    "updated_at": "2025-06-14T02:24:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nLet's assume I have a flow that runs 3 tasks, where the second task fails:\n\n```\n@task(log_prints=True)\ndef test_task(label, fail):\n    print(\"label\", label, \"should fail\", fail)\n    if fail: \n        raise RuntimeError\n\n@flow(log_prints=True)\ndef test_flow():\n    test_task.with_options(persist_result=True)(label=\"a\", fail=False)\n    test_task.with_options(persist_result=True)(label=\"b\", fail=True)\n    test_task.with_options(persist_result=True)(label=\"c\", fail=False)\n```\n\nI've deployed this flow to a prefect server, and am running it through the UI. As expected, it fails on the second task (label=\"b\").\n\nIf I manually change the state of the failed task to `Completed` through the UI (\"Change Task Run State\" button), and then retry the flow, the first task (label=\"a\") goes into a `Cached` state and is skipped, but second task (label=\"b\") gets re-run. My expectation is that changing the state of the second task to `Completed` would make the task runner skip the second task, since it's (manually forced thru the UI) state is `Completed`. It seems like updating the task state through the UI is not properly updating the result storage for the task.\n\n### Version info\n\n```Text\nVersion:             3.4.5\nAPI version:         0.8.4\nPython version:      3.10.16\nGit commit:          df37c8cf\nBuilt:               Sat, Jun 07, 2025 02:24 AM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.5\nIntegrations:\n  prefect-aws:       0.5.10\n  prefect-kubernetes: 0.3.9\n```\n\n### Additional context\n\nI previously opened a https://github.com/PrefectHQ/prefect/issues/10749 about this same bug for a version of prefect 2, which seems to have been fixed, but I only just got around to updating to Prefect 3, so I'm not sure if something has changed about the expected behavior, or if something is wrong in my setup. \n\nHere are some screenshots:\n\nStep 1: I deploy and run the flow from the UI. As expected, the second task fails. \n\n<img width=\"1365\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f965d361-199c-4f51-81c8-624c574e12be\" />\n\nStep 2: I change the failed task state from the UI\n\n<img width=\"993\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/26802a2f-996f-4b0e-9408-bb0fa8ffd85b\" />\n\nStep 3: Retry the flow, and the first task is in state \"Cached\" (expected), but the second task (label=\"b\") still gets re-run and fails.\n\n<img width=\"1307\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8b10e250-2496-452e-8010-4906a19e4ff6\" />\n\n\n",
    "comments": []
  },
  {
    "issue_number": 18295,
    "title": "Replace toml package with tomli",
    "author": "shunichironomura",
    "state": "open",
    "created_at": "2025-06-13T13:12:20Z",
    "updated_at": "2025-06-14T02:05:37Z",
    "labels": [
      "enhancement",
      "great writeup"
    ],
    "body": "### Describe the current behavior\n\n`prefect` Python package depends on `toml` package ([GitHub repository](https://github.com/uiri/toml)) and use it to read data from TOML files such as `prefect.toml` and `pyproject.toml`.\n\nBecause the [`toml` package doesn't support non-homogeneous arrays](https://github.com/uiri/toml/issues/270), it cannot parse the following TOML (an array with a string and an inline table in it), which can occur in a `pyproject.toml`:\n\n```toml\ndev = [\n    \"pre-commit>=4.2.0\",\n    { include-group = \"typing\" },\n]\n```\n\nAs a result, executing `prefect --help` in a project root with the `pyproject.toml` file with the above pattern will fail. (Try yourself using [this reproduction repository](https://github.com/shunichironomura/prefect-toml-error).)\n\n### Describe the proposed behavior\n\nI propose switching from `toml` to `tomli`, which is (now) the backport library of the built-in `tomllib` module (Read [PEP680](https://peps.python.org/pep-0680/)).\n\nMeanwhile the maintainers of the `toml` library [haven't published a new version since 2020](https://pypi.org/project/toml/#history) with the above issue (submitted in 2019). So switching to the `tomli` library is probably more reasonable than waiting for a fix, submitting one, or even forking it in my view.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "thanks for great writeup @shunichironomura!\n\nI didn't realize this was the case\n> which is (now) the backport library of the built-in tomllib module\n\nOn first read, I agree with your assessment and think we should do this"
      },
      {
        "user": "shunichironomura",
        "body": "Appreciate the kind words! \n\nThe built-in `tomllib` is based on the `tomli` package, effectively making `tomli` its backport.\nFrom [PEP680](https://peps.python.org/pep-0680/):\n> This PEP proposes basing the standard library support for reading TOML on the third-party library tomli ([github.com/hukkin/tomli](https://github.com/hukkin/tomli))."
      },
      {
        "user": "shunichironomura",
        "body": "One issue is that `tomli` doesn't have write capability: https://github.com/hukkin/tomli?tab=readme-ov-file#is-there-a-dumps-write-or-encode-function \n\nThe README of `tomli` suggests [Tomli-W](https://github.com/hukkin/tomli-w) (minimal) and [TOML Kit](https://github.com/python-poetry/tomlkit) (style-preserving), but continuing using `toml` only for the writing for now is also an option.\n\nA quick search for `toml.dump` and `toml.dumps` in the `src` directory of Prefect yields 3 results (although the third one is probably the test code):\n\nhttps://github.com/PrefectHQ/prefect/blob/5486631396a72284b1035fd98e50dc6c837d2cd0/src/prefect/server/api/server.py#L572\n\nhttps://github.com/PrefectHQ/prefect/blob/5486631396a72284b1035fd98e50dc6c837d2cd0/src/prefect/settings/profiles.py#L299\n\nhttps://github.com/PrefectHQ/prefect/blob/5486631396a72284b1035fd98e50dc6c837d2cd0/src/integrations/prefect-kubernetes/tests/test_settings.py#L102\n\nI'm not familiar with the Prefect code base, but it looks to me that at least the TOML Kit is probably an overkill.\n\nAlso, `toml.dumps` is used many times in the test code, but I think having `toml` as a dev dependency is not an issue."
      }
    ]
  },
  {
    "issue_number": 18297,
    "title": "prefect_redis: Invalid username-password pair or user is disabled",
    "author": "jcppython",
    "state": "closed",
    "created_at": "2025-06-13T15:31:20Z",
    "updated_at": "2025-06-13T17:21:06Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI can confirm that the Redis configuration is correct, but I'm not sure why there is a permission error.\n\nCould you help me and tell me where I can print the Redis connection input so that I can confirm the cause?\n\n\n```\nPREFECT_REDIS_MESSAGING_HOST=xxx\nPREFECT_REDIS_MESSAGING_PASSWORD=***\nPREFECT_REDIS_MESSAGING_DB=1\nPREFECT_REDIS_MESSAGING_PORT=6379\n```\n\nfuture: <Task finished name='Task-14' coro=<ReactiveTriggers.start() done, defined at /home/qspace/conda/lib/python3.12/site-packages/prefect/server/events/services/triggers.py:30> exception=AuthenticationError('invalid username-password pair or user is disabled.')>\n\n\n### Version info\n\n```Text\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:28 PM\nOS/Arch:             linux/x86_64\nProfile:             s1sexpt\nServer type:         server\nPydantic version:    2.10.3\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18276,
    "title": "Error running subflow inside another flow",
    "author": "davidesba",
    "state": "open",
    "created_at": "2025-06-11T16:12:42Z",
    "updated_at": "2025-06-13T14:52:27Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhile running this code inside another deployment:\n\n```\nfrom prefect.deployments.flow_runs import run_deployment\n\nflow_run: FlowRun = await run_deployment(\n        deployment_id,\n        parameters={\"request\": request.__dict__} if request else None,\n        timeout=0,\n        as_subflow=True,\n    )\n```\n\nI get the following error, (only if use as_subflow=True):\n```\nTask run failed with exception: PrefectHTTPStatusError(\"Client error '422 Unprocessable Entity' for url '[https://prefect-qa.xcade.dev/api/task_runs/'\\nResponse:](https://prefect-qa.xcade.dev/api/task_runs/'/nResponse:) {'exception_message': 'Invalid request received.', 'exception_detail': [{'type': 'missing', 'loc': ['body', 'task_key'], 'msg': 'Field required', 'input': {}}, {'type': 'missing', 'loc': ['body', 'dynamic_key'], 'msg': 'Field required', 'input': {}}], 'request_body': {}}\\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\") - No retries configured for this task.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/task_engine.py\", line 825, in run_context\n    yield self\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/task_engine.py\", line 1421, in run_task_sync\n    engine.call_task_fn(txn)\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/task_engine.py\", line 842, in call_task_fn\n    result = call_with_parameters(self.task.fn, parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/prefect/src/flows/shared_code.py\", line 115, in run_subflows\n    return run_local_flow_for_nodes(nodes, request, flow)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/prefect/src/flows/common/utils.py\", line 159, in run_local_flow_for_nodes\n    result: PrefectFuture[Response] | None = run_subflow(node.host, flow.name, request)  # type: ignore\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/utilities/asyncutils.py\", line 351, in coroutine_wrapper\n    return run_coro_as_sync(ctx_call())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/utilities/asyncutils.py\", line 207, in run_coro_as_sync\n    return call.result()\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/_internal/concurrency/calls.py\", line 365, in result\n    return self.future.result(timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/_internal/concurrency/calls.py\", line 192, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/_internal/concurrency/calls.py\", line 441, in _run_async\n    result = await coro\n             ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/utilities/asyncutils.py\", line 188, in coroutine_wrapper\n    return await task\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/utilities/asyncutils.py\", line 341, in ctx_call\n    result = await async_fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/prefect/src/flows/common/utils.py\", line 108, in run_subflow\n    flow_run: FlowRun = await run_deployment(\n                        ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/utilities/asyncutils.py\", line 341, in ctx_call\n    result = await async_fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/client/utilities.py\", line 99, in with_injected_client\n    return await fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/deployments/flow_runs.py\", line 163, in run_deployment\n    parent_task_run = await client.create_task_run(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/client/orchestration/__init__.py\", line 829, in create_task_run\n    response = await self._client.post(\"/task_runs/\", content=content)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 1892, in post\n    return await self.request(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 1574, in request\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/client/base.py\", line 361, in send\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/client/base.py\", line 162, in raise_for_status\n    raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__\nprefect.exceptions.PrefectHTTPStatusError: Client error '422 Unprocessable Entity' for url 'https://prefect-qa.xcade.dev/api/task_runs/'\nResponse: {'exception_message': 'Invalid request received.', 'exception_detail': [{'type': 'missing', 'loc': ['body', 'task_key'], 'msg': 'Field required', 'input': {}}, {'type': 'missing', 'loc': ['body', 'dynamic_key'], 'msg': 'Field required', 'input': {}}], 'request_body': {}}\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\n```\n\n\n### Version info\n\n```Text\nVersion:             3.4.4\nAPI version:         0.8.4\nPython version:      3.11.9\nGit commit:          0367d7aa\nBuilt:               Thu, May 29, 2025 09:37 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-docker:    0.6.6\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @davidesba - this seems like some client-server mismatch, potentially not related to calling `run_deployment` from a subflow (where as_subflow defaults to True)\n\nfor example, this works against 3.4.4 server\n\n```python\nfrom prefect import flow, serve\nfrom prefect.deployments import run_deployment\n\n\n@flow\ndef one_flow():\n    print(\"one_flow\")\n\n\n@flow\ndef two_flow():\n    run_deployment(\"one-flow/one-flow\")\n\n\nif __name__ == \"__main__\":\n    serve(one_flow.to_deployment(\"one-flow\"), two_flow.to_deployment(\"two-flow\"))\n```\n\ncan you share information on your server and client versions?"
      },
      {
        "user": "davidesba",
        "body": "I double check and both client and server are running prefect 3.4.4. I also captured the related packets:\n\nRequest:\n```\n[Request URI: /api/task_runs/]\nUser-Agent: prefect/3.4.4 (API 0.8.4)\\r\\n\nX-PREFECT-API-VERSION: 0.8.4\\r\\n\nData:\n{\n    \"state\": {\n        \"type\": \"PENDING\",\n        \"name\": \"Pending\",\n        \"message\": null,\n        \"state_details\": {\n            \"flow_run_id\": null,\n            \"task_run_id\": null,\n            \"child_flow_run_id\": null,\n            \"scheduled_time\": null,\n            \"cache_key\": null,\n            \"cache_expiration\": null,\n            \"deferred\": null,\n            \"untrackable_result\": false,\n            \"pause_timeout\": null,\n            \"pause_reschedule\": false,\n            \"pause_key\": null,\n            \"run_input_keyset\": null,\n            \"refresh_cache\": null,\n            \"retriable\": null,\n            \"transition_id\": null,\n            \"task_parameters_id\": null,\n            \"traceparent\": null\n        },\n        \"data\": null\n    },\n    \"name\": null,\n    \"flow_run_id\": \"3d2debd7-5d02-4d8e-a4c0-38766e41ebd9\",\n    \"task_key\": \"prefect.deployments.flow_runs.run_deployment.iadm-shared-code-list-iatsapp1-iadm-shared-code-list\",\n    \"dynamic_key\": \"0\",\n    \"cache_key\": null,\n    \"cache_expiration\": null,\n    \"task_version\": \"T1055407SharedCodeHS\",\n    \"empirical_policy\": {\n        \"max_retries\": 0,\n        \"retry_delay_seconds\": 0.0,\n        \"retries\": 0,\n        \"retry_delay\": 0,\n        \"retry_jitter_factor\": null\n    },\n    \"tags\": [],\n    \"labels\": {},\n    \"task_inputs\": {}\n}\n```\n\nSever version:\n```\nVersion:             3.4.4\nAPI version:         0.8.4\nPython version:      3.11.9\nGit commit:          0367d7aa\nBuilt:               Thu, May 29, 2025 09:37 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-docker:    0.6.6\n```\n\nResponse:\n```\n{\n    \"exception_message\": \"Invalid request received.\",\n    \"exception_detail\": [\n        {\n            \"type\": \"missing\",\n            \"loc\": [\n                \"body\",\n                \"task_key\"\n            ],\n            \"msg\": \"Field required\",\n            \"input\": {}\n        },\n        {\n            \"type\": \"missing\",\n            \"loc\": [\n                \"body\",\n                \"dynamic_key\"\n            ],\n            \"msg\": \"Field required\",\n            \"input\": {}\n        }\n    ],\n    \"request_body\": {}\n}\n```\n"
      },
      {
        "user": "davidesba",
        "body": "As an update, I found that if I create a task that runs the deployment without idempotency_key it works:\n\n```python\n@task(name=\"run_subflow\", description=\"Encapsutes subflow run\", task_run_name=\"{name}\")\nasync def run_subflow_task(\n    name: str, deployment_id: UUID, request: object\n) -> PrefectFlowRunFuture | None:\n    task_run_ctx = TaskRunContext.get()\n    if not task_run_ctx:\n        raise ValueError(\"Cannot find task run context\")\n\n    async with get_client() as client:\n        flow_run: FlowRun = await client.create_flow_run_from_deployment(\n            deployment_id,\n            parameters={\"request\": request.__dict__} if request else None,\n            state=Scheduled(scheduled_time=now(\"UTC\")),\n            idempotency_key=None,\n            parent_task_run_id=task_run_ctx.task_run.id,\n        )\n        return PrefectFlowRunFuture(flow_run.id)\n```\n"
      }
    ]
  },
  {
    "issue_number": 18293,
    "title": "Better document dynamic Asset materialization",
    "author": "aaazzam",
    "state": "open",
    "created_at": "2025-06-13T01:30:00Z",
    "updated_at": "2025-06-13T02:20:48Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nIn the most recent release of assets, Prefect supports dynamic asset materialization:\n\n```python\n\n@task\ndef scrape_url(slug: str) -> str:\n    ...\n\ndef write_to_s3(content: str) -> None:\n    ...\n\n@flow\ndef scrape_urls(slugs: list[str]) -> None:\n    for url in urls:\n        content = scrape_url(slug).with_options(asset_deps=[f'reddit://{url}'])\n        materialize(f's3://{url}')(write_to_s3)(content)\n```\n\n### Describe the proposed behavior\n\nDynamic asset materialization is a common ask in other frameworks, so let's make sure to clearly document. \n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "aaazzam",
        "body": "Motivating use cases, see good write ups here:\n\n- https://github.com/dagster-io/dagster/issues/9559#issuecomment-2777592083 by @alexlowellmartin\n- https://github.com/dagster-io/dagster/issues/9559#issuecomment-2797623517 by @funkybunchesofoats-ih\n- https://github.com/dagster-io/dagster/issues/9559#issuecomment-2667536535 by @mielkec-gene\n- https://github.com/dagster-io/dagster/issues/9559#issuecomment-1778963708 by @aaaaahaaaaa\n- https://github.com/dagster-io/dagster/issues/9559#issuecomment-1561853169 by @adjit\n"
      },
      {
        "user": "mielkec-gene",
        "body": "Even more reasons for me to learn Prefect"
      }
    ]
  },
  {
    "issue_number": 18231,
    "title": "When Flow runs more than 10000, can not delete selected flows in web UI",
    "author": "caihua",
    "state": "open",
    "created_at": "2025-06-04T08:42:27Z",
    "updated_at": "2025-06-12T23:17:21Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI found that if flow runs is more than 10000 , can not delete flow in the web UI - flows tab.\n\n### Version info\n\n```Text\n3.3.7\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @caihua - thanks for the issue! could you clarify if you're using Prefect Cloud or self-hosted server? if OSS server, any details about your installation are appreciated"
      }
    ]
  },
  {
    "issue_number": 18239,
    "title": "SnowflakeConnector repeated calls to get_connection result in connection closed",
    "author": "TWeatherston",
    "state": "closed",
    "created_at": "2025-06-05T07:39:37Z",
    "updated_at": "2025-06-12T21:58:03Z",
    "labels": [
      "bug",
      "good first issue",
      "integrations"
    ],
    "body": "### Bug summary\n\nWhen using the SnowflakeConnector block, if the block is loaded at the flow level and passed to a task, calling get_connection inside the task will result in a closed connection error after the first task run.\n\nIt seems as though the block is storing the first connection and not cleaning it up when it closes.\n\n```python\nfrom prefect import flow, task\nfrom prefect_snowflake import SnowflakeConnector\n\n\n@task(log_prints=True)\ndef query_snowflake(connector: SnowflakeConnector):\n    with connector.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT 1;\")\n        result = cursor.fetchone()\n        print(result)\n        return result[0]\n\n\n@flow\ndef example_flow():\n    connector = SnowflakeConnector.load(\"example-snowflake-connector\")\n    query_snowflake(connector)\n    query_snowflake(connector)\n\n\nif __name__ == \"__main__\":\n    example_flow()\n```\n\n```Finished in state Failed('Task run encountered an exception DatabaseError: 250002 (08003): Connection is closed')```\n\n### Version info\n\n```Text\nVersion:             3.4.3\nAPI version:         0.8.4\nPython version:      3.11.11\nGit commit:          1c2ba7a4\nBuilt:               Thu, May 22, 2025 10:00 PM\nOS/Arch:             darwin/arm64\nProfile:             staging\nServer type:         cloud\nPydantic version:    2.11.5\nIntegrations:\n  prefect-snowflake: 0.28.4\n  prefect-gcp:       0.6.7\n  prefect-aws:       0.5.9\n```\n\n### Additional context\n\nMoving the block loading inside the task does resolve this issue:\n\n```python\n@task(log_prints=True)\ndef query_snowflake():\n    connector = SnowflakeConnector.load(\"example-snowflake-connector\")\n    with connector.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT 1;\")\n        result = cursor.fetchone()\n        print(result)\n        return result[0]\n```",
    "comments": []
  },
  {
    "issue_number": 18142,
    "title": "New flow run UI very cluttered",
    "author": "liveeo-philipp",
    "state": "open",
    "created_at": "2025-05-22T12:27:38Z",
    "updated_at": "2025-06-12T21:40:27Z",
    "labels": [
      "enhancement",
      "cloud",
      "UI/UX"
    ],
    "body": "### Describe the current behavior\n\nThe new web UI for flow runs is rather cluttered. Previously, we were able to select certain view (tasks, logs, artefacts, ...) in individual pages, whereas now everything needs to be searched for across a very cluttered page without much visual guidance.\n\n### Describe the proposed behavior\n\nCleaner separation of different views (e.g., a task page, a logs page, an artefacts page, an input parameters page, ...) which can be quickly reached via links / tabs directly ‚Äì without having to search for this information amongst the very cluttered flow run page. \n\n### Example Use\n\n_No response_\n\n### Additional context\n\nWhy is this important? Often I'm looking for a list of tasks to get an overview over what's happening in my flow. I know have to apply specific filters in the UI to reduce the clutter, whereas before I could simply go to the \"tasks\" tab. Same for artefacts, which requires me to now scroll through a long list of information I don't need at that moment. The amount of text / information in both columns is waaaay to large and navigating to the data I'm actually interested in takes me much longer than just clicking on the correct tab in the old UI. \n\nI see why this UI might be nice on a dashboard on very large screens but in my personal experience it performs very subpar when you're actually trying to interact with it and find specific information that used to be \"just a tab away\".",
    "comments": [
      {
        "user": "rbanick",
        "body": "Seconding this, the new UI is very difficult to use for our team. The previous workflow which prioritized the logs was great for our use case and indeed we actively engineered around it to present useful logs. "
      },
      {
        "user": "zzstoatzz",
        "body": "thanks for your feedback @liveeo-philipp and @rbanick - we hear you and are planning to offer a more direct / raw view of logs soon!"
      }
    ]
  },
  {
    "issue_number": 18282,
    "title": "The task cannot see the parameter values",
    "author": "jcppython",
    "state": "closed",
    "created_at": "2025-06-12T05:07:15Z",
    "updated_at": "2025-06-12T16:16:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n<img width=\"980\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6e522ae7-5b16-4382-9738-456f53fec505\" />\n\ncode demo\n\n```\nfrom prefect import flow, task\nfrom typing import List, Dict, Optional\n\n@task(name=\"test_task\")\ndef process_data(\n    data: List[Dict],\n    threshold: float = 0.5,\n    config: Optional[Dict] = None\n) -> Dict:\n    result = {\"processed\": len(data), \"threshold\": threshold}\n    if config:\n        result.update(config)\n    return result\n\n@flow(name=\"test_flow\")\ndef my_flow(data_source: str, config: Dict = {\"mode\": \"normal\"}):\n    data = [{\"id\": i, \"value\": i/10} for i in range(10)]\n    \n    result = process_data(data, threshold=0.7, config=config)\n    return result\n\nif __name__ == \"__main__\":\n    my_flow(data_source=\"example_data\", config={\"mode\": \"debug\"})\n```\n\n### Version info\n\n```Text\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:28 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.10.3\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @jcppython - this is by design, task run parameter values are currently not stored in the API (which makes tasks faster and oftentimes task inputs are not serializable). if you click on the tooltip for Task Run Inputs you'll get\n\n<img width=\"532\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5f480d6b-401e-4b2f-b86a-b5b2a7609a7f\" />\n\ndo you want me to convert this to a discussion?"
      },
      {
        "user": "jcppython",
        "body": "Perhaps we could have some asynchronous ways to pull parameters, which can both meet the needs of problem - troubleshooting and avoid affecting performance.\n\nOkay, I'm glad to discuss this issue. @zzstoatzz "
      }
    ]
  },
  {
    "issue_number": 18285,
    "title": "Python 3.13 support for Prefect 2.x",
    "author": "toby-coleman",
    "state": "closed",
    "created_at": "2025-06-12T14:10:11Z",
    "updated_at": "2025-06-12T16:14:30Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nPrefect 3.x is supported on Python 3.13 - see https://github.com/PrefectHQ/prefect/pull/17577.\n\nWould it be possible to have support for Prefect 2.x versions as well?  Currently install is blocked by the `asyncpg<0.30.0` constraint - see https://github.com/MagicStack/asyncpg/releases/tag/v0.30.0.\n\n### Describe the proposed behavior\n\nWorking install on Python 3.13.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @toby-coleman - we will not be adding python 3.13 support for the 2.x lineage. 2.x will only receive bug fixes and critical security patches."
      }
    ]
  },
  {
    "issue_number": 17735,
    "title": "Exception occur in worker causing restarts",
    "author": "giacomochiarella",
    "state": "open",
    "created_at": "2025-04-04T15:33:43Z",
    "updated_at": "2025-06-12T13:31:04Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n\nIn Prefect 3.2.14, I'm having this exception. Would be great some guidance in understanding what it is about. Once the agent restarts all the running jobs are hanging in running state. The only way to make them recover is to set another deployment which restarts all the running/pending/schedule flow runs which are those states since X hours\n```\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  + Exception Group Traceback (most recent call last):\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/site-packages/prefect/cli/_utilities.py‚Äù, line 44, in wrapper\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   return fn(*args, **kwargs)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/site-packages/prefect/cli/_types.py‚Äù, line 155, in sync_fn\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   return asyncio.run(async_fn(*args, **kwargs))\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/asyncio/runners.py‚Äù, line 44, in run\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   return loop.run_until_complete(main)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/asyncio/base_events.py‚Äù, line 649, in run_until_complete\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   return future.result()\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/site-packages/prefect/cli/worker.py‚Äù, line 169, in start\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   await worker.start(\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/site-packages/prefect/workers/process.py‚Äù, line 149, in start\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   async with self as worker:\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/site-packages/prefect/workers/process.py‚Äù, line 242, in __aexit__\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   await super().__aexit__(*exc_info)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/site-packages/prefect/workers/base.py‚Äù, line 1344, in __aexit__\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   await self.teardown(*exc_info)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/site-packages/prefect/workers/base.py‚Äù, line 726, in teardown\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   await self._exit_stack.__aexit__(*exc_info)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/contextlib.py‚Äù, line 714, in __aexit__\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   raise exc_details[1]\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/contextlib.py‚Äù, line 697, in __aexit__\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   cb_suppress = await cb(*exc_details)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |  File ‚Äú/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py‚Äù, line 772, in __aexit__\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  |   raise BaseExceptionGroup(\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |  +-+---------------- 1 ----------------\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |   | Exception Group Traceback (most recent call last):\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |   |  File ‚Äú/usr/local/lib/python3.10/contextlib.py‚Äù, line 697, in __aexit__\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |   |   cb_suppress = await cb(*exc_details)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |   |  File ‚Äú/usr/local/lib/python3.10/site-packages/prefect/runner/runner.py‚Äù, line 1688, in __aexit__\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |   |   await self._runs_task_group.__aexit__(*exc_info)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |   |  File ‚Äú/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py‚Äù, line 772, in __aexit__\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |   |   raise BaseExceptionGroup(\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |   | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |   +-+---------------- 1 ----------------\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |    | Traceback (most recent call last):\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |    |  File ‚Äú/usr/local/lib/python3.10/site-packages/prefect/runner/runner.py‚Äù, line 1584, in wrapper\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |    |   result = fn(*args, **kwargs)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |    | KeyError: UUID(‚Äòdadccbfd-41a7-40ca-8ace-574390f79d2a‚Äô)\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 |    +------------------------------------\nApr 04 15:15:17 ip-172-30-1-217 start_prefect.sh[1855]: agent_1 | An exception occurred.\n```\n\nunfortunately it is not clear what the exception is about\n\n### Version info\n\n```Text\nPrefect 3.2.14\nPython 3.10\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Hey @giacomochiarella! Are you able to reliably reproduce this error? If so, what are the steps to reproduce it?"
      },
      {
        "user": "giacomochiarella",
        "body": "@desertaxle unfortunately I cannot. I don't know what is causing the error. I have the agent running in a docker container with 5 cpus and 10gb of ram and the instance was not even overloaded when it happened, just a dozen of flow runs running and not heavy because the flow runs just spins ecs tasks, the  actual computation happens in the cloud. If you have any idea what is it about and make me reproduce I would be more glad to try"
      },
      {
        "user": "giacomochiarella",
        "body": "@desertaxle I've got this other issue, any idea what could be?\n```\nFailed to submit flow run '767b6afa-cf77-4dc2-9ad0-3a2e0a34c959' to infrastructure.\n  + Exception Group Traceback (most recent call last):\n  |   File \"/usr/local/lib/python3.10/site-packages/prefect/workers/base.py\", line 1056, in _submit_run_and_capture_errors\n  |     result = await self.run(\n  |   File \"/usr/local/lib/python3.10/site-packages/prefect/workers/process.py\", line 218, in run\n  |     process = await self._runner.execute_flow_run(\n  |   File \"/usr/local/lib/python3.10/site-packages/prefect/runner/runner.py\", line 586, in execute_flow_run\n  |     async with anyio.create_task_group() as tg:\n  |   File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/usr/local/lib/python3.10/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    |     yield\n    |   File \"/usr/local/lib/python3.10/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    |     resp = await self._pool.handle_async_request(req)\n    |   File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    |     raise exc from None\n    |   File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py\", line 232, in handle_async_request\n    |     connection = await pool_request.wait_for_connection(timeout=timeout)\n    |   File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py\", line 35, in wait_for_connection\n    |     await self._connection_acquired.wait(timeout=timeout)\n    |   File \"/usr/local/lib/python3.10/site-packages/httpcore/_synchronization.py\", line 149, in wait\n    |     with map_exceptions(anyio_exc_map):\n    |   File \"/usr/local/lib/python3.10/contextlib.py\", line 153, in __exit__\n    |     self.gen.throw(typ, value, traceback)\n    |   File \"/usr/local/lib/python3.10/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    |     raise to_exc(exc) from exc\n    | httpcore.PoolTimeout\n    | \n    | The above exception was the direct cause of the following exception:\n    | \n    | Traceback (most recent call last):\n    |   File \"/usr/local/lib/python3.10/site-packages/prefect/runner/runner.py\", line 589, in execute_flow_run\n    |     flow_run = await self._client.read_flow_run(flow_run_id)\n    |   File \"/usr/local/lib/python3.10/site-packages/prefect/client/orchestration/_flow_runs/client.py\", line 668, in read_flow_run\n    |     response = await self.request(\n    |   File \"/usr/local/lib/python3.10/site-packages/prefect/client/orchestration/base.py\", line 53, in request\n    |     return await self._client.send(request)\n    |   File \"/usr/local/lib/python3.10/site-packages/prefect/client/base.py\", line 323, in send\n    |     response = await self._send_with_retry(\n    |   File \"/usr/local/lib/python3.10/site-packages/prefect/client/base.py\", line 247, in _send_with_retry\n    |     response = await send(request, *send_args, **send_kwargs)\n    |   File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1629, in send\n    |     response = await self._send_handling_auth(\n    |   File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    |     response = await self._send_handling_redirects(\n    |   File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    |     response = await self._send_single_request(request)\n    |   File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    |     response = await transport.handle_async_request(request)\n    |   File \"/usr/local/lib/python3.10/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    |     with map_httpcore_exceptions():\n    |   File \"/usr/local/lib/python3.10/contextlib.py\", line 153, in __exit__\n    |     self.gen.throw(typ, value, traceback)\n    |   File \"/usr/local/lib/python3.10/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    |     raise mapped_exc(message) from exc\n    | httpx.PoolTimeout\n    +------------------------------------\n```\n"
      }
    ]
  },
  {
    "issue_number": 18278,
    "title": "`result_async()` always timeout when accessing results from background task created using `delay()`",
    "author": "pratheekrebala",
    "state": "closed",
    "created_at": "2025-06-11T18:34:13Z",
    "updated_at": "2025-06-11T22:15:38Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI am triggering a background task using the `delay()` method and trying to access the result by awaiting the `result_async()` method. If the task isn't completed by the time a call to `result_async()` is made, it always raises an incorrect `TimeoutError`. I believe this is a bug in how the `_final_state` value is checked here:\n\nhttps://github.com/PrefectHQ/prefect/blob/90f261bc0c06e5fc6cda92e83c39ff5131f6d52a/src/prefect/futures.py#L289-L301\n\nThe promise correctly waits until the task is completed but the `TimeoutError` is consistently triggered because `_final_state` is never set. Unless of course, the task is already completed by the time the call to `result_async()` is made. \n\nTo reproduce this:\n\n1. Create a `background_worker.py` file:\n\n```python\nfrom prefect import flow, task\nfrom prefect.task_worker import serve\nimport time\n\n@task\nasync def background_task():\n    time.sleep(10)\n\nif __name__ == '__main__':\n    serve(background_task)\n```\n\n2. Run the worker: `python background_worker.py`\n\n3. While that is running, call the background task from another script:\n\n```python\nfrom background_worker import background_task\n\ntask = background_task.delay()\nresult = await task.result_async(timeout = 100) # this will always trigger a timeout unless task is already completed when this method is called. It will however, block until the result is completed.\n```\n\nIt seems like the only way to make this work is to separately wait for the task by doing:\n\n```\ntask = background_task.delay()\nawait task.wait_async()\nresult = await task.result_async()\n```\n\nIs there a way to make this workflow work? Or am I doing something incorrectly here? I'm hoping to effectively launch a background task and wait for it's results asynchronously.\n\n### Version info\n\n```Text\nVersion:             3.4.5\nAPI version:         0.8.4\nPython version:      3.11.2\nGit commit:          df37c8cf\nBuilt:               Sat, Jun 07, 2025 02:24 AM\nOS/Arch:             linux/aarch64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.11.5\nIntegrations:\n  prefect-dask:      0.3.5\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "thanks for the issue @pratheekrebala - this should now be fixed on `main` - let me know if you have a chance to test it before we release 3.4.7!"
      }
    ]
  },
  {
    "issue_number": 18121,
    "title": "Tasks stuck in \"Running\" state with task.submit() pattern",
    "author": "not-Karot",
    "state": "open",
    "created_at": "2025-05-20T11:07:32Z",
    "updated_at": "2025-06-11T20:35:54Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nTasks submitted using the task.submit() pattern get stuck in \"Running\" state indefinitely and never complete, causing flows to hang when calling future.result(). The issue is non-deterministic - sometimes the same code works perfectly, other times tasks get stuck forever.\nThis affects both ConcurrentTaskRunner and ThreadPoolTaskRunner\n\n```python\nfrom prefect import flow, task\nfrom prefect.task_runners import ConcurrentTaskRunner\nimport time\n\n@task(persist_result=False)\ndef quick_task(item_id: int) -> str:\n    \"\"\"Simple task that should complete in 1 second\"\"\"\n    time.sleep(1)\n    return f\"Processed item {item_id}\"\n\n@flow(task_runner=ConcurrentTaskRunner(max_workers=5), persist_result=False)\ndef test_parallel_submission():\n    \"\"\"Submit multiple tasks and wait for results\"\"\"\n    print(\"Submitting 10 tasks...\")\n    \n    # Submit all tasks\n    futures = []\n    for i in range(10):\n        future = quick_task.submit(i)\n        futures.append(future)\n    \n    print(\"Waiting for results...\")\n    # THIS IS WHERE TASKS GET STUCK - some futures never resolve\n    results = [f.result() for f in futures]\n    \n    print(f\"Completed: {len(results)} results\")\n    return results\n\nif __name__ == \"__main__\":\n    test_parallel_submission()\n```\n\n**Expected behavior**: All 10 tasks complete in ~2 seconds (5 concurrent workers, 2 batches of 1 second each)\n\n**Actual behavior**:\n\n- Sometimes works as expected\n- Sometimes 1-3 tasks get stuck in \"Running\" state forever\n- Flow hangs indefinitely with no info or logs\n- No errors, exceptions, or timeouts occur\n- Tasks appear as \"Running\" in Prefect UI but never transition to completed state\n\n\n### Version info\n\n```Text\nprefect 3.4.2\n\npython\t3.10.14\n```\n\n### Additional context\n\n![Image](https://github.com/user-attachments/assets/c62bc1f6-40db-40e4-9926-c311d20c76d5)\n\nMultiple set_job_role-* tasks all stuck at the same timestamp (12:53 PM) in \"Running\" state. These tasks should complete in 1-2 seconds but have been running for several minutes.\n\n\n**Workaround attempts that didn't help:**\n\n- Reducing batch sizes\n- Lowering max_workers\n- Adding delays between task submissions\n- Switching between task runners",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "thanks for the report @not-Karot !\n\nwe'll look into this\n\nas a note on this\n> This affects both ConcurrentTaskRunner and ThreadPoolTaskRunner\n\n`ConcurrentTaskRunner` is just an alias for `ThreadPoolTaskRunner` for backwards compat so it makes sense the behavior is the same"
      },
      {
        "user": "nikhil-ramanan1",
        "body": "fwiw I am also seeing this behavior with RayTaskRunner"
      },
      {
        "user": "ArosTRM",
        "body": "We have also started to see this using ThreadPoolTaskRunner, but also just normal flow().\nWe have also changed to spot nodes in our Azure Kubernetes Service cluster. \nNot sure if this can affect Prefects ability to maintain connection to the pod instances. "
      }
    ]
  },
  {
    "issue_number": 17189,
    "title": "When I try to update body of an action in automation from prefect UI, I get 404 on api/automations/templates/validate API.",
    "author": "patilpradnya",
    "state": "closed",
    "created_at": "2025-02-19T15:13:12Z",
    "updated_at": "2025-06-11T18:59:06Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### Bug summary\n\nI am using prefect 3 open-source version. I want to create automation for receiving notifications when a flow starts or stops. I am using slack webhook for notifications. \nWhen I try to create automation, and go to the action section, select action type = send notification, block = my slack webhook block, and then try to edit the default body of notification, the spinner never goes away. When I check the network console, I am getting 404 Not Found on api/automations/templates/validate API. \n\n\n\n\n### Version info\n\n```Text\nVersion:             3.1.14\nAPI version:         0.8.4\nPython version:      3.10.13\nGit commit:          5f1ebb57\nBuilt:               Thu, Jan 23, 2025 1:22 PM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-email:     0.4.1\n  prefect-slack:     0.3.1\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "Ishankoradia",
        "body": "I am facing this issue too , anything on this. "
      },
      {
        "user": "hnykda",
        "body": "same here. \n\n![Image](https://github.com/user-attachments/assets/bbe900f1-3b37-4b60-bbef-1c4a1330bb9c)\n\nit seems the endpoint is at the wrong place in the hierarchy.\n\n![Image](https://github.com/user-attachments/assets/5e3fffb4-c77e-45de-88f4-5784f84b2f5e)"
      },
      {
        "user": "hnykda",
        "body": "Maybe this solves it? https://github.com/PrefectHQ/prefect/pull/18259"
      }
    ]
  },
  {
    "issue_number": 18203,
    "title": "Subflow failing on parameter validation renders as endless bar in parent flow graph",
    "author": "GalLadislav",
    "state": "open",
    "created_at": "2025-05-29T21:02:07Z",
    "updated_at": "2025-06-11T16:11:14Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nIf i call a subflow with bad parameters in a flow, in flow run graph the subflows bar just keeps growing endlessly, to a point that soon all subflow and task bars disappears.\n\n```python\nimport prefect\n\n\n@prefect.flow\ndef flow_with_params(param: str):\n    return param\n\n\n@prefect.flow\ndef test_flow():\n    flow_with_params(454)\n\n\nif __name__ == \"__main__\":\n    test_flow()\n\n```\n\n![Image](https://github.com/user-attachments/assets/b3dfcca3-975d-4a2d-86d2-444c5a7b6792)\n\n### Version info\n\n```Text\nVersion:             3.4.2\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          c3c1c119\nBuilt:               Mon, May 19, 2025 04:04 PM\nOS/Arch:             linux/x86_64\nProfile:             client\nServer type:         server\nPydantic version:    2.11.4\nIntegrations:\n  prefect-email:     0.4.2\n```\n\n### Additional context\n\nIn DB table there are only two states, Pending and Failed.\n![Image](https://github.com/user-attachments/assets/db8a30e8-3d50-434b-ab6e-546c7d88a81a)",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "thanks @GalLadislav - will look into this"
      }
    ]
  },
  {
    "issue_number": 18254,
    "title": "`add_metadata` fails when called from a `@task`",
    "author": "zzstoatzz",
    "state": "closed",
    "created_at": "2025-06-08T00:58:29Z",
    "updated_at": "2025-06-11T16:08:24Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nrunning this results in `ValueError: Can only add metadata to assets that are arguments to @materialize`\n```python\nfrom prefect import flow, task\nfrom prefect.assets import Asset, materialize\n\nmy_asset = Asset(key=\"test://my-asset\")\n\n@materialize(my_asset)\nasync def materialize_task():\n    my_asset.add_metadata({\"key\": \"value\"})\n    return \"done\"\n\n@task\nasync def wrapper_task():\n    return await materialize_task()\n\n@flow\nasync def test_flow():\n    # Direct call - works\n    await materialize_task()\n    \n    # Nested call - fails with ValueError\n    await wrapper_task()\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(test_flow())\n```\n\n<details>\n<summary>trace</summary>\n\n```python\n19:57:27.217 | ERROR   | Flow run 'attractive-sponge' - Finished in state Failed('Flow run encountered an exception: ValueError: Can only add metadata to assets that are arguments to @materialize')\nTraceback (most recent call last):\n  File \"/Users/nate/github.com/prefecthq/marvin/asset_metadata_nested_task_mre.py\", line 25, in <module>\n    asyncio.run(test_flow())\n  File \"/Users/nate/Library/Application Support/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/nate/Library/Application Support/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/nate/Library/Application Support/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 686, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/flow_engine.py\", line 1396, in run_flow_async\n    return engine.state if return_type == \"state\" else await engine.result()\n                                                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/flow_engine.py\", line 915, in result\n    raise self._raised\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/flow_engine.py\", line 1332, in run_context\n    yield self\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/flow_engine.py\", line 1394, in run_flow_async\n    await engine.call_flow_fn()\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/flow_engine.py\", line 1346, in call_flow_fn\n    result = await call_with_parameters(self.flow.fn, self.parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/nate/github.com/prefecthq/marvin/asset_metadata_nested_task_mre.py\", line 21, in test_flow\n    await wrapper_task()\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1498, in run_task_async\n    return engine.state if return_type == \"state\" else await engine.result()\n                                                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1059, in result\n    raise self._raised\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1425, in run_context\n    yield self\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1496, in run_task_async\n    await engine.call_task_fn(txn)\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1442, in call_task_fn\n    result = await call_with_parameters(self.task.fn, parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/nate/github.com/prefecthq/marvin/asset_metadata_nested_task_mre.py\", line 13, in wrapper_task\n    return await materialize_task()\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1498, in run_task_async\n    return engine.state if return_type == \"state\" else await engine.result()\n                                                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1059, in result\n    raise self._raised\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1425, in run_context\n    yield self\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1496, in run_task_async\n    await engine.call_task_fn(txn)\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1442, in call_task_fn\n    result = await call_with_parameters(self.task.fn, parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/nate/github.com/prefecthq/marvin/asset_metadata_nested_task_mre.py\", line 8, in materialize_task\n    my_asset.add_metadata({\"key\": \"value\"})\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/assets/core.py\", line 62, in add_metadata\n    asset_ctx.add_asset_metadata(self.key, metadata)\n  File \"/Users/nate/github.com/prefecthq/marvin/.venv/lib/python3.12/site-packages/prefect/context.py\", line 562, in add_asset_metadata\n    raise ValueError(\nValueError: Can only add metadata to assets that are arguments to @materialize\n```\n</details>\n\nthis feels related to the asset context var, that is: when `materialize_task` is called directly, it creates a new `AssetContext`. when it's called from within another task, there's already an `AssetContext` set up for the wrapper task that doesn't have the assets from `materialize_task` in its `downstream_assets`? maybe?\n\n### Version info\n\n```Text\nmain\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "closed by https://github.com/PrefectHQ/prefect/pull/18265"
      }
    ]
  },
  {
    "issue_number": 18272,
    "title": "Prefect self-host server crashing when running scheduled job every minute",
    "author": "Gunnar-Stunnar",
    "state": "closed",
    "created_at": "2025-06-11T09:45:41Z",
    "updated_at": "2025-06-11T16:05:03Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI have a scheduled job that runs every minute, the prefect server seems to be slowing down immensely every time it runs. For my basic setup I just have a flow deployment that runs every minute\n\n```python\n    flow.from_source(\n        source=git_repo,\n        entrypoint=\"...\"\n    ).deploy(\n        name=f\"deployment-name\",\n        work_pool_name=\"...\",\n        work_queue_name=\"...\",\n        rrule=\"FREQ=MINUTELY;BYHOUR=6,7,8,9,10,11,12,13,14,15,16,17;INTERVAL=1\",\n        job_variables={\n            \"env\": {...}\n        }\n    )\n```  \n\nThank you\n\n### Version info\n\n```Text\n3.4.1\n```\n\n### Additional context\n\nI have ran this deployment with Kubernete work pool and process work pool and both have been crashing or slowing down the prefect-server immensely on my cluster. I self-host postgresql as the database on my cluster. ",
    "comments": []
  },
  {
    "issue_number": 18263,
    "title": "upgrade to 3.3.4 or beyond fails on large database",
    "author": "cayspekko",
    "state": "closed",
    "created_at": "2025-06-09T22:15:53Z",
    "updated_at": "2025-06-10T20:55:30Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n```\nserver-1         | ERROR:    Traceback (most recent call last):\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/starlette/routing.py\", line 692, in lifespan\nserver-1         |     async with self.lifespan_context(app) as maybe_state:\nserver-1         |                ~~~~~~~~~~~~~~~~~~~~~^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\nserver-1         |     return await anext(self.gen)\nserver-1         |            ^^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/api/server.py\", line 632, in lifespan\nserver-1         |     await run_migrations()\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/api/server.py\", line 612, in run_migrations\nserver-1         |     await db.create_db()\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/database/interface.py\", line 77, in create_db\nserver-1         |     await self.run_migrations_upgrade()\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/database/interface.py\", line 85, in run_migrations_upgrade\nserver-1         |     await run_sync_in_worker_thread(alembic_upgrade)\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/utilities/asyncutils.py\", line 233, in run_sync_in_worker_thread\nserver-1         |     result = await anyio.to_thread.run_sync(\nserver-1         |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |         call_with_mark, call, abandon_on_cancel=True, limiter=get_thread_limiter()\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     )\nserver-1         |     ^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\nserver-1         |     return await get_async_backend().run_sync_in_worker_thread(\nserver-1         |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |         func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     )\nserver-1         |     ^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\nserver-1         |     return await future\nserver-1         |            ^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\nserver-1         |     result = context.run(func, *args)\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/utilities/asyncutils.py\", line 243, in call_with_mark\nserver-1         |     return call()\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/database/alembic_commands.py\", line 36, in wrapper\nserver-1         |     return fn(*args, **kwargs)\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/database/alembic_commands.py\", line 72, in alembic_upgrade\nserver-1         |     alembic.command.upgrade(alembic_config(), revision, sql=dry_run)\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/command.py\", line 408, in upgrade\nserver-1         |     script.run_env()\nserver-1         |     ~~~~~~~~~~~~~~^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/script/base.py\", line 586, in run_env\nserver-1         |     util.load_python_file(self.dir, \"env.py\")\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/util/pyfiles.py\", line 95, in load_python_file\nserver-1         |     module = load_module_py(module_id, path)\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/util/pyfiles.py\", line 113, in load_module_py\nserver-1         |     spec.loader.exec_module(module)  # type: ignore\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\nserver-1         |   File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\nserver-1         |   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/database/_migrations/env.py\", line 201, in <module>\nserver-1         |     run_async_from_worker_thread(apply_migrations)\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/utilities/asyncutils.py\", line 254, in run_async_from_worker_thread\nserver-1         |     return anyio.from_thread.run(call)\nserver-1         |            ~~~~~~~~~~~~~~~~~~~~~^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/anyio/from_thread.py\", line 59, in run\nserver-1         |     return async_backend.run_async_from_thread(func, args, token=token)\nserver-1         |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2510, in run_async_from_thread\nserver-1         |     return f.result()\nserver-1         |            ~~~~~~~~^^\nserver-1         |   File \"/usr/local/lib/python3.13/concurrent/futures/_base.py\", line 456, in result\nserver-1         |     return self.__get_result()\nserver-1         |            ~~~~~~~~~~~~~~~~~^^\nserver-1         |   File \"/usr/local/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\nserver-1         |     raise self._exception\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2497, in task_wrapper\nserver-1         |     return await func(*args)\nserver-1         |            ^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/database/_migrations/env.py\", line 190, in apply_migrations\nserver-1         |     await connection.run_sync(do_run_migrations)\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/ext/asyncio/engine.py\", line 887, in run_sync\nserver-1         |     return await greenlet_spawn(\nserver-1         |            ^^^^^^^^^^^^^^^^^^^^^\nserver-1         |         fn, self._proxied, *arg, _require_await=False, **kw\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     )\nserver-1         |     ^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 203, in greenlet_spawn\nserver-1         |     result = context.switch(value)\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/database/_migrations/env.py\", line 159, in do_run_migrations\nserver-1         |     context.run_migrations()\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~^^\nserver-1         |   File \"<string>\", line 8, in run_migrations\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/runtime/environment.py\", line 946, in run_migrations\nserver-1         |     self.get_context().run_migrations(**kw)\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/runtime/migration.py\", line 623, in run_migrations\nserver-1         |     step.migration_fn(**kw)\nserver-1         |     ~~~~~~~~~~~~~~~~~^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/prefect/server/database/_migrations/versions/postgresql/2025_04_04_092158_7a73514ca2d6_add_ix_events__event_related_occurred_.py\", line 21, in upgrade\nserver-1         |     op.create_index(\nserver-1         |     ~~~~~~~~~~~~~~~^\nserver-1         |         \"ix_events__related_gin\",\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     ...<3 lines>...\nserver-1         |         postgresql_using=\"gin\",\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     )\nserver-1         |     ^\nserver-1         |   File \"<string>\", line 8, in create_index\nserver-1         |   File \"<string>\", line 3, in create_index\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/operations/ops.py\", line 998, in create_index\nserver-1         |     return operations.invoke(op)\nserver-1         |            ~~~~~~~~~~~~~~~~~^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/operations/base.py\", line 441, in invoke\nserver-1         |     return fn(self, operation)\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/operations/toimpl.py\", line 107, in create_index\nserver-1         |     operations.impl.create_index(idx, **kw)\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/ddl/postgresql.py\", line 98, in create_index\nserver-1         |     self._exec(CreateIndex(index, **kw))\nserver-1         |     ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/alembic/ddl/impl.py\", line 246, in _exec\nserver-1         |     return conn.execute(construct, params)\nserver-1         |            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1416, in execute\nserver-1         |     return meth(\nserver-1         |         self,\nserver-1         |         distilled_parameters,\nserver-1         |         execution_options or NO_OPTIONS,\nserver-1         |     )\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/sql/ddl.py\", line 187, in _execute_on_connection\nserver-1         |     return connection._execute_ddl(\nserver-1         |            ~~~~~~~~~~~~~~~~~~~~~~~^\nserver-1         |         self, distilled_params, execution_options\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     )\nserver-1         |     ^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1527, in _execute_ddl\nserver-1         |     ret = self._execute_context(\nserver-1         |         dialect,\nserver-1         |     ...<4 lines>...\nserver-1         |         compiled,\nserver-1         |     )\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1843, in _execute_context\nserver-1         |     return self._exec_single_context(\nserver-1         |            ~~~~~~~~~~~~~~~~~~~~~~~~~^\nserver-1         |         dialect, context, statement, parameters\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     )\nserver-1         |     ^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1983, in _exec_single_context\nserver-1         |     self._handle_dbapi_exception(\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\nserver-1         |         e, str_statement, effective_parameters, cursor, context\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     )\nserver-1         |     ^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 2355, in _handle_dbapi_exception\nserver-1         |     raise exc_info[1].with_traceback(exc_info[2])\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\nserver-1         |     self.dialect.do_execute(\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~~^\nserver-1         |         cursor, str_statement, effective_parameters, context\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     )\nserver-1         |     ^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\nserver-1         |     cursor.execute(statement, parameters)\nserver-1         |     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 580, in execute\nserver-1         |     self._adapt_connection.await_(\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\nserver-1         |         self._prepare_and_execute(operation, parameters)\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |     )\nserver-1         |     ^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\nserver-1         |     return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\nserver-1         |            ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\nserver-1         |     value = await result\nserver-1         |             ^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 558, in _prepare_and_execute\nserver-1         |     self._handle_exception(error)\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 508, in _handle_exception\nserver-1         |     self._adapt_connection._handle_exception(error)\nserver-1         |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 794, in _handle_exception\nserver-1         |     raise error\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 545, in _prepare_and_execute\nserver-1         |     self._rows = deque(await prepared_stmt.fetch(*parameters))\nserver-1         |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/asyncpg/prepared_stmt.py\", line 176, in fetch\nserver-1         |     data = await self.__bind_execute(args, 0, timeout)\nserver-1         |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/asyncpg/prepared_stmt.py\", line 267, in __bind_execute\nserver-1         |     data, status, _ = await self.__do_execute(\nserver-1         |                       ^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |         lambda protocol: protocol.bind_execute(\nserver-1         |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |             self._state, args, '', limit, True, timeout))\nserver-1         |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"/usr/local/lib/python3.13/site-packages/asyncpg/prepared_stmt.py\", line 256, in __do_execute\nserver-1         |     return await executor(protocol)\nserver-1         |            ^^^^^^^^^^^^^^^^^^^^^^^^\nserver-1         |   File \"asyncpg/protocol/protocol.pyx\", line 206, in bind_execute\nserver-1         | TimeoutError\nserver-1         |\nserver-1         | ERROR:    Application startup failed. Exiting.\nserver-1         | Server stopped!\n```\n\ndocker compose file\n```\nservices:\n  server:\n    image: prefecthq/prefect:3.4.5-python3.13\n    restart: unless-stopped\n    volumes:\n      - prefect:/root/.prefect\n    entrypoint: [\"prefect\", \"server\", \"start\", \"--host\", \"0.0.0.0\"]\n    environment:\n      - PREFECT_UI_API_URL=http://prefect.showpointlabs.com:4200/api\n      - PREFECT_LOGGING_SERVER_LEVEL=WARNING\n      - PREFECT_API_URL=http://127.0.0.1:4200/api\n      - PREFECT_API_DATABASE_CONNECTION_URL=${DATABASE_URL}\n      - PREFECT_API_DATABASE_ECHO=False\n      - PREFECT_API_DATABASE_MIGRATE_ON_START=True\n      - PREFECT_STORAGE_S3_BUCKET=my-minio-bucket\n      - PREFECT_STORAGE_S3_ACCESS_KEY_ID=secretuser\n      - PREFECT_STORAGE_S3_SECRET_ACCESS_KEY=secretkey\n      - PREFECT_STORAGE_S3_ENDPOINT_URL=http://minio:9000\n      - PREFECT_TASK_RUN_TAG_CONCURRENCY_SLOT_WAIT_SECONDS=3\n      - PREFECT_WORKER_QUERY_SECONDS=3\n      - PREFECT_WORKER_PREFETCH_SECONDS=5\n      - PREFECT_SERVER_DEPLOYMENTS_CONCURRENCY_SLOT_WAIT_SECONDS=3\n      - PREFECT_SERVER_TASKS_TAG_CONCURRENCY_SLOT_WAIT_SECONDS=10\n    ports:\n      - 4200:4200\n    depends_on:\n      - postgres\n      - minio\n  worker-docker:\n    restart: unless-stopped\n    build:\n      context: .\n      dockerfile: Dockerfile.docker-worker\n    entrypoint: [\"/opt/prefect/entrypoint.sh\", \"prefect\", \"worker\", \"start\", \"-p\", \"docker-pool\", \"--type\", \"docker\"]\n    environment:\n      - PREFECT_API_URL=http://server:4200/api\n    depends_on:\n      - server\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    scale: 5\n  minio:\n    image: minio/minio:latest\n    entrypoint: [\"minio\", \"server\", \"--address\", \"0.0.0.0:9000\", \"--console-address\", \"0.0.0.0:9001\", \"/data\"]\n    ports:\n      - 9000:9000\n      - 9001:9001\n    environment:\n      - MINIO_ROOT_USER=secretuser\n      - MINIO_ROOT_PASSWORD=secretkey\n    volumes:\n      - \"minio:/data\"\n  postgres:\n    image: postgres:16.4\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n      - ./backups:/backups\n    restart: unless-stopped\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: prefect\nvolumes:\n  prefect:\n  postgres_data:\n  minio:\n```\n\nbuilding these indexes takes awhile for me (about 60 seconds)  (my database is pushing 70 gigs, single postgresql instance without any optimizations)\n\nbut the migration times out with the error above\n\nI had to manually apply the indexes\n```\nDROP INDEX IF EXISTS ix_events__event_related_occurred;\nDROP INDEX IF EXISTS ix_events__related_resource_ids;\n\nCREATE INDEX CONCURRENTLY ix_events__related_gin\n  ON events USING gin(related);\n\nCREATE INDEX CONCURRENTLY ix_events__event_occurred\n  ON events (event, occurred);\n\nCREATE INDEX CONCURRENTLY ix_events__related_resource_ids_gin\n  ON events USING gin(related_resource_ids);\n```\n\nand then manually updated alembic\n```UPDATE alembic_version SET version_num = '7a73514ca2d6';```\n\nprobably other ways of getting around it, e.g. dumping the database, maybe more resources etc, but I thought I'd bring it up.\n\n\n\n(I went to 3.3.3 to 3.3.4, then on to 3.4.5)\n\n\n\n### Version info\n\n```Text\nVersion:             3.3.3\nAPI version:         0.8.4\nPython version:      3.13.2\nGit commit:          4100d4ea\nBuilt:               Sat, Apr 05, 2025 01:46 AM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.11.2\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @cayspekko, thanks for reporting this issue and providing the detailed workaround!\n\nthis is a known challenge when running migrations on large databases, i.e. the default database timeout of 10 seconds won't be sufficient for creating indexes on tables with millions of rows.\n\nfor large databases like yours, you'll probably need to increase the timeout before running migrations:\n\n```bash\n# Set timeout to 10 minutes (or higher as needed)\nenvironment:\n- PREFECT_API_DATABASE_TIMEOUT=600  # or PREFECT_SERVER_DATABASE_TIMEOUT\n```\n\nnote: this setting controls the client-side timeout (command_timeout in `asyncpg`). The index creation will continue on the db server even if the client times out, which is why you may see partially created indexes that need cleanup.\n\nI just added a docs PR (#18268) to help others who encounter the same issue, which includes:\n- how to prevent the timeout by increasing `PREFECT_API_DATABASE_TIMEOUT`\n- manual recovery steps (like the ones you used) if a migration does timeout\n- a note that large databases may need considerable time for some index operations\n\n\nlet me know if anything is not making sense or if you have any questions!"
      },
      {
        "user": "cayspekko",
        "body": "Thanks @zzstoatzz I was struggling to find some existing info. I figured a database timeout increase would work but couldn't figure out how to do it and didn't notice the DATABASE_TIMEOUT setting. Adding those hints to the docs is perfect! Thank you!"
      }
    ]
  },
  {
    "issue_number": 17727,
    "title": "Deleting worker from work pool deletes WRONG worker",
    "author": "LukasJerabek",
    "state": "closed",
    "created_at": "2025-04-04T09:06:33Z",
    "updated_at": "2025-06-10T20:25:28Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### Bug summary\n\nIn a work pool detail, you click on one worker, try to delete him, but then in the prompt the last one in table is the one that is being deleted, no matter which you clicked.\n\n![Image](https://github.com/user-attachments/assets/1fb15ec5-6fc7-4654-802e-bedb2c213003)\n\n\n### Version info\n\n```Text\nVersion:             3.2.14\nAPI version:         0.8.4\nPython version:      3.11.9\nGit commit:          efcde6dc\nBuilt:               Fri, Mar 21, 2025 5:28 PM\nOS/Arch:             win32/AMD64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.11.1\nServer:\n  Database:          sqlite\n  SQLite version:    3.45.1\nIntegrations:\n  prefect-email:     0.4.1\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zhen0",
        "body": "Hi @LukasJerabek - thanks for the issue. I started to look into this one but was not able to reproduce.  Is this still something you are seeing?"
      },
      {
        "user": "LukasJerabek",
        "body": "Hi, tested right now on version 3.3.7, still seeing that, didn't test on newer ones yet"
      },
      {
        "user": "zhen0",
        "body": "Hi @LukasJerabek - sorry for the slow reply here.  \n\nI think I found the issue you raised and https://github.com/PrefectHQ/prefect-ui-library/pull/3038 should close it.   \n\nThanks for raising it. üôè \n\n"
      }
    ]
  },
  {
    "issue_number": 18019,
    "title": "Deploy a deployment without overriding values that were declared in the UI",
    "author": "ChrisPaul33",
    "state": "open",
    "created_at": "2025-05-11T12:11:38Z",
    "updated_at": "2025-06-10T19:51:20Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nWhen updating a deployment, the parameters and schedules declared in the `prefect.yaml` are all overriding the current values.\nFor most cases it's fine.\nBut I have a case when there is an operation team that make changes in the UI only!\nOnce in a while, I need to update the image of the deployment, but I don't want to override any (or most) of the UI parameters, schedules and more. \n\n### Describe the proposed behavior\n\nThere are a few ideas I can think of.\n\n1. Add an extra flag named `--no-override-ui` (or similar) that overrides the data only if it is explicitly declared. It will keep the current behavior and only add that specific use case.\n2. Add a special key in the yaml itself that enables the same behavior.\n3. In the prefect.yaml, if the keys of `parameters`, `schedules`, etc. are not declared, it will keep the values declared in the UI. Then, if I don't want schedules at all, I explicitly declare it as an empty value (`{}` or `none`) in the yaml. I believe this is the best approach.\n\n### Example Use\n\nHow option 3 would look like:\n```\ndeployments:\n  - name: deployment-1\n    # Because \"schedules\" key is missing, it will keep the current values.\n    entrypoint: flows/hello.py:my_flow\n   # Overrides the parameters \"number\", \"message\"  and \"need_to_override\" from the UI. If there are other parameters, they keep the same value that was declared in the UI.\n    parameters: \n        number: 42,\n        message: Don't panic!\n        need_to_override: none\n    work_pool:\n        name: my-process-work-pool\n        work_queue_name: primary-queue\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "dadasilva",
        "body": "I feel your pain @ChrisPaul33, this is definitely an option worth including. I let the operations team make adjustments in the UI, but my CI doesn't always account for those adjustments and we can lose parameters/schedules without knowing. "
      }
    ]
  },
  {
    "issue_number": 16385,
    "title": "Update ECS Worker Guide worker policies",
    "author": "EmilRex",
    "state": "open",
    "created_at": "2024-12-13T20:45:04Z",
    "updated_at": "2025-06-10T16:47:13Z",
    "labels": [
      "docs"
    ],
    "body": "The ECS Worker Guide instructs users to assign the `arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy` policy to their worker ECSTask, but this role is insufficient for actually starting flow runs ECSTasks. The role should include the following permissions (also defined [here](https://github.com/PrefectHQ/prefect-recipes/blob/main/devops/infrastructure-as-code/aws/tf-prefect2-ecs-worker/main.tf#L101-L117)):\r\n\r\n```\r\n\"ec2:DescribeSubnets\",\r\n\"ec2:DescribeVpcs\",\r\n\"ecr:BatchCheckLayerAvailability\",\r\n\"ecr:BatchGetImage\",\r\n\"ecr:GetAuthorizationToken\",\r\n\"ecr:GetDownloadUrlForLayer\",\r\n\"ecs:DeregisterTaskDefinition\",\r\n\"ecs:DescribeTaskDefinition\",\r\n\"ecs:DescribeTasks\",\r\n\"ecs:RegisterTaskDefinition\",\r\n\"ecs:RunTask\",\r\n\"ecs:StopTask\",\r\n\"iam:PassRole\",\r\n\"logs:CreateLogGroup\",\r\n\"logs:CreateLogStream\",\r\n\"logs:GetLogEvents\",\r\n\"logs:PutLogEvents\"\r\n```",
    "comments": [
      {
        "user": "EmilRex",
        "body": "Also discovered that `ecs:TagResource` is necessary."
      },
      {
        "user": "peterbygrave",
        "body": "Thank you for posting this! Really helped me out today.\n\nI think the most up to date reference for policies is: https://github.com/PrefectHQ/terraform-prefect-ecs-worker/blob/main/main.tf#L106-L123"
      }
    ]
  },
  {
    "issue_number": 18006,
    "title": "Logs missing when using result_serializer with a GCS Block",
    "author": "essamik",
    "state": "open",
    "created_at": "2025-05-08T10:36:02Z",
    "updated_at": "2025-06-10T06:55:16Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen providing a GCS Block to my flow decorator `result_storage`, logs don't appear neither on my local IDE when running a deployment locally from the python main, neither on my self hosted prefect server when running a deployment on k8s directly.\n\nRemoving that `result_storage` parameter line fix the problems and logs are again visible.\n\n```python\n\n@flow(\n    name=\"flow-name\",\n    result_serializer=\"json\",\n    result_storage=\"gcs-bucket/gcs-prefect-block\", # Removing this line and logs show up again\n    persist_result=True\n)\ndef my_flow(day_delta: int = 30) -> None:\n```\n\nIt's weird that setting a result_storage creates a side effects on the logging, seems like a bug to me.\n\nSee image, there should be a lot more logs at task level.\n![Image](https://github.com/user-attachments/assets/1e061a51-0fc5-49c1-b8fb-3266ef2fdd65)\n\n### Version info\n\n```Text\nVersion:             3.4.0\nAPI version:         0.8.4\nPython version:      3.12.3\nGit commit:          c80e4442\nBuilt:               Fri, May 02, 2025 08:02 PM\nOS/Arch:             darwin/arm64\nProfile:             dev\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-gcp:       0.6.2\n  prefect-docker:    0.6.3\n  prefect-kubernetes: 0.5.9\n```\n\n### Additional context\n\nI checked the log table in the prefect DB and they also do not appear there.\nI'm using the `get_run_logger` helper to log in my tasks.\nLog level is correctly set to default value.",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @essamik - thanks for the issue. can you provide a complete MRE here? this does not appear to reproduce\n```python\nfrom prefect import flow, get_run_logger\n\n@flow(\n    name=\"flow-name\",\n    result_serializer=\"json\",\n    result_storage=\"gcs-bucket/gcs-prefect-block\",  # logs show regardless of this kwarg\n    persist_result=True,\n)\ndef my_flow() -> None:\n    logger = get_run_logger()\n    logger.info(\"Hello, world!\")\n\n\nif __name__ == \"__main__\":\n    my_flow()\n```\n\nie it seems there must be something more specific going on in your case where you don't see logs"
      },
      {
        "user": "essamik",
        "body": "Thanks for the reply, after some debugging I could narrow the issue to the usage of concurrency/parallelism (adding submit/result/wait to my tasks in my flow). I took the github star example from the Prefect tutorial (https://docs.prefect.io/v3/tutorials/pipelines#run-your-improved-flow) and added the GCS result storage.\n\n```\nfrom typing import Any\n\nimport httpx\nfrom prefect import flow, get_run_logger, task\n\n\n@task\ndef fetch_stats(github_repo: str) -> dict[str, Any]:\n    get_run_logger().info(\"fetch stats\") # Added a log here\n    return httpx.get(f\"https://api.github.com/repos/{github_repo}\").json()\n\n\n@task\ndef get_stars(repo_stats: dict[str, Any]) -> int:\n    get_run_logger().info(\"get_stars\") # Added a log here\n    return repo_stats[\"stargazers_count\"]\n\n@flow(\n    log_prints=True,\n    result_storage=\"gcs-bucket/gcs-prefect-block\" # the problematic line\n)\ndef show_stars(github_repos: list[str]) -> None:\n    stats_futures = fetch_stats.map(github_repos)\n    stars = get_stars.map(stats_futures).result()\n\n    for repo, star_count in zip(github_repos, stars):\n        print(f\"{repo}: {star_count} stars\")\n\n\n# Run the flow\nif __name__ == \"__main__\":\n    show_stars([\n        \"PrefectHQ/prefect\",\n        \"pydantic/pydantic\",\n        \"huggingface/transformers\"\n    ])\n```\n\nHere are the logs I get in my console output without the result_storage config:\n```\n12:59:43.784 | INFO    | Flow run 'tireless-ladybug' - Beginning flow run 'tireless-ladybug' for flow 'show-stars'\n12:59:43.788 | INFO    | Flow run 'tireless-ladybug' - View at http://prefect.dev.int.com/runs/flow-run/345b0e31-c2ff-42e8-8aea-4ce7cf321064\n12:59:44.897 | INFO    | Task run 'fetch_stats-7ac' - fetch stats\n12:59:44.897 | INFO    | Task run 'fetch_stats-ac4' - fetch stats\n12:59:44.899 | INFO    | Task run 'fetch_stats-a8f' - fetch stats\n12:59:45.130 | INFO    | Task run 'fetch_stats-7ac' - Finished in state Completed()\n12:59:45.137 | INFO    | Task run 'get_stars-b90' - get_stars\n12:59:45.139 | INFO    | Task run 'fetch_stats-ac4' - Finished in state Completed()\n12:59:45.140 | INFO    | Task run 'get_stars-b90' - Finished in state Completed()\n12:59:45.144 | INFO    | Task run 'get_stars-878' - get_stars\n12:59:45.145 | INFO    | Task run 'get_stars-878' - Finished in state Completed()\n12:59:45.307 | INFO    | Task run 'fetch_stats-a8f' - Finished in state Completed()\n12:59:45.314 | INFO    | Task run 'get_stars-db0' - get_stars\n12:59:45.315 | INFO    | Task run 'get_stars-db0' - Finished in state Completed()\n12:59:45.317 | INFO    | Flow run 'tireless-ladybug' - PrefectHQ/prefect: 19237 stars\n12:59:45.318 | INFO    | Flow run 'tireless-ladybug' - pydantic/pydantic: 23782 stars\n12:59:45.318 | INFO    | Flow run 'tireless-ladybug' - huggingface/transformers: 144157 stars\n12:59:45.777 | INFO    | Flow run 'tireless-ladybug' - Finished in state Completed()\n\nProcess finished with exit code 0\n```\n\nAnd here the same run with the result_storage set to GCS block:\n```\n12:58:36.071 | INFO    | Flow run 'soft-badger' - Beginning flow run 'soft-badger' for flow 'show-stars'\n12:58:36.099 | INFO    | Flow run 'soft-badger' - View at http://prefect.dev.int.com/runs/flow-run/8540ad76-c805-4266-9b75-ee43d1f5231f\n12:58:38.661 | INFO    | Task run 'fetch_stats-3fc' - fetch stats\n\nProcess finished with exit code 0\n```\n\nNo other side effects outside of logging, the flow gets executed correctly and create the desired output."
      },
      {
        "user": "mdturp",
        "body": "Experiencing similar problems with my setup"
      }
    ]
  },
  {
    "issue_number": 18258,
    "title": "Fatal Error config PREFECT_RUNNER_HEARTBEAT_FREQUENCY",
    "author": "jcppython",
    "state": "closed",
    "created_at": "2025-06-08T15:55:43Z",
    "updated_at": "2025-06-09T16:26:10Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nConfig Set Success\n```\nprefect config set PREFECT_RUNNER_HEARTBEAT_FREQUENCY=5\nSet 'PREFECT_RUNNER_HEARTBEAT_FREQUENCY' to '5'.\nUpdated profile 'default'.\n```\n\nConfig Show Error\n```\nprefect config view --show-defaults \n...\npydantic_core._pydantic_core.ValidationError: 1 validation error for RunnerSettings\nheartbeat_frequency\n  Input should be greater than or equal to 30 [type=greater_than_equal, input_value='5', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/greater_than_equal\n```\n\nConfig Set Error\n```\nprefect config set PREFECT_RUNNER_HEARTBEAT_FREQUENCY=40\n\n....\nheartbeat_frequency\n  Input should be greater than or equal to 30 [type=greater_than_equal, input_value='5', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/greater_than_equal\n```\n\n### Version info\n\n```Text\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:28 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.10.3\n```\n\n### Additional context\n\nWhen referring to Document [detect-zombie-flows](https://docs.prefect.io/v3/advanced/detect-zombie-flows) to solve Problem #18257 , I encountered the above issue.\n\n**After changing PREFECT_RUNNER_HEARTBEAT_FREQUENCY to 40, the flow did not report heartbeat events.**. It should be noted that I am using a process worker.\n\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @jcppython - thanks for the issue!\n\nhmm I think \"config set\" should have failed bc 5 < 30 and if it had, you wouldn't have encountered errors showing or setting again\n\nso I think we just need to make sure we fail right away if an invalid value is provided to `config set`\n"
      }
    ]
  },
  {
    "issue_number": 18104,
    "title": "Update Snowpark Container Services worker to handle changes and deprecations in API",
    "author": "bjorhn",
    "state": "closed",
    "created_at": "2025-05-19T06:46:55Z",
    "updated_at": "2025-06-09T16:06:39Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\n- The worker checks for status messages that are no longer returned from the API.\n- The worker uses method get_service_status() which is deprecated in version 1.5.0 of the Snowflake package.\n\n### Describe the proposed behavior\n\n- The worker should check for status messages that are actually in used by the API today.\n- The worker should replace usage of get_service_status() with get_containers(), which is what Snowflake recommends.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18151,
    "title": "Submitting many tasks simultaneously raises TaskRun AttributeError",
    "author": "rcash",
    "state": "closed",
    "created_at": "2025-05-22T21:48:36Z",
    "updated_at": "2025-06-09T15:10:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nInvoking a flow that submits multiple tasks within a short period of time to the `ThreadPoolTaskRunner`raises an `AttributeError: 'TaskRun' object has no attribute 'state'` in the flow where task submission seems to fail and the flow fails.\n\nThis error appears to happen intermittently - I have seen it on auto-scheduled runs while no other flows are running but have been able to (most) reliably reproduce it when starting a flow like below multiple times within a few seconds using a k8s work pool (potentially resource related?), where some of the invoked flow runs will complete and others will error out.\n\nstack trace:\n```\nEncountered exception during execution: AttributeError(\"'TaskRun' object has no attribute 'state'\")\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/prefect/task_engine.py\", line 1227, in initialize_run\n    validated_state=self.task_run.state,\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/pydantic/main.py\", line 989, in __getattr__\n    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\nAttributeError: 'TaskRun' object has no attribute 'state'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/prefect/flow_engine.py\", line 1332, in run_context\n    yield self\n  File \"/usr/local/lib/python3.12/site-packages/prefect/flow_engine.py\", line 1394, in run_flow_async\n    await engine.call_flow_fn()\n  File \"/usr/local/lib/python3.12/site-packages/prefect/flow_engine.py\", line 1346, in call_flow_fn\n    result = await call_with_parameters(self.flow.fn, self.parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/prefect/workdir/flows/async_repro.py\", line 16, in sleepy_flow\n    results = [t.result() for t in tasks]\n               ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/prefect/futures.py\", line 210, in result\n    future_result = self._wrapped_future.result(timeout=timeout)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/usr/local/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/prefect/task_engine.py\", line 1436, in run_task_async\n    async with engine.start(task_run_id=task_run_id, dependencies=dependencies):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/prefect/task_engine.py\", line 1294, in start\n    async with self.initialize_run(\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/prefect/task_engine.py\", line 1265, in initialize_run\n    self.log_finished_message()\n  File \"/usr/local/lib/python3.12/site-packages/prefect/task_engine.py\", line 257, in log_finished_message\n    display_state = repr(self.state) if PREFECT_DEBUG_MODE else str(self.state)\n                                                                    ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/prefect/task_engine.py\", line 142, in state\n    if not self.task_run or not self.task_run.state:\n                                ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/pydantic/main.py\", line 989, in __getattr__\n    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\nAttributeError: 'TaskRun' object has no attribute 'state'\n```\n\nmre:\n```python\nimport asyncio\n\nfrom prefect import flow, task\n\n\n@task\nasync def sleepy_task() -> None:\n    await asyncio.sleep(15)\n\n\n@flow(log_prints=True)\nasync def sleepy_flow() -> None:\n    tasks =[sleepy_task.submit() for _ in range(100)]\n    print(\"Waiting for all tasks to complete...\")\n\n    results = [t.result() for t in tasks]\n    print(f\"collected {len(results)} results\")\n```\n\n\n\n### Version info\n\n```Text\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             darwin/arm64\nProfile:             sbx-rowdy-cloud\nServer type:         cloud\nPydantic version:    2.11.5\nIntegrations:\n  prefect-kubernetes: 0.6.1\n  prefect-docker:    0.6.2\n```\n\n### Additional context\n\nHave seen this using the `3.4.1-python3.12-kubernetes` image for worker & flow containers. Screenshot of the task run count for one of the failed runs included (which you would expect, since it seems not all tasks could be created successfully).\n\n![Image](https://github.com/user-attachments/assets/2f6228ea-10ff-4129-bdc2-375b45ddc971)",
    "comments": [
      {
        "user": "psharrma",
        "body": "Any one else facing this issue? any workaround? We are also facing similar issue in production environment inconsistently.\nMay be some version change could help?"
      },
      {
        "user": "adithya-raviraja",
        "body": "We have faced similar problems in our solution as well, In our case we were using a local dask cluster to help with multiprocessing. At first I thought it could have been a limitation of the dask cluster itself, but this looks to be common issue. In our case, we are using prefect 3.4.3."
      },
      {
        "user": "GalLadislav",
        "body": "It occurred to us too using just ThreadPoolTaskRunner on a process worker. If i execute the mre just by calling the flow, the exception wont trigger."
      }
    ]
  },
  {
    "issue_number": 18257,
    "title": "Running State Misrepresentation and Unrecoverable Execution after Worker Failure",
    "author": "jcppython",
    "state": "closed",
    "created_at": "2025-06-08T15:02:02Z",
    "updated_at": "2025-06-08T15:30:00Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n<img width=\"1241\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2cdb305c-c1af-4ede-af11-d807972f8e12\" />\n\nWhen a flow is scheduled to be executed by a worker, and an exception occurs to the worker during the process (for example, the machine crashes. Here, we simulate this by killing the worker). It is found that: \n\n1) The flow appears to be \"Running\" in the UI, but in fact, it has stopped execution, and the exception is not detected. \n2) Even if the worker is restored, the flow cannot actually resume running. \n\nAdditionally, during this process, the worker pool can perceive the worker's status in a timely and accurate manner.\n\n\n### Version info\n\n```Text\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:28 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.10.3\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18083,
    "title": "prefect-dbt with dbt-snowlfake>=1.9.4 errors out",
    "author": "steveh-101",
    "state": "closed",
    "created_at": "2025-05-16T09:20:45Z",
    "updated_at": "2025-06-07T17:02:51Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI tried upgrading my dependencies which moved `dbt-snowflake` from 1.9.2 to 1.9.4\n\nHowever my `prefect-dbt==0.5.5` flows began to fail with:\n```\nin exception_handler\n    raise DbtRuntimeError(str(e)) from e\ndbt_common.exceptions.base.DbtRuntimeError: Runtime Error\n  Range too big. The sandbox blocks ranges larger than MAX_RANGE (100).\n```\n\nHowever, outside of Prefect, with the same version of dbt-snowflake, dbt itself works fine.\n\ndowngrading `dbt-snowflake` to 1.9.2 fixed the issue. \n\nI also tried `prefect-dbt[snowflake]` but this will install 1.9.4. and get the same error. \n\n\n\n### Version info\n\n```Text\nVersion:             2.20.18\nAPI version:         0.8.4\nPython version:      3.12.6\nGit commit:          b7059e89\nBuilt:               Wed, Apr 30, 2025 2:37 PM\nOS/Arch:             darwin/arm64\nProfile:             dev\nServer type:         cloud\n```\n\n### Additional context\n\nI am using `trigger_dbt_cli_command` to invoke dbt.",
    "comments": [
      {
        "user": "aaazzam",
        "body": "Here's an MRE. \n\n```python\n\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#   \"prefect==2.20.18\",\n#   \"prefect-dbt\",\n#   \"dbt-snowflake==1.9.4\",\n# ]\n# ///\n\nfrom pathlib import Path\nimport tempfile\nimport textwrap\n\nfrom prefect import flow\nfrom prefect_dbt.cli.commands import trigger_dbt_cli_command\n\n\n@flow\ndef reproduce_range_error():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        tmp = Path(tmpdir)\n\n        # Minimal dbt project configuration\n        (tmp / \"dbt_project.yml\").write_text(\n            textwrap.dedent(\n                \"\"\"\n                name: example\n                version: 1.0\n                profile: my-snowflake-db\n                \"\"\"\n            )\n        )\n\n        # Dummy Snowflake profile (parse does not require a real connection)\n        (tmp / \"profiles.yml\").write_text(\n            textwrap.dedent(\n                \"\"\"\n                my-snowflake-db:\n                  target: dev\n                  outputs:\n                    dev:\n                      type: snowflake\n                      account: fake\n                      user: fake\n                      password: fake\n                      database: fake\n                      warehouse: fake\n                      schema: public\n                \"\"\"\n            )\n        )\n\n        # Macro using a range larger than Prefect's sandbox limit\n        macros = tmp / \"macros\"\n        macros.mkdir()\n        macros.joinpath(\"large_range.sql\").write_text(\n            textwrap.dedent(\n                \"\"\"\n                {% macro large_range() %}\n                {% for i in range(101) %}\n                {{ i }}\n                {% endfor %}\n                {% endmacro %}\n                \"\"\"\n            )\n        )\n\n        # Model invoking the macro\n        models = tmp / \"models\"\n        models.mkdir()\n        models.joinpath(\"model.sql\").write_text(\"select '{{ large_range() }}' as numbers\")\n\n        # Running dbt through Prefect triggers the sandbox limit\n        trigger_dbt_cli_command(\n            command=\"dbt parse\",\n            project_dir=str(tmp),\n            profiles_dir=str(tmp),\n        )\n\n\nif __name__ == \"__main__\":\n    reproduce_range_error()\n```"
      },
      {
        "user": "aaazzam",
        "body": "The error originates from Prefect‚Äôs template sandbox configuration. When Prefect is imported, it sets a global Jinja2 limit that reduces the allowed range size for Jinja‚Äôs range() function. This happens in src/prefect/server/utilities/user_templates.py:\n\n```python\nlogger: \"logging.Logger\" = get_logger(__name__)\njinja2.sandbox.MAX_RANGE = 100\nMAX_LOOP_COUNT = 10\nMAX_NESTED_LOOP_DEPTH = 2\n```\n\nOur tests confirm that exceeding this limit triggers an error:\n\n```python\nassert rendered[0] == (\n    \"Failed to render template due to the following error: \"\n    \"OverflowError(\"\n    \"'Range too big. The sandbox blocks ranges larger than MAX_RANGE (100).'\"\n    \")\\n\"\n```\n\nBecause trigger_dbt_cli_command runs dbt within the Prefect process, the lowered MAX_RANGE setting also applies to dbt‚Äôs Jinja environment. dbt‚Äësnowflake 1.9.4 introduces macros that call range() with more than 100 items, so they fail under Prefect with ‚ÄúRange too big. The sandbox blocks ranges larger than MAX_RANGE (100).‚Äù Running dbt outside Prefect does not hit this limit since Jinja‚Äôs default MAX_RANGE is much higher (100000).\n\nI don't know how important it is for us to have such a low MAX_RANGE, @zzstoatzz do you know off hand? Can we bump this up willy nilly?"
      },
      {
        "user": "zzstoatzz",
        "body": "i'm not familiar with a reason this is set so low, i'd assume we can bump it"
      }
    ]
  },
  {
    "issue_number": 18094,
    "title": "Improve the performance of multiple deployments",
    "author": "steveh-101",
    "state": "open",
    "created_at": "2025-05-17T09:55:37Z",
    "updated_at": "2025-06-07T01:13:57Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nFor context: [issue 15428](https://github.com/PrefectHQ/prefect/issues/15428).\n\nWhen attempting to deploy multiple deployments at the same time, the process does not scale. This is because the external resource (whether a github repo or an s3 bucket, for example) will get repeated for each deployment. This only needs to happen once. \n\nRelated to that - there is no current way to specify a sub-directory of a GitRepository when cloning. In mono-repo setups all the code is downloaded when only a sub section of that is required. the `get_directory()` function looks like it could work here however that first clones the repo and then moves the folder. It would be more efficient to use a sparse checkout of the sub-directory.\n\n### Describe the proposed behavior\n\nWhen downloading external resources, use a cache. Repeated calls should then not re-download external resources. \n\nIf only a sub-path of the external resource is required, then only that should get downloaded.\n\n### Example Use\n\n```python\nfrom prefect import deploy\n\nfrom my_project.flows import example_flow as ex\n\nentrypoint = \"flows/example_flow.py:example_flow\"\n\n# deployment defaults defined elsewhere\nfrom utils.deployment_defaults import (\n    default_storage,\n    default_work_pool,\n    default_tags\n)\n\n# specifying all the n deployment params as required\ndefault_params = {}\nn_params = {}\nx_params = {}\n\n# this needs to be performant and perhaps more ergonomic\nif __name__ == \"__main__\":\n    deploy(\n         ex.example_flow.from_source(\n            source=default_storage, # repeating for all deployments of the same source/entrypoint\n            entrypoint=entrypoint, \n        ).to_deployment(\n            name=\"default\",\n            tags=[*default_tags],\n            parameters=default_params,\n            enforce_parameter_schema=True,\n        ),\n        ex.example_flow.from_source(\n            source=default_storage,\n            entrypoint=entrypoint,\n        ).to_deployment(\n            name=\"some_n_deployment\",\n            tags=[*default_tags],\n            parameters=n_params,\n            enforce_parameter_schema=True,\n        ),\n        ex.example_flow.from_source(\n            source=default_storage,\n            entrypoint=entrypoint,\n        ).to_deployment(\n            name=\"deployment_x\",\n            tags=[*default_tags],\n            parameters=x_params,\n            enforce_parameter_schema=True,\n        ),\n        work_pool_name=default_work_pool,\n        print_next_steps_message=False,\n    )\n```\n\n\n### Additional context\n\nIncidentally we have a `sparse_clone_repo` function in one of our flows which may be of use when designing a solution (incomplete code example):\n\n```python\nimport cotextlib\nimport os\nfrom prefect_shell import shell_run_command\nfrom pathlib import Path\n\nPROJECT_DIR = \"some_dir\"\n\n\n@contextlib.contextmanager\ndef working_directory(path: str):\n    \"\"\"Changes working directory then changes back to original on exit.\n\n    Args:\n        path (str): the path to change to.\n    \"\"\"\n    current = Path.cwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(current)\n\n\ndef sparse_clone_repo(sub_directory: str, branch: str, repo_url: str) -> str:\n    \"\"\"Sparsely clones a sub-directory from a GitHub repository.\"\"\"\n\n    command = (\n        \"git clone \"\n        f\"--branch {branch} \"\n        \"--depth 1 --sparse \"\n        \"--no-checkout \"\n        f\"{repo_url}; \"\n        f\"cd {repo} && \"\n        f\"git checkout {branch} && \" \n        f\"git sparse-checkout set {sub_directory}\"\n    )\n\n    with working_directory(PROJECT_DIR):\n        return shell_run_command.with_options(name=\"sparse_clone_repo\")(\n            command=command,\n            return_all=True,\n        )\n...\n```",
    "comments": [
      {
        "user": "steveh-101",
        "body": "What I'm considering is that now each deployment needs to be it's own file whilst this issue is outstanding. It will help in CI in that we now would only deploy changed deployments (insead of everything). It may also end up parallelizing the process in cases where there are multiple deployment changes for the same entrypoint. Feels like a workaround though."
      }
    ]
  },
  {
    "issue_number": 18140,
    "title": "Async retry_condition_fn in @task decorator creates type hinting problems",
    "author": "RFARQ3",
    "state": "closed",
    "created_at": "2025-05-22T05:33:22Z",
    "updated_at": "2025-06-06T17:30:27Z",
    "labels": [
      "good first issue",
      "typing"
    ],
    "body": "### Bug summary\n\nWhile async retry_condition_fn function successfully applies retry logic to async tasks, type hinting does not support Coroutine[bool] return type.\n\nWhen an async retry_condition_fn function is used in the @task decorator the following type hinting Pylance problems are created:\n\n```python \n\nfrom typing import Callable, cast\nfrom prefect import Task, task\nfrom prefect.client.schemas.objects import TaskRun, State\n\n\nasync def retry_handler(task: Task, task_run: TaskRun, state: State) -> bool:\n    \"\"\"Custom retry handler that specifies when to retry a task\"\"\"\n    try:\n        # Attempt to get the result of the task\n        await state.result()\n    except RuntimeError:\n        # Do not retry\n        return False\n    except:\n        # For any other exception, retry\n        return True\n    \n    return True\n\n@task(retry_condition_fn=retry_handler)\nasync def prefect_task_1():\n    pass\n\n\n# Manual casting attempted creates Pylance Problem #3\n@task(retry_condition_fn=cast(Callable[[Task, TaskRun, State], bool], retry_handler))\nasync def prefect_task_2():\n    pass\n\nasync def test_task():\n    await prefect_task_2()\n\n\n# Pylance Problem #1\n# --------------------------------------------------------------------\n# @task(retry_condition_fn=retry_handler)\n# --------------------------------------------------------------------\n# No overloads for \"task\" match the provided arguments\n\n\n# Pylance Problem #2\n# --------------------------------------------------------------------\n# @task(retry_condition_fn=retry_handler)\n# --------------------------------------------------------------------\n# Argument of type \"(task: Task[..., Unknown], task_run: TaskRun, state: State[Any]) -> CoroutineType[Any, Any, bool]\" cannot be assigned to parameter \"retry_condition_fn\" of type \"((Task[P@task, Any], TaskRun, State[Any]) -> bool) | None\" in function \"task\"\n# ¬†¬†Type \"(task: Task[..., Unknown], task_run: TaskRun, state: State[Any]) -> CoroutineType[Any, Any, bool]\" is not assignable to type \"((Task[P@task, Any], TaskRun, State[Any]) -> bool) | None\"\n# ¬†¬†¬†¬†\"function\" is not assignable to \"None\"\n# ¬†¬†¬†¬†Type \"(task: Task[..., Unknown], task_run: TaskRun, state: State[Any]) -> CoroutineType[Any, Any, bool]\" is not assignable to type \"(Task[P@task, Any], TaskRun, State[Any]) -> bool\"\n# ¬†¬†¬†¬†¬†¬†Function return type \"CoroutineType[Any, Any, bool]\" is incompatible with type \"bool\"\n# ¬†¬†¬†¬†¬†¬†¬†¬†\"CoroutineType[Any, Any, bool]\" is not assignable to \"bool\"\n\n\n# Pylance Problem #3\n# --------------------------------------------------------------------\n#     await prefect_task_2()\n# --------------------------------------------------------------------\n# \"None\" is not awaitable\n# ¬†¬†\"None\" is incompatible with protocol \"Awaitable[_T_co@Awaitable]\"\n# ¬†¬†¬†¬†\"__await__\" is not present\n\n```\n\n\n\n\n\n### Version info\n\n```Text\nVersion:             3.4.2\nAPI version:         0.8.4\nPython version:      3.11.9\nGit commit:          c3c1c119\nBuilt:               Mon, May 19, 2025 04:04 PM\nOS/Arch:             win32/AMD64\nProfile:             local\nServer type:         server\nPydantic version:    2.9.2\nIntegrations:\n  prefect-docker:    0.6.2\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "thanks for the report @RFARQ3 - we'll take a look!"
      },
      {
        "user": "zzstoatzz",
        "body": "@RFARQ3 this should be fixed the PR linked above and will be released later today in 3.4.5"
      }
    ]
  },
  {
    "issue_number": 17685,
    "title": "Proactive trigger \"within\" window not behaving as expected when part of compound automation",
    "author": "armalite",
    "state": "open",
    "created_at": "2025-04-01T20:34:59Z",
    "updated_at": "2025-06-05T18:03:26Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n**Issue Summary:**\n\nWe are seeing unexpected behaviour when using a **Proactive** trigger alongside a **Reactive** trigger in a compound automation. Specifically, the **Proactive trigger's `within` window (e.g. 600s)** appears to be **enforced longer than intended**, and/or its evaluation is **inconsistent** across repeated triggering scenarios.\n\n* * * * *\n\n**Context:**\n\nWe emit custom events (`xos.edo.event.received`) into the Prefect event feed when specific deployments complete. Each event contains a unique `eventSourceURN` that identifies the source deployment.\n\nWe then use a compound automation to:\n\n-   **IF** job A and job B have completed in the last 6 hours (Reactive triggers),\n\n-   **AND** job C has *not* completed in the last 10 minutes (Proactive trigger),\n\n-   **THEN** run job C (the automation action).\n\n‚úÖ The intent of the Proactive trigger is to **suppress the automation if job C has run recently**.\n\nSee image below showing how we are expecting the automation definition to translate to the above trigger conditions and actions.\n\n![Image](https://github.com/user-attachments/assets/36f932ba-6d75-4da8-8e39-99f0cf79e4e5)\n\n\n* * * * *\n\n**Automation Definition (simplified - contains only 1 upstream dependency):**\n\n```json\n\n{\n  \"type\": \"compound\",\n  \"triggers\": [\n    {\n      \"type\": \"compound\",\n      \"triggers\": [\n        {\n          \"type\": \"event\",\n          \"match\": { \"eventSourceURN\": \"urn:...:job:main-test\" },\n          \"expect\": [\"xos.edo.event.received\"],\n          \"posture\": \"Reactive\",\n          \"threshold\": 1,\n          \"within\": 0,\n          \"for_each\": []\n        }\n      ],\n      \"require\": \"all\",\n      \"within\": 21600\n    },\n    {\n      \"type\": \"event\",\n      \"match\": { \"eventSourceURN\": \"urn:...:job:edo-test-single-dependency\" },\n      \"expect\": [\"xos.edo.event.received\"],\n      \"posture\": \"Proactive\",\n      \"threshold\": 1,\n      \"within\": 600\n    }\n  ],\n  \"require\": \"all\",\n  \"within\": 600\n}\n```\n\n* * * * *\n\n**Observed Behaviour:**\n\n-   ‚úÖ **Initial trigger works** as expected: the automation fires and runs job C after job A completes.\n\n-   ‚úÖ **Subsequent triggers within the 10-minute Proactive window are correctly suppressed**.\n\n-   ‚ùå **Triggers beyond the 10-minute window sometimes do not fire**, even though all conditions are satisfied.\n\n-   ‚ùå **Delays** of 5--10+ minutes occur between the upstream Reactive event and the automation firing, even when the Proactive trigger window has expired.\n\n-   ‚ùå In some cases, it takes **20--30 minutes** before the automation retriggers, even though the Proactive `within` value is set to 600 seconds.\n\n* * * * *\n\n**Example Test Steps and Timeline:**\nWe see variation in behaviour in different testing scenarios, so we'll provide a couple examples below. Ultimately we still believe it is the proactive trigger's time bucket that is preventing the automation action being triggered, even when all criteria are met. Note there are no differences to the automation between these observation scenarios (i.e. the simplified definition shown above)\n\n**Observation Scenario 1**\n| Step | Upstream Job Run | Expected | Observed |\n| --- | --- | --- | --- |\n| 1 | Initial | Should run Job C | ‚úÖ Works |\n| 2 | Within 10m | Automation should be suppressed | ‚úÖ Suppressed |\n| 3 | After 10m | Should run Job C | ‚ùå Did **not** trigger |\n| 4 | Another run after 10m | Should run Job C | ‚úÖ ran but delayed |\n| 5 | After 10m from step 4 | Should run Job C | ‚ùå Did **not** trigger |\n| 6 | Waited and retried (>20m since last job C) | Should run Job C | ‚ùå Did **not** trigger |\n| 7 | Waited and retried (>30m since last job C run) | Should run Job C | ‚úÖ Job C ran |\n\n**Observation Scenario 2**\n| Step | Upstream Job Run | Expected | Observed |\n| --- | --- | --- | --- |\n| 1 | Initial | Should run Job C | ‚úÖ Works |\n| 2 | Within 10m | Should be suppressed | ‚úÖ Suppressed |\n| 3 | After 10m | Should run Job C | ‚úÖ But delayed |\n| 4 | Within new 10m | Should be suppressed | ‚úÖ Suppressed |\n| 5 | After 10m again | Should run Job C | ‚ùå Did **not** trigger |\n| 6 | Waited and retried | Should run Job C | ‚úÖ Finally triggered after further delay |\n\nBelow is a diagram showing expected behavior vs. actual behavior. It illustrates the behavior outlined in Observation 1.\n\n![Image](https://github.com/user-attachments/assets/b446113c-7716-440b-96a6-9a2112e6d537)\n\n* * * * *\n\n**Suspected Cause:**\n\n-   The Proactive trigger's `within` time may be evaluated using **time buckets** or other internal timing semantics that cause the actual enforcement window to **exceed the defined limit** (e.g. `within: 600`).\n\n-   This leads to **false negatives**, where events outside the intended window still prevent the automation.\n\n-   The delay in evaluation could also be due to **internal queueing or latency** in the automation processing engine, but the alignment with the Proactive window hints at a possible **implementation bug** in how Proactive triggers are evaluated.\n\n* * * * *\n\n**Expected Behaviour:**\n\n-   If a Proactive trigger is configured with `within: 600`, it should only suppress the automation if a matching event occurred **within the last 600 seconds** from the evaluation time.\n\n-   Beyond that, the trigger should be considered **satisfied**, and the automation should fire as long as all other conditions are met.\n\nNote: The custom events are emitted whenever a flow-run of our deployments complete, and this is true regardless of how these flow-runs are started (i.e. via automation, UI, API, schedule etc.). We still expect these events to be evaluated as per the automation definition\n\n* * * * *\n\nLet me know if there's any additional debug output, logs, or experiments I can run to help reproduce or isolate this further. Thanks!\n\n### Version info\n\n```Text\nVersion:             2.20.16\nAPI version:         0.8.4\nPython version:      3.11.5\nGit commit:          b5047953\nBuilt:               Thu, Dec 19, 2024 10:55 AM\nOS/Arch:             darwin/x86_64\nProfile:             default\nServer type:         cloud\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "chrisguidry",
        "body": "Hi @armalite, we've released some reliability improvements to Prefect Cloud over the last few weeks that should have improved this situation, and I have included a permanent set of regression tests to our test suite for this case (see below):\n\n\n```python\n@pytest.fixture\nasync def xero_automation(\n    cleared_buckets: None,\n    automations_session: AsyncSession,\n    account: UUID,\n    workspace: UUID,\n) -> Automation:\n    automation = Automation(\n        account=account,\n        workspace=workspace,\n        name=\"Xero Suppression Trigger\",\n        actions=[DoNothing()],\n        trigger=CompoundTrigger(\n            id=UUID(\"eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee\"),\n            require=\"all\",\n            within=timedelta(minutes=10),\n            triggers=[\n                # This trigger represents one or more events that would normally trigger\n                # this automation...\n                CompoundTrigger(\n                    id=UUID(\"dddddddd-dddd-dddd-dddd-dddddddddddd\"),\n                    require=\"all\",\n                    within=timedelta(minutes=10),\n                    triggers=[\n                        EventTrigger(\n                            id=UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\"),\n                            posture=Posture.Reactive,\n                            expect=[\"A\"],\n                            within=timedelta(0),\n                        ),\n                        EventTrigger(\n                            id=UUID(\"bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb\"),\n                            posture=Posture.Reactive,\n                            expect=[\"B\"],\n                            within=timedelta(0),\n                        ),\n                    ],\n                ),\n                # ...unless this event has already happened in the last 10 minutes\n                EventTrigger(\n                    id=UUID(\"cccccccc-cccc-cccc-cccc-cccccccccccc\"),\n                    posture=Posture.Proactive,\n                    expect=[\"C\"],\n                    within=timedelta(minutes=10),\n                ),\n            ],\n        ),\n    )\n\n    persisted = await automations.create_automation(automations_session, automation)\n    automation.created = persisted.created\n    automation.updated = persisted.updated\n    await automations_session.commit()\n\n    load_automation(persisted)\n    return automation\n\n\n@pytest.fixture\ndef proactive_trigger(xero_automation: Automation) -> EventTrigger:\n    compound = xero_automation.trigger\n    assert isinstance(compound, CompoundTrigger)\n\n    proactive = compound.triggers[1]\n    assert isinstance(proactive, EventTrigger)\n    assert proactive.posture == Posture.Proactive\n\n    return proactive\n\n\nasync def test_xero_automation_fires_when_it_should(\n    xero_automation: Automation,\n    start_of_test: DateTime,\n    act: mock.AsyncMock,\n    proactive_trigger: EventTrigger,\n):\n    base_time = start_of_test\n\n    assert isinstance(proactive_trigger, EventTrigger)\n    assert proactive_trigger.posture == Posture.Proactive\n\n    def event(name: str, time: timedelta) -> ReceivedEvent:\n        return Event(\n            occurred=base_time + time,\n            event=name,\n            resource=Resource({\"prefect.resource.id\": \"resource-1\"}),\n            id=uuid.uuid4(),\n        ).receive(\n            account=xero_automation.account,\n            workspace=xero_automation.workspace,\n        )\n\n    for i in range(5):\n        act.reset_mock()\n        base_time += timedelta(minutes=i * 11)\n\n        await proactive_evaluation(proactive_trigger, base_time)\n        act.assert_not_awaited()\n\n        await reactive_evaluation(event(\"A\", timedelta(minutes=3)))\n        # Event A is not sufficient to fire the automation\n        act.assert_not_awaited()\n\n        await proactive_evaluation(proactive_trigger, base_time)\n        # We haven't seen a \"B\" event yet, so the automation should not fire\n        act.assert_not_awaited()\n\n        await reactive_evaluation(event(\"B\", timedelta(minutes=4)))\n        await proactive_evaluation(proactive_trigger, base_time + timedelta(minutes=10))\n\n        # Here, we should have fired at least once because either the \"B\" event or the\n        # knowledge that we haven't seen a \"C\" even in the last 10 minutes\n        act.assert_awaited_once()\n\n\nasync def test_xero_automation_is_suppressed_when_it_should_be(\n    xero_automation: Automation,\n    start_of_test: DateTime,\n    act: mock.AsyncMock,\n    proactive_trigger: EventTrigger,\n):\n    base_time = start_of_test\n\n    def event(name: str, time: timedelta) -> ReceivedEvent:\n        return Event(\n            occurred=base_time + time,\n            event=name,\n            resource=Resource({\"prefect.resource.id\": \"resource-1\"}),\n            id=uuid.uuid4(),\n        ).receive(\n            account=xero_automation.account,\n            workspace=xero_automation.workspace,\n        )\n\n    for i in range(5):\n        act.reset_mock()\n        base_time += timedelta(minutes=i * 11)\n\n        await proactive_evaluation(proactive_trigger, base_time)\n        act.assert_not_awaited()\n\n        await reactive_evaluation(event(\"A\", timedelta(minutes=3)))\n        # Event A is not sufficient to fire the automation\n        act.assert_not_awaited()\n\n        await proactive_evaluation(proactive_trigger, base_time)\n        # We haven't seen a \"B\" event yet, so the automation should not fire\n        act.assert_not_awaited()\n\n        # Now we see a \"C\" event, so the automation should be suppressed for the\n        # duration of the window\n        await reactive_evaluation(event(\"C\", timedelta(minutes=4)))\n\n        await reactive_evaluation(event(\"B\", timedelta(minutes=5)))\n        await proactive_evaluation(proactive_trigger, base_time + timedelta(minutes=10))\n\n        # Here, we should not have fired because even thought we have a \"B\" event, we\n        # have also seen a \"C\" event in the last 10 minutes\n        act.assert_not_awaited()\n```\n\nDoes this reproduce the issue you were seeing effectively?"
      },
      {
        "user": "armalite",
        "body": "Hey @chrisguidry thanks for the response and for sharing the regression tests. They‚Äôre helpful and seem to validate the core functionality of Proactive and Reactive trigger interactions.\n\nThat said, I think one important edge case is still untested: whether the Proactive trigger ever suppresses automation for longer than the configured within period.\n\nIn my original issue, one of the key observations was that the Proactive trigger appeared to continue suppressing the automation even after the cooldown window (e.g. 600 seconds) had passed, sometimes requiring 20‚Äì30 minutes before the automation fired again.\n\nTo ensure this case is covered, a test like the following might cover that scenario (feel free to update accordingly, as I've not tested this):\n\n```python\nasync def test_proactive_suppression_does_not_exceed_within_window(\n    xero_automation: Automation,\n    start_of_test: DateTime,\n    act: mock.AsyncMock,\n    proactive_trigger: EventTrigger,\n):\n    base_time = start_of_test\n\n    def event(name: str, time: timedelta) -> ReceivedEvent:\n        return Event(\n            occurred=base_time + time,\n            event=name,\n            resource=Resource({\"prefect.resource.id\": \"resource-1\"}),\n            id=uuid.uuid4(),\n        ).receive(\n            account=xero_automation.account,\n            workspace=xero_automation.workspace,\n        )\n\n    act.reset_mock()\n\n    # Proactive event \"C\" at time = 0\n    await reactive_evaluation(event(\"C\", timedelta(seconds=0)))\n    await proactive_evaluation(proactive_trigger, base_time)\n\n    # Advance time just beyond the 10-minute suppression window\n    base_time += timedelta(seconds=601)\n\n    # Reactive events \"A\" and \"B\" arrive after the suppression window has expired\n    await reactive_evaluation(event(\"A\", timedelta(seconds=601)))\n    await reactive_evaluation(event(\"B\", timedelta(seconds=602)))\n\n    await proactive_evaluation(proactive_trigger, base_time)\n\n    # Assert that the automation is no longer suppressed\n    act.assert_awaited_once()\n\n```"
      },
      {
        "user": "chrisguidry",
        "body": "Wonderful, I'll get that into our test suite as well.  These are currently going into the test suite for Prefect Cloud, but I'll also backport them to OSS for visibility (without the Xero references)"
      }
    ]
  },
  {
    "issue_number": 15826,
    "title": "retry_condition_fn doesn't work on Failed state",
    "author": "ihor-ramskyi-globallogic",
    "state": "closed",
    "created_at": "2024-10-28T14:56:15Z",
    "updated_at": "2025-06-05T18:01:20Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nFrom def task docstring:\r\n```\r\nretry_condition_fn: An optional callable run when a task run returns a Failed state. Should\r\n    return `True` if the task should continue to its retry policy (e.g. `retries=3`), and `False` if the task\r\n    should end as failed. Defaults to `None`, indicating the task should always continue\r\n    to its retry policy.\r\n```\r\n\r\nWhen I return Failed state instead of raising error, it doesn't trigger. Minimal reproducible example:\r\n```python\r\nfrom prefect import flow, task, get_run_logger\r\nfrom prefect.states import Failed\r\n\r\n\r\ndef cond(task, task_run, state) -> bool:\r\n    try:\r\n        state.result()\r\n    except:\r\n        return \"please_retry\" in state.message\r\n\r\n\r\n@task(retries=1, retry_condition_fn=cond)\r\ndef task_a():\r\n    logger = get_run_logger()\r\n    logger.info(\"Inside task_a\")\r\n    return Failed(message=\"please_retry\")\r\n\r\n\r\n@task(retries=1, retry_condition_fn=cond)\r\ndef task_b():\r\n    logger = get_run_logger()\r\n    logger.info(\"Inside task_b\")\r\n    raise Exception(\"please_retry\")\r\n\r\n\r\n@flow()\r\ndef generic_flow():\r\n    task_a()\r\n    task_b()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    generic_flow()\r\n\r\n```\r\n\r\nTraceback:\r\n<details>\r\n<summary>Logs and traceback:</summary>\r\n\r\n```\r\n\r\n16:34:49.831 | INFO    | prefect.engine - Created flow run 'warm-moth' for flow 'generic-flow'\r\n16:34:49.835 | INFO    | prefect.engine - View at http://127.0.0.1:4200/runs/flow-run/113d3b35-c54f-4d83-9b7e-a91a6818380b\r\n16:34:49.981 | INFO    | Task run 'task_a-4ee' - Inside task_a\r\n16:34:49.990 | ERROR   | Task run 'task_a-4ee' - Finished in state Failed('please_retry')\r\n16:34:50.009 | INFO    | Task run 'task_b-53a' - Inside task_b\r\n16:34:50.010 | INFO    | Task run 'task_b-53a' - Task run failed with exception: Exception('please_retry') - Retry 1/1 will start immediately\r\n16:34:50.016 | INFO    | Task run 'task_b-53a' - Inside task_b\r\n16:34:50.017 | ERROR   | Task run 'task_b-53a' - Task run failed with exception: Exception('please_retry') - Retries are exhausted\r\nTraceback (most recent call last):\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 768, in run_context\r\n    yield self\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 1318, in run_task_sync\r\n    engine.call_task_fn(txn)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 791, in call_task_fn\r\n    result = call_with_parameters(self.task.fn, parameters)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\utilities\\callables.py\", line 206, in call_with_parameters\r\n    return fn(*args, **kwargs)\r\n  File \"My_Path\\test_retries.py\", line 24, in task_b\r\n    raise Exception(\"please_retry\")\r\nException: please_retry\r\n16:34:50.028 | ERROR   | Task run 'task_b-53a' - Finished in state Failed('Task run encountered an exception Exception: please_retry')\r\n16:34:50.029 | ERROR   | Flow run 'warm-moth' - Encountered exception during execution: Exception('please_retry')\r\nTraceback (most recent call last):\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flow_engine.py\", line 655, in run_context\r\n    yield self\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flow_engine.py\", line 699, in run_flow_sync\r\n    engine.call_flow_fn()\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flow_engine.py\", line 678, in call_flow_fn\r\n    result = call_with_parameters(self.flow.fn, self.parameters)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\utilities\\callables.py\", line 206, in call_with_parameters\r\n    return fn(*args, **kwargs)\r\n  File \"My_Path\\test_retries.py\", line 30, in generic_flow\r\n    task_b()\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\tasks.py\", line 1002, in __call__\r\n    return run_task(\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 1507, in run_task\r\n    return run_task_sync(**kwargs)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 1320, in run_task_sync\r\n    return engine.state if return_type == \"state\" else engine.result()\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 475, in result\r\n    raise self._raised\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 768, in run_context\r\n    yield self\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 1318, in run_task_sync\r\n    engine.call_task_fn(txn)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 791, in call_task_fn\r\n    result = call_with_parameters(self.task.fn, parameters)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\utilities\\callables.py\", line 206, in call_with_parameters\r\n    return fn(*args, **kwargs)\r\n  File \"My_Path\\test_retries.py\", line 24, in task_b\r\n    raise Exception(\"please_retry\")\r\nException: please_retry\r\n16:34:50.089 | ERROR   | Flow run 'warm-moth' - Finished in state Failed('Flow run encountered an exception: Exception: please_retry')\r\nTraceback (most recent call last):\r\n  File \"My_Path\\test_retries.py\", line 34, in <module>\r\n    generic_flow()\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flows.py\", line 1355, in __call__\r\n    return run_flow(\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flow_engine.py\", line 821, in run_flow\r\n    return run_flow_sync(**kwargs)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flow_engine.py\", line 701, in run_flow_sync\r\n    return engine.state if return_type == \"state\" else engine.result()\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flow_engine.py\", line 255, in result\r\n    raise self._raised\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flow_engine.py\", line 655, in run_context\r\n    yield self\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flow_engine.py\", line 699, in run_flow_sync\r\n    engine.call_flow_fn()\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\flow_engine.py\", line 678, in call_flow_fn\r\n    result = call_with_parameters(self.flow.fn, self.parameters)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\utilities\\callables.py\", line 206, in call_with_parameters\r\n    return fn(*args, **kwargs)\r\n  File \"My_Path\\test_retries.py\", line 30, in generic_flow\r\n    task_b()\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\tasks.py\", line 1002, in __call__\r\n    return run_task(\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 1507, in run_task\r\n    return run_task_sync(**kwargs)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 1320, in run_task_sync\r\n    return engine.state if return_type == \"state\" else engine.result()\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 475, in result\r\n    raise self._raised\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 768, in run_context\r\n    yield self\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 1318, in run_task_sync\r\n    engine.call_task_fn(txn)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\task_engine.py\", line 791, in call_task_fn\r\n    result = call_with_parameters(self.task.fn, parameters)\r\n  File \"My_Path\\venv\\lib\\site-packages\\prefect\\utilities\\callables.py\", line 206, in call_with_parameters\r\n    return fn(*args, **kwargs)\r\n  File \"My_Path\\test_retries.py\", line 24, in task_b\r\n    raise Exception(\"please_retry\")\r\nException: please_retry\r\n\r\n```\r\n\r\n</details>\n\n### Version info\n\n```Text\nVersion:             3.0.11\r\nAPI version:         0.8.4\r\nPython version:      3.10.11\r\nGit commit:          a17ccfcf\r\nBuilt:               Thu, Oct 24, 2024 5:36 PM\r\nOS/Arch:             win32/AMD64\r\nProfile:             default\r\nServer type:         server\r\nPydantic version:    2.7.1\n```\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "thank you for the issue!\r\n\r\nThe issue makes a lot of sense given the wording of the docstring\r\n\r\n> An optional callable run when a task run returns a Failed state\r\n\r\nbut I think there's an argument (as suggested by @desertaxle) to be made that returning `Failed` as a literal value from tasks would be a shortcut for forcing a task to fail - returning `State` objects as literal values is a bit of an escape hatch as is.\r\n\r\nIf you don't mind @ihor-ramskyi-globallogic - can you explain a bit about the motivation for retrying tasks where you explicitly return a `Failed` state instead of letting an exception raise?"
      },
      {
        "user": "ihor-ramskyi-globallogic",
        "body": "Thank you for the reply, @zzstoatzz !\r\nWe want visualizer to mark task red when it worked without failure, but data returned is wrong. It made a lot of sense at the time it was first implemented to return Failed state with this incorrect data, possibly continuing with the flow further, using `task_name.submit().result(raise_on_failure=False)` and mark task failed for further investigation, not disrupting the work of flow itself. So we agreed on convention to return Failed state from try/except instead of letting the error raise."
      },
      {
        "user": "ihor-ramskyi-globallogic",
        "body": "@zzstoatzz is there an option for this bug to be fixed in foreseeable future? It interrupts our upgrade from Prefect2."
      }
    ]
  },
  {
    "issue_number": 18228,
    "title": "Docker `volumes` not parsed when provided via `job_variables` in a `deploy` call.",
    "author": "PontyPandySam",
    "state": "closed",
    "created_at": "2025-06-04T00:52:48Z",
    "updated_at": "2025-06-05T15:56:19Z",
    "labels": [],
    "body": "### Bug summary\n\nThere seems to be a missing interface for providing a list of volumes via the `flow.deploy` interface. In `flows.py` the only way to provide `volumes` is via `job_variables`:\n\nhttps://github.com/PrefectHQ/prefect/blob/81274e086cdb2f297de773f8345e48833bc938b8/src/prefect/flows.py#L203-L227\n\nBut in the container build code is the following:\n\nhttps://github.com/PrefectHQ/prefect/blob/81274e086cdb2f297de773f8345e48833bc938b8/src/integrations/prefect-docker/prefect_docker/worker.py#L665-L669\n\nAs `volumes` exists as an attribute on the configuration model it is not parsed when provided via a `container_create_kwargs` field within the `job_variables`?\n\nhttps://github.com/PrefectHQ/prefect/blob/81274e086cdb2f297de773f8345e48833bc938b8/src/integrations/prefect-docker/prefect_docker/worker.py#L155-L159\n\nUnless I have missed another way to provide this it doesn't appear possible to add a volume definition at deployment time?\n\n### Version info\n\n```Text\nVersion:             3.4.3\nAPI version:         0.8.4\nPython version:      3.12.3\nGit commit:          1c2ba7a4\nBuilt:               Thu, May 22, 2025 10:00 PM\nOS/Arch:             linux/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.5\nIntegrations:\n  prefect-docker:    0.6.5\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "SpinoPi",
        "body": "I was also confused by volumes within `container_create_kwargs`. I found that you can define `volumes` directly in `job_variables` like this: job_variables={\"volumes\": [\"pathA:pathB\"]}. This approach worked for me."
      },
      {
        "user": "zzstoatzz",
        "body": "thanks for the issue @PontyPandySam \n\n> I was also confused by volumes within container_create_kwargs. I found that you can define volumes directly in job_variables like this: job_variables={\"volumes\": [\"pathA:pathB\"]}\n\nyes @SpinoPi (thanks!) this is the intended usage - I will get some docs in for this. we could potentially log a warning if you pass `volumes` to `container_create_kwargs` that we're ignoring it since job variable is the intended interface"
      },
      {
        "user": "PontyPandySam",
        "body": "Thanks for the clarification and for planning to include both the warning and updated documentation! That'll definitely help avoid confusion for others down the line. Much appreciated üôå"
      }
    ]
  },
  {
    "issue_number": 18210,
    "title": "Issue with deploy",
    "author": "Bryan-Meier",
    "state": "closed",
    "created_at": "2025-05-30T22:07:05Z",
    "updated_at": "2025-06-04T22:30:31Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWe are using the PrefectHQ/actions-prefect-deploy@v4 GitHub action for our deployments. In looking through that code it looks like it uses a Prefect dependency of `prefect>=3,<4` and I can see that it's installing 3.4.4 at the time of deployment. There appears to be a breaking change in this latest version of Prefect as far deployments go which is causing all our deployments going through our CI/CD process to fail. I create the following issue ([PrefectHQ/actions-prefect-deploy@v4](https://github.com/PrefectHQ/actions-prefect-deploy/issues/65)) in the actions repo which doesn't seem like there is much attention on that repo and therefore pointing out the issues here in case this can be handled outside the action and here in the Prefect repo.\n\nThe error that is occurring is a Prefect error and not necessarily related to the action itself. Here is the error we are presented with:\n\n```\nInstalls a bunch of dependencies...\n....\n...\nUsing cached prefect-3.4.4-py3-none-any.whl (6.0 MB)\nInstalling collected packages: prefect\n  Attempting uninstall: prefect\n    Found existing installation: prefect 3.2.15\n    Uninstalling prefect-3.2.15:\n      Successfully uninstalled prefect-3.2.15\nSuccessfully installed prefect-3.4.4\nTraceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.11.12/x64/lib/python3.11/site-packages/prefect/cli/_utilities.py\", line 44, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.12/x64/lib/python3.11/site-packages/prefect/cli/_types.py\", line 156, in sync_fn\n    return asyncio.run(async_fn(*args, **kwargs))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.12/x64/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.12/x64/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.12/x64/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.12/x64/lib/python3.11/site-packages/prefect/cli/deploy.py\", line 483, in deploy\n    await _run_single_deploy(\n  File \"/opt/hostedtoolcache/Python/3.11.12/x64/lib/python3.11/site-packages/prefect/cli/deploy.py\", line 505, in _run_single_deploy\n    deploy_config = _merge_with_default_deploy_config(deploy_config)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.12/x64/lib/python3.11/site-packages/prefect/cli/deploy.py\", line 1013, in _merge_with_default_deploy_config\n    if k not in deploy_config[key]:\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: argument of type 'NoneType' is not iterable\nAn exception occurred.\n```\n\n### Version info\n\n```Text\nI don't have the exact out because it's running in an action but here is the details:\n\nVersion: 3.4.4\nPython version: 3.11.7\nOS: ubuntu-22.04\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @Bryan-Meier thanks for the issue! can you share the `prefect.yaml` that this breaks on? ideally a minimal reproducible example"
      },
      {
        "user": "Bryan-Meier",
        "body": "@zzstoatzz Below is my prefect.yaml. I was able to include 99% of it since we have parameterized most of it. Please let me know if I can provide more. Thank you for the help in advance!\n\n```\nname: some_name\nprefect-version: 2.14.4\n\nbuild:\n\n\npush:\n  - prefect_docker.deployments.steps.push_docker_image:\n      requires: prefect-docker\n      image_name: \"{{ build-image.image_name }}\"\n      tag: \"{{ build-image.tag }}\"\n\n\npull:\n\n\ndefinitions:\n  docker_build:\n    - prefect_docker.deployments.steps.build_docker_image: &docker_build_config\n        id: build-image\n        requires: prefect-docker>=0.6.0\n        image_name: \"\"\n        tag: \"\"\n        dockerfile: DockerFile\n        platform: \"linux/amd64\"\n  git_clone:\n    - prefect.deployments.steps.git_clone: &git_clone\n        id: clone-step\n        repository: \"\"\n        branch: \"\"\n        credentials: \"{{ prefect.blocks.github-credentials.github-creds }}\"\n  default_pull: &default_pull\n    - prefect.deployments.steps.git_clone:\n        <<: *git_clone\n        repository: \"{{ get-git-repo-url.stdout }}\"\n        branch: \"{{ get-git-repo-branch.stdout }}\"\n  default_build: &default_build\n    - prefect.deployments.steps.run_shell_script:\n        id: get-github-token\n        script: echo $GITHUB_TOKEN\n        stream_output: false\n        expand_env_vars: true\n\n    - prefect.deployments.steps.run_shell_script:\n        id: get-git-repo-url\n        script: echo $GIT_REPO_URL\n        stream_output: false\n        expand_env_vars: true\n\n    - prefect.deployments.steps.run_shell_script:\n        id: get-git-repo-name\n        script: echo $GIT_REPO_NAME\n        stream_output: false\n        expand_env_vars: true\n\n    - prefect.deployments.steps.run_shell_script:\n        id: get-git-repo-branch\n        script: echo $GIT_REPO_BRANCH\n        stream_output: false\n        expand_env_vars: true\n\n    - prefect.deployments.steps.run_shell_script:\n        id: get-ecr-repo-name\n        script: echo $ECR_REPO_NAME\n        stream_output: false\n        expand_env_vars: true\n\n    - prefect_docker.deployments.steps.build_docker_image:\n        <<: *docker_build_config\n        image_name: \"{{ get-ecr-repo-name.stdout }}\"\n        tag: \"{{ get-git-repo-branch.stdout }}\"\n        buildargs:\n          GITHUB_TOKEN: \"{{ get-github-token.stdout }}\"\n          OS_ENV: \"{{ get-git-repo-branch.stdout }}\"\n\n\ndeployments:\n  - name: \"prd-{{ get-git-repo-name.stdout }}-someflow\"\n    version:\n    tags: [\"environment:{{ get-git-repo-branch.stdout }}\"]\n    if: '{{ get-git-repo-branch.stdout }} == \"prd\"'\n    description: \"Some description\"\n    schedule: \n      cron: \"0 0 * * *\"\n      timezone: \"America/Los_Angeles\"\n      active: false \n    entrypoint: \"main.py:someflow\"\n    parameters: {}\n    pull:\n      *default_pull\n    build:\n      *default_build\n    work_pool:\n```"
      },
      {
        "user": "Bryan-Meier",
        "body": "@zzstoatzz I figured out the issue. In the deployments section of the prefect.yml, the workpool is empty. I didn't catch this when I copied the contents from another one. I merely stumbled upon this as I was looking at our other projects.\n\nThe error that is thrown doesn't indicate anything about the workpool configuration being an issue. This isn't meant to be negative but rather helpful to others using the product. I don't know if there is another issue out there related to better error descriptions and handling but feel free to add this one to that list if there is one. If I would have had any indication that the issue was a workpool, I could have fixed this in a few minutes instead of days."
      }
    ]
  },
  {
    "issue_number": 18227,
    "title": "Setting a bucket folder on GCS Bucket block breaks task caching",
    "author": "jmesterh",
    "state": "closed",
    "created_at": "2025-06-03T23:45:57Z",
    "updated_at": "2025-06-04T20:18:03Z",
    "labels": [
      "bug",
      "integrations"
    ],
    "body": "### Bug summary\n\nI created a GCS Bucket block called \"prefect-cache\", and in that block set the Bucket Folder to `alpha`.\nI set a flow to use this block as result storage:\n```\n@flow(persist_result=True, result_storage=\"gcs-bucket/prefect-cache\")\n```\n\nWhen a task finishes, it writes the cache file to:\n```\nalpha/alpha/761db2ef8cf25ad06b5203ca47f4f20d\n```\ninstead of \n```\nalpha/761db2ef8cf25ad06b5203ca47f4f20d\n```\nas expected.\n\nIf you execute the task again, it acts as if there is a cache miss, and then overwrites the existing key (with the duplicate prefix). Strangely, this only happens when _writing_ the cache, _reading_ looks in the correct place.\n\nIf I manually copy (using gsutil)\n```\nalpha/alpha/761db2ef8cf25ad06b5203ca47f4f20d\n```\nto \n```\nalpha/761db2ef8cf25ad06b5203ca47f4f20d\n```\nand run the flow again, there is a cache hit. \n\n### Version info\n\n```Text\nVersion:             3.4.0\nAPI version:         0.8.4\nPython version:      3.11.12\nGit commit:          c80e4442\nBuilt:               Fri, May 02, 2025 08:02 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-gcp:       0.6.7\n  prefect-dbt:       0.6.6\n  prefect-shell:     0.3.1\n  prefect-slack:     0.3.1\n  prefect-sqlalchemy: 0.5.2\n  prefect-ray:       0.4.4\n  prefect-dask:      0.3.5\n  prefect-docker:    0.6.4\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "thanks for the issue @jmesterh - this should be fixed and released in [`prefect-gcp==0.6.8`](https://pypi.org/project/prefect-gcp/0.6.8/)"
      }
    ]
  },
  {
    "issue_number": 15762,
    "title": "Flow gets stuck when sending logs to UI from some libraries",
    "author": "williamjamir",
    "state": "open",
    "created_at": "2024-10-19T10:05:33Z",
    "updated_at": "2025-06-04T16:42:31Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\r\n\r\nI'm having trouble using Prefect with the pymc library. When I send logs to the UI, the flow gets stuck and doesn't finish, so I have to cancel it manually. This problem happens with both Prefect versions 2 and 3. \r\n\r\nI've encountered similar issues with other situations, like with Dask and other integrations, but I couldn't figure out the problem. I think solving this issue will also help with other similar unnoticed problems.\r\n\r\nHere's a minimal reproducible example. When it's working properly (without sending logs to the UI), it should run in 5 to 6 seconds.\r\n\r\nlogging.yaml\r\n```\r\nversion: 1\r\ndisable_existing_loggers: False\r\nhandlers:\r\n  console:\r\n    level: WARNING\r\n    class: prefect.logging.handlers.PrefectConsoleHandler\r\n\r\n  api:\r\n    class: prefect.logging.handlers.APILogHandler\r\n\r\nloggers:\r\n    pymc:\r\n        level: INFO\r\n        handlers: [ api ]\r\n        propagate: false\r\n\r\nroot:\r\n  level: INFO\r\n  handlers: [ console ]\r\n```\r\n\r\nacme.py\r\n```python\r\nfrom pathlib import Path\r\nfrom prefect import flow\r\nfrom pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\r\nimport pandas as pd \r\n\r\n@flow()\r\ndef acme():\r\n    if Path(\"data.csv\").exists():\r\n        data = pd.read_csv('data.csv')\r\n    else:\r\n        data_url = 'https://raw.githubusercontent.com/pymc-labs/pymc-marketing/main/data/mmm_example.csv'\r\n        data = pd.read_csv(data_url, parse_dates=['date_week'])\r\n        data.to_csv('data.csv')\r\n    \r\n    mmm = MMM(\r\n        adstock=GeometricAdstock(l_max=1),\r\n        saturation=LogisticSaturation(),\r\n        date_column='date_week',\r\n        channel_columns=['x1', 'x2'],\r\n        control_columns=['event_1', 'event_2', 't'],\r\n        yearly_seasonality=1,\r\n    )\r\n    X = data.drop('y', axis=1)\r\n    y = data['y']\r\n    mmm.fit(X, y)\r\n\r\nif __name__ == '__main__':\r\n    acme()\r\n```\r\n\r\n\r\n```\r\npip install prefect pymc-marketing\r\n```\r\n\r\n```\r\nPREFECT_LOGGING_SETTINGS_PATH =<path>/logging.yml python acme.py\r\n```\r\n\r\n\r\n### Version info (`prefect version` output)\r\n\r\n```Text\r\nVersion:             3.0.0\r\nAPI version:         0.8.4\r\nPython version:      3.11.9\r\nGit commit:          c40d069d\r\nBuilt:               Tue, Sep 3, 2024 11:13 AM\r\nOS/Arch:             darwin/arm64\r\nProfile:             local\r\nServer type:         server\r\nPydantic version:    2.9.2\r\n```\r\n\r\n\r\n### Additional context\r\n\r\nI already tried disabling the progress bar from Pymc, but the issue is still persisting (flow gets stuck)\r\n```\r\n    mmm.fit(X, y,  progressbar=False)\r\n```\r\n\r\n\r\nAlso, this is the stack trace when interrupting the execution on Prefect 3\r\n```python\r\npython -m acme\r\nUsing NumPy C-API based implementation for BLAS functions.\r\nSampling 4 chains, 1 divergences ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ  92% 0:00:01 / 0:00:39\r\n^C^CCrash detected! Execution was aborted by an interrupt signal.\r\nFinished in state Crashed('Execution was aborted by an interrupt signal.')\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/opt/opensource/dummy_folder/acme.py\", line 31, in <module>\r\n    hi()\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/prefect/flows.py\", line 1334, in __call__\r\n    return run_flow(\r\n           ^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/prefect/flow_engine.py\", line 810, in run_flow\r\n    return run_flow_sync(**kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/prefect/flow_engine.py\", line 688, in run_flow_sync\r\n    engine.call_flow_fn()\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/prefect/flow_engine.py\", line 667, in call_flow_fn\r\n    result = call_with_parameters(self.flow.fn, self.parameters)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/prefect/utilities/callables.py\", line 206, in call_with_parameters\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/opensource/dummy_folder/acme.py\", line 27, in hi\r\n    mmm.fit(X, y)\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc_marketing/model_builder.py\", line 606, in fit\r\n    idata = pm.sample(**sampler_kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/sampling/mcmc.py\", line 870, in sample\r\n    return _sample_return(\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/sampling/mcmc.py\", line 938, in _sample_return\r\n    idata = pm.to_inference_data(mtrace, **ikwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/backends/arviz.py\", line 520, in to_inference_data\r\n    return InferenceDataConverter(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/backends/arviz.py\", line 208, in __init__\r\n    self.posterior_trace, self.warmup_trace = self.split_trace()\r\n                                              ^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/backends/arviz.py\", line 253, in split_trace\r\n    trace_posterior = self.trace[self.ntune :]\r\n                      ~~~~~~~~~~^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/backends/base.py\", line 362, in __getitem__\r\n    return self._slice(idx)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/backends/base.py\", line 529, in _slice\r\n    new_traces = [trace._slice(slice) for trace in self._straces.values()]\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/backends/base.py\", line 529, in <listcomp>\r\n    new_traces = [trace._slice(slice) for trace in self._straces.values()]\r\n                  ^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/backends/ndarray.py\", line 168, in _slice\r\n    sliced = NDArray(model=self.model, vars=self.vars)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/backends/ndarray.py\", line 44, in __init__\r\n    super().__init__(name, model, vars, test_point)\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/backends/base.py\", line 158, in __init__\r\n    self.fn = model.compile_fn(vars, inputs=model.value_vars, on_unused_input=\"ignore\")\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/model/core.py\", line 1648, in compile_fn\r\n    fn = compile_pymc(\r\n         ^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pymc/pytensorf.py\", line 1040, in compile_pymc\r\n    pytensor_function = pytensor.function(\r\n                        ^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/compile/function/__init__.py\", line 315, in function\r\n    fn = pfunc(\r\n         ^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/compile/function/pfunc.py\", line 465, in pfunc\r\n    return orig_function(\r\n           ^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/compile/function/types.py\", line 1750, in orig_function\r\n    m = Maker(\r\n        ^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/compile/function/types.py\", line 1523, in __init__\r\n    self.prepare_fgraph(inputs, outputs, found_updates, fgraph, mode, profile)\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/compile/function/types.py\", line 1411, in prepare_fgraph\r\n    rewriter_profile = rewriter(fgraph)\r\n                       ^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/graph/rewriting/basic.py\", line 125, in __call__\r\n    return self.rewrite(fgraph)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/graph/rewriting/basic.py\", line 121, in rewrite\r\n    return self.apply(fgraph, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/graph/rewriting/basic.py\", line 291, in apply\r\n    sub_prof = rewriter.apply(fgraph)\r\n               ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/graph/rewriting/basic.py\", line 2427, in apply\r\n    node_rewriter_change = self.process_node(\r\n                           ^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/graph/rewriting/basic.py\", line 1965, in process_node\r\n    fgraph.replace_all_validate_remove(  # type: ignore\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/graph/features.py\", line 628, in replace_all_validate_remove\r\n    chk = fgraph.replace_all_validate(replacements, reason=reason, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/graph/features.py\", line 603, in replace_all_validate\r\n    fgraph.validate()\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/graph/features.py\", line 478, in validate_\r\n    ret = fgraph.execute_callbacks(\"validate\")\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/william/.pyenv/versions/3.11.9/envs/dummy_env/lib/python3.11/site-packages/pytensor/graph/fg.py\", line 712, in execute_callbacks\r\n    fn = getattr(feature, name)\r\n         ^^^^^^^^^^^^^^^^^^^^^^\r\nKeyboardInterrupt\r\nmake: *** [run] Interrupt: 2",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Thanks for the bug report @williamjamir! Have you tried using the `PREFECT_EXTRA_LOGGERS` setting? That setting is the recommended way to include logs from other libraries (here's where it's referenced in the [docs](https://docs.prefect.io/3.0/develop/logging#include-logs-from-other-libraries)). \r\n\r\nIf that doesn't work we can dig in more. My hunch is that when creating a custom `logging.yaml` you need to include some parts of Prefect's default `logging.yaml`."
      },
      {
        "user": "williamjamir",
        "body": "Hi @desertaxle , thanks for the prompt response!\r\n\r\nIf you meant `PREFECT_LOGGING_EXTRA_LOGGERS`, then yes, I have tried that, but unfortunately, it didn‚Äôt resolve the issue. \r\n\r\nFor reference, I used the following command:\r\n\r\n```bash\r\nPREFECT_LOGGING_EXTRA_LOGGERS='pymc' python -m acme\r\n```\r\n\r\nRegarding the custom logging configuration, the provided code is a minimal reproducible example. \r\nIn my actual application, I have integrated `structlog,` so I need to customize this file. \r\nInstead of `get_run_logger`, I use `structlog.get_logger`, and my logs are written to a file in a structured format.\r\n\r\nI‚Äôd appreciate any guidance on how to properly configure structlog while still incorporating parts of Prefect‚Äôs default logging behavior. Any specific instructions or examples would be incredibly helpful.\r\n\r\nAs for the original issue, I‚Äôm happy to assist in any way necessary. \r\nI look forward to your thoughts and suggestions on how we can move forward!"
      },
      {
        "user": "williamjamir",
        "body": "\r\nHi @desertaxle,\r\n\r\nIs there any way I can help with this investigation? Are there any leads or suggestions on where I should begin to assist in resolving this issue?\r\n\r\nAt the moment, I have the logs disabled, which is unfortunate, as it would be ideal to fully utilize the UI dashboard.\r\n\r\nThank you!"
      }
    ]
  },
  {
    "issue_number": 18225,
    "title": "Kopf Handler \"_replicate_pod_event\" fails on Worker Start",
    "author": "masonmenges",
    "state": "closed",
    "created_at": "2025-06-03T18:28:32Z",
    "updated_at": "2025-06-04T05:00:11Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen starting up a prefect worker it seems to consistently generate an error related to a kopf handler with the following stack trace, this doesn't seem to affect anything functionally and only seems to occur when the worker starts up, subsequent events related to the same handler appear to succeed\n\n```\n18:19:40.047 | ERROR   | kopf.objects - Handler '_replicate_pod_event' failed with an exception. Will ignore.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/kopf/_core/actions/execution.py\", line 274, in execute_handler_once\n    result = await invoke_handler(\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/kopf/_core/actions/execution.py\", line 369, in invoke_handler\n    result = await invocation.invoke(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/kopf/_core/actions/invocation.py\", line 114, in invoke\n    result = await fn(**kwargs)  # type: ignore\n             ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect_kubernetes/observer.py\", line 113, in _replicate_pod_event\n    response = await orchestration_client.request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/client/orchestration/base.py\", line 53, in request\n    return await self._client.send(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/client/base.py\", line 361, in send\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.11/site-packages/prefect/client/base.py\", line 162, in raise_for_status\n    raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__\nprefect.exceptions.PrefectHTTPStatusError: Client error '422 Unprocessable Entity' for url 'https://api.prefect.cloud/api/accounts/9b649228-0419-40e1-9e0d-44954b5c0ab6/workspaces/c43bcc47-3a05-461e-8020-8920ad28a14b/events/filter'\nResponse: {'exception_message': 'Query time range is too large. The maximum allowed range is 32 days.'}\n```\n\n### Version info\n\n```Text\nVersion:             3.4.4\nAPI version:         0.8.4\nPython version:      3.11.12\nGit commit:          0367d7aa\nBuilt:               Thu, May 29, 2025 09:40 PM\nOS/Arch:             linux/aarch64\nProfile:             ephemeral\nServer type:         cloud\nPydantic version:    2.11.5\nIntegrations:\n  prefect-redis:     0.2.2\n  prefect-kubernetes: 0.6.1\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18217,
    "title": "Sendgrid Email Block does not send emails to the recipient emails if it was updated programtically",
    "author": "laxmibalami",
    "state": "closed",
    "created_at": "2025-06-02T17:48:32Z",
    "updated_at": "2025-06-03T19:31:57Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nCreate a Sengrid Email Block with the following config:\nRecipient emails: []\nNotify Type: prefect_default\nSender email id: [john@foo.com](mailto:john@foo.com)\n\nThen update the recipients email list in the code:\n\n```python\ndef main():\n    sg_block = SendgridEmail.load(\"my-sendgrid-email-block\")\n\n    sg_block.to_emails = [\"jane@bar.com\"]\n    sg_block.save(SENDGRID_EMAIL_BLOCK, overwrite=True)\n    print(f\"Email list updated to: {sg_block.to_emails}\")\n\n    sg_block.notify(body=f\"Hello, <br><br> This is a test email.\")\n    \n   # Following can be commented out to see that it is actually able to overwrite the email list, just does not use that updated list when sending emails.\n    sg_block.to_emails = []\n    sg_block.save(SENDGRID_EMAIL_BLOCK, overwrite=True)\n    print(f\"Email list reset back to: {sg_block.to_emails}\")\n```\n\nHowever, with the above, it does update the block. If we do not reset it to empty list, we see the change reflected in the UI as well. But it sends the email to the sender's email address instead of the new recipients/target email address when the list is updated from the code.\nBut if I set the same recipient email list in the UI, then it is able to send them correctly.\n\n### Version info\n\n```Text\nVersion:             2.20.18\nAPI version:         0.8.4\nPython version:      3.10.12\nGit commit:          b7059e89\nBuilt:               Wed, Apr 30, 2025 2:37 PM\nOS/Arch:             linux/x86_64\nProfile:             default\nServer type:         cloud\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @laxmibalami - this should be fixed and released in https://github.com/PrefectHQ/prefect/releases/tag/2.20.19"
      },
      {
        "user": "laxmibalami",
        "body": "You are awesome!! Just checked it out and its now working as expected. Thank you :) "
      }
    ]
  },
  {
    "issue_number": 18222,
    "title": "Exception in task called with `.wait()` or `wait_for=` does not propagate to flow",
    "author": "jgaucher-cs",
    "state": "closed",
    "created_at": "2025-06-03T14:08:12Z",
    "updated_at": "2025-06-03T15:44:35Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nSee the following example:\n```python\nfrom prefect import flow, task\n\n@task\ndef task1():\n    raise RuntimeError(\"Error from task1\")\n\n@flow\ndef my_flow() -> str:\n    future1 = task1.submit()\n    future1.wait()\n\nif __name__ == \"__main__\":\n    my_flow()\n```\n\nI would expect my flow to fail because of the `RuntimeError`. I run it with:\n```bash\npython /path/to/my/script.py; echo $?\n```\n\nThe output is: \n```\n# ...\nline 5, in task1\n    raise RuntimeError(\"Error from task1\")\nRuntimeError: Error from task1\n15:51:57.383 | ERROR   | Task run 'task1-1f8' - Finished in state Failed('Task run encountered an exception RuntimeError: Error from task1')\n15:51:57.398 | INFO    | Flow run 'romantic-barracuda' - Finished in state Completed()\n15:51:57.415 | WARNING | EventsWorker - Still processing items: 3 items remaining...\n0\n```\n\nMy task failed as excepted, but my flow finished successfully (`Finished in state Completed()` and my return code is `0`). I would except it to fail also.\n\nSame result with `wait_for=` (but note that `task2` is never run, as expected):\n```python\n@task\ndef task1():\n    raise RuntimeError(\"Error from task1\")\n\n@task\ndef task2():\n    print(\"Hello from task2\")\n\n@flow\ndef my_flow() -> str:\n    future1 = task1.submit()\n    task2.submit(wait_for=[future1])\n```\n\nNote that if I use `.result()`, the flow fails as expected:\n```python\n@task\ndef task1():\n    raise RuntimeError(\"Error from task1\")\n\n@flow\ndef my_flow() -> str:\n    future1 = task1.submit()\n    future1.result()\n\nif __name__ == \"__main__\":\n    my_flow()\n```\n\nOutput:\n```\nline 5, in task1\n    raise RuntimeError(\"Error from task1\")\nRuntimeError: Error from task1\n15:58:59.778 | INFO    | Flow run 'petite-octopus' - Finished in state Failed('Flow run encountered an exception: RuntimeError: Error from task1')\n# ...\nRuntimeError: Error from task1\n15:58:59.792 | WARNING | EventsWorker - Still processing items: 3 items remaining...\n1\n```\n\nNote `Flow run 'petite-octopus' - Finished in state Failed('Flow run encountered an exception: RuntimeError: Error from task1')` and return code is `1`. I would except the same behaviour when using `.wait()` or `wait_for=`. Thank you.\n\n\n### Version info\n\n```Text\nVersion:             3.4.4\nAPI version:         0.8.4\nPython version:      3.11.7\nGit commit:          0367d7aa\nBuilt:               Thu, May 29, 2025 09:37 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.11.5\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @jgaucher-cs - this is expected behavior. `.result()` unwraps the trapped exception and you'd need to do that in the flow if you want the flow to fail as a result of the task failing\n\nsee https://docs.prefect.io/v3/develop/write-flows#final-state-determination and https://docs.prefect.io/v3/resources/upgrade-to-prefect-3#examples"
      },
      {
        "user": "jgaucher-cs",
        "body": "Thank you for your response. In fact if I call `future2.result()` at the end of my flow, the exception from `task1` is propagated. But not if I call `future2.wait()`. I guess this is the way to go.\n```python\n@task\ndef task1():\n    raise RuntimeError(\"Error from task1\")\n\n@task\ndef task2():\n    print(\"Hello from task2\")\n\n@flow\ndef my_flow() -> str:\n    future1 = task1.submit()\n    future2 = task2.submit(wait_for=[future1])\n    future2.result() # exception is propagated\n    # future2.wait() # exception is NOT propagated"
      },
      {
        "user": "zzstoatzz",
        "body": "correct, if you use `__call__` then `.result()` is effectively called for you, but not if you use `.wait()`. Using `.wait()` is essentially just making sure the future reaches a terminal state, but doesn't unwrap the result (which might be an exception)"
      }
    ]
  },
  {
    "issue_number": 17415,
    "title": "Prevent Orphaned Slots in Concurrency Limits",
    "author": "aaazzam",
    "state": "open",
    "created_at": "2025-03-07T17:29:43Z",
    "updated_at": "2025-06-03T15:33:48Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nCurrently, Prefect implements global concurrency limits that allow tasks to acquire locks on a number of slots from a shared pool. These limits can optionally be configured with a \"leaky bucket\" algorithm that gradually returns slots to the pool over time, effectively functioning as a rate limit.\n\nWhen the slot decay is not configured (standard concurrency limit), there are scenarios where tasks can indefinitely hold onto concurrency slots:\n\n1. If the container running a task dies unexpectedly\n2. In some instances when a parent flow is canceled\n3. When a task fails in a way that prevents proper cleanup\n\nThese orphaned concurrency slots become unavailable to other tasks, gradually depleting the pool. This situation is particularly difficult to debug as there's no clear indication of which tasks are holding onto slots or when they were acquired, leading to mysterious \"concurrency limit reached\" errors that require manual intervention to resolve.\n\n\n\n### Describe the proposed behavior\n\nTo address these issues, we propose implementing a \"leased lock\" paradigm, common in distributed concurrency systems:\n\n1. Each concurrency limit would enforce a lease duration for acquired slots\n2. Tasks that acquire slots would need to periodically renew their lease while they're still running\n3. If a task fails to renew its lease within the specified timeframe, the slots would be automatically released back to the pool\n\nThis approach provides several benefits:\n\n- **Self-healing:** Orphaned slots automatically return to the pool after the lease expires\n- **Visibility:** The renewal pattern provides a heartbeat mechanism that can be monitored\n- **Configurable safety:** Lease durations can be tuned based on workload patterns\n- **Compatibility:** Can work alongside existing slot decay configurations\n\n## Implementation Details\n\nThe leased lock system would include:\n\n- A configurable `lease_duration` parameter for concurrency limits (defaulting to a reasonable value like 5 minutes)\n- A background renewal process that automatically extends leases for running tasks\n- A cleanup mechanism that scans for expired leases and returns slots to the pool\n- Logging and observability improvements to track lease acquisitions and releases\n\nUsers would be able to:\n- Specify different lease durations for different concurrency limits\n- Monitor lease status through the Prefect UI\n- Configure alert policies for frequently expired leases (which might indicate infrastructure issues)\n\n## Migration Path\n\nThis change would be backward compatible with existing concurrency limit configurations. By default, the lease duration would be set sufficiently high to prevent unexpected slot releases for most workloads, but low enough to recover from common failure scenarios.\n\nFor users experiencing frequent orphaned slots, enabling shorter lease durations would provide immediate relief without requiring changes to their task code.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "jonahduffin",
        "body": "+1-- task run concurrency limits are crucial to the way we use Prefect and we are currently jumping through a lot of hoops to catch instances of these \"orphaned slots\" and free them up. We use the Prefect Client to get a list of concurrency tags, fetch the active task runs in the concurrency limit, and set the state to failed if we're past a timeout threshold, in a flow that is scheduled to run every 15 minutes. This method isn't foolproof and often orphans occupy a slot much longer than we'd like.\n\nHaving some kind of heartbeat for these tasks that become orphaned, and a lease for the concurrency slot they occupy that is returned automatically after expiration, would be great."
      }
    ]
  },
  {
    "issue_number": 18224,
    "title": "Add ability to resize columns in the deployment table",
    "author": "robfreedy",
    "state": "open",
    "created_at": "2025-06-03T15:08:41Z",
    "updated_at": "2025-06-03T15:08:41Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nCurrently, if my deployment has a long name, I am not able to adjust the size of the columns in the UI to show the entire deployment name. I have to hover over each individual deployment to show the entire name. \n<img width=\"1176\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3f174fda-780f-4f87-b4fe-7166c05c5071\" />\n\n### Describe the proposed behavior\n\nMake the table columns on the deployment table adjustable so that the name column can be expanded to show the entire name of the deployment (within reason) \n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18181,
    "title": "Enhanced support for uv dynamic virtual environments in Prefect agent executions",
    "author": "eduardojandre",
    "state": "closed",
    "created_at": "2025-05-27T21:05:36Z",
    "updated_at": "2025-06-03T15:02:25Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nCurrently, Prefect agents execute flows using the environment in which the agent process is running. While this works well in homogeneous environments, it complicates scenarios where different flows require distinct Python environments or dependency sets.\n\nThe current alternatives ‚Äî using multiple agent configurations or dynamically instantiated containers with specialized images ‚Äî introduce additional operational complexity, infrastructure overhead, and slower startup times.\n\n### Describe the proposed behavior\n\n**Proposed Behavior**\nIntroduce a mechanism within the Prefect agent or flow runner that allows:\n\n- Optional detection of a .venv or uv environment in the cloned repository.\n- Invocation of the virtual environment's Python binary to execute the flow, instead of relying on the agent's global Python environment.\n\n**Potential Implementation Ideas**\n- Modify `src/prefect/workers/process.py` (`ProcessWorker`) to support **automatic detection** of a `.venv` or `uv` environment in the cloned repository.  \n  If such an environment exists, the worker can invoke the flow run using the **Python binary within that virtual environment**, instead of the agent's default `sys.executable`.\n\n- Consider extending similar logic to other workers to support it, ensuring consistent behavior across different deployment types, including **containerized** and **Kubernetes-based** workers.\n\n- Provide a **configuration option** (e.g., environment variable or deployment setting) to enable or disable this **auto-detection and invocation** of virtual environments, allowing for flexible adoption.\n\n### Example Use\n\nAn organization operates a Prefect agent on a shared server where multiple teams deploy flows with different and potentially conflicting dependencies.\n\nWith this feature, each team can commit their flow code, and use a Utility step on the pull action to create a dedicated `.venv` or `uv` environment within their repository. When the flow is deployed and executed by the agent:\n\n1. The agent clones the repository \n2. Agent runs Utility step to generate virtual env\n3. The and detects the presence of the virtual environment, and instead of using the agent's global Python interpreter, it automatically invokes the flow using the `python` binary inside the detected environment.\n4. This allows different flows to run with completely isolated dependencies without requiring multiple agents or complex container orchestration.\nFor example:\n- **Flow A** requires `pandas==1.5`.\n- **Flow B** requires `pandas==2.2`.\n\nBoth flows can run independently on the same agent using their respective virtual environments, improving deployment flexibility and minimizing dependency conflicts.\n\n\n### Additional context\n\n## Benefits\n\n### Simplified and Faster Iteration\n\nSwitching Python environments via virtual environments (`venv`, `uv`, etc.) can be faster and simpler than building, pushing, and managing container images. Developers can adjust dependencies without modifying deployment infrastructure.\n\nAdditionally, execution startup time can be significantly reduced on some scenarios ‚Äî invoking a subprocess using the environment's Python binary is can be faster than instantiating a new container. This makes the approach particularly well-suited for **short-lived flows** or **frequent deployments**.\n\n### Greater Flexibility in Deployment\n\nEnables the use of **multiple Python environments within a single worker**, allowing diverse flows with different dependencies to coexist without requiring container orchestration.\n\n### Improved Compatibility\n\nSupports scenarios where **running inside a container is restricted**,",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @eduardojandre - thanks for the write-up! this is possible today by overriding the `command` job variable on a per deployment basis like [this](https://github.com/zzstoatzz/prefect-pack/blob/main/examples/run_a_prefect_worker/on_local/prefect.yaml). you can also run arbitrary shell commands in a `pull` step instead to setup your flow run venv as needed.\n\n> Modify src/prefect/workers/process.py (ProcessWorker) to support automatic detection of a .venv or uv environment in the cloned repository.\n\nwe are unlikely to do this, because the above escape hatch already exists and there is a great deal of variation on what \"automation detection\" could mean here\n\n\nadditionally, you might be interested in ad-hoc infra submission.\n\ndocs: https://docs.prefect.io/v3/deploy/submit-flows-directly-to-dynamic-infrastructure\nissue: https://github.com/PrefectHQ/prefect/issues/17194\n\ngiven the optionality that exists today and the documented plans to expand ad-hoc infra over time, I think we can close this issue. what do you think?"
      },
      {
        "user": "eduardojandre",
        "body": "Hi @zzstoatzz, thank you very much for the super quick response!\n\nI appreciate the flexibility that Prefect offers with `command` overrides and pull steps. I had actually been experimenting with something along those lines, but I ran into a limitation: the pull step seems to execute *after* the `command`, which makes it tricky for my use case.\n\nSpecifically, I was trying to set up something like this:\n\n```yaml\ndeployments:\n  - name: test-labs-uv4\n    ...\n    pull:\n      - prefect.deployments.steps.git_clone:\n          id: clone-step\n          repository: \"https://dev.azure.com/prefect-demo/_git/prefect-demo\"\n          branch: \"testeduardo\"\n          access_token: \"{{ prefect.blocks.secret.azure-devops-token }}\"\n      - prefect.deployments.steps.run_shell_script:\n          script: |\n            uv venv .venv\n            uv pip install -r requirements.txt\n    work_pool:\n      name: my-work-pool\n      job_variables:\n        command: \"uv run -m prefect.engine\"\n```\nBut since the repo isn‚Äôt cloned yet when command runs, the environment isn‚Äôt available, so this approach doesn‚Äôt quite solve the problem I‚Äôm aiming to address.\n\nI completely understand the concern about generalizing \"automatic detection,\" especially given the variations in environments. I also looked into ad-hoc infrastructure submission as you suggested ‚Äî while it‚Äôs a great feature, it doesn‚Äôt quite fit my use case either.\n\nMy use case is to support lightweight flows with slightly different dependency requirements, all running on the same worker, without having to manage and provision entire container images for each variation.  I also understand what I proposed as a solution will not work either because of the order. \nBut I think the use case is still valid and we might find another way of doing. maybe a pre pull action where we download and install requirements.\n\nThanks again!"
      },
      {
        "user": "zzstoatzz",
        "body": "hi @eduardojandre - you shouldn't need both. you're correct about the sequencing, but I'm suggesting you could _either_ \n\n- override `command` if you want to point at a repo or existing package (use `--with`)\n- setup a venv as needed in a `pull` step if you want to install from cloned requirements\n\nin this case it sounds like you'd prefer the latter, so check out [this example](https://github.com/zzstoatzz/prefect-pack/blob/main/examples/run_a_prefect_worker/on_local/prefect.yaml#L25-L35):\n\n```bash\n¬ª prefect deployment run 'uses-pandas/deployment-3'\nCreating flow run for deployment 'uses-pandas/deployment-3'...\nCreated flow run 'eccentric-caterpillar'.\n‚îî‚îÄ‚îÄ UUID: b1e8875a-a358-4abc-9a4b-d9c1cc21dc6f\n‚îî‚îÄ‚îÄ Parameters: {}\n‚îî‚îÄ‚îÄ Job Variables: {}\n‚îî‚îÄ‚îÄ Scheduled start time: 2025-05-28 16:18:33 CDT (now)\n‚îî‚îÄ‚îÄ URL: http://127.0.0.1:4200/runs/flow-run/b1e8875a-a358-4abc-9a4b-d9c1cc21dc6f\n\n¬ª uv run --isolated prefect worker start --pool 'local' --run-once\nInstalled 134 packages in 239ms\nDiscovered type 'process' for work pool 'local'.\nWorker 'ProcessWorker bae703d0-8d76-4078-b457-0e09c236f168' started!\n16:18:46.965 | INFO    | prefect.flow_runs.worker - Worker 'ProcessWorker bae703d0-8d76-4078-b457-0e09c236f168' submitting flow run 'b1e8875a-a358-4abc-9a4b-d9c1cc21dc6f'\n16:18:47.015 | INFO    | prefect.flow_runs.runner - Opening process...\n16:18:47.037 | INFO    | prefect.flow_runs.worker - Completed submission of flow run 'b1e8875a-a358-4abc-9a4b-d9c1cc21dc6f'\n16:18:47.612 | INFO    | Flow run 'eccentric-caterpillar' -  > Running git_clone step...\n16:18:48.080 | INFO    | Flow run 'eccentric-caterpillar' -  > Running run_shell_script step...\nResolved 7 packages in 7ms\nInstalled 3 packages in 62ms\n + emoji==2.14.1\n + numpy==2.2.6\n + pandas==2.2.3\n16:18:48.700 | INFO    | Flow run 'eccentric-caterpillar' - Beginning flow run 'eccentric-caterpillar' for flow 'uses-pandas'\n16:18:48.700 | INFO    | Flow run 'eccentric-caterpillar' - View at http://127.0.0.1:4200/runs/flow-run/b1e8875a-a358-4abc-9a4b-d9c1cc21dc6f\n16:18:55.115 | INFO    | Flow run 'eccentric-caterpillar' -    a  b\n0  1  4\n1  2  5\n2  3  6\n16:18:55.142 | INFO    | Flow run 'eccentric-caterpillar' - Finished in state Completed()\n16:18:55.266 | INFO    | prefect.flow_runs.runner - Process for flow run 'eccentric-caterpillar' exited cleanly.\nWorker 'ProcessWorker bae703d0-8d76-4078-b457-0e09c236f168' stopped!\n\n¬ª uv pip list | rg pandas\n```\n\nyou should have full flexibility via `uv` with how you want to reuse or recreate a given virtual environment in a `pull` step\n\nfor example, you might want to define `dependency-groups` in your `pyproject.toml` and have each flow create a new venv like `uv sync --group this-flow-group` in your `pull` step"
      }
    ]
  },
  {
    "issue_number": 18214,
    "title": "UI Overflow when the Flow Run Name is too large",
    "author": "josemarcosrf",
    "state": "open",
    "created_at": "2025-06-02T14:50:48Z",
    "updated_at": "2025-06-03T13:40:31Z",
    "labels": [
      "bug",
      "ui",
      "cloud"
    ],
    "body": "### Bug summary\n\nThe Prefect Cloud's UI hides away the buttons on the top right of the \"Runs\" view, making these not accessible when the Flow run name is too large. Is best understood in the attached screenshot. Scrolling to the right won't help, but using the browser's zoom-out until the buttons fit again does.\n\n### Version info\n\n```Text\nVersion:             3.4.3\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          1c2ba7a4\nBuilt:               Thu, May 22, 2025 10:00 PM\nOS/Arch:             darwin/arm64\nProfile:             cloud\nServer type:         cloud\nPydantic version:    2.11.5\nIntegrations:\n  prefect-aws:       0.5.10\n```\n\n### Additional context\n\n![Image](https://github.com/user-attachments/assets/4ddc9b77-a0f5-40c6-8f9d-42be0787c131)",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @josemarcosrf - thanks for the issue! we'll take a look at this"
      }
    ]
  },
  {
    "issue_number": 18220,
    "title": "The flow deployed with Docker is not visible on the server.",
    "author": "Mingdr",
    "state": "closed",
    "created_at": "2025-06-03T06:34:11Z",
    "updated_at": "2025-06-03T13:21:22Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nHey\n\n---\n\nI started a server on the server with IP 220 using the following command:\n\n```bash\ndocker run --name prefect -p 7200:4200 -d --rm prefect:3-latest -- prefect server start --host 0.0.0.0\n```\n\nMy own machine has the IP 123. I used PyCharm to start a simple flow:\n\n```python\nmysql_to_es.serve(name=\"mysql_to_es\", interval=60)\n```\n\nI can see this deployment in the Prefect UI on the 220 server. However, when I run a flow encapsulated in a Docker container with the same code, I can't see this deployment on the 220 server, even though the container is running.\n\nI have already set the `PREFECT_API_URL` in the `.env` file in the flow container to `http://192.168.0.220:7200/api`.\n\nCould you please explain the difference between running the flow directly in PyCharm and deploying it as a Docker container? How can I configure the Docker-deployed flow so that it appears in the UI on the 220 server? Thank you.\n\n### Version info\n\n```Text\npycharm flowÔºö\nVersion:             3.4.2\nAPI version:         0.8.4\nPython version:      3.10.12\nGit commit:          c3c1c119\nBuilt:               Mon, May 19, 2025 04:04 PM\nOS/Arch:             win32/AMD64\nProfile:             local\nServer type:         ephemeral\nPydantic version:    2.10.2\nServer:\n  Database:          sqlite\n  SQLite version:    3.45.3\n220 server and flowÔºö\nVersion:             3.4.3\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          1c2ba7a4\nBuilt:               Thu, May 22, 2025 10:03 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.11.5\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 16245,
    "title": "Prefect deploy failing due to incompatible Prefect Client and Prefect Server versions after 3.1.5",
    "author": "jonahduffin",
    "state": "open",
    "created_at": "2024-12-06T15:54:12Z",
    "updated_at": "2025-06-02T17:31:30Z",
    "labels": [
      "bug",
      "great writeup"
    ],
    "body": "### Bug summary\n\nWe use deployments defined in a `prefect.yaml` file to deploy to a locally-hosted Prefect server. Our CI platform uses an image with a version of Prefect that is frequently updated to run the `prefect --no-prompt deploy --all` command, which will deploy many kubernetes-based deployments to our Prefect server.\r\n\r\nAfter upgrading the image that runs the prefect deploy step to version `3.1.5`, the deploy command started failing with the following message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/prefect/cli/_utilities.py\", line 42, in wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/prefect/cli/_types.py\", line 153, in sync_fn\r\n    return asyncio.run(async_fn(*args, **kwargs))\r\n  File \"/home/circleci/.pyenv/versions/3.10.14/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/home/circleci/.pyenv/versions/3.10.14/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/prefect/cli/deploy.py\", line 438, in deploy\r\n    await _run_multi_deploy(\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/prefect/cli/deploy.py\", line 877, in _run_multi_deploy\r\n    await _run_single_deploy(deploy_config, actions, prefect_file=prefect_file)\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/prefect/client/utilities.py\", line 103, in with_injected_client\r\n    return await fn(*args, **kwargs)\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/prefect/cli/deploy.py\", line 714, in _run_single_deploy\r\n    flow_id = await client.create_flow_from_name(deploy_config[\"flow_name\"])\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/prefect/client/orchestration.py\", line 490, in create_flow_from_name\r\n    response = await self._client.post(\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 1905, in post\r\n    return await self.request(\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 1585, in request\r\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/prefect/client/base.py\", line 361, in send\r\n    response.raise_for_status()\r\n  File \"/home/circleci/project/.venv/lib/python3.10/site-packages/prefect/client/base.py\", line 174, in raise_for_status\r\n    raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__\r\nprefect.exceptions.PrefectHTTPStatusError: Client error '422 Unprocessable Entity' for url 'https://prefect-stage.dcai.corp.adobe.com/api/flows/'\r\nResponse: {'exception_message': 'Invalid request received.', 'exception_detail': [{'type': 'extra_forbidden', 'loc': ['body', 'labels'], 'msg': 'Extra inputs are not permitted', 'input': {}}], 'request_body': {'name': 'llm-based-idk-flow', 'tags': [], 'labels': {}}}\r\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\r\nAn exception occurred.\r\n```\r\n\r\nHere's an example of a deployment from our prefect.yaml file that would fail during this deploy step:\r\n\r\n```\r\ndeployments:\r\n  - name: 'Example Flow{{ get-clean-branch-name.stdout }}'\r\n    version: 0.0.1\r\n    description: Example flow\r\n    entrypoint: our/entrypoint/here:function\r\n    path: /our/path/here\r\n    tags:\r\n      - example\r\n      - '{{ IMAGE_TAG }}'\r\n    push: null\r\n    schedule:\r\n    parameters: {}\r\n    work_pool:\r\n      <<: *default_kubernetes_pool\r\n      job_variables:\r\n        image: 'our_image_here'\r\n        cpu_request: '1'\r\n        cpu_limit: '2'\r\n        memory_request: '2Gi'\r\n        memory_limit: '4Gi'\r\n```\r\n\r\n\r\nOur Prefect server was running Prefect version 3.1.4 and the client was running version 3.1.5 when this error started occurring. It appears that 3.1.5 is not backwards compatible for the deployment step. I believe it's related to this change:\r\nhttps://github.com/PrefectHQ/prefect/commit/9490d6c6216c9998804c07063517b3cce2b95eac\r\n\r\nWe have addressed this issue ourselves by upgrading our Prefect server and worker to version 3.1.5 but it's concerning that a breaking change would occur like this. We'd expect a Prefect client with a single minor version difference from a Prefect server would still be able to deploy, such that we are not required to run all of our Helm commands and upgrade our Prefect infrastructure with every minor release to the Prefect client.\n\n### Version info\n\n```Text\nPrefect version of the client running the deploy step. Note that this version command was not run on the infrastructure that actually executes the deploy step but the Prefect version is the same.\r\n\r\nVersion:             3.1.5\r\nAPI version:         0.8.4\r\nPython version:      3.10.13\r\nGit commit:          3c06654e\r\nBuilt:               Mon, Dec 2, 2024 6:57 PM\r\nOS/Arch:             darwin/arm64\r\nProfile:             default\r\nServer type:         server\r\nPydantic version:    2.10.3\r\n\r\nOur Prefect server and worker were running 3.1.4 when this issue first occurred but the issue has been resolved after upgrading them to Prefect 3.1.5.\n```\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @jonahduffin - thank you for the detailed write up, and sorry you ran into trouble!\r\n\r\n> We'd expect a Prefect client with a single minor version difference from a Prefect server would still be able to deploy, such that we are not required to run all of our Helm commands and upgrade our Prefect infrastructure with every minor release to the Prefect client.\r\n\r\nI would expect the same! we'll look into this to make sure something like this doesn't happen again"
      },
      {
        "user": "desertaxle",
        "body": "Thanks for the issue @jonahduffin! We should take a look at all our server models and make sure that they're ignoring extra fields. It won't help already released versions, but future server versions will be more flexible when new features are added to the client."
      },
      {
        "user": "ericmeadows",
        "body": "@desertaxle - I ran into this with a different thing - please fix this."
      }
    ]
  },
  {
    "issue_number": 18212,
    "title": "Does retry not reuse original task node?",
    "author": "jcppython",
    "state": "closed",
    "created_at": "2025-06-02T10:07:13Z",
    "updated_at": "2025-06-02T15:00:14Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n<img width=\"826\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8813ddaf-f331-4c11-a857-8df21a0c03a5\" />\n\nA new task node will be created when UI retry? \n\n### Version info\n\n```Text\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:28 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.10.3\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @jcppython - can you elaborate on your expectation and observation here?\n\ni am not seeing retries show up as a separate node in the UI\n\n```python\nfrom prefect import flow, task\n\n@flow\ndef f():\n    @task(retries=1)\n    def t(): 1 / 0\n    t()\n```\n\nlooking at the image you provided, I see two runs of think (5d3, b26) and two runs of search (121, 17c)\n\n![Image](https://github.com/user-attachments/assets/6d5c58b8-5ab7-4ab4-b91b-a1189fd88410)"
      },
      {
        "user": "jcppython",
        "body": "***Action***: Retry the already-run flow by clicking \"Retry\" at the upper right corner of the screenshot.\n***Expectation***: The generated task nodes should be executed in place instead of creating new nodes. As shown in the figure, only nodes 5d3 and 121 are expected, without the appearance of nodes b26 and 17c. In other words, it is desired that not only the flow is retried, but the tasks within it are also retried (i.e., the existing task nodes are reused for retry execution).\n\nthanks @zzstoatzz "
      },
      {
        "user": "zzstoatzz",
        "body": "thanks @jcppython - so it seems like this is roughly the situation you have\n```python\nfrom prefect import flow, task\n\n@task\ndef think():\n    print(\"Thinking...\")\n\n\n@task\ndef search():\n    print(\"Searching...\")\n\n\n@flow(retries=1)\ndef main():\n    think()\n    search()\n    1 / 0  # fail\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n![Image](https://github.com/user-attachments/assets/c054e85a-5bf6-40d7-b77b-e517c1c7a3e3)\n\nI'll check with the UI folks whether this is expected"
      }
    ]
  },
  {
    "issue_number": 18053,
    "title": "Robustness issue in 3.4.1 with how container flows communicate events back to the server",
    "author": "jeffcarrico",
    "state": "open",
    "created_at": "2025-05-14T20:15:15Z",
    "updated_at": "2025-06-01T02:18:20Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nSometimes (20-30%) this causes the flow to fail to start with a cryptic error, sometimes the flow runs (and the UI shows success) but some final cleanup operations fail (which we can see in the ECS task logs). The later seems benign, but obviously the failure to start is a problem.\n\nThe flow is a simple hello world with no dependencies and no input parameters, running from a docker image on ECS. Self hosted prefect server.\nhttps://reference.prefect.io/prefect_aws/workers/ecs_worker/\n\n```python\nfrom prefect import flow\nfrom prefect.logging import get_run_logger\n\n@flow\ndef hello_world():\n    logger = get_run_logger()\n    logger.info(\"Hello from ECS!!\")\n```\n\nUnfortunately, I have not yet been able to produce a MRE for this, or anything simple to reproduce it. However, I do know that taking the latest HEAD commit (27cccbf654) and reverting 6869a02a04 378fc4f6b7 eliminates the issue. I believe the commit which introduced the problem is 378fc4f6b7. \n\nI debated whether to submit this without the MRE, figured I'd at least get the information out there. Maybe a second look at 378fc4f6b7 turns something up, or another issue brings more information which helps triangulate the problem. Feel free to close if this is not deemed helpful.\n\n### Version info\n\n```Text\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.11.4\nServer:\n  Database:          sqlite\n  SQLite version:    3.40.1\nIntegrations:\n  prefect-aws:       0.5.10\n```\n\n### Additional context\n\n\nAn example error message from the ECS task logs\n```\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|   timestamp   |                                                                                    message                                                                                     |\n|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1747197365272 | 04:36:05.267 | INFO    | prefect.flow_runs.runner - Opening process...                                                                                                         |\n| 1747197365346 | 04:36:05.344 | INFO    | prefect.flow_runs.runner - Loading flow to check for on_cancellation hooks                                                                            |\n| 1747197365415 | 04:36:05.413 | INFO    | prefect.flow_runs.runner - Loading flow to check for on_cancellation hooks                                                                            |\n| 1747197365512 | 04:36:05.511 | INFO    | prefect.flow_runs.runner - Loading flow to check for on_cancellation hooks                                                                            |\n| 1747197365546 | 04:36:05.545 | INFO    | prefect.flow_runs.runner - Loading flow to check for on_cancellation hooks                                                                            |\n| 1747197365636 | 04:36:05.635 | INFO    | prefect.flow_runs.runner - Loading flow to check for on_cancellation hooks                                                                            |\n| 1747197365751 | 04:36:05.749 | WARNING | prefect.flow_runs.runner - Runner failed to retrieve flow to execute on_cancellation hooks for flow run UUID('fbc41e6a-adac-41ab-a006-62450513ad79'). |\n| 1747197365754 | 04:36:05.752 | WARNING | prefect.flow_runs.runner - Runner failed to retrieve flow to execute on_cancellation hooks for flow run UUID('fbc41e6a-adac-41ab-a006-62450513ad79'). |\n| 1747197365755 | 04:36:05.754 | WARNING | prefect.flow_runs.runner - Runner failed to retrieve flow to execute on_cancellation hooks for flow run UUID('fbc41e6a-adac-41ab-a006-62450513ad79'). |\n| 1747197365804 |   + Exception Group Traceback (most recent call last):                                                                                                                         |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/site-packages/prefect/cli/_utilities.py\", line 44, in wrapper                                                                            |\n| 1747197365804 |   |     return fn(*args, **kwargs)                                                                                                                                             |\n| 1747197365804 |   |            ^^^^^^^^^^^^^^^^^^^                                                                                                                                             |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/site-packages/prefect/cli/_types.py\", line 156, in sync_fn                                                                               |\n| 1747197365804 |   |     return asyncio.run(async_fn(*args, **kwargs))                                                                                                                          |\n| 1747197365804 |   |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                          |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 195, in run                                                                                                    |\n| 1747197365804 |   |     return runner.run(main)                                                                                                                                                |\n| 1747197365804 |   |            ^^^^^^^^^^^^^^^^                                                                                                                                                |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 118, in run                                                                                                    |\n| 1747197365804 |   |     return self._loop.run_until_complete(task)                                                                                                                             |\n| 1747197365804 |   |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                             |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete                                                                                 |\n| 1747197365804 |   |     return future.result()                                                                                                                                                 |\n| 1747197365804 |   |            ^^^^^^^^^^^^^^^                                                                                                                                                 |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/site-packages/prefect/cli/flow_run.py\", line 375, in execute                                                                             |\n| 1747197365804 |   |     await runner.execute_flow_run(id)                                                                                                                                      |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/runner.py\", line 571, in execute_flow_run                                                                   |\n| 1747197365804 |   |     async with context:                                                                                                                                                    |\n| 1747197365804 |   |                ^^^^^^^                                                                                                                                                     |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/runner.py\", line 1542, in __aexit__                                                                         |\n| 1747197365804 |   |     await self._exit_stack.__aexit__(*exc_info)                                                                                                                            |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/contextlib.py\", line 754, in __aexit__                                                                                                   |\n| 1747197365804 |   |     raise exc_details[1]                                                                                                                                                   |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/contextlib.py\", line 737, in __aexit__                                                                                                   |\n| 1747197365804 |   |     cb_suppress = await cb(*exc_details)                                                                                                                                   |\n| 1747197365804 |   |                   ^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                   |\n| 1747197365804 |   |   File \"/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__                                                                       |\n| 1747197365804 |   |     raise BaseExceptionGroup(                                                                                                                                              |\n| 1747197365804 |   | ExceptionGroup: unhandled errors in a TaskGroup (3 sub-exceptions)                                                                                                         |\n| 1747197365804 |   +-+---------------- 1 ----------------                                                                                                                                       |\n| 1747197365804 |     | Traceback (most recent call last):                                                                                                                                       |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/runner.py\", line 973, in _cancel_run                                                                      |\n| 1747197365805 |     |     await self._mark_flow_run_as_cancelled(                                                                                                                              |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/runner.py\", line 1435, in _mark_flow_run_as_cancelled                                                     |\n| 1747197365805 |     |     await self._client.set_flow_run_state(flow_run.id, state, force=True)                                                                                                |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/prefect/client/orchestration/_flow_runs/client.py\", line 804, in set_flow_run_state                                      |\n| 1747197365805 |     |     state=state_create.model_dump(mode=\"json\", serialize_as_any=True),                                                                                                   |\n| 1747197365805 |     |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                    |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/pydantic/main.py\", line 463, in model_dump                                                                               |\n| 1747197365805 |     |     return self.__pydantic_serializer__.to_python(                                                                                                                       |\n| 1747197365805 |     |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                       |\n| 1747197365805 |     | TypeError: 'MockValSer' object cannot be converted to 'SchemaSerializer'                                                                                                 |\n| 1747197365805 |     +---------------- 2 ----------------                                                                                                                                       |\n| 1747197365805 |     | Traceback (most recent call last):                                                                                                                                       |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/runner.py\", line 973, in _cancel_run                                                                      |\n| 1747197365805 |     |     await self._mark_flow_run_as_cancelled(                                                                                                                              |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/runner.py\", line 1435, in _mark_flow_run_as_cancelled                                                     |\n| 1747197365805 |     |     await self._client.set_flow_run_state(flow_run.id, state, force=True)                                                                                                |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/prefect/client/orchestration/_flow_runs/client.py\", line 804, in set_flow_run_state                                      |\n| 1747197365805 |     |     state=state_create.model_dump(mode=\"json\", serialize_as_any=True),                                                                                                   |\n| 1747197365805 |     |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                    |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/pydantic/main.py\", line 463, in model_dump                                                                               |\n| 1747197365805 |     |     return self.__pydantic_serializer__.to_python(                                                                                                                       |\n| 1747197365805 |     |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                       |\n| 1747197365805 |     | TypeError: 'MockValSer' object cannot be converted to 'SchemaSerializer'                                                                                                 |\n| 1747197365805 |     +---------------- 3 ----------------                                                                                                                                       |\n| 1747197365805 |     | Traceback (most recent call last):                                                                                                                                       |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/runner.py\", line 973, in _cancel_run                                                                      |\n| 1747197365805 |     |     await self._mark_flow_run_as_cancelled(                                                                                                                              |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/runner.py\", line 1435, in _mark_flow_run_as_cancelled                                                     |\n| 1747197365805 |     |     await self._client.set_flow_run_state(flow_run.id, state, force=True)                                                                                                |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/prefect/client/orchestration/_flow_runs/client.py\", line 804, in set_flow_run_state                                      |\n| 1747197365805 |     |     state=state_create.model_dump(mode=\"json\", serialize_as_any=True),                                                                                                   |\n| 1747197365805 |     |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                    |\n| 1747197365805 |     |   File \"/usr/local/lib/python3.12/site-packages/pydantic/main.py\", line 463, in model_dump                                                                               |\n| 1747197365805 |     |     return self.__pydantic_serializer__.to_python(                                                                                                                       |\n| 1747197365805 |     |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                       |\n| 1747197365805 |     | TypeError: 'MockValSer' object cannot be converted to 'SchemaSerializer'                                                                                                 |\n| 1747197365805 |     +------------------------------------                                                                                                                                      |\n| 1747197365805 | An exception occurred.                                                                                                                                                         |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n```",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @jeffcarrico - thanks for the issue! do you know for sure that `2.11.4` is the actual version of `pydantic` where you're encountering this? I've seen traces like this before on lower pydantic versions"
      },
      {
        "user": "jeffcarrico",
        "body": "@zzstoatzz Yes pydantic is 2.11.4\n\nTwo more pieces of information:\n1. The server has PREFECT_RESULTS_PERSIST_BY_DEFAULT=true with a PREFECT_DEFAULT_RESULT_STORAGE_BLOCK configured\n2. This is the log when the flow completes successfully but there are post-flow errors, the AttributeError is what led me to look at #17973 and run the experiment where I reverted it out\n\n```\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|   timestamp   |                                                                              message                                                                              |\n|---------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1747253859144 | 20:17:39.139 | INFO    | prefect.flow_runs.runner - Opening process...                                                                                            |\n| 1747253860463 | 20:17:40.458 | INFO    | Flow run 'slim-aardwolf' -  > Running set_working_directory step...                                                                      |\n| 1747253860968 | 20:17:40.966 | INFO    | Flow run 'slim-aardwolf' - Beginning flow run 'slim-aardwolf' for flow 'hello-world'                                                     |\n| 1747253860969 | 20:17:40.968 | INFO    | Flow run 'slim-aardwolf' - View at https://my-prefect-server.com/prefect/runs/flow-run/868adaf8-9135-4fab-b310-fc9129622718 |\n| 1747253860969 | 20:17:40.968 | INFO    | Flow run 'slim-aardwolf' - Hello from ECS!!                                                                                              |\n| 1747253861286 | 20:17:41.284 | INFO    | Flow run 'slim-aardwolf' - Finished in state Completed()                                                                                 |\n| 1747253861509 | 20:17:41.508 | INFO    | prefect.flow_runs.runner - Process for flow run 'slim-aardwolf' exited cleanly.                                                          |\n| 1747253861570 | 20:17:41.567 | ERROR   | prefect.FlowRunCancellingObserver - Error consuming events                                                                               |\n| 1747253861570 | Traceback (most recent call last):                                                                                                                                |\n| 1747253861570 |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/_observers.py\", line 56, in __aexit__                                                              |\n| 1747253861570 |     await self._consumer_task                                                                                                                                     |\n| 1747253861570 |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/_observers.py\", line 34, in _consume_events                                                        |\n| 1747253861570 |     self.on_cancelling(flow_run_id)                                                                                                                               |\n| 1747253861570 |   File \"/usr/local/lib/python3.12/site-packages/prefect/runner/runner.py\", line 1506, in <lambda>                                                                 |\n| 1747253861570 |     on_cancelling=lambda flow_run_id: self._runs_task_group.start_soon(                                                                                           |\n| 1747253861570 |                                       ^^^^^^^^^^^^^^^^^^^^^                                                                                                       |\n| 1747253861570 | AttributeError: 'Runner' object has no attribute '_runs_task_group'. Did you mean: '_loops_task_group'?                                                           |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n```"
      },
      {
        "user": "jeffcarrico",
        "body": "I just tested with `@flow(persist_result=False)` and got the crash on startup scenario I showed the log for in the OP so I don't think results persistence is related"
      }
    ]
  },
  {
    "issue_number": 14031,
    "title": "Document which template context variables are available for automation actions",
    "author": "chrisguidry",
    "state": "open",
    "created_at": "2024-06-14T15:16:41Z",
    "updated_at": "2025-05-30T16:02:42Z",
    "labels": [
      "docs",
      "3.x"
    ],
    "body": "Our current documentation for Jinja templating in automation actions _only_ mentions the dynamic context variables (like `flow_run`, `work_pool`, etc), but these variables are shortcuts to pull objects from the API and can be problematic due to the timing gaps between the original event and when the action runs (like if the flow run has changed states since the automation was triggered, etc).\n\nhttps://docs-3.prefect.io/3.0rc/react/understand-events/automations-triggers#templating-with-jinja\n\nWe should _start_ with the standard variables:\n\n```\n{\n    \"automation\": triggered_action.automation,\n    \"event\": triggered_action.triggering_event,\n    \"labels\": LabelDiver(triggered_action.triggering_labels),\n    \"firing\": triggered_action.firing,\n    \"firings\": triggered_action.all_firings(),\n    \"events\": triggered_action.all_events(),\n}\n```\n\nAnd discuss them in detail, and _then_ we can talk about the shortcuts like `flow_run` and the tradeoffs of using them.",
    "comments": [
      {
        "user": "rcash",
        "body": "Documenting the jinja filters available as part of this would also be quite useful!"
      }
    ]
  },
  {
    "issue_number": 18186,
    "title": "Remember user UI configuration",
    "author": "roman-segador",
    "state": "open",
    "created_at": "2025-05-28T10:53:16Z",
    "updated_at": "2025-05-30T15:39:58Z",
    "labels": [
      "enhancement",
      "UI/UX"
    ],
    "body": "### Describe the current behavior\n\nThere are a few places in prefect UI where the user configuration in the UI examples are:\n- top right filter of tags\n- Filters selection in run details (flows/tasks/events/... selectors)\n\nBut there are still several places where that configuration is not persisted making the UX quite bad as we need to do the same clicks in the  UI over and over again.\n\n### Describe the proposed behavior\n\nPersists all user UI config. \nExamples:\n- Graph layout in run details:\n![Image](https://github.com/user-attachments/assets/e67aa284-77db-4ed1-baa1-1d2421aa7e11)\n- Runs list / group by configuration:\n![Image](https://github.com/user-attachments/assets/c8014ed9-4624-4495-b7ba-01429ce05a81)\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18183,
    "title": "Tags - Select from existing ones not available in some forms",
    "author": "roman-segador",
    "state": "open",
    "created_at": "2025-05-28T10:13:23Z",
    "updated_at": "2025-05-30T15:39:47Z",
    "labels": [
      "enhancement",
      "UI/UX"
    ],
    "body": "### Describe the current behavior\n\nTags are useful to help us to filter out deployments/flows/runs everywhere in the UI or for automations rules. Currently there are a few places when at the time of complete the tags in a form there is the option to multi select from the existing tags in the system. \nExamples:\n- Top right filter:\n![Image](https://github.com/user-attachments/assets/1112d418-3af8-4cdd-8c78-773445347e99)\n- Filters on Run Details:\n![Image](https://github.com/user-attachments/assets/4baf0bbd-1b7b-4a7d-9412-0999c9e57d84)\n\n### Describe the proposed behavior\n\nExtend that feature to all forms where tags are available to be completed without autocomplete. \n\nExamples:\n- Custom Automation - trigger - Flow Run State:\n![Image](https://github.com/user-attachments/assets/c4516d42-8652-462e-a458-639827f7f79c)\n- Deployment - Custom Run - Additional Options (The ones defined in the deployment are added always, but new ones cannot be autocompleted:\n![Image](https://github.com/user-attachments/assets/bc3e40ca-5678-444d-add6-591811f2c776)\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 15485,
    "title": "Add Cached state to filter tasks",
    "author": "nil-sahin",
    "state": "closed",
    "created_at": "2024-09-24T20:47:29Z",
    "updated_at": "2025-05-30T15:22:23Z",
    "labels": [
      "enhancement",
      "ui"
    ],
    "body": "### Describe the current behavior\n\nCached states cannot be selected in the Task Runs tab\n\n### Describe the proposed behavior\n\nCached states can be selected in the Task Runs tab\n\n### Example Use\n\nSelect Cached tasks\n\n### Additional context\n\nWhen we search for tasks for run states, we can't select Cached as it is not in the dropdown menu. Could you please kindly add this? It will get really handy as we rely on Cached tasks a lot in our workflows.",
    "comments": [
      {
        "user": "zhen0",
        "body": "Closed by https://github.com/PrefectHQ/prefect-ui-library/pull/2834"
      }
    ]
  },
  {
    "issue_number": 12118,
    "title": "Validation inconsistency between the UI and API in work pool environment variables",
    "author": "zhen0",
    "state": "closed",
    "created_at": "2024-02-29T19:55:07Z",
    "updated_at": "2025-05-30T15:18:34Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### First check\n\n- [X] I added a descriptive title to this issue.\n- [X] I used the GitHub search to find a similar issue and didn't find it.\n- [X] I refreshed the page and this issue still occurred.\n- [X] I checked if this issue was specific to the browser I was using by testing with a different browser.\n\n### Bug summary\n\nWhen editing a work pool from the UI it is possible to enter a boolean in the environment variable input.  It is shown as valid json and accepted by the UI.  However, the api rejects it and fails to save the work pool.  From the network tab you can find the error \"Validation failed for field 'env'. Failure reason: False is not of type 'object'\"\n\n### Reproduction\n\nCreate a work pool\r\nEnter 'True' or 'False' in the environment variables tab.  \r\nTry to save it. \n\n### Error\n\nUI approves it as valid json. \r\nApi retruns \"Validation failed for field 'env'. Failure reason: False is not of type 'object'\"\n\n### Browsers\n\n- [X] Chrome\n- [ ] Firefox\n- [ ] Safari\n- [ ] Edge\n\n### Prefect version\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "urimandujano",
        "body": "@zhen0 are you seeing this in the OSS UI?\r\n\r\nI may inadvertently provide a fix for this in https://github.com/PrefectHQ/nebula/pull/7083/commits/062a899d1e6675aa4e9c915d79a9e3f68c94ecb0 - I had to pull in some updates to our templating logic from OSS and that update specifically seems to affect environment variable resolution in templates. Let me check in on the fix once that commit is fully merged."
      },
      {
        "user": "zhen0",
        "body": "This is no longer an issue I can produce. "
      }
    ]
  },
  {
    "issue_number": 16956,
    "title": "ECS work pool display bug: An empty AwsCredentials block raises an error in the openapi schema of he work pool",
    "author": "obendidi",
    "state": "closed",
    "created_at": "2025-02-04T15:52:11Z",
    "updated_at": "2025-05-30T14:14:47Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### Bug summary\n\n\nAn image is better than a 1000 words:\n\n![Image](https://github.com/user-attachments/assets/1b7543aa-2997-4e00-b911-de2bb7dc8a00)\n\nThis happens when using an empty AwsCredentials block in the ECS work pool, by empty I mean no field is set in the block (so that it uses the IAM role of my prefect worker)\n\n### Version info\n\n```Text\nVersion:             3.1.14\nAPI version:         0.8.4\nPython version:      3.12.3\nGit commit:          5f1ebb57\nBuilt:               Thu, Jan 23, 2025 1:22 PM\nOS/Arch:             linux/x86_64\nProfile:             production\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-slack:     0.3.1\n  prefect-aws:       0.5.3\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zhen0",
        "body": "Hi @obendidi - I wasn't able to recreate this but from the error message you posted I believe it will have been fixed by https://github.com/PrefectHQ/prefect-ui-library/pull/3040.  Please do let us know if you are still seeing the issue. "
      }
    ]
  },
  {
    "issue_number": 15678,
    "title": "Base Job Configuration section (UI) of custom work pool disappears when adding a block reference",
    "author": "bjorhn",
    "state": "closed",
    "created_at": "2024-10-14T08:23:14Z",
    "updated_at": "2025-05-30T10:40:48Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### Bug summary\n\nWhen creating a work pool by running a custom worker for the first time, the work pool behaves as expected until you add a block reference to it. After saving the changes, the next time you look at the work pool, the section under \"Base Job Configuration\" will be empty:\r\n![image](https://github.com/user-attachments/assets/66e36ebb-7a2e-4187-8c36-3d83bc902f51)\r\n\r\nI'm not sure if this happens with all types of blocks, I've tried it with **AzureContainerInstanceCredentials** and **SnowflakeCredential**.\r\n\r\nThe same thing happens if you create the work pool using the `--base-job-template` argument to `prefect worker start`, pointing at a template that contains a block_document_id reference.\r\n\r\nI can confirm that the settings (including the block) do get saved correctly, since I can retrieve them with no issues when running deployments on my custom worker connected to the work pool.\r\n\r\nMRE: https://gist.github.com/bjorhn/2037a580f57b78813a7caf4419e60cfe\n\n### Version info (`prefect version` output)\n\n```Text\nVersion:             3.0.8\r\nAPI version:         0.8.4\r\nPython version:      3.11.10\r\nGit commit:          0894bad4\r\nBuilt:               Thu, Oct 10, 2024 10:17 AM\r\nOS/Arch:             linux/x86_64\r\nProfile:             local\r\nServer type:         cloud\r\nPydantic version:    2.9.2\r\nIntegrations:\r\n  prefect-azure:     0.4.0\r\n  prefect-dbt:       0.6.1\r\n  prefect-shell:     0.3.0\r\n  prefect-snowflake: 0.28.0\n```\n\n\n### Additional context\n\n@zzstoatzz confirmed in slack that this issue happens in both Prefect Cloud and the open-source server.",
    "comments": [
      {
        "user": "bjorhn",
        "body": "I saw someone having a similar problem with Kubernetes work pools (https://prefect-community.slack.com/archives/CL09KU1K7/p1747369107595709), perhaps it's related? I still have this problem when running the Snowpark Container Services worker, even after merging my PR into Prefect. I think we can rule out this only happening with custom workers."
      },
      {
        "user": "zhen0",
        "body": "I think this may be the same issue as [https://github.com/PrefectHQ/prefect/issues/13189](https://github.com/PrefectHQ/prefect/issues/13189) though I'm not able to reproduce that. "
      },
      {
        "user": "bjorhn",
        "body": "It does seem related. I can get the Defaults and Advanced base job template sections to show using the resizing technique described in #13189.\n\nIn my case (with the Snowpark Container Services work pool), none of the actual values set on the work pool are filled in though, all input boxes are empty, block credentials looks like they haven't been selected, etc."
      }
    ]
  },
  {
    "issue_number": 18185,
    "title": "Prefect Managed Work Pool - Improve Information on how hours are consumed",
    "author": "roman-segador",
    "state": "open",
    "created_at": "2025-05-28T10:42:27Z",
    "updated_at": "2025-05-30T07:19:44Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nCurrently, the only information available about the prefect managed work pool consumption is this summary (which can be retrieved though the API too):\n\n![Image](https://github.com/user-attachments/assets/e2b1a587-6e6f-4b2e-9183-1e0192c26a27)\n\nWhile this summary if good and needed, there is a lack of transparence on how those hours are calculated, as no further details are available.\n\nOnly way to guess the consumption is from the duration in run details (runs / group by work pool / run details). Main issue there is that the duration displayed do not match the time consumed. That duration is calculated as the time difference between `prefect.flow-run.Running` and `prefect.flow-run.Completed` statuses. That metric is useful to analyze the performance of our flows but not for this use case. If I'm not wrong, the computed time for the managed work pool includes also the time between the events `prefect.flow-run.Pending` and `prefect.flow-run.Running` which I guess is the the time needed for the worker to initialise and be ready with all the defined dependencies.\n\n![Image](https://github.com/user-attachments/assets/b42903fd-adc3-4958-9d45-25c7f13a9700)\n\n### Describe the proposed behavior\n\n1. Improve the documentation to detail how the time is consumed (from which event to which event, if it is rounding to the second, to the minute, etc.)\n2. Add a new field in Run details, towards the current duration one or towards the `Work Pool` when a run is executed in a managed worker with the seconds consumed by that run. Something like: \n![Image](https://github.com/user-attachments/assets/a7543c68-9fd7-4e99-adff-e50fd5585bb1)\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "jakekaplan",
        "body": "Hi @roman-segador thanks for the issue and the call out. That is definitely something should be documented and I think possibly was inadvertently removed at some point. I've opened a [PR](https://github.com/PrefectHQ/prefect/pull/18194) to correct that. Copying here what I've added:\n\n> Every account has a compute usage limit per workspace that resets monthly. Compute used is defined as the duration\n> between compute startup and teardown, rounded up to the nearest minute. This is approximately, although not exactly,\n> the duration of a flow run going from `PENDING` to `COMPLETED`.\n\nHappy to answer any more questions about that if I can. Additionally including compute usage per flow run is something we can definitely explore adding."
      },
      {
        "user": "roman-segador",
        "body": "Thanks @jakekaplan for the quick response. I think is more clear now with that comment. \nAlso, knowing now that in the definition there are round ups to minutes and that, with the current information is just `approximately` between two values present in the UI, I think the time consumed per run is something needed in the runs."
      }
    ]
  },
  {
    "issue_number": 17303,
    "title": "Unable to create a deployment with null pull steps",
    "author": "masonmenges",
    "state": "open",
    "created_at": "2025-02-27T18:31:31Z",
    "updated_at": "2025-05-29T23:40:36Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nTrying to create a deployment with null pull steps in a prefect yaml file results in a validation error, this doesn't seem to affect the build or push steps.\n\nError:  \n```\n1 validation error for DeploymentUpdate\npull_steps.0\n  Input should be a valid dictionary \n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\n```\n\n### Version info\n\n```Text\nVersion:             3.2.7\nAPI version:         0.8.4\nPython version:      3.12.4\nGit commit:          d4d9001e\nBuilt:               Fri, Feb 21, 2025 7:39 PM\nOS/Arch:             darwin/arm64\nProfile:             masonsandbox\nServer type:         cloud\nPydantic version:    2.10.6\nIntegrations:\n  prefect-dask:      0.3.2\n  prefect-snowflake: 0.28.0\n  prefect-slack:     0.3.0\n  prefect-gcp:       0.6.2\n  prefect-aws:       0.5.0\n  prefect-gitlab:    0.3.1\n  prefect-dbt:       0.6.4\n  prefect-docker:    0.6.1\n  prefect-sqlalchemy: 0.5.1\n  prefect-shell:     0.3.1\n```\n\n### Additional context\n\nexample yaml\n```\nbuild:\n- prefect_docker.deployments.steps.build_docker_image:\n    id: build_image\n    requires: prefect-docker>=0.3.1\n    image_name: IMAGENAME\n    tag: TAG\n    dockerfile: auto\n    platform: linux/amd64\n\n# push section allows you to manage if and how this project is uploaded to remote locations\npush:\n- prefect_docker.deployments.steps.push_docker_image:\n    requires: prefect-docker>=0.3.1\n    image_name: \"{{ build_image.image_name }}\"\n    tag: \"{{ build_image.tag }}\"\n\n# pull section allows you to provide instructions for cloning this project in remote locations\npull: null\n```",
    "comments": [
      {
        "user": "cicdw",
        "body": "Assuming you are trying to set no pull steps, you can avoid this by setting `pull: {}`; we can probably update the `DeploymentUpdate` schema to account for this also."
      },
      {
        "user": "masonmenges",
        "body": "I did actually try with an empty dictionary and ran into the same issue, I ran through a few different attempts to get the deployment to create, It only worked for me if I passed \"something\" into the pull steps even just a dummy validation step like this \n\n```\npull:\n- prefect.deployments.steps.run_shell_script:\n      id: test\n      script: ls\n      stream_output: True\n```\n\n`pull: null` and `pull: {}` both triggered the same validation error for me."
      },
      {
        "user": "cicdw",
        "body": "oh sorry I meant `pull: []` @masonmenges "
      }
    ]
  },
  {
    "issue_number": 18161,
    "title": "Prefect caching activated when setting cache_policy",
    "author": "OliverKleinBST",
    "state": "closed",
    "created_at": "2025-05-24T15:14:40Z",
    "updated_at": "2025-05-29T20:56:08Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen I define a task like\n\n```\n@task(name=\"Extract Task\", cache_policy=TASK_SOURCE)\ndef extract(x):\n    logger = get_run_logger()\n    logger.info(\"inside extract task\")\n    return x + 1\n```\ncaching is automatically activated, even though I do not define persist_result=True or set PREFECT_RESULTS_PERSIST_BY_DEFAULT=true\n\nWhat I want to achieve is that I can control caching via\nPREFECT_RESULTS_PERSIST_BY_DEFAULT=true / false\n\nto allow me to globally enable or disable caching - while explicitly not setting persists_result in the decorator.\nThis is currently not possible as a cache_policy setting activates the caching implicitly.\n\n### Version info\n\n```Text\nVersion:             3.3.7\nAPI version:         0.8.4\nPython version:      3.10.13\nGit commit:          8f86aaee\nBuilt:               Mon, Apr 28, 2025 03:04 PM\nOS/Arch:             win32/AMD64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.10.6\nServer:\n  Database:          sqlite\n  SQLite version:    3.41.2\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @OliverKleinBST - thanks for the issue! currently, explicitly setting a `cache_policy` is meant to enable caching.\n\n\n> What I want to achieve is that I can control caching via\nPREFECT_RESULTS_PERSIST_BY_DEFAULT=true / false\n\n> to allow me to globally enable or disable caching - while explicitly not setting persists_result in the decorator.\nThis is currently not possible as a cache_policy setting activates the caching implicitly.\n\n\nIt is possible to globally enable/disable caching with an env var (`PREFECT_RESULTS_PERSIST_BY_DEFAULT`) without setting `persist_result` in the decorator, you just cannot also explicitly set a `cache_policy`\n\nif you wanted to do something custom based on another setting or env var, you could use a `CacheKeyFnPolicy` that reads your desired flag at runtime, but generally that's not needed, so feel free to share your motivation here\n"
      },
      {
        "user": "OliverKleinBST",
        "body": "Hi @zzstoatzz \n\nMy motivation is that I need a cache_policy when caching is enabled to exclude one parameter that is not serializable.\n`Like in mytask = task(cache_policy=DEFAULT - \"param\")(function_name)`\n\nThis is done when I e.g. run the flow based on some event where the cache can get of relevance.\n\nHowever, i also run the flow in a automation framework at scale (like hundreds of variations), where I dont need or want caching.\nAnd my thought was I could just disable it with the environment variable assuming that the cache policy setting would then just ignored.\n\nAlternatively of course I could probably create the task definition at runtime based on the env var\n\n```\nif cache:\n   simulate_task = task(cache_policy=DEFAULT - \"logger\")(simulate)\nelse:\n   simulate_task = task()(simulate)\n\n```\nwhich felt redundant to me.\n"
      },
      {
        "user": "zzstoatzz",
        "body": "hi @OliverKleinBST - that makes sense. I think we can introduce a `PREFECT_TASKS_DISABLE_CACHE` setting that will unequivocally disable caching, regardless of selected `cache_policy` - how does that sound?"
      }
    ]
  },
  {
    "issue_number": 18199,
    "title": "@task retry_delay_seconds type error for floats with fractional parts",
    "author": "OnurKerimoglu",
    "state": "closed",
    "created_at": "2025-05-29T08:38:45Z",
    "updated_at": "2025-05-29T15:41:09Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nDescription: \nOn a fresh install (see the version info below), argument for the task decorator `retry_delay_seconds` does not accept floats with fractional parts: it works with any integers (e.g., 1), any float without fractional part (e.g., 1.0) but not with a float that has a fractional part (e.g., 1.1, or 0.1).\n\nCode:\n```\nfrom prefect import task, flow\n\n@task(retries=2, retry_delay_seconds=1.1, log_prints=True)\ndef print_plus_one(x):\n    return x+1\n\n@flow\ndef main_flow(\n    x: int):\n    x = print_plus_one(x)\n    print (f'incremented value: x: {x}')\n\n\nif __name__ == '__main__':\n    main_flow(x=1)\n```\n\nTraceback:\n```\n10:16:26.604 | INFO    | Flow run 'speedy-caiman' - Beginning flow run 'speedy-caiman' for flow 'main-flow'\n10:16:26.613 | INFO    | Flow run 'speedy-caiman' - View at http://127.0.0.1:4200/runs/flow-run/b26e8cae-ec15-4ff3-b3c0-26917246e473\n10:16:26.635 | ERROR   | Flow run 'speedy-caiman' - Encountered exception during execution: 2 validation errors for TaskRunPolicy\nretry_delay.int\n  Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=1.1, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_from_float\nretry_delay.list[int]\n  Input should be a valid list [type=list_type, input_value=1.1, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type\nTraceback (most recent call last):\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flow_engine.py\", line 763, in run_context\n    yield self\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flow_engine.py\", line 1370, in run_flow_sync\n    engine.call_flow_fn()\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flow_engine.py\", line 783, in call_flow_fn\n    result = call_with_parameters(self.flow.fn, self.parameters)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n    return fn(*args, **kwargs)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/03-orchestration/prefect_bug.py\", line 10, in main_flow\n    x = print_plus_one(x)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/tasks.py\", line 1063, in __call__\n    return run_task(\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/task_engine.py\", line 1636, in run_task\n    return run_task_sync(**kwargs)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/task_engine.py\", line 1417, in run_task_sync\n    with engine.start(task_run_id=task_run_id, dependencies=dependencies):\n  File \"/home/onur/.pyenv/versions/3.9.15/lib/python3.9/contextlib.py\", line 119, in __enter__\n    return next(self.gen)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/task_engine.py\", line 750, in start\n    with self.initialize_run(task_run_id=task_run_id, dependencies=dependencies):\n  File \"/home/onur/.pyenv/versions/3.9.15/lib/python3.9/contextlib.py\", line 119, in __enter__\n    return next(self.gen)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/task_engine.py\", line 669, in initialize_run\n    self.task_run = run_coro_as_sync(\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/utilities/asyncutils.py\", line 207, in run_coro_as_sync\n    return call.result()\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/_internal/concurrency/calls.py\", line 365, in result\n    return self.future.result(timeout=timeout)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/_internal/concurrency/calls.py\", line 192, in result\n    return self.__get_result()\n  File \"/home/onur/.pyenv/versions/3.9.15/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/_internal/concurrency/calls.py\", line 441, in _run_async\n    result = await coro\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/utilities/asyncutils.py\", line 188, in coroutine_wrapper\n    return await task\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/tasks.py\", line 975, in create_local_run\n    empirical_policy=TaskRunPolicy(\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/pydantic/main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\npydantic_core._pydantic_core.ValidationError: 2 validation errors for TaskRunPolicy\nretry_delay.int\n  Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=1.1, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_from_float\nretry_delay.list[int]\n  Input should be a valid list [type=list_type, input_value=1.1, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type\n10:16:26.677 | INFO    | Flow run 'speedy-caiman' - Finished in state Failed('Flow run encountered an exception: ValidationError: 2 validation errors for TaskRunPolicy\\nretry_delay.int\\n  Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=1.1, input_type=float]\\n    For further information visit https://errors.pydantic.dev/2.11/v/int_from_float\\nretry_delay.list[int]\\n  Input should be a valid list [type=list_type, input_value=1.1, input_type=float]\\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type')\nTraceback (most recent call last):\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/03-orchestration/prefect_bug.py\", line 15, in <module>\n    main_flow(x=1)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flows.py\", line 1691, in __call__\n    return run_flow(\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flow_engine.py\", line 1527, in run_flow\n    ret_val = run_flow_sync(**kwargs)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flow_engine.py\", line 1372, in run_flow_sync\n    return engine.state if return_type == \"state\" else engine.result()\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flow_engine.py\", line 350, in result\n    raise self._raised\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flow_engine.py\", line 763, in run_context\n    yield self\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flow_engine.py\", line 1370, in run_flow_sync\n    engine.call_flow_fn()\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/flow_engine.py\", line 783, in call_flow_fn\n    result = call_with_parameters(self.flow.fn, self.parameters)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n    return fn(*args, **kwargs)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/03-orchestration/prefect_bug.py\", line 10, in main_flow\n    x = print_plus_one(x)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/tasks.py\", line 1063, in __call__\n    return run_task(\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/task_engine.py\", line 1636, in run_task\n    return run_task_sync(**kwargs)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/task_engine.py\", line 1417, in run_task_sync\n    with engine.start(task_run_id=task_run_id, dependencies=dependencies):\n  File \"/home/onur/.pyenv/versions/3.9.15/lib/python3.9/contextlib.py\", line 119, in __enter__\n    return next(self.gen)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/task_engine.py\", line 750, in start\n    with self.initialize_run(task_run_id=task_run_id, dependencies=dependencies):\n  File \"/home/onur/.pyenv/versions/3.9.15/lib/python3.9/contextlib.py\", line 119, in __enter__\n    return next(self.gen)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/task_engine.py\", line 669, in initialize_run\n    self.task_run = run_coro_as_sync(\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/utilities/asyncutils.py\", line 207, in run_coro_as_sync\n    return call.result()\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/_internal/concurrency/calls.py\", line 365, in result\n    return self.future.result(timeout=timeout)\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/_internal/concurrency/calls.py\", line 192, in result\n    return self.__get_result()\n  File \"/home/onur/.pyenv/versions/3.9.15/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/_internal/concurrency/calls.py\", line 441, in _run_async\n    result = await coro\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/utilities/asyncutils.py\", line 188, in coroutine_wrapper\n    return await task\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/prefect/tasks.py\", line 975, in create_local_run\n    empirical_policy=TaskRunPolicy(\n  File \"/home/onur/WORK/DS/repos/MLOps/nytaxi_mlops/.venv/lib/python3.9/site-packages/pydantic/main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\npydantic_core._pydantic_core.ValidationError: 2 validation errors for TaskRunPolicy\nretry_delay.int\n  Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=1.1, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_from_float\nretry_delay.list[int]\n  Input should be a valid list [type=list_type, input_value=1.1, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type\n```\n\nSuggestion: apparently the int_from_float  error (`'Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=1.1, input_type=float]`) is a giveaway for Pydantic V1. Indeed, I see that in prefect.flows, there are imports from pydantic.v1, which may explain the problem.\n\n### Version info\n\n```Text\nVersion:             3.4.3\nAPI version:         0.8.4\nPython version:      3.9.15\nGit commit:          1c2ba7a4\nBuilt:               Thu, May 22, 2025 10:00 PM\nOS/Arch:             linux/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.5\nIntegrations:\n  prefect-aws:       0.5.10\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @OnurKerimoglu - thanks for the issue!\n\nthis should be resolved by the PR linked above and will be released later today"
      }
    ]
  },
  {
    "issue_number": 18191,
    "title": "Slow dask task creation for complex functions",
    "author": "bnaul",
    "state": "open",
    "created_at": "2025-05-28T15:52:04Z",
    "updated_at": "2025-05-29T14:48:21Z",
    "labels": [
      "bug",
      "integrations"
    ],
    "body": "### Bug summary\n\nI've been seeing very slow task creation (~1/s) when `.map`ing a classmethod from a not particularly crazy class. Using plain old Dask `client.map()` on the same function creates the tasks several hundred times faster. For a reproducible example I created a more monstrous method that is somewhat slow even for Dask's `.map` but incredibly slow for Prefect's. Obviously this is a fairly pathological case but hopefully it's useful for diagnosing where the gap between the pure Dask and Prefect+Dask performance is coming from.\n\n```python\nimport time\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List\n\nfrom prefect import flow, task\nfrom prefect.context import FlowRunContext\nfrom prefect_dask import DaskTaskRunner\n\n\n@dataclass\nclass Node:\n    \"\"\"A node in our complex object tree\"\"\"\n\n    id: str\n    data: Dict[str, Any]\n    children: List['Node']\n    parent_ref: Any\n    closures: List[Any]\n\n\nclass ComplexDataProcessor:\n    def __init__(self, depth=5, breadth=5):\n        # Create deeply nested tree with circular references\n        self.root = self._build_tree(depth, breadth)\n        self.root.parent_ref = self\n\n        # Add lots of closures that capture self\n        self.closures = []\n        for i in range(100):\n            captured_data = f\"data_{i}\" * 100\n\n            def make_closure(n, obj=self):\n                def inner(x):\n                    return x + n + len(obj.root.id) + len(captured_data)\n                return inner\n            self.closures.append(make_closure(i))\n\n        # Add lambdas with captured state\n        self.lambdas = {\n            f\"lambda_{i}\": lambda x, i=i, obj=self: x + i + len(obj.root.data)\n            for i in range(100)\n        }\n\n        # Add recursive/circular references\n        self.recursive_dict = {'self': self, 'root': self.root}\n        for i in range(50):\n            self.recursive_dict[f'level_{i}'] = {\n                'parent': self.recursive_dict,\n                'processor': self,\n                'data': {'nested': self.recursive_dict},\n                'more_refs': [self.recursive_dict for _ in range(10)]\n            }\n\n        # Add regular classes with complex initialization\n        self.complex_objects = []\n        for i in range(50):\n            obj = type('ComplexObj', (), {\n                'value': i,\n                'get_processor': lambda self, p=self: p,\n                'compute': lambda self, x, p=self: x + len(p.root.id)\n            })()\n            obj.processor_ref = self\n            obj.recursive_ref = self.recursive_dict\n            self.complex_objects.append(obj)\n\n        # Add custom objects with methods\n        class CustomObj:\n            def __init__(self, processor, index):\n                self.processor = processor\n                self.index = index\n                self.data = {'proc': processor, 'idx': index}\n\n            def compute(self, x):\n                return x + self.index + len(self.processor.root.id)\n\n        self.custom_objects = [CustomObj(self, i) for i in range(50)]\n\n        # Add nested functions\n        def outer_func(x):\n            def middle_func(y):\n                def inner_func(z):\n                    return x + y + z + len(self.root.id)\n                return inner_func\n            return middle_func\n\n        self.nested_funcs = [outer_func(i) for i in range(50)]\n\n    def _build_tree(self, depth, breadth, parent=None):\n        \"\"\"Build a tree with lots of complex references\"\"\"\n        node_id = f\"node_depth{depth}_{'x' * 50}\"\n\n        # Create closures specific to this node\n        node_closures = []\n        for i in range(10):\n            def make_node_closure(n, node_id=node_id):\n                def closure(x):\n                    return x + n + len(node_id)\n                return closure\n            node_closures.append(make_node_closure(i))\n\n        node = Node(\n            id=node_id,\n            data={\n                'level': depth,\n                'static_data': {f'key_{i}': i * 0.1 for i in range(100)},\n                'text': 'x' * 5000,\n                'nested_dict': {str(i): {str(j): i*j for j in range(50)} for i in range(50)}\n            },\n            children=[],\n            parent_ref=parent,\n            closures=node_closures\n        )\n\n        if depth > 0:\n            node.children = [\n                self._build_tree(depth - 1, breadth, parent=node)\n                for _ in range(breadth)\n            ]\n\n        return node\n\n    def process_item(self, item):\n        \"\"\"Method without @task decorator for direct client.map comparison\"\"\"\n        return item * 2\n\n    @task\n    def process_item_task(self, item):\n        \"\"\"Same method but with @task decorator\"\"\"\n        return item * 2\n\n\n@flow(task_runner=DaskTaskRunner())\ndef test_task_creation_methods():\n    # Get the Dask client from the task runner\n    context = FlowRunContext.get()\n    client = context.task_runner.client\n\n    # Create a complex processor instance\n    print(\"Creating VERY complex processor...\")\n    processor = ComplexDataProcessor(depth=4, breadth=4)\n\n    # Generate items to process\n    items = list(range(10))  # Fewer items because it's so slow\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"COMPARISON: Complex class method\")\n    print(\"=\"*60)\n\n    # Method 1: Direct client.map (FAST)\n    print(\"\\n1. Using client.map directly...\")\n    start_time = time.time()\n\n    futures = client.map(processor.process_item, items)\n\n    client_map_time = time.time() - start_time\n    print(f\"   Time: {client_map_time:.3f} seconds\")\n\n    # Method 2: Using @task decorator (VERY SLOW)\n    print(\"\\n2. Using @task decorator...\")\n    start_time = time.time()\n\n    task_results = processor.process_item_task.map(items)\n\n    task_map_time = time.time() - start_time\n    print(f\"   Time: {task_map_time:.3f} seconds\")\n    print(f\"   Slowdown: {task_map_time / client_map_time:.1f}x\")\n\n    # Wait for results\n    print(\"\\nGathering results...\")\n    results1 = client.gather(futures)\n    results2 = task_results.result()\n\n    print(f\"Results match: {results1 == results2}\")\n\n\n# Show the serialization overhead directly\ndef demonstrate_serialization_issue():\n    import cloudpickle\n\n    print(\"Demonstrating why it's slow...\\n\")\n\n    processor = ComplexDataProcessor(depth=3, breadth=3)\n\n    print(\"Serializing the complex processor object:\")\n    start = time.time()\n    serialized = cloudpickle.dumps(processor)\n    print(f\"  Serialization time: {time.time() - start:.3f} seconds\")\n    print(f\"  Serialized size: {len(serialized) / (1024*1024):.1f} MB\")\n\n\nif __name__ == \"__main__\":\n    # First show why it's slow\n    demonstrate_serialization_issue()\n\n    print(\"\\n\" + \"=\"*60 + \"\\n\")\n\n    # Run the comparison\n    test_task_creation_methods()\n```\nOutput:\n```\nDemonstrating why it's slow...\n\nSerializing the complex processor object:\n  Serialization time: 0.017 seconds\n  Serialized size: 1.1 MB\n\n============================================================\n\n11:37:50.076 | INFO    | Flow run 'archetypal-quoll' - Beginning flow run 'archetypal-quoll' for flow 'test-task-creation-methods'\n11:37:50.085 | INFO    | Flow run 'archetypal-quoll' - View at https://app.prefect.cloud/account/1e4d7e04-0fb7-4aa3-8ef5-746e9f404f4f/workspace/849a6829-4afb-48c3-9cc2-2dc6b262fd9c/runs/flow-run/068372dc-dd1c-7a75-8000-0f83470e14de\n11:37:50.086 | INFO    | prefect.task_runner.dask - Creating a new Dask cluster with `distributed.deploy.local.LocalCluster`\n11:37:50.364 | INFO    | distributed.http.proxy - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n11:37:50.380 | INFO    | distributed.scheduler - State start\n11:37:50.383 | INFO    | distributed.scheduler -   Scheduler at:     tcp://127.0.0.1:51524\n11:37:50.383 | INFO    | distributed.scheduler -   dashboard at:  http://127.0.0.1:8787/status\n...\n11:37:51.698 | INFO    | distributed.core - Starting established connection to tcp://127.0.0.1:51548\n11:37:51.699 | INFO    | prefect.task_runner.dask - The Dask dashboard is available at http://127.0.0.1:8787/status\nCreating VERY complex processor...\n\n============================================================\nCOMPARISON: Complex class method\n============================================================\n\n1. Using client.map directly...\n   Time: 3.206 seconds\n\n2. Using @task decorator...\n11:38:18.586 | INFO    | Task run 'process_item_task-2ba' - Finished in state Completed()\n11:38:32.258 | INFO    | Task run 'process_item_task-ed5' - Finished in state Completed()\n...\n   Time: 274.437 seconds\n   Slowdown: 85.6x\n\nGathering results...\nResults match: True\n\n11:42:45.356 | INFO    | Flow run 'archetypal-quoll' - Finished in state Completed()\n```\n\n### Version info\n\n```Text\nVersion:             3.4.2\nAPI version:         0.8.4\nPython version:      3.10.16\nGit commit:          c3c1c119\nBuilt:               Mon, May 19, 2025 04:04 PM\nOS/Arch:             darwin/arm64\nProfile:             default\nServer type:         cloud\nPydantic version:    2.9.2\nIntegrations:\n  prefect-kubernetes: 0.6.1\n  prefect-dask:      0.3.5\n  prefect-gcp:       0.6.4\n\n\n\ndask, version 2025.5.1\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "bnaul",
        "body": "I tried registering a custom tokenizer function with dask and that seemed to speed things up by about 3x (not claiming this is necessarily a legit implementation, just a first attempt)\n```python\n    @dask.tokenize.normalize_token.register(prefect.tasks.Task)\n    def tokenize_task(task):\n        \"\"\"Custom tokenizer for Prefect tasks. Speeds up creation of high volume mapped tasks.\"\"\"\n        return (\n            dask.tokenize.normalize_token(prefect.tasks.Task),\n            task.task_key,\n            task.name,\n            task.version or \"\",\n            getattr(task.fn, \"__code__\", None) and task.fn.__code__.co_code.hex(),\n            frozenset(task.tags or set()),\n        )\n```\nSame flow:\n```\n   Time: 84.910 seconds\n   Slowdown: 28.6x\n```\n(previously 274.437 seconds, 85.6x)"
      },
      {
        "user": "zzstoatzz",
        "body": "thank you @bnaul ! this is useful and we'll explore this as time allows"
      }
    ]
  },
  {
    "issue_number": 15945,
    "title": "Job State Stuck in Running After Pod Eviction",
    "author": "hpapazov",
    "state": "open",
    "created_at": "2024-11-07T18:15:23Z",
    "updated_at": "2025-05-29T06:14:36Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhile testing Prefect Worker with prefect-kubernetes 0.4.6, I observed a change in how the package handles pod eviction compared to prefect-kubernetes 0.4.3. In version 0.4.3, when a pod is evicted, the job transitions into a \"Crashed\" state. However, in prefect-kubernetes 0.4.6 version, when a pod is evicted, the job remains stuck in a \"Running\" state. I had to manually suspend the job after observing it in this state for an extended period.\r\n\r\nRelated Issue: In the past, we raised a similar [issue **](https://github.com/PrefectHQ/prefect/issues/12988)regarding** pod eviction handling, which was addressed in version 0.4.3.\n\n### Version info\n\n```Text\nprefect-kubernetes 0.4.6\r\nKubernetes Cluster versions tested:\r\n- 1.28.5\r\n- 1.29.6\n```\n\n\n### Additional context\n\n**To reproduce the issue:**\r\n- Deploy a worker with prefect-kubernets 0.4.6.\r\n- Manually evict the pod associated with the job (using the provided script below).\r\n- Observe the job status remaining \"Running\" instead of transitioning to \"Crashed.\"\r\n\r\n**Script we used for pod eviction test:**\r\n\r\n```python\r\n# Install kubernetes package\r\n# pip install kubernetes==31.0.0\r\n\r\nfrom kubernetes import client, config\r\n\r\nk8s_config_file = \"/home/vscode/.kube/config\"\r\ncluster = \"<YOUR CLUSTER>\"\r\nnamespace = \"<YOUR NAMESPACE>\"\r\npod_name = \"<YOUR POD NAME>\"\r\n\r\nconfig.load_kube_config(config_file=k8s_config_file, context=cluster)\r\nv1 = client.CoreV1Api()\r\nv1.create_namespaced_pod_eviction(\r\n    name=pod_name,\r\n    namespace=namespace,\r\n    body=client.V1Eviction(metadata=client.V1ObjectMeta(name=pod_name))\r\n)\r\n```\r\n\r\n**Logs:**\r\nEvicting pod with worker running prefect-kubernetes 0.4.6\r\n![image](https://github.com/user-attachments/assets/6c9a1a88-9d60-4735-91cb-489b372153b3)\r\n\r\nEvicting pod with worker running prefect-kubernetes 0.4.3\r\n![image](https://github.com/user-attachments/assets/ad85e2d6-3b6f-4b7d-888b-c23a0f039a62)\r\n",
    "comments": [
      {
        "user": "rasharab",
        "body": "So is anyone looking into this / or is there a plan forward?\nWe hit this issue today as well."
      }
    ]
  },
  {
    "issue_number": 18187,
    "title": "prefect_docker: Error if manually adding extra_hosts to container_create_kwargs",
    "author": "davidesba",
    "state": "closed",
    "created_at": "2025-05-28T11:18:14Z",
    "updated_at": "2025-05-28T18:02:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI need docker flow_run to be able to access docker host network in my worker. In order to do that i added the following config in the job variables configuration:\n\n```\n  \"container_create_kwargs\": {\n    \"extra_hosts\": [\n      \"host.docker.internal:host-gateway\"\n    ]\n  }\n```\n\nWhen running the flow i get the following error:\n\n```\nFailed to submit flow run '6f05f65f-3f41-4ef1-82c6-5339b315bd11' to infrastructure.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/workers/base.py\", line 1281, in _submit_run_and_capture_errors\n    result = await self.run(\n             ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect_docker/worker.py\", line 447, in run\n    container, created_event = await run_sync_in_worker_thread(\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/utilities/asyncutils.py\", line 233, in run_sync_in_worker_thread\n    result = await anyio.to_thread.run_sync(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 859, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect/utilities/asyncutils.py\", line 243, in call_with_mark\n    return call()\n           ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect_docker/worker.py\", line 691, in _create_and_start_container\n    container_settings = self._build_container_settings(\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/prefect_docker/worker.py\", line 661, in _build_container_settings\n    return dict(\n           ^^^^^\nTypeError: dict() got multiple values for keyword argument 'extra_hosts'\n```\n\nIt seems extra_hosts is already added if PREFECT_API_URL is not present or if it is pointing to a localhost, but this behavior cannot be overwritten (In my case is pointing to a server):\n\n```\nif sys.platform == \"linux\" and (\n            # Do not warn if the user has specified a host manually that does not use\n            # a local address\n            \"PREFECT_API_URL\" not in self.env\n            or re.search(\n                \".*(localhost)|(127.0.0.1)|(host.docker.internal).*\",\n                self.env[\"PREFECT_API_URL\"],\n            )\n        )\n```\n\n\n### Version info\n\n```Text\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.11.9\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.10.6\nServer:\n  Database:          postgresql\nIntegrations:\n  prefect-docker:    0.6.4\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18184,
    "title": "Prefect crashes my services",
    "author": "slavaGanzin",
    "state": "closed",
    "created_at": "2025-05-28T10:30:44Z",
    "updated_at": "2025-05-28T17:51:10Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nHello, I like prefect very much and it's a huge step up from airflow-like garbage.\n\nBut there is a problem with python \"let it fall\" approach, because in contrast with [Erlang were this idea emerged](https://medium.com/@vamsimokari/erlang-let-it-crash-philosophy-53486d2a6da), Python doesn't work on the actors and messages concept because it is tragically synchronous (hello GIL and \"asynchronous\" asyncio library).\n\nAnd also there is a problem with the fact that prefect is observability platform, it observes if something is working or not and that's it. You may think that by wrapping it into flow and task, you became a task workflow orchestration mumbo jumbo, but it's not: my code run some tasks, my code do some job, prefect - observe.\n\nSo in general, I don't care that much if prefect \"Couln't authorize\", \"You limit is reached\" or any other problems with prefect, if my tasks continue working. But I found out, that prefect have such inflated ego, that it crashes my services, because it couldn't login. Are you serious?\n\nAnd you know what happens? I cannot get alerts from my prefect because my service couldn't log in to indicate that it failed. So I need to periodically check manually if my jobs are working fine. Then I may ask, for what reason do I have an observability tool.\n\nMaybe if one of my services couldn't login, you may show me that in prefect UI and not silently crash my service? Just a suggestion\n\n\nYou need to think about it. But here is a quick fix.\n\n1. Stop throwing exceptions outside if prefect cannot authorize, synchronize, find closing part of a log entry, show warnings in log and that's it\n2. If prefect, couldn't authorize, synchronize or dance inside itself in the way it likes, continue working as everything was fine. I still need my jobs to working properly\n\nYou are digging a hole under yourself, unreliable workflow orchestration is worse than having none. I recommended prefect numerous times as a go to substitution for airflow and others, and now I feel uneasy about it\n\n### Version info\n\n```Text\nI don't use prefect locally on my machine, but it is what it is\n\n$ prefect version\n\nTraceback (most recent call last):\n  File \"/Users/slava/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/prefect/cli/_utilities.py\", line 44, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/slava/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/prefect/cli/_types.py\", line 156, in sync_fn\n    return asyncio.run(async_fn(*args, **kwargs))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/slava/.asdf/installs/python/3.12.4/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/slava/.asdf/installs/python/3.12.4/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/slava/.asdf/installs/python/3.12.4/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/slava/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/prefect/cli/root.py\", line 142, in version\n    integrations = get_prefect_integrations()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/slava/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/prefect/cli/root.py\", line 155, in get_prefect_integrations\n    if dist.metadata[\"Name\"].startswith(\"prefect-\"):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'startswith'\nAn exception occurred.\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 13189,
    "title": "After adding a block as a job variable default to a work pool, its base job template does not load on the Edit page unless I resize to a different breakpoint",
    "author": "abrookins",
    "state": "closed",
    "created_at": "2024-05-01T17:37:39Z",
    "updated_at": "2025-05-28T16:14:03Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### First check\r\n\r\n- [X] I added a descriptive title to this issue.\r\n- [X] I used the GitHub search to find a similar issue and didn't find it.\r\n- [X] I refreshed the page and this issue still occurred.\r\n- [X] I checked if this issue was specific to the browser I was using by testing with a different browser.\r\n\r\n### Bug summary\r\n\r\nIf I add a block as a job variable default to a work pool's base job template (using the \"Advanced\" tab), the form saves successfully, but when I edit the work pool again, its base job template section doesn't load (neither \"Defaults\" nor \"Advanced\"). However, if I resize the window to trigger another breakpoint, the page redraws with that section loaded.\r\n\r\n### Reproduction\r\n\r\n1. Create a JSON block with one field, `{\"name\": \"bob\"}`, save the block ID\r\n2. Create a new Local Process work pool\r\n3. Choose next and then the Advanced tab\r\n4. Add an entry to the \"variables\" object in the JSON that references the block:\r\n\r\n```\r\n                \"user\": {\r\n                    \"title\": \"User\",\r\n                    \"description\": \"user details\",\r\n                    \"type\": \"object\",\r\n                    \"additionalProperties\": {\"type\": \"string\"},\r\n                    \"default\": {\"$ref\": {\"block_document_id\": \"block ID\"}}\r\n                }\r\n```\r\n5. Save\r\n6. Click the triple dot menu then \"Edit\"\r\n7. The base job template shouldn't load\r\n8. Resize the window (I make mine very small) and witness the form appear\r\n\r\n### Error\r\n\r\nI don't see any obviously related console errors. \r\n\r\nAfter saving and returning to the Edit page:\r\n<img width=\"1138\" alt=\"image\" src=\"https://github.com/PrefectHQ/prefect/assets/97182/2cda861f-5a3e-457a-873e-008770e77ea1\">\r\n\r\nAfter resizing:\r\n<img width=\"528\" alt=\"image\" src=\"https://github.com/PrefectHQ/prefect/assets/97182/7432a5a6-883e-4a9f-877a-367889af8172\">\r\n\r\n\r\n### Browsers\r\n\r\n- [X] Chrome\r\n- [ ] Firefox\r\n- [ ] Safari\r\n- [ ] Edge\r\n\r\n### Prefect version\r\n\r\n```Text\r\n2.18.1+56.g09bfc6260d.dirty\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n```\r\nVersion:             2.18.1+53.g9b1b59cfd5\r\nAPI version:         0.8.4\r\nPython version:      3.11.8\r\nGit commit:          9b1b59cf\r\nBuilt:               Wed, May 1, 2024 11:33 AM\r\nOS/Arch:             darwin/arm64\r\nProfile:             default\r\nServer type:         server\r\n```",
    "comments": [
      {
        "user": "zhen0",
        "body": "Should be closed by https://github.com/PrefectHQ/prefect-ui-library/pull/3040 and is superseded by https://github.com/PrefectHQ/prefect/issues/15678 which includes a fuller description. "
      }
    ]
  },
  {
    "issue_number": 14869,
    "title": "Work queue concurrency limit not working (as expected)",
    "author": "mnchsmn",
    "state": "open",
    "created_at": "2024-08-09T13:03:00Z",
    "updated_at": "2025-05-28T14:35:52Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWe have a work queue with a concurrency limit of 1. Usually this works well but I've just observed that it does not always apparently. See the following screenshot\r\n<img width=\"1446\" alt=\"Screenshot 2024-08-09 at 14 59 04\" src=\"https://github.com/user-attachments/assets/ac46b51c-18fa-4adc-a34a-3c132d4574ee\">\r\n\r\nThe two tasks `statuesque-sidewinder` and `massive-mole` were executed concurently.\r\nBoth had been waiting for `laughing-oxpecker` (that's good). That's why one of them is 24 min late. However, once `laughing-oxpecker` finished. Both `statuesque-sidewinder` and `massive-mole` were started almost at the same time.\n\n### Version info (`prefect version` output)\n\n```Text\n2.19.4\n```\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zangell44",
        "body": "@mnchsmn is this an issue with open source prefect server or prefect cloud?"
      }
    ]
  },
  {
    "issue_number": 18182,
    "title": "`prefect.deployments.steps.set_working_directory` doesn't work with relative paths",
    "author": "NicholasPini",
    "state": "open",
    "created_at": "2025-05-28T08:35:38Z",
    "updated_at": "2025-05-28T08:35:38Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI have a deployment that runs on Kubernetes. It depends on a custom Docker container which sets the `WORKDIR` to `/app`. \n\nIn my pull steps, I then clone a repository inside `/app`, which contains a folder `prefect_flow`(note that I do not need to change directory here: the flow starts in the `WORKDIR` as expected) . I need to change directory to this one in order for the flow to work. So, in the pull steps of my deployment, I've set the following step:\n```json\n{\n  \"prefect.deployments.steps.set_working_directory\": {\n    \"directory\": \"prefect_flow\"\n  }\n}\n```\n\nBut this fails, saying that there is no such file or directory named `prefect_flow`. If I use and absolute path instead (`/app/prefect_flow`) this works as expected.\n\nHere is the weirder part. Looking at [set_working_directory](https://prefect-python-sdk-docs.netlify.app/prefect/deployments/steps/pull/?h=set_work#prefect.deployments.steps.pull.set_working_directory)'s source code, it uses `os.chdir` under the hood. I have written a similar function that looks like this:\n```python\ndef chdir(directory: str | Path, log_result: bool = False):\n    os.chdir(directory)\n    if log_result:\n        print(f\"Changed directory to {os.getcwd()}\")\n```\n\nThis function is defined in a local package named `prefect_steps.steps` which is pulled when cloning the repo.\n\nIf use this function in my pull steps, like this:\n```json\n{\n  \"prefect_steps.steps.chdir\": {\n    \"requires\": \"prefect-steps>=0.1.0\",\n    \"directory\": \"prefect_flow\"\n  }\n}\n```\n\neverything works as expected: I can pass a relative path to this step and it works. I am not sure why this is the case, it seems to me that these two functions do the same thing.\n\n### Version info\n\n```Text\nVersion:             3.4.3\nAPI version:         0.8.4\nPython version:      3.10.15\nGit commit:          1c2ba7a4\nBuilt:               Thu, May 22, 2025 10:00 PM\nOS/Arch:             darwin/arm64\nProfile:             default\nServer type:         cloud\nPydantic version:    2.11.5\nIntegrations:\n  prefect-aws:       0.5.10\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18174,
    "title": "ParameterTypeError with date type in quick run",
    "author": "Noordle",
    "state": "closed",
    "created_at": "2025-05-27T15:12:19Z",
    "updated_at": "2025-05-28T08:20:15Z",
    "labels": [],
    "body": "### Bug summary\n\ni have flow with date parameter and default value None\n\n```\nimport datetime\n\n@flow\ndef test(target_date: datetime.date = None):\n    print('test')\n```\n\nWhen i run them with \"Quick run\", it pass empty Parameters\n\n![Image](https://github.com/user-attachments/assets/7eb38a9e-ba54-4034-a39b-2bd03e52484c)\n\nin logs i have error with traceback\n\n```\nEngine execution exited with unexpected exception\nTraceback (most recent call last):\n  File \"/.venv/lib/python3.11/site-packages/prefect/flow_engine.py\", line 1527, in run_flow\n    ret_val = run_flow_sync(**kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.11/site-packages/prefect/flow_engine.py\", line 1372, in run_flow_sync\n    return engine.state if return_type == \"state\" else engine.result()\n                                                       ^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.11/site-packages/prefect/flow_engine.py\", line 350, in result\n    raise self._raised\n  File \"/.venv/lib/python3.11/site-packages/prefect/flow_engine.py\", line 299, in begin_run\n    self.parameters = self.flow.validate_parameters(self.parameters or {})\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.11/site-packages/prefect/flows.py\", line 639, in validate_parameters\n    raise ParameterTypeError.from_validation_error(exc) from None\nprefect.exceptions.ParameterTypeError: Flow run received invalid parameters:\n - target_date: Input should be a valid date\n```\n\n### Version info\n\n```Text\nVersion:             3.4.3\nAPI version:         0.8.4\nPython version:      3.11.12\nGit commit:          1c2ba7a4\nBuilt:               Thu, May 22, 2025 10:03 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.11.5\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\nIn Prefect v.2 it works well",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @Noordle - thanks for the issue! the proplem here appears to be your type hinting, whereas this likely does that you expect\n```python\nimport datetime\n\nfrom prefect import flow\n\n@flow\ndef test(target_date: datetime.date | None = None):\n    print(target_date)\n\n\nif __name__ == \"__main__\":\n    test.serve()\n```\n\nnote that the type hints do matter in the signature by default, since we use pydantic to validate the signature and `None` is not a valid `datetime`. You can disable validation with `validate_parameters=False` or use correct type hints\n\nperhaps there's an argument to be made that we should fail at deployment time, but that seems potentially more complex"
      },
      {
        "user": "Noordle",
        "body": "@zzstoatzz \n\n`datetime.date | None = None`\n\nit works, thank you for help"
      }
    ]
  },
  {
    "issue_number": 18082,
    "title": "No logs from DaskTaskRunner with cluster address set",
    "author": "oliver-batchelor",
    "state": "open",
    "created_at": "2025-05-16T09:02:09Z",
    "updated_at": "2025-05-27T22:44:47Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen running tasks on a \"remote\" dask cluster I get no log information. Below is a minimal example - in the UI there is simply  `This run didn't generate Logs` for both the tasks generated. \n\nAttached are logs with prefect `PREFECT_LOGGING_LEVEL=DEBUG`  - in the case which works (the local dask cluster) there's a LOT of logs, the other almost none. They both run the task OK and return a result.\n\n[remote.log](https://github.com/user-attachments/files/20241328/remote.log)\n[local.log](https://github.com/user-attachments/files/20241329/local.log)\n\n\nThe strange thing about this is that in production, we DO get logs for SOME tasks‚Äîand it's unclear to me what pattern there is to when a task will have logs‚Äîbut in the simplest case (below), we don't.\n\n\n```\nimport asyncio\nfrom prefect import flow, get_run_logger, task\nfrom prefect_dask import DaskTaskRunner\n\n@task(tags=[\"hello_world\"], log_prints=True,  task_run_name=\"my_task\")\nasync def my_task(name:str):\n    get_run_logger().critical(f\"HELP {name}!\")\n    print(f\"Hello {name}!\")\n    return name\n\ntask_runner = DaskTaskRunner(cluster_kwargs=dict(n_workers=1))    <- YES LOGS\n#task_runner = DaskTaskRunner(address=\"localhost:8786\")                 <- NO LOGS\n\n# Simplified worker creation as a substitute for dask cluster:\n# dask worker localhost:8786 > $HOME/log/worker.log 2>&1 &\n# dask scheduler > $HOME/log/scheduler.log 2>&1  &\n\n\n@flow(name=\"my_flow\", log_prints=True, task_runner=task_runner)\nasync def my_flow(name:str):\n    get_run_logger().info(f\"Flow started with name: {name}\")\n    futures = [my_task.submit(f\"name_{i}\") for i in range(2)]\n    print(f\"Task results: {[future.result() for future in futures]}\")\n\n\nasync def main():\n  await my_flow(\"test\")\n\n\nif __name__ == \"__main__\":\n  asyncio.run(main())\n```\n\n### Version info\n\n```Text\n(cropvision) ‚ûú  cropvision-workspace git:(main) ‚úó prefect version\n20:59:37.951 | DEBUG   | prefect.profiles - Using profile 'local'\nVersion:             3.3.3\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          4100d4ea\nBuilt:               Sat, Apr 05, 2025 01:44 AM\nOS/Arch:             linux/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.4\nIntegrations:\n  prefect-dask:      0.3.5\n```\n\n### Additional context\n\n(cropvision) ‚ûú  cropvision-workspace git:(main) ‚úó dask --version\ndask, version 2025.5.0\n",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "this was also reported with this MRE:\n\n```python\nfrom prefect import flow, task\nfrom prefect.logging import get_run_logger\nfrom prefect_dask import DaskTaskRunner\n\n@task\ndef display_object(obj):\n    logger = get_run_logger()\n    logger.critical(f\"hello from prefect run_logger: {obj}\")\n    print(\"hello from print\")\n    return True\n\n@flow(name=\"simple_flow\", task_runner=DaskTaskRunner(address='127.0.0.1:6666'), log_prints=True)\ndef simple_flow():\n    display_object.submit(\"Hello?\")\n\nsimple_flow()\n```\n\nsuspect it has something to do with using a \"remote\" cluster"
      }
    ]
  },
  {
    "issue_number": 18172,
    "title": "Cannot use env variables in run_shell_script pull step",
    "author": "NicholasPini",
    "state": "closed",
    "created_at": "2025-05-27T08:58:40Z",
    "updated_at": "2025-05-27T19:31:02Z",
    "labels": [
      "bug",
      "needs:mre"
    ],
    "body": "### Bug summary\n\nI have the following pull step for a deployment:\n```json\n{\n  \"prefect.deployments.steps.run_shell_script\": {\n    \"script\": \"git clone https://$GITHUB_TOKEN@github.com/some_user/some_private_repo.git .\"\n  }\n}\n```\n\n`GITHUB_TOKEN` is set as an env variable in the workpool. The issue is that this doesn't work: `$GITHUB_TOKEN` gets turned into `%24GITHUB_TOKEN` when running the script, messing it up.\n\nNote: I know I can use the built in `prefect.deployments.steps.git_clone`, but this function does not let me set the directory into which to clone the repo. And I cannot `prefect.deployments.steps.set_working_directory` because it doesn't work with relative paths in a docker container using WORKDIR, for some reason. \n\n### Version info\n\n```Text\nVersion:             3.4.3\nAPI version:         0.8.4\nPython version:      3.10.15\nGit commit:          1c2ba7a4\nBuilt:               Thu, May 22, 2025 10:00 PM\nOS/Arch:             darwin/arm64\nProfile:             default\nServer type:         cloud\nPydantic version:    2.11.4\nIntegrations:\n  prefect-aws:       0.5.10\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @NicholasPini - you should be able to use env vars as shown [here](https://docs.prefect.io/v3/deploy/infrastructure-concepts/prefect-yaml#utility-steps)\n```yaml\ndeployments:\n  - name: repros-17621\n    entrypoint: repros/17621.py:test_flow\n    work_pool:\n      name: local\n    \n    build: null\n\n    pull:\n    - prefect.deployments.steps.run_shell_script:\n        script: echo \"Hello, $USER!\"\n        expand_env_vars: true\n\n```\n\n```python\n08:47:28.376 | INFO    | Flow run 'mahogany-shellfish' -  > Running run_shell_script step...\nHello, nate!\n```\n\n\nI would also be curious to see a specific case of this in a separate issue if you have a reproduction!\n\n> I cannot `prefect.deployments.steps.set_working_directory` because it doesn't work with relative paths in a docker container using `WORKDIR`, for some reason"
      },
      {
        "user": "zzstoatzz",
        "body": "going to close this for now - feel free to reopen if you think I'm missing something / something is broken"
      }
    ]
  },
  {
    "issue_number": 18156,
    "title": "Docker image vulnerabilities",
    "author": "cbelsole",
    "state": "closed",
    "created_at": "2025-05-23T15:35:01Z",
    "updated_at": "2025-05-27T19:30:24Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWe implemented a pull through cache for the prefect image recently and are now caching them in our ECR. As such they are now subject to vulnerability scans. The following vulnerabilities have been identified.\n\n[CVE-2025-39735](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-39735)\n[CVE-2024-46774](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2024-46774)\n[CVE-2024-36908](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2024-36908)\n[CVE-2023-52425](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2023-52425)\n[CVE-2025-37838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-37838)\n[CVE-2024-50063](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2024-50063)\n[CVE-2025-37785](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-37785)\n[CVE-2025-22088](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-22088)\n[CVE-2025-22041](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-22041)\n[CVE-2025-22056](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-22056)\n[CVE-2025-22097](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-22097)\n[CVE-2025-22038](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-22038)\n[CVE-2025-22040](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-22040)\n[CVE-2025-22035](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2025-22035)\n\nIt looks like the [python slim image](https://hub.docker.com/layers/library/python/3.10-slim/images/sha256-c8dcd87426242f2161232b82f6f51cd60d5a8be80f1eec8a71760f16dfec92ad) has a bunch of known vulnerabilities. It may be worth changing the distro to AWS Linux 2023 which seems to keep up with these more.\n\n### Version info\n\n```Text\nThe prefect image is prefecthq/prefect:3.3.6-python3.12.\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @cbelsole - thanks for the issue!\n\n> It looks like the [python slim image](https://hub.docker.com/layers/library/python/3.10-slim/images/sha256-c8dcd87426242f2161232b82f6f51cd60d5a8be80f1eec8a71760f16dfec92ad) has a bunch of known vulnerabilities. It may be worth changing the distro to AWS Linux 2023 which seems to keep up with these more.\n\nworth noting we use [3.9-slim](https://hub.docker.com/layers/library/python/3.9-slim/images/sha256-731b2613ca521da59e21ea80c888d76ae14e53a3048a2496dd4e9918e2ab9e77), not 3.10 at this time (though this will be updated after 3.9's EoL)\n\nin general its very risky for us to change the base image because of the OS level deps people implicitly rely upon, changing this would very likely break workflows for a lot of people. additionally, we're unlikely to couple ourselves to an AWS specific image.\n\nthe best way to avoid the upstream vulnerabilities would be to build your own image with a base of your choice"
      },
      {
        "user": "zzstoatzz",
        "body": "going to close this for now - please feel free to re-open if you have objections"
      }
    ]
  },
  {
    "issue_number": 15090,
    "title": "Prefect package dependency on Griffe is spilling into user software",
    "author": "mahiki",
    "state": "closed",
    "created_at": "2024-08-27T02:37:52Z",
    "updated_at": "2025-05-27T14:54:05Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\r\n\r\nI am deploying a flow from a `prefect.yaml` file with the following:\r\n\r\n```bash\r\nprefect --no-prompt deploy --all --prefect-file ./build/projects/decks/prefect.yaml\r\n\r\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\r\n‚îÇ Deploying wbr_gne                                                                                                                    ‚îÇ\r\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\r\nDefining a schedule via the `schedule` key in the deployment has been deprecated. It will not be available after Sep 2024. Please use\r\n`schedules` instead by renaming the `schedule` key to `schedules` and providing a list of schedule objects.\r\n/Users/merlinr/Library/Caches/pypoetry/virtualenvs/templisher-4hu7XBP5-py3.11/lib/python3.11/site-packages/prefect/utilities/importtools.py:511: RuntimeWarning: coroutine 'Block.load' was never awaited\r\n  logger.debug(\"Failed to compile: %s\", e)\r\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\r\n19:26:50.774 | WARNING | griffe - <module>:3: No type or annotation for parameter 'deck_id'\r\n19:26:50.775 | WARNING | griffe - <module>:4: Failed to get 'name: description' pair from 'deck_id.yaml.'\r\n19:26:50.775 | WARNING | griffe - <module>:5: No type or annotation for parameter 'stage_exclude_list'\r\n/Users/merlinr/Library/Caches/pypoetry/virtualenvs/templisher-4hu7XBP5-py3.11/lib/python3.11/site-packages/prefect/flows.py:1693: RuntimeWarning: coroutine 'Block.load' was never awaited\r\n  if flow is None:\r\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\r\n19:26:50.776 | WARNING | griffe - <module>:3: No type or annotation for parameter 'deck_id'\r\n19:26:50.776 | WARNING | griffe - <module>:4: Failed to get 'name: description' pair from 'deck_id.yaml.'\r\n19:26:50.776 | WARNING | griffe - <module>:5: No type or annotation for parameter 'stage_exclude_list'\r\n1 validation error for MinimalDeploymentSchedule\r\nactive\r\n  value could not be parsed to a boolean (type=type_error.bool)\r\n```\r\n\r\nWe can ignore the warning about coroutine not awaited.\r\n\r\nWhy am I getting warnings about what's in the docstrings of the data application I'm building? What's next, enforcing Black formatting?\r\n\r\nAbsolutely fine to use Pydantic, Griffe, Black, whatever you want in the Prefect package, fantastic. But there needs to be some border between my application code and the orchestrator which is a dependency.\r\n\r\n### Version info (`prefect version` output)\r\n\r\n```Text\r\npoetry run prefect version\r\nVersion:             2.20.3\r\nAPI version:         0.8.4\r\nPython version:      3.11.9\r\nGit commit:          b8c27aa0\r\nBuilt:               Thu, Aug 22, 2024 3:13 PM\r\nOS/Arch:             darwin/arm64\r\nProfile:             dev\r\nServer type:         server\r\n```\r\n\r\n\r\n\r\n### Additional context\r\n\r\nI love Prefect, been investing a lot and I can't wait for 3.0. ",
    "comments": [
      {
        "user": "jia6214876",
        "body": "to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select \"gcc.\""
      },
      {
        "user": "jia6214876",
        "body": "to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select \"gcc.\""
      },
      {
        "user": "jia6214876",
        "body": "to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select \"gcc.\""
      }
    ]
  },
  {
    "issue_number": 18173,
    "title": "Reverse Proxy `URL_PREFIX` connection error",
    "author": "Pk13055",
    "state": "closed",
    "created_at": "2025-05-27T09:20:21Z",
    "updated_at": "2025-05-27T14:51:28Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nThe `worker` and `deploy` scripts are unable to communicate with the API server (in docker compose with nginx reverse proxy), ie, UI: `/prefect` and API `/prefect/api`\n\nCompose file:\n```yaml\nservices:\n  ### Prefect Database\n  database:\n    image: postgres:alpine\n    restart: always\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - POSTGRES_DB=${POSTGRES_DB}\n    networks:\n      - prefect_network\n    volumes:\n      - ./data/db:/var/lib/postgresql/data\n\n  ### Nginx Reverse Proxy\n  nginx:\n    image: nginx:alpine\n    restart: always\n    ports:\n      - 8000:80\n    networks:\n      - ui_network\n    volumes:\n      - ./config/nginx/:/etc/nginx/conf.d/\n    depends_on:\n      - server\n\n  ### Prefect Server API and UI\n  server:\n    image: prefecthq/prefect:3-python3.12\n    restart: always\n    volumes:\n      - prefect:/root/.prefect\n    entrypoint: [\"/opt/prefect/entrypoint.sh\", \"prefect\", \"server\", \"start\"]\n    environment:\n      - PREFECT_UI_URL=http://0.0.0.0:8000/prefect\n      - PREFECT_API_URL=http://0.0.0.0:8000/prefect/api\n      - PREFECT_UI_SERVE_BASE=/prefect\n      # If you want to access Prefect Server UI from anywhere other than the Docker host machine, you will need to change\n      # PREFECT_UI_URL and PREFECT_API_URL to match the external hostname/IP of the host machine. For example:\n      #- PREFECT_UI_URL=http://external-ip/prefect\n      #- PREFECT_API_URL=http://external-ip/prefect/api\n      - PREFECT_SERVER_API_HOST=${PREFECT_SERVER_API_HOST}\n      - PREFECT_API_DATABASE_CONNECTION_URL=${PREFECT_API_DATABASE_CONNECTION_URL}\n      # - EXTRA_PIP_PACKAGES=${EXTRA_PIP_PACKAGES}\n    depends_on:\n      - database\n    networks:\n      - ui_network\n      - prefect_network\n\n  ### Auto-discovery and deployment service\n  deploy:\n    image: prefecthq/prefect:3-python3.12\n    restart: \"no\"\n    working_dir: \"/root/flows\"\n    volumes:\n      - \"./flows:/root/flows\"\n      - \"./deployments:/root/deployments\"\n    environment:\n      # - PREFECT_API_URL=${PREFECT_API_URL:-http://0.0.0.0:8000/prefect/api}\n      - PREFECT_API_URL=http://0.0.0.0:8000/prefect/api\n      - WORK_POOL_NAME=${WORK_POOL_NAME}\n      - WORK_POOL_TYPE=${WORK_POOL_TYPE}\n    depends_on:\n      - server\n    networks:\n      - ui_network\n      - prefect_network\n    entrypoint:\n      - bash\n      - -c\n      - |\n        echo 'Waiting for Prefect server to be ready...'\n        sleep 30\n        echo 'Creating work pool...'\n        prefect work-pool create $${WORK_POOL_NAME} --type $${WORK_POOL_TYPE} || echo 'Work pool already exists'\n        echo 'Deploying flows...'\n        cd /root/flows\n\n        # List all Python files for debugging\n        echo 'Found Python files:'\n        find . -name \"*.py\" -not -path \"./__pycache__/*\" | head -10\n\n        # Find all flow functions and deploy them using xargs\n        echo 'Discovering and deploying flows...'\n\n        # Create a function to extract flow entrypoints\n        extract_flows() {\n            for py_file in $$(find . -name \"*.py\" -not -path \"./__pycache__/*\"); do\n                if grep -q \"@flow\" \"$$py_file\"; then\n                    # Extract flow function names that follow @flow decorator\n                    grep -A 1 \"@flow\" \"$$py_file\" | grep \"def \" | sed 's/def \\([^(]*\\).*/\\1/' | while read flow_func; do\n                        if [ ! -z \"$$flow_func\" ]; then\n                            # Output in format: filename:function_name\n                            echo \"$${py_file#./}:$$flow_func\"\n                        fi\n                    done\n                fi\n            done\n        }\n\n        # Extract all flows and deploy them using xargs\n        extract_flows | while read entrypoint; do\n            if [ ! -z \"$$entrypoint\" ]; then\n                flow_name=$$(echo \"$$entrypoint\" | cut -d':' -f2)\n                deployment_name=\"$$flow_name-deployment\"\n\n                echo \"Deploying: $$entrypoint as $$deployment_name\"\n\n                prefect deploy \\\n                    --name \"$$deployment_name\" \\\n                    --pool \"$${WORK_POOL_NAME}\" \\\n                    \"$$entrypoint\" || echo \"Failed to deploy $$entrypoint\"\n            fi\n        done\n        echo 'Auto-discovery and deployment completed!'\n\n  ## Prefect Worker (Process-based for local execution)\n  worker:\n    image: prefecthq/prefect:3-python3.12\n    restart: always\n    entrypoint:\n      [\n        \"/opt/prefect/entrypoint.sh\",\n        \"prefect\",\n        \"worker\",\n        \"start\",\n        \"--pool\",\n        \"${WORK_POOL_NAME}\",\n        \"--type\",\n        \"${WORK_POOL_TYPE}\",\n      ]\n    environment:\n      - PREFECT_API_URL=http://0.0.0.0:8000/prefect/api\n      # - PREFECT_API_URL=${PREFECT_API_URL:-http://0.0.0.0:8000/prefect/api}\n      # - EXTRA_PIP_PACKAGES=${EXTRA_PIP_PACKAGES}\n    networks:\n      - ui_network\n      - prefect_network\n    volumes:\n      - ./flows:/root/flows\n    depends_on:\n      - deploy\n\nvolumes:\n  prefect:\n\nnetworks:\n  ui_network:\n  prefect_network:\n```\nNGINX config:\n```nginx\nupstream prefect_server {\n    server server:4200;\n}\n\n# now we declare our main server\nserver {\n\n    listen 80;\n    server_name _;\n    client_max_body_size 10M;\n\n    location /prefect/api {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n        rewrite ^/prefect/api/?(.*)$ /api/$1 break;\n        proxy_pass http://prefect_server;\n    }\n    location /prefect/ {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n        proxy_pass http://prefect_server;\n    }\n}\n```\n`.env` vars:\n```\nCOMPOSE_PROJECT_NAME=scheduler\n# Database Configuration\nPOSTGRES_USER=prefect\nPOSTGRES_PASSWORD=prefect123\nPOSTGRES_DB=prefect\n\n# Prefect Server Configuration\nPREFECT_UI_URL=http://0.0.0.0:8000/prefect\nPREFECT_API_URL=http://0.0.0.0:8000/prefect/api\nPREFECT_SERVER_API_HOST=0.0.0.0\nPREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://prefect:prefect123@database:5432/prefect\n\n# Work Pool Configuration\nWORK_POOL_NAME=default-pool\nWORK_POOL_TYPE=process\n\n# EXTRA_PIP_PACKAGES=\n```\n\n### Version info\n\n```Text\nVersion:             3.4.3\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          1c2ba7a4\nBuilt:               Thu, May 22, 2025 10:03 PM\nOS/Arch:             linux/aarch64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.11.5\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\nTraceback:\n```python\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n     yield\n   File \"/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n     resp = await self._pool.handle_async_request(req)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n     raise exc from None\n   File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n     response = await connection.handle_async_request(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n     raise exc\n   File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n     stream = await self._connect(request)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n     stream = await self._network_backend.connect_tcp(**kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n     return await self._backend.connect_tcp(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n     with map_exceptions(exc_map):\n          ^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/contextlib.py\", line 158, in __exit__\n     self.gen.throw(value)\n   File \"/usr/local/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n     raise to_exc(exc) from exc\n httpcore.ConnectError: All connection attempts failed\n\n The above exception was the direct cause of the following exception:\n\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.12/site-packages/prefect/cli/_utilities.py\", line 44, in wrapper\n     return fn(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/prefect/cli/_types.py\", line 156, in sync_fn\n     return asyncio.run(async_fn(*args, **kwargs))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 195, in run\n     return runner.run(main)\n            ^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 118, in run\n     return self._loop.run_until_complete(task)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n     return future.result()\n            ^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/prefect/cli/worker.py\", line 116, in start\n     is_paused = await _check_work_pool_paused(work_pool_name)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/prefect/cli/worker.py\", line 181, in _check_work_pool_paused\n     work_pool = await client.read_work_pool(work_pool_name=work_pool_name)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/prefect/client/orchestration/_work_pools/client.py\", line 416, in read_work_pool\n     response = await self.request(\n                ^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/prefect/client/orchestration/base.py\", line 53, in request\n     return await self._client.send(request)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/prefect/client/base.py\", line 330, in send\n     response = await self._send_with_retry(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/prefect/client/base.py\", line 250, in _send_with_retry\n     response = await send(request, *send_args, **send_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n     response = await self._send_handling_auth(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n     response = await self._send_handling_redirects(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n     response = await self._send_single_request(request)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n     response = await transport.handle_async_request(request)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n     with map_httpcore_exceptions():\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.12/contextlib.py\", line 158, in __exit__\n     self.gen.throw(value)\n   File \"/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n     raise mapped_exc(message) from exc\n httpx.ConnectError: All connection attempts failed\n An exception occurred.\n```",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @Pk13055 - this appears to be a configuration issue, in particular\n\n```\n- PREFECT_API_URL=http://0.0.0.0:8000/prefect/api\n```\n\nport 8000 isn't available for inter-container communication per the above config\n\n feel free to reopen with a description of what's violating your expectations if you think I'm mistaken\n\n[docs\n](https://docs.prefect.io/v3/manage/server/index#reverse-proxy)\n\n[another example](https://github.com/zzstoatzz/prefect-pack/blob/main/examples/run_a_prefect_server/docker_compose/with_a_reverse_proxy/compose.yaml#L51-L52)"
      }
    ]
  },
  {
    "issue_number": 18171,
    "title": "cache: azure.core.exceptions.ResourceExistsError: The specified blob already exists",
    "author": "OliverKleinBST",
    "state": "open",
    "created_at": "2025-05-27T08:09:18Z",
    "updated_at": "2025-05-27T14:26:20Z",
    "labels": [
      "bug",
      "needs:mre"
    ],
    "body": "### Bug summary\n\nI run prefect with caching on Azure blob storage.\nI do rerun some flow/task with unchanged parameters and set\nPREFECT_TASKS_REFRESH_CACHE: \"true\"\nto force the cache to be refreshed.\n\nThis though leads to \ncache: azure.core.exceptions.ResourceExistsError: The specified blob already exists\n\n\n### Version info\n\n```Text\nVersion:             3.3.7\nAPI version:         0.8.4\nPython version:      3.11.12\nGit commit:          8f86aaee\nBuilt:               Mon, Apr 28, 2025 03:04 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.11.5\nIntegrations:\n  prefect-shell:     0.3.1\n  prefect-kubernetes: 0.6.1\n  prefect-docker:    0.6.5\n  prefect-azure:     0.4.4\n```\n\n### Additional context\n\nAn error was encountered while committing transaction '5a83a50a6433c8a3dd5e9df9f7d8657c'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/prefect/transactions.py\", line 353, in commit\n    self.store.persist_result_record(result_record=self._staged_value)\n  File \"/usr/local/lib/python3.11/site-packages/prefect/results.py\", line 844, in persist_result_record\n    return self._persist_result_record(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 347, in coroutine_wrapper\n    return run_coro_as_sync(ctx_call())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 207, in run_coro_as_sync\n    return call.result()\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 329, in result\n    return self.future.result(timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 192, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/usr/local/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 405, in _run_async\n    result = await coro\n             ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 188, in coroutine_wrapper\n    return await task\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 341, in ctx_call\n    result = await async_fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/results.py\", line 824, in _persist_result_record\n    await _call_explicitly_async_block_method(\n  File \"/usr/local/lib/python3.11/site-packages/prefect/results.py\", line 302, in _call_explicitly_async_block_method\n    return await getattr(block, method).aio(block, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect_azure/blob_storage.py\", line 715, in write_path\n    await self.upload_from_file_object(BytesIO(content), path)\n  File \"/usr/local/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 341, in ctx_call\n    result = await async_fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect_azure/blob_storage.py\", line 485, in upload_from_file_object\n    await blob_client.upload_blob(from_file_object, **upload_kwargs)\n  File \"/usr/local/lib/python3.11/site-packages/azure/core/tracing/decorator_async.py\", line 119, in wrapper_use_tracer\n    return await func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/azure/storage/blob/aio/_blob_client_async.py\", line 597, in upload_blob\n    return cast(Dict[str, Any], await upload_block_blob(**options))\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/azure/storage/blob/aio/_upload_helpers.py\", line 177, in upload_block_blob\n    process_storage_error(error)\n  File \"/usr/local/lib/python3.11/site-packages/azure/storage/blob/_shared/response_handlers.py\", line 186, in process_storage_error\n    exec(\"raise error from None\")   # pylint: disable=exec-used # nosec\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<string>\", line 1, in <module>\n  File \"/usr/local/lib/python3.11/site-packages/azure/storage/blob/aio/_upload_helpers.py\", line 85, in upload_block_blob\n    response = cast(Dict[str, Any], await client.upload(\n                                    ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/azure/core/tracing/decorator_async.py\", line 119, in wrapper_use_tracer\n    return await func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/azure/storage/blob/_generated/aio/operations/_block_blob_operations.py\", line 262, in upload\n    map_error(status_code=response.status_code, response=response, error_map=error_map)\n  File \"/usr/local/lib/python3.11/site-packages/azure/core/exceptions.py\", line 163, in map_error\n    raise error\nazure.core.exceptions.ResourceExistsError: The specified blob already exists.\nRequestId:db57fe9a-901e-001e-55dd-cebe59000000\nTime:2025-05-27T07:59:15.9770778Z\nErrorCode:BlobAlreadyExists\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobAlreadyExists</Code><Message>The specified blob already exists.\nRequestId:db57fe9a-901e-001e-55dd-cebe59000000\nTime:2025-05-27T07:59:15.9770778Z</Message></Error>",
    "comments": []
  },
  {
    "issue_number": 10330,
    "title": "read_work_queue_by_name raises JSONDecodeError",
    "author": "apolisskaya",
    "state": "open",
    "created_at": "2023-07-28T17:30:11Z",
    "updated_at": "2025-05-27T09:01:05Z",
    "labels": [
      "bug"
    ],
    "body": "### First check\n\n- [X] I added a descriptive title to this issue.\n- [X] I used the GitHub search to find a similar issue and didn't find it.\n- [X] I searched the Prefect documentation for this issue.\n- [X] I checked that this issue is related to Prefect and not one of its dependencies.\n\n### Bug summary\n\n`client`: `OrionClient`\r\n`queue_name`: `\"sync_assessments\"`\r\n\r\nWhen running the following, raises `JSONDecodeError`:\r\n\r\n```python3\r\nwork_queue = await client.read_work_queue_by_name(queue_name)\r\n```\r\n\r\nReferencing the Prefect source code and stepping through the execution with `pdb`:\r\n`response = await self._client.get(f\"/work_queues/name/{name}\")` -> returns a 200 OK with `response.is_success == True`\r\nand `response.text` looks like\r\n```\r\n'<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta http-equiv=\"Cache-Control\" content=\"max-age=0, must-revalidate\"/>\\n    <meta http-equiv=\"Pragma\" content=\"no-cache\"/>\\n    <meta http-equiv=\"Expires\" content=\"0\" />\\n    <meta charset=\"UTF-8\" />\\n    <link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/ico/apple-touch-icon.png\">\\n  \\n    <link rel=\"icon\" id=\"favicon-32\" type=\"image/png\" sizes=\"32x32\" href=\"/ico/favicon-32x32.png\" media=\"(prefers-color-scheme:light)\">\\n    <link rel=\"icon\" id=\"favicon-16\" type=\"image/png\" sizes=\"16x16\" href=\"/ico/favicon-16x16.png\" media=\"(prefers-color-scheme:light)\">\\n\\n    <link rel=\"icon\" id=\"favicon-32-dark\" type=\"image/png\" sizes=\"32x32\" href=\"/ico/favicon-32x32-dark.png\" media=\"(prefers-color-scheme:dark)\">\\n    <link rel=\"icon\" id=\"favicon-16-dark\" type=\"image/png\" sizes=\"16x16\" href=\"/ico/favicon-16x16-dark.png\" media=\"(prefers-color-scheme:dark)\">\\n\\n    <link rel=\"icon\" href=\"/favicon.ico\" media=\"(prefers-color-scheme:no-preference)\">\\n    <link rel=\"icon\" href=\"/favicon.ico\" media=\"(prefers-color-scheme:light)\">\\n    <link rel=\"icon\" href=\"/favicon-dark.ico\" media=\"(prefers-color-scheme:dark)\">\\n\\n    <link rel=\"manifest\" href=\"/ico/site.webmanifest\">\\n    <link rel=\"mask-icon\" href=\"/ico/safari-pinned-tab.svg\" color=\"#5bbad5\">\\n    <meta name=\"msapplication-TileColor\" content=\"#da532c\">\\n    <meta name=\"theme-color\" content=\"#ffffff\">\\n\\n    <script src=\"/registerSW.js\"></script>\\n\\n    <!-- https://stripe.com/docs/payments/quickstart -->\\n    <!-- Don‚Äôt include the script in a bundle or host it yourself. -->\\n    <script src=\"https://js.stripe.com/v3/\"></script>\\n\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\\n    <title>Prefect Cloud</title>\\n    <script type=\"module\" crossorigin src=\"/index.50a5a629.js\"></script>\\n    <link rel=\"stylesheet\" href=\"/assets/index-18768f53.css\">\\n  </head>\\n  <body>\\n    <div id=\"app\"></div>\\n    \\n  </body>\\n</html>\\n'\r\n```\r\nwhich is definitely not valid JSON, so the return statement which looks like `return WorkQueue.parse_obj(response.json())` is raising `json.decoder.JSONDecodeError`\n\n### Reproduction\n\n```python3\nUsing `OrionClient` and any string as `queue_name`, make a call to `read_work_queue_by_name`\n```\n\n\n### Error\n\n```python3\nThis was reported by an engineer on our team with the following stack trace:\r\n\r\n\r\nTraceback (most recent call last):\r\n File \"/Users/cwatts/Code/api/manage.py\", line 31, in <module>\r\n  execute_from_command_line(sys.argv)\r\n File \"/Users/cwatts/.local/share/virtualenvs/api-mwSkRMLi/lib/python3.9/site-packages/django/core/management/__init__.py\", line 446, in execute_from_command_line\r\n  utility.execute()\r\n File \"/Users/cwatts/.local/share/virtualenvs/api-mwSkRMLi/lib/python3.9/site-packages/django/core/management/__init__.py\", line 440, in execute\r\n  self.fetch_command(subcommand).run_from_argv(self.argv)\r\n File \"/Users/cwatts/.local/share/virtualenvs/api-mwSkRMLi/lib/python3.9/site-packages/django/core/management/base.py\", line 402, in run_from_argv\r\n  self.execute(*args, **cmd_options)\r\n File \"/Users/cwatts/.local/share/virtualenvs/api-mwSkRMLi/lib/python3.9/site-packages/django/core/management/base.py\", line 448, in execute\r\n  output = self.handle(*args, **options)\r\n File \"/Users/cwatts/Code/api/octave_core/management/commands/prefect.py\", line 96, in handle\r\n  asyncio.run(self.register(flow_groups))\r\n File \"/usr/local/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/runners.py\", line 44, in run\r\n  return loop.run_until_complete(main)\r\n File \"/usr/local/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\n  return future.result()\r\n File \"/Users/cwatts/Code/api/octave_core/management/commands/prefect.py\", line 120, in register\r\n  await flow_group.create_flow_deployments(tags=[\"agent:octave-core\"])\r\n File \"/Users/cwatts/Code/api/octave_core/prefect/flows/__init__.py\", line 196, in create_flow_deployments\r\n  deployment_ids.update(set(await flow_info.create_deployments(client, tags=deployment_tags.copy())))\r\n File \"/Users/cwatts/Code/api/octave_core/prefect/flows/flow_info.py\", line 21, in create_deployments\r\n  deployment_id = await deploy.create_for_flow(\r\n File \"/Users/cwatts/Code/api/octave_core/prefect/flows/deployment.py\", line 128, in create_for_flow\r\n  work_queue = await client.read_work_queue_by_name(self.queue)\r\n File \"/Users/cwatts/.local/share/virtualenvs/api-mwSkRMLi/lib/python3.9/site-packages/prefect/client/orchestration.py\", line 920, in read_work_queue_by_name\r\n  return WorkQueue.parse_obj(response.json())\r\n File \"/Users/cwatts/.local/share/virtualenvs/api-mwSkRMLi/lib/python3.9/site-packages/httpx/_models.py\", line 756, in json\r\n  return jsonlib.loads(self.text, **kwargs)\r\n File \"/usr/local/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/__init__.py\", line 346, in loads\r\n  return _default_decoder.decode(s)\r\n File \"/usr/local/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/decoder.py\", line 337, in decode\r\n  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n File \"/usr/local/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/decoder.py\", line 355, in raw_decode\r\n  raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\n```\n\n\n### Versions\n\n```Text\n--- % prefect version\r\nVersion:             2.11.0\r\nAPI version:         0.8.4\r\nPython version:      3.9.16\r\nGit commit:          eeb9e219\r\nBuilt:               Thu, Jul 20, 2023 4:34 PM\r\nOS/Arch:             darwin/arm64\r\nProfile:             default\r\nServer type:         server\r\n```\n```\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zhen0",
        "body": "I haven't reproduced this so first step for whoever picks it up is to check you can reproduce and add more details. "
      },
      {
        "user": "Wh1isper",
        "body": "You should include `/api` in `PREFECT_API_URL`, e.g. `export PREFECT_API_URL=http://127.0.0.1:4200/api`"
      },
      {
        "user": "Pk13055",
        "body": "Facing the same issue when trying to use prefect behind nginx (via reverse proxy)\n\nFollowing is my nginx conf:\n```nginx\nupstream prefect_server {\n    server server:4200;\n}\n\n# now we declare our main server\nserver {\n\n    listen 80;\n    server_name _;\n    client_max_body_size 10M;\n\n    location /prefect/api {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n        rewrite ^/prefect/api/?(.*)$ /api/$1 break;\n        proxy_pass http://prefect_server;\n    }\n    location /prefect/ {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n        proxy_pass http://prefect_server;\n    }\n}\n```\nFollowing is the `compose.yaml`:\n```yaml\nservices:\n  ### Prefect Database\n  database:\n    image: postgres:alpine\n    restart: always\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - POSTGRES_DB=${POSTGRES_DB}\n    networks:\n      - prefect_network\n    volumes:\n      - ./data/db:/var/lib/postgresql/data\n\n  ### Nginx Reverse Proxy\n  nginx:\n    image: nginx:alpine\n    restart: always\n    ports:\n      - 8000:80\n    networks:\n      - ui_network\n    volumes:\n      - ./config/nginx/:/etc/nginx/conf.d/\n    depends_on:\n      - server\n\n  ### Prefect Server API and UI\n  server:\n    image: prefecthq/prefect:3-python3.12\n    restart: always\n    volumes:\n      - prefect:/root/.prefect\n    entrypoint: [\"/opt/prefect/entrypoint.sh\", \"prefect\", \"server\", \"start\"]\n    environment:\n      - PREFECT_UI_URL=http://0.0.0.0:8000/prefect\n      - PREFECT_API_URL=/prefect/api\n      - PREFECT_UI_SERVE_BASE=/prefect\n      # If you want to access Prefect Server UI from anywhere other than the Docker host machine, you will need to change\n      # PREFECT_UI_URL and PREFECT_API_URL to match the external hostname/IP of the host machine. For example:\n      #- PREFECT_UI_URL=http://external-ip/prefect\n      #- PREFECT_API_URL=http://external-ip/prefect/api\n      - PREFECT_SERVER_API_HOST=${PREFECT_SERVER_API_HOST}\n      - PREFECT_API_DATABASE_CONNECTION_URL=${PREFECT_API_DATABASE_CONNECTION_URL}\n      # - EXTRA_PIP_PACKAGES=${EXTRA_PIP_PACKAGES}\n    depends_on:\n      - database\n    networks:\n      - ui_network\n      - prefect_network\n\n  ### Auto-discovery and deployment service\n  deploy:\n    image: prefecthq/prefect:3-python3.12\n    restart: \"no\"\n    working_dir: \"/root/flows\"\n    volumes:\n      - \"./flows:/root/flows\"\n      - \"./deployments:/root/deployments\"\n    environment:\n      # - PREFECT_API_URL=${PREFECT_API_URL:-http://0.0.0.0:8000/prefect/api}\n      - PREFECT_API_URL=http://server:4200/prefect/api\n      - WORK_POOL_NAME=${WORK_POOL_NAME}\n      - WORK_POOL_TYPE=${WORK_POOL_TYPE}\n    depends_on:\n      - server\n    networks:\n      - prefect_network\n    entrypoint:\n      - bash\n      - -c\n      - |\n        echo 'Waiting for Prefect server to be ready...'\n        sleep 30\n        echo 'Creating work pool...'\n        prefect work-pool create $${WORK_POOL_NAME} --type $${WORK_POOL_TYPE} || echo 'Work pool already exists'\n\n  ## Prefect Worker (Process-based for local execution)\n  worker:\n    image: prefecthq/prefect:3-python3.12\n    restart: always\n    entrypoint:\n      [\n        \"/opt/prefect/entrypoint.sh\",\n        \"prefect\",\n        \"worker\",\n        \"start\",\n        \"--pool\",\n        \"${WORK_POOL_NAME}\",\n        \"--type\",\n        \"${WORK_POOL_TYPE}\",\n      ]\n    environment:\n      - PREFECT_API_URL=http://server:4200/prefect/api\n      # - PREFECT_API_URL=${PREFECT_API_URL:-http://0.0.0.0:8000/prefect/api}\n      # - EXTRA_PIP_PACKAGES=${EXTRA_PIP_PACKAGES}\n    networks:\n      - prefect_network\n    volumes:\n      - ./flows:/root/flows\n    depends_on:\n      - deploy\n\nvolumes:\n  prefect:\n\nnetworks:\n  ui_network:\n  prefect_network:\n\n```"
      }
    ]
  },
  {
    "issue_number": 18166,
    "title": "Flow run page graph should show all flow runs of a given date range",
    "author": "OliverKleinBST",
    "state": "open",
    "created_at": "2025-05-26T06:34:29Z",
    "updated_at": "2025-05-26T06:34:29Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nToday, the flow run page only shows \"dots\" of flow runs which are filtered by the \"items per page\" selector.\nWhile filtering what is shown on the graph via the selector on the top of the page filtering by the items per page selector seems not to be useful and is limiting the graphs usefulness for us\n\n### Describe the proposed behavior\n\nThe flow runs graph should ignore \"items per page\" selector settings.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 16656,
    "title": "Custom Webhook Block Bug on UI",
    "author": "soamicharan",
    "state": "closed",
    "created_at": "2025-01-09T06:28:33Z",
    "updated_at": "2025-05-26T03:18:25Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### Bug summary\n\nThe Custom Webhook Block `Query Parameters`, `Headers`, `Form Data` is not editable on Prefect UI.\r\n\r\nTo reproduce this, simply navigate to Blocks UI page, and create new block of type Custom Webhook, you see object type fields are not editable.\r\n\r\n![Screenshot 2025-01-09 115252](https://github.com/user-attachments/assets/19e079cb-2ef6-462f-a331-505ba92ba936)\r\n\n\n### Version info\n\n```Text\nVersion:             3.1.0\r\nAPI version:         0.8.4\r\nPython version:      3.10.12\r\nGit commit:          a83ba39b\r\nBuilt:               Thu, Oct 31, 2024 12:43 PM\r\nOS/Arch:             linux/x86_64\r\nProfile:             local\r\nServer type:         unconfigured\r\nPydantic version:    2.8.2\n```\n\n\n### Additional context\n\n![Screenshot 2025-01-09 115252](https://github.com/user-attachments/assets/11254c34-55e0-4cb4-9fa3-42cce968fc93)\r\n",
    "comments": [
      {
        "user": "obendidi",
        "body": "+1\n"
      },
      {
        "user": "zhen0",
        "body": "I'm not able to reproduce this issue on 3.4.0 so am going to close it.  If you upgrade and are still encountering it, please let me know and I'll look into it again. "
      }
    ]
  },
  {
    "issue_number": 16791,
    "title": "Worker Logs UI not tailing nicely",
    "author": "jfloodnet",
    "state": "open",
    "created_at": "2025-01-21T04:30:43Z",
    "updated_at": "2025-05-26T02:53:08Z",
    "labels": [
      "UI/UX"
    ],
    "body": "### Bug summary\n\nWhen on the worker logs if you try to scroll down to the most recent logs the view constantly scrolls up a little bit, so that you can't easily sit and read the logs. \n\nSee attached video for demonstration.\n\nhttps://github.com/user-attachments/assets/5b40ad8e-20c7-4ce1-8e48-db5126d0ef77\n\n### Version info\n\n```Text\nPrefect workers using 3.1.16\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "jfloodnet",
        "body": "Ok I just realised you can change the sort direction which solves this issue. You should probably make that the default.\n\n![Image](https://github.com/user-attachments/assets/767bd6f3-bc43-4895-b772-dd475e3fa221)"
      }
    ]
  },
  {
    "issue_number": 18005,
    "title": "CancellationCleanup service stuck permanently at 100% CPU usage",
    "author": "gabr1elt",
    "state": "closed",
    "created_at": "2025-05-07T21:47:58Z",
    "updated_at": "2025-05-23T22:02:04Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nCancellationCleanup constantly keeps the CPU at 90+% (the service still works tough, and with no delays)\n\nStopping the service as described in [settings ref](https://docs.prefect.io/v3/api-ref/settings-ref#enabled-4) frees up the cpu (so I assume its that).\n\n### logs\n\nThe server only ever showed a warning once:\n\n`19:21:17.691 | WARNING | prefect.server.services.cancellationcleanup - `stop(block=True)` was called on CancellationCleanup but more than one loop interval (20.0 seconds) has passed. This usually means something is wrong. If `stop()` was called from inside the loop service, use `stop(block=False)` instead.`\n\nthen it never mentioned it again\n\n### how to stop it\n\nResetting the DB is the only way I found to fix it (but obviously not ideal)\n\n### Maybe related\n\nMy infra crashed a few times and had to do some manual cleanup\n\nalso found this\n\nhttps://github.com/PrefectHQ/prefect/issues/15231\n\n\n### Version info\n\n```Text\nPrefect Server: docker.io/prefecthq/prefect:3.4.0-python3.10\nPostgres: docker.io/postgres:17.4\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @gabr1elt - thanks for the issue! can you share whatever details you can on your setup (infra, database settings etc) and load (roughly how much work, task-heavy, what infra, how many workers etc)? that will help pinpoint what the actual issue is"
      },
      {
        "user": "gabr1elt",
        "body": "Hi @zzstoatzz \n\nIt's a small deployment, less than 100 flows an hour not IO or CPU intensive.\nI'm running it on a Podman Pod for now (untill I solve all the issues)\n1 Worker\nThe runners are docker containers (actually podman but should not be any different)\n\nThis started since I moved to 3.4.0 (from 3.2.13) \n\nThe database has no particular conf changes from the base image\n\n```\npg_config --version\n\nPostgreSQL 17.4 (Debian 17.4-1.pgdg120+2)\n```\n\n```\npodman version\n\nVersion:             3.4.0\nAPI version:         0.8.4\nPython version:      3.10.17\nGit commit:          c80e4442\nBuilt:               Fri, May 02, 2025 08:04 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.11.4\nServer:\n  Database:          postgresql\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\nServer conf\n\n```\n# prefect.toml\n\n[server]\nlogging_level = \"INFO\"\nanalytics_enabled = false\n\n[server.api]\nhost = \"0.0.0.0\"\nport = 4200\ncsrf_protection_enabled = true\ncors_allowed_origins = \"http://127.0.0.1:4200\"\n\n[server.ui]\napi_url = \"http://127.0.0.2:4200/api\"\n\n[server.database]\nconnection_url = \"postgresql+asyncpg://prefect:${PREFECT_SERVER_DATABASE_PASSWORD}@127.0.0.1:5432/prefect\"\n\n[server.deployments]\nconcurrency_slot_wait_seconds = 10\n```\n\nWorker conf\n\n```\n# prefect.toml\n\n[api]\nurl = \"http://127.0.1.1:4200/api\"\n\n[worker.webserver]\nhost = \"0.0.0.0\"\nport = 4300\n\n[runner]\nheartbeat_frequency = 30\n```\n\nanything else, let me know"
      },
      {
        "user": "zzstoatzz",
        "body": "excellent, thanks!\n"
      }
    ]
  },
  {
    "issue_number": 14629,
    "title": "threading problems with `QueueService` subclasses",
    "author": "zzstoatzz",
    "state": "open",
    "created_at": "2024-07-16T16:16:30Z",
    "updated_at": "2025-05-23T20:18:57Z",
    "labels": [
      "bug",
      "3.x"
    ],
    "body": "### First check\n\n- [X] I added a descriptive title to this issue.\n- [X] I used the GitHub search to find a similar issue and didn't find it.\n- [X] I searched the Prefect documentation for this issue.\n- [X] I checked that this issue is related to Prefect and not one of its dependencies.\n\n### Bug summary\n\nin 3.x versions, we have noticed quite a few _strange_ errors related to different implementations of `QueueService`, like:\r\n- `APILogWorker`\r\n```python\r\n**errors about registering atexit handler**\r\n```\r\n\r\n- `EventsWorker`\r\n```python\r\nRuntimeError: Cannot put items in a stopped service instance.\r\n```\n\n### Reproduction\n\n```python3\nthere is not a clean MRE as there is not one specific problem here\r\n\r\nthere is some problem related to threading here\n```\n\n\n### Error\n\n```python3\n(written above)\n```\n\n\n### Versions (`prefect version` output)\n\n```Text\n¬ª prefect version\r\nVersion:             3.0.0rc10+88.g6df736e5cc\r\nAPI version:         0.8.4\r\nPython version:      3.12.3\r\nGit commit:          6df736e5\r\nBuilt:               Tue, Jul 16, 2024 10:57 AM\r\nOS/Arch:             darwin/arm64\r\nProfile:             bleeding\r\nServer type:         server\r\nPydantic version:    2.8.2\r\nIntegrations:\r\n  prefect-aws:       0.5.0rc2\r\n  prefect-github:    0.3.0rc2\r\n  prefect-docker:    0.6.0rc3\n```\n\n\n### Additional context\n\npotentially relevant:\r\n- https://github.com/python/cpython/issues/113964",
    "comments": [
      {
        "user": "lelouvincx",
        "body": "Still happen in 3.2.14"
      },
      {
        "user": "zzstoatzz",
        "body": "maybe related to \n\nhttps://github.com/PrefectHQ/prefect/pull/13831"
      }
    ]
  },
  {
    "issue_number": 18144,
    "title": "Task in tags context doesn't reuse thread",
    "author": "nkanazawa1989",
    "state": "closed",
    "created_at": "2025-05-22T13:48:32Z",
    "updated_at": "2025-05-23T18:01:45Z",
    "labels": [
      "bug",
      "great writeup"
    ],
    "body": "### Bug summary\n\nI'm writing a scientific workflow which includes optimization loops. Multiple optimization loops and cost function evaluations run in parallel with the `ThreadPoolTaskRunner`. \n\nAfter certain loop number, I encounter\n```\nTask run failed with exception: OSError(24, 'Too many open files') - Retries are exhausted\n```\nwhich indicates a soft limit of file descriptor open per process. \n\nThe simplified structure of my program looks like:\n\n```python\nfrom prefect import flow, task, tags\n\n@task\ndef inner_task():\n    # do something\n    pass\n\n@task\ndef outer_task():\n    for i in range(5):\n        with tags(f\"loop {i}\"):\n            futures = []\n            for j in range(4):\n                with tags(f\"instance {j}\"):\n                    fut = inner_task.submit()\n                futures.append(fut)\n            for fut in futures:\n                fut.result()\n\n@flow\ndef main():\n    fut1 = outer_task.submit()\n    ...\n    fut1.wait()\n\nif __name__ == \"__main__\":\n    main()\n```\n\nHere I assume the `inner_task` is a time consuming IO bound subroutine of the cost function. For better traceability and visibility of my cost function execution I want to tag them recursively -- this is why `tags` context is helpful here. As a side effect, _each cost function spawns a new thread_ and the thread is pooled by the thread pool executor even after execution. \n\nThis accumulates the file descriptor associated with threads during the loop, and at certain point the workflow crashes due to their soft limit. Of course I can change the soft limit, but this is just putting off the problem.\n\nTo confirm above description of the problem, you can try the following complete script.\n\n```python\nimport os\nimport psutil\n\nfrom prefect import flow, task, tags\nfrom prefect.logging import get_run_logger\n\n@task\ndef inner_task():\n    # do something\n    pass\n\n@task\ndef outer_task():\n    logger = get_run_logger()\n    \n    for i in range(5):\n        with tags(f\"loop {i}\"):  # loop tag\n            num_fd = psutil.Process(os.getpid()).num_fds()\n            num_th = psutil.Process(os.getpid()).num_threads()\n            logger.info(f\"Open FDs at loop {i} begin: {num_fd}\")\n            logger.info(f\"Threads at loop {i} begin: {num_th}\")\n        \n            futures = []\n            for j in range(4):\n                with tags(f\"instance {j}\"):  # instance tag\n                    fut = inner_task.submit()\n                futures.append(fut)\n                \n            for fut in futures:\n                fut.result()\n                \n            num_fd_after = psutil.Process(os.getpid()).num_fds()\n            num_th_after = psutil.Process(os.getpid()).num_threads()\n            logger.info(f\"Open FDs at loop {i} end: {num_fd_after} (+ {num_fd_after - num_fd})\")\n            logger.info(f\"Threads at loop {i} end: {num_th_after} (+ {num_th_after - num_th})\")\n\n@flow\ndef main():\n    fut = outer_task.submit()\n    fut.wait()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Version info\n\n```Text\nVersion:             3.4.2\nAPI version:         0.8.4\nPython version:      3.11.11\nGit commit:          c3c1c119\nBuilt:               Mon, May 19, 2025 04:04 PM\nOS/Arch:             darwin/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.4\n```\n\n### Additional context\n\nI did a small experiment with different settings.\n\n#### As-is (every function call spawns new thread / 4 instances per loop):\n```\n...\nThreads at loop 4 end: 32 (+ 4)\nOpen FDs at loop 4 end: 33 (+ 5)\n...\n```\n\n#### I remove instance tag (some threads are reused -- behavior is a bit stochastic):\n```\n...\nThreads at loop 4 end: 17 (+ 1)\nOpen FDs at loop 4 end: 18 (+ 2)\n...\n```\n\n#### I remove loop tag (no thread is accumulated, 8 threads are added at loop 0):\n```\n...\nThreads at loop 4 end: 16 (+ 0)\nOpen FDs at loop 4 end: 17 (+ 1)\n...\n```\n\n#### I remove both tags (no thread is accumulated, 4 threads are added at loop 0):\n```\n...\nThreads at loop 4 end: 12 (+ 0)\nOpen FDs at loop 4 end: 12 (+ 0)\n...\n```\n\nThis result indicates tags context doesn't work well with the thread pool executor. ",
    "comments": [
      {
        "user": "nkanazawa1989",
        "body": "Seems like this is a problem of contextvar (i.e. `context.run` in multithreaded environment). \nhttps://github.com/PrefectHQ/prefect/blob/7d4d41615e614b9f46eb317a2948a3d0643a07f5/src/prefect/task_runners.py#L326-L330\nIf I understand correctly context is thread local and this is why always new thread is created? -- so this is expected behavior, not a bug. It would be great if this file descriptor accumulation with Prefect context is documented as a note in `ThreadPoolTaskRunner` API docs."
      },
      {
        "user": "nkanazawa1989",
        "body": "@zzstoatzz Thanks for the quick response, but I found another edge case. Can you reopen this or give me your thoughts?\n\n```python\nimport asyncio\nimport psutil\n\nfrom prefect import flow, task\nfrom prefect.logging import get_run_logger\n\n@task\nasync def my_task():\n    await asyncio.sleep(1)\n\n@task\nasync def outer():\n    logger = get_run_logger()\n\n    for i in range(3):\n        num_fd = psutil.Process().num_fds()\n        num_th = psutil.Process().num_threads()\n        logger.info(f\"Start of the loop {i}: FD={num_fd}, Thread={num_th}\")\n\n        coros = []\n        for j in range(3):\n            tagged = my_task.with_options(tags=[f\"loop{i}\", f\"instance{j}\"])\n            coro = tagged()\n            coros.append(coro)\n        await asyncio.gather(*coros)\n                    \n        num_fd_after = psutil.Process().num_fds()\n        num_th_after = psutil.Process().num_threads()\n        logger.info(f\"End of the loop {i}: FD={num_fd_after} (+{num_fd_after-num_fd}), Thread={num_th_after} (+{num_th_after-num_th})\")\n\n\n@flow\ndef main():\n    outer.submit().wait()\n\nif __name__ == \"__main__\":\n    main()\n```\n\nIn this example I don't use the task runner. Instead, I run multiple async tasks with `asyncio.gather`. Unlike the multithreading, this workflow is expected to run on a single thread with context switching. However, the log indicates each loop accumulates 3 threads with file descriptors -- this is totally unexpected and the behavior doesn't look async. Indeed this behavior is a bit surprising.\n\nI scrutinized the problem and found how this happened:\n\nThe async task is called inside `AsyncTaskRunEngine` context:\nhttps://github.com/PrefectHQ/prefect/blob/857102f2418e2be4cd8e771c0866cedb71980f0d/src/prefect/task_engine.py#L1444\n\nThis checks concurrency slot with tags:\nhttps://github.com/PrefectHQ/prefect/blob/857102f2418e2be4cd8e771c0866cedb71980f0d/src/prefect/task_engine.py#L774-L776\n\nThis slot check invokes `ConcurrencySlotAcquisitionService` with tags:\nhttps://github.com/PrefectHQ/prefect/blob/857102f2418e2be4cd8e771c0866cedb71980f0d/src/prefect/concurrency/_asyncio.py#L29\n\nUsually the service instance is cached by the `QueueService`, but now tags are considered in the cache key evaluation:\nhttps://github.com/PrefectHQ/prefect/blob/857102f2418e2be4cd8e771c0866cedb71980f0d/src/prefect/_internal/concurrency/services.py#L260-L263\n\nThis results in creation of new service per tag, and new thread is created here with a file descriptor.\nhttps://github.com/PrefectHQ/prefect/blob/857102f2418e2be4cd8e771c0866cedb71980f0d/src/prefect/_internal/concurrency/services.py#L281-L283\n\nThis means Prefect always creates unnecessary threads for concurrency check when tags are assigned dynamically. The task runner issue above might be also due to this concurrency slot check -- not context copy but I didn't confirm.\n\nIt would be nice if Prefect can support extra tags that are not considered in the concurrency control, i.e. pure metadata. The only workaround I can come up with is to embed such information in the task_run_name, but this looks a bit awkward."
      }
    ]
  },
  {
    "issue_number": 18045,
    "title": "Cancel flow run in automation errors with 403 response",
    "author": "mvdb-enspi",
    "state": "closed",
    "created_at": "2025-05-14T13:04:10Z",
    "updated_at": "2025-05-23T16:22:44Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nCancelling a flow run via automations is not working and returns a 403 error.\n\nTo reproduce, start a Prefect server and run the following flow file:\n\n```python\nfrom prefect import flow, task\nfrom time import sleep\n\n\n@task\ndef sleep_task(seconds: int) -> None:\n    print(f\"Sleeping for {seconds} seconds...\")\n    sleep(seconds)\n    print(\"Done sleeping!\")\n\n\n@flow(flow_run_name=\"sleep-for-{seconds}\", log_prints=True)\ndef sleepy_flow(seconds: int = 300):\n    sleep_task(seconds)\n\n\nif __name__ == \"__main__\":\n    sleepy_flow.serve()\n```\n\nThis job will just sleep for 5 minutes.\n\nI created an automation via the GUI that looks as follows:\n\n![Image](https://github.com/user-attachments/assets/363a7861-90b0-4e57-a2a7-b5aed09e87b9).\n\nWhen creating a flow run, this automation is triggered and successfully sends a notification but the flow run is not cancelled. In the event feed, there's a failed automation action with the following raw return:\n\n![Image](https://github.com/user-attachments/assets/50d0ced1-f8a2-4c52-a7fd-2934b6613057)\n\nI tried changing the cancel action to suspend but this gave the same 403 error.\n\n### Version info\n\n```Text\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.13.1\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             linux/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.4\n\nThe server is also running on 3.4.1.\n```\n\n### Additional context\n\nThe JSON representation of the automation trigger:\n\n```json\n{\n  \"type\": \"event\",\n  \"match\": {\n    \"prefect.resource.id\": \"prefect.flow-run.*\"\n  },\n  \"match_related\": {\n    \"prefect.resource.id\": [\n      \"prefect.flow.e2e27dee-bd6a-4e0f-9772-1e853d2586ad\"\n    ],\n    \"prefect.resource.role\": \"flow\"\n  },\n  \"after\": [\n    \"prefect.flow-run.Running\"\n  ],\n  \"expect\": [\n    \"prefect.flow-run.*\"\n  ],\n  \"for_each\": [\n    \"prefect.resource.id\"\n  ],\n  \"posture\": \"Proactive\",\n  \"threshold\": 1,\n  \"within\": 30\n}\n```",
    "comments": [
      {
        "user": "chrisguidry",
        "body": "@mvdb-enspi, thanks for the bug report.  One clarifying question: do you have authentication enabled on your server?"
      },
      {
        "user": "mvdb-enspi",
        "body": "No"
      },
      {
        "user": "mvdb-enspi",
        "body": "These are all server settings (defined in the pyproject.toml file):\n\n```toml\napi.url = \"http://127.0.0.1:4200/api\"\nserver.ui.api_url = \"/api\"\ninternal.logging_level = \"INFO\"\nserver.api.csrf_protection_enabled = true\nserver.logging_level = \"INFO\"\nserver.analytics_enabled = false\nserver.ephemeral.enabled = false\n```"
      }
    ]
  },
  {
    "issue_number": 18025,
    "title": "UI should connect to server using relative URLs by default, not absolute URLs",
    "author": "Hueburtsonly",
    "state": "open",
    "created_at": "2025-05-12T02:29:36Z",
    "updated_at": "2025-05-23T15:51:41Z",
    "labels": [
      "bug",
      "UI/UX"
    ],
    "body": "### Bug summary\n\nTL;DR: Please default PREFECT_UI_API_URL to \"/api\", not \"http://_host_:_port_/api\".\n\nAt the moment, the value of PREFECT_UI_API_URL defaults to something like this:\n\n\"http://127.0.0.1:4000/api\"\n\nThe first thing I do on any deployment of prefect is set an env var to set PREFECT_UI_API_URL to this:\n\n\"/api\"\n\nMy only question is, why does Prefect use the full absolute path as a default? This can only possibly serve to break things, because if any kind of tunneling/proxying/cloudflaring is done, the end user will be connecting to http://something.else/ , but that website will try to connect to 127.0.0.1. Much better to just use relative paths, which will ALWAYS* work.\n\nInappropriate use of absolute URLs where relative URLs would be perfectly fine is a common mistake in web design, and it's being made here in the Prefect UI.\n\n'* And if they don't work for some weird reason, we still have the env var available. \n\n### Version info\n\n```Text\n>prefect version\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.13.2\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             win32/AMD64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.4\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18047,
    "title": "too many values to unpack (expected 2)",
    "author": "mizell-lipseys",
    "state": "open",
    "created_at": "2025-05-14T15:03:30Z",
    "updated_at": "2025-05-23T15:51:04Z",
    "labels": [
      "bug",
      "2.x"
    ],
    "body": "### Bug summary\n\nThis is the same issue as the one outlined here:\nhttps://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Flinen.prefect.io%2Ft%2F16009724%2Fhi-i-ve-been-trying-out-the-new-flow-deploy-function-for-dep&data=05%7C02%7Cphillips%40lipseys.com%7C447a7ba502ea426dc6e108dd92f7635a%7Cfd78ae5f2bcd4db2b08eff7920601431%7C0%7C0%7C638828313443064442%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=rYVfCtHtWEFebhnuHAaNUlIXI5hy8PzQXsKyT6Sm0mY%3D&reserved=0\n\n### Version info\n\n```Text\nVersion:             2.10.2\nAPI version:         0.8.4\nPython version:      3.10.9\nGit commit:          16747be2\nBuilt:               Fri, Apr 7, 2023 10:19 AM\nOS/Arch:             win32/AMD64\nProfile:             default\nServer type:         cloud\n```\n\n### Additional context\n\nCan't update out of fear for existing company processes",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @mizell-lipseys - sorry you're running into an issue! this should be fixed on newer versions of `prefect` 2.x and 3.x., are you able to upgrade to newer 2.x versions? generally between minor versions you should be okay to upgrade without manual intervention, and feel free to post here if you do hit something\n\n"
      }
    ]
  },
  {
    "issue_number": 18055,
    "title": "Events in FlowRun graph cause Out Of Memory",
    "author": "GalLadislav",
    "state": "open",
    "created_at": "2025-05-14T20:41:40Z",
    "updated_at": "2025-05-23T15:49:06Z",
    "labels": [
      "bug",
      "UI/UX"
    ],
    "body": "### Bug summary\n\nMy flow run recently stucked in AwaitingConcurencySlot for more than a day. When i canceled this flow run, flow run graph appeared ending with 50 events. Probably there where more than 50, but i guess that this is some cap limit for the UI. As the graph loaded, my memory usage jumped more than 3GB up (16GB total, ~40%). It happened that i opened this flowrun in more that one tab and this caused OOM right away.\n\n\n![Image](https://github.com/user-attachments/assets/92555307-051d-454a-a4fb-ff2d05a78ef5)\n![Image](https://github.com/user-attachments/assets/0768f979-47f9-45d0-a34b-0874f0c75a93)\n\n### Version info\n\n```Text\nBrowser: Edge\nSystem: Ubuntu 22\nPrefect 3.3.7 OSS\nPython 3.12\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @GalLadislav - thanks for the report! we'll look into this"
      }
    ]
  },
  {
    "issue_number": 18155,
    "title": "prefect.concurrency - Slot Release, Performance, and Inconsistent Execution",
    "author": "letsplaywithdata",
    "state": "open",
    "created_at": "2025-05-23T15:10:31Z",
    "updated_at": "2025-05-23T15:40:09Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWe are experiencing several issues when using prefect.concurrency for managing concurrent task executions in Prefect version 2.16.9. These issues include:\n\nSlots not released on flow failure: If a flow utilizing prefect.concurrency fails, the acquired concurrency slots are not always released, leading to a \"hanging state.\" We've implemented a workaround by resetting concurrency limits from within the flow before it starts, but this feels like a temporary fix.\n\nSlower performance compared to asyncio.Semaphore: Our tests indicate that using prefect.concurrency is noticeably slower than managing concurrency with asyncio.Semaphore directly. The provided code snippet and accompanying observations highlight this performance discrepancy.\n\nGaps in task execution: We've observed unexplained gaps in the execution of tasks when using prefect.concurrency, suggesting potential inefficiencies or scheduling issues.\n\nInconsistent results: Runs using prefect.concurrency with the same parameters yield different execution times. While our test case uses asyncio.sleep, the variance is more pronounced in our actual, longer-running Prefect Cloud flows.\n\nOur main concern is that relying on asyncio.Semaphore means we are not fully leveraging Prefect's built-in concurrency management features and might miss out on other benefits or integrations.\n\nTo Reproduce\n\nPlease see the following Python script which demonstrates the performance difference between prefect.concurrency and asyncio.Semaphore.\n\n```python\nimport asyncio\nimport time\n\nimport pandas as pd\nfrom prefect import flow, get_run_logger, task\nfrom prefect.concurrency.asyncio import concurrency\nfrom prefect.task_runners import ConcurrentTaskRunner\n\n\n@task\nasync def api_call(page: int) -> str:\n    \"\"\"Simulated API call with 5-second delay.\"\"\"\n    log = get_run_logger()\n    log.info(f\"API call start: page {page}\")\n    await asyncio.sleep(5)\n    log.info(f\"API call end: page {page}\")\n    return f\"done-{page}\"\n\n\n@flow(name=\"Chunked Flow with Prefect Concurrency\", task_runner=ConcurrentTaskRunner())\nasync def chunked_concurrency_flow(limit: int, total_pages: int) -> float:\n    log = get_run_logger()\n    pages_to_gather = list(range(1, total_pages + 1))\n    pages_to_gather.reverse()\n\n    start = time.perf_counter()\n    async with concurrency(names=\"concurrency-limit\", occupy=limit):\n        while pages_to_gather:\n            chunk = [\n                pages_to_gather.pop() for _ in range(min(limit, len(pages_to_gather)))\n            ]\n            log.info(f\"Processing chunk with concurrency: {chunk}\")\n            futures = await api_call.map(page=chunk)\n            await asyncio.gather(*[f.result() for f in futures])\n    return round(time.perf_counter() - start, 2)\n\n\n@flow(name=\"Chunked Flow with Semaphore\", task_runner=ConcurrentTaskRunner())\nasync def chunked_semaphore_flow(limit: int, total_pages: int) -> float:\n    log = get_run_logger()\n    sem = asyncio.Semaphore(limit)\n    pages_to_gather = list(range(1, total_pages + 1))\n    pages_to_gather.reverse()\n\n    async def run_page(page: int) -> str:\n        async with sem:\n            log.info(f\"API call start (semaphore): page {page}\")\n            await asyncio.sleep(5)\n            log.info(f\"API call end (semaphore): page {page}\")\n            return f\"done-{page}\"\n\n    start = time.perf_counter()\n    coros = [run_page(p) for p in pages_to_gather]\n    await asyncio.gather(*coros)\n    return round(time.perf_counter() - start, 2)\n\n\nasync def compare_results():\n    limits = [5]\n    pages = 100\n    data = []\n\n    for limit in limits:\n        print(f\"\\n--- Running for limit={limit} ---\")\n        t1 = await chunked_concurrency_flow(limit=limit, total_pages=pages)\n        t2 = await chunked_semaphore_flow(limit=limit, total_pages=pages)\n        data.append(\n            {\n                \"Limit\": limit,\n                \"Total Pages\": pages,\n                \"Concurrency Time (s)\": t1,\n                \"Semaphore Time (s)\": t2,\n                \"Diff (Concurrency - Semaphore)\": round(t1 - t2, 2),\n            }\n        )\n\n    df = pd.DataFrame(data)\n    return df\n\n\nif __name__ == \"__main__\":\n    df = asyncio.run(compare_results())\n    print(\"\\n Chunked Concurrency vs Semaphore Timing:\\n\")\n    print(df.to_markdown(index=False))\n```\n\n\n### Expected Behavior\n\n- Concurrency slots should be reliably released upon flow completion, whether successful or failed.\n- prefect.concurrency should offer performance comparable to or better than asyncio.Semaphore for managing task concurrency, or the performance difference should be justifiable (e.g., by added features/overhead).\n- Task execution should be continuous without unexplained gaps when using prefect.concurrency.\n- Execution times for flows using prefect.concurrency should be reasonably consistent for the same inputs, similar to what's observed with asyncio.Semaphore.\n\n### Actual Behavior\n- Slots remain occupied after flow failures, requiring manual intervention or workarounds.\n- prefect.concurrency is significantly slower in the provided test case.\n- While actual differences in these have been more in cloud, the above has been represented using local test environment with asyncio.sleep() where we shouldn't have seen any difference in execution. \n- Gaps are observed in the execution timeline (visualized in Prefect Cloud/UI when tasks are spaced out more than expected).\n- Execution times vary noticeably between identical runs using prefect.concurrency.\n\n![Image](https://github.com/user-attachments/assets/b954e4c9-7977-4358-bd74-3669341d0569)\n\n![Image](https://github.com/user-attachments/assets/2aea9ae2-4452-46b8-bf7f-d656bb067901)\n\n### Version info\n\n```Text\nVersion:             2.16.9\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          083def52\nBuilt:               Thu, Apr 4, 2024 3:11 PM\nOS/Arch:             linux/x86_64\nProfile:             default\nServer type:         server\n```\n\n### Additional context\n\nThe issue of slots not being released on failure seems to be a known problem, as indicated by discussions in online forum : Reference: https://github.com/prefecthq/prefect/issues/17415\n\nOur primary goal is to use Prefect's native concurrency features effectively. The current behavior pushes us towards asyncio.Semaphore, which feels like a workaround rather than a solution leveraging the framework's full potential.",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @letsplaywithdata - is this issue meaningfully different from https://github.com/PrefectHQ/prefect/issues/17415, if so how?\n\nit is best to keep issues scoped to one problem, and the main problem here seems to be what's expressed in that existing issue\n\nwould it make sense for you to add your commentary there?"
      }
    ]
  },
  {
    "issue_number": 17850,
    "title": "Make uv an optional runtime dependency",
    "author": "BwL1289",
    "state": "open",
    "created_at": "2025-04-17T21:28:27Z",
    "updated_at": "2025-05-23T14:53:53Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nRight now `uv` is a required runtime dependency but is only used in `_experimental` functionality [here](https://github.com/PrefectHQ/prefect/blob/main/src/prefect/_experimental/bundles.py#L27). It's also wrapped in try / except so the functionality may work without it being installed anyway.\n\nI know that the `_experimental` module is used in a few places in the repo (notably [here](https://github.com/PrefectHQ/prefect/blob/main/src/prefect/runner/runner.py#L659-L660)), but because `uv` is not a typical runtime dependency, and because it's rust based, it should be avoided if not strictly necessary.\n### Describe the proposed behavior\n\nMake `uv` an optional dependency.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @BwL1289 - can you share your motivation here? [when you install `uv` from pypi its just a pre-built wheel](https://docs.astral.sh/uv/getting-started/installation/#pypi), It was compiled using rust, but that should have no impact on the installer.\n\nis it causing some issue for you?"
      },
      {
        "user": "BwL1289",
        "body": "hi @zzstoatzz few motivations:\n\n1. Selfishly, I (and many distros) build libraries (and their dependencies) from source, and would love to avoid compiling `uv` if possible.\n2. `uv` ships with wheels but if a wheel is not available for a given platform, uv will be [built from source](https://docs.astral.sh/uv/getting-started/installation/#standalone-installer:~:text=if%20a%20wheel%20is%20not%20available%20for%20a%20given%20platform%2C%20uv%20will%20be%20built%20from%20source%2C%20which%20requires%20a%20Rust%20toolchain), which requires a Rust toolchain\n3. Would love to avoid an extra ~17mb in my container from `uv` üôÇ "
      },
      {
        "user": "zzstoatzz",
        "body": "@BwL1289 thanks for the context - that makes sense. however, we have gradually introduced `uv` intentionally since we plan to use it more broadly (for ad-hoc installs and in the currently experimental features you noticed) so I'd say we're unlikely to remove it as a required dependency at this time.\n\nthat said, let's keep this issue open and if it becomes a pain point for many users then we can reconsider."
      }
    ]
  },
  {
    "issue_number": 18154,
    "title": "Search and filter results are incomplete in the new UI",
    "author": "Ponsaille",
    "state": "open",
    "created_at": "2025-05-23T13:55:41Z",
    "updated_at": "2025-05-23T13:55:41Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen viewing a flow that contains a large number of tasks or events, using the new prefect flow run UI‚Äôs filtering or search functionality only operates on the subset of data currently loaded in the browser. Filtered output can be incomplete or miss the very tasks (e.g. failures) you‚Äôre looking for.\n\n**Steps to reproduce**\n\n1. Create a prefect flow with thousands of events and a failed task (ideally at the end of the flow as it seam that the UI is getting the data from start to end of the flow.\n2. Open the new prefect flow run UI\n3. Filter for failed tasks\n4. Observe that no tasks were found\n\n<img width=\"1242\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/951fb5b8-ba9b-4bad-b0c3-1218c3269122\" />\n\n**Expected bahaviour**\n\nThe filtering and search functionalities should run on the whole data\n\n### Version info\n\n```Text\nVersion:             2.20.16\nAPI version:         0.8.4\nPython version:      3.10.14\nGit commit:          b5047953\nBuilt:               Thu, Dec 19, 2024 10:55 AM\nOS/Arch:             linux/x86_64\nProfile:             default\nServer type:         cloud\n```\n\n### Additional context\n\nThis can be a quite critical issue as it is currently making debugging large flow very complex since the removal of the old interface.",
    "comments": []
  },
  {
    "issue_number": 18084,
    "title": "Unexpected UI behavior for Union of models with shared parameters",
    "author": "j-tr",
    "state": "open",
    "created_at": "2025-05-16T10:12:41Z",
    "updated_at": "2025-05-23T08:20:39Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nThe parameter input form shows unexpected behavior in Prefect Cloud when using parameters that are are Union of two or more models that have parameters with the same name.\n\nThis flow demonstrates the issue where `MyModel` and `MyModel2` have both a parameter called `shared_parameter`.\n\n```\nfrom prefect import flow\nfrom pydantic import BaseModel\n\nclass MyModel(BaseModel):\n    shared_parameter: int = 42\n    \nclass MyModel2(BaseModel):\n    shared_parameter: int = 24\n\n@flow\ndef my_flow(param: MyModel | MyModel2):\n   pass\n\nif __name__ == \"__main__\":\n    my_flow.serve()\n```\n\nAt first, the form looks as expected.\n\nWhen clicking on MyModel2, the form content changes but the MyModel2 is only highlighted for a very brief moment and then switched back to MyModel.\n\nWhen clicking on MyModel2 again, it is correctly highlighted but the form content disappears.\n\nFrom thereon, when switching between the models, MyModel is displayed correctly but MyModel2 stays blank.\n\n![Image](https://github.com/user-attachments/assets/4254f94e-c530-4641-b0fd-f0e445b7f3b3)\n\n### Version info\n\n```Text\nVersion:             2.20.18\nAPI version:         0.8.4\nPython version:      3.10.17\nGit commit:          b7059e89\nBuilt:               Wed, Apr 30, 2025 2:37 PM\nOS/Arch:             darwin/arm64\nProfile:             cloud_dev\nServer type:         cloud\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "acylam",
        "body": "Can confirm that I'm also experiencing this bug with `prefect==3.4.1`"
      }
    ]
  },
  {
    "issue_number": 17754,
    "title": "Prefect Task is not serialized by distributed.protocol.serialize",
    "author": "nkanazawa1989",
    "state": "open",
    "created_at": "2025-04-07T08:04:39Z",
    "updated_at": "2025-05-23T05:32:00Z",
    "labels": [
      "bug",
      "upstream dependency"
    ],
    "body": "### Bug summary\n\nIt seems like the Prefect Task cannot be serialized with `distributed.protocol.serialize`. I slightly modified [this test](https://github.com/PrefectHQ/prefect/blob/de45c87cee0eae0ef7b4937ba6dbe01f84f695f3/src/integrations/prefect-dask/tests/test_task_runners.py#L85-L108) and created the following script.\n\n```python\nfrom dask_jobqueue import PBSCluster\nfrom prefect import flow, task\nfrom prefect_dask import DaskTaskRunner\n\n\n@task\ndef task_a():\n    return \"a\"\n\n\n@task\ndef task_b():\n    return \"b\"\n\n\n@task\ndef task_c(b: str):\n    return b + \"c\"\n\n\n@flow(version=\"test\")\ndef test_flow():\n    a = task_a.submit()\n    b = task_b.submit()\n    c = task_c.submit(b)\n    return a, b, c\n\n\ndef main():\n    with PBSCluster(\n        walltime=\"00:02:00\",\n        processes=1,\n        cores=2,\n        memory=\"2GiB\",\n        local_directory=\"/tmp\",\n        job_extra_directives=[\"-V\"],\n    ) as cluster:\n        task_runner = DaskTaskRunner(cluster=cluster)\n        \n        a, b, c = test_flow.with_options(task_runner=task_runner)()\n\n    print(a, b, c)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nRunning this script on my PBS login node with Prefect server installation will cause the following exception.\n```\nEncountered exception during execution: TypeError('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 1 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7f39bf1a42d0>\\n 0. 139885965356672\\n>')\nTraceback (most recent call last):\n  File \"...venv/lib/python3.11/site-packages/distributed/protocol/pickle.py\", line 60, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_pickle.PicklingError: Can't pickle <function task_a at 0x7f39bd445ee0>: it's not the same object as __main__.task_a\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"...venv/lib/python3.11/site-packages/distributed/protocol/pickle.py\", line 65, in dumps\n    pickler.dump(x)\n_pickle.PicklingError: Can't pickle <function task_a at 0x7f39bd445ee0>: it's not the same object as __main__.task_a\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"...venv/lib/python3.11/site-packages/distributed/protocol/serialize.py\", line 366, in serialize\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/distributed/protocol/serialize.py\", line 78, in pickle_dumps\n    frames[0] = pickle.dumps(\n                ^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/distributed/protocol/pickle.py\", line 77, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/cloudpickle/cloudpickle.py\", line 1537, in dumps\n    cp.dump(obj)\n  File \"...venv/lib/python3.11/site-packages/cloudpickle/cloudpickle.py\", line 1303, in dump\n    return super().dump(obj)\n           ^^^^^^^^^^^^^^^^^\nTypeError: cannot pickle 'generator' object\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"...venv/lib/python3.11/site-packages/prefect/flow_engine.py\", line 763, in run_context\n    yield self\n  File \"...venv/lib/python3.11/site-packages/prefect/flow_engine.py\", line 1370, in run_flow_sync\n    engine.call_flow_fn()\n  File \"...venv/lib/python3.11/site-packages/prefect/flow_engine.py\", line 783, in call_flow_fn\n    result = call_with_parameters(self.flow.fn, self.parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/.../simple-test.py\", line 24, in test_flow\n    a = task_a.submit()\n        ^^^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/prefect/tasks.py\", line 1230, in submit\n    future = task_runner.submit(self, parameters, wait_for)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/prefect_dask/task_runners.py\", line 431, in submit\n    future = self.client.submit(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/prefect_dask/client.py\", line 64, in submit\n    future = super().submit(\n             ^^^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/distributed/client.py\", line 2174, in submit\n    futures = self._graph_to_futures(\n              ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/distributed/client.py\", line 3377, in _graph_to_futures\n    header, frames = serialize(ToPickle(dsk), on_error=\"raise\")\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...venv/lib/python3.11/site-packages/distributed/protocol/serialize.py\", line 392, in serialize\n    raise TypeError(msg, str_x) from exc\nTypeError: ('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 1 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7f39bf1a42d0>\\n 0. 139885965356672\\n>')\n```\n\nPBS dependency is too much complicated to debug further.\n\n### Version info\n\n```Text\nVersion:             3.3.2\nAPI version:         0.8.4\nPython version:      3.11.11\nGit commit:          e49e3185\nBuilt:               Thu, Apr 03, 2025 09:25 PM\nOS/Arch:             linux/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.9.2\nIntegrations:\n  prefect-dask:      0.3.4\n```\n\n### Additional context\n\n```\ndistributed==2025.3.0\ndask==2025.3.0\ndask-jobqueue==0.9.0\nprefect-dask==0.3.4\n```",
    "comments": [
      {
        "user": "nkanazawa1989",
        "body": "After carefully reading the code, I realized that this is not an issue of the `PBSCluster`. I updated the issue title. In the same environment following code works (the task is serialized by cloudpickle as expected).\n\n```python\nfrom prefect import task\nfrom distributed.protocol.serialize import pickle_dumps\n\n@task\ndef task_a():\n        return \"a\"\n\nout = pickle_dumps(task_a)\n```\n"
      },
      {
        "user": "nkanazawa1989",
        "body": "This made me think this was an issue of `DaskTaskRunner` itself (e.g. it may implicitly modifies object attributes) and I tested the following code with the same environment.\n\n```python\ndef main():\n    a, b, c = test_flow.with_options(task_runner=DaskTaskRunner)()\n\n    print(a, b, c)\n```\n\nContrary to my expectations, this code worked. If I understand right, the difference between two experiments only lies in the underlying cluster class (i.e. `PBSCluster` vs `LocalCluster`). As far as I can read the codebase the cluster class doesn't look injecting a custom logic into serialization. \n\nHas anyone tested DaskTaskRunner + PBSCluster?"
      },
      {
        "user": "aaazzam",
        "body": "Hi @nkanazawa1989! This is ultimately a cloudpickle issue, see:\n\n- https://github.com/dask/distributed/issues/7954\n- https://github.com/cloudpipe/cloudpickle/issues/509\n\ntl;dr you can't pickle functions decorated through contextlib. There's a _workaround_ in the meantime, which is to not use task/flow a decorators and instead instantiate them properly, e.g. this works (but pains me how ugly it is):\n\n```python\nfrom dask_jobqueue import PBSCluster\nfrom prefect import Task, Flow\nfrom prefect_dask import DaskTaskRunner\n\n\ndef _task_a():\n    return \"a\"\n\ndef _task_b():\n    return \"b\"\n\ndef _task_c(b: str):\n    return b + \"c\"\n\ntask_a = Task(_task_a)\ntask_b = Task(_task_b)\ntask_c = Task(_task_c)\n\n\ndef _test_flow():\n    a = task_a()\n    b = task_b()\n    c = task_c(b)\n    return a, b, c\n\ntest_flow = Flow(_test_flow, version=\"test\")\ndef main():\n    with PBSCluster(\n        walltime=\"00:02:00\",\n        processes=1,\n        cores=2,\n        memory=\"2GiB\",\n        local_directory=\"/tmp\",\n        job_extra_directives=[\"-V\"],\n    ) as cluster:\n        task_runner = DaskTaskRunner(cluster=cluster)\n        \n        a, b, c = test_flow.with_options(task_runner=task_runner)()\n\n    print(a, b, c)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n"
      }
    ]
  },
  {
    "issue_number": 10136,
    "title": "Docs: Add a tutorial for how to use Prefect with a job scheduler (e.g. Slurm) on an HPC machine ",
    "author": "Andrew-S-Rosen",
    "state": "open",
    "created_at": "2023-07-01T21:51:20Z",
    "updated_at": "2025-05-22T23:52:20Z",
    "labels": [
      "docs"
    ],
    "body": "### First check\r\n\r\n- [X] I added a descriptive title to this issue.\r\n- [X] I used GitHub search to find a similar request and didn't find it üòá\r\n\r\n### Describe the issue\r\n\r\nThere are currently no examples for how to use Prefect with a job scheduling system (e.g. SLURM, PBS, MOAB) on an HPC machine. I think this is a pretty important omission because most academics and users of the top supercomputers might not be aware how they can use Prefect.\r\n\r\n### Describe the proposed change\r\n\r\nAdd a tutorial.\r\n\r\n### Additional context\r\n\r\nThere's mention of spinning up a Dask cluster but no representative example of how this is done in practice with a given job scheduling system. ",
    "comments": [
      {
        "user": "alexisthual",
        "body": "I'm all in for a tutorial on how to use Prefect with a stack using `submitit`, `hydra`, and possibly `dora`!"
      },
      {
        "user": "Andrew-S-Rosen",
        "body": "I must admit that I have no clue what any of those 3 are üòÖ  I was thinking more of the recommended approach of using a `prefect-dask.DaskTaskRunner` instantiated with a `dask-jobqueue.SLURMCluster`, which would be passed to the `task_runner` kwarg in the `@flow` decorator."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 30 days with no activity. To keep this issue open remove stale label or comment."
      }
    ]
  },
  {
    "issue_number": 16299,
    "title": "Prefect Server Experiencing Timeouts Due to Slow Database Communication",
    "author": "tomukmatthews",
    "state": "open",
    "created_at": "2024-12-09T23:04:43Z",
    "updated_at": "2025-05-22T21:19:53Z",
    "labels": [
      "bug",
      "performance",
      "Database"
    ],
    "body": "### Bug summary\r\n\r\nWe're experiencing significant performance issues with our Prefect server installation, primarily manifesting as timeouts in database communications. The services are consistently running longer than their designated loop intervals:\r\n\r\nFlowRunNotifications: Taking ~6.8s vs 4s interval\r\nRecentDeploymentsScheduler: Taking ~7.5s vs 5s interval\r\n\r\nThe primary error appears to be a timeout in the PostgreSQL asyncpg connection:\r\n\r\n```\r\nTimeoutError raised in sqlalchemy/dialects/postgresql/asyncpg.py\r\n```\r\n\r\n### Version info (for the server)\r\n\r\n```Text\r\nVersion:             3.1.4\r\nAPI version:         0.8.4\r\nPython version:      3.12.7\r\nGit commit:          78ee41cb\r\nBuilt:               Wed, Nov 20, 2024 7:37 PM\r\nOS/Arch:             linux/x86_64\r\nProfile:             ephemeral\r\nServer type:         server\r\nPydantic version:    2.10.0\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n- We've deployed the same flows in prefect 2.19 and there were no issues\r\n- We're using Postgres with engine version 14.2. The number of database connections is well below the maximum limit and the load on the DB is low\r\n- We attempted wiping the database completely and starting fresh (which didn't work)\r\n- Our prefect client which we used to deploy the flows is on the same prefect, pydantic and python version\r\n- We've deployed prefect in Kubernetes (EKS) using the helm chart at https://prefecthq.github.io/prefect-helm, vesion: `2024.12.3011129`\r\n- I've noticed the perfect server experiencing OOMs and restarting, doubling its CPU and memory limits alleviated this.\r\n- I've been experiencing issues around client - server comms (despite increasing the `proxy-read-timeout` in my nginx ingress):\r\n```\r\n23:23:53.081 | WARNING | prefect.events.clients - Unable to connect to 'ws://prefect-dev.shared.unitary.ai/api/events/in'. Please check your network settings to ensure websocket connections to the API are allowed. Otherwise event data (including task run data) may be lost. Reason: . Set PREFECT_DEBUG_MODE=1 to see the full error.\r\n23:23:53.082 | ERROR   | GlobalEventLoopThread | prefect._internal.concurrency - Service 'EventsWorker' failed with 5 pending items.\r\n```\r\n\r\nServer logs:\r\n```\r\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 949, in connect\r\n    await_only(creator_fn(*arg, **kw)),\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\r\n    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\r\n    value = await result\r\n            ^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/asyncpg/connection.py\", line 2420, in connect\r\n    async with compat.timeout(timeout):\r\n               ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/asyncio/timeouts.py\", line 115, in __aexit__\r\n    raise TimeoutError from exc_val\r\nTimeoutError\r\n13:54:15.592 | WARNING | prefect.server.services.flowrunnotifications - FlowRunNotifications took 6.797844 seconds to run, which is longer than its loop interval of 4 seconds.\r\n13:54:16.315 | WARNING | prefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 7.518834 seconds to run, which is longer than its loop interval of 5 seconds.\r\n```",
    "comments": [
      {
        "user": "CorMazz",
        "body": "I'm having what I think may be a related issue? I tested out Prefect no problem on WSL, because we're considering using it as a workflow orchestrator. Now when I'm trying to test that same workflow on a corporate RH Linux environment, the server feels like it is starved for resources (even though I'm running a simple workflow and the VM has 4 cores & 64 GB RAM), since tons of services are taking longer to run than their designated loop intervals and it is slow to respond to UI interaction. Likely related: the SQLite database seems to always be locked.\r\n\r\n```\r\nVersion:             3.1.5\r\nAPI version:         0.8.4\r\nPython version:      3.10.8\r\nGit commit:          3c06654e\r\nBuilt:               Mon, Dec 2, 2024 6:57 PM\r\nOS/Arch:             linux/x86_64\r\nProfile:             local\r\nServer type:         server\r\nPydantic version:    2.10.3\r\n\r\n```\r\n\r\n```bash\r\n11:29:52.962 | WARNING | prefect.server.services.marklateruns - MarkLateRuns took 16.083751 seconds to run, which is longer than its loop interval of 5.0 seconds.\r\n11:29:52.977 | ERROR   | uvicorn.error - Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\r\n    self.dialect.do_execute(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 941, in do_execute\r\n    cursor.execute(statement, parameters)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 147, in execute\r\n    self._adapt_connection._handle_exception(error)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 298, in _handle_exception\r\n    raise error\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 129, in execute\r\n    self.await_(_cursor.execute(operation, parameters))\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\r\n    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\r\n    value = await result\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/aiosqlite/cursor.py\", line 48, in execute\r\n    await self._execute(self._cursor.execute, sql, parameters)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/aiosqlite/cursor.py\", line 40, in _execute\r\n    return await self._conn._execute(fn, *args, **kwargs)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/aiosqlite/core.py\", line 132, in _execute\r\n    return await future\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/aiosqlite/core.py\", line 115, in run\r\n    result = function()\r\nsqlite3.OperationalError: database is locked\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/prefect/server/api/server.py\", line 149, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 460, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 20, in __call__\r\n    await responder(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 39, in __call__\r\n    await self.app(scope, receive, self.send_with_gzip)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/prefect/server/utilities/server.py\", line 47, in handle_response_scoped_depends\r\n    response = await default_handler(request)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/prefect/server/api/task_runs.py\", line 72, in create_task_run\r\n    model = await models.task_runs.create_task_run(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/prefect/server/database/dependencies.py\", line 168, in async_wrapper\r\n    return await func(db, *args, **kwargs)  # type: ignore\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/prefect/server/models/task_runs.py\", line 80, in create_task_run\r\n    await session.execute(insert_stmt)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/ext/asyncio/session.py\", line 461, in execute\r\n    result = await greenlet_spawn(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\r\n    result = context.throw(*sys.exc_info())\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 2362, in execute\r\n    return self._execute_internal(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 2247, in _execute_internal\r\n    result: Result[Any] = compile_state_cls.orm_execute_statement(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/orm/bulk_persistence.py\", line 1294, in orm_execute_statement\r\n    result = conn.execute(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1418, in execute\r\n    return meth(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py\", line 515, in _execute_on_connection\r\n    return connection._execute_clauseelement(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1640, in _execute_clauseelement\r\n    ret = self._execute_context(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1846, in _execute_context\r\n    return self._exec_single_context(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1986, in _exec_single_context\r\n    self._handle_dbapi_exception(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 2355, in _handle_dbapi_exception\r\n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\r\n    self.dialect.do_execute(\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 941, in do_execute\r\n    cursor.execute(statement, parameters)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 147, in execute\r\n    self._adapt_connection._handle_exception(error)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 298, in _handle_exception\r\n    raise error\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/dialects/sqlite/aiosqlite.py\", line 129, in execute\r\n    self.await_(_cursor.execute(operation, parameters))\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\r\n    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\r\n    value = await result\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/aiosqlite/cursor.py\", line 48, in execute\r\n    await self._execute(self._cursor.execute, sql, parameters)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/aiosqlite/cursor.py\", line 40, in _execute\r\n    return await self._conn._execute(fn, *args, **kwargs)\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/aiosqlite/core.py\", line 132, in _execute\r\n    return await future\r\n  File \"/home/my_username/python_environment/.venv/lib/python3.10/site-packages/aiosqlite/core.py\", line 115, in run\r\n    result = function()\r\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked\r\n[SQL: INSERT INTO task_run (flow_run_id, task_key, dynamic_key, cache_key, cache_expiration, task_version, flow_run_run_count, empirical_policy, task_inputs, tags, labels, name, run_count, total_run_time, id, created, updated) VALUES (:flow_run_id, :task_key, :dynamic_key, :cache_key, :cache_expiration, :task_version, :flow_run_run_count, :empirical_policy, :task_inputs, :tags, :labels, :name, :run_count, :total_run_time, :id, :created, :updated) ON CONFLICT (flow_run_id, task_key, dynamic_key) DO NOTHING]\r\n[parameters: {'flow_run_id': '541ccbb0-372e-4580-862c-3a5b56177058', 'task_key': 'process_case_flow-a8ab69b3', 'dynamic_key': '0', 'cache_key': None, 'cache_expiration': None, 'task_version': '5fd26d2d027b6eb680eda186427acafc', 'flow_run_run_count': 0, 'empirical_policy': '{\"max_retries\": 0, \"retry_delay_seconds\": 0.0, \"retries\": 0, \"retry_delay\": 0, \"retry_jitter_factor\": null}', 'task_inputs': '{\"case_data\": [{\"input_type\": \"task_run\", \"id\": \"08e56be0-6a98-4122-898a-1111edbc8ebd\"}]}', 'tags': '[]', 'labels': '{\"prefect.flow.id\": \"b7b42394-69e0-4734-9921-b99a2292fa38\", \"prefect.flow-run.id\": \"541ccbb0-372e-4580-862c-3a5b56177058\"}', 'name': 'Process Case 1a-0', 'run_count': 0, 'total_run_time': '1970-01-01 00:00:00.000000', 'id': '924ccd4a-285f-46c2-b529-05ef671e8f1f', 'created': '2024-12-10 16:29:36.273078', 'updated': '2024-12-10 16:29:47.872167'}]\r\n(Background on this error at: https://sqlalche.me/e/20/e3q8)\r\n11:29:52.994 | WARNING | prefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 16.116992 seconds to run, which is longer than its loop interval of 5 seconds.\r\n11:29:52.998 | WARNING | prefect.server.services.failexpiredpauses - FailExpiredPauses took 16.119 seconds to run, which is longer than its loop interval of 5.0 seconds.\r\n11:29:53.019 | WARNING | prefect.server.services.flowrunnotifications - FlowRunNotifications took 19.135031 seconds to run, which is longer than its loop interval of 4 seconds.\r\n11:29:53.162 | WARNING | prefect.server.services.foreman - Foreman took 16.29065 seconds to run, which is longer than its loop interval of 15.0 seconds.\r\n11:30:44.210 | WARNING | prefect.server.services.failexpiredpauses - FailExpiredPauses took 6.207736 seconds to run, which is longer than its loop interval of 5.0 seconds.\r\n11:30:44.268 | WARNING | prefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 6.266031 seconds to run, which is longer than its loop interval of 5 seconds.\r\n11:30:44.294 | WARNING | prefect.server.services.marklateruns - MarkLateRuns took 6.318531 seconds to run, which is longer than its loop interval of 5.0 seconds.\r\n11:30:44.333 | WARNING | prefect.server.services.flowrunnotifications - FlowRunNotifications took 7.302994 seconds to run, which is longer than its loop interval of 4 seconds.\r\n11:31:04.703 | WARNING | prefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 10.43387 seconds to run, which is longer than its loop interval of 5 seconds.\r\n11:31:04.749 | WARNING | prefect.server.services.marklateruns - MarkLateRuns took 10.451921 seconds to run, which is longer than its loop interval of 5.0 seconds.\r\n11:31:04.754 | WARNING | prefect.server.services.flowrunnotifications - FlowRunNotifications took 12.412788 seconds to run, which is longer than its loop interval of 4 seconds.\r\n11:31:04.760 | WARNING | prefect.server.services.failexpiredpauses - FailExpiredPauses took 10.545929 seconds to run, which is longer than its loop interval of 5.0 seconds.\r\n11:31:35.370 | WARNING | prefect.server.services.foreman - Foreman took 42.202888 seconds to run, which is longer than its loop interval of 15.0 seconds.\r\n11:31:35.429 | WARNING | prefect.server.services.failexpiredpauses - FailExpiredPauses took 25.666847 seconds to run, which is longer than its loop interval of 5.0 seconds.\r\n11:31:35.441 | WARNING | prefect.server.services.marklateruns - MarkLateRuns took 25.68895 seconds to run, which is longer than its loop interval of 5.0 seconds.\r\n11:31:35.445 | WARNING | prefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 25.739011 seconds to run, which is longer than its loop interval of 5 seconds.\r\n11:31:35.495 | WARNING | prefect.server.services.flowrunnotifications - FlowRunNotifications took 26.73926 seconds to run, which is longer than its loop interval of 4 seconds.\r\n11:32:06.644 | WARNING | prefect.server.services.flowrunnotifications - FlowRunNotifications took 31.147717 seconds to run, which is longer than its loop interval of 4 seconds.\r\n11:32:06.906 | WARNING | prefect.server.services.foreman - Foreman took 31.535556 seconds to run, which is longer than its loop interval of 15.0 seconds.\r\n\r\n```"
      },
      {
        "user": "will-uai",
        "body": "These seem to be related (https://github.com/PrefectHQ/prefect/issues/16304)\r\nWe are also experiencing these issues."
      },
      {
        "user": "CorMazz",
        "body": "It definitely has something to do with the database. I figured I'd test out running with an in-memory database and I had no issues whatsoever.\r\n\r\n```bash\r\n(python-environment) bash-4.4$ prefect config set PREFECT_API_DATABASE_CONNECTION_URL=\"sqlite+aiosqlite:///file::memory:?cache=shared&uri=true&check_same_thread=false\"\r\n\r\nSet 'PREFECT_API_DATABASE_CONNECTION_URL' to 'sqlite+aiosqlite:///file::memory:?cache=shared&uri=true&check_same_thread=false'.\r\nUpdated profile 'local'.\r\n```"
      }
    ]
  },
  {
    "issue_number": 10297,
    "title": "Remove/upgrade linux-libc-dev in docker images",
    "author": "jozo",
    "state": "open",
    "created_at": "2023-07-25T07:55:27Z",
    "updated_at": "2025-05-22T20:30:33Z",
    "labels": [
      "bug",
      "security"
    ],
    "body": "### First check\r\n\r\n- [X] I added a descriptive title to this issue.\r\n- [X] I used the GitHub search to find a similar issue and didn't find it.\r\n- [X] I searched the Prefect documentation for this issue.\r\n- [X] I checked that this issue is related to Prefect and not one of its dependencies.\r\n\r\n### Bug summary\r\n\r\nHi,\r\n\r\nour security scanners notified us regarding a few CVEs in Prefect docker images. You should rebuild docker images (to install the newer version of `linux-libc-dev`) or remove `build-essential` at the end of the Dockerfile (I guess `linux-libc-dev` is installed as part of `build-essential`).\r\n\r\n### Reproduction\r\n\r\n```python3\r\ngrype prefecthq/prefect:2.11.0-python3.11 --fail-on medium --only-fixed\r\n```\r\n\r\n\r\n### Error\r\n\r\n```python3\r\nNAME            INSTALLED  FIXED-IN  TYPE  VULNERABILITY   SEVERITY \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2022-48425  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-2124   High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-21255  Unknown   \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-2156   High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-2269   Medium    \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-3090   High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-31084  Medium    \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-3141   High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-3212   Medium    \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-32247  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-32248  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-32250  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-32252  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-32254  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-32257  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-32258  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-3268   High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-3269   High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-3390   High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-34256  Medium    \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-35788  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-35823  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-35824  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-35826  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-35828  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-35829  High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-3609   High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-3610   High      \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-38426  Unknown   \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-38427  Unknown   \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-38428  Unknown   \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-38429  Unknown   \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-38430  Unknown   \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-38431  Unknown   \r\nlinux-libc-dev  6.1.27-1   6.1.37-1  deb   CVE-2023-38432  Unknown   \r\nlinux-libc-dev  6.1.27-1   6.1.38-1  deb   CVE-2023-31248  High      \r\nlinux-libc-dev  6.1.27-1   6.1.38-1  deb   CVE-2023-35001  High\r\n1 error occurred:\r\n\t* discovered vulnerabilities at or above the severity threshold\r\n```\r\n\r\n\r\n### Versions\r\n\r\n```Text\r\nVersion:             2.11.0\r\nAPI version:         0.8.4\r\nPython version:      3.11.4\r\nGit commit:          eeb9e219\r\nBuilt:               Thu, Jul 20, 2023 4:34 PM\r\nOS/Arch:             linux/x86_64\r\nProfile:             default\r\nServer type:         ephemeral\r\nServer:\r\n  Database:          sqlite\r\n  SQLite version:    3.40.1\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "hronecviktor",
        "body": "+1 for upgrade, `6.1.38-1` is in bookworm repos\r\n\r\nGoogle's GKE Security Posture is also complaining about the 37 CVEs\r\n\r\n![image](https://github.com/PrefectHQ/prefect/assets/5576281/0088eae5-44b1-4805-b7fd-a2653faef3c8)\r\n\r\n"
      },
      {
        "user": "jawnsy",
        "body": "Hello there! Thanks for the report.\r\n\r\nWe rebuild against the upstream `python` images in Docker Library, and we do not have any kind of build/package caching, so once this is resolved in the official Docker images, then they should be resolved in our next release build."
      },
      {
        "user": "hronecviktor",
        "body": "Thanks for the info @jawnsy \r\nHave you considered running `apt upgrade` during the docker build or does this go against some reproducibility goal you are aiming for? \r\nI might be missing something here, but I think the chances of an upgrade breaking something (on a regularly updated stable debian base) are miniscule and people can always pin to a hash if they really want to freeze everything. \r\nI would consider upgrading to be a safer alternative to distributing images with unpatched CVEs that have a fix available. \r\nE: No need to run a full upgrade, here's what we are using to upgrade only packages with security fixes:\r\n```apt-get update && apt-get install -y debsecan && apt-get install --no-install-recommends -y $(debsecan --suite bullseye --format packages --only-fixed)```"
      }
    ]
  },
  {
    "issue_number": 9688,
    "title": "Make the `sync_compatible` decorator work with type-checkers",
    "author": "rmorshea",
    "state": "closed",
    "created_at": "2023-05-22T23:15:18Z",
    "updated_at": "2025-05-22T17:17:32Z",
    "labels": [
      "enhancement"
    ],
    "body": "### First check\r\n\r\n- [X] I added a descriptive title to this issue.\r\n- [X] I used the GitHub search to find a similar request and didn't find it.\r\n- [X] I searched the Prefect documentation for this feature.\r\n\r\n### Prefect Version\r\n\r\n2.x\r\n\r\n### Describe the current behavior\r\n\r\nType checkers really dislike the `sync_compatible` decorator that is applied throughout the Prefect codebase. This is because there is no way for them to statically determine whether a given `sync_compatible` decoratored function/method will return an object or a coroutine that, if awaited, will return that object. Presently, the `sync_compatible` decorator returns a function that itself returns a coroutine. This results in complaints in code similar to the following:\r\n\r\n```python\r\nfrom prefect.deployments import Deployment\r\n\r\ndep = Deployment.build_from_flow(...)  # complains unawaited coroutine\r\n\r\nif __name__ == \"__main__\":\r\n    dep.apply()  # complains coroutine has no attribute apply\r\n```\r\n\r\nThis is rather annoying for two reasons:\r\n\r\n1. You need to introduce `type: ignore` comments to get past the issue which makes code harder to read\r\n2. You no longer get the benefits of the type annotations the Prefect team have added throughout their codebase\r\n\r\nUnfortunately the fundamentally dynamic behavior of `sync_compatible` is unlikely to ever be understood by type checkers. As a result, it would be nice if `sync_compatible` were able to provide a reasonable workaround.\r\n\r\n### Describe the proposed behavior\r\n\r\nIn short, what if, instead of the problematic code above. I could somehow declare that I intend to use the sync version of the function/method in question by accessing a `.sync` attribute and calling that instead:\r\n\r\n```python\r\nfrom prefect.deployments import Deployment\r\n\r\ndep = Deployment.build_from_flow.sync(...)\r\n\r\nif __name__ == \"__main__\":\r\n    dep.apply.sync()\r\n```\r\n\r\nThis would allow sync usages to be made explicit while preserving both the original dynamic behavior and its current type annotation which claims that `sync_compatible` functions/methods return coroutines.\r\n\r\nTo get this done, you need to use a `Protocol` that is generic on the return type and signature of the function/method (via [`ParamSpec`](https://peps.python.org/pep-0612/)) in question and which has `sync` method which, instead of returning a coroutine just returns the function's result. Here's what this looks like in practice:\r\n\r\n```python\r\nfrom typing import ParamSpec, TypeVar, Callable, Coroutine, Protocol\r\n\r\nP = ParamSpec(\"P\")\r\nR = TypeVar(\"R\")\r\n\r\ndef sync_compatible(async_func: Callable[P, Coroutine[None, None, R]]) -> SyncCompatible[P, R]: ...\r\n\r\nclass SyncCompatible(Protocol[P, R]):\r\n    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> Coroutine[None, None, R]: ...\r\n    def sync(self, *args: P.args, **kwargs: P.kwargs) -> R: ...\r\n```\r\n\r\nThe changes to runtime behavior would be relatively straightforward. All you need to do is add the sync-only implementation as an attribute of the coroutine function:\r\n\r\n```python\r\ndef sync_compatible(async_func):\r\n    def wrapper(*a, **kw):\r\n        ...\r\n\r\n    wrapper.sync = syncify(async_func)\r\n\r\n    return wrapper\r\n```\r\n\r\nI haven't look at the code in too much detail, but I expect a function like `syncify` already exists in Prefect or one could be created relatively trivially just by using `asyncio.run`.\r\n\r\n### Example Use\r\n\r\n```python\r\nfrom prefect.deployments import Deployment\r\n\r\ndep = Deployment.build_from_flow.sync(...)  # no type checker complaint\r\n\r\nif __name__ == \"__main__\":\r\n    dep.apply.sync()  # no type checker complaint\r\n```\r\n\r\n\r\n### Additional context\r\n\r\nAn `aio` attribute [is also added to the `wrapper`](https://github.com/PrefectHQ/prefect/blob/main/src/prefect/utilities/asyncutils.py#L271) in a similar manner and could likewise be included in `SyncCompatible`.",
    "comments": [
      {
        "user": "rmorshea",
        "body": "I'd be happy to implement this if it's of interest."
      },
      {
        "user": "zanieb",
        "body": "As a starting point, can we just return `Union[T, Awaitable[T]]` instead of `Awaitable[T]`? Then, we can consider following up with specific `sync_` and `async_` protocol methods?"
      },
      {
        "user": "rmorshea",
        "body": "Unfortunately that doesn't really help with issues like \"coroutine has no attribute apply\" since a similar complaint would arise from the possible `Awaitable` return type. It would also have the converse problem of type checkers complaining that you \"cannot await T\".\r\n\r\nIt does allow IntelliSense to give you hints in your IDE though."
      }
    ]
  },
  {
    "issue_number": 17388,
    "title": "create explicit methods for ambiguously typed `Task` kwargs",
    "author": "zzstoatzz",
    "state": "open",
    "created_at": "2025-03-05T18:36:24Z",
    "updated_at": "2025-05-22T17:16:24Z",
    "labels": [
      "enhancement",
      "development",
      "typing"
    ],
    "body": "### Describe the current behavior\n\n## Problem\nThere are many ways to use the task decorator, whether on functions or instance methods,  which today we try to handle all this with overloads.\n\nIn particular, using keyword arguments like `return_state` and `wait_for` in task calls makes proper type checking (nearly, if not) impossible. These kwargs modify the return type of a task (e.g., changing from `R` to `State[R]` when `return_state=True`), which breaks static type checking in `mypy` and `pyright`. This creates frustrating developer experiences and makes it difficult to rely on type checkers for preventing bugs.\n\n```python\nstate = my_task(arg1, return_state=True)  # we can't quite handle all cases with overloads\n```\n\n### Describe the proposed behavior\n\nCreate explicit methods on `Task` objects that clearly indicate their return type, e.g.\n- `call_and_return_state`\n- `submit_and_return_state`\n- existing `__call__`\n- existing `submit`\n\n### Example Use\n\n```python\n# Use explicit methods that return exactly what they say\nstate = my_task.call_and_return_state(...)\nstate = my_task.submit_and_return_state(...)\n```\n\n### Additional context\n\nWe'll want to re-use some interface so we don't have to maintain even more methods in different places\n\nrelated to #17379 ",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "supersedes https://github.com/PrefectHQ/prefect/issues/11210"
      }
    ]
  },
  {
    "issue_number": 11210,
    "title": "await task.map() and await task.map(return_state=True) should have same type hints has (sync) task.map and task.map(return_state=True)",
    "author": "maitlandmarshall",
    "state": "closed",
    "created_at": "2023-11-20T03:07:40Z",
    "updated_at": "2025-05-22T17:15:39Z",
    "labels": [
      "development"
    ],
    "body": "### First check\n\n- [X] I am a contributor to the Prefect codebase\n\n### Description\n\nGiven an async task like this:\r\n![image](https://github.com/PrefectHQ/prefect/assets/38200645/d24764fb-ade2-48ed-9383-4a9029e0df55)\r\n\r\nThe awaited async get.map should have the same type hints as a sync get.map:\r\n\r\nasync:\r\n![image](https://github.com/PrefectHQ/prefect/assets/38200645/960b5e97-9a22-44bc-b3ed-7ef74072f32d)\r\n\r\nsync:\r\n![image](https://github.com/PrefectHQ/prefect/assets/38200645/70308aaf-eaef-4c4a-aa00-4c60bf425437)\r\n![image](https://github.com/PrefectHQ/prefect/assets/38200645/1de2be5a-b664-4b57-a42d-af51e6079716)\r\n\n\n### Impact\n\nDeveloper experience is important to me. Type validations are useful too.\r\nNot much actual impact, can manually type hint the variable.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @maitlandmarshall - thanks for the issue!\r\n\r\nwe will look into this as soon as we can"
      },
      {
        "user": "zzstoatzz",
        "body": "closing in favor of issue linked above"
      }
    ]
  },
  {
    "issue_number": 18069,
    "title": "Custom Run Timezone follows local machine timezone instead of Timezone set in Preferences",
    "author": "robfreedy",
    "state": "open",
    "created_at": "2025-05-15T14:29:35Z",
    "updated_at": "2025-05-22T16:48:49Z",
    "labels": [
      "bug",
      "UI/UX"
    ],
    "body": "### Bug summary\n\nCurrent Behavior: In the UI, when selecting the Custom Run for a deployed flow that has a datetime parameter, the Datetime widget incorrectly converts the datetime passed in to the local machine's timezone instead (which if expected behavior, the conversion is incorrect) \n\nExpected behavior: Using the timezone specified in the user's preferences when submitting datetime parameters through the datetime widget. \n\nReproduction: \n\nRun the following in your local environment\n \n```Python \nfrom prefect import flow\nfrom datetime import datetime\n\n@flow\ndef datetime_flow(input_datetime: datetime):\n    print(f\"Received datetime: {input_datetime}\")\n    return input_datetime\n\nif __name__ == \"__main__\":\n    # Example usage with current time\n    datetime_flow.serve()\n```\n\nIn user preferences, select UTC as the timezone.\n\n<img width=\"1485\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bc05e89d-2a30-4408-a41a-35633bf6e884\" />\n\nThen, go to the deployment and hit custom run. \n\n<img width=\"1175\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f3fdfffd-ab17-4426-815a-0eb904a9b217\" />\n\nLook at the result flow runs inputted parameters and see the incorrect datetime as it takes the local machine's time/timezone (for me this is UTC‚àí05:00 or EST)  \n\n<img width=\"1175\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cbf67395-bf50-4c47-b05d-f6fb845b0649\" />\n\n### Version info\n\n```Text\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.11.10\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             darwin/arm64\nProfile:             test\nServer type:         cloud\nPydantic version:    2.10.2\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18141,
    "title": "In the new prefect UI, task/flow status keeps \"resetting\"",
    "author": "liveeo-philipp",
    "state": "open",
    "created_at": "2025-05-22T12:21:36Z",
    "updated_at": "2025-05-22T16:47:14Z",
    "labels": [
      "bug",
      "UI/UX"
    ],
    "body": "### Bug summary\n\nIn the new prefect UI, the icon for the task or flow status keeps reloading, resetting to (...). This results in a UI where most of the time, the actual status is not visible, which makes it asserting the status of your flows/tasks rather difficult.\n\nSee screen recording for an example.\n\n### Version info\n\n```Text\nVersion:             2.20.18\nAPI version:         0.8.4\nPython version:      3.10.12\nGit commit:          b7059e89\nBuilt:               Wed, Apr 30, 2025 2:37 PM\nOS/Arch:             linux/x86_64\nProfile:             default\nServer type:         ephemeral\nServer:\n  Database:          sqlite\n  SQLite version:    3.31.1\n```\n\n### Additional context\n\nhttps://github.com/user-attachments/assets/1660e065-6e78-4929-805c-20c658307ac6",
    "comments": []
  },
  {
    "issue_number": 18143,
    "title": "Task tabs hidden in the new UI",
    "author": "liveeo-philipp",
    "state": "open",
    "created_at": "2025-05-22T12:31:39Z",
    "updated_at": "2025-05-22T16:46:35Z",
    "labels": [
      "enhancement",
      "UI/UX"
    ],
    "body": "### Describe the current behavior\n\nIn the new flow run UI, the list of task in the left column does not show tags anymore (compared to the previous UI version).\n\n### Describe the proposed behavior\n\nTags should be visible when scrolling through the list of tags.\n\n### Example Use\n\nWe often use tags to differentiate between individual task runs ‚Äì for example, they could correlate to the input data or other parameters the task is called with. It was very helpful to at a glance get an intuition for why a subset of tasks failed ‚Äì for example, if they all have the same parameter (different from the ones that succeeded). It was also a great way to select a slightly larger number of tasks based on their input parameters (e.g. to open their logs in a new tabs) without having to filter for each individual one.\nIn general, the new UI removes a previously available great tool to better understand what's happening in your tasks at a glance, which will make understanding failures more challenging in our use-cases.\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18080,
    "title": "Call in prefect._internal.concurrency.calls raises AttributeError on equality",
    "author": "willhcr",
    "state": "closed",
    "created_at": "2025-05-16T05:16:59Z",
    "updated_at": "2025-05-22T15:42:14Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nCall objects stored in contextvars raise AttributeError when compared using != after internal attributes (args, kwargs) have been deleted. This code `del self.args, self.kwargs` is run in a few places in Call. The error occurs because the class uses an auto-generated `__eq__` from `@dataclass`, which accesses these now-missing fields during comparison.\n\nThis led to an error when using async code with prefect and Django in asgiref. Fixing this would allow Prefect tasks and Django async ORM methods to interoperate.\n\n```python\n@task\nasync def get_account(account_uuid: UUID):\n    return await Account.objects.select_related('domain').aget(uuid=account_uuid)\n```\n\nException from asgiref:\n```python\ndef _restore_context(context: contextvars.Context) -> None:\n    # Check for changes in contextvars, and set them to the current\n    # context for downstream consumers\n    for cvar in context:\n        cvalue = context.get(cvar)\n        try:\n            if cvar.get() != cvalue:\n                cvar.set(cvalue)\n        except LookupError:\n            cvar.set(cvalue)\n```\n\n```\n  if cvar.get() != cvalue:\nAttributeError: 'Call' object has no attribute 'args'\n```\n\n\n\n### Version info\n\n```Text\nVersion:             3.3.7\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          8f86aaee\nBuilt:               Mon, Apr 28, 2025 03:04 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.10.6\nServer:\n  Database:          sqlite\n  SQLite version:    3.47.1\nIntegrations:\n  prefect-azure:     0.4.4\n  prefect-docker:    0.6.4\n```\n\n### Additional context\n\nImplementing object equality resolved this issue. But is attribute level comparison used elsewhere with Call?\n\n```python\nclass Call(Generic[T]):\n...\n    def __eq__(self, other):\n        return self is other\n```\n\n",
    "comments": [
      {
        "user": "willhcr",
        "body": "Or perhaps one of these approaches would be right:\n\n```python\ndef __eq__(self, other):\n    if not isinstance(other, Call):\n        return NotImplemented\n\n    try:\n        self_args = self.args\n        self_kwargs = self.kwargs\n        other_args = other.args\n        other_kwargs = other.kwargs\n    except AttributeError:\n        # Option 1: fallback to identity comparison\n        return self is other\n\n        # Option 2: treat dropped Calls as always unequal\n        # return False\n\n    return (\n        self.future == other.future and\n        self.fn == other.fn and\n        self_args == other_args and\n        self_kwargs == other_kwargs and\n        self.context == other.context and\n        self.timeout == other.timeout and\n        self.runner == other.runner\n    )\n```"
      },
      {
        "user": "GalLadislav",
        "body": "Hi, this is something i would love to have resolved too. I would suggest to reasign `self.args` and `self.kwargs` to empty objects (after the `del` call) and then use internal flag to signal that the values were dropped. But i am not sure where in the code it relies on `AttributeError` being raised other than in `Call.__repr__`.\n\nI can create the PR if this suggestion (or resolving the issue) is approved."
      },
      {
        "user": "zzstoatzz",
        "body": "hi @willhcr and @GalLadislav - thanks for the detail! I've started on this here: https://github.com/PrefectHQ/prefect/pull/18110\n\nwe generally want to be careful with changes to `_internal/concurrency` as it underpins quite a lot, so we'll plan to test it a bit more thoroughly. any poking/tests you want to do there would be appreciated!"
      }
    ]
  },
  {
    "issue_number": 18138,
    "title": "Flow engine does not execute pull steps when loading flow from flow run",
    "author": "GalLadislav",
    "state": "open",
    "created_at": "2025-05-21T21:24:08Z",
    "updated_at": "2025-05-22T13:09:49Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI am using process worker to run deployments with pull steps. When flow run process starts it crashes on ModuleNotFoundError when loading flow from flow run. No pull steps ran prior to that.\n\nMy deployment has defined pull steps (from git repository) and entrypoint defined as module path. I am defining my deployment using Python instead of prefect.yaml and deploying using deploy function. Example below:\n\n```python\nfrom myflows import my flow\nfrom prefect.flows import EntrypointType\nfrom prefect.deployments import deploy\nfrom prefect.runner.storage import GitRepository\n\n\ndeployment = flow.to_deployment(\n    name=\"MyDeployment\",\n    work_pool_name=\"default\",\n    concurrency_limit=1,\n    entrypoint_type=EntrypointType.MODULE_PATH,\n)\ndeployment._storage = GitRepository(url=\"...\")\n\n\nif __name__ == \"__main__\":\n    deploy(\n        deployment,\n        work_pool_name=\"default\",\n        build=False,\n        push=False,\n    )\n\n```\n\nMy investigation lead me to here:\nhttps://github.com/PrefectHQ/prefect/blob/7d4d41615e614b9f46eb317a2948a3d0643a07f5/src/prefect/flows.py#L2566\n\nand more precisely to this section: \nhttps://github.com/PrefectHQ/prefect/blob/7d4d41615e614b9f46eb317a2948a3d0643a07f5/src/prefect/flows.py#L2596\nwhen an entrypoint is given as module path, it loads the flow without running pull steps first.\n\nIs it possible to run pull steps before that?\n\n### Version info\n\n```Text\nVersion:             3.4.2\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          c3c1c119\nBuilt:               Mon, May 19, 2025 04:04 PM\nOS/Arch:             linux/x86_64\nProfile:             server\nServer type:         unconfigured\nPydantic version:    2.11.4\nIntegrations:\n  prefect-email:     0.4.2\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18134,
    "title": "prefect_docker: Default volumes not mounted when running docker flows directly",
    "author": "Rahlir",
    "state": "open",
    "created_at": "2025-05-21T15:58:28Z",
    "updated_at": "2025-05-22T09:30:54Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen running [flows directly in dynamic infrastructure](https://docs.prefect.io/v3/deploy/submit-flows-directly-to-dynamic-infrastructure), I believe there is a bug when spinning up new docker images when no storage is configured for the docker work pool. Let's say you create a docker work-pool 'docker' and set volumes to `[\"result-storage:/result-storage\"]`. This volume is not mounted in the flow container. Using `docker inspect {imageTag}` on the image created by the worker, I can see that:\n```json\n\"Volumes\": {\n                \"/tmp/\": {}\n            }\n```\n\nI think the issue is in the file `prefect_docker/worker.py`. Specifically [here](https://github.com/PrefectHQ/prefect/blob/7d4d41615e614b9f46eb317a2948a3d0643a07f5/src/integrations/prefect-docker/prefect_docker/worker.py#L507). The problem seems to be with the path `configuration.properties.volumes.default`. In the docker base job template, the default volumes are stored under the key `variables.properties.volumes.default`...\n\n### Version info\n\n```Text\nVersion:             3.4.0\nAPI version:         0.8.4\nPython version:      3.11.9\nGit commit:          c80e4442\nBuilt:               Fri, May 02, 2025 08:02 PM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.3\nIntegrations:\n  prefect-docker:    0.6.4\n  prefect-kubernetes: 0.5.10\n  prefect-aws:       0.5.10\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @Rahlir - thanks for the report! just to clarify the scenario you're talking about, it sounds like you're saying that\n\nthe `volume` that is a default on the work pool (which does not have configured storage via `prefect work-pool storage configure`) is not being inherited by the eventual flow run container\n\nis that right?\n\nif you have a complete MRE, please feel free to share"
      },
      {
        "user": "Rahlir",
        "body": "Yes, that is correct.\n\nFor the MRE (I am sorry I am unable to provide you a fully docker based setup right now, let me know if you are unable to reproduce it using the steps outlined below and I will try to spin up a docker-compose example):\n\nHere is the base job template for the docker work-pool:\n\n[local-template-copy.json](https://github.com/user-attachments/files/20384899/local-template-copy.json)\n\nNow, use this template with docker workpool by running `prefect work-pool create \"docker-example\" --type docker --base-job-template local-template-copy.json` (note that network `orchestrator-network` needs to exist, you can create one with `docker network create -d bridge orchestrator-network`; also the volume `result-storage` should exist, run `docker volume create result-storage`). You can see that the volume is specified in the GUI when checking the work-pool configuration:\n\n![Image](https://github.com/user-attachments/assets/0d456dbc-d16f-4dc8-bcf0-9110281d10a6)\n\nNow, run this code:\n\n```python\nimport asyncio\nfrom pathlib import Path\n\nfrom prefect import flow\nfrom prefect.logging import get_run_logger\n\n@flow\nasync def sample_flow():\n    run_logger = get_run_logger()\n    path_to_volume = Path(\"/result-storage\")\n    run_logger.info(f\"Does the volume exist? -> {path_to_volume.exists()}\")\n\n\nasync def main():\n    async with DockerWorker(work_pool_name=\"docker-example\") as worker:\n        await worker.submit(sample_flow)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nThe flow will log _\"Does the volume exist? -> False\"_. You can also see that the volume was not mounted with `docker inspect {createdImageTag}`. The only mounted volume is \"/tmp/\"."
      }
    ]
  },
  {
    "issue_number": 17686,
    "title": "Tag Concurrency Limits Break for Submitted Tasks from 3.2.14 -> 3.2.15",
    "author": "jonahduffin",
    "state": "closed",
    "created_at": "2025-04-01T23:40:57Z",
    "updated_at": "2025-05-21T19:30:39Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen upgrading from Prefect 3.2.14 to 3.2.15 and above (including the latest 3.3.1), we observed that task tag concurrency limits broke. Submitted tasks that have not yet starting executing (as they're waiting for dependent tasks defined in the `wait_for` parameter to finish) immediately consume a concurrency slot after being submitted but before beginning execution. \n\nThis is very bad-- sometimes the concurrency slots can be filled 100% by submitted tasks that are waiting on other tasks to finish before executing, which deadlocks all tasks, as the tasks the pending ones depend on will never get a slot to execute themselves. Even if tasks can trickle through, they're throttled much more than the concurrency limit should throttle them.\n\nMRE:\n```python\nimport asyncio\nimport time\n\nfrom prefect import flow, task, get_client\n\n\nasync def set_task_concurrency_limits_async(task_limits: dict[str, int]) -> None:\n    async with get_client() as client:\n        for task_tag, limit in task_limits.items():\n            await client.create_concurrency_limit(tag=task_tag, concurrency_limit=limit)\n\n\n@task(tags=[\"mre_concurrency_limit\"])\ndef concurrently_limited_task(seconds: int):\n    time.sleep(seconds)\n\n\n@flow\ndef concurrency_mre():\n    asyncio.run(set_task_concurrency_limits_async({\"mre_concurrency_limit\": 10}))\n\n    futures = []\n\n    prev_future = None\n    for i in range(20):\n        current_future = concurrently_limited_task.submit(seconds=5, wait_for=prev_future)\n        futures.append(current_future)\n        prev_future = current_future\n\n    [f.wait() for f in futures]\n\n\nif __name__ == \"__main__\":\n    concurrency_mre()\n```\n\nThis issue is not present in Prefect 3.2.14 or earlier. Upon upgrading the client (and server, but I believe the issue is client-side) to 3.2.15, this issue is produced, so I figure the issue is somewhere in here:\n\nhttps://github.com/PrefectHQ/prefect/compare/3.2.14...3.2.15\n\n### Version info\n\n```Text\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.10.16\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:28 PM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.0b1\nIntegrations:\n  prefect-aws:       0.5.9\n```\n\n### Additional context\n\nAs you can see, pending tasks occupy concurrency slots for the above MRE. This is new behavior in 3.2.15 and very bad for us. We would expect to see only Running tasks in here (or Retrying, etc).\n\n![Image](https://github.com/user-attachments/assets/43d85f4b-d647-4f2a-bd33-00ab2024f3a0)\n\nSince tasks should execute sequentially in this MRE as seen in this dependency chart, there should only be one task occupying a concurrency slot at once.\n![Image](https://github.com/user-attachments/assets/a1c8fa1f-4e64-444f-bbbd-991ffd5913cc)\n\nIn this particularly bad case, all concurrency slots were filled by pending tasks, so the tasks they depend on never got a slot to execute and all tasks with this tag were deadlocked.\n![Image](https://github.com/user-attachments/assets/6a49071b-fd29-4145-9a6d-09df7278e19b)\n\nNote that the task execution has stopped after 5 tasks, with 15 more to go, due to this deadlock\n![Image](https://github.com/user-attachments/assets/466b224e-2d5b-4513-b46d-7934504abbf8)",
    "comments": [
      {
        "user": "cicdw",
        "body": "Thank you for the well written issue @jonahduffin - I can reproduce; it was almost certainly [this PR](https://github.com/PrefectHQ/prefect/pull/17597) that caused the bug. It's not obvious to me why this deadlocks yet but I'm looking into it."
      },
      {
        "user": "cicdw",
        "body": "OK so I think this is some kind of threading deadlock specific to the `ConcurrentTaskRunner`: if I change the task runner the example runs smoothly:\n\n```python\nfrom prefect_dask import DaskTaskRunner\n\n...\n@flow(task_runner=DaskTaskRunner())\n...\n```\n\nInterestingly, I also got it to complete by switching to a fully asynchronous task and flow (along with `await asyncio.sleep`).\n\nI will probably need some help investigating this so it might take some time to get to the bottom of."
      },
      {
        "user": "jonahduffin",
        "body": "Thanks @cicdw for the details. I'll stay tuned for an update."
      }
    ]
  },
  {
    "issue_number": 11456,
    "title": "No depenency arrows for between sub-flows",
    "author": "thomasfrederikhoeck",
    "state": "open",
    "created_at": "2023-12-18T10:30:30Z",
    "updated_at": "2025-05-21T17:21:59Z",
    "labels": [
      "bug",
      "3.x",
      "2.x"
    ],
    "body": "### First check\n\n- [X] I added a descriptive title to this issue.\n- [X] I used the GitHub search to find a similar issue and didn't find it.\n- [X] I refreshed the page and this issue still occurred.\n- [X] I checked if this issue was specific to the browser I was using by testing with a different browser.\n\n### Bug summary\n\nOnly tasks show depedency arrows not sub-flows despit having `wait_for`. See image\r\n\r\n\r\n\n\n### Reproduction\n\n```\r\nfrom prefect.deployments import run_deployment\r\nfrom prefect import task, flow, serve\r\n\r\n@task\r\ndef task_one():\r\n    return 1\r\n\r\n@flow\r\ndef my_sub_flow():\r\n    pass\r\n\r\n@flow\r\ndef my_flow():\r\n    one1 = task_one.submit()\r\n    flow1 = my_sub_flow(wait_for=[one1])\r\n    one2 = task_one.submit()\r\n    flow2 = my_sub_flow(wait_for=[flow1])\r\n    flow3 = my_sub_flow(wait_for=[flow2,one2])\r\n```\n\n### Error\n\n![image](https://github.com/PrefectHQ/prefect/assets/44194839/d7b64dc9-0ed5-4f0b-94db-06048fdd7c3c)\r\n\n\n### Browsers\n\n- [X] Chrome\n- [ ] Firefox\n- [ ] Safari\n- [ ] Edge\n\n### Prefect version\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "rohullaa",
        "body": "Hey!\r\n\r\nI am experiencing the same bug in UI. I am trying to run a simpler version of example code from **datarootsio/prefect-dbt-flow** : \r\n\r\nhttps://github.com/datarootsio/prefect-dbt-flow/blob/main/examples/sample_project/my_dbt_flow_other_deps_dev.py\r\n\r\n## Code\r\n\r\n```\r\nfrom prefect import task, flow\r\nfrom prefect.task_runners import SequentialTaskRunner\r\n\r\n@flow\r\ndef upstream_flow():\r\n    @task\r\n    def upstream_flow_task():\r\n        print(\"upstream flow\")\r\n\r\n    upstream_flow_task()\r\n\r\n@flow\r\ndef downstream_flow():\r\n    @task\r\n    def downstream_flow_task():\r\n        print(\"downstream flow\")\r\n\r\n    downstream_flow_task()\r\n\r\n@task\r\ndef upstream_task():\r\n    print(\"upstream task\")\r\n\r\n@task\r\ndef downstream_task():\r\n    print(\"downstream task\")\r\n\r\n@flow(task_runner=SequentialTaskRunner())\r\ndef main_flow():\r\n    uf_future = upstream_flow(return_state=True)\r\n    ut_future = upstream_task(return_state=True)\r\n\r\n    _test = downstream_flow(wait_for=[uf_future, ut_future])\r\n\r\n    downstream_task(wait_for=[_test])\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main_flow()\r\n```\r\n\r\n## Problem with Prefect version 2.14.16\r\n\r\nThis script executes without any errors. However, when I attempt to visualize the workflow in the UI, I notice a discrepancy: the graph does not display the expected dependency arrows between the flows or subflows. For instance, there should be an arrow indicating a dependency from upstream_flow to downstream_flow. The missing link is evident in this UI screenshot:\r\n\r\n![image](https://github.com/PrefectHQ/prefect/assets/54434823/446d0f24-ab7f-4ab4-ad6c-a495609d0d49)\r\n\r\n## Comparison with Prefect Version 2.14.3\r\n\r\nIn contrast, when the same code is run using Prefect version 2.14.3, the UI correctly displays the workflow, including the expected dependency arrows, as seen in this diagram:\r\n\r\n![image](https://github.com/PrefectHQ/prefect/assets/54434823/d107f799-dd01-45a2-bb57-0020fae73720)\r\n\r\n\r\n## Seeking Clarification\r\n\r\nIs the missing dependency arrow in Prefect version 2.14.16's UI a bug, or has there been a change in the `wait_for `feature for flows in this version? "
      },
      {
        "user": "pleek91",
        "body": "I looked into this today and I wasn't able to ever get the arrow between the flow runs to show up even on prefect 2.14.3. But this is a bug and we would expect the arrows to show up. \r\n\r\nLooks like we're not tracking the dependency. Thanks for reporting!"
      },
      {
        "user": "KirillBortman",
        "body": "Still reproduced in Prefect version 3.2.9"
      }
    ]
  },
  {
    "issue_number": 17749,
    "title": "Improving Daylight-Savings-Time robustness in scheduler",
    "author": "ariebovenberg",
    "state": "open",
    "created_at": "2025-04-05T09:20:52Z",
    "updated_at": "2025-05-21T15:42:22Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nFirst let me say I‚Äôm very happy to see my project (`whenever`) is useful to you. I check in on new adopters of the library to gain valuable feedback on how it‚Äôs used in the real world.\n\nReading prefect‚Äôs datetime code using `whenever`, I see you already take care to account for DST.\nI had a suggestion to improve things a bit more.\n\nIn `server/schemas/schedules.py`, it says:\n\n>             # break the interval into `days` and `seconds` because the datetime\n>             # library will handle DST boundaries properly if days are provided, but not\n>             # if we add `total seconds`. Therefore, `next_date + self.interval`\n>             # fails while `next_date.add(days=days, seconds=seconds)` works.\n\nYes‚Äîbut it‚Äôs a bit more nuanced. The problem more precisely is that `datetime.timedelta` always assumes 24-hour days. So `timedelta(days=2)` and `timedelta(hours=48)` are indistinguishable:\n\n```python\n>>> timedelta(hours=48)\ndatetime.timedelta(days=2)\n>>> timedelta(days=2)\ndatetime.timedelta(days=2)\n```\n\nDue to this, Prefect users would still have problems scheduling ‚Äúexactly 48 hours‚Äù, since the code in `schedules.py` will silently assume this means 2 calendar days. But, of course, assuming the opposite would hurt users that specify `timedelta(days=2)`.\n\nBelow an example where this distinction matters. So long as you can‚Äôt distinguish the two durations, you can‚Äôt have consistently correct code.\n\n```\n2023-03-25T10[Europe/Amsterdam] + 48 hours -> 2023-03-27T11[Europe/Amsterdam]\n2023-03-25T10[Europe/Amsterdam] + 2 days -> 2023-03-27T10[Europe/Amsterdam]\n```\n\n\n\n### Describe the proposed behavior\n\nIn the end, this can‚Äôt be fixed using `datetime.timedelta` (or `pendulum.Duration`), since they don‚Äôt distinguish calendar days from 24-hour periods.\n\nDatetime libraries in other languages have solved this problem by separating calendar days from exact hours. Drawing inspiration from these libraries, `whenever` defines the `DateTimeDelta` type:\n\n```python\n>>> from whenever import DateTimeDelta\n>>> DateTimeDelta(days=2)\nDateTimeDelta(P2D)\n>>> DateTimeDelta(hours=48)\nDateTimeDelta(PT48H)\n```\n\n`ZonedDateTime(‚Ä¶).add(delta)` will then work as expected, distinguishing calendar days from exact hours.\n\nNow, I'm not suggesting to *replace* `timedelta` with `DateTimeDelta` üòâ, but supporting it as an option could help users be explicit when they need to be.\n\n*PS: of course, let me know if there are any other suggestions/complaints about `whenever` so far*\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Thanks so much for the insight @ariebovenberg! I like your suggestion for supporting deltas from `whenever` as a robust way to handle edge cases like the one you described. We're currently only using `whenever` when `prefect` is installed with Python 3.13, so I think we'll wait until we're using `whenever` with all Python versions to add `whenever` deltas as an option for interval schedules.\n\nOne question related to accepting `whenever` deltas: what sort of compatibility does `whenever` have with `pydantic`? We heavily use `pydantic` for serialization and deserialization between our client and API, so we opt for standard library `datetime` objects in our public interfaces. `whenever` compatibility with `pydantic` would help us expand the types that we accept in our scheduling interfaces."
      },
      {
        "user": "ariebovenberg",
        "body": "> [..] what sort of compatibility does whenever have with pydantic? We heavily use pydantic for serialization and deserialization between our client and API, so we opt for standard library datetime objects in our public interfaces. whenever compatibility with pydantic would help us expand the types that we accept in our scheduling interfaces.\n\nI'm not a big Pydantic user myself, but can completely understand how Pydantic support could be useful üëç . I could use your help though in articulating the requirements. For example, I'm unsure if using `Annotated` would be enough for most people, or that it's worth adding `__get_pydantic_core_schema__` directly on the classes.\n\nTo keep this issue on topic, the best place to discuss pydantic support is here: https://github.com/ariebovenberg/whenever/issues/175"
      },
      {
        "user": "ariebovenberg",
        "body": "~FYI I'm close to releasing _basic_ pydantic support in all `whenever` classes (i.e. serializing/deserializing in ISO format), I have a prerelease out if you'd like to try it out: `0.8.1rc1`.~\n\nedit: apologies, this prerelease is broken. Will come back soon with a patched version"
      }
    ]
  },
  {
    "issue_number": 18133,
    "title": "Prefect 2.x task timeout does not work",
    "author": "ConstantinoSchillebeeckx",
    "state": "open",
    "created_at": "2025-05-21T15:38:02Z",
    "updated_at": "2025-05-21T15:38:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen running the following, I would expect the task to timeout:\n\n```python\nimport asyncio\nfrom prefect import flow, task\n\nimport time\n\n@task\nasync def tsk():\n    time.sleep(5)\n    print(\"done sleep\")\n\n@flow\nasync def flo():\n    await tsk.with_options(timeout_seconds=1)()\n\nasyncio.run(flo())\n```\n\nRunning the flow gives me:\n```\n10:30:36.048 | INFO    | prefect.engine - Created flow run 'ebony-limpet' for flow 'flo'\n10:30:36.049 | INFO    | Flow run 'ebony-limpet' - View at https://app.prefect.cloud/account/asdf/workspace/asdf/flow-runs/flow-run/0682df19-c011-7c99-8000-c4ad2455335f\n10:30:36.775 | INFO    | Flow run 'ebony-limpet' - Created task run 'tsk-0' for task 'tsk'\n10:30:36.776 | INFO    | Flow run 'ebony-limpet' - Executing 'tsk-0' immediately...\ndone sleep\n10:30:42.084 | INFO    | Task run 'tsk-0' - Finished in state Completed()\n10:30:42.207 | INFO    | Flow run 'ebony-limpet' - Finished in state Completed('All states completed.')\n```\n\nIf I change the sleep call to `await asyncio.sleep(5)` then I get the expected timeout:\n```\n10:36:40.975 | INFO    | prefect.engine - Created flow run 'celadon-chimpanzee' for flow 'flo'\n10:36:40.976 | INFO    | Flow run 'celadon-chimpanzee' - View at https://app.prefect.cloud/account/asdf/workspace/asdf/flow-runs/flow-run/0682df30-8f2b-727b-8000-9a7a648e260c\n10:36:41.327 | INFO    | Flow run 'celadon-chimpanzee' - Created task run 'tsk-0' for task 'tsk'\n10:36:41.327 | INFO    | Flow run 'celadon-chimpanzee' - Executing 'tsk-0' immediately...\n10:36:42.509 | ERROR   | Task run 'tsk-0' - Encountered exception during execution:\nTraceback (most recent call last):\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 327, in aresult\n    return await asyncio.wrap_future(self.future)\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/engine.py\", line 2169, in orchestrate_task_run\n    result = await call.aresult()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 329, in aresult\n    raise CancelledError() from exc\nprefect._internal.concurrency.cancellation.CancelledError\n10:36:42.619 | ERROR   | Task run 'tsk-0' - Finished in state TimedOut('Task run exceeded timeout of 1.0 seconds TimeoutError: ', type=FAILED)\n10:36:42.626 | ERROR   | Flow run 'celadon-chimpanzee' - Encountered exception during execution:\nTraceback (most recent call last):\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 327, in aresult\n    return await asyncio.wrap_future(self.future)\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/engine.py\", line 2169, in orchestrate_task_run\n    result = await call.aresult()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 329, in aresult\n    raise CancelledError() from exc\nprefect._internal.concurrency.cancellation.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/engine.py\", line 894, in orchestrate_flow_run\n    result = await flow_call.aresult()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 327, in aresult\n    return await asyncio.wrap_future(self.future)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 389, in _run_async\n    result = await coro\n  File \"/Users/constantinoschillebeeckx/Documents/hs-de-workflows/foo.py\", line 13, in flo\n    await tsk.with_options(timeout_seconds=1)()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/api.py\", line 150, in wait_for_call_in_loop_thread\n    return call.result()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 318, in result\n    return self.future.result(timeout=timeout)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 179, in result\n    return self.__get_result()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/3.10.13/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 389, in _run_async\n    result = await coro\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/engine.py\", line 1605, in get_task_call_return_value\n    return await future._result()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/futures.py\", line 237, in _result\n    return await final_state.result(raise_on_failure=raise_on_failure, fetch=True)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/states.py\", line 91, in _get_state_result\n    raise await get_state_exception(state)\nTimeoutError\n10:36:42.863 | ERROR   | Flow run 'celadon-chimpanzee' - Finished in state Failed('Flow run encountered an exception. TimeoutError: ')\nTraceback (most recent call last):\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 327, in aresult\n    return await asyncio.wrap_future(self.future)\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/engine.py\", line 2169, in orchestrate_task_run\n    result = await call.aresult()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 329, in aresult\n    raise CancelledError() from exc\nprefect._internal.concurrency.cancellation.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/constantinoschillebeeckx/Documents/hs-de-workflows/foo.py\", line 16, in <module>\n    asyncio.run(flo())\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/3.10.13/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/3.10.13/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/api.py\", line 150, in wait_for_call_in_loop_thread\n    return call.result()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 318, in result\n    return self.future.result(timeout=timeout)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 179, in result\n    return self.__get_result()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/3.10.13/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 389, in _run_async\n    result = await coro\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/client/utilities.py\", line 100, in with_injected_client\n    return await fn(*args, **kwargs)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/engine.py\", line 410, in create_then_begin_flow_run\n    return await state.result(fetch=True)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/states.py\", line 91, in _get_state_result\n    raise await get_state_exception(state)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/engine.py\", line 894, in orchestrate_flow_run\n    result = await flow_call.aresult()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 327, in aresult\n    return await asyncio.wrap_future(self.future)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 389, in _run_async\n    result = await coro\n  File \"/Users/constantinoschillebeeckx/Documents/hs-de-workflows/foo.py\", line 13, in flo\n    await tsk.with_options(timeout_seconds=1)()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/api.py\", line 150, in wait_for_call_in_loop_thread\n    return call.result()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 318, in result\n    return self.future.result(timeout=timeout)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 179, in result\n    return self.__get_result()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/3.10.13/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 389, in _run_async\n    result = await coro\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/engine.py\", line 1605, in get_task_call_return_value\n    return await future._result()\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/futures.py\", line 237, in _result\n    return await final_state.result(raise_on_failure=raise_on_failure, fetch=True)\n  File \"/Users/constantinoschillebeeckx/.pyenv/versions/hs-de/lib/python3.10/site-packages/prefect/states.py\", line 91, in _get_state_result\n    raise await get_state_exception(state)\nTimeoutError\n```\n\n### Version info\n\n```Text\nVersion:             2.20.13\nAPI version:         0.8.4\nPython version:      3.10.13\nGit commit:          4d50c394\nBuilt:               Wed, Nov 13, 2024 8:48 AM\nOS/Arch:             darwin/arm64\nProfile:             default\nServer type:         cloud\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18124,
    "title": "Pylance Displaying \"Problems\" in VSCode",
    "author": "Bryan-Meier",
    "state": "closed",
    "created_at": "2025-05-20T15:19:37Z",
    "updated_at": "2025-05-20T16:34:30Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen using the Prefect Snowflake connector and going after the connection, Pylance is reporting get_connection as an unknown attribute which isn't the case because it runs fine and is what is in a lot of Prefect's documentation.\n\n```\nfrom prefect import flow\nfrom prefect_snowflake import SnowflakeConnector\nimport pandas as pd\n\n@flow\ndef entrpoint():\n    with SnowflakeConnector.load(\"some_block\").get_connection() as conn:        \n        df_data = pd.read_sql(\"select * from table_a\", conn)\n```\n\nOutput from VSCode \"PROBLEMS\" pane showing Pylance issues:\n```\n[{\n\t\"resource\": \"main.py\",\n\t\"owner\": \"pylance\",\n\t\"code\": {\n\t\t\"value\": \"reportAttributeAccessIssue\",\n\t\t\"target\": {\n\t\t\t\"$mid\": 1,\n\t\t\t\"path\": \"/microsoft/pylance-release/blob/main/docs/diagnostics/reportAttributeAccessIssue.md\",\n\t\t\t\"scheme\": \"https\",\n\t\t\t\"authority\": \"github.com\"\n\t\t}\n\t},\n\t\"severity\": 8,\n\t\"message\": \"Cannot access attribute \\\"get_connection\\\" for class \\\"Coroutine[Any, Any, SnowflakeConnector]\\\"\\n¬†¬†Attribute \\\"get_connection\\\" is unknown\",\n\t\"source\": \"Pylance\",\n\t\"startLineNumber\": 13,\n\t\"startColumn\": 48,\n\t\"endLineNumber\": 13,\n\t\"endColumn\": 62\n},{\n\t\"resource\": \"main.py\",\n\t\"owner\": \"pylance\",\n\t\"code\": {\n\t\t\"value\": \"reportArgumentType\",\n\t\t\"target\": {\n\t\t\t\"$mid\": 1,\n\t\t\t\"path\": \"/microsoft/pylance-release/blob/main/docs/diagnostics/reportArgumentType.md\",\n\t\t\t\"scheme\": \"https\",\n\t\t\t\"authority\": \"github.com\"\n\t\t}\n\t},\n\t\"severity\": 8,\n\t\"message\": \"Argument of type \\\"SnowflakeConnection | Unknown\\\" cannot be assigned to parameter \\\"con\\\" of type \\\"_SQLConnection\\\" in function \\\"read_sql\\\"\\n¬†¬†Type \\\"SnowflakeConnection | Unknown\\\" is not assignable to type \\\"_SQLConnection\\\"\\n¬†¬†¬†¬†Type \\\"SnowflakeConnection\\\" is not assignable to type \\\"_SQLConnection\\\"\\n¬†¬†¬†¬†¬†¬†\\\"SnowflakeConnection\\\" is not assignable to \\\"str\\\"\\n¬†¬†¬†¬†¬†¬†\\\"SnowflakeConnection\\\" is not assignable to \\\"ConnectionEventsTarget\\\"\\n¬†¬†¬†¬†¬†¬†\\\"SnowflakeConnection\\\" is not assignable to \\\"Connection\\\"\",\n\t\"source\": \"Pylance\",\n\t\"startLineNumber\": 14,\n\t\"startColumn\": 56,\n\t\"endLineNumber\": 14,\n\t\"endColumn\": 60\n}]\n```\n\n### Version info\n\n```Text\nVersion:             3.2.15\nAPI version:         0.8.4\nPython version:      3.11.7\nGit commit:          0f5a3081\nBuilt:               Fri, Mar 28, 2025 3:28 PM\nOS/Arch:             win32/AMD64\nProfile:             default\nServer type:         cloud\nPydantic version:    2.11.4\nIntegrations:\n  prefect-snowflake: 0.28.4\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @Bryan-Meier - there are quite a few [static typing issues](https://github.com/PrefectHQ/prefect/discussions/16167) throughout the integrations (like `prefect-snowflake`) which may be distracting and obstruct some understanding of the SDK but should not affect the behavior of the code.\n\nFor example, #15008 is the cause of the first one you're seeing there. I vote we close this for now, as 15008 well articulates this problem and there will be a long tail of fixes we need to make to pass strict typing everywhere - how does that sound?"
      },
      {
        "user": "Bryan-Meier",
        "body": "@zzstoatzz this makes sense to me. My search prior to creating this bug came up empty, which I thought was weird since this has been around for a while. Knowing that there is another open enhancement on this topic, I am good with closing this."
      }
    ]
  },
  {
    "issue_number": 18122,
    "title": "Azure Event Grid Subscription Support",
    "author": "isteemers",
    "state": "open",
    "created_at": "2025-05-20T14:56:42Z",
    "updated_at": "2025-05-20T16:03:38Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nPrefect event webhooks do not support Azure Event Grid‚Äôs subscription validation handshake. When attempting to subscribe Prefect‚Äôs webhook endpoint directly to an Event Grid topic (such as for blob upload events in Azure Blob Storage), the subscription fails because Prefect does not return the expected validation response. As a result, Prefect cannot receive events directly from Azure Event Grid.\n\n### Describe the proposed behavior\n\nPrefect event webhooks should support Azure Event Grid‚Äôs validation handshake protocol. When an Event Grid subscription validation event is sent, Prefect should recognize and respond according to the [Event Grid webhook validation requirements](https://learn.microsoft.com/en-us/azure/event-grid/webhook-event-delivery), enabling direct webhook integration.\n\n### Example Use\n\nA user wants to trigger a Prefect workflow automatically whenever a new file is uploaded to Azure Blob Storage. They configure Blob Storage events to publish to an Event Grid topic, then register a Prefect event webhook as a subscriber. Once validated, Prefect receives blob upload events directly, allowing immediate and seamless pipeline orchestration‚Äîwithout needing to maintain an intermediate Azure Function or relay.\n\n### Additional context\n\nDirect support for Event Grid subscriptions in Prefect would simplify Azure-based data pipelines, reduce architectural complexity, and minimize event delivery latency.\nCurrently, users must build and maintain additional cloud functions or webhook relays solely to handle Event Grid‚Äôs validation handshake. Native support would enable more efficient integrations for anyone using Azure services with Prefect Cloud.",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @isteemers - thanks for the issue! this makes sense to me, in fact I've wanted a version of this for slack app mentions where slack needs a response (so today you need some intermediate service to send the response). it would likely be a non-trivial change but one i think it worth thinking about"
      }
    ]
  },
  {
    "issue_number": 17890,
    "title": "CallWebhook action double json encodes payload",
    "author": "willhcr",
    "state": "closed",
    "created_at": "2025-04-23T04:26:52Z",
    "updated_at": "2025-05-20T13:50:21Z",
    "labels": [
      "bug",
      "good first issue",
      "great writeup"
    ],
    "body": "### Bug summary\n\nWhen creating a payload, which is a jinja2 template, the event json is json encoded againt resulting in an escaped string.\n\n```python\nCallWebhook(block_document_id=webhook_id, payload='{{ event.model_dump_json()}}')\n```\n\n### Expected result\nWebhook receives: `'{\"json\":\"data\"}'`\n\n### Actual Result\nWebhook receives: `'\"{\\\"json\\\":\\\"data\\\"}\"'`\n\n### Details\n\n`payload` is now a str (seems it used to be a dict), but call() is defined as `async def call(self, payload: dict[str, Any] | None = None) -> Response:`\n\nhttps://github.com/PrefectHQ/prefect/blob/093237bfa0681dfcb1baeaee9b96da9803b6a879/src/prefect/server/events/actions.py#L1222\n\n`payload` is passed to httpx request with the `json` keyword which results in the string being json encoded again\n\nhttps://github.com/PrefectHQ/prefect/blob/093237bfa0681dfcb1baeaee9b96da9803b6a879/src/prefect/blocks/webhook.py#L76\n\n### Resolution\n\nIf call() was updated to take a `str` and pass it to httpx.request as `content=payload` it should resolve the issue.\n\n### Version info\n\n```Text\nVersion:             3.3.5\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          db4b7a33\nBuilt:               Thu, Apr 17, 2025 09:25 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-docker:    0.6.3\n  prefect-azure:     0.4.3\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Hey @willhcr! Just so I understand the issue fully, are you providing `event.model_dump_json()` as the payload, and, if so, how are you providing it? Could you use `event.model_dump()` instead?"
      },
      {
        "user": "willhcr",
        "body": "Thanks @desertaxle! To clarify:\n\nYes, I‚Äôm using `{{ event.model_dump_json() }}` as the Jinja2 template, which is rendered by the CallWebhook block. The event object is provided by the Prefect Automations context.\n\nThe key issue is this: event.model_dump_json() produces a JSON string, which is what I want the webhook to receive.\n\nHowever, the rendered result of that Jinja2 template is a str, and the block.call() method (from CallWebhook) currently passes it as json=payload to httpx.request(). This causes double JSON encoding‚Äîthe JSON string gets wrapped in quotes and escaped.\n\nExpected:\n\n`{\"key\": \"value\"}`\n\nActual:\n\n`\"{\\\"key\\\": \\\"value\\\"}\"`\n\nIf I try event.model_dump() instead, it gets rendered as a Python dict string (e.g., \"{'key': 'value'}\"), which is not valid JSON, this string then gets JSON encoded in call() by httpx.request() and fails Event.model_validate_json() with:\n\n`[type=json_invalid, input_value=\"{'occurred': DateTime(20...zinfo=Timezone('UTC'))}\", input_type=str]`\n\nSuggested fix:\n\nUpdate the call() method to detect if the rendered payload is a string and use content=payload instead of json=payload when passing it to httpx.\n\nThis would allow users to control serialization via Jinja templates."
      },
      {
        "user": "desertaxle",
        "body": "Cool, thanks for clarifying! I like the sound of your proposed fix. Would you like to submit a PR for the fix? No worries if not; your explanation is very thorough, so I think this would be a good issue for another contributor to take on."
      }
    ]
  },
  {
    "issue_number": 17239,
    "title": "Display pydantic.Field parameter `examples` for flow parameters",
    "author": "laxmibalami",
    "state": "open",
    "created_at": "2025-02-21T19:08:21Z",
    "updated_at": "2025-05-19T19:58:24Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nPydantic.Field  `examples` parameter allows us to add example values for a given field. However they are not displayed in the prefect cloud UI even if a field has one. \n```python\nfrom prefect import flow\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n   name: str = Field(title=\"name\", description=\"First name\", examples=[\"Bob\", \"Alice\"])\n\n@flow\ndef main(params: Foo):\n   pass\n```\n\n![Image](https://github.com/user-attachments/assets/b24447c1-e343-4f2d-bf6d-c1375bb61eb4)\n\n### Describe the proposed behavior\n\nFields with examples are displayed in the UI separate/distinct from the description.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @laxmibalami - thanks for the issue!\n\nthis makes sense to me as a UI enhancement, since the information is already present in the parameter schema\n\n```python\n¬ª ipython\n\n#[1]\nfrom prefect import flow\nfrom pydantic import BaseModel, Field\n\nclass Foo(BaseModel):\n   name: str = Field(title=\"name\", description=\"First name\", examples=[\"Bob\", \"Alice\"])\n\n@flow\ndef main(params: Foo):\n   pass\n\n#[2]\nmain.parameters.model_dump()\nOut[2]:\n{'title': 'Parameters',\n 'type': 'object',\n 'properties': {'params': {'$ref': '#/definitions/Foo',\n   'position': 0,\n   'title': 'params'}},\n 'required': ['params'],\n 'definitions': {'Foo': {'properties': {'name': {'description': 'First name',\n     'examples': ['Bob', 'Alice'],\n     'title': 'name',\n     'type': 'string'}},\n   'required': ['name'],\n   'title': 'Foo',\n   'type': 'object'}}}\n```"
      },
      {
        "user": "bkkkk",
        "body": "Just to be clear on the intention here, is this in lieu of/addition to using enums to define a strict set of acceptable values? or to add a consistently formatted list of example values in the field description??"
      },
      {
        "user": "zzstoatzz",
        "body": "@bkkkk itd be in addition to, `examples` would be a separate piece of metadata that the UI would render, regardless if the type of that value is constrained (like an `Enum` or a `Literal`)"
      }
    ]
  },
  {
    "issue_number": 15865,
    "title": "Too Many Concurrent Attempts to Register Task Definition in ECS Work Pool",
    "author": "torbiczuk",
    "state": "closed",
    "created_at": "2024-10-30T14:52:40Z",
    "updated_at": "2025-05-19T19:55:38Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen multiple Prefect flows start simultaneously using an ECS work pool, they attempt to register a new AWS task definition at the same time. This leads to a ClientException with the message: \"Too many concurrent attempts to create a new revision of the specified family.\"\r\n\r\nI have 5 scheduled flows that start simultaneously each day. These flows attempt to register new task definition each, likely because a new deployment version is created each night.\r\n\r\nI've tried to set these variables in my worker Dockerfile:\r\nENV AWS_RETRY_MODE=adaptive\r\nENV AWS_MAX_ATTEMPTS=100\r\nhowever, this didn't resolve the issue.\r\n\r\n I'm using the same Docker image for each flow with only input parameter differing(I have enabled Match Latest Revision In Family (Optional) but it's not working)\r\n\r\n\n\n### Version info\n\n```Text\nVersion:             2.20.2\r\nAPI version:         0.8.4\r\nPython version:      3.11.9\r\nGit commit:          51c3f290\r\nBuilt:               Wed, Aug 14, 2024 11:27 AM\r\nOS/Arch:             darwin/arm64\r\nProfile:             default\r\nServer type:         server\n```\n\n\n### Additional context\n\nI'm using prefect-aws: 0.4.2\r\n\r\n\r\n**Task Definitions Comparison:**\r\n- Only differences are revision, registeredAt, and registeredBy.\r\n- The Docker image remains the same: prefect-ecs-flow-image:latest.\r\n- Flow run names remain unchanged.\r\n\r\n**Attempts to Mitigate:**\r\n\r\n- Disabled AWS logging to prevent automatic task definition creation.\r\n- Enabled \"Match Latest Revision In Family\" without success.\r\n\r\n**Related Issue:**\r\n[PrefectHQ/prefect#10102](https://github.com/PrefectHQ/prefect/issues/10102)\r\n\r\n\r\n**Environment Variables Set:**\r\ndockerfile\r\nENV AWS_RETRY_MODE=adaptive\r\nENV AWS_MAX_ATTEMPTS=100\r\n\r\n**Task definitions created for the same deployment (differenf flows run):**\r\n[task_definition_1.json](https://github.com/user-attachments/files/17574683/task_definition_1.json)\r\n[task_definition_2.json](https://github.com/user-attachments/files/17574684/task_definition_2.json)",
    "comments": [
      {
        "user": "sys-git",
        "body": "+1\r\n\r\nI'm also facing a similar issue."
      },
      {
        "user": "AndreaPiccione",
        "body": "+1"
      },
      {
        "user": "Samreay",
        "body": "Same here. @torbiczuk did you find a workaround?"
      }
    ]
  },
  {
    "issue_number": 16256,
    "title": "Kubernetes pod lifecycle handling",
    "author": "masonmenges",
    "state": "closed",
    "created_at": "2024-12-06T21:09:11Z",
    "updated_at": "2025-05-19T19:51:34Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nWhen running a worker in Kubernetes it's difficult to handle scenarios involving Pod Evictions both in the context of a job executing flows runs or in the context of a worker pod restarting in either scenario prefect seems to periodically lose context around running jobs resulting in Flows Runs becoming stuck in a Pending/Running state.\r\n\r\n\n\n### Describe the proposed behavior\n\nThe prefect-kuberentes worker/runner would handle Kubernetes pod eviction/scaling events that occur in the context of a flow-run executing within the cluster and report back any failures as a result. [Container lifecycle hooks](https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/) are one came up in conversation as a potential suggestion for handling Kubernetes specific events like pod evictions in the context of a container.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\nTo deal with this users need to configure automations that attempt to cancel long running jobs or trigger notifications when they're occurring so an engineer can triage, this is particularly inconvenient as a good portion of these events occur as a result of leveraging Kubernetes functionality to scale work appropriately based on demand, i.e. scaling the number of worker pods to handle a large influx of jobs and subsequently scaling down when the demand lessens. ",
    "comments": [
      {
        "user": "jpedrick-numeus",
        "body": "#15408"
      },
      {
        "user": "cicdw",
        "body": "This should be resolved in the latest releases of `prefect-kubernetes`"
      }
    ]
  },
  {
    "issue_number": 18096,
    "title": "Need support PostgreSQL TLS authentication",
    "author": "29x10",
    "state": "closed",
    "created_at": "2025-05-18T16:04:56Z",
    "updated_at": "2025-05-19T18:10:40Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nasyncpg currently do not support specifiy `ssl_mode` in connection string, we need to manually pass ssl context to `connect_args['ssl']` when calling `create_async_engine`\n\n### Describe the proposed behavior\n\nwe need extra configuration that allowing us to pass TLS context\n\n| config key | description |\n| --- | --- |\n| PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_ENABLED | Whether PostgreSQL support mTLS authentication |\n| PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_CA_FILE | Path to client certificate ROOT CA |\n| PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_CERT_FILE | Path to client full certificate chain without ROOT CA if we have intermediate certificate |\n| PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_KEY_FILE | Path to client certificate key |\n| PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_CHECK_HOSTNAME | Whether to verify PostgreSQL hostname |\n\n### Example Use\n\nSpecifify connection url with username but without password\n\n```\nprefect config set PREFECT_API_DATABASE_CONNECTION_URL=\"postgresql+asyncpg://postgres_username@localhost:5432/prefect_db_name\"\n```\n\nThen enable TLS configuration\n\n```\nprefect config set PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_ENABLED=True\nprefect config set PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_CA_FILE=/path/to/root/ca/file\nprefect config set PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_CERT_FILE=/path/to/client/cert/file\nprefect config set PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_KEY_FILE=/path/to/client/key\nprefect config set PREFECT_SERVER_DATABASE_SQLALCHEMY_CONNECT_ARGS_TLS_CHECK_HOSTNAME=True\n```\n\nThen when starting prefect server will properly connect to mTLS authentication enabled PostgreSQL database\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18106,
    "title": "Cancellation events picked up by multiple process worker runners in 3.4.1",
    "author": "carettines",
    "state": "closed",
    "created_at": "2025-05-19T13:16:12Z",
    "updated_at": "2025-05-19T16:43:30Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nIn 3.2.X versions of Prefect, there was logic inside of the runner to only attempt processing cancellation requests for runs belonging to the specific runner. In 3.4.1, Prefect seems to have switched to a event/observer model where any cancellation event is broadcast to every runner. However, it looks like the filtering step has been removed from the runner so that every runner will attempt to process every cancellation event.\n\nSteps to reproduce:\n1. Launch flow run in work pool A, picked up by worker/runner A.\n2. Launch worker B running for work pool B.\n3. Cancel flow run from work pool A\n4. Worker/Runner B attempts to process cancellation event despite the fact that the flow run does not belong to it.\n\nI believe [this](https://github.com/PrefectHQ/prefect/pull/17973/files#diff-f60ba0fc2a760d44765aae5daec2fa3c7564830f77f9f78d8ee21137d7474a7f) is what introduced the bug. There is no filter on the Observer itself - it broadcasts every cancellation event regardless of origin. And in the Runner, we have the following code in the event that the Runner cannot find the PID associated with the flow run attached to the cancellation event - it still tries to load the flow and run cancellation hooks:\n\n` \n\n      pid = process_map_entry.get(\"pid\") if process_map_entry else None\n      if not pid:\n          if flow_run.state:\n              await self._run_on_cancellation_hooks(flow_run, flow_run.state)\n          await self._mark_flow_run_as_cancelled(\n              flow_run,\n              state_updates={\n                  \"message\": (\n                      \"Could not find process ID for flow run\"\n                      \" and cancellation cannot be guaranteed.\"\n                  )\n              },\n          )\n          return\n`\n\n\n\n\n\n### Version info\n\n```Text\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.9.4\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             win32/AMD64\nProfile:             default\nServer type:         cloud\nPydantic version:    2.9.2\nIntegrations:\n  prefect-dask:      0.3.3\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Thanks for the issue @carettines! I think your root cause analysis is spot on. I'll work on a fix for this and open a PR ASAP!"
      },
      {
        "user": "carettines",
        "body": "> Thanks for the issue [@carettines](https://github.com/carettines)! I think your root cause analysis is spot on. I'll work on a fix for this and open a PR ASAP!\n\nThank you - in order to prevent lots of unrelated cancellation events from reaching the _cancel_run function, I think passing through a \"related resource\" filter to the observer might be helpful (e.g. only looking for cancellations belonging to the same worker that launched the runner), as well as fully respecting the runner PID map."
      },
      {
        "user": "desertaxle",
        "body": "I'd like to have a filter on the subscription, but the `Runner` is the one that manages the subscription, so we can't always rely on the worker related resource. Plus, we can't currently update the filter after the subscription has been established. I think the fix in https://github.com/PrefectHQ/prefect/pull/18108 should do it."
      }
    ]
  },
  {
    "issue_number": 12938,
    "title": "Ability to save the Dockerfile created by `build_docker_image` deployment step",
    "author": "discdiver",
    "state": "closed",
    "created_at": "2023-06-14T13:09:22Z",
    "updated_at": "2025-05-19T15:23:32Z",
    "labels": [
      "enhancement",
      "good first issue"
    ],
    "body": "### First check\n\n- [X] I added a descriptive title to this issue.\n- [X] I used the GitHub search to find a similar request and didn't find it.\n- [X] I searched the Prefect documentation for this feature.\n\n### Prefect Version\n\n2.x\n\n### Describe the current behavior\n\nNo Dockerfile is persisted.\n\n### Describe the proposed behavior\n\nA Dockerfile is persisted.\n\n### Example Use\n\nThe user could then inspect the Dockerfile to make deployments with Docker more transparent.\r\n\r\nThe user could also then modify the Dockerfile and specify the modified Dockerfile in prefect.yaml.\n\n### Additional context\n\nI talked to @desertaxle about this a bit already. \r\n\r\nIn my own use and at PACC it was observed that this functionality would be helpful for debugging.",
    "comments": [
      {
        "user": "serinamarie",
        "body": "I think if we removed deletion of the Dockerfile at the end of `prefect_docker.steps.build_docker_image`, the Dockerfile will persist, but the build step will still have `dockerfile: auto` which would no longer be accurate the next time that the build step was run, as we'd want to use the Dockerfile they have. So we'd want to think about that."
      },
      {
        "user": "AliLordLoss",
        "body": "Hi, I would like to work on this issue."
      }
    ]
  },
  {
    "issue_number": 16277,
    "title": "Formatting of teams web hook is broken",
    "author": "tharwan",
    "state": "open",
    "created_at": "2024-12-09T14:39:51Z",
    "updated_at": "2025-05-19T15:19:19Z",
    "labels": [],
    "body": "### Bug summary\n\nWhen configuring a teams web hook block using the new workflow based method from microsoft and creating an automation to notify on certain events, the notification does not get formatted properly. It looses all line breaks and contains no links. \r\n\r\nSince the notification text is automatically wrapped in an \"adaptive card\" it appears no to be possible to create properly formatted notifications as has been the case previously using the (now outdated) web hooks from teams directly. \r\n\r\nSee also: https://github.com/PrefectHQ/prefect/issues/14575\r\n\n\n### Version info\n\n```Text\nprefect cloud\n```\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "tharwan",
        "body": "I found a workaround that is good enough for our use case. The text that the automation sends will be interpreted as markdown in teams. Therefore links and line breaks can be created according to markdown syntax. This solves the most essential problems. E.g.\r\n\r\nsubject `{{ flow.name }} run notification`\r\n\r\nand body: \r\n```\r\nFlow run {{ flow.name }}/{{ flow_run.name }} observed in state `{{ flow_run.state.name }}` at {{ flow_run.state.timestamp }}.\r\n\r\n[{{ deployment.name }}]({{ deployment|ui_url }}) / [{{ flow_run.name }}]({{ flow_run|ui_url }})\r\n\r\nState message: {{ flow_run.state.message }} \r\n```\r\n\r\nresult in an workable message in teams."
      },
      {
        "user": "tharwan",
        "body": "One more thing: there is no support for things like block quotes in the markdown block of adaptive cards: https://learn.microsoft.com/en-us/microsoftteams/platform/task-modules-and-cards/cards/cards-format?tabs=adaptive-md%2Cdesktop%2Cdesktop1%2Cdesktop2%2Cconnector-html#format-cards-with-markdown\r\n\r\nso the current default for wrapping the state of a flow in a markdown block does not work, at least in terms of formatting."
      },
      {
        "user": "tharwan",
        "body": "One problem with the missing ability to control the format of the adaptive card is also apparent when displaying the notifications on a mobile device with a small screen, where the [default](https://learn.microsoft.com/en-us/microsoftteams/platform/task-modules-and-cards/cards/design-effective-cards?tabs=design#text) of teams is apparently to clip text instead of wrapping it. "
      }
    ]
  },
  {
    "issue_number": 18064,
    "title": "Support flows defined as staticmethod/classmethod in serve operation",
    "author": "vyagubov",
    "state": "closed",
    "created_at": "2025-05-15T11:28:44Z",
    "updated_at": "2025-05-19T15:04:28Z",
    "labels": [
      "enhancement"
    ],
    "body": "\nTest example:\n```\nfrom orch.flow.demo.depl import Demo\n\nif __name__ == \"__main__\":\n    Demo.flow.serve(name=Demo.deployment_name)\n```\n\n- Demo is a class\n- flow is a @staticmethod decorated with @flow(name=\"Demo\")\n- Prefect version: 3.4.1\n- Python: 3.12\n\nDeployment is created successfully, but in Prefect UI the Entrypoint is shown as:\n```\norch/flow/demo/depl.py:flow\n```\nHowever, the actual entrypoint should be:\n```\norch/flow/demo/depl.py:Demo.flow\n```\n\nWhen I try to run the flow from the UI or CLI, it fails with:\n```\nAttributeError: module '__prefect_loader_4420209840__' has no attribute 'flow'\n```\nFollowed by:\n```\nRuntimeError: Function with name 'flow' not found in 'orch/flow/demo/depl.py'\n```\nI assume flow.serve() doesn't correctly detect static/class method entrypoints when used inside a class.\n\n\n",
    "comments": [
      {
        "user": "vyagubov",
        "body": "I will try to tackle it myself."
      }
    ]
  },
  {
    "issue_number": 17996,
    "title": "fetch_all() returns zero rows with SQLAlchemy ‚â• 2.0.0 using mssql+pymssql",
    "author": "DSauterIntersport",
    "state": "closed",
    "created_at": "2025-05-07T07:49:42Z",
    "updated_at": "2025-05-19T13:32:11Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWe‚Äôre experiencing an issue when querying tables from a Microsoft SQL Server using _prefect_sqlalchemy_ and the _mssql+pymssql_ driver. The problem occurs only with SQLAlchemy versions 2.0.0 and above.\n\n### Minimal Example:\n\n```\n@flow\nasync def f_some_import() -> None:\n    connection = await SqlAlchemyConnector.load(INSERT_BLOCK_NAME_HERE)\n    query = f\"SELECT * FROM dbo.{table}\"\n    with connection as conn:\n        result = conn.fetch_all(operation=query)\n```\nWith SQLAlchemy < 2.0.0, this returns results as expected.\nWith SQLAlchemy ‚â• 2.0.0, `fetch_all()` always returns an **empty result set**.\n\n### Findings:\nThe issue appears to be due to a `commit()` call inside the `_async_sync_execute()` method. If lines 504‚Äì505 (where `commit()` is invoked) are commented out, the `SELECT` query works properly and returns results.\n \n![Image](https://github.com/user-attachments/assets/4ed5bc50-7d34-489a-bc4e-24c879b3c1e5)\n\nAccording to the [SQLAlchemy 2.0 documentation](https://docs.sqlalchemy.org/en/20/orm/session_basics.html#opening-and-closing-a-session), `commit()` is not required ‚Äî and should not be used ‚Äî for pure `SELECT` operations.\n\n### Environment:\n\n- Database: Microsoft SQL Server\n- Driver: mssql+pymssql\n- SQLAlchemy version: 2.0.0 and above\n\n### Version info\n\n```Text\nVersion:             2.20.15\nAPI version:         0.8.4\nPython version:      3.10.17\nGit commit:          b21233f3\nBuilt:               Wed, Nov 27, 2024 10:45 AM\nOS/Arch:             linux/x86_64\nProfile:             postgres\nServer type:         server\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18085,
    "title": "Timing issue when starting multiple workers at the same time due how workpools are managed in _update_local_work_pool_info",
    "author": "ogenstad",
    "state": "open",
    "created_at": "2025-05-16T11:38:57Z",
    "updated_at": "2025-05-16T11:38:57Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nIn our environment we start a number of custom Prefect workers, these are a child class of [BaseWorker](https://github.com/PrefectHQ/prefect/blob/3.4.1/src/prefect/workers/base.py#L436). Occasionally we run into issues where a worker fails to start after calling [setup()](https://github.com/PrefectHQ/prefect/blob/3.4.1/src/prefect/workers/base.py#L915) which calls [sync_with_backend()[https://github.com/PrefectHQ/prefect/blob/3.4.1/src/prefect/workers/base.py#L1103]\n\nThe problem then comes in [_update_local_work_pool_info](https://github.com/PrefectHQ/prefect/blob/3.4.1/src/prefect/workers/base.py#L989-L1015)\n\n```python\n    async def _update_local_work_pool_info(self) -> None:\n        if TYPE_CHECKING:\n            assert self._client is not None\n        try:\n            work_pool = await self._client.read_work_pool(\n                work_pool_name=self._work_pool_name\n            )\n\n        except ObjectNotFound:\n            if self._create_pool_if_not_found:\n                wp = WorkPoolCreate(\n                    name=self._work_pool_name,\n                    type=self.type,\n                )\n                if self._base_job_template is not None:\n                    wp.base_job_template = self._base_job_template\n\n                work_pool = await self._client.create_work_pool(work_pool=wp)\n                self._logger.info(f\"Work pool {self._work_pool_name!r} created.\")\n            else:\n                self._logger.warning(f\"Work pool {self._work_pool_name!r} not found!\")\n                if self._base_job_template is not None:\n                    self._logger.warning(\n                        \"Ignoring supplied base job template because the work pool\"\n                        \" already exists\"\n                    )\n                return\n```\n\nWhat happens is that if two workers start around the same type both of them will fail get fail to find a workpool and the `ObjectNotFound` exception is raised. Then both of the workers will try to create the pool in the server but only one of them will succeed.\n\nThe other will fail:\n\n```python\n  File \"/usr/local/lib/python3.12/site-packages/prefect/workers/base.py\", line 931, in setup\n    await self.sync_with_backend()\n  File \"/usr/local/lib/python3.12/site-packages/prefect/workers/base.py\", line 1108, in sync_with_backend\n    await self._update_local_work_pool_info()\n  File \"/usr/local/lib/python3.12/site-packages/prefect/workers/base.py\", line 1006, in _update_local_work_pool_info\n    work_pool = await self._client.create_work_pool(work_pool=wp)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/prefect/client/orchestration/_work_pools/client.py\", line 503, in create_work_pool\n    raise ObjectAlreadyExists(http_exc=e) from e\nprefect.exceptions.ObjectAlreadyExists\nAn exception occurred.\n```\n\nThis is mostly relevant within the CI environment as as the worker would just be restarted in production and then the workpool would exist.\n\nI can fix this in our code base as well but wanted to bring it here to see if this is a behaviour you'd consider changing within Prefect.\n\n### Version info\n\n```Text\n‚ùØ prefect version\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.12.8\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-redis:     0.2.2\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18081,
    "title": "Add optional OR clause for related_resources in EventFilter",
    "author": "ogenstad",
    "state": "open",
    "created_at": "2025-05-16T07:33:26Z",
    "updated_at": "2025-05-16T07:33:26Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nThe PR #18003 introduced the possibility to have multiple related resources for EventFilters when it comes to searching for events with the API or when setting up Automations.\n\n\n\n### Describe the proposed behavior\n\nIt would be great to have another optional parameter that allowed that query to be switched to an OR version so that a user can choose if an automation should trigger if at least one of the related resources had a match. Alternatively when querying the API you'd be able to indicate that you wanted to either have the default option where AND is used or if you switched to OR only one of the related_resources would have to match the filter.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18066,
    "title": "Power BI Support - Use Case - Refresh Data set",
    "author": "robfreedy",
    "state": "open",
    "created_at": "2025-05-15T13:11:41Z",
    "updated_at": "2025-05-15T15:38:19Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nN/A\n\n### Describe the proposed behavior\n\nWe note that airflow has support for refreshing  a Power BI dataset, we have coded this up by hand for now. However we figured that this is possibly a candidate for inclusion as a default future capability in Prefect.\n\n \n\nWe have permissioned the same Managed Identity the requisite Azure Container Instance uses to the Power BI Workspace. We then use the DefaultAzureCredential(managed_identity_client_id={the Client Id of the managed identity}). It need to get a token given the Power BI scope (https://analysis.usgovcloudapi.net/powerbi/api/.default) ‚Äì note this is for the US GCC endpoint.\n\n \n\nWe‚Äôre using the following API:\n\nhttps://learn.microsoft.com/en-us/rest/api/power-bi/datasets/get-datasets-in-group - to map from name to ID\n\nhttps://learn.microsoft.com/en-us/rest/api/power-bi/datasets/refresh-dataset-in-group - to initiate the refresh\n\nhttps://learn.microsoft.com/en-us/rest/api/power-bi/datasets/get-refresh-history-in-group - to track completeness\n\n \n\nIt‚Äôs similar to the DBT Cloud Job functionality ‚Äì that then waits for the job to complete.\n\n \n\nThis also in hindsight also ties back to the Snowpipe functionality we [raised](https://github.com/PrefectHQ/prefect/issues/17937), and making it a standard block behavior.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n",
    "comments": []
  },
  {
    "issue_number": 17891,
    "title": "Add ability to trigger an event that matches multiple related resources",
    "author": "ogenstad",
    "state": "closed",
    "created_at": "2025-04-23T09:13:41Z",
    "updated_at": "2025-05-15T06:50:20Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nWhen you create an automation you can match an event trigger against a dictionary for the related resources of the event. This limits how you can match against related resources of an event. Using the compound trigger you can do the same kind of match using multiple triggers however the compound triggers requires that there are multiple events before firing the automation.\n\n### Describe the proposed behavior\n\nWhat I'd like to do is to have the possibility to create a trigger that matches against a single event but where it also supports multiple related resources.\n\nLocally we've implemented a workaround where we can use the compound trigger to achieve this but I'd love to see it implemented directly in Prefect.\n\nWhen creating an automation we'd have a payload that looks something like this:\n\n```python\n{\n  \"name\": \"combined-event-trigger\",\n  \"description\": \"Automation that triggers when an event matches multiple related fields in combination\",\n  \"enabled\": true,\n  \"trigger\": {\n    \"type\": \"compound\",\n    \"require\": \"all\",\n    \"within\": 0,\n    \"triggers\": [\n      {\n        \"type\": \"event\",\n        \"id\": \"cb96616c-6936-42c7-b54d-8dc0ad057029\",\n        \"match\": {\n          \"infrahub.node.kind\": \"BuiltinTag\"\n        },\n        \"match_related\": {\n            \"infrahub.field.name\": \"name\",\n            \"infrahub.attribute.value\": [\"name1\", \"name2\", \"name3\"]\n        },\n        \"after\": [],\n        \"expect\": [\n          \"infrahub.node.updated\"\n        ],\n        \"for_each\": [],\n        \"posture\": \"Reactive\",\n        \"threshold\": 1,\n        \"within\": 0\n      },\n      {\n        \"type\": \"event\",\n        \"id\": \"cb96616c-6936-42c7-b54d-8dc0ad057029\",\n        \"match\": {\n          \"infrahub.node.kind\": \"BuiltinTag\"\n        },\n        \"match_related\": {\n            \"infrahub.field.name\": \"description\",\n            \"infrahub.attribute.value\": [\"desc1\", \"desc2\", \"desc3\"]\n        },\n        \"after\": [],\n        \"expect\": [\n          \"infrahub.node.updated\"\n        ],\n        \"for_each\": [],\n        \"posture\": \"Reactive\",\n        \"threshold\": 1,\n        \"within\": 0\n      }\n    ]\n  },\n  \"actions\": [\n    {\n      \"type\": \"run-deployment\",\n      \"source\": \"selected\",\n      \"deployment_id\": \"a6cfdc5e-de44-496d-b451-38bce14449e6\",\n      \"parameters\": {\n        \"event_id\": \"{{ event.id }}\",\n        \"event_type\": \"{{ event.event }}\",\n        \"webhook_id\": \"18370d71-2616-0780-58d0-1746326b7f1d\",\n        \"branch_name\": \"{{ event.resource['infrahub.branch.name'] }}\",\n        \"event_payload\": {\n          \"value\": {\n            \"template\": \"{{ event.payload | tojson }}\",\n            \"__prefect_kind\": \"jinja\"\n          },\n          \"__prefect_kind\": \"json\"\n        },\n        \"event_occured_at\": \"{{ event.occurred }}\"\n      },\n      \"job_variables\": {}\n    }\n  ],\n  \"actions_on_trigger\": [],\n  \"actions_on_resolve\": []\n}\n```\n\nThe idea would be when we have an event that has multiple related resources where matching key might be the same. In this instance we'd want to trigger an automation if we had a related resource with that matched both of these:\n\n* infrahub.field.name == \"name\" AND \"infrahub.attribute.value\" in [\"name1\", \"name2\", \"name3]\n* infrahub.field.name == \"description\" AND \"infrahub.attribute.value\" in [\"desc1\", \"desc1\", \"desc1]\n\nIn our workaround we manage this by overriding the `find_interested_triggers` function within Prefect. However I'd like it to be natively supported within Prefect.\n\nI'm not sure what this would be called perhaps an expanded of the regular EventTrigger, alternatively if EventTrigger could get an additional attribute where `match_related` would be a list of dictionaries instead of a single dictionary.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\nIf this feature is accepted I'd be happy to help with the implementation.",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "Hi @ogenstad, thanks for the well-written issue! I think enabling matching across multiple related resources makes a lot of sense and it seems you're correct that this is not directly possible today. this might be a somewhat involved change that may need to coordinate a bit with Prefect Cloud for compatibilities sake but any implementation spikes you want to do in open source would be appreciated!"
      },
      {
        "user": "chrisguidry",
        "body": "This is very similar to another request we've had to allow for querying events by multiple ANDed related resources.  I think this makes a lot of sense.  We'd have to coordinate the query and trigger schema changes across Prefect Cloud.  One possibility is to allow `match_related` to be `ResourceSpecification | list[ResourceSpecification]`, and it's understood that these are `AND`ed together."
      },
      {
        "user": "ogenstad",
        "body": "Thank you for the consideration! I haven't had time to get back to this yet but I noticed that a PR was opened. I should have time time tomorrow or Monday to at least check that part out. But this is great news! :) "
      }
    ]
  },
  {
    "issue_number": 18060,
    "title": "Add prefect-dbt package to conda forge",
    "author": "rcash",
    "state": "open",
    "created_at": "2025-05-14T22:06:32Z",
    "updated_at": "2025-05-14T23:46:58Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nAs with #18054, add a `prefect-dbt` conda-forge feedstock.\n\n### Version info\n\n```Text\n0.6.7 if possible\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 18054,
    "title": "Add prefect-azure package to conda forge",
    "author": "rcash",
    "state": "open",
    "created_at": "2025-05-14T20:22:02Z",
    "updated_at": "2025-05-14T20:22:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nThere is no way to install some of Prefect's integration libraries today using conda - including `prefect-azure`.\n\n### Version info\n\n```Text\nN/A but 0.4.4 would be appreciated!\n```\n\n### Additional context\n\n![Image](https://github.com/user-attachments/assets/6748a93d-1520-4213-b269-a31c27de8f3f)",
    "comments": []
  },
  {
    "issue_number": 17219,
    "title": "Conda forge missing Prefect 2.x versions",
    "author": "robfreedy",
    "state": "closed",
    "created_at": "2025-02-20T19:22:03Z",
    "updated_at": "2025-05-14T20:06:44Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nThe Prefect conda forge repository is missing all Prefect 2.x versions after 2.20.4 (see the files here: https://anaconda.org/conda-forge/prefect/files) \n\n### Version info\n\n```Text\nN/A but versions >2.20.4\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "robfreedy",
        "body": "Closing as 2.20.16 has been published on Conda Forge and other tickets can be opened for other 2.x versions "
      }
    ]
  },
  {
    "issue_number": 11257,
    "title": "Unable to supply an access token in any other format besides a Secret block the GitRepository object.",
    "author": "taylor-curran",
    "state": "open",
    "created_at": "2023-11-27T23:42:25Z",
    "updated_at": "2025-05-14T19:47:06Z",
    "labels": [
      "enhancement"
    ],
    "body": "### First check\r\n\r\n- [X] I added a descriptive title to this issue.\r\n- [X] I used the GitHub search to find a similar issue and didn't find it.\r\n- [X] I searched the Prefect documentation for this issue.\r\n- [X] I checked that this issue is related to Prefect and not one of its dependencies.\r\n\r\n### Bug summary\r\n\r\nMany users are prevented from using Prefect as a secret storage solution based on their internal secret storage protocols.\r\n\r\nIf the error message is to be believed, when configuring remote github storage for `flow.from_source()`, a user is forced into using a Secret block for this.\r\n\r\n`ValueError: Please save your access token as a Secret block before converting this storage object to a pull step.`\r\n\r\n[`GitCredentials`](https://github.com/PrefectHQ/prefect/blob/c123ab20d1064f96120f2637634b74539e574aee/src/prefect/runner/storage.py#L68) shows that you can pass a `str`, but it looks like it isn't any case to handle strings in [`to_pull_step`](https://github.com/PrefectHQ/prefect/blob/c123ab20d1064f96120f2637634b74539e574aee/src/prefect/runner/storage.py#L256).\r\n\r\n\r\n### Reproduction\r\n\r\n```python3\r\nfrom child_flows import child_flow_d\r\nfrom prefect.runner.storage import GitRepository, GitCredentials\r\nimport os\r\n\r\nGITHUB_ACCESS_TOKEN = os.getenv(\"GITHUB_ACCESS_TOKEN\")\r\n\r\nif __name__ == \"__main__\":\r\n    child_flow_d.from_source(\r\n        source=GitRepository(\r\n            url=\"https://github.com/taylor-curran/private-test.git\",\r\n            credentials=GitCredentials(\r\n                username=\"taylor-curran\", access_token=GITHUB_ACCESS_TOKEN\r\n            ),\r\n        ),\r\n        entrypoint=\"child_flows.py:child_flow_d\",\r\n    ).deploy(\r\n        name=\"my-dep\",\r\n        work_pool_name=\"my-k8s-pool\",\r\n        image=\"docker.io/taycurran/child-d:demo\",\r\n        build=False,\r\n    )\r\n```\r\n\r\n\r\n### Error\r\n\r\n```python3\r\nTraceback (most recent call last):\r\n  File \"/Users/taylorcurran/Documents/november/dot-deploy-examples/raw_string.py\", line 16, in <module>\r\n    ).deploy(\r\n      ^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 255, in coroutine_wrapper\r\n    return call()\r\n           ^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 398, in __call__\r\n    return self.result()\r\n           ^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 284, in result\r\n    return self.future.result(timeout=timeout)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 168, in result\r\n    return self.__get_result()\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\r\n    raise self._exception\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 355, in _run_async\r\n    result = await coro\r\n             ^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/site-packages/prefect/flows.py\", line 969, in deploy\r\n    deployment_ids = await deploy(\r\n                     ^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/site-packages/prefect/deployments/runner.py\", line 830, in deploy\r\n    await deployment.apply(image=image_ref, work_pool_name=work_pool_name)\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/site-packages/prefect/deployments/runner.py\", line 267, in apply\r\n    [self.storage.to_pull_step()] if self.storage else []\r\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/dot-deploy/lib/python3.11/site-packages/prefect/runner/storage.py\", line 277, in to_pull_step\r\n    raise ValueError(\r\nValueError: Please save your access token as a Secret block before converting this storage object to a pull step.\r\n```\r\n\r\n\r\n### Versions\r\n\r\n```Text\r\nVersion:             v2.14.6\r\nAPI version:         0.8.4\r\nPython version:      3.11.5\r\nGit commit:          b99932a4\r\nBuilt:               Wed, Nov 22, 2023 3:19 PM\r\nOS/Arch:             darwin/arm64\r\nProfile:             prefect-patterns\r\nServer type:         cloud\r\n```\r\n\r\n\r\n### Additional context\r\n\r\nFor context, the user would like to do something like the following:\r\n\r\n```python\r\nfrom <file_name> import <flow_name>\r\nfrom prefect.runner.storage import GitRepository\r\nfrom azure.keyvault.secrets import SecretClient\r\nfrom azure.identity import DefaultAzureCredential\r\n\r\nkeyVaultName = '<kv-name>'\r\nKVUri = f\"https://{keyVaultName}.vault.azure.net\"\r\n\r\ncredential = DefaultAzureCredential()\r\nclient = SecretClient(vault_url=KVUri, credential=credential)\r\n\r\nsecretName = '<secret-name>'\r\n\r\nif __name__ == \"__main__\":\r\n    <flow_name>.from_source(\r\n        source = GitRepository(\r\n            url = \"https://github.com/<org-name>/<repo-name>.git\",\r\n            branch = \"<branch-name>\",\r\n            credentials = {\"access_token\": client.get_secret(secretName).value}), \r\n        entrypoint = \"<path>/<file_name>.py:<flow_name>\").deploy(\r\n                name = '<deployment-name>',\r\n                work_pool_name = '<work-pool-name>')\r\n```",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Thanks for the issue @taylor-curran! I've updated this from a bug to an enhancement because it works as designed. We don't allow strings to be used for sensitive fields when using `.deploy` because we generate a `git_clone` pull step and store that serverside. We require a `Secret` block to avoid storing access tokens in plaintext.\r\n\r\nI think we can allow secure usage of external secrets by expanding the set of blocks that can be used to provide sensitive fields to include blocks like our `AwsSecret` block. This will allow users to configure IAM roles or default credentials on their instances that can be used to retrieve secrets from third-party secret stores."
      }
    ]
  },
  {
    "issue_number": 18049,
    "title": "missing events with multi-server setup",
    "author": "zzstoatzz",
    "state": "open",
    "created_at": "2025-05-14T16:14:57Z",
    "updated_at": "2025-05-14T17:47:37Z",
    "labels": [
      "bug"
    ],
    "body": "[using this setup](https://github.com/zzstoatzz/prefect-pack/tree/main/examples/run_a_prefect_server/docker_compose/multi-server)\n\n### Bug summary\n\nnot sure if this is an MRE yet, but it should generally repro\n\n```python\nfrom prefect import flow, serve\nfrom prefect.events import DeploymentCompoundTrigger, DeploymentEventTrigger\n\n\n@flow\ndef upstream_flow():\n    print(\"upstream flow\")\n\n\n@flow\ndef other_upstream_flow():\n    print(\"other upstream flow\")\n\n\n@flow\ndef downstream_flow():\n    print(\"downstream flow\")\n\n\nif __name__ == \"__main__\":\n    upstream_deployment = upstream_flow.to_deployment(name=\"upstream_deployment_a\")\n    other_upstream_deployment = other_upstream_flow.to_deployment(\n        name=\"upstream_deployment_b\"\n    )\n    downstream_deployment = downstream_flow.to_deployment(\n        name=\"downstream_deployment\",\n        triggers=[\n            DeploymentCompoundTrigger(\n                require=\"all\",\n                triggers=[\n                    DeploymentEventTrigger(\n                        expect={\"prefect.flow-run.Completed\"},\n                        match_related={\n                            \"prefect.resource.name\": \"upstream_deployment_a\",\n                        },\n                    ),\n                    DeploymentEventTrigger(\n                        expect={\"prefect.flow-run.Completed\"},\n                        match_related={\n                            \"prefect.resource.name\": \"upstream_deployment_b\",\n                        },\n                    ),\n                ],\n            )\n        ],\n    )\n\n    serve(upstream_deployment, other_upstream_deployment, downstream_deployment)\n```\n\nsometimes completed events are missing, sometimes other flow run events are missing\n\n\nNOTE i am not missing them with cloud, all events come through and the automation action executes\n\n### Version info\n\n```Text\nVersion:             3.4.2.dev3+1.g27cccbf654\nAPI version:         0.8.4\nPython version:      3.12.8\nGit commit:          27cccbf6\nBuilt:               Wed, May 14, 2025 04:10 PM\nOS/Arch:             darwin/arm64\nProfile:             bleeding\nServer type:         server\nPydantic version:    2.11.3\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 10452,
    "title": "Prefect UI only shows blank white screen",
    "author": "hotplot",
    "state": "closed",
    "created_at": "2023-08-20T21:49:27Z",
    "updated_at": "2025-05-14T14:16:34Z",
    "labels": [
      "bug",
      "ui",
      "needs:mre"
    ],
    "body": "### First check\r\n\r\n- [X] I added a descriptive title to this issue.\r\n- [X] I used the GitHub search to find a similar issue and didn't find it.\r\n- [X] I refreshed the page and this issue still occurred.\r\n- [X] I checked if this issue was specific to the browser I was using by testing with a different browser.\r\n\r\n### Bug summary\r\n\r\nWhen trying to load the Prefect UI, I simply get a blank screen. This happens in Chrome, Edge, and Firefox. \r\n\r\nThe issue is almost certainly related to accessing the Prefect instance over a VPN, however I believe the root cause and fix is simply to return a Content-Length header when serving the dashboard assets.\r\n\r\n### Reproduction\r\n\r\nThe Prefect instance is self hosted, using a completely standard container running `prefecthq/prefect:2-python3.11`. The container is started using the following compose file:\r\n\r\n```\r\nversion: \"3.9\"\r\nservices:\r\n  prefect:\r\n    image: prefecthq/prefect:2-python3.11\r\n    restart: always\r\n    volumes:\r\n      - prefect:/root/.prefect\r\n    entrypoint: [ \"prefect\", \"server\", \"start\" ]\r\n    environment:\r\n      PREFECT_SERVER_API_HOST: 0.0.0.0\r\n    ports:\r\n      - 9200:4200\r\n\r\nvolumes:\r\n  prefect:\r\n```\r\n\r\nTo reproduce the issue, I only have to try loading `hostname:9200` in a browser over our corporate Citrix VPN. It's likely that behaviour varies between different VPNs.\r\n\r\n### Error\r\n\r\nPrefect returns a valid HTML response for the index page, but when Chrome tries to load `assets/index-9892f350.js` it generates a `net::ERR_INCOMPLETE_CHUNKED_ENCODING 200 (OK)` error:\r\n\r\n![image](https://github.com/PrefectHQ/prefect/assets/6686759/fff7099d-2f5a-4f2e-ba44-ac37c6614655)\r\n\r\n### Browers\r\n\r\n- [X] Chrome\r\n- [X] Firefox\r\n- [ ] Safari\r\n- [X] Edge\r\n\r\n### Prefect version\r\n\r\n```Text\r\nVersion:             2.11.4\r\nAPI version:         0.8.4\r\nPython version:      3.11.4\r\nGit commit:          e15183f3\r\nBuilt:               Thu, Aug 17, 2023 3:09 PM\r\nOS/Arch:             linux/x86_64\r\nProfile:             default\r\nServer type:         server\r\n```\r\n\r\n\r\n### Additional context\r\n\r\nThis only happens when accessing Prefect over Citrix, but I suspect it could be resolved by returning a `Content-Length` header with the responses.",
    "comments": [
      {
        "user": "hotplot",
        "body": "It's worth noting that the index page and favicon are both served (successfully) with a `Content-Length` header, whereas the js and css file are both served chunked and without a content length. "
      },
      {
        "user": "znicholasbrown",
        "body": "Hi @hotplot - thanks for the detailed report! I'm not able to reproduce this using a VPN alone so as you mentioned it's likely something unique to your setup. From my understanding that chunk error is nonspecific but given the commonality between browsers is likely due to the webserver dropping parts of the response or not being able to field the request; I'd be surprised if it had to do with a missing header but don't have enough info to confirm that suspicion.\r\n\r\nThat said, feel free to PR the missing headers or to provide a reproduction (if you have one) as that'll give us both more to work off. "
      },
      {
        "user": "avnav0",
        "body": "i'm having the same issue.  i even tried this:\r\n\r\n```\r\n\r\nlocation /prefect/ {\r\n    rewrite ^/prefect/(.*) /$1 break;\r\n    include mime.types;\r\n    default_type application/octet-stream;\r\n\r\n    proxy_pass http://10.235.77.21:4200/prefect/;\r\n    proxy_set_header Host $http_host;\r\n    proxy_set_header X-Real-IP $remote_addr;\r\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r\n    proxy_set_header X-Forwarded-Proto $scheme;\r\n    proxy_set_header X-NginX-Proxy true;\r\n    proxy_set_header X-Custom-Header \"prefect\";\r\n\r\n    real_ip_header X-Real-IP;\r\n\r\n    proxy_connect_timeout 300;\r\n\r\n    proxy_http_version 1.1;\r\n    proxy_set_header Upgrade $http_upgrade;\r\n    proxy_set_header Connection \"upgrade\";\r\n\r\n    chunked_transfer_encoding off;\r\n}\r\n\r\nlocation ~ ^/(.*) {\r\n    if ($is_prefect_referer) {\r\n        set $asset_url \"/prefect/$1\";\r\n    }\r\n    proxy_pass http://10.235.77.21:4200$asset_url;\r\n    proxy_set_header Host $http_host;\r\n    proxy_set_header X-Real-IP $remote_addr;\r\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r\n    proxy_set_header X-Forwarded-Proto $scheme;\r\n    proxy_set_header X-NginX-Proxy true;\r\n    proxy_set_header X-Custom-Header \"prefect\";\r\n\r\n    real_ip_header X-Real-IP;\r\n\r\n    proxy_connect_timeout 300;\r\n\r\n    proxy_http_version 1.1;\r\n    proxy_set_header Upgrade $http_upgrade;\r\n    proxy_set_header Connection \"upgrade\";\r\n\r\n    chunked_transfer_encoding off;\r\n}\r\n\r\n```\r\n\r\nwhich gets passed the .css errors and such but i still get a blank page\r\n\r\n![image](https://github.com/PrefectHQ/prefect/assets/108008768/73d67d49-07a7-4dd6-8d37-8350ef809d50)\r\n\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 12028,
    "title": "Errors raised in mapped tasks result in UnfinishedRun when futures are passed to another task, source error not included in stack trace",
    "author": "austinweisgrau",
    "state": "closed",
    "created_at": "2024-02-20T18:02:49Z",
    "updated_at": "2025-05-14T12:47:39Z",
    "labels": [
      "bug"
    ],
    "body": "### First check\n\n- [X] I added a descriptive title to this issue.\n- [X] I used the GitHub search to find a similar issue and didn't find it.\n- [X] I searched the Prefect documentation for this issue.\n- [X] I checked that this issue is related to Prefect and not one of its dependencies.\n\n### Bug summary\n\n\"Normally,\" when an unhandled exception is raised in a task, that error will be raised all the way to the \"top\" of the stack and the Prefect flow will fail on that exception, with a clear stack trace showing the context that raised the error. This is true if a task is called directly or is mapped.\r\n\r\nHowever, if the results from a mapped task \"A\" are passed to another task \"B\" as an argument, and an exception is raised within an instance of task \"A\", then the Prefect flow will fail when calling task \"B\" with an UnfinishedRun error, and the stack trace will not show the exception in task \"A\". \n\n### Reproduction\n\n```python3\nfrom prefect import flow, task\r\n\r\n\r\n@task\r\ndef fetch_values() -> list[int]:\r\n    result = [i for i in range(10)]\r\n    return result\r\n\r\n\r\n@task\r\ndef transform_value(value: int) -> int:\r\n    if value == 6:\r\n        raise ValueError\r\n    result = value * 2\r\n    return result\r\n\r\n\r\n@task\r\ndef log_results(values: list[int]) -> None:\r\n    for i in values:\r\n        print(i)\r\n\r\n\r\n@flow\r\ndef myflow():\r\n    values = fetch_values()\r\n    transformed = transform_value.map(values)\r\n\r\n    # If this method is not called, the ValueError will be raised clearly and coherently\r\n    # But if this method is called, the flow will fail here with an UnfinishedRun exception\r\n    log_results(transformed)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    myflow()\n```\n\n\n### Error\n\n```python3\n10:01:41.075 | INFO    | prefect.engine - Created flow run 'dashing-snail' for flow 'myflow'\r\n10:01:41.082 | INFO    | Flow run 'dashing-snail' - View at ...\r\n10:01:42.714 | INFO    | Flow run 'dashing-snail' - Created task run 'fetch_values-0' for task 'fetch_values'\r\n10:01:42.716 | INFO    | Flow run 'dashing-snail' - Executing 'fetch_values-0' immediately...\r\n10:01:44.052 | INFO    | Task run 'fetch_values-0' - Finished in state Completed()\r\n10:01:44.365 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-7' for task 'transform_value'\r\n10:01:44.367 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-7' for execution.\r\n10:01:44.384 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-0' for task 'transform_value'\r\n10:01:44.387 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-0' for execution.\r\n10:01:44.397 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-2' for task 'transform_value'\r\n10:01:44.399 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-2' for execution.\r\n10:01:44.419 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-5' for task 'transform_value'\r\n10:01:44.421 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-5' for execution.\r\n10:01:44.433 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-6' for task 'transform_value'\r\n10:01:44.436 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-6' for execution.\r\n10:01:44.447 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-9' for task 'transform_value'\r\n10:01:44.450 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-9' for execution.\r\n10:01:44.457 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-3' for task 'transform_value'\r\n10:01:44.459 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-3' for execution.\r\n10:01:44.468 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-4' for task 'transform_value'\r\n10:01:44.470 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-4' for execution.\r\n10:01:44.484 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-1' for task 'transform_value'\r\n10:01:44.487 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-1' for execution.\r\n10:01:44.496 | INFO    | Flow run 'dashing-snail' - Created task run 'transform_value-8' for task 'transform_value'\r\n10:01:44.498 | INFO    | Flow run 'dashing-snail' - Submitted task run 'transform_value-8' for execution.\r\n10:01:44.699 | INFO    | Flow run 'dashing-snail' - Created task run 'log_results-0' for task 'log_results'\r\n10:01:44.701 | INFO    | Flow run 'dashing-snail' - Executing 'log_results-0' immediately...\r\n10:01:45.043 | ERROR   | Task run 'transform_value-6' - Encountered exception during execution:\r\nTraceback (most recent call last):\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/engine.py\", line 1623, in orchestrate_task_run\r\n    result = await call.aresult()\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 181, in aresult\r\n    return await asyncio.wrap_future(self.future)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 194, in _run_sync\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/home/aradox/code/wfp/wfp-prefect/example.py\", line 12, in transform_value\r\n    raise ValueError\r\nValueError\r\n10:01:45.230 | INFO    | Task run 'transform_value-2' - Finished in state Completed()\r\n10:01:45.246 | INFO    | Task run 'transform_value-4' - Finished in state Completed()\r\n10:01:45.305 | ERROR   | Task run 'transform_value-6' - Finished in state Failed('Task run encountered an exception: ValueError\\n')\r\n10:01:45.312 | INFO    | Task run 'transform_value-3' - Finished in state Completed()\r\n10:01:45.319 | INFO    | Task run 'transform_value-7' - Finished in state Completed()\r\n10:01:45.349 | INFO    | Task run 'transform_value-0' - Finished in state Completed()\r\n10:01:45.356 | INFO    | Task run 'transform_value-9' - Finished in state Completed()\r\n10:01:45.368 | INFO    | Task run 'transform_value-8' - Finished in state Completed()\r\n10:01:45.374 | INFO    | Task run 'transform_value-5' - Finished in state Completed()\r\n10:01:45.689 | INFO    | Task run 'transform_value-1' - Finished in state Completed()\r\n10:01:45.994 | ERROR   | Flow run 'dashing-snail' - Encountered exception during execution:\r\nTraceback (most recent call last):\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/engine.py\", line 742, in orchestrate_flow_run\r\n    result = await flow_call.aresult()\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 181, in aresult\r\n    return await asyncio.wrap_future(self.future)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 194, in _run_sync\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/home/aradox/code/wfp/wfp-prefect/example.py\", line 25, in myflow\r\n    log_results(transformed)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/tasks.py\", line 505, in __call__\r\n    return enter_task_run_engine(\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/engine.py\", line 1045, in enter_task_run_engine\r\n    return from_sync.wait_for_call_in_loop_thread(begin_run)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/api.py\", line 232, in wait_for_call_in_loop_thread\r\n    return call.result()\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 173, in result\r\n    return self.future.result(timeout=timeout)\r\n  File \"/home/aradox/.pyenv/versions/3.10.9/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\r\n    return self.__get_result()\r\n  File \"/home/aradox/.pyenv/versions/3.10.9/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 218, in _run_async\r\n    result = await coro\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/engine.py\", line 1210, in get_task_call_return_value\r\n    return await future._result()\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/futures.py\", line 241, in _result\r\n    return await final_state.result(raise_on_failure=raise_on_failure, fetch=True)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/states.py\", line 85, in _get_state_result\r\n    raise UnfinishedRun(\r\nprefect.exceptions.UnfinishedRun: Run is in PENDING state, its result is not available.\r\n10:01:46.297 | ERROR   | Flow run 'dashing-snail' - Finished in state Failed('Flow run encountered an exception. UnfinishedRun: Run is in PENDING state, its result is not available.\\n')\r\nTraceback (most recent call last):\r\n  File \"/home/aradox/code/wfp/wfp-prefect/example.py\", line 28, in <module>\r\n    myflow()\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/flows.py\", line 518, in __call__\r\n    return enter_flow_run_engine_from_flow_call(\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/engine.py\", line 198, in enter_flow_run_engine_from_flow_call\r\n    retval = from_sync.wait_for_call_in_loop_thread(\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/api.py\", line 232, in wait_for_call_in_loop_thread\r\n    return call.result()\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 173, in result\r\n    return self.future.result(timeout=timeout)\r\n  File \"/home/aradox/.pyenv/versions/3.10.9/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\r\n    return self.__get_result()\r\n  File \"/home/aradox/.pyenv/versions/3.10.9/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 218, in _run_async\r\n    result = await coro\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/client/utilities.py\", line 40, in with_injected_client\r\n    return await fn(*args, **kwargs)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/engine.py\", line 292, in create_then_begin_flow_run\r\n    return await state.result(fetch=True)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/states.py\", line 92, in _get_state_result\r\n    raise await get_state_exception(state)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/engine.py\", line 742, in orchestrate_flow_run\r\n    result = await flow_call.aresult()\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 181, in aresult\r\n    return await asyncio.wrap_future(self.future)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 194, in _run_sync\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/home/aradox/code/wfp/wfp-prefect/example.py\", line 25, in myflow\r\n    log_results(transformed)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/tasks.py\", line 505, in __call__\r\n    return enter_task_run_engine(\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/engine.py\", line 1045, in enter_task_run_engine\r\n    return from_sync.wait_for_call_in_loop_thread(begin_run)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/api.py\", line 232, in wait_for_call_in_loop_thread\r\n    return call.result()\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 173, in result\r\n    return self.future.result(timeout=timeout)\r\n  File \"/home/aradox/.pyenv/versions/3.10.9/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\r\n    return self.__get_result()\r\n  File \"/home/aradox/.pyenv/versions/3.10.9/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 218, in _run_async\r\n    result = await coro\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/engine.py\", line 1210, in get_task_call_return_value\r\n    return await future._result()\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/futures.py\", line 241, in _result\r\n    return await final_state.result(raise_on_failure=raise_on_failure, fetch=True)\r\n  File \"/home/aradox/.pyenv/versions/prefect/lib/python3.10/site-packages/prefect/states.py\", line 85, in _get_state_result\r\n    raise UnfinishedRun(\r\nprefect.exceptions.UnfinishedRun: Run is in PENDING state, its result is not available.\n```\n\n\n### Versions\n\n```Text\nVersion:             2.10.11\r\nAPI version:         0.8.4\r\nPython version:      3.10.9\r\nGit commit:          8c651ffc\r\nBuilt:               Thu, May 25, 2023 2:59 PM\r\nOS/Arch:             linux/x86_64\r\nProfile:             default\r\nServer type:         cloud\n```\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "austinweisgrau",
        "body": "Might be related to #8124?"
      },
      {
        "user": "serinamarie",
        "body": "HI @austinweisgrau, thanks for the issue! While I've not reproduced it, the true error from task A certainly appears to be something we'd ideally raise here. We've added it to our backlog :) "
      },
      {
        "user": "mattiamatrix",
        "body": "Hello, do you have any updates on this? I am facing the same issue.\r\n\r\nI have a `.map` applied to a task and I would like to `allow_failure` to some of them. \r\n\r\nI also tried this but it does not seem to work\r\n\r\n```py\r\ntasks = my_function.map(\r\n    ...,\r\n    return_state=True,\r\n)\r\n\r\nfor task in tasks:\r\n    result = task.result(raise_on_failure=False)\r\n    logger.info(result)\r\n```"
      }
    ]
  },
  {
    "issue_number": 17895,
    "title": "Super slow api and UI after upgrade",
    "author": "Ishankoradia",
    "state": "open",
    "created_at": "2025-04-24T06:04:05Z",
    "updated_at": "2025-05-14T04:17:06Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWe recently did an upgrade from `2.18.3` to `3.1.15`. The apis we have integrated became very slow & sluggish. The api to just fetch a flow run i.e. `GET /flow_runs/{id}` is taking ~ 3 secs. It was much faster before the upgrade. \n\nThis is what our configuration setting looks like at the moment in prefect `3.1.15`. \n```\nhome: /home/ddp/.prefect\nprofiles_path: /home/ddp/.prefect/profiles.toml\ndebug_mode: false\napi: {\n  \"url\": \"http://localhost:4200/api\",\n  \"auth_string\": null,\n  \"key\": null,\n  \"tls_insecure_skip_verify\": false,\n  \"ssl_cert_file\": null,\n  \"enable_http2\": false,\n  \"request_timeout\": 60\n}\ncli: {\n  \"colors\": true,\n  \"prompt\": null,\n  \"wrap_lines\": true\n}\nclient: {\n  \"max_retries\": 5,\n  \"retry_jitter_factor\": 0.2,\n  \"retry_extra_codes\": [],\n  \"csrf_support_enabled\": true,\n  \"metrics\": {\n    \"enabled\": false,\n    \"port\": 4201\n  }\n}\ncloud: {\n  \"api_url\": \"https://api.prefect.cloud/api\",\n  \"enable_orchestration_telemetry\": true,\n  \"ui_url\": \"https://app.prefect.cloud\"\n}\ndeployments: {\n  \"default_work_pool_name\": null,\n  \"default_docker_build_namespace\": null\n}\nexperiments: {\n  \"warn\": true,\n  \"lineage_events_enabled\": false\n}\nflows: {\n  \"default_retries\": 0,\n  \"default_retry_delay_seconds\": 0\n}\ninternal: {\n  \"logging_level\": \"ERROR\"\n}\nlogging: {\n  \"level\": \"INFO\",\n  \"config_path\": \"/home/ddp/.prefect/logging.yml\",\n  \"extra_loggers\": [],\n  \"log_prints\": false,\n  \"colors\": true,\n  \"markup\": false,\n  \"to_api\": {\n    \"enabled\": true,\n    \"batch_interval\": 2,\n    \"batch_size\": 4000000,\n    \"max_log_size\": 1000000,\n    \"when_missing_flow\": \"warn\"\n  }\n}\nresults: {\n  \"default_serializer\": \"pickle\",\n  \"persist_by_default\": false,\n  \"default_storage_block\": null,\n  \"local_storage_path\": \"/home/ddp/.prefect/storage\"\n}\nrunner: {\n  \"process_limit\": 5,\n  \"poll_frequency\": 10,\n  \"heartbeat_frequency\": null,\n  \"server\": {\n    \"enable\": false,\n    \"host\": \"localhost\",\n    \"port\": 8080,\n    \"log_level\": \"ERROR\",\n    \"missed_polls_tolerance\": 2\n  }\n}\nserver: {\n  \"logging_level\": \"WARNING\",\n  \"analytics_enabled\": true,\n  \"metrics_enabled\": false,\n  \"log_retryable_errors\": false,\n  \"register_blocks_on_start\": true,\n  \"memoize_block_auto_registration\": true,\n  \"memo_store_path\": \"/home/ddp/.prefect/memo_store.toml\",\n  \"deployment_schedule_max_scheduled_runs\": 50,\n  \"api\": {\n    \"auth_string\": null,\n    \"host\": \"127.0.0.1\",\n    \"port\": 4200,\n    \"default_limit\": 500,\n    \"keepalive_timeout\": 5,\n    \"csrf_protection_enabled\": false,\n    \"csrf_token_expiration\": \"PT1H\",\n    \"cors_allowed_origins\": \"*\",\n    \"cors_allowed_methods\": \"*\",\n    \"cors_allowed_headers\": \"*\"\n  },\n  \"database\": {\n    \"sqlalchemy\": {\n      \"connect_args\": {\n        \"application_name\": null\n      },\n      \"pool_size\": 60,\n      \"pool_recycle\": 3600,\n      \"pool_timeout\": 30,\n      \"max_overflow\": 10\n    },\n    \"connection_url\": \"**********\",\n    \"driver\": null,\n    \"host\": null,\n    \"port\": null,\n    \"user\": null,\n    \"name\": null,\n    \"password\": \"**********\",\n    \"echo\": false,\n    \"migrate_on_start\": true,\n    \"timeout\": 60,\n    \"connection_timeout\": 60\n  },\n  \"deployments\": {\n    \"concurrency_slot_wait_seconds\": 30\n  },\n  \"ephemeral\": {\n    \"enabled\": false,\n    \"startup_timeout_seconds\": 20\n  },\n  \"events\": {\n    \"stream_out_enabled\": true,\n    \"related_resource_cache_ttl\": \"PT5M\",\n    \"maximum_labels_per_resource\": 500,\n    \"maximum_related_resources\": 500,\n    \"maximum_size_bytes\": 1500000,\n    \"expired_bucket_buffer\": \"PT1M\",\n    \"proactive_granularity\": \"PT5S\",\n    \"retention_period\": \"P7D\",\n    \"maximum_websocket_backfill\": \"PT15M\",\n    \"websocket_backfill_page_size\": 250,\n    \"messaging_broker\": \"prefect.server.utilities.messaging.memory\",\n    \"messaging_cache\": \"prefect.server.utilities.messaging.memory\",\n    \"maximum_event_name_length\": 1024\n  },\n  \"flow_run_graph\": {\n    \"max_nodes\": 10000,\n    \"max_artifacts\": 10000\n  },\n  \"services\": {\n    \"cancellation_cleanup\": {\n      \"enabled\": true,\n      \"loop_seconds\": 120\n    },\n    \"event_persister\": {\n      \"enabled\": false,\n      \"batch_size\": 20,\n      \"flush_interval\": 5\n    },\n    \"flow_run_notifications\": {\n      \"enabled\": true\n    },\n    \"foreman\": {\n      \"enabled\": true,\n      \"loop_seconds\": 120,\n      \"inactivity_heartbeat_multiple\": 3,\n      \"fallback_heartbeat_interval_seconds\": 30,\n      \"deployment_last_polled_timeout_seconds\": 300,\n      \"work_queue_last_polled_timeout_seconds\": 300\n    },\n    \"late_runs\": {\n      \"enabled\": true,\n      \"loop_seconds\": 60,\n      \"after_seconds\": \"PT1M\"\n    },\n    \"scheduler\": {\n      \"enabled\": true,\n      \"loop_seconds\": 60,\n      \"deployment_batch_size\": 100,\n      \"max_runs\": 100,\n      \"min_runs\": 3,\n      \"max_scheduled_time\": \"P100D\",\n      \"min_scheduled_time\": \"PT1H\",\n      \"insert_batch_size\": 500\n    },\n    \"pause_expirations\": {\n      \"enabled\": true,\n      \"loop_seconds\": 5\n    },\n    \"task_run_recorder\": {\n      \"enabled\": true\n    },\n    \"triggers\": {\n      \"enabled\": true\n    }\n  },\n  \"tasks\": {\n    \"tag_concurrency_slot_wait_seconds\": 30,\n    \"max_cache_key_length\": 2000,\n    \"scheduling\": {\n      \"max_scheduled_queue_size\": 1000,\n      \"max_retry_queue_size\": 100,\n      \"pending_task_timeout\": \"PT0S\"\n    }\n  },\n  \"ui\": {\n    \"enabled\": true,\n    \"api_url\": \"http://localhost:4200/api\",\n    \"serve_base\": \"/\",\n    \"static_directory\": null\n  }\n}\ntasks: {\n  \"refresh_cache\": false,\n  \"default_retries\": 0,\n  \"default_retry_delay_seconds\": 0,\n  \"default_persist_result\": null,\n  \"runner\": {\n    \"thread_pool_max_workers\": null\n  },\n  \"scheduling\": {\n    \"default_storage_block\": null,\n    \"delete_failed_submissions\": true\n  }\n}\ntesting: {\n  \"test_mode\": false,\n  \"unit_test_mode\": false,\n  \"unit_test_loop_debug\": true,\n  \"test_setting\": \"FOO\"\n}\nworker: {\n  \"heartbeat_seconds\": 30,\n  \"query_seconds\": 10,\n  \"prefetch_seconds\": 10,\n  \"webserver\": {\n    \"host\": \"0.0.0.0\",\n    \"port\": 8080\n  }\n}\nui_url: http://localhost:4200\nsilence_api_url_misconfiguration: false\n```\n\n\n### Version info\n\n```Text\nVersion:             3.1.15\nAPI version:         0.8.4\nPython version:      3.10.9\nGit commit:          3ac3d548\nBuilt:               Thu, Jan 30, 2025 11:31 AM\nOS/Arch:             linux/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.10.6\nIntegrations:\n  prefect-sqlalchemy: 0.5.2\n  prefect-github:    0.3.1\n  prefect-dbt:       0.6.6\n  prefect-gcp:       0.6.4\n  prefect-shell:     0.3.1\n```\n\n### Additional context\n\nSome interesting RDS graphs at the time of upgrade . I can confirm that the slopes in the both the graphs are at the time of upgrade. \n\n<img width=\"327\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2959dfac-8938-4ffd-abe1-dfe781459070\" />\n\n<img width=\"360\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/52ea7a7c-f509-44c2-95c5-a758e88b0cce\" />\n\nWe also saw that prefect has been taking good amout of locks on the db which is probably making the reads slower. CPU of RDS is well under the limit. \nIs there some configuration that will allow me to use more connections & hopefully solve this bottleneck ? I have pasted our settings above. \n\nI am also seeing these in prefect server logs\n\n```\nprefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 59.25218 seconds to run, which is longer than its loop interval of 5 seconds.\n.\n.\n.\nprefect.server.services.recentdeploymentsscheduler - RecentDeploymentsScheduler took 163.870647 seconds to run, which is longer than its loop interval of 5 seconds.\n.\n.\n.\nprefect.server.services.flowrunnotifications - FlowRunNotifications took 8.167483 seconds to run, which is longer than its loop interval of 4 seconds.\n.\n.\n.\nprefect.server.services.failexpiredpauses - FailExpiredPauses took 6.492807 seconds to run, which is longer than its loop interval of 5.0 seconds.\n```\n\nThis has been very frustrating. Any help or guidance is much appreciated ?",
    "comments": [
      {
        "user": "SuperLamic",
        "body": "Probably the same issue as in https://github.com/PrefectHQ/prefect/issues/17767 . I also experience it."
      },
      {
        "user": "Ishankoradia",
        "body": "> Probably the same issue as in https://github.com/PrefectHQ/prefect/issues/17767 . I also experience it.\n\n@SuperLamic looked at that one but we don't see any (postgres) deadlock errors in server logs. Do you see deadlock errors ? "
      },
      {
        "user": "SuperLamic",
        "body": "@Ishankoradia I do. Sometimes it's just slow (I presume because of the number of locks), but sometimes Prefect times out on the deadlock (giving me 500). Interesting thing is that we have multiple deployments of the same version of Prefect and one of them runs fine, while the other struggles to keep up with the locks."
      }
    ]
  },
  {
    "issue_number": 17887,
    "title": "Prefect client creates incorrect flow runs parameters due to unexpected serialization behavior",
    "author": "levzem",
    "state": "closed",
    "created_at": "2025-04-22T22:33:38Z",
    "updated_at": "2025-05-13T23:29:47Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nhttps://github.com/PrefectHQ/prefect/blob/main/src/prefect/client/orchestration/_deployments/client.py#L614\n\nThe Prefect client uses the `exclude_unset` setting for `model_dump()`, which has the unfortunate side effect of dropping default fields that were mutated afterwards.\n\nA simple example is\n\n```\nclass MyFlowParam(BaseModel):\n\n    dropped: list[str] = Field(default_factory=list)\n\nparam = MyFlowParam()\nparam.dropped.append(\"mutated\")\n\nmy_flow(param)\n```\n\nthis will persist the flow run parameters as `{\"param\": {}}` instead of `{\"param\": {\"dropped\": [\"mutated\"]}}`\n\nI would recommend considering setting `exclude_default` instead of `exclude_unset` as this is a really nasty foot gun and took a lot of time for me to get to the bottom of.\n\nSee for more explanations below:\nhttps://github.com/pydantic/pydantic/discussions/5749\nhttps://github.com/pydantic/pydantic/issues/9866\n\n### Version info\n\n```Text\nprefect: 3.2.6\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "@levzem thanks for the issue and the linked context\n\nyour example doesn't quite reproduce, but this does\n```python\nfrom threading import Thread\n\nfrom pydantic import BaseModel, Field\n\nfrom prefect import flow\nfrom prefect.deployments import run_deployment\n\n\nclass MyFlowParam(BaseModel):\n    dropped: list[str] = Field(default_factory=list)\n\n\n@flow\ndef my_flow(param: MyFlowParam):\n    print(param)\n\n\nif __name__ == \"__main__\":\n    param = MyFlowParam()\n    param.dropped.append(\"foo\")\n\n    Thread(target=my_flow.serve, daemon=True).start()\n\n    run_deployment(\n        \"my-flow/my-flow\",\n        parameters={\"param\": param}, # works fine if you replace `param` with `param.model_dump()`\n    )\n```\n\nlooking more into this now! please feel free to add more detail on how you're actually hitting this in your usage of prefect"
      },
      {
        "user": "levzem",
        "body": "thanks for fixing this so promptly @zzstoatzz !"
      }
    ]
  },
  {
    "issue_number": 17977,
    "title": "Prefect UI overwrites the url of my link Artifact",
    "author": "liviumanea",
    "state": "closed",
    "created_at": "2025-05-04T19:49:00Z",
    "updated_at": "2025-05-13T22:38:43Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nIn my prefect deployment I have to run it behind nginx in order to be able to serve the link artifacts and I have to configure prefect as follows so that it works behind nginx:\n\n```Dockerfile\nENV PREFECT_SERVER_API_BASE_PATH=/prefect/api \\\n    PREFECT_API_URL=http://localhost:4200/prefect/api \\\n    PREFECT_UI_API_URL=/prefect/api \\\n    PREFECT_UI_SERVE_BASE=/prefect\n```\n\nIn my flows, I am creating artifacts with the following code:\n\n```python\n    await create_link_artifact(\n        key=\"pga-report\",\n        link=\"http://localhost:8200/prefect/products/2025/05/04/pga-report.pdf\",\n        link_text=\"PGA Report\",\n        description=\"\"\"\n        # PGA Report\n\n        this is the pga report\n        \"\"\",\n    )\n```\n\nWhen I go look at this artifact, I see the following data from the backend\n\n```json\n{\"id\":\"4a61752c-1735-41c6-a7d7-ea4a16604f24\",\"created\":\"2025-05-04T19:03:20.506642Z\",\"updated\":\"2025-05-04T19:03:20.506642Z\",\"key\":\"pga-report\",\"type\":\"markdown\",\"description\":\"\\n        # PGA Report\\n\\n        this is the pga report\\n        \",\"data\":\"[PGA Report](http://localhost:8200/prefect/products/2025/05/04/pga-report.pdf)\",\"metadata_\":null,\"flow_run_id\":\"c6750171-cf29-4089-8a0c-0c79122e8ad4\",\"task_run_id\":null}\n```\n\nIf I visit `localhost:8200/prefect/products/2025/05/04/pga-report.pdf` in my browser then I can access the pdf file just fine. If I hover over the link on the artifact page the link looks like this:\n`http://localhost:8200/prefect/prefect/products/2025/05/04/pga-report.pdf`. So it somehow adds another \"prefect\" in there and I don't know how to stop that. If I click that link, the UI tries to navigate as a single page app to that instead of just rendering the file in a new tab as I would like.\n\nIf I add a link that's completely unrelated to my prefect paths, for example\n\n```python\nlink=\"https://www.prefect.io/\"\n```\n\nThen it's not messing with it.\n\nAlso, here is how I'm forwarding traffic with nginx onto Prefect\n\n```text\n  server {\n    listen 80;\n\n    # Serve static PDFs\n    location /prefect/products/ {\n      alias /usr/share/nginx/html/products/;\n      autoindex on;\n    }\n\n    # Proxy to Prefect (UI and API)\n    location /prefect {\n      proxy_pass http://prefect:4200/prefect;\n      proxy_http_version 1.1;\n\n      proxy_set_header Host $host;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n\n    }\n```\n\nWhat I really wanted to do is set the link to something unrelated to prefect like\n\n```text\nhttp://localhost:8200/products/2025/05/04/pga-report.pdf\n```\n\n### Version info\n\n```Text\nVersion:             3.3.7\nAPI version:         0.8.4\nPython version:      3.13.3\nGit commit:          8f86aaee\nBuilt:               Mon, Apr 28, 2025 03:04 PM\nOS/Arch:             linux/aarch64\nProfile:             ephemeral\nServer type:         server\nPydantic version:    2.11.4\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "liviumanea",
        "body": "It seems that adding `PREFECT_UI_SERVE_BASE` causes this issue but I do need that in order to expose prefect's ui on another prefix. Am I doing it wrong ? Am I misusing this setting or where should I store the static files so that the link Artifact will serve it properly ?"
      },
      {
        "user": "znicholasbrown",
        "body": "Hi @liviumanea - thanks for the report; I'm looking into this. For context I think [this component](https://github.com/PrefectHQ/prefect-design/blob/main/src/components/MarkdownRenderer/parser.ts#L148-L153) is responsible for this behavior but I'm not totally sure what the fix is yet"
      },
      {
        "user": "liviumanea",
        "body": "Hi, @znicholasbrown . Thanks for taking the time. Seeing as this is some user provided content inside of that component, and seeing that we can't store generated assets on prefect itself (please correct me if I'm wrong), shouldn't we not normalize the hrefs that are rendered there and just rely on the user to provide proper hrefs (be it absolute or relative) at all times ?"
      }
    ]
  },
  {
    "issue_number": 18035,
    "title": "ModuleNotFoundError: No module named 'referencing'",
    "author": "Daveography",
    "state": "closed",
    "created_at": "2025-05-13T17:52:08Z",
    "updated_at": "2025-05-13T20:55:31Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nIn the process of upgrading a project from Prefect V2 to V3. On running `prefect server database upgrade` I get the following:\n\n```\nTraceback (most recent call last):\n  File \"<redacted>\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"<redacted>\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"<redacted>\\.venv\\Scripts\\prefect.exe\\__main__.py\", line 4, in <module>\n    from prefect.cli import app\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\cli\\__init__.py\", line 24, in <module>\n    import prefect.cli.server\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\cli\\server.py\", line 32, in <module>\n    from prefect.server.services.base import Service\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\server\\__init__.py\", line 1, in <module>\n    from . import models, orchestration, schemas, services\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\server\\models\\__init__.py\", line 1, in <module>\n    from . import (\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\server\\models\\artifacts.py\", line 9, in <module>\n    from prefect.server.database import PrefectDBInterface, db_injector, orm_models\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\server\\database\\__init__.py\", line 6, in <module>\n    from prefect.server.database.interface import PrefectDBInterface\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\server\\database\\interface.py\", line 9, in <module>\n    from prefect.server.database import orm_models\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\server\\database\\orm_models.py\", line 26, in <module>\n    from prefect.server.events.actions import ServerActionTypes\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\server\\events\\actions.py\", line 86, in <module>\n    from prefect.utilities.schema_tools.hydration import (\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\utilities\\schema_tools\\__init__.py\", line 2, in <module>\n    from .validation import (\n  File \"<redacted>\\.venv\\lib\\site-packages\\prefect\\utilities\\schema_tools\\validation.py\", line 9, in <module>\n    from referencing.jsonschema import ObjectSchema, Schema\nModuleNotFoundError: No module named 'referencing'\n```\n\nAppears to be related to https://github.com/PrefectHQ/prefect/pull/16298, where a dependency on the `referencing` package was added, however it does not appear to be included in the Prefect dependencies.\n\nAdding the dependency to my own project resolves the issue as a workaround.\n\n### Version info\n\n```Text\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.10.11\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             win32/AMD64\nProfile:             local\nServer type:         ephemeral\nPydantic version:    2.11.4\nServer:\n  Database:          sqlite\n  SQLite version:    3.40.1\nIntegrations:\n  prefect-snowflake: 0.28.4\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Thanks for the issue @Daveography! `referencing` is a dependency of `jsonschema`, which is a dependency of `prefect`, so I'm surprised that `referencing` wasn't installed when you installed `prefect`. Could you share how you installed `prefect` so I can see if I can recreate the issue?"
      },
      {
        "user": "Daveography",
        "body": "> Thanks for the issue [@Daveography](https://github.com/Daveography)! `referencing` is a dependency of `jsonschema`, which is a dependency of `prefect`, so I'm surprised that `referencing` wasn't installed when you installed `prefect`. Could you share how you installed `prefect` so I can see if I can recreate the issue?\n\nI previously had Prefect 2.16.5 installed, which at the time installed `jsonschema` 4.17.3. `jsonschema` was not upgraded on updating `prefect` to 3.4.1. It looks like the `referencing` dependency was added just after, in [4.18.0](https://github.com/python-jsonschema/jsonschema/blob/main/CHANGELOG.rst#v4180)."
      },
      {
        "user": "Daveography",
        "body": "With that in mind, updating `jsonschema` to >=4.18.0 in my project was also an effective workaround."
      }
    ]
  },
  {
    "issue_number": 16904,
    "title": "'No module named ...' when referencing external files from flow with DaskTaskRunner",
    "author": "cvetelinandreevdreamix",
    "state": "open",
    "created_at": "2025-01-30T06:00:08Z",
    "updated_at": "2025-05-13T17:26:30Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nHi, all,\n\nI'm opening a new issue as it seems my comments on the closed one (#15783 ) are ignored.\n\nI'm attaching a project you can use to reproduce the error\n[test-prefect-migration copy.zip](https://github.com/user-attachments/files/18036318/test-prefect-migration.copy.zip)\n```\n2024-12-06 12:06:12 ModuleNotFoundError: No module named 'common'\n```\n\nIt happens when:\nI deploy the flow to Minio. Use python deployment.py\nI use a function from another module in the task I want to run with dask\nIf I run the flow from the local file system (python deployment.py) then it works fine\nIf I deploy and flow and I don't use a function imported from another module, then it works fine\n\nMy case is that I have a shared code between multiple flows and it is extracted in a shared module (common).\n\nI can see the common folder is deployed to Minio.\n\nLet me know If you need more information. I need to migrate to prefect 3 and use the automations.\n\nThank you for your time!\n\n### Version info\n\n```Text\nVersion:             2.20.9\nAPI version:         0.8.4\n\nPython version:      3.10.10\nGit commit:          b101915a\nBuilt:               Tue, Oct 1, 2024 12:41 PM\nOS/Arch:             darwin/arm64\nProfile:             pd-flow-local\nServer type:         server\n\n###### Requirements with Version Specifiers ######\nprefect~=2.20.9\nprefect_dask~=0.2.10\nprefect-docker==0.4.0\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "tsafacjo",
        "body": "Can I pick it ?"
      },
      {
        "user": "cicdw",
        "body": "Hi @tsafacjo if you have an idea for how to solve the issue, then yes please feel free to give it a go!"
      },
      {
        "user": "cvetelinandreevdreamix",
        "body": "Hi there,\n\nIt seems a hard one, right?"
      }
    ]
  },
  {
    "issue_number": 16472,
    "title": "High Memory Usage ",
    "author": "aaazzam",
    "state": "open",
    "created_at": "2024-12-21T02:50:05Z",
    "updated_at": "2025-05-13T01:28:20Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\r\n\r\nUsing uv and memray we can run\r\n\r\n```\r\nuvx --with prefect memray run --force --output flow.bin --force -c \"from prefect import flow\" \\\r\n&& uvx memray flamegraph --force flow.bin \\\r\n&& open memray-flamegraph-flow.html \r\n```\r\n\r\nand look at the corresponding flamegraph. You'll notice importing flow allocates ~1GB+ in memory to simply import flow, of which ~50MB stems from Prefect \"primitives\" whereas a whopping _1GB_ comes from Pendulum. \r\n\r\n### Version info\r\n\r\n```Text\r\nVersion:             3.1.9\r\nAPI version:         0.8.4\r\nPython version:      3.10.14\r\nGit commit:          e1fe7943\r\nBuilt:               Fri, Dec 20, 2024 4:33 PM\r\nOS/Arch:             darwin/arm64\r\nProfile:             local\r\nServer type:         server\r\nPydantic version:    2.10.4\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "cicdw",
        "body": "Pendulum is also holding us back from 3.13 support at the moment (ref issue: https://github.com/pydantic/pydantic-extra-types/issues/239). "
      },
      {
        "user": "cicdw",
        "body": "Would be curious to see how this looks in Python 3.13 specifically "
      }
    ]
  },
  {
    "issue_number": 16185,
    "title": "inconsistent hashing with more than one object",
    "author": "onlyjsmith",
    "state": "closed",
    "created_at": "2024-12-03T13:35:55Z",
    "updated_at": "2025-05-13T01:24:05Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\r\n\r\n- Run the code below in the terminal twice. \r\n- It gives consistent hashes for `data` and `config` objects alone, but an inconsistent  hash when putting them together.\r\n- I've tried with two objects of same type e.g. two Pydantic models, or two Pandas DataFrames, and the combined hash is consistent.\r\n- Hope I'm doing some stupid - please point out!\r\n\r\nEDIT: should point out that I got to this after finding the Prefect cache wasn't being hit when I thought it should\r\n\r\n```py\r\nimport pandas as pd\r\nfrom prefect.utilities.hashing import hash_objects\r\nfrom pydantic import BaseModel\r\n\r\n\r\nclass Config(BaseModel):\r\n    active: bool = False\r\n\r\n\r\nconfig = Config()\r\ndata = pd.DataFrame({\"value\": [True]})\r\n\r\nprint(f\"data                 : {hash_objects(data)}\")\r\nprint(f\"config               : {hash_objects(config)}\")\r\nprint(f\"data, config         : {hash_objects(data, config)}\")\r\n```\r\n\r\n### Version info\r\n\r\n```Text\r\nVersion:             3.1.5\r\nAPI version:         0.8.4\r\nPython version:      3.12.2\r\nGit commit:          3c06654e\r\nBuilt:               Mon, Dec 2, 2024 6:57 PM\r\nOS/Arch:             darwin/arm64\r\nProfile:             default\r\nServer type:         cloud\r\nPydantic version:    2.10.2\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n<img width=\"417\" alt=\"SCR-20241203-mwyr-2\" src=\"https://github.com/user-attachments/assets/668e4349-1a4c-469d-9bc3-26fa18128ca5\">\r\n",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @onlyjsmith - thank you for the issue! interesting, will take a look"
      },
      {
        "user": "zzstoatzz",
        "body": "just to document the process, i'm noticing that the hash of multiple objects is stable within the same process, but not between\r\n\r\n<details>\r\n\r\n```python\r\n# /// script\r\n# dependencies = [\"pandas\", \"prefect\"]\r\n# ///\r\n\r\nimport pathlib\r\n\r\nimport pandas as pd\r\nfrom pydantic import BaseModel\r\n\r\nfrom prefect.utilities.hashing import hash_objects\r\n\r\n\r\nclass Config(BaseModel):\r\n    active: bool = False\r\n\r\n\r\nconfig = Config()\r\ndata = pd.DataFrame({\"value\": [True]})\r\n\r\nw = 20\r\n# Save the first hash\r\nfirst_combined_hash = hash_objects(data, config)\r\nassert first_combined_hash is not None\r\n\r\nprint(\"\\nWithin same run:\")\r\nprint(f\"{'data':<{w}}: {hash_objects(data)}\")\r\nprint(f\"{'config':<{w}}: {hash_objects(config)}\")\r\nprint(f\"{'data, config (1)':<{w}}: {first_combined_hash}\")\r\nprint(f\"{'data, config (2)':<{w}}: {hash_objects(data, config)}\")\r\n\r\n# This will pass because it's within the same run\r\nassert hash_objects(data, config) == hash_objects(data, config)\r\n\r\n\r\nhash_file = pathlib.Path(\"previous_hash.txt\")\r\nif hash_file.exists():\r\n    previous_hash = hash_file.read_text().strip()\r\n    print(f\"\\nPrevious run's hash   : {previous_hash}\")\r\n    print(f\"This run's hash       : {first_combined_hash}\")\r\n    if previous_hash != first_combined_hash:\r\n        print(\"‚ùå Hashes differ between runs!\")\r\n    else:\r\n        print(\"‚úÖ Hashes match between runs!\")\r\n\r\nhash_file.write_text(first_combined_hash)\r\n\r\n```\r\n</details>"
      },
      {
        "user": "zzstoatzz",
        "body": "@onlyjsmith ok!\r\n\r\nThe issue appears to occur because `DataFrame` objects can't be directly JSON serialized, causing the hash to fall back to `cloudpickle` which includes non-deterministic elements between runs.\r\n\r\nin the short term, you can use the `to_dict` method to dump the df:\r\n```python\r\ndata_dict = data.to_dict(orient=\"split\")\r\ncombined_hash = hash_objects(data_dict, config)\r\n```\r\n\r\nI'll look more into what exactly is changing between runs here\r\n\r\n<details>\r\n\r\n<summary>repro</summary>\r\n\r\n```python\r\n# /// script\r\n# dependencies = [\"pandas\", \"prefect\"]\r\n# ///\r\n\r\nimport pathlib\r\nimport sys\r\n\r\nimport cloudpickle\r\nimport pandas as pd\r\nfrom pydantic import BaseModel\r\n\r\nfrom prefect.utilities.hashing import JSONSerializer, hash_objects\r\n\r\n\r\nclass Config(BaseModel):\r\n    active: bool = False\r\n\r\n\r\nconfig = Config()\r\ndata = pd.DataFrame({\"value\": [True]})\r\nhash_file = pathlib.Path(\"previous_hash.txt\")\r\n\r\n# Check for --reset flag\r\nif len(sys.argv) > 1 and sys.argv[1] == \"--reset\":\r\n    if hash_file.exists():\r\n        hash_file.unlink()\r\n        print(\"Previous hash file deleted.\")\r\n\r\n# Let's test each serialization method directly\r\nprint(\"\\nTesting serialization methods:\")\r\ntry:\r\n    serializer = JSONSerializer(dumps_kwargs={\"sort_keys\": True})\r\n    json_bytes = serializer.dumps((data, config))\r\n    print(\"‚úÖ JSON serialization succeeded\")\r\nexcept Exception as e:\r\n    print(f\"‚ùå JSON serialization failed: {e}\")\r\n\r\ntry:\r\n    pickle_bytes = cloudpickle.dumps((data, config))\r\n    print(\"‚úÖ Cloudpickle serialization succeeded\")\r\nexcept Exception as e:\r\n    print(f\"‚ùå Cloudpickle serialization failed: {e}\")\r\n\r\nw = 20\r\n# Convert DataFrame to a deterministic format before hashing\r\ndata_dict = data.to_dict(orient=\"split\")\r\nfirst_combined_hash = hash_objects(data_dict, config)\r\nassert first_combined_hash is not None\r\n\r\nprint(\"\\nWithin same run:\")\r\nprint(f\"{'data':<{w}}: {hash_objects(data)}\")\r\nprint(f\"{'config':<{w}}: {hash_objects(config)}\")\r\nprint(f\"{'data, config (1)':<{w}}: {first_combined_hash}\")\r\nprint(f\"{'data, config (2)':<{w}}: {hash_objects(data, config)}\")\r\n\r\n# This will pass because it's within the same run\r\nassert hash_objects(data, config) == hash_objects(data, config)\r\n\r\nif hash_file.exists():\r\n    previous_hash = hash_file.read_text().strip()\r\n    print(f\"\\nPrevious run's hash   : {previous_hash}\")\r\n    print(f\"This run's hash       : {first_combined_hash}\")\r\n    if previous_hash != first_combined_hash:\r\n        print(\"‚ùå Hashes differ between runs!\")\r\n    else:\r\n        print(\"‚úÖ Hashes match between runs!\")\r\n\r\nhash_file.write_text(first_combined_hash)\r\n\r\n```\r\n\r\n```python\r\n¬ª uv run repros/16185.py\r\nReading inline script metadata from `repros/16185.py`\r\n\r\nTesting serialization methods:\r\n‚ùå JSON serialization failed: Unable to serialize unknown type: <class 'pandas.core.frame.DataFrame'>\r\n‚úÖ Cloudpickle serialization succeeded\r\n\r\nWithin same run:\r\ndata                : 7bb5352e4148cf86417c819f588eb33e\r\nconfig              : 74053ae609dd793e29f12548541a58ef\r\ndata, config (1)    : 8fdf53aec8d6839e51ef9cb8ac1c22af\r\ndata, config (2)    : 1e41378dbf1e4b949458487b5bf6bd60\r\n\r\nPrevious run's hash   : 8fdf53aec8d6839e51ef9cb8ac1c22af\r\nThis run's hash       : 8fdf53aec8d6839e51ef9cb8ac1c22af\r\n‚úÖ Hashes match between runs!\r\n```\r\n\r\n\r\n</details>\r\n\r\n[these docs](https://docs.prefect.io/v3/develop/task-caching#handling-non-serializable-objects) may be useful to you "
      }
    ]
  },
  {
    "issue_number": 16977,
    "title": "Prefect Workers crash when server returns 500s",
    "author": "ashtuchkin",
    "state": "closed",
    "created_at": "2025-02-05T09:17:03Z",
    "updated_at": "2025-05-13T01:17:58Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWe run a pretty big self-hosted installation of Prefect 2.x (20k flow runs/day, 200k tasks) and noticed that when self-hosted API server becomes overloaded, it starts returning HTTP 500s.\n\nThat's OK by itself, but this makes Workers (we use Kubernetes) exit unexpectedly, then get restarted by K8s (issue 1).\nSpecifically we see the following problematic stack traces after which it exits:\n * _submit_run -> _check_flow_run -> read_deployment\n * cancel_run -> _get_configuration -> read_deployment\n\nRestarting also seems OK by itself, however we noticed that if a different flow was marked PENDING, but no K8s Job was scheduled yet when the worker exited, it'll be stuck in PENDING forever (issue 2). Here's the relevant code:\n\nhttps://github.com/PrefectHQ/prefect/blob/c4ac23189af1d27f8260452df302be8daee792b6/src/prefect/workers/base.py#L972-L977\n\nIssue 2 seems relatively hard to fully resolve, as it's impossible to atomically mark flow as pending and submit a job to K8s. Maybe we can do something by storing the state locally, but that will not work if the pod is restarted on a different node.\n\nIssue 1 looks more straightforward though. There's already a try/except around these places, but it only catches some exceptions, not all of them. Hopefully it'd be easy to resolve.\n\n### Version info\n\n```Text\nVersion:             2.20.14\nAPI version:         0.8.4\nPython version:      3.11.7\nGit commit:          fb919c67\nBuilt:               Mon, Nov 18, 2024 4:41 PM\nOS/Arch:             linux/x86_64\nProfile:             default\nServer type:         ephemeral\nServer:\n  Database:          sqlite\n  SQLite version:    3.40.1\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "cicdw",
        "body": "Hey @ashtuchkin - thank you for the detailed bug report! \n\nFirst question I have: could you share a stack trace including the exact status codes you're seeing? That would help me with error-type, etc. when updating the base worker logic.\n\nThe underlying client does have some amount of [finite retry logic for certain status codes](https://github.com/PrefectHQ/prefect/blob/main/src/prefect/client/base.py#L329-L333) that can be augmented through the `PREFECT_CLIENT_RETRY_EXTRA_CODES` setting. If your status code is one not listed here, adding that could be a temporary bandaid while we work on a more robust fix.\n\nExample of setting:\n```\nPREFECT_CLIENT_RETRY_EXTRA_CODES='500,421'\n```"
      },
      {
        "user": "ashtuchkin",
        "body": "Sure! Here's an example exception after which worker exits:\n```\n  + Exception Group Traceback (most recent call last):\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect/cli/_utilities.py\", line 42, in wrapper\n  |     return fn(*args, **kwargs)\n  |            ^^^^^^^^^^^^^^^^^^^\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 311, in coroutine_wrapper\n  |     return call()\n  |            ^^^^^^\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 432, in __call__\n  |     return self.result()\n  |            ^^^^^^^^^^^^^\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 318, in result\n  |     return self.future.result(timeout=timeout)\n  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 179, in result\n  |     return self.__get_result()\n  |            ^^^^^^^^^^^^^^^^^^^\n  |   File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n  |     raise self._exception\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 389, in _run_async\n  |     result = await coro\n  |              ^^^^^^^^^^\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect/cli/worker.py\", line 169, in start\n  |     async with worker_cls(\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect/workers/base.py\", line 1143, in __aexit__\n  |     await self.teardown(*exc_info)\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect_kubernetes/worker.py\", line 666, in teardown\n  |     await super().teardown(*exc_info)\n  |   File \"/usr/local/lib/python3.11/site-packages/prefect/workers/base.py\", line 541, in teardown\n  |     await self._runs_task_group.__aexit__(*exc_info)\n  |   File \"/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 736, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/usr/local/lib/python3.11/site-packages/prefect/workers/base.py\", line 871, in _submit_run\n    |     await self._check_flow_run(flow_run)\n    |   File \"/usr/local/lib/python3.11/site-packages/prefect/workers/base.py\", line 854, in _check_flow_run\n    |     deployment = await self._client.read_deployment(flow_run.deployment_id)\n    |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/prefect/client/orchestration.py\", line 1783, in read_deployment\n    |     response = await self._client.get(f\"/deployments/{deployment_id}\")\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1814, in get\n    |     return await self.request(\n    |            ^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1585, in request\n    |     return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/prefect/client/base.py\", line 358, in send\n    |     response.raise_for_status()\n    |   File \"/usr/local/lib/python3.11/site-packages/prefect/client/base.py\", line 171, in raise_for_status\n    |     raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__\n    | prefect.exceptions.PrefectHTTPStatusError: Server error '500 Internal Server Error' for url 'http://prefect-server:4200/api/deployments/9f495603-99ef-43c9-9a6d-6e5f18660219'\n    | Response: {'exception_message': 'Internal Server Error'}\n    | For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n    +------------------------------------\n```"
      },
      {
        "user": "ashtuchkin",
        "body": "Thanks for the pointer to `PREFECT_CLIENT_RETRY_EXTRA_CODES`! We'll use that when we bump into this next time. For now we've increased the server deployment size to try to hopefully avoid 500s altoghether."
      }
    ]
  },
  {
    "issue_number": 15928,
    "title": "Deploying a Flow with Docker Desktop in Europe raises BuildError: failed to export image: NotFound: content digest",
    "author": "anze3db",
    "state": "closed",
    "created_at": "2024-11-05T16:54:00Z",
    "updated_at": "2025-05-13T01:08:58Z",
    "labels": [
      "bug",
      "upstream dependency"
    ],
    "body": "### Bug summary\n\nTrying to create a deployment with a Docker Image on MacOS with Docker Desktop fails with the following error, but only if you are located in Europe:\r\n\r\n```\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/utilities/asyncutils.py\", line 399, in coroutine_wrapper\r\n    return run_coro_as_sync(ctx_call())\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/utilities/asyncutils.py\", line 243, in run_coro_as_sync\r\n    return call.result()\r\n           ^^^^^^^^^^^^^\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/_internal/concurrency/calls.py\", line 312, in result\r\n    return self.future.result(timeout=timeout)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/_internal/concurrency/calls.py\", line 182, in result\r\n    return self.__get_result()\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/zidar/.asdf/installs/python/3.12.1/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\r\n    raise self._exception\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/_internal/concurrency/calls.py\", line 383, in _run_async\r\n    result = await coro\r\n             ^^^^^^^^^^\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/utilities/asyncutils.py\", line 225, in coroutine_wrapper\r\n    return await task\r\n           ^^^^^^^^^^\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/utilities/asyncutils.py\", line 389, in ctx_call\r\n    result = await async_fn(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/deployments/runner.py\", line 925, in deploy\r\n    image.build()\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/docker/docker_image.py\", line 73, in build\r\n    build_image(**build_kwargs)\r\n  File \"/Users/zidar/.asdf/installs/python/3.12.1/lib/python3.12/contextlib.py\", line 81, in inner\r\n    return func(*args, **kwds)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/zidar/programming/app/.venv/lib/python3.12/site-packages/prefect/utilities/dockerutils.py\", line 194, in build_image\r\n    raise BuildError(event[\"error\"])\r\nprefect.utilities.dockerutils.BuildError: failed to export image: NotFound: content digest sha256:7fb66093b170bccb413f3e1c8f4b92fa440ea68fc4cddccf4c3b47e2673cfb9c: not found\r\n```\r\n\r\nIf you change your location (with a VPN) to the US, the issue does not reproduce.\r\nIf you use OrbStack instead of Docker Desktop, the issue also does not reproduce.\r\n\r\nWe figured this out because coworkers in US had no trouble creating the deployment, but others in the EU constantly get the error.\r\n\r\nExample deployment code:\r\n\r\n```\r\njob.deploy(\r\n    work_pool_name=work_pool_name,\r\n    image=DockerImage(\r\n        name=docker_image_name,\r\n        platform=\"linux/amd64\",\r\n        dockerfile=\"Dockerfile\",\r\n        target=target,\r\n    ),\r\n)\r\n```\r\n\r\nBuilding the Dockerfile manually doesn't raise this error.\n\n### Version info\n\n```Text\nVersion:             3.1.0\r\nAPI version:         0.8.4\r\nPython version:      3.12.6\r\nGit commit:          a83ba39b\r\nBuilt:               Thu, Oct 31, 2024 12:43 PM\r\nOS/Arch:             darwin/arm64\r\nProfile:             local\r\nServer type:         server\r\nPydantic version:    2.9.2\r\nIntegrations:\r\n  prefect-docker:    0.6.1\n```\n\n\n### Additional context\n\nThis issue has also popped up in the Prefect Community Slack: https://prefect-community.slack.com/archives/CL09KU1K7/p1730205889746789",
    "comments": [
      {
        "user": "teocns",
        "body": "Just out of curiosity, does this also occur when explicitly passing the registry name in the image? \r\n```\r\ndocker.io/your_username/image:tag\r\n```"
      },
      {
        "user": "anze3db",
        "body": "I haven't tried docker.io, but the issue reproduces with AWS ECR, no matter how I specify the registry name in the image.\r\n\r\nThe issue even reproduces if neither ECR nor docker.io configured and I'm building and using the image locally without pushing it to a remote repository."
      },
      {
        "user": "uskudnik",
        "body": "We found a workaround for the issue, but haven't pinpointed the exact culprit yet.\n\nIf one removes `platform=\"linux/amd64\",`, so:\n\n```python\njob.deploy(\n    work_pool_name=work_pool_name,\n    image=DockerImage(\n        name=docker_image_name,\n        #platform=\"linux/amd64\",\n        dockerfile=\"Dockerfile\",\n        target=target,\n    ),\n)\n```\nand runs the deployment, even if the deployment fails (this is only in our case due to unrelated issue with some `go` package), and then adds back in the `platform=\"linux/amd64\",`, the deployment goes through successfully on the second attempt. \n\nATM I can't reproduce the issue so I can't gather more data, but the solution worked for @anze3db today when he again stumbled upon it. When I was debugging the problem, the only thing that stood out and might be related was:\n\n```\n$ tail -f ~/Library/Containers/com.docker.docker/Data/log/vm/dockerd.log\n...\ntime=\"2025-02-07T23:41:30.272689258Z\" level=warning msg=\"failed to determine platform specific size\" digest=\"sha256:6365712bd66a08e836f2308a17f0fef28f3358bc0249fd6e87fdc4ee7cb000f7\" error=\"NotFound: content digest sha256:6365712bd66a08e836f2308a17f0fef28f3358bc0249fd6e87fdc4ee7cb000f7: not found\" image=\"docker.io/prefecthq/prefect:3.0.11-python3.12\" isPseudo=false manifest=\"{application/vnd.docker.distribution.manifest.v2+json sha256:6365712bd66a08e836f2308a17f0fef28f3358bc0249fd6e87fdc4ee7cb000f7 3256 [] map[] [] 0x4001d2f0e0 }\"\n...\n```\n\nSo maybe something with `platform` isn't propagated and built correctly? But that's just speculation ü§∑ "
      }
    ]
  },
  {
    "issue_number": 16949,
    "title": "pip install failing at \"Collecting pendulum<4,>=3.0.0 (from prefect)\" step",
    "author": "nibab",
    "state": "closed",
    "created_at": "2025-02-03T22:04:29Z",
    "updated_at": "2025-05-13T01:07:59Z",
    "labels": [
      "bug",
      "upstream dependency"
    ],
    "body": "### Bug summary\n\nTrying to run a vanilla pip install on a brand new venv returns:\n\n```\n(prefect-venv) ‚ûú  ngen-sheet-connectors git:(main) pip install -U prefect\nCollecting prefect\n  Using cached prefect-3.1.15-py3-none-any.whl.metadata (14 kB)\nCollecting aiosqlite<1.0.0,>=0.17.0 (from prefect)\n  Using cached aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\nCollecting alembic<2.0.0,>=1.7.5 (from prefect)\n  Using cached alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\nCollecting apprise<2.0.0,>=1.1.0 (from prefect)\n  Using cached apprise-1.9.2-py3-none-any.whl.metadata (52 kB)\nCollecting asyncpg<1.0.0,>=0.23 (from prefect)\n  Using cached asyncpg-0.30.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (5.0 kB)\nCollecting click<8.2,>=8.0 (from prefect)\n  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting cryptography>=36.0.1 (from prefect)\n  Using cached cryptography-44.0.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\nCollecting dateparser<2.0.0,>=1.1.1 (from prefect)\n  Using cached dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\nCollecting docker<8.0,>=4.0 (from prefect)\n  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphviz>=0.20.1 (from prefect)\n  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\nCollecting jinja2<4.0.0,>=3.0.0 (from prefect)\n  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\nCollecting jinja2-humanize-extension>=0.4.0 (from prefect)\n  Using cached jinja2_humanize_extension-0.4.0-py3-none-any.whl.metadata (3.6 kB)\nCollecting humanize<5.0.0,>=4.9.0 (from prefect)\n  Using cached humanize-4.11.0-py3-none-any.whl.metadata (7.8 kB)\nCollecting pytz<2025,>=2021.1 (from prefect)\n  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting readchar<5.0.0,>=4.0.0 (from prefect)\n  Using cached readchar-4.2.1-py3-none-any.whl.metadata (7.5 kB)\nCollecting sqlalchemy<3.0.0,>=2.0 (from sqlalchemy[asyncio]<3.0.0,>=2.0->prefect)\n  Using cached SQLAlchemy-2.0.37-cp313-cp313-macosx_10_13_x86_64.whl.metadata (9.6 kB)\nCollecting typer!=0.12.2,<0.16.0,>=0.12.0 (from prefect)\n  Using cached typer-0.15.1-py3-none-any.whl.metadata (15 kB)\nCollecting anyio<5.0.0,>=4.4.0 (from prefect)\n  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting asgi-lifespan<3.0,>=1.0 (from prefect)\n  Using cached asgi_lifespan-2.1.0-py3-none-any.whl.metadata (10 kB)\nCollecting cachetools<6.0,>=5.3 (from prefect)\n  Using cached cachetools-5.5.1-py3-none-any.whl.metadata (5.4 kB)\nCollecting cloudpickle<4.0,>=2.0 (from prefect)\n  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting coolname<3.0.0,>=1.0.4 (from prefect)\n  Using cached coolname-2.2.0-py2.py3-none-any.whl.metadata (6.2 kB)\nCollecting croniter<7.0.0,>=1.0.12 (from prefect)\n  Using cached croniter-6.0.0-py2.py3-none-any.whl.metadata (32 kB)\nCollecting exceptiongroup>=1.0.0 (from prefect)\n  Using cached exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\nCollecting fastapi<1.0.0,>=0.111.0 (from prefect)\n  Using cached fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\nCollecting fsspec>=2022.5.0 (from prefect)\n  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\nCollecting griffe<2.0.0,>=0.49.0 (from prefect)\n  Using cached griffe-1.5.6-py3-none-any.whl.metadata (5.0 kB)\nCollecting httpcore<2.0.0,>=1.0.5 (from prefect)\n  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\nCollecting httpx!=0.23.2,>=0.23 (from httpx[http2]!=0.23.2,>=0.23->prefect)\n  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting jsonpatch<2.0,>=1.32 (from prefect)\n  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting jsonschema<5.0.0,>=4.0.0 (from prefect)\n  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\nCollecting opentelemetry-api<2.0.0,>=1.27.0 (from prefect)\n  Using cached opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting orjson<4.0,>=3.7 (from prefect)\n  Using cached orjson-3.10.15-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\nCollecting packaging<24.3,>=21.3 (from prefect)\n  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pathspec>=0.8.0 (from prefect)\n  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\nCollecting pendulum<4,>=3.0.0 (from prefect)\n  Using cached pendulum-3.0.0.tar.gz (84 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  √ó Preparing metadata (pyproject.toml) did not run successfully.\n  ‚îÇ exit code: 1\n  ‚ï∞‚îÄ> [6 lines of output]\n\n      Cargo, the Rust package manager, is not installed or is not on PATH.\n      This package requires Rust and Cargo to compile extensions. Install it through\n      the system's package manager or via https://rustup.rs/\n\n      Checking for Rust toolchain....\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n√ó Encountered error while generating package metadata.\n‚ï∞‚îÄ> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n```\n\n### Version info\n\n```Text\nno version. this is at pip install.\n\nrunning on Macos 13.6.9 (22G830)\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @nibab - what version of `python` are you using? if you're using 3.13 this might be https://github.com/pydantic/pydantic-extra-types/issues/239"
      },
      {
        "user": "obendidi",
        "body": "I think pendulum doesn't have a wheel available for macos with arm chips, so it is trying to build the package for arm.\n\nYou have to install rustup and cargo on your machine and then reinstall using pip\n\n(from memory you also need install a missing rust dependency using `rustup`, you will see the command to run when it fails)  "
      },
      {
        "user": "zzstoatzz",
        "body": "> I think pendulum doesn't have a wheel available for macos with arm chips, so it is trying to build the package for arm.\n> \n> You have to install rustup and cargo on your machine and then reinstall using pip\n> \n> (from memory you also need install a missing rust dependency using `rustup`, you will see the command to run when it fails)\n\n@obendidi right, I think this is the case in 3.13, as mentioned in https://github.com/pydantic/pydantic-extra-types/issues/239"
      }
    ]
  },
  {
    "issue_number": 16088,
    "title": "Feature Request: Cloud Free Tier Audit Log Retention",
    "author": "Btibert3",
    "state": "closed",
    "created_at": "2024-11-22T05:01:40Z",
    "updated_at": "2025-05-13T00:59:01Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nAs a Cloud Free Tier user, I do not have access to logs, which is 100% understandable.  I recently had a scenario where (small) managed deployments on Cloud started crashing.  They have been running without issue for almost two months, and no code changes were made to the underlying Github repo.  Perhaps the free tier can have access to logs for 24 hours?  12 hours?  In the console, the error was that the container crashed without any other info to help debug what changed.\r\n\r\n![](\r\n![image](https://github.com/user-attachments/assets/803c9001-3c53-492e-84ae-20bf1ada8838)\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/590d0989-6cee-4009-aa89-e34c5bef8fd0)\r\n\n\n### Describe the proposed behavior\n\nLogs are available in the console to help debug issues where manged work pools crash.\n\n### Example Use\n\nDebug issues with a deployment\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "cicdw",
        "body": "Audit logs would actually not help debug this deeper; that being said, these black-box infra failures now have better logging that should be visible on the free tier as well."
      }
    ]
  },
  {
    "issue_number": 16250,
    "title": "Add prefect injected labels to a k8s job's pods.",
    "author": "matt-mckeithen",
    "state": "closed",
    "created_at": "2024-12-06T19:13:12Z",
    "updated_at": "2025-05-13T00:56:18Z",
    "labels": [
      "enhancement",
      "integrations"
    ],
    "body": "### Describe the current behavior\n\nPrefect injected labels like `prefect.io/deployment-name` and `prefect.io/flow-name` are only applied to the k8s job.\n\n### Describe the proposed behavior\n\n Apply these labels to the pods as well.\n\n### Example Use\n\nWe'd like to use these labels for more granular usage and cost reporting.  Having them only on the job makes it more difficult and inefficient to report on this data.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @matt-mckeithen - thanks for the issue!\r\n\r\nany interest in / capacity for a contribution to `prefect-kubernetes` for this?"
      },
      {
        "user": "desertaxle",
        "body": "This is added in the alpha version of the new Kubernetes worker via https://github.com/PrefectHQ/prefect/pull/17401. You can test it out by installing `prefect-kubernetes==0.6.0a2`."
      },
      {
        "user": "cicdw",
        "body": "This has now been released üéâ "
      }
    ]
  },
  {
    "issue_number": 16982,
    "title": "Upgrade `websockets.legacy` usage",
    "author": "cicdw",
    "state": "closed",
    "created_at": "2025-02-05T16:51:54Z",
    "updated_at": "2025-05-13T00:52:36Z",
    "labels": [
      "enhancement",
      "upstream dependency"
    ],
    "body": "### Describe the current behavior\n\nCurrently we rely on imported objects from `websockets.legacy` which is deprecated (as can be seen here, for example: https://github.com/PrefectHQ/prefect/actions/runs/13147657643/job/36689175326?pr=16972).\n\n### Describe the proposed behavior\n\nWe need to plan to move to the newer asyncio implementation following the guidelines outlined [here](https://websockets.readthedocs.io/en/stable/howto/upgrade.html). We believe this should be straightforward as we don't rely on anything deep cut, but opening this issue to track so we don't get caught off guard with an upgrade.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "cicdw",
        "body": "Completed in https://github.com/PrefectHQ/prefect/pull/17274"
      }
    ]
  },
  {
    "issue_number": 16043,
    "title": "Bugged UI. Does not show any task run and data flow chart",
    "author": "kavvkon",
    "state": "closed",
    "created_at": "2024-11-18T13:48:52Z",
    "updated_at": "2025-05-13T00:48:43Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen I visualize a flow run in a local server, the flow runs successfully but the visual info on server does not work very well. I do not see any task runs, and the flow runs do not appear in the UI as blocks.\r\nVersion 2 was working fine !\r\n\r\n\r\nSuccessful run:\r\nC:\\Windows>C:/Users/paul/Anaconda3/envs/prefect_env/python.exe my_gh_workflow.py\r\n\r\n15:05:55.665 | INFO    | prefect.engine - Created flow run 'righteous-dragon' for flow 'log-repo-info'\r\n15:05:55.665 | INFO    | prefect.engine - View at http://192.168.1.10:4200/runs/flow-run/b99db794-7b04-4b70-af3d-7eeb6bd51948\r\n15:05:57.441 | INFO    | Task run 'get_repo_info-2bd' - Finished in state Completed()\r\n15:05:57.441 | INFO    | Flow run 'righteous-dragon' - Stars üå† : 17477\r\n15:05:59.144 | INFO    | Task run 'get_contributors-838' - Finished in state Completed()\r\n15:05:59.146 | INFO    | Flow run 'righteous-dragon' - Number of contributors üë∑: 30\r\n15:05:59.188 | INFO    | Flow run 'righteous-dragon' - Finished in state Completed()\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/0fb4debc-1ddc-4d05-beaa-ba6d86fe0abe)\r\n\r\n![image](https://github.com/user-attachments/assets/6815410d-65bb-47c1-947e-582a17991cd2)\r\n\n\n### Version info\n\n```Text\nVersion:             3.1.1\r\nAPI version:         0.8.4\r\nPython version:      3.12.7\r\nGit commit:          6b50a2b9\r\nBuilt:               Fri, Nov 8, 2024 12:38 PM\r\nOS/Arch:             win32/AMD64\r\nProfile:             default\r\nServer type:         server\r\nPydantic version:    2.9.2\n```\n\n\n### Additional context\n\nCalling this file my_gh_workflow.py based on this example from https://docs.prefect.io/3.0/get-started/quickstart\r\n```python\r\nimport httpx   # an HTTP client library and dependency of Prefect\r\nfrom prefect import flow, task\r\n\r\n@task(retries=2)\r\ndef get_repo_info(repo_owner: str, repo_name: str):\r\n    \"\"\"Get info about a repo - will retry twice after failing\"\"\"\r\n    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}\"\r\n    api_response = httpx.get(url)\r\n    api_response.raise_for_status()\r\n    repo_info = api_response.json()\r\n    return repo_info\r\n\r\n@task\r\ndef get_contributors(repo_info: dict):\r\n    \"\"\"Get contributors for a repo\"\"\"\r\n    contributors_url = repo_info[\"contributors_url\"]\r\n    response = httpx.get(contributors_url)\r\n    response.raise_for_status()\r\n    contributors = response.json()\r\n    return contributors\r\n\r\n@flow(log_prints=True)\r\ndef log_repo_info(repo_owner: str = \"PrefectHQ\", repo_name: str = \"prefect\"):\r\n    \"\"\"\r\n    Given a GitHub repository, logs the number of stargazers\r\n    and contributors for that repo.\r\n    \"\"\"\r\n    repo_info = get_repo_info(repo_owner, repo_name)\r\n    print(f\"Stars üå† : {repo_info['stargazers_count']}\")\r\n\r\n    contributors = get_contributors(repo_info)\r\n    print(f\"Number of contributors üë∑: {len(contributors)}\")\r\n\r\nif __name__ == \"__main__\":\r\n    log_repo_info()\r\n```",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @kavvkon - thanks for the issue. Do you have any rules that would prevent websocket traffic? also do you see anything about the `EventsWorker` still processing items?\r\n\r\nI think this is likely a duplicate of [this](https://github.com/PrefectHQ/prefect/issues/15153) issue"
      },
      {
        "user": "kavvkon",
        "body": "Yes that should be it. The prefect server is hosted in another PC in intranet, (not through https). But I am in a corporate network and there are some restrictions. Any idea how to bypass and ignore SSL related errors ?"
      },
      {
        "user": "kavvkon",
        "body": "This code works btw\r\n```python\r\nimport asyncio\r\n\r\nfrom prefect.events.clients import PrefectEventsClient\r\n\r\nasync def main():\r\n    async with PrefectEventsClient() as client:\r\n        print(f\"Connected to: {client._events_socket_url}\")\r\n        pong = await client._websocket.ping()\r\n        pong_time = await pong\r\n        print(f\"Response received in: {pong_time}\")\r\n\r\nif __name__ == '__main__':\r\n    asyncio.run(main())\r\n```\r\n \r\n```\r\nConnected to: ws://192.168.1.10:4200/api/events/in\r\nResponse received in: 0.0025052999990293756\r\n```"
      }
    ]
  },
  {
    "issue_number": 17703,
    "title": "Dangling flow runs from deleted schedules cause duplicates",
    "author": "Ultramann",
    "state": "open",
    "created_at": "2025-04-02T21:12:20Z",
    "updated_at": "2025-05-12T21:11:17Z",
    "labels": [
      "bug",
      "great writeup"
    ],
    "body": "### Bug summary\n\nWe've been experiencing incorrectly duplicated flow runs executing at the same time on our self-hosted prefect server. This is an issue for us because some of our flow require resource locks and so duplicated flows create a race condition.\n\nDigging into the prefect database I found that the duplication seems to be coming from different idempotency keys where the difference comes from the schedule id, and the duplicate flow runs seem to be coming from schedules that no longer exist.\n```sql\nwith cte as (\n  select deployment_id, expected_start_time\n  from flow_run\n  group by deployment_id, expected_start_time\n  having count(*) > 1\n)\nselect cte.deployment_id, fr.created, fr.state_type, fr.idempotency_key, fr.created_by, ds.id is not null as schedule_exists\nfrom flow_run fr\njoin cte\non fr.deployment_id = cte.deployment_id\nand fr.expected_start_time = cte.expected_start_time\nleft join deployment_schedule ds\non created_by->>'id' = ds.id::text\norder by cte.deployment_id, cte.expected_start_time\n```\n\nBelow you can see some flow run records from this query. The first two rows show duplicate runs where one failed and the other completed. This isn't remarkable as we expect deployment schedules to be deleted over time. The next two rows, however, show duplicate scheduled flow runs where one has an existing schedule, and the other refers to a schedule that no longer exists. Because they refer to different schedules the idepempotency key is different and therefore the duplicates aren't detected.\n\n![Image](https://github.com/user-attachments/assets/b1d36117-8798-4ea3-9889-84a7d92e0b5d)\n\nIf I'm reading the source code correctly, I think running `prefect deploy` for a deployment that already exists should result in the old flow runs getting deleted, see code path below, so I'm not sure why this isn't happening for my scheduled flow runs all the time. Code path\n1. [`prefect deploy`](https://github.com/PrefectHQ/prefect/blob/3f0c87e32e54e1f184df275c4263705b9ad90b6e/src/prefect/cli/deploy.py#L239)\n2. [`_run_single_deploy`](https://github.com/PrefectHQ/prefect/blob/3f0c87e32e54e1f184df275c4263705b9ad90b6e/src/prefect/cli/deploy.py#L487)\n3. [`RunnerDeployment.apply`](https://github.com/PrefectHQ/prefect/blob/af89e980e1c2ffbd54ca5a8b4e3ce4782a62570f/src/prefect/deployments/runner.py#L430)\n4. [`RunnerDeployment._update`](https://github.com/PrefectHQ/prefect/blob/af89e980e1c2ffbd54ca5a8b4e3ce4782a62570f/src/prefect/deployments/runner.py#L370)\n5. [`DeploymentAsyncClient.update_deployment`](https://github.com/PrefectHQ/prefect/blob/c8986edebb2dde3e2a931adbe24d2eaefcb799cb/src/prefect/client/orchestration/_deployments/client.py#L706)\n6. [`update_deployment`](https://github.com/PrefectHQ/prefect/blob/3f0c87e32e54e1f184df275c4263705b9ad90b6e/src/prefect/server/api/deployments.py#L183)\n7. [`models.deployments.update_deployment`](https://github.com/PrefectHQ/prefect/blob/3f0c87e32e54e1f184df275c4263705b9ad90b6e/src/prefect/server/api/deployments.py#L303)\n8. [`_delete_scheduled_runs`](https://github.com/PrefectHQ/prefect/blob/3f0c87e32e54e1f184df275c4263705b9ad90b6e/src/prefect/server/models/deployments.py#L265)\n\nI might be missing something in that order, but I'm pretty sure it's close because I see logs for PATCH requests to `/api/deployments`, number 6 above, on the server during deployment. Note, we don't have slugs for our schedules.\n\n## Reproduction\nI have tried and failed to create a minimal repro. I can't tell if this is because the issue is related to the way we deploy which is difficult to reproduce minimally, or if it's related to something else in our setup that doesn't exist in a local repro, or if it just occurs randomly with low probability and I haven't gotten 'lucky\" enough to reproduce it. I can say that this happens consistently during our deployments.\n\n### Version info\n\n```Text\nVersion:             3.2.11\nAPI version:         0.8.4\nPython version:      3.11.11\nGit commit:          9481694f\nBuilt:               Wed, Mar 5, 2025 10:00 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.11.1\nServer:\n  Database:          postgresql\nIntegrations:\n  prefect-gcp:       0.6.4\n```",
    "comments": [
      {
        "user": "cicdw",
        "body": "Thank you for the detailed issue @Ultramann!! I've seen someone report this before and I can't figure it out -- if `_delete_scheduled_run` fails it should cause the `PATCH` update request to fail, so I don't understand how some of these runs stick around. \n\nOne possibility is that the runs are actually in a `PENDING` state at the time of the update -- when you see this happen, are the duplicates always \"earlier\" in the list of scheduled runs?"
      },
      {
        "user": "Ultramann",
        "body": "@cicdw, thanks for the quick response. It seems the duplicates have no correlation with their order in the list; that is, I've found duplicated flow runs where the one with a deleted schedule is first in the list, and other pairs where it's second in the list.\n\nSince the `PENDING` state seemed a reasonable guess I tried to reproduce the issue locally by forcing an artificially long period of `PENDING` state via adding a `time.sleep` [here](https://github.com/PrefectHQ/prefect/blob/c2d440c31676cecf2e0c6827817feaed81b27c62/src/prefect/flow_engine.py#L311). That gave me a large window to recreate a deployment, on a `'* * * * *'` cron, while one of its scheduled runs was pending. Unfortunately, no repro. I ran this query before and after doing the redeploy\n\n```sql\nselect fr.expected_start_time, fr.state_type, idempotency_key, ds.id is not null as schedule_exists\nfrom flow_run fr left join deployment_schedule ds\non created_by->>'id' = ds.id where state_type != 'COMPLETED'\norder by fr.expected_start_time limit 5;\n```\n\nHere's the result before\n\n```\nexpected_start_time         state_type  idempotency_key                                               schedule_exists\n--------------------------  ----------  ------------------------------------------------------------  ---------------\n2025-04-03 14:45:00.000000  PENDING     scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e a185da86-b19a  1              \n                                        -4d5f-b6cc-02201c3d3e6b 2025-04-03 14:45:00+00:00                            \n\n2025-04-03 14:46:00.000000  SCHEDULED   scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e a185da86-b19a  1              \n                                        -4d5f-b6cc-02201c3d3e6b 2025-04-03 14:46:00+00:00                            \n\n2025-04-03 14:47:00.000000  SCHEDULED   scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e a185da86-b19a  1              \n                                        -4d5f-b6cc-02201c3d3e6b 2025-04-03 14:47:00+00:00                            \n\n2025-04-03 14:48:00.000000  SCHEDULED   scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e a185da86-b19a  1              \n                                        -4d5f-b6cc-02201c3d3e6b 2025-04-03 14:48:00+00:00                            \n\n2025-04-03 14:49:00.000000  SCHEDULED   scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e a185da86-b19a  1              \n                                        -4d5f-b6cc-02201c3d3e6b 2025-04-03 14:49:00+00:00                         \n```\nand after\n```\nexpected_start_time         state_type  idempotency_key                                               schedule_exists\n--------------------------  ----------  ------------------------------------------------------------  ---------------\n2025-04-03 14:45:00.000000  PENDING     scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e a185da86-b19a  0              \n                                        -4d5f-b6cc-02201c3d3e6b 2025-04-03 14:45:00+00:00                            \n\n2025-04-03 14:46:00.000000  SCHEDULED   scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e 94bbb4f1-ad91  1              \n                                        -4783-b653-b1b4934391b7 2025-04-03 14:46:00+00:00                            \n\n2025-04-03 14:47:00.000000  SCHEDULED   scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e 94bbb4f1-ad91  1              \n                                        -4783-b653-b1b4934391b7 2025-04-03 14:47:00+00:00                            \n\n2025-04-03 14:48:00.000000  SCHEDULED   scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e 94bbb4f1-ad91  1              \n                                        -4783-b653-b1b4934391b7 2025-04-03 14:48:00+00:00                            \n\n2025-04-03 14:49:00.000000  SCHEDULED   scheduled 16cba0e8-5672-419a-b5e2-6442d63f364e 94bbb4f1-ad91  1              \n                                        -4783-b653-b1b4934391b7 2025-04-03 14:49:00+00:00                  \n```\n\nSome random thoughts I had.\n* The `PENDING` state cause seemed plausible to me because it would result in somewhat random occurrences\n* Other things that could have more stochastic behavior would be concurrent/parallel processes \n  * Are there any during `prefect deploy`? I don't see any, but thought I'd check.\n  * Maybe they could be processes related to server start up? We recreate updated deployments right after the server starts. Could that be causing background processes to validate/update/recreate the existing scheduled runs?\n  * If you look at the original screenshot I took of the db state, you'll see that the `created` time for the duplicates is very close, but I don't know what's creating the old scheduled run?\n* This sequence of code seemed odd to me, in [`update_deployment`](https://github.com/PrefectHQ/prefect/blob/3f0c87e32e54e1f184df275c4263705b9ad90b6e/src/prefect/server/api/deployments.py#L183)\nhttps://github.com/PrefectHQ/prefect/blob/3f0c87e32e54e1f184df275c4263705b9ad90b6e/src/prefect/server/api/deployments.py#L303\n  this creates new deployment schedules at the end of the function\n  https://github.com/PrefectHQ/prefect/blob/3f0c87e32e54e1f184df275c4263705b9ad90b6e/src/prefect/server/models/deployments.py#L275-L288\n  but back in `update_deployment` when `models.deployments.update_deployment` returns, we do that again\n  https://github.com/PrefectHQ/prefect/blob/3f0c87e32e54e1f184df275c4263705b9ad90b6e/src/prefect/server/api/deployments.py#L315-L327\n\n  There's likely a good reason for this duplication, and maybe sqlalchemy handles this gracefully, but the duplicated code caught my eye."
      },
      {
        "user": "Ultramann",
        "body": "@cicdw @zzstoatzz any other ideas on this? I'm happy to test out hypotheses. Many thanks."
      }
    ]
  },
  {
    "issue_number": 18017,
    "title": "RedisLockManager breaks Ray and Dask flows",
    "author": "bogdibota",
    "state": "closed",
    "created_at": "2025-05-10T03:12:35Z",
    "updated_at": "2025-05-12T19:38:04Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nHello! I tried to use cached tasks with `IsolationLevel.SERIALIZABLE` in a flow with `RayTaskRunner` (or `DaskTaskRunner`), but it errors before starting. From my understanding, something related to `RedisLockManager` seems to error when being serialized.\n\n\nHere is the minimal example\n```python\n# /// script\n# dependencies = [\"prefect\", \"prefect[redis]\", \"prefect[dask]\", \"prefect[ray]\"]\n# ///\n\nimport prefect\nfrom prefect.cache_policies import TASK_SOURCE, INPUTS\nfrom prefect.transactions import IsolationLevel\nfrom prefect_dask.task_runners import DaskTaskRunner\nfrom prefect_redis import RedisLockManager\nfrom prefect_ray import RayTaskRunner\n\n@prefect.task(\n    cache_policy=(TASK_SOURCE + INPUTS).configure(\n        isolation_level=IsolationLevel.SERIALIZABLE,\n        lock_manager=RedisLockManager(host=\"localhost\", port=6379),\n    )\n)\ndef cached_task(input: int):\n    logger = prefect.get_run_logger()\n    logger.info(f\"executed task for {input}\")\n    return input + 1\n\ndef simple_flow():\n    logger = prefect.get_run_logger()\n    results = cached_task.map([1, 2, 2, 3, 3, 3]).result()\n    logger.info(f\"results: {results}\")\n\nif __name__ == \"__main__\":\n    ray_flow = prefect.flow(task_runner=RayTaskRunner)(simple_flow)\n    dask_flow = prefect.flow(task_runner=DaskTaskRunner)(simple_flow)\n    concurrent_flow = prefect.flow()(simple_flow)\n\n    ray_flow() # fails\n    dask_flow() # fails\n    concurrent_flow() # works as expected\n```\n\n`concurrent_flow` work as expected, but both `ray_flow` and `dask_flow` flow. I did comment/uncomment to run only one at a time.\n\ndisclaimer: i'm new to both python and prefect, maybe i'm missing something here üòÖ \n\n### Version info\n\n```Text\nVersion:             3.4.1\nAPI version:         0.8.4\nPython version:      3.13.2\nGit commit:          b47ad8e1\nBuilt:               Thu, May 08, 2025 08:42 PM\nOS/Arch:             linux/x86_64\nProfile:             prefect-cloud\nServer type:         cloud\nPydantic version:    2.11.4\n```\n\n### Additional context\n\nThe regular flow works as expected:\n```\n05:49:16.413 | INFO    | Task run 'cached_task-ed2' - executed task for 3\n05:49:16.414 | INFO    | Task run 'cached_task-6cb' - executed task for 1\n05:49:16.414 | INFO    | Task run 'cached_task-642' - executed task for 2\n05:49:16.421 | INFO    | Task run 'cached_task-ed2' - Finished in state Completed()\n05:49:16.423 | INFO    | Task run 'cached_task-642' - Finished in state Completed()\n05:49:16.423 | INFO    | Task run 'cached_task-6cb' - Finished in state Completed()\n05:49:16.517 | INFO    | Task run 'cached_task-bc0' - Finished in state Cached(type=COMPLETED)\n05:49:16.519 | INFO    | Task run 'cached_task-a90' - Finished in state Cached(type=COMPLETED)\n05:49:16.616 | INFO    | Task run 'cached_task-cec' - Finished in state Cached(type=COMPLETED)\n05:49:16.618 | INFO    | Flow run 'mustard-cat' - results: [2, 3, 3, 4, 4, 4]\n05:49:16.803 | INFO    | Flow run 'mustard-cat' - Finished in state Completed()\n```\n\nRay errors with:\n```\nTypeError: Could not serialize the argument <prefect.tasks.Task object at 0x7f203c529950> for a task or actor prefect_ray.task_runners.RayTaskRunner._run_prefect_task:\n=========================================================================\nChecking Serializability of <prefect.tasks.Task object at 0x7f203c529950>\n=========================================================================\n!!! FAIL serialization: cannot pickle '_thread.lock' object\n    Serializing '__wrapped__' <function cached_task at 0x7f2040f2b1a0>...\n    Serializing 'fn' <function cached_task at 0x7f2040f2b1a0>...\n    Serializing 'apply_async' <bound method Task.apply_async of <prefect.tasks.Task object at 0x7f203c529950>>...\n    !!! FAIL serialization: cannot pickle '_thread.lock' object\n        Serializing '__func__' <function Task.apply_async at 0x7f203d4620c0>...\n    WARNING: Did not find non-serializable object in <bound method Task.apply_async of <prefect.tasks.Task object at 0x7f203c529950>>. This may be an oversight.\n=========================================================================\nVariable:\n\n        FailTuple(apply_async [obj=<bound method Task.apply_async of <prefect.tasks.Task object at 0x7f203c529950>>, parent=<prefect.tasks.Task object at 0x7f203c529950>])\n\nwas found to be non-serializable. There may be multiple other undetected variables that were non-serializable.\nConsider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class.\n=========================================================================\nCheck https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.\nIf you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/\n=========================================================================\n```\n\nDask errors with:\n```\nTraceback (most recent call last):\n  File \"/home/bogdi/projects/reactive/varyn/varyn-core/flows/redis_lock_bug.py\", line 34, in <module>\n    dask_flow()\n    ~~~~~~~~~^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/flows.py\", line 1691, in __call__\n    return run_flow(\n        flow=self,\n    ...<2 lines>...\n        return_type=return_type,\n    )\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/flow_engine.py\", line 1527, in run_flow\n    ret_val = run_flow_sync(**kwargs)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/flow_engine.py\", line 1372, in run_flow_sync\n    return engine.state if return_type == \"state\" else engine.result()\n                                                       ~~~~~~~~~~~~~^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/flow_engine.py\", line 350, in result\n    raise self._raised\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/flow_engine.py\", line 763, in run_context\n    yield self\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/flow_engine.py\", line 1370, in run_flow_sync\n    engine.call_flow_fn()\n    ~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/flow_engine.py\", line 783, in call_flow_fn\n    result = call_with_parameters(self.flow.fn, self.parameters)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n    return fn(*args, **kwargs)\n  File \"/home/bogdi/projects/reactive/varyn/varyn-core/flows/redis_lock_bug.py\", line 25, in simple_flow\n    results = cached_task.map([1, 2, 2, 3, 3, 3]).result()\n              ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/tasks.py\", line 1438, in map\n    futures = task_runner.map(self, parameters, wait_for)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect_dask/task_runners.py\", line 481, in map\n    return super().map(task, parameters, wait_for)\n           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect/task_runners.py\", line 208, in map\n    self.submit(\n    ~~~~~~~~~~~^\n        task=task,\n        ^^^^^^^^^^\n    ...<2 lines>...\n        dependencies=task_inputs,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect_dask/task_runners.py\", line 448, in submit\n    future = self.client.submit(\n        task,\n    ...<3 lines>...\n        return_type=\"state\",\n    )\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/prefect_dask/client.py\", line 64, in submit\n    future = super().submit(\n        wrapper_func,\n    ...<10 lines>...\n        **run_task_kwargs,\n    )\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/distributed/client.py\", line 2141, in submit\n    expr = LLGExpr(\n        {\n    ...<6 lines>...\n        }\n    )\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/_expr.py\", line 72, in __new__\n    inst._name\n  File \"/usr/lib64/python3.13/functools.py\", line 1042, in __get__\n    val = self.func(instance)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/_expr.py\", line 522, in _name\n    return self._funcname + \"-\" + self.deterministic_token\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/_expr.py\", line 517, in deterministic_token\n    self._determ_token = self.__dask_tokenize__()\n                         ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/_expr.py\", line 147, in __dask_tokenize__\n    self._determ_token = _tokenize_deterministic(type(self), *self.operands)\n                         ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 457, in _tokenize_deterministic\n    return tokenize(*args, ensure_deterministic=True, **kwargs)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 76, in tokenize\n    return _tokenize(*args, **kwargs)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 34, in _tokenize\n    token: object = _normalize_seq_func(args)\n                    ~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 154, in _normalize_seq_func\n    return tuple(map(_inner_normalize_token, seq))\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 147, in _inner_normalize_token\n    return normalize_token(item)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/utils.py\", line 772, in __call__\n    return meth(arg, *args, **kwargs)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 122, in normalize_dict\n    return \"dict\", _normalize_seq_func(\n                   ~~~~~~~~~~~~~~~~~~~^\n        sorted(d.items(), key=lambda kv: str(kv[0]))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 154, in _normalize_seq_func\n    return tuple(map(_inner_normalize_token, seq))\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 147, in _inner_normalize_token\n    return normalize_token(item)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/utils.py\", line 772, in __call__\n    return meth(arg, *args, **kwargs)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 161, in normalize_seq\n    return type(seq).__name__, _normalize_seq_func(seq)\n                               ~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 154, in _normalize_seq_func\n    return tuple(map(_inner_normalize_token, seq))\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 147, in _inner_normalize_token\n    return normalize_token(item)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/utils.py\", line 772, in __call__\n    return meth(arg, *args, **kwargs)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 198, in normalize_object\n    return method()\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/_task_spec.py\", line 707, in __dask_tokenize__\n    return self._get_token()\n           ~~~~~~~~~~~~~~~^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/_task_spec.py\", line 696, in _get_token\n    self._token = tokenize(\n                  ~~~~~~~~^\n        (\n        ^\n    ...<4 lines>...\n        )\n        ^\n    )\n    ^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 76, in tokenize\n    return _tokenize(*args, **kwargs)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 34, in _tokenize\n    token: object = _normalize_seq_func(args)\n                    ~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 154, in _normalize_seq_func\n    return tuple(map(_inner_normalize_token, seq))\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 147, in _inner_normalize_token\n    return normalize_token(item)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/utils.py\", line 772, in __call__\n    return meth(arg, *args, **kwargs)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 161, in normalize_seq\n    return type(seq).__name__, _normalize_seq_func(seq)\n                               ~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 154, in _normalize_seq_func\n    return tuple(map(_inner_normalize_token, seq))\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 147, in _inner_normalize_token\n    return normalize_token(item)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/utils.py\", line 772, in __call__\n    return meth(arg, *args, **kwargs)\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 212, in normalize_object\n    _maybe_raise_nondeterministic(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        f\"Object {o!r} cannot be deterministically hashed. This likely \"\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        \"indicates that the object cannot be serialized deterministically.\"\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/bogdi/.cache/uv/environments-v2/redis-lock-bug-63ce28e9b3b3ffe7/lib64/python3.13/site-packages/dask/tokenize.py\", line 89, in _maybe_raise_nondeterministic\n    raise TokenizationError(msg)\ndask.tokenize.TokenizationError: Object <function cached_task at 0x7f8addd1f9c0> cannot be deterministically hashed. This likely indicates that the object cannot be serialized deterministically.\n```",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @bogdibota - thanks for the issue! this should be fixed in the PR linked above\n\ncan you try your reproduction against this branch? for example, copying everything besides the inline script metadata\n```bash\npbpaste | uv run \\\n--with git+https://github.com/prefecthq/prefect.git@redis-locking-serialization#subdirectory=src/integrations/prefect-redis \\\n--with git+https://github.com/prefecthq/prefect.git@redis-locking-serialization#subdirectory=src/integrations/prefect-ray \\\n--with git+https://github.com/prefecthq/prefect.git@redis-locking-serialization#subdirectory=src/integrations/prefect-dask -\n```"
      },
      {
        "user": "bogdibota",
        "body": "hi @zzstoatzz thanks for looking into this. the PR fixes indeed the serialization bug, thanks!\n\nhowever, there seems to be a new error, but just for the dask runner\n\nhere are the full logs\n```\n‚ùØ pbpaste | uv run \\\n--with git+https://github.com/prefecthq/prefect.git@redis-locking-serialization#subdirectory=src/integrations/prefect-redis \\\n--with git+https://github.com/prefecthq/prefect.git@redis-locking-serialization#subdirectory=src/integrations/prefect-ray \\\n--with git+https://github.com/prefecthq/prefect.git@redis-locking-serialization#subdirectory=src/integrations/prefect-dask -\n17:15:00.530 | INFO    | prefect.task_runner.ray - Creating a local Ray instance\n2025-05-11 17:15:02,493 INFO worker.py:1879 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265\n17:15:03.167 | INFO    | prefect.task_runner.ray - Using Ray cluster with 1 nodes.\n17:15:03.168 | INFO    | prefect.task_runner.ray - The Ray UI is available at 127.0.0.1:8265\n17:15:03.189 | INFO    | Flow run 'nano-shrimp' - Beginning flow run 'nano-shrimp' for flow 'simple-flow'\n17:15:03.192 | INFO    | Flow run 'nano-shrimp' - View at https://app.prefect.cloud/account/7df0ec92-8d72-4cfa-ab4f-43da476b3ab4/workspace/3831377d-1f9b-4e26-a17e-54bf408ec811/runs/flow-run/06820b0e-38fd-72c3-8000-1cbd4964782b\n(cached_task pid=270116) 17:15:04.265 | INFO    | prefect.task_runner.ray - Local Ray instance is already initialized. Using existing local instance.\n(cached_task pid=270116) 17:15:04.467 | INFO    | Task run 'cached_task-783' - executed task for 2\n(cached_task pid=270116) 17:15:04.489 | INFO    | Task run 'cached_task-783' - Finished in state Completed()\n(cached_task pid=270140) 17:15:04.531 | INFO    | Task run 'cached_task-b27' - Finished in state Cached(type=COMPLETED)\n17:15:04.613 | INFO    | Flow run 'nano-shrimp' - results: [2, 3, 3, 4, 4, 4]\n(cached_task pid=270122) 17:15:04.344 | INFO    | prefect.task_runner.ray - Local Ray instance is already initialized. Using existing local instance. [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(cached_task pid=270119) 17:15:04.504 | INFO    | Task run 'cached_task-ba8' - executed task for 3 [repeated 2x across cluster]\n(cached_task pid=270119) 17:15:04.524 | INFO    | Task run 'cached_task-ba8' - Finished in state Completed() [repeated 2x across cluster]\n(cached_task pid=270120) 17:15:04.609 | INFO    | Task run 'cached_task-362' - Finished in state Cached(type=COMPLETED) [repeated 2x across cluster]\n17:15:06.403 | INFO    | Flow run 'nano-shrimp' - Finished in state Completed()\n17:15:07.271 | INFO    | Flow run 'humongous-giraffe' - Beginning flow run 'humongous-giraffe' for flow 'simple-flow'\n17:15:07.272 | INFO    | Flow run 'humongous-giraffe' - View at https://app.prefect.cloud/account/7df0ec92-8d72-4cfa-ab4f-43da476b3ab4/workspace/3831377d-1f9b-4e26-a17e-54bf408ec811/runs/flow-run/06820b0e-ab99-7fb5-8000-73a329054e53\n17:15:07.273 | INFO    | prefect.task_runner.dask - Creating a new Dask cluster with `distributed.deploy.local.LocalCluster`\n17:15:07.701 | INFO    | prefect.task_runner.dask - The Dask dashboard is available at http://127.0.0.1:8787/status\n17:15:09.074 | ERROR   | Task run 'cached_task-910' - Error encountered when computing cache key - result will not be persisted.\nTraceback (most recent call last):\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/task_engine.py\", line 169, in compute_transaction_key\n    key = self.task.cache_policy.compute_key(\n        task_ctx=task_run_context,\n        inputs=self.parameters or {},\n        flow_parameters=parameters or {},\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 214, in compute_key\n    policy_key = policy.compute_key(\n        task_ctx=task_ctx,\n    ...<2 lines>...\n        **kwargs,\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 295, in compute_key\n    lines = inspect.getsource(task_ctx.task)\n  File \"/usr/lib64/python3.13/inspect.py\", line 1256, in getsource\n    lines, lnum = getsourcelines(object)\n                  ~~~~~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1238, in getsourcelines\n    lines, lnum = findsource(object)\n                  ~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1105, in findsource\n    raise OSError('lineno is out of bounds')\nOSError: lineno is out of bounds\n17:15:09.093 | INFO    | Task run 'cached_task-910' - executed task for 2\n17:15:09.095 | INFO    | Task run 'cached_task-910' - Finished in state Completed()\n17:15:09.099 | ERROR   | Task run 'cached_task-160' - Error encountered when computing cache key - result will not be persisted.\nTraceback (most recent call last):\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/task_engine.py\", line 169, in compute_transaction_key\n    key = self.task.cache_policy.compute_key(\n        task_ctx=task_run_context,\n        inputs=self.parameters or {},\n        flow_parameters=parameters or {},\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 214, in compute_key\n    policy_key = policy.compute_key(\n        task_ctx=task_ctx,\n    ...<2 lines>...\n        **kwargs,\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 295, in compute_key\n    lines = inspect.getsource(task_ctx.task)\n  File \"/usr/lib64/python3.13/inspect.py\", line 1256, in getsource\n    lines, lnum = getsourcelines(object)\n                  ~~~~~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1238, in getsourcelines\n    lines, lnum = findsource(object)\n                  ~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1105, in findsource\n    raise OSError('lineno is out of bounds')\nOSError: lineno is out of bounds\n17:15:09.117 | INFO    | Task run 'cached_task-160' - executed task for 3\n17:15:09.101 | ERROR   | Task run 'cached_task-6f5' - Error encountered when computing cache key - result will not be persisted.\nTraceback (most recent call last):\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/task_engine.py\", line 169, in compute_transaction_key\n    key = self.task.cache_policy.compute_key(\n        task_ctx=task_run_context,\n        inputs=self.parameters or {},\n        flow_parameters=parameters or {},\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 214, in compute_key\n    policy_key = policy.compute_key(\n        task_ctx=task_ctx,\n    ...<2 lines>...\n        **kwargs,\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 295, in compute_key\n    lines = inspect.getsource(task_ctx.task)\n  File \"/usr/lib64/python3.13/inspect.py\", line 1256, in getsource\n    lines, lnum = getsourcelines(object)\n                  ~~~~~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1238, in getsourcelines\n    lines, lnum = findsource(object)\n                  ~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1105, in findsource\n    raise OSError('lineno is out of bounds')\nOSError: lineno is out of bounds\n17:15:09.119 | INFO    | Task run 'cached_task-160' - Finished in state Completed()\n17:15:09.102 | ERROR   | Task run 'cached_task-1bb' - Error encountered when computing cache key - result will not be persisted.\nTraceback (most recent call last):\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/task_engine.py\", line 169, in compute_transaction_key\n    key = self.task.cache_policy.compute_key(\n        task_ctx=task_run_context,\n        inputs=self.parameters or {},\n        flow_parameters=parameters or {},\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 214, in compute_key\n    policy_key = policy.compute_key(\n        task_ctx=task_ctx,\n    ...<2 lines>...\n        **kwargs,\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 295, in compute_key\n    lines = inspect.getsource(task_ctx.task)\n  File \"/usr/lib64/python3.13/inspect.py\", line 1256, in getsource\n    lines, lnum = getsourcelines(object)\n                  ~~~~~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1238, in getsourcelines\n    lines, lnum = findsource(object)\n                  ~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1105, in findsource\n    raise OSError('lineno is out of bounds')\nOSError: lineno is out of bounds\n17:15:09.118 | INFO    | Task run 'cached_task-6f5' - executed task for 3\n17:15:09.120 | INFO    | Task run 'cached_task-1bb' - executed task for 1\n17:15:09.123 | INFO    | Task run 'cached_task-6f5' - Finished in state Completed()\n17:15:09.125 | INFO    | Task run 'cached_task-1bb' - Finished in state Completed()\n17:15:09.162 | ERROR   | Task run 'cached_task-4e5' - Error encountered when computing cache key - result will not be persisted.\nTraceback (most recent call last):\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/task_engine.py\", line 169, in compute_transaction_key\n    key = self.task.cache_policy.compute_key(\n        task_ctx=task_run_context,\n        inputs=self.parameters or {},\n        flow_parameters=parameters or {},\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 214, in compute_key\n    policy_key = policy.compute_key(\n        task_ctx=task_ctx,\n    ...<2 lines>...\n        **kwargs,\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 295, in compute_key\n    lines = inspect.getsource(task_ctx.task)\n  File \"/usr/lib64/python3.13/inspect.py\", line 1256, in getsource\n    lines, lnum = getsourcelines(object)\n                  ~~~~~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1238, in getsourcelines\n    lines, lnum = findsource(object)\n                  ~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1105, in findsource\n    raise OSError('lineno is out of bounds')\nOSError: lineno is out of bounds\n17:15:09.165 | ERROR   | Task run 'cached_task-b28' - Error encountered when computing cache key - result will not be persisted.\nTraceback (most recent call last):\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/task_engine.py\", line 169, in compute_transaction_key\n    key = self.task.cache_policy.compute_key(\n        task_ctx=task_run_context,\n        inputs=self.parameters or {},\n        flow_parameters=parameters or {},\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 214, in compute_key\n    policy_key = policy.compute_key(\n        task_ctx=task_ctx,\n    ...<2 lines>...\n        **kwargs,\n    )\n  File \"/home/bogdi/.cache/uv/archive-v0/HIMIIbtKGHyJ209LIq_FI/lib64/python3.13/site-packages/prefect/cache_policies.py\", line 295, in compute_key\n    lines = inspect.getsource(task_ctx.task)\n  File \"/usr/lib64/python3.13/inspect.py\", line 1256, in getsource\n    lines, lnum = getsourcelines(object)\n                  ~~~~~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1238, in getsourcelines\n    lines, lnum = findsource(object)\n                  ~~~~~~~~~~^^^^^^^^\n  File \"/usr/lib64/python3.13/inspect.py\", line 1105, in findsource\n    raise OSError('lineno is out of bounds')\nOSError: lineno is out of bounds\n17:15:09.181 | INFO    | Task run 'cached_task-4e5' - executed task for 3\n17:15:09.183 | INFO    | Task run 'cached_task-b28' - executed task for 2\n17:15:09.186 | INFO    | Task run 'cached_task-4e5' - Finished in state Completed()\n17:15:09.188 | INFO    | Task run 'cached_task-b28' - Finished in state Completed()\n17:15:09.208 | INFO    | Flow run 'humongous-giraffe' - results: [2, 3, 3, 4, 4, 4]\n17:15:09.665 | WARNING | EventsWorker - Still processing items: 2 items remaining...\n17:15:09.711 | WARNING | EventsWorker - Still processing items: 5 items remaining...\n17:15:09.985 | WARNING | EventsWorker - Still processing items: 6 items remaining...\n17:15:11.772 | INFO    | Flow run 'humongous-giraffe' - Finished in state Completed()\n17:15:12.652 | INFO    | Flow run 'calculating-locust' - Beginning flow run 'calculating-locust' for flow 'simple-flow'\n17:15:12.652 | INFO    | Flow run 'calculating-locust' - View at https://app.prefect.cloud/account/7df0ec92-8d72-4cfa-ab4f-43da476b3ab4/workspace/3831377d-1f9b-4e26-a17e-54bf408ec811/runs/flow-run/06820b0f-01fc-7544-8000-3220849dedec\n17:15:12.705 | INFO    | Task run 'cached_task-7ac' - executed task for 3\n17:15:12.706 | INFO    | Task run 'cached_task-21c' - executed task for 1\n17:15:12.708 | INFO    | Task run 'cached_task-09b' - executed task for 2\n17:15:12.714 | INFO    | Task run 'cached_task-09b' - Finished in state Completed()\n17:15:12.715 | INFO    | Task run 'cached_task-7ac' - Finished in state Completed()\n17:15:12.717 | INFO    | Task run 'cached_task-21c' - Finished in state Completed()\n17:15:12.810 | INFO    | Task run 'cached_task-172' - Finished in state Cached(type=COMPLETED)\n17:15:12.811 | INFO    | Task run 'cached_task-ea9' - Finished in state Cached(type=COMPLETED)\n17:15:12.815 | INFO    | Task run 'cached_task-36f' - Finished in state Cached(type=COMPLETED)\n17:15:12.816 | INFO    | Flow run 'calculating-locust' - results: [2, 3, 3, 4, 4, 4]\n17:15:13.047 | INFO    | Flow run 'calculating-locust' - Finished in state Completed()\n17:15:13.161 | WARNING | EventsWorker - Still processing items: 17 items remaining...\n```\n\nand, as the error states, the cache does not work as expected (all the task invocations were executed)\n\nwhat could this be about?"
      },
      {
        "user": "zzstoatzz",
        "body": "hmm interesting, I didn't encounter that. I don't think its the same issue, so if its persisting, can we create a new issue for it?"
      }
    ]
  },
  {
    "issue_number": 17864,
    "title": "Update documentation for accessing transaction data with tasks",
    "author": "willhcr",
    "state": "open",
    "created_at": "2025-04-19T04:25:09Z",
    "updated_at": "2025-05-12T17:23:52Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nAccessing transaction data from an on_rollback hook works, but not from an on_commit hook. Also it appears if an on_commit hook fails, the subsequent on_rollback hook also cannot access transaction data.\n\n```python\nfrom prefect import task\nfrom prefect.transactions import transaction\n\n\n@task\ndef begin(msg: str):\n    print(f'begin: {msg}')\n\n\n@task\ndef end(msg: str):\n    print(f'end: {msg}')\n\n\n@begin.on_commit\n@begin.on_rollback\ndef always_cleanup(txn):\n    job = txn.get('job')\n    end(job)\n\n\n@task\ndef txn_do_job():\n    \"\"\"This results in a ValueError(f\"Could not retrieve value for unknown key: {name}\")\"\"\"\n\n    with transaction() as txn:\n        job = '12345'\n        txn.set('job', job)\n        begin(job)\n        # do work\n\n\n@task\ndef txn_do_job_with_exc():\n    \"\"\"This runs begin() and end()\"\"\"\n\n    with transaction() as txn:\n        job = '12345'\n        txn.set('job', job)\n        begin(job)\n        # do work\n        raise Exception('Error')\n\n\nif __name__ == '__main__':\n    txn_do_job()\n    txn_do_job_with_exc()\n```\n\n### Version info\n\n```Text\nVersion:             3.3.5\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          db4b7a33\nBuilt:               Thu, Apr 17, 2025 09:25 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.10.6\nServer:\n  Database:          sqlite\n  SQLite version:    3.47.1\nIntegrations:\n  prefect-docker:    0.6.2\n  prefect-azure:     0.4.3\n```\n\n### Additional context\n\n```\nbegin: 12345\n21:22:50.787 | INFO    | Task run 'begin' - Finished in state Completed()\n21:22:50.792 | INFO    | Task run 'begin' - Running commit hook 'always_cleanup'\n21:22:50.792 | ERROR   | Task run 'begin' - An error was encountered while running commit hook 'always_cleanup'\n21:22:50.793 | ERROR   | Task run 'begin' - An error was encountered while committing transaction 'cf4cbc18f0fd6547e2311d85e77bd4e8'\nTraceback (most recent call last):\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 349, in commit\n    self.run_hook(hook, \"commit\")\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 401, in run_hook\n    raise exc\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 395, in run_hook\n    hook(self)\n  File \"/home/user/dev/proj/flows/tests/txn_sample.py\", line 18, in always_cleanup\n    job = txn.get('job')\n          ^^^^^^^^^^^^^^\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 162, in get\n    value = parent.get(name, default)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 167, in get\n    raise ValueError(f\"Could not retrieve value for unknown key: {name}\")\nValueError: Could not retrieve value for unknown key: job\n21:22:50.794 | INFO    | Task run 'begin' - Running rollback hook 'always_cleanup'\n21:22:50.795 | ERROR   | Task run 'begin' - An error was encountered while running rollback hook 'always_cleanup'\n21:22:50.796 | ERROR   | Task run 'begin' - An error was encountered while rolling back transaction 'cf4cbc18f0fd6547e2311d85e77bd4e8'\nTraceback (most recent call last):\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 349, in commit\n    self.run_hook(hook, \"commit\")\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 401, in run_hook\n    raise exc\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 395, in run_hook\n    hook(self)\n  File \"/home/user/dev/proj/flows/tests/txn_sample.py\", line 18, in always_cleanup\n    job = txn.get('job')\n          ^^^^^^^^^^^^^^\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 162, in get\n    value = parent.get(name, default)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 167, in get\n    raise ValueError(f\"Could not retrieve value for unknown key: {name}\")\nValueError: Could not retrieve value for unknown key: job\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 414, in rollback\n    self.run_hook(hook, \"rollback\")\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 401, in run_hook\n    raise exc\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 395, in run_hook\n    hook(self)\n  File \"/home/user/dev/proj/flows/tests/txn_sample.py\", line 18, in always_cleanup\n    job = txn.get('job')\n          ^^^^^^^^^^^^^^\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 162, in get\n    value = parent.get(name, default)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/dev/proj/.venv/lib/python3.12/site-packages/prefect/transactions.py\", line 167, in get\n    raise ValueError(f\"Could not retrieve value for unknown key: {name}\")\nValueError: Could not retrieve value for unknown key: job\n21:22:50.798 | INFO    | Task run 'txn_do_job' - Finished in state Completed()\n```",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Thanks for the issue @willhcr!\n\nI think you're seeing this because you're starting a transaction inside a task, which is different from the transaction passed to the `on_commit` and `on_rollback` hooks. Each task run has a transaction context, and that transaction is passed into the `on_commit` and `on_rollback` hooks.\n\nI think updating your example to use `get_transaction` to use the task's transaction context should allow this to work:\n```python\nfrom prefect import task\nfrom prefect.transactions import get_transaction, transaction\n\n\n@task\ndef begin(msg: str):\n    print(f\"begin: {msg}\")\n\n\n@task\ndef end(msg: str):\n    print(f\"end: {msg}\")\n\n\n@begin.on_commit\n@begin.on_rollback\ndef always_cleanup(txn):\n    job = txn.get(\"job\")\n    end(job)\n\n\n@task\ndef txn_do_job():\n    \"\"\"This results in a ValueError(f\"Could not retrieve value for unknown key: {name}\")\"\"\"\n\n    txn = get_transaction()\n    job = \"12345\"\n    txn.set(\"job\", job)\n    begin(job)\n    # do work\n\n\n@task\ndef txn_do_job_with_exc():\n    \"\"\"This runs begin() and end()\"\"\"\n\n    txn = get_transaction()\n    job = \"12345\"\n    txn.set(\"job\", job)\n    begin(job)\n    # do work\n    raise Exception(\"Error\")\n\n\nif __name__ == \"__main__\":\n    txn_do_job()\n    txn_do_job_with_exc()\n```\n\nIf there's another reason for opening a new transaction context that I'm missing, please let me know, and I can investigate further."
      },
      {
        "user": "willhcr",
        "body": "Thanks, that works.\n\nI was using `with transaction()` because that‚Äôs what the docs show: https://docs-3.prefect.io/v3/develop/transactions#write-your-first-transaction. It was surprising that on_rollback could access txn.get('job') but on_commit couldn‚Äôt. I expected them to behave the same.\n\nJust to clarify:\n\n1. Every task has an implicit transaction, but flows don‚Äôt?\n2. A with transaction() block inside a task creates a separate transaction that doesn‚Äôt share data with the implicit ones of sub-tasks?\n3. Hooks receive the task‚Äôs implicit transaction for on_commit(), but on_rollback() receives the nested one?\n"
      },
      {
        "user": "desertaxle",
        "body": "1. Yes, every task implicity runs within a transaction\n2. A `with transaction()` block creates a parent transaction for the task transaction. Child transactions can access the data set by parent transactions, but parent transactions can't access the data set by child transactions.\n3. Both `on_commit` and `on_rollback` receive the implicit task transaction.\n\nI'll update this issue to track adding more documentation for this behavior. If you have any other questions, let me know!"
      }
    ]
  },
  {
    "issue_number": 18026,
    "title": "dbt-prefect path handling for DbtCoreOperation can not handle spaces nor escaping in paths",
    "author": "dumkydewilde",
    "state": "open",
    "created_at": "2025-05-12T12:41:45Z",
    "updated_at": "2025-05-12T12:41:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nThe [`_append_dirs_to_commands` function](https://github.com/PrefectHQ/prefect/blob/7f25bbdf45fc81cca6dc23fb6a7377d436b70c83/src/integrations/prefect-dbt/prefect_dbt/cli/commands.py#L369) does not add the paths in quotes, meaning that if a path contains a space the generated shell script will fail. Unfortunately escaping with a `\\` also fails since `relative_path_to_current_platform()` replaces all backslashes.\n\nMinimal example:\n```\nresult = DbtCoreOperation(\n        commands=[\n            \"dbt debug\"\n            ],\n        project_dir=os.getenv(\"DBT_PROJECT_DIR\", \"/local user name with spaces/my_dbt_project\")\n    ).run()\n```\n\n### Version info\n\n```Text\n3.4.0\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 8566,
    "title": "Task Runs Concurrency slots not released when flow runs in Kubernetes are cancelled",
    "author": "masonmenges",
    "state": "open",
    "created_at": "2023-02-16T22:15:26Z",
    "updated_at": "2025-05-12T03:57:25Z",
    "labels": [
      "bug",
      "api",
      "concurrency"
    ],
    "body": "### First check\n\n- [X] I added a descriptive title to this issue.\n- [X] I used the GitHub search to find a similar issue and didn't find it.\n- [X] I searched the Prefect documentation for this issue.\n- [X] I checked that this issue is related to Prefect and not one of its dependencies.\n\n### Bug summary\n\nWhen running a prefect flow as a kubernetes job if the flow run is cancelled while tasks are in a running state the concurrency slots used by the tasks are not released though the tasks are in a cancelled state.\r\n\r\nThis is reproducible via the following steps with the code below with a flow run triggered as a kuberenetes job\r\n\r\n1. Create a concurrency limit in Prefect Cloud\r\n2. Add a task label to use that concurrency limit\r\n3. Trigger the flow and cancel the flow once the tasks that use that concurrency limit are in a ‚Äúrunning‚Äù state and populate the concurrency limit queue\r\n4. The tasks‚Äô state will change from ‚Äúrunning‚Äù to ‚Äúcanceled‚Äù, but will remain in the concurrency limit queue\r\n\r\n\r\nKubernetesJob Config:\r\n![k8sjobconfig](https://user-images.githubusercontent.com/76698667/219498682-a8356a48-5384-4863-be3d-f7d5a24f28cf.png)\r\n\r\npotentially related but separate issue:\r\nhttps://github.com/PrefectHQ/prefect/issues/7732\r\n\r\n\n\n### Reproduction\n\n```python3\nfrom prefect import flow, task, get_run_logger\r\nimport time\r\n\r\n@task(tags=[\"some_concurrency_tag\"])\r\ndef log_something(x):\r\n    logger = get_run_logger()\r\n    logger.info(f\"this is log number {x}\")\r\n    time.sleep(60)\r\n\r\n@flow\r\ndef smoke_test_flow():\r\n    for x in range(0, 100):\r\n        log_something.submit(x)\r\n\r\nif __name__ == \"__main__\":\r\n    smoke_test_flow()\n```\n\n\n### Error\n\n_No response_\n\n### Versions\n\n```Text\nruns from the base docker image prefecthq/prefect:2.8.0-python3.10\n```\n\n\n### Additional context\n\nCluster config, minus any sensitive information\r\n\r\n```\r\n{\r\n    \"location\": \"southcentralus\",\r\n    \"name\": \"prefect-k8s-dev\",\r\n    \"tags\": {\r\n        \"Application\": \"\",\r\n        \"BudgetAlert\": \"\",\r\n        \"BusinessGroup\": \"Data Analytics\",\r\n        \"CostCode\": \"\",\r\n        \"Priority\": \"\",\r\n        \"TechnicalContact\": \"\",\r\n        \"environment\": \"dev\",\r\n        \"prefect\": \"true\"\r\n    },\r\n    \"type\": \"Microsoft.ContainerService/ManagedClusters\",\r\n    \"properties\": {\r\n        \"provisioningState\": \"Succeeded\",\r\n        \"powerState\": {\r\n            \"code\": \"Running\"\r\n        },\r\n        \"kubernetesVersion\": \"1.24.9\",\r\n        \"dnsPrefix\": \"prefect-k8s-dev\",\r\n        \"agentPoolProfiles\": [\r\n            {\r\n                \"name\": \"default\",\r\n                \"count\": 2,\r\n                \"vmSize\": \"Standard_DS2_v2\",\r\n                \"osDiskSizeGB\": 50,\r\n                \"osDiskType\": \"Ephemeral\",\r\n                \"kubeletDiskType\": \"OS\",\r\n                \"maxPods\": 110,\r\n                \"type\": \"VirtualMachineScaleSets\",\r\n                \"enableAutoScaling\": false,\r\n                \"provisioningState\": \"Succeeded\",\r\n                \"powerState\": {\r\n                    \"code\": \"Running\"\r\n                },\r\n                \"orchestratorVersion\": \"1.24.9\",\r\n                \"enableNodePublicIP\": false,\r\n                \"mode\": \"System\",\r\n                \"enableEncryptionAtHost\": false,\r\n                \"enableUltraSSD\": false,\r\n                \"osType\": \"Linux\",\r\n                \"osSKU\": \"Ubuntu\",\r\n                \"nodeImageVersion\": \"AKSUbuntu-1804gen2containerd-2023.01.20\",\r\n                \"upgradeSettings\": {},\r\n                \"enableFIPS\": false\r\n            }\r\n        ]\r\n    }\r\n}\r\n```",
    "comments": [
      {
        "user": "Samreay",
        "body": "I've [posted to the slack group](https://prefect-community.slack.com/archives/CL09KU1K7/p1679885271937589) about this too, but this is not exclusive to Kubernetes, I have stock standard tasks being sent to a dask cluster, and when the parent flow Crashes for any reason, the slots aren't released. \r\n\r\nI wonder if this is the same issue as reported over in https://github.com/PrefectHQ/prefect/issues/5995"
      },
      {
        "user": "zanieb",
        "body": "Thanks @Samreay ‚Äî is this helped by https://github.com/PrefectHQ/prefect/pull/8408 ?"
      },
      {
        "user": "Samreay",
        "body": "Hmmm, if I've understood the merge, then potentially, though it would be good to have that cli endpoint invoked by prefect. I can see the reset method seems to be available in https://docs.prefect.io/api-ref/prefect/cli/concurrency_limit/, so I could add a flow that runs every few minutes which simply calls reset on all active limits Ive got defined.\r\n\r\nThat said, does that reset endpoint clear all slots, or just zombie slots? It looks like the slot override would end up being none, and so it would remove even valid still running tasks from the slot, right?"
      }
    ]
  },
  {
    "issue_number": 16800,
    "title": "Fix inconsistent deployment concurrency limit documentation",
    "author": "EmilRex",
    "state": "open",
    "created_at": "2025-01-21T20:39:41Z",
    "updated_at": "2025-05-10T22:35:18Z",
    "labels": [
      "docs"
    ],
    "body": "[This](https://docs.prefect.io/v3/deploy/infrastructure-concepts/prefect-yaml#concurrency-limit-fields) concurrency limit fields section of the docs suggests that the deployment YAML has the following structure:\n\n```yaml\ndeployments:\n  ...\n  concurrency_limit:\n    limit: <int>\n    collision_strategy: <str>\n```\n\nWhen in fact it has the following structure:\n\n```yaml\ndeployments:\n  ...\n  concurrency_limit: <int>\n  concurrency_options:\n    collision_strategy: <str>\n```\n\nThis also slightly conflicts with the documentation [here](https://docs.prefect.io/v3/deploy/index#concurrency-limiting).",
    "comments": [
      {
        "user": "ConnorWallace15",
        "body": "Hi @EmilRex, I agree that it suggests that limit is a field for concurrency limits. The description is also different from the other documentation. \n\n![Image](https://github.com/user-attachments/assets/faf60ab1-573a-4bcc-a081-551fe936056c)\n\nHow does this look?"
      }
    ]
  },
  {
    "issue_number": 15373,
    "title": "Allow formatting of flow parameter documentation strings",
    "author": "tjordahl",
    "state": "open",
    "created_at": "2024-09-13T14:37:40Z",
    "updated_at": "2025-05-09T19:39:08Z",
    "labels": [
      "enhancement",
      "ui"
    ],
    "body": "### Describe the current behavior\r\n\r\nCurrently the docstring of (e.g.) enum parameters is dumped on to the UI as one long string.  This makes it hard to self-document flow parameters for people running flows.\r\n\r\nExample:\r\n```python\r\nfrom pydantic import BaseModel\r\n\r\nfrom prefect import flow\r\n\r\n\r\nclass BadParams(BaseModel):\r\n    \"\"\"This is a realllllly long description that should be rendered as markdown\r\n\r\n\r\n    Attributes:\r\n        a: This is a really long description that should be rendered as markdown\r\n        b: This is a really long description that should be rendered as markdown\r\n        c: This is a really long description that should be rendered as markdown\r\n        x: This is a really long description that should be rendered as markdown\r\n        y: This is a really long description that should be rendered as markdown\r\n        z: This is a really long description that should be rendered as markdown\r\n    \"\"\"\r\n\r\n    a: int\r\n    b: int\r\n    c: int\r\n    x: int\r\n    y: int\r\n    z: int\r\n\r\n\r\n@flow\r\ndef bad_params_render(params: BadParams):\r\n    pass\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    bad_params_render.serve()\r\n ```\r\n![image](https://github.com/user-attachments/assets/9e552735-7bed-47a0-94a8-e01db964d3b2)\r\n\r\n\r\n### Describe the proposed behavior\r\n\r\nSupport some sort of formatting, Markdown would be the obvious choice, to allow flow developers to nicely format this text.\r\n\r\n### Example Use\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\nPrefect slack thread from 2024-09-13:\r\nhttps://prefect-community.slack.com/archives/C0192RWGJQH/p1726235752005299\r\n",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @tjordahl - perhaps [this](https://gist.github.com/zzstoatzz/551ee34cb0306635995483e1689b4758) example is roughly what you're looking to do?\r\n\r\nif so, would that close this issue for you?"
      },
      {
        "user": "tjordahl",
        "body": "Putting markdown in the enum docstring did not seem to work.  I still get everything run together.\r\nHere is how my code is structure, with TargetIdentifier used in multiple flows:\r\n```\r\n@flow\r\ndef myflow1(\r\n    target_identifier: TargetIdentifier,\r\n    ...\r\n)\r\n@flow\r\ndef myflow2(\r\n    target_identifier: TargetIdentifier,\r\n    ...\r\n)\r\n...\r\n\r\nclass TargetIdentifier(Enum):\r\n    \"\"\"\r\n    The list of target identifiers.\r\n\r\n    *Environments*:\r\n\r\n    - DEV: Development environment\r\n    - STAGE: Stage environment\r\n    \"\"\"\r\n    DEV = \"Dev\"\r\n    STAGE = \"Stage\"\r\n"
      },
      {
        "user": "serinamarie",
        "body": "@tjordahl Using `<br>` at the end of each argument worked for me, although I ended up just using Pydantic Field descriptions"
      }
    ]
  },
  {
    "issue_number": 18015,
    "title": "Ability to store the prefect database to a custom PostgreSQL schema",
    "author": "francbartoli",
    "state": "open",
    "created_at": "2025-05-09T17:44:13Z",
    "updated_at": "2025-05-09T17:44:13Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nAt the moment there is no way to set a custom `search_path` in the connection string of the database. This is an hard constraint in certain circumstances and environments where databases are not allowed to be containerized and the database is unique and shared across different applications. \n\n### Describe the proposed behavior\n\nI'd like to have a setting in the [database settings](https://docs-3.prefect.io/v3/manage/server/index#database-settings) `PREFECT_API_DATABASE_SCHEMA='foo'`\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 16979,
    "title": "Expand filtering options for related resource pairs in /api/events/filter",
    "author": "ogenstad",
    "state": "closed",
    "created_at": "2025-02-05T12:33:09Z",
    "updated_at": "2025-05-08T13:41:17Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nWhen using /api/events/filter it's possible to match against related resources using the [EventFilter](https://github.com/PrefectHQ/prefect/blob/3.1.15/src/prefect/server/events/filters.py#L565) object in JSON format. The [EventRelatedFilter](https://github.com/PrefectHQ/prefect/blob/3.1.15/src/prefect/server/events/filters.py#L333C7-L333C25) object allows us to filter the events based on a set of keys and values contained within an related resource where all fields are expected to match a related resource.\n\n### Describe the proposed behavior\n\nIt would be great if there was an additional option to search and match against multiple pairs at once.\n\nThe [filter](https://github.com/PrefectHQ/prefect/blob/3.1.15/src/prefect/server/events/filters.py#L333-L348), could then be expanded with an additional attribute looking something like this:\n\n```python\nclass EventRelatedFilter(EventDataFilter):\n    label_pairs: Optional[List[ResourceSpecification]] = Field(\n        None, description=\"Only include events for related resources with these label pairs\"\n    )\n```\n\n\n\n### Example Use\n\nPosting to the new API might look like this:\n\n```json\n{\n  \"filter\": {\n      \"related\": {\n          \"label_pairs\": [\n              {\n                  \"prefect.resource.role\": \"department\",\n                  \"prefect.resource.id\": \"develop\"\n              },\n              {\n                \"prefect.resource.role\": \"environment\",\n                \"prefect.resource.id\": \"test\"\n            }\n          ]\n      }\n  }\n}\n```\n\nThe above would return all events that had a combination of the related resources.\n\n### Additional context\n\nNote this is somewhat related to #16978 and a similar operator selection as described there would also be useful.",
    "comments": [
      {
        "user": "ogenstad",
        "body": "Is this something that you'd be open to support? If acceptable I can look at adding a PR to it. As we needed it on our side I created a wrapper around the Prefect API in our environment (https://github.com/opsmill/infrahub/pull/5720) that adds a custom API endpoint which does what I need. I would prefer to not have to do this though."
      }
    ]
  },
  {
    "issue_number": 16978,
    "title": "Expand filtering options for resources_in_roles in /api/events/filter",
    "author": "ogenstad",
    "state": "closed",
    "created_at": "2025-02-05T09:56:59Z",
    "updated_at": "2025-05-08T13:41:17Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nCurrently when listing events you can filter them by using a [EventFilter](https://github.com/PrefectHQ/prefect/blob/3.1.15/src/prefect/server/events/filters.py#L565) object in JSON format. This allows you to search for events based on the event name, resource, related resource.\n\nHowever the engine is quite opinionated with regards to how the filter is then used to query the database. For example when using related nodes by specifying an [EventRelatedFilter](https://github.com/PrefectHQ/prefect/blob/3.1.15/src/prefect/server/events/filters.py#L333C7-L333C25) we can see that we can specify a list of key-pairs of `prefect.resource.role` and `prefect.resource.id` in the `resources_in_roles` attribute. The resulting query is then using `sa.or_` as seen [here](https://github.com/PrefectHQ/prefect/blob/3.1.15/src/prefect/server/events/filters.py#L362-L373):\n\n```python\n        if self.resources_in_roles:\n            filters.append(\n                sa.or_(\n                    *(\n                        sa.and_(\n                            db.EventResource.resource_id == resource_id,\n                            db.EventResource.resource_role == role,\n                        )\n                        for resource_id, role in self.resources_in_roles\n                    )\n                )\n            )\n```\n\n### Describe the proposed behavior\n\nFor my use-case I'd want this to be an `sa.and_` instead, and while I don't expect a breaking change here it would be nice to be able to control this behaviour. Perhaps by introducing a `resources_in_roles_operation` which would default a value indicating `sa.or_` but that you could then set to `sa.and_`.\n\nThe goal would be to use the `resources_in_roles` as a constraint to further filter the events to a smaller set instead of including additional events for each pair of role and resource_id.\n\n\n\n### Example Use\n\n_No response_\n\n### Additional context\n\nIt looks like the /api/automation/filter supports this concept of operator in one location.",
    "comments": []
  },
  {
    "issue_number": 18001,
    "title": "Mismatch between frontend and backend API Default Limit Settings",
    "author": "eric-martial",
    "state": "open",
    "created_at": "2025-05-07T17:51:16Z",
    "updated_at": "2025-05-07T18:43:08Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### Bug summary\n\n## Summary\nAfter migrating from Prefect v3.3.4 to v3.4.0, there's a discrepancy in the API default limit values between frontend and backend. The frontend consistently sends a limit of 200 when making requests, but the backend seems to be expecting 100, resulting in 422 Unprocessable Entity errors when viewing flow run logs.\nThe issue specifically impacts log retrieval. \nSetting the environment variable doesn't override the frontend request parameter.\n\n## Environment\n- Prefect Version: 3.4.0\n- Environment Setup: Using the environment variable `PREFECT_SERVER_API_DEFAULT_LIMIT` to attempt to modify the default limit\n\n## Steps to Reproduce\n1. Set up Prefect 3.4.0 server\n2. Create a simple flow using the Prefect Python SDK:\n   ```python\n   from prefect import flow, task\n   \n   @task(name=\"test-task\", log_prints=True)\n   def my_task():\n       for i in range(250):\n           print(f\"Log line {i}\")\n       return \"done\"\n   \n   @flow(name=\"test-flow\")\n   def my_flow():\n       return my_task()\n   \n   if __name__ == \"__main__\":\n       my_flow()\n   ```\n3. Run the flow from the command line: `python some_flow.py`\n4. Open the Prefect UI and navigate to the Runs page\n5. Click on the flow run to view details\n6. Open browser developer tools (F12) and go to the Network tab\n7. Click on the \"Logs\" tab in the UI\n8. Observe the 422 errors in the browser console for requests to `api/logs/filter` endpoint\n\n## Actual Behavior\nThe browser makes POST requests to `api/logs/filter` with a limit value of 200, resulting in a 422 Unprocessable Entity error with message 'Invalid limit: must be less than or equal to 100.'. \nSetting the `PREFECT_SERVER_API_DEFAULT_LIMIT` environment variable to 100 does not change this behavior - frontend requests continue to use 200.\n\nThe logs are actually being generated correctly (confirmed by checking the database `log` table which contains entries), but they cannot be retrieved through the UI due to this API validation error. \n\nAdditionally, the UI displays a misleading message \"This run didn't generate logs\" when in fact logs exist but cannot be retrieved due to the API limit mismatch. This misleading message further obscures the actual problem.\n\n## Console Error (Partial)\n```\n{\n  \"message\": \"Request failed with status code 422\",\n  \"name\": \"AxiosError\",\n  \"stack\": <removed>\n  \"config\": {\n    \"transitional\": {\n      \"silentJSONParsing\": true,\n      \"forcedJSONParsing\": true,\n      \"clarifyTimeoutError\": false\n    },\n    \"adapter\": [\n      \"xhr\",\n      \"http\",\n      \"fetch\"\n    ],\n    \"transformRequest\": [\n      null\n    ],\n    \"transformResponse\": [\n      null\n    ],\n    \"timeout\": 0,\n    \"xsrfCookieName\": \"XSRF-TOKEN\",\n    \"xsrfHeaderName\": \"X-XSRF-TOKEN\",\n    \"maxContentLength\": -1,\n    \"maxBodyLength\": -1,\n    \"env\": {},\n    \"headers\": {\n      \"Accept\": \"application/json, text/plain, */*\",\n      \"Content-Type\": \"application/json\",\n      \"X-PREFECT-UI\": \"true\"\n    },\n    \"baseURL\": \"http://<api_host>/api\",\n    \"method\": \"post\",\n    \"url\": \"/logs/filter\",\n    \"data\": \"{\\\"logs\\\":{\\\"level\\\":{\\\"ge_\\\":0},\\\"flow_run_id\\\":{\\\"any_\\\":[\\\"<flow_run_id>\\\"]}},\\\"sort\\\":\\\"TIMESTAMP_ASC\\\",\\\"offset\\\":0,\\\"limit\\\":200}\",\n    \"allowAbsoluteUrls\": true\n  },\n  \"code\": \"ERR_BAD_REQUEST\",\n  \"status\": 422\n}\n```\n\n## Possible Causes\n1. **Configuration Loading Issue**: The default limit value is not being properly loaded or propagated from backend to frontend.\n\n2. **Environment Variable Inconsistency**: There appear to be two different environment variables in the codebase:\n   - `PREFECT_API_DEFAULT_LIMIT` (found in multiple files)\n   - `PREFECT_SERVER_API_DEFAULT_LIMIT` (shown in the documentation)\n\n3. **Frontend/Backend Synchronization**: The frontend JavaScript code might have a hardcoded default limit of 200, while the backend API validates against a different value (potentially 100).\n\n4. **Settings Propagation Failure**: The configured limit value isn't being properly sent to the client-side code when the UI is loaded, causing a mismatch in expectations.\n\n5. **Inaccurate Error Handling**: When log retrieval fails due to API validation errors, the UI incorrectly displays \"This run didn't generate logs\" instead of indicating that logs exist but could not be retrieved.\n\n\n### Version info\n\n```Text\nVersion:             3.4.0\nAPI version:         0.8.4\nPython version:      3.13.3\nGit commit:          c80e4442\nBuilt:               Fri, May 02, 2025 08:02 PM\nOS/Arch:             linux/x86_64\nProfile:             \nServer type:         server\nPydantic version:    2.11.4\nIntegrations:\n  prefect-github:    0.3.1\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @eric-martial - I've reproduced this, this seems like a bug in the [prefect-ui-library](https://github.com/PrefectHQ/prefect-ui-library) . we'll take a look"
      }
    ]
  },
  {
    "issue_number": 12922,
    "title": "Make the use of `BuildKit` possible in the image building step.",
    "author": "FeryET",
    "state": "open",
    "created_at": "2023-05-09T10:27:43Z",
    "updated_at": "2025-05-07T18:37:15Z",
    "labels": [],
    "body": "Hi.\r\n\r\nDocker buildkit helps me to  build images much faster on my local PC, wanted to see if there is anyway to use the buildkit in the image building phase in prefect_docker?\r\n\r\nI can help with the code if needed.",
    "comments": [
      {
        "user": "ngriffiths13",
        "body": "I second this. It would be great if there was a way to use buildkit."
      },
      {
        "user": "desertaxle",
        "body": "Based on https://github.com/docker/docker-py/issues/2230 it looks like `docker-py` (the Docker library that `prefect-docker` uses) does not support BuildKit.  As a result, supporting BuildKit in `prefect-docker` will be difficult. One possible solution is to use another library like [pyton-on-whales](https://github.com/gabrieldemarmiesse/python-on-whales) that supports BuildKit, but migrating will require rewriting large portions of `prefect-docker`.\r\n"
      },
      {
        "user": "desertaxle",
        "body": "It looks like it is possible to set the `buildkit` flag at the daemon level: https://github.com/PrefectHQ/prefect/issues/10331#issuecomment-1662282377. "
      }
    ]
  },
  {
    "issue_number": 3058,
    "title": "Kubernetes Cluster AutoScaler can result in failed Flow Runs",
    "author": "szelenka",
    "state": "closed",
    "created_at": "2020-07-30T15:48:13Z",
    "updated_at": "2025-05-07T17:44:48Z",
    "labels": [],
    "body": "## Description\r\nThis only happens on K8 clusters with [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler) to dynamically provision the K8 nodes based on current workload demands in the cluster. In my case, I've observed this on AWS EKS with manual NodeGroups.\r\n\r\nThe Cluster Autoscaler will evaluate the utilization of each Node in the K8 cluster to determine if it can scale down the number of Nodes. By default this happens every 10 minutes (configurable by `--scale-down-unneeded-time flag`), and is determined by a number of factors, the main one being the sum of cpu and memory requests of all pods running on this node is smaller than 50% of the node's allocatable (configurable by `--scale-down-utilization-threshold`). There's also a back-off period after a Node is first added to the cluster of 10 minutes (configurable by `--scale-down-delay-after-add flag`).\r\n\r\nThis functionality of K8 assumes that all Pods created by a controller object (deployment, replica set, **job**, stateful set etc) are ephemeral, and can be relocated to a different Node, with zero impact to the functionality of those Pods. The Prefect Kubernetes Agent triggers Flow Runs via K8 Jobs (effectively declaring them as ephemeral), which spawn the Pods to perform the actual execution. \r\n\r\nThe problem is intermittent, and only comes into play under the following situation:\r\n\r\n1. A new, long-running (i.e. >21 minutes), Prefect Flow Run is scheduled, and picked up by a Prefect Agent running in K8 w/Cluster Autoscaler\r\n2. Prefect Agent creates a K8 Job, but there are no available K8 Nodes to run this Job within 1-5 minutes\r\n3. Cluster Autoscaler scales up the Nodes in K8. The Node it scales up has at least 2x cpu/memory allocatable than what the single Prefect Flow Run would consume.\r\n4. The Prefect Flow Run is assigned to the new Node Cluster Autoscaler just started\r\n5. Capacity frees up on the other Nodes in the K8 cluster, such that no other Pods are scheduled on the new Node the Cluster Autoscaler just started (in step 3)\r\n6. After 20 minutes, the Cluster Autoscaler determines that the Node from step 3 is a candidate to scale down because it meets the [criteria](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node) (i.e. sum of cpu/memory of all pods is smaller than 50%, the pods were created by a controller object, etc.)\r\n7. The Pod (which is executing the Flow Run) is evicted from this Node and rescheduled to a different Node in K8, and the Cluster Autoscaler Node is removed from K8\r\n\r\nNow the fun part:\r\n\r\n1. Because K8 evicted the Pod, Prefect Cloud will attempt to keep track of the original Pod (which no longer exists)\r\n2. When the evicted Pod starts again, it will begin the Flow Run execution again ... because this was a long-running Flow Run, it'll take it >21 minutes to complete\r\n3. Eventually Prefect will encounter `No heartbeat detected from the remote task; marking the run as failed.` for the initial Pod it created, which was since relocated by the Cluster AutoScaler.\r\n4. You can observe the newly started Pod is actually reporting logs to Prefect Cloud, so it's curious why it detected no heartbeat from that new process. I'm guessing it's looking for some metadata to identify a heartbeat from the original Pod IP.\r\n5. Because no heartbeat was detected from that original Pod, the Lazarus process will attempt to reschedule the Flow Run .. but the Flow Run was already restarted by K8, it's just in a different Pod now\r\n6. This confusion will result in the Flow Run being marked as \"Failed\"\r\n\r\nWhile less likely, it's feasible for a Flow Run to be scheduled on a Node that has been up for awhile, but is a candidate to be scaled down in the next 1 minute. Then after ~1 minute of executing the Flow Run on this Node, capacity on other Nodes could free up, such that the Cluster AutoScaler can determine to relocate all the running Pods, which would result in the same type of failure. In this case, the failure would happen to any Flow Run, regardless of execution time.\r\n\r\n## Expected Behavior\r\n\r\nThe root of the problem is that the Cluster AutoScaler is moving the Pod created by the Job the Prefect Agent generated for this Flow Run, before that Flow Run had time to complete. It seems the scheduler may not be capable of handling situations where the Flow Run is restarted by processes outside of it's control.\r\n\r\nA simple way to prevent the Cluster AutoScaler from attempting to evict a Pod from a Node, would be to add this annotation to it's manifest:\r\n\r\n```\r\n\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"\r\n```\r\n\r\nIf the AutoScaler see's this annotation on a Pod, it will not consider the Node for scaling down. In theory you could add this to a `job_spec_file`, but it'd be exhausting to do this for every Flow you want to execute in a K8 w/Cluster AutoScaler.\r\n\r\n## Reproduction\r\nThere's a public flow that simply sleeps for a few minutes:\r\nhttps://hub.docker.com/repository/docker/szelenka/long-running-flow\r\n\r\nIn AWS, I have an EKS cluster with a simple NodeGroup of t3.2xlarge, where the Prefect Agent spawns Jobs with 2 vCPU and 2G RAM. Initially, we only have the Prefect Agent running on a single Node in K8.\r\n\r\nThrough Prefect Cloud, we start 3 Flow Runs to run at 5 minutes each. This fills up the capacity of the single Node in K8. \r\n\r\nThen we start another Flow Run, with a run time of 25 minutes. This causes the Cluster AutoScaler to scale up a Node in K8, and assign this Job to that new Node.\r\n\r\nBecause it takes longer than 20 minutes to complete, and the other Flow Runs have completed, it will be evicted and relocated to the other Node that capacity freed up on, and trigger the above failure.\r\n\r\n\r\n## Environment\r\n```json\r\n{\r\n  \"config_overrides\": {\r\n    \"cloud\": {\r\n      \"use_local_secrets\": true\r\n    },\r\n    \"context\": {\r\n      \"secrets\": false\r\n    }\r\n  },\r\n  \"env_vars\": [],\r\n  \"system_information\": {\r\n    \"platform\": \"macOS-10.15.6-x86_64-i386-64bit\",\r\n    \"prefect_version\": \"0.12.6\",\r\n    \"python_version\": \"3.8.1\"\r\n  }\r\n}\r\n```\r\n",
    "comments": [
      {
        "user": "joshmeek",
        "body": "@szelenka Thanks for a well written issue! I think the best bet here would be to add the eviction policy option setting to the agent so you could say something along the lines of:\r\n\r\n```\r\nprefect agent install/start kubernetes --safe-to-evict=False\r\n```\r\n\r\nAnd then all created jobs will have the annotation\r\n\r\n```\r\n\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"\r\n```"
      },
      {
        "user": "szelenka",
        "body": "I agree, that's probably the best path forward.\r\n\r\nSince the user would know if they're using Cluster AutoScaler in the cluster they're attempting to register the agent, it makes sense to have a flag when they generate the YAML to install the agent.\r\n\r\nI'm guessing by using that CLI argument, it would add some parameter to the EnvVar of the Agent, similar to the `JOB_*` ones used for the CPU/MEM limit/requests. Then when the agent spawns the Pod, it'd check for that EnvVar to add that annotation to the mainfest?\r\n\r\nThe Prefect Agent itself can be evicted without problems (that I've observed). However, one thing I have not tested is what happens to Pods that are \"Completed\" but aren't removed from the namespace (because Resource-Manager isn't being used). Would having that annotation on a \"Completed\" Pod prevent the Cluster AutoScaler from evicting it? I'll try to run a test later this week or next to figure out (unless someone else knows the answer)."
      },
      {
        "user": "joshmeek",
        "body": "> I'm guessing by using that CLI argument, it would add some parameter to the EnvVar of the Agent, similar to the JOB_* ones used for the CPU/MEM limit/requests. Then when the agent spawns the Pod, it'd check for that EnvVar to add that annotation to the mainfest?\r\n\r\nYep that's exactly how it should be done! For the Completed pods it would be good to test because I think k8s should be deallocating resources for the pods after completion"
      }
    ]
  },
  {
    "issue_number": 17814,
    "title": "Add a --dry-run switch or equivalent to prefect deploy",
    "author": "tjordahl",
    "state": "open",
    "created_at": "2025-04-14T18:57:46Z",
    "updated_at": "2025-05-07T17:22:00Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nMy continuous integration/deployment fails when there is a problem in the prefect.yaml file and I would like something to be able to validate all the entries so this can be done before the changes are merged and attempted to be deployed. \n\nThe current `prefect deploy --all` will deploy all flows up until it encounters an error, then stop.  This leaves my Depoyments in a state of half new, half old deployments.\n\n\n\n\n### Describe the proposed behavior\n\nIn addition to validating the YAML schema for `prefect.yaml`, this should catch items such as:\n- Invalid parameter values\n- Incorrect endpoint specifications (missing file/function)\n- Other validation\n\nIn short, everything but the `POST /deployment` operation.  A `--dry-run` switch to `prefect deploy` would be ideal\n\n### Example Use\n\n```bash\nprefect deploy --all --dry-run\n```\n\n\n### Additional context\n\nSlack thread: https://prefect-community.slack.com/archives/CL09KU1K7/p1744654952403849\nRelevant issue: https://github.com/PrefectHQ/prefect/issues/16996",
    "comments": [
      {
        "user": "tjordahl",
        "body": "Just wanted to ping on this - any plans to implement soon(ish)?\nWe keep getting caught with bad deployment config which leaves our server with partially updated deployments as everything above any error deploys successfully and anything below doesn't get run."
      },
      {
        "user": "cicdw",
        "body": "We are interested in this feature but I don't have a timeline to share on when we will be able to pick it up. We would also accept a PR if you have the energy to do that!"
      }
    ]
  },
  {
    "issue_number": 17865,
    "title": "Variable name validation restrictions - underscores only?",
    "author": "lelouvincx",
    "state": "closed",
    "created_at": "2025-04-19T08:32:09Z",
    "updated_at": "2025-05-07T02:11:37Z",
    "labels": [],
    "body": "## Background\n\nI'm currently working on upgrading from the deprecated `String` blocks to the newer `Variable` objects as part of preparation for the June 2025 deprecation deadline (don't remember exactly, but I have an impression that I've read somewhere). However, I've hit a roadblock with variable naming conventions.\n\n## The Issue\n\nWhen trying to register a block with a variable name that contains hyphens, I'm receiving this validation error:\n\n```\nFailed to register GCP secrets for ci: 1 validation error for `VariableCreate` name.\nValue error: Variable names must only contain lowercase letters, numbers, and underscores.\n```\n\nThe specific variable name that's failing is `\"gcp-cred\"` because it contains a hyphen (`-`).\n\n## Question\n\n**Is there a specific technical reason or design principle behind restricting Variable names to only lowercase letters, numbers, and underscores?** \n\nThis seems more restrictive than many other systems that commonly allow hyphens in identifiers. Since I need to migrate several blocks that use hyphenated naming conventions, understanding the reasoning would help me plan my migration approach better.\n\nAlso, if this is an intentional restriction, it might be helpful to have this more explicitly documented in the migration guides for those of us moving from String blocks (which presumably allowed hyphens) to Variables.\n\nThanks for any insights!",
    "comments": [
      {
        "user": "lelouvincx",
        "body": "Hello @aaazzam and @cicdw , is there any context on this?"
      },
      {
        "user": "zzstoatzz",
        "body": "hi @lelouvincx - thanks for the issue! there was a historical reason for doing this, but at this point I don't think there's a strong reason remaining. we will allow hyphenated variable names"
      },
      {
        "user": "lelouvincx",
        "body": "Thanks @zzstoatzz "
      }
    ]
  },
  {
    "issue_number": 15650,
    "title": ".serve throws prefect.deployments.runner.DeploymentApplyError ",
    "author": "aaazzam",
    "state": "open",
    "created_at": "2024-10-10T17:14:23Z",
    "updated_at": "2025-05-07T00:11:07Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen attempting to create a deployment for a simple Prefect flow, I'm encountering a 422 Unprocessable Entity error. The error message suggests that there are extra inputs that are not permitted, specifically concurrency_limit and concurrency_options.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a dummy flow and run .serve\r\n```python\r\nfrom prefect import flow \r\n\r\n@flow\r\ndef hey(plumbus: str, n: int) -> None:\r\n    print(f\"Hey {plumbus}!\")\r\n\r\nif __name__ == \"__main__\":\r\n    hey.serve()\r\n```\r\n\r\n## Output \r\n\r\n```\r\nprefect.deployments.runner.DeploymentApplyError: Error while applying deployment\r\n\r\nClient error '422 Unprocessable Entity' for url 'http://127.0.0.1:4200/api/deployments/'\r\n\r\nResponse: {\r\n    'exception_message': 'Invalid request received.',\r\n    'exception_detail': [\r\n        {\r\n            'type': 'extra_forbidden',\r\n            'loc': ['body', 'concurrency_limit'],\r\n            'msg': 'Extra inputs are not permitted',\r\n            'input': None\r\n        },\r\n        {\r\n            'type': 'extra_forbidden',\r\n            'loc': ['body', 'concurrency_options'],\r\n            'msg': 'Extra inputs are not permitted',\r\n            'input': None\r\n        }\r\n    ],\r\n    'request_body': {\r\n        'name': 'test',\r\n        'flow_id': '590575ab-3843-417e-b779-8ab183f834f1',\r\n        'paused': False,\r\n        'schedules': [],\r\n        'concurrency_limit': None,\r\n        'concurrency_options': None,\r\n        'enforce_parameter_schema': True,\r\n        'parameter_openapi_schema': {\r\n            'title': 'Parameters',\r\n            'type': 'object',\r\n            'properties': {}\r\n        },\r\n        'parameters': {},\r\n        'tags': [],\r\n        'work_queue_name': None,\r\n        'storage_document_id': None,\r\n        'infrastructure_document_id': None,\r\n        'description': 'None',\r\n        'path': '.',\r\n        'version': '10126c1587d22530d17f49083d596641',\r\n        'entrypoint': 'test.py:test',\r\n        'job_variables': {}\r\n    }\r\n}\r\n\r\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\r\n\r\n```\n\n### Version info (`prefect version` output)\n\n```Text\nVersion:             3.0.8\r\nAPI version:         0.8.4\r\nPython version:      3.9.19\r\nGit commit:          0894bad4\r\nBuilt:               Thu, Oct 10, 2024 10:17 AM\r\nOS/Arch:             darwin/arm64\r\nProfile:             default\r\nServer type:         server\r\nPydantic version:    2.7.4\n```\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "ericmeadows",
        "body": "See [this issue](https://github.com/PrefectHQ/prefect/issues/16245)"
      }
    ]
  },
  {
    "issue_number": 15954,
    "title": "Invalid Schema when creating a deployment from flow",
    "author": "VictorRosarioIdener",
    "state": "open",
    "created_at": "2024-11-08T15:31:48Z",
    "updated_at": "2025-05-07T00:11:01Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\r\n\r\nWhen retrieving code from a bucket to then deploy it in my prefect instance with the following code:\r\n\r\n```python\r\n    # Retrieve the flow you want to run\r\n    result_flow: Flow = await ml_pipeline.from_source(\r\n                            source=code_from_bucket, \r\n                            entrypoint=f\"{os.path.basename(__file__)}:ml_pipeline\"\r\n                        )\r\n\r\n    # Create the flow deployment\r\n    deploy_id = await result_flow.deploy(name=\"my-deployment\",\r\n        work_pool_name=\"work-pool-docker\",\r\n        image=\"prefecthq/prefect:3.0.4-python3.11\",\r\n        build=False,\r\n        push=False,\r\n        job_variables=job_variables,\r\n        cron=\"*/15 * * * *\"\r\n    )\r\n    Error shows up with the following message:\r\n    Traceback (most recent call last):\r\n  File \"C:\\Users\\Idener\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\prefect\\deployments\\runner.py\", line 311, in apply\r\n    deployment_id = await client.create_deployment(**create_payload)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Idener\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\prefect\\client\\orchestration.py\", line 1727, in create_deployment\r\n    response = await self._client.post(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Idener\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py\", line 1905, in post\r\n    return await self.request(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Idener\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py\", line 1585, in request\r\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Idener\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\prefect\\client\\base.py\", line 361, in send\r\n    response.raise_for_status()\r\n  File \"C:\\Users\\Idener\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\prefect\\client\\base.py\", line 174, in raise_for_status\r\n    raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__\r\nprefect.exceptions.PrefectHTTPStatusError: Client error '422 Unprocessable Entity' for url 'http://127.0.0.1:4200/api/deployments/'\r\nResponse: {'detail': \"Invalid schema: 'true' is not of type 'object', 'boolean'\"}\r\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\r\n```\r\n\r\n### Version info\r\n\r\n```Text\r\nVersion: 3.0.4\r\nAPI version: 3.0.4\r\nPython version: 3.11.0\r\nBuilt: Friday, Nov 8, 2024 16:30\r\nOS/Arch: Windows 11\r\nprofile: local\r\nserver type: self-hosted\r\nIntegrations:\r\n   boto3\r\n   prefect-aws\r\n```",
    "comments": [
      {
        "user": "ericmeadows",
        "body": "See [this issue](https://github.com/PrefectHQ/prefect/issues/16245)"
      }
    ]
  },
  {
    "issue_number": 17988,
    "title": "flow crashes due to FileNotFoundError from uv at import",
    "author": "rcash",
    "state": "closed",
    "created_at": "2025-05-05T22:38:33Z",
    "updated_at": "2025-05-06T18:14:51Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nuv raises a [FileNotFoundError](https://github.com/astral-sh/uv/blob/3218e364ae97d8bf1fe2007af7ec243e05b44da2/python/uv/_find_uv.py#L36) in new versions of Prefect at import when trying to kick a flow run off - when importing [bundles.py](https://github.com/PrefectHQ/prefect/blob/3.3.5/src/prefect/_experimental/bundles.py#L25) or functionality related to it. this is seen because the path `uv` looks for its binary at does not appear to match the Python installation used (Anaconda), in this case at least.\n\nstack trace\n```\n19:27:46.471 | ERROR   | Flow run 'phi5-iyaar' - Unexpected exception encountered when trying to load flow\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/engine.py\", line 112, in <module>\n    flow: \"Flow[..., Any]\" = load_flow(flow_run)\n                             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/flow_engine.py\", line 140, in load_flow\n    flow = run_coro_as_sync(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 207, in run_coro_as_sync\n    return call.result()\n           ^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 329, in result\n    return self.future.result(timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 192, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 405, in _run_async\n    result = await coro\n             ^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 188, in coroutine_wrapper\n    return await task\n           ^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/client/utilities.py\", line 69, in wrapper\n    return await func(client, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/flows.py\", line 2518, in load_flow_from_flow_run\n    from prefect.deployments.steps.core import StepExecutionError, run_steps\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/deployments/steps/__init__.py\", line 2, in <module>\n    from .pull import (\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/deployments/steps/pull.py\", line 12, in <module>\n    from prefect.runner.storage import BlockStorageAdapter, GitRepository, RemoteStorage\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/runner/__init__.py\", line 1, in <module>\n    from .runner import Runner\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/runner/runner.py\", line 72, in <module>\n    from prefect._experimental.bundles import (\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/_experimental/bundles.py\", line 28, in <module>\n    uv_path = uv.find_uv_bin()\n              ^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/uv/_find_uv.py\", line 36, in find_uv_bin\n    raise FileNotFoundError(path)\nFileNotFoundError: /home/ray/.local/bin/uv\n19:27:46.483 | ERROR   | prefect.engine - Execution of flow run 'f43cc11f-5ba4-459d-91b0-7e02270af306' exited with unexpected exception\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/engine.py\", line 57, in handle_engine_signals\n    yield\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/engine.py\", line 112, in <module>\n    flow: \"Flow[..., Any]\" = load_flow(flow_run)\n                             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/flow_engine.py\", line 140, in load_flow\n    flow = run_coro_as_sync(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 207, in run_coro_as_sync\n    return call.result()\n           ^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 329, in result\n    return self.future.result(timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 192, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/_internal/concurrency/calls.py\", line 405, in _run_async\n    result = await coro\n             ^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/utilities/asyncutils.py\", line 188, in coroutine_wrapper\n    return await task\n           ^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/client/utilities.py\", line 69, in wrapper\n    return await func(client, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/flows.py\", line 2518, in load_flow_from_flow_run\n    from prefect.deployments.steps.core import StepExecutionError, run_steps\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/deployments/steps/__init__.py\", line 2, in <module>\n    from .pull import (\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/deployments/steps/pull.py\", line 12, in <module>\n    from prefect.runner.storage import BlockStorageAdapter, GitRepository, RemoteStorage\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/runner/__init__.py\", line 1, in <module>\n    from .runner import Runner\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/runner/runner.py\", line 72, in <module>\n    from prefect._experimental.bundles import (\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/prefect/_experimental/bundles.py\", line 28, in <module>\n    uv_path = uv.find_uv_bin()\n              ^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/uv/_find_uv.py\", line 36, in find_uv_bin\n    raise FileNotFoundError(path)\nFileNotFoundError: /home/ray/.local/bin/uv\n```\n\n### Version info\n\n```Text\n3.3.5\n```\n\n### Additional context\n\nthis seems like it could be an upstream issue due to how `uv` resolves the path to itself, but also makes me wonder if this could have any broader impact on `conda` users of Prefect",
    "comments": [
      {
        "user": "rcash",
        "body": "The base image used is `anyscale/ray:2.44.1-slim-py311-cpu` and prefect is installed into it using a requirements file during the build process"
      },
      {
        "user": "zzstoatzz",
        "body": "hi @rcash - do you have reproduction steps for this? the simplest case below doesn't seem to reproduce\n\n```bash\n¬ª docker run --rm anyscale/ray:2.44.1-slim-py311-cpu /bin/bash -c \"python -m pip install prefect==3.3.5 --quiet && python -c 'import uv; print(uv.find_uv_bin())'\"\n/home/ray/anaconda3/bin/uv\n```\n\nwe could probably just catch the `FileNotFoundError` though"
      },
      {
        "user": "rcash",
        "body": "thanks for hopping in @zzstoatzz ! similarly, I have been unable to repro. it does seem like wrapping the uv binary lookup in a function as part of #17878 isolated the issue though for versions > 3.3.5. still scratching my head at how `.local` was selected for use over `anaconda3`..."
      }
    ]
  },
  {
    "issue_number": 17958,
    "title": "Flow run heartbeats accumulating until the end of a run",
    "author": "chrisguidry",
    "state": "closed",
    "created_at": "2025-05-01T17:59:34Z",
    "updated_at": "2025-05-06T17:21:01Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWe've had some reports from customers of a situation where automations they have set up on `prefect.flow-run.heartbeat` events are misbehaving.  In the several cases we've seen, we notice a pattern like this:\n\n![Image](https://github.com/user-attachments/assets/8465d828-c556-4b9c-b758-66125a0ab143)\n\nWhere there's a long gap of hours between when a heartbeat _occurs_ and when it is _received_ by a Prefect server (Prefect Cloud in the pictured case).  In these cases,  we'll see that the events appear to all have `occurred` times correctly spaced out through the full duration of the flow run, but they all arrive (`received`) at the end very close to the `prefect.flow-run.Completed` event.\n\n![Image](https://github.com/user-attachments/assets/c5ea7b43-ea06-4d18-86a5-0ebdb1fca86e)\n\n\n\n### Version info\n\n```Text\nWe have observed this in Prefect 3.2 and 3.3 clients, don't have specific version details.\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "cicdw",
        "body": "[This is the function](https://github.com/PrefectHQ/prefect/blob/main/src/prefect/runner/runner.py#L1152-L1156) responsible for emitting flow run heartbeats within the `Runner` which is scheduled using `ansyncio.create_task` + our `critical_service_loop` utility here: https://github.com/PrefectHQ/prefect/blob/5d6e0c61baba4513aa9744c942297d645fc4db2c/src/prefect/runner/runner.py#L1660-L1666\n\nSharing in case this clues anyone into a possible cause."
      },
      {
        "user": "chrisguidry",
        "body": "We had another possibly related report from another customer where they were using `emit_event` outside the context of a flow run (perhaps in a long-running server process?).  In that case, they were also seeing some odd behavior where an event (with the same ID, etc) was emitted multiple times, hours or even days apart.  I wonder if there's a relationship here with using `emit_event` (and the `EventWorker`) outside of a flow run context?  I think the `Runner` would be outside of a flow run context too?\n\nOne idea I had for the heartbeats is that it might be cleaner to use the `EventsClient` directly, open one up for the life of the flow run, and use `checkpoint_every=1` to make sure the events are sent off immediately."
      }
    ]
  },
  {
    "issue_number": 17956,
    "title": "ModuleNotFoundError with Kubernetes Dask task runner",
    "author": "TWeatherston",
    "state": "open",
    "created_at": "2025-05-01T12:36:44Z",
    "updated_at": "2025-05-06T16:18:55Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nI'm honestly not too sure if this is a bug in Prefect or some issue with our setup but, after spending a couple of days trying to resolve it, I'm hoping you can point me in the right direction!\n\nWe are upgrading our Prefect from 2 -> 3. We have a number of flows that are running in Kubernetes using the Dask task runner. These flows work fine in Prefect 2 but always fail in 3 due to a `ModuleNotFoundError`. Non Dask flows all complete successfully.\n\nA cut down example of code that is failing\n```python\n@flow(\n    name=\"Daily Collection\",\n    task_runner=DaskTaskRunner(\n        cluster_class=\"dask_kubernetes.operator.KubeCluster\",\n        adapt_kwargs={\"minimum\": 15, \"maximum\": 200},\n    ),\n)\ndef daily_collection(\n    collection_date: Optional[datetime.date] = None,\n    feed_names: Optional[list[str]] = None,\n):\n    api = APIClient.load(\"default\")\n\n    feed_names = feed_names or get_list_of_feeds(api)\n    file_name_futures = [get_file_names.submit(api, feed, collection_date) for feed in feed_names]\n    file_names = [f.result() for f in file_name_futures]\n```\n\nThe flows will fail with the following stacktrace:\n```\nEngine execution exited with unexpected exception\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/distributed/scheduler.py\", line 4852, in update_graph\n    graph = deserialize(graph_header, graph_frames).data\n    ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/distributed/protocol/serialize.py\", line 452, in deserialize\n    return loads(header, frames)\n      ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/distributed/protocol/serialize.py\", line 111, in pickle_loads\n    return pickle.loads(pik, buffers=buffers)\n      ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/distributed/protocol/pickle.py\", line 92, in loads\n    return pickle.loads(x)\n    ^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'collection_flow'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 1527, in run_flow\n    ret_val = run_flow_sync(**kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 1372, in run_flow_sync\n    return engine.state if return_type == \"state\" else engine.result()\n                                                       ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 350, in result\n    raise self._raised\n  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 763, in run_context\n    yield self\n  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 1370, in run_flow_sync\n    engine.call_flow_fn()\n  File \"/usr/local/lib/python3.11/site-packages/prefect/flow_engine.py\", line 783, in call_flow_fn\n    result = call_with_parameters(self.flow.fn, self.parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/prefect/atheon_prefect/flows/collection/collection_flow.py\", line 186, in daily_collection\n    result = future.result()\n             ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/prefect_dask/task_runners.py\", line 135, in result\n    future_result = self._wrapped_future.result(timeout=timeout)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/distributed/client.py\", line 402, in result\n    return self.client.sync(self._result, callback_timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/distributed/client.py\", line 410, in _result\n    raise exc.with_traceback(tb)\nRuntimeError: Error during deserialization of the task graph. This frequently\noccurs if the Scheduler and Client have different environments.\nFor more information, see\nhttps://docs.dask.org/en/stable/deployment-considerations.html#consistent-software-environments\n```\n\nIt runs perfectly fine if I run locally, using a local Dask cluster so I think it could be something to do with the Docker image but, as mentioned, this runs fine using essentially the same Docker image with Prefect 2.\n\n\n### Version info\n\n```Text\nVersion:             3.3.7\nAPI version:         0.8.4\nPython version:      3.11.11\nGit commit:          8f86aaee\nBuilt:               Mon, Apr 28, 2025 03:04 PM\nOS/Arch:             darwin/arm64\nProfile:             staging\nServer type:         cloud\nPydantic version:    2.11.2\nIntegrations:\n  prefect-gcp:       0.6.5\n  prefect-dask:      0.3.5\n  prefect-dbt:       0.6.6\n  prefect-github:    0.3.1\n  prefect-snowflake: 0.28.2\n  prefect-aws:       0.5.9\n  prefect-shell:     0.3.1\n```\n\n### Additional context\n\nLooking at the various containers, I can see that the Prefect container, Dask worker and Dask scheduler all have the same file structure and PYTHONPATH (checked this as it's usually the issue for this kind of error)\n",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Hey @TWeatherston! There are changes in the `DaskTaskRunner` from 2.x to 3.x, but nothing is coming to mind as an obvious reason why this is failing. \n\nYou could try submitting directly to Dask without Prefect and see if the issue persists. If the issue goes away, then you could try using the `PrefectDaskClient` from `prefect-dask` to see if that works. That would help determine which layer is introducing the error."
      },
      {
        "user": "TWeatherston",
        "body": "> Hey [@TWeatherston](https://github.com/TWeatherston)! There are changes in the `DaskTaskRunner` from 2.x to 3.x, but nothing is coming to mind as an obvious reason why this is failing.\n> \n> You could try submitting directly to Dask without Prefect and see if the issue persists. If the issue goes away, then you could try using the `PrefectDaskClient` from `prefect-dask` to see if that works. That would help determine which layer is introducing the error.\n\n@desertaxle Thanks so much for getting back to me! I've managed to open a shell in the container of a long running flow and have ran all of these commands successfully:\n\n```python\ncluster = create_kubernetes_cluster()\napi = APIClient.load(\"default\")\n\nclient = dask.distributed.Client(cluster)\nx = client.submit(get_list_of_feeds.fn, api)\nx.result()\n\n# ----------------\n\nclient = PrefectDaskClient(cluster)\nx = client.submit(get_list_of_feeds.fn, api)\nx.result()\n\n# -------------\n\nwith DaskTaskRunner(cluster=cluster) as runner:\n    x = runner.submit(get_list_of_feeds, {\"api\":api})\n    x.result()\n```\n\nThey all return the expected results without any errors üòÖ "
      },
      {
        "user": "OliverKleinBST",
        "body": "Have you checked on that part ? Scheduler and Client have different environments. I had a similar problem which was happening due to different versions of cloudpickle"
      }
    ]
  },
  {
    "issue_number": 17798,
    "title": "JSON input in UI corrupts parameters",
    "author": "j-tr",
    "state": "closed",
    "created_at": "2025-04-10T09:15:29Z",
    "updated_at": "2025-05-06T14:16:27Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nSwitching dict parameters to JSON parameter inputs view serializes the value and adds a \"__prefect_kind\": \"json\" field.\n\nThis means the structure of the parameters that arrive at the flow run upon execution is different depending on whether the flow was started with the form or json view. When using pydantic models, this results in a validation error when submitting flows from the json view.\n\n```\nfrom prefect import flow\n\n@flow\ndef main(\n    my_param: dict[str, str] = {\n        \"foo\": \"bar\",\n    },\n):\n    print(f\"Hello! {my_param}\")\n\nif __name__ == \"__main__\":\n    main.serve(name=\"mre\")\n```\n\nForm view\n![Image](https://github.com/user-attachments/assets/91a1329a-581f-4fa5-a534-5bfea80d5fdd)\n\nOutput: `Hello! {'foo': 'bar'}`\n\nJson view\n![Image](https://github.com/user-attachments/assets/94c0e3da-0706-41e2-91e6-c0eb428bfb83)\nOutput: `Hello! {'value': '{\\n  \"foo\": \"bar\"\\n}', '__prefect_kind': 'json'}`\n\n### Version info\n\n```Text\nVersion:             3.3.3\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          4100d4ea\nBuilt:               Sat, Apr 05, 2025 01:44 AM\nOS/Arch:             linux/x86_64\nServer type:         cloud\nPydantic version:    2.11.3\n```\n\n### Additional context\n\npossibly related to https://github.com/PrefectHQ/prefect/issues/17782",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @j-tr - thanks for the issue and MRE! I can reproduce this - we'll take a look"
      },
      {
        "user": "znicholasbrown",
        "body": "Hi @j-tr - thanks for the report; this'll be fixed in the next release!"
      }
    ]
  },
  {
    "issue_number": 17782,
    "title": "[Bug] Prefect UI Schedule Parameter Override Corrupts Parameters,but  API Works Correctly",
    "author": "zyn71",
    "state": "closed",
    "created_at": "2025-04-09T03:48:30Z",
    "updated_at": "2025-05-06T14:15:56Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n**Describe the bug**\n\nConfiguring parameter overrides for a Schedule via the Prefect UI results in corrupted parameters being passed to the flow run. Specifically, the target parameter field (e.g., `dataset_ids`) is incorrectly replaced with the *entire* flow parameter set, serialized as a JSON string within a `{\"value\": \"...\", \"__prefect_kind\": \"json\"}` structure which will cause flow run error.\n\nCrucially, configuring the same parameter override directly via the Prefect API (e.g., using `httpx` or the Python client) works as expected.\n\n**To Reproduce**\n\n1.  Define a flow with parameters (e.g., `dataset_ids: List[int]`).\n2.  Create a Deployment for the flow.\n3.  **Using the Prefect UI**: Create a Schedule for the deployment and configure it to override the `dataset_ids` parameter (e.g., set value to `[35]`).\n4.  Trigger the schedule or wait for it to run.\n5.  Observe the flow run logs: The `dataset_ids` parameter received by the flow is the corrupted JSON structure containing the full parameter set, not `[35]`. This often leads to `TypeError: unhashable type: 'dict'`.\n\n**Expected behavior**\n\nWhen using the UI to override `dataset_ids` to `[35]`, the flow run should receive `dataset_ids=[35]`, consistent with the behavior when using the API.\n\n**Actual behavior (UI only)**\n\nThe flow run receives `dataset_ids` as `{\"value\": \"{ \"dataset_ids\": [1, 2]}\", \"__prefect_kind\": \"json\"}` (example structure).\n\n\n### Version info\n\n```Text\nVersion:             3.2.7\nAPI version:         0.8.4\nPython version:      3.10.12\nGit commit:          d4d9001e\nBuilt:               Fri, Feb 21, 2025 7:39 PM\nOS/Arch:             linux/x86_64\nProfile:             ephemeral\nServer type:         ephemeral\nPydantic version:    2.10.6\nServer:\n  Database:          sqlite\n  SQLite version:    3.37.2\nIntegrations:\n  prefect-docker:    0.6.2\n  prefect-kubernetes: 0.5.3\n  prefect-gitlab:    0.3.1\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @zyn71 - thanks for the issue! can you make an MRE for this? (I can probably interpret your steps correctly, its just easier if there's no ambiguity)"
      },
      {
        "user": "znicholasbrown",
        "body": "Hi @zyn71 - thanks for the report, this'll be fixed in the next release!"
      }
    ]
  },
  {
    "issue_number": 17892,
    "title": "Support `staticmethod` and `classmethod` flow entrypoints",
    "author": "vyagubov",
    "state": "closed",
    "created_at": "2025-04-23T13:56:45Z",
    "updated_at": "2025-05-06T13:49:56Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Bug summary\n\nI'm using Prefect with static methods defined inside classes and deploying with source code from GitLab using an explicit entrypoint.\n\nMy flow is defined like this:\n\n```python\nclass ClassName:\n\n    @staticmethod\n    @flow\n    def run() -> None:\n        ...\n```\nI try to use:\n\n```python\nentrypoint = \"path/flow.py:ClassName.run\"\n```\nHowever, Prefect raises an error saying it couldn't find the flow.\n\nExpected Behavior:\n\nSince @flow supports @staticmethod based on the source code here:\nhttps://github.com/PrefectHQ/prefect/blob/093237bfa0681dfcb1baeaee9b96da9803b6a879/src/prefect/flows.py#L281 \nI expect that specifying ClassName.run in the entrypoint should work.\n\nRoot Cause:\nIn the same file, later in the Flow class, the entrypoint is hardcoded as:\nhttps://github.com/PrefectHQ/prefect/blob/093237bfa0681dfcb1baeaee9b96da9803b6a879/src/prefect/flows.py#L405\n\nThis ignores the class context (ClassName) and causes the flow to be inaccessible via the expected entrypoint path.\n\n\nCurrent Workaround:\nTo make this work, I need to add a top-level alias:\n\n```python\nclass ClassName:\n\n    @staticmethod\n    @flow\n    def run() -> None:\n        ...\n\nrun = ClassName.run  # workaround for entrypoint to resolve\n\n# entrypoint = \"path/flow.py:run\"\n```\nSuggested Fix:\n\nRespect the class context in the entrypoint, or allow users to specify ClassName.run explicitly and resolve it correctly during loading.\n\n### Version info\n\n```Text\nthe last version of Prefect which is available in github.\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "desertaxle",
        "body": "Thanks for opening this, @vyagubov! Adding support for `@staticmethod` and `@classmethod` flow entrypoints seems doable. The bulk of the changes should be in `load_flow_from_entrypoint` and `safe_load_flow_from_entrypoint` in **src/prefect/flows.py**.\n\nAre you up for giving it a shot? I'm happy to help if you run into any issues.\n"
      },
      {
        "user": "vyagubov",
        "body": "I will give it a try. Expect PR on Tuesday (on Monday I am on vacation)."
      },
      {
        "user": "vyagubov",
        "body": "For `load_flow_from_entrypoint` it was straightforward ‚Äî I only needed to adjust `import_object`, and it already works in my Orchestrator project:\nhttps://github.com/vyagubov/prefect/commit/0e74a487085cb42ba4a40b73e8648d1312e9de6a (I will also add some tests later).\n\nHowever, for `safe_load_flow_from_entrypoint` it's more complicated.  \nI had to extend `_entrypoint_definition_and_source` to support `ast.ClassDef`.  \nIn the case of a class, I also need to pass `ClassName.method_name`, not just `ClassName`, back to the caller.  \nCurrently, I'm doing it via `func_def.name = object_path`, but it feels like a hack:\nhttps://github.com/vyagubov/prefect/commit/7e84285fbe96039bbccb9782262af3a916ae6a6\n\nI see a few options:\n- Do not change `safe_load_flow_from_entrypoint` for now, since it is just a fallback inside `load_flow_from_entrypoint`.\n- Keep the workaround with `func_def.name = object_path`.\n- Refactor `_entrypoint_definition_and_source` to return an additional value like `Tuple[Union[ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef], str, List[str]]`, and update `is_entrypoint_async` and `load_flow_arguments_from_entrypoint` accordingly.\n\n@desertaxle \n**What would you suggest?**\nPersonally, I lean toward the third option (adding a separate return value) ‚Äî it feels cleaner and would avoid hacks inside AST nodes. However, it would require a broader refactor (affecting safe_load_flow_from_entrypoint, is_entrypoint_async, and load_flow_arguments_from_entrypoint), and initially I planned to avoid large refactoring here. \n"
      }
    ]
  },
  {
    "issue_number": 16353,
    "title": "Toggle 12 hour vs. 24 hour times in the UI ",
    "author": "robfreedy",
    "state": "open",
    "created_at": "2024-12-12T02:02:07Z",
    "updated_at": "2025-05-06T11:35:36Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nCurrently in Prefect Cloud UI, dates and times are displayed using the 12 hour AM/PM times. \n\n### Describe the proposed behavior\n\nIn the user profile settings, allow a toggle to specify whether you want to the time to be shown based on a 12 hour AM/PM clock vs. 24 hour clock i.e. if a flow ran at 5:00 PM being able to toggle whether that shows as 5:00 PM or 17:00. \n\n### Example Use\n\nProviding a toggle on the My Profile page to turn this on or off\r\n<img width=\"1512\" alt=\"image\" src=\"https://github.com/user-attachments/assets/23dc3114-7252-475d-aaff-517d23c8d6e7\" />\r\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "andersob",
        "body": "Duplicate of #15078 "
      }
    ]
  },
  {
    "issue_number": 17989,
    "title": "Extra inputs are not permitted: loc': ['body', 'version_info']",
    "author": "Gunnar-Stunnar",
    "state": "closed",
    "created_at": "2025-05-06T02:32:32Z",
    "updated_at": "2025-05-06T03:19:11Z",
    "labels": [],
    "body": "### Bug summary\n\ntoday I started experience this deployment error in my CICD pipeline\n\ncurrent deployment code:\n```python\ngit_repo = GitRepository(\n        url=\"...\",\n        credentials=GitHubCredentials.load(\"el-document-code\")\n    )\n\n    flow.from_source(\n            source=git_repo,\n            entrypoint=\"src/flows/<flow entrypoint>\",\n        ).deploy(\n        name=\"inference\",\n        image=docker image,\n        work_pool_name=\"...\",\n        work_queue_name=\"...\",\n        push=False\n    )\n``` \n\nCurrent stack trace:\n```\nTraceback (most recent call last):\n  File \"/runner/_work/.../.../deployFlow.py\", line 97, in <module>\n    main()\n  File \"/runner/_work/.../.../deployFlow.py\", line 94, in main\n    deploy_flows(worker_pools)\n  File \"/runner/_work/.../.../deployFlow.py\", line 65, in deploy_flows\n    flow.from_source(\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/utilities/asyncutils.py\", line [351](https://github.com/Stunn-Inc/AzureFR-validator/actions/runs/14849701852/job/41692531275#step:6:352), in coroutine_wrapper\n    return run_coro_as_sync(ctx_call())\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/utilities/asyncutils.py\", line 207, in run_coro_as_sync\n    return call.result()\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 329, in result\n    return self.future.result(timeout=timeout)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 192, in result\n    return self.__get_result()\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/concurrent/futures/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/_internal/concurrency/calls.py\", line 405, in _run_async\n    result = await coro\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/utilities/asyncutils.py\", line 188, in coroutine_wrapper\n    return await task\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/utilities/asyncutils.py\", line 341, in ctx_call\n    result = await async_fn(*args, **kwargs)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/flows.py\", line 1554, in deploy\n    deployment_ids = await deploy_coro\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/utilities/asyncutils.py\", line 341, in ctx_call\n    result = await async_fn(*args, **kwargs)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/deployments/runner.py\", line 1281, in deploy\n    await deployment.apply(image=image_ref, work_pool_name=work_pool_name)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/utilities/asyncutils.py\", line 341, in ctx_call\n    result = await async_fn(*args, **kwargs)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/deployments/runner.py\", line 505, in apply\n    return await self._create(work_pool_name, image, version_info)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/prefect/deployments/runner.py\", line 394, in _create\n    raise DeploymentApplyError(\nprefect.deployments.runner.DeploymentApplyError: Error while applying deployment: Client error '422 Unprocessable Entity' for url '***/deployments/'\nResponse: {'exception_message': 'Invalid request received.', 'exception_detail': [{'type': 'extra_forbidden', 'loc': ['body', 'version_info'], 'msg': 'Extra inputs are not permitted', 'input': {'type': 'vcs:github', 'version': 'fe998bb8', 'commit_sha': 'fe998bb8b2de0ba05ca122f8975a8af6ad7b0d9b', 'message': 'Merge pull request #97 from Stunn-Inc/NoAzure_prefect', 'branch': 'master', 'repository': 'Stunn-Inc/AzureFR-validator', 'url': 'https://github.com/Stunn-Inc/AzureFR-validator/tree/fe998bb8b2de0ba05ca122f8975a8af6ad7b0d9b'}}], 'request_body': {'name': 'RA-documentai-workPool-dev', 'flow_id': '7b1a27d1-52eb-493b-9da2-3ec8559d2dca', 'paused': False, 'schedules': [], 'concurrency_limit': None, 'concurrency_options': None, 'enforce_parameter_schema': True, 'parameter_openapi_schema': {'title': 'Parameters', 'type': 'object', 'properties': {'fileReference': {'position': 0, 'title': 'fileReference'}}, 'required': ['fileReference']}, 'parameters': {}, 'tags': [], 'labels': {}, 'pull_steps': [{'prefect.deployments\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\n```\n\n### Version info\n\n```Text\nprefect version: 3.4.0\nprefect-github version: 0.3.1\nprefect-server version: 3.2.13\n```\n\n### Additional context\n\nJust start receiving this error today, it was working and fully deploying the last few days ",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @Gunnar-Stunnar it looks like you're using a new client with an old server, which [we don't recommend](https://docs.prefect.io/v3/manage/server/index) because you can run into schema mismatches like this where the client is trying to use something new that the server doesn't know about"
      },
      {
        "user": "Gunnar-Stunnar",
        "body": "Awesome! just upgraded everything, it is all working now. Thanks!"
      },
      {
        "user": "zzstoatzz",
        "body": "great! thanks for following up"
      }
    ]
  },
  {
    "issue_number": 17982,
    "title": "UI Deployment Schedule Enable/Disable toggle resets Parameters Overrides",
    "author": "gabr1elt",
    "state": "closed",
    "created_at": "2025-05-05T17:43:44Z",
    "updated_at": "2025-05-06T00:00:41Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nParameters overrides in schedules are reset (dissapear) after enabling/disabling the schedule.\n\n### To reproduce\n\nSet a parameter override on a schedule (from API or UI)\nEnable or disable the Schedule.\nParameter override dissapears\n\n### Version info\n\n```Text\ndocker.io/prefecthq/prefect:3.4.0-python3.10\n```\n\n### Additional context\n\nMaybe related to #17782 ",
    "comments": [
      {
        "user": "znicholasbrown",
        "body": "Thanks for the report @gabr1elt, this'll be fixed in the next release"
      }
    ]
  },
  {
    "issue_number": 17883,
    "title": "Snowflake-Credentials block from Prefect-Snowflake does not work with encrypted private keys due to bad regex",
    "author": "konwiddak",
    "state": "closed",
    "created_at": "2025-04-22T13:26:37Z",
    "updated_at": "2025-05-05T20:48:22Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nSnowflake-Credentials can be set up using private-public keypair. Ideally the key would be encrypted with a private key passphrase.\n\n![Image](https://github.com/user-attachments/assets/f7438382-10dc-4b19-b0b4-d6318740c4b7)\n\nAn encrypted private key pem file is structured (this is not a real key!):\n\n```\n-----BEGIN RSA PRIVATE KEY-----\nProc-Type: 4,ENCRYPTED\nDEK-Info: DES-EDE3-CBC,ABCDEF304983345\n\neeiuf9ehfiuehf989879heifhaiefa78fheahfe8a7hf8e7hf8ea7hf8ea7\nef9hea9afhe98y395874938749q38ya9r8h3f938hf938hf93qhf93ahhhh\n30ru83q9r83q98r3q98rh39h8r3h838383rh39rh398qh938rh3q9hrjjjj\n-----END RSA PRIVATE KEY-----\n```\n\nUnlike unencrypted keys, it contains the keywords Proc-Type and DEK-Info.\n\nprefect/src/integrations/prefect-snowflake/prefect-snowflake/credentials.py\n\nLine 226:\n\n```\ncomposed_private_key = self._compose_pem(private_key)\n```\n\n_compose_pem does a regex on the PEM file using the following pattern to split the key into parts:\n\n```\n_SIMPLE_PEM_CERTIFICATE_REGEX = \"^(-+[^-]+-+)([^-]+)(-+[^-]+-+)\"\n```\n\n Unfortunately this regex identifies the \"-\" in the \"Proc-Type\" keyword as the start of the last line of the PEM file and the - in DEK-Info as the end of the file. This means that it parses the key to be:\n\n```\n-----BEGIN RSA PRIVATE KEY-----\\nProc\\n-Type: 4,ENCRYPTED\\r\\nDEK-\n```\n\nThis is now an invalid key and it throws an error:\n\n```\n ValueError('Could not deserialize key data. The data may be in an incorrect format, the provided password may be incorrect, it may be encrypted with an unsupported algorithm, or it may be an unsupported key type (e.g. EC curves with explicit parameters).', [<OpenSSLError(code=503841036, lib=60, reason=524556, reason_text=unsupported)>])\n```\n\nIf I patch line 226 to be:\n\n```\ncomposed_private_key = private_key\n```\n\nThen it loads and decrypts the key correctly, just without protection of malformed pem files.\n\nThe process needs to be adjusted to reassemble the pem file correctly.\n\n### Version info\n\n```Text\nVersion:             3.3.5\nAPI version:         0.8.4\nPython version:      3.12.10\nGit commit:          db4b7a33\nBuilt:               Thu, Apr 17, 2025 09:25 PM\nOS/Arch:             win32/AMD64\nProfile:             prod\nServer type:         server\nPydantic version:    2.11.3\nIntegrations:\n  prefect-dask:      0.3.4\n  prefect-snowflake: 0.28.2\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "konwiddak",
        "body": "A bit more info around this issue, there's a bit of subtlety to ensure the pem is in the right format.\n\nIn credentials.py\nThis alternative pattern works as intended to split the PEM correctly:\n```\n^(-+[^-]+-+)([\\s\\S]+?)(--+[^-]+-+)\n```\nHowever this still breaks because the _compose_pem then regex splits on \"\\s+\" which strips whitespace and reforming with new lines. However this breaks up this part of the pem key due to the space after the colon:\n\n```\nProc-Type: 4,ENCRYPTED\nDEK-Info: DES-EDE3-CBC,ABCDEF304983345\n```\nIt's possible to adjust the regex to ignore the space after the colon, however this removes the new line between DEK-Info and the key. This also breaks the pem format.\n\nThis key will not work:\n```\n-----BEGIN RSA PRIVATE KEY-----\nProc-Type: 4,ENCRYPTED\nDEK-Info: DES-EDE3-CBC,ABCDEF304983345\neeiuf9ehfiuehf989879heifhaiefa78fheahfe8a7hf8e7hf8ea7hf8ea7\nef9hea9afhe98y395874938749q38ya9r8h3f938hf938hf93qhf93ahhhh\n30ru83q9r83q98r3q98rh39h8r3h838383rh39rh398qh938rh3q9hrjjjj\n-----END RSA PRIVATE KEY-----\n```\nThis key is ok because it contains the new line:\n```\n-----BEGIN RSA PRIVATE KEY-----\nProc-Type: 4,ENCRYPTED\nDEK-Info: DES-EDE3-CBC,ABCDEF304983345\n\neeiuf9ehfiuehf989879heifhaiefa78fheahfe8a7hf8e7hf8ea7hf8ea7\nef9hea9afhe98y395874938749q38ya9r8h3f938hf938hf93qhf93ahhhh\n30ru83q9r83q98r3q98rh39h8r3h838383rh39rh398qh938rh3q9hrjjjj\n-----END RSA PRIVATE KEY-----\n```\n\nI'm presuming the malformed pem comes from the Block storing new lines incorrectly (?) perhaps the code should not manipulate the pem if it's being directly read from a file?\n\n\n"
      },
      {
        "user": "zzstoatzz",
        "body": "the fix (linked above) is now released in `prefect-snowflake==0.28.4` - let us know if you're still seeing anything unexpected!"
      }
    ]
  },
  {
    "issue_number": 17984,
    "title": "this is a test, please ignore",
    "author": "cicdw",
    "state": "closed",
    "created_at": "2025-05-05T19:54:35Z",
    "updated_at": "2025-05-05T20:27:28Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\n![Image](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExNnFjODFuYmV2ZjZibWV4NmtrY2x2aDJ2MHdjeWsybWpqbmlmanBmNSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/hzpn4rIxysA7471IwF/giphy.gif)\n\n### Version info\n\n```Text\nn/a\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "chrisguidry",
        "body": "I'm sorry, this broke my workflow. \n\n![Image](https://github.com/user-attachments/assets/384d247d-7f71-4772-82ca-da14aa81296a)"
      }
    ]
  },
  {
    "issue_number": 9191,
    "title": "Conflicting task names when using prefect-gcp and prefect-aws simultaneously",
    "author": "john-jam",
    "state": "closed",
    "created_at": "2023-04-12T01:07:31Z",
    "updated_at": "2025-05-05T20:22:34Z",
    "labels": [
      "bug",
      "good first issue"
    ],
    "body": "### First check\r\n\r\n- [X] I added a descriptive title to this issue.\r\n- [X] I used the GitHub search to find a similar issue and didn't find it.\r\n- [X] I searched the Prefect documentation for this issue.\r\n- [X] I checked that this issue is related to Prefect and not one of its dependencies.\r\n\r\n### Bug summary\r\n\r\nWhen using simultaneously `prefect-gcp` and `prefect-aws`, the 4 following tasks have naming conflicts:\r\n- `create_secret`\r\n- `update_secret`\r\n- `read_secret`\r\n- `delete_secret`\r\n\r\nLet me know if I should report this issue on either `prefect-gcp` or `prefect-aws` repo.\r\n### Reproduction\r\n\r\n```python3\r\nfrom prefect import flow, task\r\n\r\n\r\n@task\r\ndef dummy_task():\r\n    pass\r\n\r\n\r\n@flow\r\ndef dummy_flow():\r\n    dummy_task()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    dummy_flow()\r\n```\r\n\r\n\r\n### Error\r\n\r\n```python3\r\n.../lib/python3.8/site-packages/prefect/tasks.py:279: UserWarning: A task named 'create_secret' and defined at '.../venv/lib/python3.8/site-packages/prefect_gcp/secret_manager.py:29' conflicts with another task. Consider specifying a unique `name` parameter in the task definition:\r\n\r\n `@task(name='my_unique_name', ...)`\r\n  warnings.warn(\r\n.../venv/lib/python3.8/site-packages/prefect/tasks.py:279: UserWarning: A task named 'update_secret' and defined at '.../venv/lib/python3.8/site-packages/prefect_gcp/secret_manager.py:85' conflicts with another task. Consider specifying a unique `name` parameter in the task definition:\r\n\r\n `@task(name='my_unique_name', ...)`\r\n  warnings.warn(\r\n.../venv/lib/python3.8/site-packages/prefect/tasks.py:279: UserWarning: A task named 'read_secret' and defined at '.../venv/lib/python3.8/site-packages/prefect_gcp/secret_manager.py:142' conflicts with another task. Consider specifying a unique `name` parameter in the task definition:\r\n\r\n `@task(name='my_unique_name', ...)`\r\n  warnings.warn(\r\n.../venv/lib/python3.8/site-packages/prefect/tasks.py:279: UserWarning: A task named 'delete_secret' and defined at '.../venv/lib/python3.8/site-packages/prefect_gcp/secret_manager.py:192' conflicts with another task. Consider specifying a unique `name` parameter in the task definition:\r\n\r\n `@task(name='my_unique_name', ...)`\r\n  warnings.warn(\r\n10:03:06.178 | INFO    | prefect.engine - Created flow run 'gorgeous-mole' for flow 'dummy-flow'\r\n10:03:06.353 | INFO    | Flow run 'gorgeous-mole' - Created task run 'dummy_task-0' for task 'dummy_task'\r\n10:03:06.354 | INFO    | Flow run 'gorgeous-mole' - Executing 'dummy_task-0' immediately...\r\n10:03:06.413 | INFO    | Task run 'dummy_task-0' - Finished in state Completed()\r\n10:03:06.433 | INFO    | Flow run 'gorgeous-mole' - Finished in state Completed('All states completed.')\r\n```\r\n\r\n\r\n### Versions\r\n- `prefect-gcp`: `0.4.0`\r\n- `prefect-aws`: `0.3.0`\r\n- `prefect`:\r\n```Text\r\nVersion:             2.10.3\r\nAPI version:         0.8.4\r\nPython version:      3.8.16\r\nGit commit:          f9ddd259\r\nBuilt:               Tue, Apr 11, 2023 11:55 AM\r\nOS/Arch:             linux/x86_64\r\nProfile:             default\r\nServer type:         ephemeral\r\nServer:\r\n  Database:          sqlite\r\n  SQLite version:    3.31.1\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "discdiver",
        "body": "I reproduced. "
      },
      {
        "user": "zanieb",
        "body": "We should provide unique names for these tasks with the `name` option i.e. \"Create GCP Secret\""
      },
      {
        "user": "JakobLS",
        "body": "I get this error despite specifying unique names for each task using prefect `2.10.9`. I run everything locally and don't have `prefect-gcp` nor `prefect-aws` installed."
      }
    ]
  },
  {
    "issue_number": 17968,
    "title": "[ui] SendGrid Email Block doesn't show python code snippet correctly",
    "author": "devinvillarosa",
    "state": "open",
    "created_at": "2025-05-02T15:25:14Z",
    "updated_at": "2025-05-05T17:21:17Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nWhen viewing the details of my SendGrid Email block, the python code snippet isn't parsing correctly.\n\nSeems like `block_type.code_example` is not parsing correctly with https://github.com/PrefectHQ/prefect-ui-library/blob/main/src/components/BlockTypeSnippet.vue#L17\n\n![Image](https://github.com/user-attachments/assets/45c13c9f-1f4e-402f-bee4-ac3a4d4b9bc7)\n\n### Version info\n\n```Text\nVersion:             3.3.6\nAPI version:         0.8.4\nPython version:      3.11.11\nGit commit:          01441afa\nBuilt:               Thu, Apr 24, 2025 07:26 PM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         server\nPydantic version:    2.10.4\n```\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 14747,
    "title": "Suspending a flow run from the UI or outside of a flow should use a \"suspending\" state, not \"suspended\"",
    "author": "abrookins",
    "state": "open",
    "created_at": "2024-07-24T22:18:30Z",
    "updated_at": "2025-05-05T17:13:32Z",
    "labels": [
      "enhancement",
      "ui"
    ],
    "body": "### First check\n\n- [X] I added a descriptive title to this issue.\n- [X] I used the GitHub search to find a similar request and didn't find it.\n- [X] I searched the Prefect documentation for this feature.\n\n### Describe the current behavior\n\nWhen you suspend a flow run from outside of the flow, i.e. from the UI, the flow doesn't suspend immediately, like it would if you suspended from _within_ the flow, through a call to `suspend_flow_run()`. Instead, the UI marks the flow run as _suspended_. The flow run will only learn that it's suspended when if it tries to run another task, at which time it will _actually_ suspend and shut itself down.\r\n\r\nThis may confuse users the flow will appear as \"suspended\" in the UI even though it is, in fact, still running. The UI will allow you to resume the flow even though the flow process hasn't exited yet, and then you'll get another parallel run of the same flow, which is not what you wanted.\n\n### Describe the proposed behavior\n\nWhen you suspend a flow run from the UI, we will mark it as _suspending_, indicating an in-process action. \"Resume\" may appear but will be grayed out until the flow acknowledges its process is finished by setting its state to _suspended_.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "gabr1elt",
        "body": "Actually right now, clicking on the Suspend button on the UI does not make any change in the flow process. Just marks it as Suspended in the server.\nI f the flow finishes, it will transition to Completed, even if the Server marks it as Failed after a timeout.\n\n![Image](https://github.com/user-attachments/assets/01cdfa07-995e-40ae-8868-9b6cce249965)\n\nalso see #17822 \n\n"
      }
    ]
  },
  {
    "issue_number": 17944,
    "title": "[UI] Change the state of multiple flow runs at once",
    "author": "Moortiii",
    "state": "open",
    "created_at": "2025-04-30T06:42:49Z",
    "updated_at": "2025-05-05T16:47:00Z",
    "labels": [
      "enhancement",
      "ui"
    ],
    "body": "### Describe the current behavior\n\nIn the current UI it is not possible to change the state for multiple Flow runs easily from the Runs tab. To achieve this you must either use the Prefect CLI, or the Prefect API.\n\n### Describe the proposed behavior\n\nIn a previous release, the ability to delete multiple Flow runs from the Flow Run page was added. Make it possible to bulk-select Flow runs in the same way, and use this to change the state instead of deleting the flow runs.\n\n### Example Use\n\n**Health monitoring example:**\n\nAn issue with a third-party causes multiple flow runs to fail in a row (100+). External monitoring picks this up and reports that there are many failed jobs. This raises alarms and someone is dispatched to investigate. Once the issue is resolved, they want to change state of the flows, in order to show that everything is operating as expected again. Deleting the flows is poor practice, because it erases the history of the flow run. Normally, at this point, the user would have to run a script that leverages the Prefect CLI or Prefect API in order to bulk-update flow state.\n\nWith this new feature, the user would instead navigate to the \"Runs\" tab. Next, they select the necessary filters and tags they, and click on the \"Select all\" checkbox (or individual checkboxes). A new button is introduced that allows them to update the state for the selected flow runs. This brings up the same modal that is shown when updating the state for a single flow run. The user chooses the new state, and provides an optional description. This state-change is then applied to all selected flows.\n\nThe user has now changed the state for multiple flows, without having to leverage API-keys, or needing to install and use the Prefect CLI.\n\n### Additional context\n\nAt our organization, users rarely have the Prefect CLI installed on their local system, as flows are deployed automatically by CI/CD and have their schedule pre-configured. For obvious reasons, we want to limit how frequently users have to reach for the Prefect API.\n\nThe example above is highly specific to (one of) the ways we perform health-monitoring of Prefect flow runs, but the feature itself feels like a natural extension to the Runs tab in Prefect 3.",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @Moortiii - thanks for the great write up! I agree this would be a great enhancement, there's likely some backend work would that would need to happen here as it relates to bulk endpoints"
      }
    ]
  },
  {
    "issue_number": 17963,
    "title": "Better documentation for parameters including BaseModel parameter groups",
    "author": "bkkkk",
    "state": "closed",
    "created_at": "2025-05-02T07:12:33Z",
    "updated_at": "2025-05-05T15:55:02Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Describe the current behavior\n\nThe documentation around parameters is good to get people started but there's a lot of details and more advanced features that are not documented, particularly around BaseModel groups and how the UI handles different data types.\n\n### Describe the proposed behavior\n\nThe documentation for parameters could be way more helpful for end users. Off the top of my head we've encountered the following issues where documentation would've been helped:\n\n* Using a `pendulum.Date` parameter renders a nice UI element in Cloud but `pendulum.DateTime` doesn't. It would be helpful to have a table with some kind of \"support\" tier for data types.\n* Parameter groups using BaseModel behave quite differently and provide additional options above and beyond regular \"function argument\" parameters but these options are not really documented anywhere (As far as I can tell but maybe I'm wrong) and I end up finding out about these features when posting on Slack. Special features includes things like title to control the formatting of the field title in the UI, description to provide better documentation (using markdown for formatting), and position to determine the order of fields in the *Custom Run* form.\n\nHappy to take a crack at the BaseModel docs part. Before I do that though I wanted to bring this up for discussion. It would be a shame for changes to be made such that the docs become outdated. It's maybe not worth starting if there's no appetite to take this on longer term.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @bkkkk - thanks for the issue! totally agreed we were missing detailed docs on this, so I opened the PR linked above\n\npreview [here](https://prefect-bd373955-form-building.mintlify.app/v3/tutorials/form-building) - should be available on the main docs soon! lmk if you have thoughts / questions"
      }
    ]
  },
  {
    "issue_number": 8962,
    "title": "Add Message to Toast Notifications When Failing to Delete a Block Associated With a Deployment",
    "author": "dylanbhughes",
    "state": "closed",
    "created_at": "2023-03-28T14:22:07Z",
    "updated_at": "2025-05-05T13:34:28Z",
    "labels": [
      "enhancement",
      "ui"
    ],
    "body": "### First check\n\n- [X] I added a descriptive title to this issue.\n- [X] I used the GitHub search to find a similar request and didn't find it.\n- [X] I searched the Prefect documentation for this feature.\n\n### Prefect Version\n\n2.x\n\n### Describe the current behavior\n\n* I tried to delete a block and got a toast message on the screen that just said \"failed to delete block\" or something similar\r\n* I looked at the console and I was getting a 409 with a message saying that I can't delete a block associated with a deployment\r\nI think it'd be really helpful to have this message describing why I can't delete this block in the toast notifications in this case.\n\n### Describe the proposed behavior\n\nInclude a reason the block deletion failed in the toast notification in the UI.\n\n### Example Use\n\n_No response_\n\n### Additional context\n\nSteps to recreate:\r\n* Create blocks for a deployment\r\n* Create a deployment that uses those blocks\r\n* Attempt to delete one of those blocks from the UI",
    "comments": [
      {
        "user": "serinamarie",
        "body": "Report of this in the community as well:\r\n\r\nhttps://prefect-community.slack.com/archives/CL09KU1K7/p1680294726634039"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 30 days with no activity. To keep this issue open remove stale label or comment."
      },
      {
        "user": "dylanbhughes",
        "body": "comment to keep fresh"
      }
    ]
  },
  {
    "issue_number": 17354,
    "title": "Classmethod flows lose class binding after .with_options()",
    "author": "bnaul",
    "state": "closed",
    "created_at": "2025-03-04T01:11:00Z",
    "updated_at": "2025-05-05T13:29:29Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nOn Prefect 3.2.9, `classmethod` flows lose their `cls` inheritance structure when copied with `.with_options()`:\n\nMinimal reproducer:\n```python\n\nfrom prefect import flow\n\nclass BaseProcessor:\n    @classmethod\n    def get_multiplier(cls):\n        return 1\n    \n    @classmethod\n    @flow\n    def process(cls, x: int):\n        return x * cls.get_multiplier()\n\nclass ChildProcessor(BaseProcessor):\n    @classmethod\n    def get_multiplier(cls):\n        return 2\n\n# Original flow works correctly\nassert BaseProcessor.process(5) == 5    # Returns 5\nassert ChildProcessor.process(5) == 10  # Returns 10\n\n# After with_options(), cls binding is lost and inheritance breaks\nnew_flow = ChildProcessor.process.with_options()\nassert new_flow(5) == 10  # This fails - loses class context\n```\nI would definitely expect the original `cls` reference to be preserved by the copied flow.\n\n### Version info\n\n```Text\nVersion:             3.2.9\nAPI version:         0.8.4\nPython version:      3.10.16\nGit commit:          27eb408c\nBuilt:               Fri, Feb 28, 2025 8:12 PM\nOS/Arch:             darwin/arm64\nProfile:             default\nServer type:         cloud\nPydantic version:    2.10.6\nIntegrations:\n  prefect-gcp:       0.6.2\n  prefect-dask:      0.3.3\n  prefect-kubernetes: 0.5.3\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "cicdw",
        "body": "There's something weird and stateful going on here related to the descriptor protocol implemented [here](https://github.com/PrefectHQ/prefect/blob/b657c9c458b29f51ee4e1a10234b59522634af54/src/prefect/flows.py#L399).\n\nHere's an example where your test passes:\n\n```python\n# Original flow works correctly\nassert BaseProcessor.process(5) == 5    # Returns 5\nassert ChildProcessor.process(5) == 10  # Returns 10\n\n# After with_options(), cls binding is lost and inheritance breaks\nnew_flow = ChildProcessor.process.with_options()\n\n## call as a classmethod first\nassert ChildProcessor.process(5) == 10  # Returns 10\nassert new_flow(5) == 10  # This now succeeds\n```\n"
      }
    ]
  },
  {
    "issue_number": 17348,
    "title": "Resolving `DaskTaskRunner` task futures can cause a global client error in context hydration.",
    "author": "kzvezdarov",
    "state": "closed",
    "created_at": "2025-03-03T17:42:10Z",
    "updated_at": "2025-05-05T13:20:58Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nResolving the result of a `PrefectDaskFuture` returned by a task submitted to a `DaskTaskRunner` randomly causes a `RuntimeError: No global client found and no address provided` error to be raised in the calling flow/task.\n\nIt's significantly easier to reproduce in my cloud environment, with close to 30 to 50% chance of occurence in some flows; reproducing locally is much more difficult, but I've included a flow and traceback that managed to. Both the cloud and local flow use a `distributed.LocalCluster` and have similar structure and types.\n\nLastly, it seems like it might be related to this PR: https://github.com/PrefectHQ/prefect/pull/15341, though I've seen this raised in both flows and tasks calling `PrefectDaskFuture.result`.\n\nFlow for reproducing locally (though it has a very low occurrence rate it seems):\n```python\nfrom prefect import flow, task, serve\nimport dask.dataframe as dd\nimport numpy as np\nfrom prefect_dask.utils import get_dask_client\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n\n@task\ndef generate_data() -> np.ndarray:\n    return np.random.random(size=1_000_000)\n\n\n@task(retries=5, retry_delay_seconds=1)\ndef load_dataframe(xs, ys) -> dd.DataFrame:\n    with get_dask_client():\n        return (\n            dd.DataFrame.from_dict(\n                {\n                    \"x\": xs,\n                    \"y\": ys,\n                }\n            )\n            .mean()\n            .compute()\n        )\n\n\n@task\ndef load_dataframes() -> dict[int, dd.DataFrame]:\n    xs = generate_data()\n    ys = generate_data()\n\n    tasks = {}\n    for idx in range(50):\n        tasks[idx] = load_dataframe.submit(xs, ys)\n\n    results = {}\n    for idx, task_future in tasks.items():\n        results[idx] = task_future.result()\n\n    return results\n\n\n@flow(task_runner=DaskTaskRunner(cluster_class=\"distributed.LocalCluster\"))\ndef nested_fanout_flow():\n    results = load_dataframes()\n    return results\n\n\nif __name__ == \"__main__\":\n    nested_deploy = nested_fanout_flow.to_deployment(\n        name=\"nested-fanout-deployment\", work_pool_name=\"local\"\n    )\n\n    serve(nested_deploy)\n```\n\n\nTraceback:\n```python\nTask run failed with exception: RuntimeError('No global client found and no address provided') - Retries are exhausted\nTraceback (most recent call last):\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 805, in run_context\n    yield self\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1387, in run_task_sync\n    engine.call_task_fn(txn)\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 828, in call_task_fn\n    result = call_with_parameters(self.task.fn, parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect/utilities/callables.py\", line 208, in call_with_parameters\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/global_client_err.py\", line 39, in load_dataframes\n    results[idx] = task_future.result()\n                   ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect_dask/task_runners.py\", line 132, in result\n    future_result = self._wrapped_future.result(timeout=timeout)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/distributed/client.py\", line 401, in result\n    return self.client.sync(self._result, callback_timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect_dask/client.py\", line 62, in wrapper_func\n    return run_task_sync(*args, **kwargs)\n    ^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 1383, in run_task_sync\n    with engine.start(task_run_id=task_run_id, dependencies=dependencies):\n      ^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n    ^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 751, in start\n    with self.initialize_run(task_run_id=task_run_id, dependencies=dependencies):\n    ^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n    ^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect/task_engine.py\", line 663, in initialize_run\n    with hydrated_context(self.context):\n    ^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n    ^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect/context.py\", line 97, in hydrated_context\n    task_runner = stack.enter_context(flow.task_runner.duplicate())\n    ^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 526, in enter_context\n    result = _enter(cm)\n    ^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect_dask/task_runners.py\", line 416, in __enter__\n    raise RuntimeError(\"No global client found and no address provided\")\n    ^^^^^^^^^^^^^^^^^\nRuntimeError: No global client found and no address provided\n```\n\n### Version info\n\n```Text\nVersion:             3.2.9\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          27eb408c\nBuilt:               Fri, Feb 28, 2025 8:12 PM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         ephemeral\nPydantic version:    2.10.6\nServer:\n  Database:          sqlite\n  SQLite version:    3.49.1\nIntegrations:\n  prefect-dask:      0.3.3\n```\n\n### Additional context\n\nFull flow run logs for the reproduced local case:\n\n[turquoise-coati.csv](https://github.com/user-attachments/files/19057253/turquoise-coati.csv)",
    "comments": [
      {
        "user": "kzvezdarov",
        "body": "It seems that this was fixed by https://github.com/PrefectHQ/prefect/pull/17648, or at least I haven't encountered it since upgrading to `prefect-dask-0.3.4`"
      }
    ]
  },
  {
    "issue_number": 17334,
    "title": "DaskTaskRunner tasks occasionally fail with `AttributeError: 'NoneType' object has no attribute 'address'`",
    "author": "kzvezdarov",
    "state": "closed",
    "created_at": "2025-03-02T01:07:22Z",
    "updated_at": "2025-05-05T13:15:07Z",
    "labels": [
      "bug"
    ],
    "body": "### Bug summary\n\nTasks launched via a `DaskTaskRunner` randomly fail with the following exception:\n```python\n  File \"/Users/kzvezdarov/git/prefect-dask-test/attr_err_flow.py\", line 11, in load_dataframe\n    with get_dask_client():\n      ^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n    ^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect_dask/utils.py\", line 101, in get_dask_client\n    client_kwargs = _generate_client_kwargs(\n      ^^^^^^^^^^^^^^^^^\n  File \"/Users/kzvezdarov/git/prefect-dask-test/.venv/lib/python3.12/site-packages/prefect_dask/utils.py\", line 29, in _generate_client_kwargs\n    address = get_client().scheduler.address\n    ^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'address'\n```\n\nMinimum flow to reproduce this (somewhat reliably; executed on a local process workpool):\n```python\nfrom prefect import flow, task, serve\nimport dask.dataframe as dd\nimport numpy as np\nfrom prefect.futures import PrefectFutureList\nfrom prefect_dask.utils import get_dask_client\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n\n@task\ndef load_dataframe() -> dd.DataFrame:\n    with get_dask_client():\n        return (\n            dd.DataFrame.from_dict(\n                {\n                    \"x\": np.random.random(size=1_000),\n                    \"y\": np.random.random(size=1_000),\n                }\n            )\n            .mean()\n            .compute()\n        )\n\n\n@flow(task_runner=DaskTaskRunner())\ndef attr_err_flow():\n    tasks = PrefectFutureList()\n    for _ in range(10):\n        tasks.append(load_dataframe.submit())\n\n    return tasks.result()\n\n\nif __name__ == \"__main__\":\n    attr_err_deploy = attr_err_flow.to_deployment(\n        name=\"attr-err-deployment\", work_pool_name=\"local\"\n    )\n\n    serve(attr_err_deploy)\n```\n\nThis seems like some kind of a race condition, because increasing the amount work each\ntask has to do (via `size`) makes it less and less likely to manifest.\n\nThis appears to happen both when using both `LocalCluster` and `DaskKubernetesOperator`\nephemeral clusters.\n\nFinally, a fairly straightforward workaround seems to be simly retrying the task when\nthat exception is encountered.\n\n### Version info\n\n```Text\nVersion:             3.2.9\nAPI version:         0.8.4\nPython version:      3.12.9\nGit commit:          27eb408c\nBuilt:               Fri, Feb 28, 2025 8:12 PM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         ephemeral\nPydantic version:    2.10.6\nServer:\n  Database:          sqlite\n  SQLite version:    3.49.1\nIntegrations:\n  prefect-dask:      0.3.3\n```\n\n### Additional context\n\nFull flow run logs:\n\n[vengeful-wildcat.csv](https://github.com/user-attachments/files/19041296/vengeful-wildcat.csv)",
    "comments": [
      {
        "user": "kzvezdarov",
        "body": "Following the discussion in https://github.com/PrefectHQ/prefect/issues/12971, it seems to be the same issue; using `dask.distributed.worker_client` instead of `prefect_dask.utils.get_dask_client` resolves the problem."
      }
    ]
  },
  {
    "issue_number": 17971,
    "title": "Unable to use PREFECT_API_AUTH_STRING with task worker based on `serve`",
    "author": "Rahlir",
    "state": "closed",
    "created_at": "2025-05-02T19:55:53Z",
    "updated_at": "2025-05-05T01:44:42Z",
    "labels": [
      "bug",
      "great writeup"
    ],
    "body": "### Bug summary\n\nI have a self-hosted kubernetes based prefect server. The prefect server is setup with `PREFECT_SERVER_API_AUTH_STRING`. Everything works fine as long as I use a dedicated kubernetes prefect worker based on helm. However, I am also trying to deploy a task-worker using a docker image that runs:\n```python\nfrom prefect.task_worker import serve\n...\nif __name__ == '__main__':\n    serve(some_task)\n```\n\nThis doesn't seem to work. When the pod based on this image starts up, I get an error:\n```\nUnable to authenticate to the subscription. Please ensure the provided `PREFECT_API_KEY` you are using is valid for this environment. Reason: Auth required but no token provided\n```\n\nI also tried to set `PREFECT_API_KEY={username}:{password}`. This _seemingly works at first_, the pod spins up just fine. However, when new task run is created and the task-runner tries to run it, I get an error:\n```\nClient error '401 Unauthorized' for url 'http://{my-server}:4200/api/csrf-token?client=fb8c9655-5f2a-4cc7-9ed7-ee803fb006a8'\n```\nwhich makes sense I guess, since the task-worker is probably using `API_KEY` instead of `AUTH_STRING` due to how priorities are setup.\n\nI am guessing the problem is in `src/prefect/client/subscriptions.py`. For some reason on [line 81](https://github.com/PrefectHQ/prefect/blob/c80e444246c8805f1dfb684267e0d88dbfcc8d38/src/prefect/client/subscriptions.py#L81), the client uses `PREFECT_API_KEY` no matter whether `PREFECT_AUTH_STRING` is set or not. I am guessing server **expects a token** because `PREFECT_SERVER_API_AUTH_STRING` is set, but client only sends `PREFECT_API_KEY`, which should be unset when `PREFECT_API_AUTH_STRING` is used...\n\n### Version info\n\n```Text\nVersion:             3.3.5\nAPI version:         0.8.4\nPython version:      3.11.9\nGit commit:          db4b7a33\nBuilt:               Thu, Apr 17, 2025 09:25 PM\nOS/Arch:             darwin/arm64\nProfile:             local\nServer type:         server\nPydantic version:    2.11.3\nIntegrations:\n  prefect-kubernetes: 0.5.10\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @Rahlir - thanks for the report! this seems like a bug - will take a look\n\nlikely related to https://github.com/PrefectHQ/prefect/pull/17741"
      }
    ]
  },
  {
    "issue_number": 17039,
    "title": "gui shows task run time as 4m 60s instead of 5m",
    "author": "ramelito",
    "state": "open",
    "created_at": "2025-02-07T08:50:22Z",
    "updated_at": "2025-05-04T03:02:18Z",
    "labels": [
      "bug",
      "ui"
    ],
    "body": "### Bug summary\n\ngui shows task run time as 4m 60s instead of 5m\n\n### Version info\n\n```Text\nVersion:             3.1.15\nAPI version:         0.8.4\nPython version:      3.12.3\nGit commit:          3ac3d548\nBuilt:               Thu, Jan 30, 2025 11:31 AM\nOS/Arch:             linux/x86_64\nProfile:             local\nServer type:         server\nPydantic version:    2.9.2\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "zzstoatzz",
        "body": "hi @ramelito - thanks for the report!\n\ncan you clarify whether you're using looking at the prefect open source server UI or the cloud UI?"
      },
      {
        "user": "ramelito",
        "body": "at prefect self hosted"
      },
      {
        "user": "zhen0",
        "body": "Hi @ramelito - Can you give a bit more info on what you're seeing here? \n\nIs the time you're seeing the task run duration? And is the difference happening on every run? \n\nIf you're able to provide an mre/ a few more details it would make it easier to look into. üôè"
      }
    ]
  }
]