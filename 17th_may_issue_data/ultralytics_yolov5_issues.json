[
  {
    "issue_number": 13621,
    "title": "Details for Output Tensor",
    "author": "ShreyPatel1311",
    "state": "open",
    "created_at": "2025-06-17T12:35:51Z",
    "updated_at": "2025-06-17T12:36:16Z",
    "labels": [
      "question",
      "pose"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nPose Landmark coordinates' Range in output tensor\n\n### Additional\n\nHello guyz, \nfirst of all great work with YOLOv5, I am able to use it in my embedded systems without any lags. The output size for this model is shown to be (1, 56, 8400). I understood that there are 8400 predictions made by the model. What I would like to know is about 56 features which the model is predicting. Mainly, I would like to get the coordinates for the human-pose detected by model. Can anyone please tell me what is the range for pose landmarks?!\n\nRegards",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @ShreyPatel1311, thank you for your interest in YOLOv5 ðŸš€ and for your feedback! Your question about pose landmark coordinates in the output tensor is a good one.\n\nThis is an automated response to help you get started while an Ultralytics engineer reviews your issue and will assist you soon ðŸ˜Š.\n\nFor questions like this, itâ€™s helpful if you can provide a **minimum reproducible example (MRE)** that demonstrates your workflow and the specific output you are seeing. This allows us to better understand your setup and provide more precise guidance.\n\nPlease also make sure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) and include as much detail as possible, such as example images, code snippets, and any logs if applicable.\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 can be run in these verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThanks again for your question, and an Ultralytics engineer will reply with more details soon!"
      }
    ]
  },
  {
    "issue_number": 36,
    "title": "Load YOLOv5 from PyTorch Hub â­",
    "author": "glenn-jocher",
    "state": "open",
    "created_at": "2020-06-11T04:14:04Z",
    "updated_at": "2025-06-16T04:58:32Z",
    "labels": [
      "documentation",
      "enhancement"
    ],
    "body": "ðŸ“š This guide explains how to load YOLOv5 ðŸš€ from PyTorch Hub https://pytorch.org/hub/ultralytics_yolov5. See YOLOv5 [Docs](https://docs.ultralytics.com) for additional details. UPDATED 26 March 2023.\r\n\r\n## Before You Start\r\n\r\nInstall [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\r\n\r\n```bash\r\npip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\r\n```\r\n\r\nðŸ’¡ ProTip: Cloning [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5) is **not** required ðŸ˜ƒ\r\n\r\n## Load YOLOv5 with PyTorch Hub\r\n\r\n### Simple Example\r\n\r\nThis example loads a pretrained YOLOv5s model from PyTorch Hub as `model` and passes an image for inference. `'yolov5s'` is the lightest and fastest YOLOv5 model. For details on all available models please see the [README](https://github.com/ultralytics/yolov5#pretrained-checkpoints).\r\n```python\r\nimport torch\r\n\r\n# Model\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\r\n\r\n# Image\r\nim = 'https://ultralytics.com/images/zidane.jpg'\r\n\r\n# Inference\r\nresults = model(im)\r\n\r\nresults.pandas().xyxy[0]\r\n#      xmin    ymin    xmax   ymax  confidence  class    name\r\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\r\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\r\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\r\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\r\n```\r\n\r\n\r\n### Detailed Example\r\n\r\nThis example shows **batched inference** with **PIL** and **OpenCV** image sources. `results` can be **printed** to console, **saved** to `runs/hub`, **showed** to screen on supported environments, and returned as **tensors** or **pandas** dataframes.\r\n```python\r\nimport cv2\r\nimport torch\r\nfrom PIL import Image\r\n\r\n# Model\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\r\n\r\n# Images\r\nfor f in 'zidane.jpg', 'bus.jpg':\r\n    torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f)  # download 2 images\r\nim1 = Image.open('zidane.jpg')  # PIL image\r\nim2 = cv2.imread('bus.jpg')[..., ::-1]  # OpenCV image (BGR to RGB)\r\n\r\n# Inference\r\nresults = model([im1, im2], size=640) # batch of images\r\n\r\n# Results\r\nresults.print()  \r\nresults.save()  # or .show()\r\n\r\nresults.xyxy[0]  # im1 predictions (tensor)\r\nresults.pandas().xyxy[0]  # im1 predictions (pandas)\r\n#      xmin    ymin    xmax   ymax  confidence  class    name\r\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\r\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\r\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\r\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\r\n```\r\n<img src=\"https://user-images.githubusercontent.com/26833433/124915064-62a49e00-dff1-11eb-86b3-a85b97061afb.jpg\" width=\"500\">  <img src=\"https://user-images.githubusercontent.com/26833433/124915055-60424400-dff1-11eb-9055-24585b375a29.jpg\" width=\"300\">\r\n\r\nFor all inference options see YOLOv5 `AutoShape()` forward method:\r\nhttps://github.com/ultralytics/yolov5/blob/30e4c4f09297b67afedf8b2bcd851833ddc9dead/models/common.py#L243-L252\r\n\r\n### Inference Settings\r\nYOLOv5 models contain various inference attributes such as **confidence threshold**, **IoU threshold**, etc. which can be set by:\r\n```python\r\nmodel.conf = 0.25  # NMS confidence threshold\r\n      iou = 0.45  # NMS IoU threshold\r\n      agnostic = False  # NMS class-agnostic\r\n      multi_label = False  # NMS multiple labels per box\r\n      classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs\r\n      max_det = 1000  # maximum number of detections per image\r\n      amp = False  # Automatic Mixed Precision (AMP) inference\r\n\r\nresults = model(im, size=320)  # custom inference size\r\n```\r\n\r\n\r\n### Device\r\nModels can be transferred to any device after creation:\r\n```python\r\nmodel.cpu()  # CPU\r\nmodel.cuda()  # GPU\r\nmodel.to(device)  # i.e. device=torch.device(0)\r\n```\r\n\r\nModels can also be created directly on any `device`:\r\n```python\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', device='cpu')  # load on CPU\r\n```\r\n\r\nðŸ’¡ ProTip: Input images are automatically transferred to the correct model device before inference.\r\n\r\n### Silence Outputs\r\nModels can be loaded silently with `_verbose=False`:\r\n```python\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', _verbose=False)  # load silently\r\n```\r\n\r\n### Input Channels\r\nTo load a pretrained YOLOv5s model with 4 input channels rather than the default 3:\r\n```python\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', channels=4)\r\n```\r\nIn this case the model will be composed of pretrained weights **except for** the very first input layer, which is no longer the same shape as the pretrained input layer. The input layer will remain initialized by random weights.\r\n\r\n### Number of Classes\r\nTo load a pretrained YOLOv5s model with 10 output classes rather than the default 80:\r\n```python\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', classes=10)\r\n```\r\nIn this case the model will be composed of pretrained weights **except for** the output layers, which are no longer the same shape as the pretrained output layers. The output layers will remain initialized by random weights.\r\n\r\n### Force Reload\r\nIf you run into problems with the above steps, setting `force_reload=True` may help by discarding the existing cache and force a fresh download of the latest YOLOv5 version from PyTorch Hub.\r\n```python\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)  # force reload\r\n```\r\n\r\n### Screenshot Inference\r\nTo run inference on your desktop screen:\r\n```python\r\nimport torch\r\nfrom PIL import ImageGrab\r\n\r\n# Model\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\r\n\r\n# Image\r\nim = ImageGrab.grab()  # take a screenshot\r\n\r\n# Inference\r\nresults = model(im)\r\n```\r\n\r\n### Multi-GPU Inference\r\n\r\nYOLOv5 models can be be loaded to multiple GPUs in parallel with threaded inference:\r\n\r\n```python\r\nimport torch\r\nimport threading\r\n\r\ndef run(model, im):\r\n  results = model(im)\r\n  results.save()\r\n\r\n# Models\r\nmodel0 = torch.hub.load('ultralytics/yolov5', 'yolov5s', device=0)\r\nmodel1 = torch.hub.load('ultralytics/yolov5', 'yolov5s', device=1)\r\n\r\n# Inference\r\nthreading.Thread(target=run, args=[model0, 'https://ultralytics.com/images/zidane.jpg'], daemon=True).start()\r\nthreading.Thread(target=run, args=[model1, 'https://ultralytics.com/images/bus.jpg'], daemon=True).start()\r\n```\r\n\r\n### Training\r\nTo load a YOLOv5 model for training rather than inference, set `autoshape=False`. To load a model with randomly initialized weights (to train from scratch) use `pretrained=False`. You must provide your own training script in this case. Alternatively see  our YOLOv5 [Train Custom Data Tutorial](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data) for model training.\r\n```python\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)  # load pretrained\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False, pretrained=False)  # load scratch\r\n```\r\n\r\n### Base64 Results\r\nFor use with API services. See https://github.com/ultralytics/yolov5/pull/2291 and [Flask REST API](https://github.com/ultralytics/yolov5/tree/master/utils/flask_rest_api) example for details.\r\n```python\r\nresults = model(im)  # inference\r\n\r\nresults.ims # array of original images (as np array) passed to model for inference\r\nresults.render()  # updates results.ims with boxes and labels\r\nfor im in results.ims:\r\n    buffered = BytesIO()\r\n    im_base64 = Image.fromarray(im)\r\n    im_base64.save(buffered, format=\"JPEG\")\r\n    print(base64.b64encode(buffered.getvalue()).decode('utf-8'))  # base64 encoded image with results\r\n```\r\n\r\n### Cropped Results\r\nResults can be returned and saved as detection crops:\r\n```python\r\nresults = model(im)  # inference\r\ncrops = results.crop(save=True)  # cropped detections dictionary\r\n```\r\n\r\n### Pandas Results\r\nResults can be returned as [Pandas DataFrames](https://pandas.pydata.org/):\r\n```python\r\nresults = model(im)  # inference\r\nresults.pandas().xyxy[0]  # Pandas DataFrame\r\n```\r\n<details>\r\n  <summary>Pandas Output (click to expand)</summary>\r\n\r\n```python\r\nprint(results.pandas().xyxy[0])\r\n#      xmin    ymin    xmax   ymax  confidence  class    name\r\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\r\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\r\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\r\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\r\n```\r\n</details>\r\n\r\n### Sorted Results\r\nResults can be sorted by column, i.e. to sort license plate digit detection left-to-right (x-axis):\r\n```python\r\nresults = model(im)  # inference\r\nresults.pandas().xyxy[0].sort_values('xmin')  # sorted left-right\r\n```\r\n\r\n### Box-Cropped Results\r\nResults can be returned and saved as detection crops:\r\n```python\r\nresults = model(im)  # inference\r\ncrops = results.crop(save=True)  # cropped detections dictionary\r\n```\r\n\r\n### JSON Results\r\nResults can be returned in JSON format once converted to `.pandas()` dataframes using the `.to_json()` method. The JSON format can be modified using the `orient` argument. See pandas `.to_json()` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html) for details.\r\n```python\r\nresults = model(ims)  # inference\r\nresults.pandas().xyxy[0].to_json(orient=\"records\")  # JSON img1 predictions\r\n```\r\n<details>\r\n  <summary>JSON Output (click to expand)</summary>\r\n\r\n```json\r\n[\r\n{\"xmin\":749.5,\"ymin\":43.5,\"xmax\":1148.0,\"ymax\":704.5,\"confidence\":0.8740234375,\"class\":0,\"name\":\"person\"},\r\n{\"xmin\":433.5,\"ymin\":433.5,\"xmax\":517.5,\"ymax\":714.5,\"confidence\":0.6879882812,\"class\":27,\"name\":\"tie\"},\r\n{\"xmin\":115.25,\"ymin\":195.75,\"xmax\":1096.0,\"ymax\":708.0,\"confidence\":0.6254882812,\"class\":0,\"name\":\"person\"},\r\n{\"xmin\":986.0,\"ymin\":304.0,\"xmax\":1028.0,\"ymax\":420.0,\"confidence\":0.2873535156,\"class\":27,\"name\":\"tie\"}\r\n]\r\n  ```\r\n</details>\r\n\r\n## Custom Models\r\nThis example loads a custom 20-class [VOC](https://github.com/ultralytics/yolov5/blob/master/data/voc.yaml)-trained YOLOv5s model `'best.pt'` with PyTorch Hub.\r\n```python\r\nmodel = torch.hub.load('ultralytics/yolov5', 'custom', path='path/to/best.pt')  # local model\r\nmodel = torch.hub.load('path/to/yolov5', 'custom', path='path/to/best.pt', source='local')  # local repo\r\n```\r\n\r\n## TensorRT, ONNX and OpenVINO Models\r\n\r\nPyTorch Hub supports inference on most YOLOv5 export formats, including custom trained models. See [TFLite, ONNX, CoreML, TensorRT Export tutorial](https://github.com/ultralytics/yolov5/issues/251) for details on exporting models.\r\n\r\nðŸ’¡ ProTip: **TensorRT** may be up to 2-5X faster than PyTorch on [**GPU benchmarks**](https://github.com/ultralytics/yolov5/pull/6963)\r\nðŸ’¡ ProTip: **ONNX** and **OpenVINO** may be up to 2-3X faster than PyTorch on [**CPU benchmarks**](https://github.com/ultralytics/yolov5/pull/6613)\r\n\r\n```python\r\nmodel = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5s.pt')  # PyTorch\r\n                                                            'yolov5s.torchscript')  # TorchScript\r\n                                                            'yolov5s.onnx')  # ONNX\r\n                                                            'yolov5s_openvino_model/')  # OpenVINO\r\n                                                            'yolov5s.engine')  # TensorRT\r\n                                                            'yolov5s.mlmodel')  # CoreML (macOS-only)\r\n                                                            'yolov5s.tflite')  # TFLite\r\n                                                            'yolov5s_paddle_model/')  # PaddlePaddle\r\n```\r\n\r\n## Environments\r\n\r\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\r\n\r\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\r\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\r\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\r\n\r\n\r\n## Status\r\n\r\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\r\n\r\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.",
    "comments": [
      {
        "user": "synked16",
        "body": "@glenn-jocher \r\nso can i fit a model with it?"
      },
      {
        "user": "MohamedAliRashad",
        "body": "Can someone use the training script with this configuration ?"
      },
      {
        "user": "rlalpha",
        "body": "Can I ask about the meaning of the output?\r\nHow can I reconstruct as box prediction results via the output?\r\nThanks"
      }
    ]
  },
  {
    "issue_number": 12220,
    "title": "How to train using npy format",
    "author": "Nido051",
    "state": "closed",
    "created_at": "2023-10-11T16:02:05Z",
    "updated_at": "2025-06-13T09:45:28Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI would like to perform multi-channel image processing. Is it possible to train by inputting a format such as npy or npz instead of an image format? If someone can do it, I would appreciate it if you could tell me the code. thank you\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @Nido051, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@Nido051 yes, it is possible to train YOLOv5 using the npy or npz format instead of the image format. The code for training with npy or npz files will depend on the structure and content of your data.\n\nTo train YOLOv5 with npy or npz files, you will need to modify the data loading code in the `datasets` module of the YOLOv5 repository. Specifically, you will need to create a custom dataset class that extends the `torch.utils.data.Dataset` class and implement the necessary methods to load and transform your npy or npz data.\n\nHere is a basic outline of the steps you can follow:\n\n1. Create a custom dataset class that extends `torch.utils.data.Dataset` and implement the necessary methods such as `__len__`, `__getitem__`, and any additional methods required for loading and preprocessing your npy or npz data.\n\n2. In the `__getitem__` method of your dataset class, write the code to load the npy or npz files and return the necessary inputs (image data) and targets (bounding box annotations or class labels) required for training.\n\n3. Modify the `train.py` script in the YOLOv5 repository to use your custom dataset class instead of the default COCO dataset class. You will need to update the `datasets` import statement and modify the dataset instantiation to use your custom dataset class.\n\n4. Make sure to also update any other relevant parts of the training script, such as data augmentation options and hyperparameters, to suit your needs.\n\nRemember to refer to the documentation and code of the YOLOv5 repository for more detailed information on dataset creation and training customization. Additionally, you can consult the YOLOv5 community for further assistance if needed.\n\nGood luck with your YOLOv5 training using npy or npz files!"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      }
    ]
  },
  {
    "issue_number": 13619,
    "title": "Classification model export2tflite model failed",
    "author": "jshh0401",
    "state": "open",
    "created_at": "2025-06-10T06:19:59Z",
    "updated_at": "2025-06-12T15:53:44Z",
    "labels": [
      "enhancement",
      "classify",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar feature requests.\n\n\n### Description\n\nBecause the classification model is created in a strange way, it is based on the detection model, deletes the detect head, and then replaces it with the classification head. The model is not created directly from yaml. This causes the classification model to fail when exported to tflite. I think this change is not difficult, but it is very useful. It can help users share YOLOv5 code modules and backbones to do image classification tasks.\n\nref: https://github.com/ultralytics/yolov5/blob/master/models/yolo.py (345-375)\nhttps://github.com/ultralytics/yolov5/blob/master/export.py (1431) \n\n`assert not isinstance(model, ClassificationModel), \"ClassificationModel export to TF formats not yet supported.\"`\n\n`class ClassificationModel(BaseModel):\n    \"\"\"YOLOv5 classification model for image classification tasks, initialized with a config file or detection model.\"\"\"\n\n    def __init__(self, cfg=None, model=None, nc=1000, cutoff=10):\n        \"\"\"Initializes YOLOv5 model with config file `cfg`, input channels `ch`, number of classes `nc`, and `cuttoff`\n        index.\n        \"\"\"\n        super().__init__()\n        self._from_detection_model(model, nc, cutoff) if model is not None else self._from_yaml(cfg)\n\n    def _from_detection_model(self, model, nc=1000, cutoff=10):\n        \"\"\"Creates a classification model from a YOLOv5 detection model, slicing at `cutoff` and adding a classification\n        layer.\n        \"\"\"\n        if isinstance(model, DetectMultiBackend):\n            model = model.model  # unwrap DetectMultiBackend\n        model.model = model.model[:cutoff]  # backbone\n        m = model.model[-1]  # last layer\n        ch = m.conv.in_channels if hasattr(m, \"conv\") else m.cv1.conv.in_channels  # ch into module\n        c = Classify(ch, nc)  # Classify()\n        c.i, c.f, c.type = m.i, m.f, \"models.common.Classify\"  # index, from, type\n        model.model[-1] = c  # replace\n        self.model = model.model\n        self.stride = model.stride\n        self.save = []\n        self.nc = nc\n\n    def _from_yaml(self, cfg):\n        \"\"\"Creates a YOLOv5 classification model from a specified *.yaml configuration file.\"\"\"\n        self.model = None`\n\n\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @jshh0401, thank you for bringing up this issue and for your detailed feedback on YOLOv5 ðŸš€!\n\nThis is an automated response to let you know that an Ultralytics engineer will review your report and assist you soon.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** if you haven't already. This helps us understand and resolve the problem efficiently.\n\nFor additional resources and guidance, you can refer to our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/), which cover a wide range of topics from [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) to [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your contribution and patience as we look into this! ðŸ”Ž"
      },
      {
        "user": "pderrenger",
        "body": "Thanks for the detailed analysis @jshh0401! You're absolutely right that the current ClassificationModel implementation creates challenges for TFLite export since it's derived from detection models rather than built natively from YAML configs. Creating classification models directly from YAML would indeed be cleaner and resolve the TFLite export limitation. We'd welcome your PR to implement this enhancement - it would be a valuable contribution to make YOLOv5 classification workflows more robust across all export formats."
      },
      {
        "user": "jshh0401",
        "body": "Thank you for your reply. I will work on this problem in the next few weekends. I plan to implement tflite export by adding TFClassify implementation in tf.py. If you have any suggestions, feel free to communicate with me."
      }
    ]
  },
  {
    "issue_number": 2835,
    "title": "Can't understand the \"P3/8\".",
    "author": "Zpadger",
    "state": "closed",
    "created_at": "2021-04-18T13:57:25Z",
    "updated_at": "2025-06-11T23:56:13Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "## â”Question\r\nWho can teach me the mean of these \"# P3/8\"/ \"# P4/16\" / \"# P5/32\"ï¼ŸWhat is the mean of \"P3\"? And what is the mean of \"8\"? Why \"8\"? Thank you very much for you help.\r\n\r\n## Additional context\r\nanchors:\r\n  - [10,13, 16,30, 33,23]  # P3/8\r\n  - [30,61, 62,45, 59,119]  # P4/16\r\n  - [116,90, 156,198, 373,326]  # P5/32",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @Zpadger, thank you for your interest in ðŸš€ YOLOv5! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://www.ultralytics.com or email Glenn Jocher at glenn.jocher@ultralytics.com.\n\n## Requirements\n\nPython 3.8 or later with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including `torch>=1.7`. To install run:\n```bash\n$ pip install -r requirements.txt\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/models/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "fabien-lionti",
        "body": "Yolo is based on the principle of receptive cells.\r\n\r\nFor example you can put an imaginary 80 by 80 grid on the image. When the center of an object fall into a grid cell, the output neurones associated with this cell will be responsible to output x, y, width, height, objectness score and class of that object.\r\n\r\nTo detect object from different range of size, it can be advantageous to use features from the same image at different scales.\r\n\r\nThese feature maps are obtained by using different strides (8, 16 and 32).\r\n\r\nSo P3, P4 and P5 correspond to the outputs of Yolo network associated with each of these feature maps. 8, 16, 32 correspond to the stride used to obtain these feature map.\r\n\r\nIf you want to preserve spatial information about the feature maps, you will \"construct a grid\" with size adapted to the feature map size.\r\n\r\nFor a 640 x 640 input image and a feature map obtained with a stride of 8, the final grid size will be 640 / 8 = 80 by 80 grid size."
      },
      {
        "user": "Zpadger",
        "body": "@fabien-lionti  Thank you very muchï¼"
      }
    ]
  },
  {
    "issue_number": 7331,
    "title": "Objects365 pretrained model",
    "author": "jainrahulsethi",
    "state": "closed",
    "created_at": "2022-04-07T10:31:22Z",
    "updated_at": "2025-06-10T08:36:59Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nCan anyone provide the pretrained model on the objects365 classes?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @jainrahulsethi, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions\"><img src=\"https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg\" alt=\"CI CPU testing\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), validation ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@jainrahulsethi we have an Objects 365 pretrained YOLOv5m model here:\r\nhttps://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5m_Objects365.pt"
      },
      {
        "user": "jainrahulsethi",
        "body": "Do we have the lighter versions or the heavier versions as well?"
      }
    ]
  },
  {
    "issue_number": 13580,
    "title": "Do negative samples participate in cls_loss calculation?",
    "author": "jshh0401",
    "state": "closed",
    "created_at": "2025-04-30T05:56:42Z",
    "updated_at": "2025-06-10T06:09:40Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI have fully trained a YOLOv5 model on a private dataset. When dealing with the false positives, I found that `x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf` . Then I visualized the cls_conf of negative samples and found that for a large number of negative samples, cls_conf is close to 1. I found that since negative samples do not participate in the calculation of cls loss, the model tends to output the probability of 1. Intuitively, I think cls_conf -> 1 / num_classes would be better. Is this correct?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @jshh0401, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nðŸ”” This is an automated response to help guide your issue. An Ultralytics engineer will review and assist you soon!"
      },
      {
        "user": "pderrenger",
        "body": "You're making an astute observation about the classification confidence behavior in YOLOv5. You're correct that negative samples (background) don't participate in the cls_loss calculation, which can lead to the model outputting arbitrary classification confidences for negative samples - sometimes with values close to 1 for certain classes.\n\nYour intuition that cls_conf should ideally tend toward 1/num_classes for negative samples is reasonable from a probabilistic perspective. This would indicate maximum uncertainty across all classes when no object is present. However, YOLOv5 relies on the objectness score (obj_conf) to filter out these false positives, rather than enforcing uniform classification distribution on negatives.\n\nIf you're dealing with many false positives, you might:\n\n1. Try increasing your confidence threshold during inference\n2. Adjust the balance between positive/negative samples during training with the `--hyp obj` hyperparameter\n3. Consider using more augmentation to improve generalization\n\nThe final confidence calculation (obj_conf * cls_conf) is designed to help mitigate this issue by downweighting detections with low objectness, even if they have high class confidence."
      },
      {
        "user": "jshh0401",
        "body": "Thank you for your answer and suggestions!\n\nI understand that YOLOv5 uses obj_conf to filter negative samples. In reality, we hope to perform detection on low-computing and low-resolution platforms. The fewer FPs, the better, and at the same time, we need to ensure recall (although this is difficult to do). Currently, FP < 5%.\n\nBefore this, data enhancement has been done. Increasing the threshold will reduce recall (now threshold = 0.25). Adjusting the proportion of positive and negative samples can reduce FP and also reduce recall. Most methods of reducing FP will also reduce recall, a trade-off.\n\nI want to know why we only rely on obj_conf to filter FP without considering negative samples.\n\nWill negative samples participate in loss calculations and greatly affect the learning of positive samples when controlling cls_neg_loss <= cls_pos_loss?\n\nAdd negative samples to cls loss. If there are too many negative samples involved in the calculation, the positive samples may be overwhelmed. Therefore, I plan to take the Top K to ensure that cls_neg_loss<=cls_pos_loss.\n\nThanks for your great work and looking forward to your suggestions!\n\nBest regards."
      }
    ]
  },
  {
    "issue_number": 13618,
    "title": "YOLOv5 MODEL CONVERSION",
    "author": "879514175",
    "state": "open",
    "created_at": "2025-06-09T08:11:39Z",
    "updated_at": "2025-06-09T22:23:03Z",
    "labels": [
      "bug",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\n[ERROR:0@1.192] global onnx_importer.cpp:1036 handleNode DNN/ONNX: ERROR during processing node with 1 inputs and 1 outputs: [Floor]:(onnx_node!/model.11/Floor) from domain='ai.onnx'\nTraceback (most recent call last):\n  File \"/home/orbbec/pyorbbecsdk/examples/Orbbec-Depth-Sensing-Object-Detection-using-YOLOv5/depthScale_orbbec.py\", line 135, in <module>\n    net = cv2.dnn.readNet('models/yolov5s.onnx')\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ncv2.error: OpenCV(4.11.0) /io/opencv/modules/dnn/src/onnx/onnx_importer.cpp:1058: error: (-2:Unspecified error) in function 'handleNode'\n> Node [Floor@ai.onnx]:(onnx_node!/model.11/Floor) parse error: OpenCV(4.11.0) /io/opencv/modules/dnn/src/layers/elementwise_layers.cpp:260: error: (-215:Assertion failed) src.size == dst.size && src.type() == dst.type() && src.isContinuous() && dst.isContinuous() && src.type() == CV_32F in function 'forward'\n> \n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @879514175, thank you for your interest in YOLOv5 ðŸš€! This is an automated response to help you get started. An Ultralytics engineer will also review your issue and assist you soon.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us debug the problem. This should include the exact code, model files, and steps required to reproduce the error you are encountering.\n\nIf you're working on custom training or model export â“, please share as much information as possible, such as model export commands, environment details, and your ONNX/OpenCV versions, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nLooking forward to your update! ðŸ˜Š"
      },
      {
        "user": "879514175",
        "body": "è¿™æ˜¯æ¥è‡ªQQé‚®ç®±çš„å‡æœŸè‡ªåŠ¨å›žå¤é‚®ä»¶ã€‚\nÂ \næ‚¨å¥½ï¼Œæˆ‘æœ€è¿‘æ­£åœ¨ä¼‘å‡ä¸­ï¼Œæ— æ³•äº²è‡ªå›žå¤æ‚¨çš„é‚®ä»¶ã€‚æˆ‘å°†åœ¨å‡æœŸç»“æŸåŽï¼Œå°½å¿«ç»™æ‚¨å›žå¤ã€‚"
      },
      {
        "user": "879514175",
        "body": "wget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt\npython export.py --weights yolov5s.pt --include onnx --opset 11 --dynamic"
      }
    ]
  },
  {
    "issue_number": 13617,
    "title": "multi-machine multi-GPU acceleration ratio",
    "author": "xiaoxiaodecheng",
    "state": "open",
    "created_at": "2025-06-09T02:16:03Z",
    "updated_at": "2025-06-09T11:25:01Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI was testing the training speedup comparison between 1 machine with 2 GPUs and 2 machines with 4 GPUs. The results show that the training efficiency of 2 machines with 4 GPUs is worse than that of 1 machine with 2 GPUs. Is this normal? Are there any official multi-machine multi-GPU benchmark results for comparison?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @xiaoxiaodecheng, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n_This is an automated response. An Ultralytics engineer will also assist you here soon!_ ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Yes, this is normal behavior. Multi-machine training often performs worse than single-machine multi-GPU due to network communication overhead between machines, which creates bottlenecks during gradient synchronization that don't exist with intra-machine GPU communication via NVLink/PCIe. We don't have official multi-machine benchmarks published, but you can optimize by ensuring high-bandwidth, low-latency networking between machines or consider scaling up with more GPUs on fewer machines instead."
      }
    ]
  },
  {
    "issue_number": 12609,
    "title": "Problems with quantized model",
    "author": "joaopdss",
    "state": "closed",
    "created_at": "2024-01-10T19:57:08Z",
    "updated_at": "2025-06-07T04:42:03Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHey, I've been having some problems with the YoloV5 model after quantization:\r\n\r\n- I am getting \"dots\" boxes in certain parts of my output\r\n- It don't work as expected if compared to the non quantized model.\r\n\r\nNot exactly sure if it's something related to my input, but I don't think so\r\n\r\nI'll provide a part of my code below\r\n\r\nCode for quantized (uint8) model:\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter(\"model_non_quantized.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\nimage = cv2.imread(r\"C:\\Users\\Admin\\Documents\\Projetos\\vehicle-detector\\dataset\\valid\\images\\000193_jpeg.rf.4eb3678b99257c194bad925261b3fcc2.jpg\")\r\nimage = cv2.resize(original_image, (224, 224))\r\nimage = image.astype(np.float32)\r\nimage = cv2.cvtColor(image.astype(np.float32), cv2.COLOR_BGR2RGB)\r\nimage /= 255\r\nimg = np.expand_dims(image, axis=0)\r\n\r\noutput_zero = interpreter.get_output_details()[0]['quantization'][1]\r\noutput_scale = interpreter.get_output_details()[0]['quantization'][0]\r\nimg = (img / output_scale + output_zero).astype(np.uint8)\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], img)\r\ninterpreter.invoke()\r\n\r\noutput = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\r\noutput = (output.astype(np.float32) - output_zero) * output_scale\r\n\r\nw, h = input_details[0][\"shape\"][1:3]\r\noutput[0][..., :4] *= [w, h, w, h]\r\nprint(output[0][..., :4])\r\n```\r\n\r\noutput:\r\n```\r\n[[  0.          7.4162035   6.489178    9.270255 ]\r\n [  0.          7.4162035  12.978356   14.832407 ]\r\n [  0.          7.4162035   7.4162035  13.905382 ]\r\n ...\r\n [221.55908   200.23749     0.          0.       ]\r\n [200.23749   200.23749     0.          0.       ]\r\n [200.23749   200.23749     0.          0.       ]]\r\n```\r\n\r\nafter applying NMS, we have the following results\r\n![image](https://github.com/ultralytics/yolov5/assets/72708214/c370b9a8-41fd-4b52-af32-00968c1a093c)\r\n\r\nCode for non quantized (float32) model:\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter(\"model_quantized.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\nimage = cv2.imread(r\"C:\\Users\\Admin\\Documents\\Projetos\\vehicle-detector\\dataset\\valid\\images\\000193_jpeg.rf.4eb3678b99257c194bad925261b3fcc2.jpg\")\r\nimage = cv2.resize(original_image, (224, 224))\r\nimage = image.astype(np.float32)\r\nimage = cv2.cvtColor(image.astype(np.float32), cv2.COLOR_BGR2RGB)\r\nimage /= 255\r\nimg = np.expand_dims(image, axis=0)\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], img)\r\ninterpreter.invoke()\r\n\r\noutput = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\r\n\r\nw, h = input_details[0][\"shape\"][1:3]\r\noutput[0][..., :4] *= [w, h, w, h]\r\nprint(output[0][..., :4])\r\n```\r\n\r\noutput:\r\n```\r\n[[  2.7024584   4.1183963   8.367914   10.148943 ]\r\n [  3.111283    4.3573003   8.727393   10.300795 ]\r\n [  3.2405856   4.46125     9.25566    11.281149 ]\r\n ...\r\n [212.82776   200.03745    21.792944   36.60185  ]\r\n [209.99141   194.01413    30.422749   41.619286 ]\r\n [199.55162   199.17126    55.95596    42.3295   ]]\r\n```\r\n\r\nafter applying NMS, we have the following results\r\n![image](https://github.com/ultralytics/yolov5/assets/72708214/afb09f79-a700-4c5a-bdaa-51348e0d0b71)\r\n\r\nAny ideas about the reason I am getting these huge differences between the results? Am I doing something wrong with the input for the quantized model?\r\nAnd why the output of the quantized model is like this?\r\n\r\n```\r\n[221.55908   200.23749     0.          0.       ]\r\n [200.23749   200.23749     0.          0.       ]\r\n [200.23749   200.23749     0.          0.       ]]\r\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @joaopdss, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@joaopdss hello! Thanks for reaching out with your quantization issue. Quantization can indeed introduce some discrepancies due to the reduced precision of the model weights and activations. Here are a few things to consider:\n\n1. **Quantization Parameters**: Ensure that the quantization parameters (scale and zero-point) are correctly applied. It's crucial that these are accurate for both the input and output tensors.\n\n2. **Model Accuracy**: Post-quantization, some drop in accuracy is expected. It's important to evaluate the quantized model's performance on a representative dataset to understand the impact.\n\n3. **Quantization Aware Training (QAT)**: If you haven't already, consider using QAT, which can help the model to adapt to the lower precision and potentially improve the quantized model's performance.\n\n4. **Debugging**: Check for any potential bugs in the preprocessing and postprocessing steps. The \"dots\" you're seeing could be due to incorrect bounding box scaling or non-maximum suppression (NMS) issues.\n\n5. **Model Compatibility**: Ensure that the model architecture is compatible with quantization. Some operations may not quantize well, leading to significant degradation.\n\n6. **Version Mismatch**: Verify that you're using the same version of TensorFlow for both training and inference, as differences can lead to unexpected behavior.\n\nIf you continue to face issues, consider providing more details on the quantization process you followed, and check out our documentation for any additional guidance. Keep in mind that quantization is a trade-off between model size, speed, and accuracy. ðŸ¤–ðŸ”"
      },
      {
        "user": "joaopdss",
        "body": "> @joaopdss hello! Thanks for reaching out with your quantization issue. Quantization can indeed introduce some discrepancies due to the reduced precision of the model weights and activations. Here are a few things to consider:\r\n> \r\n> 1. **Quantization Parameters**: Ensure that the quantization parameters (scale and zero-point) are correctly applied. It's crucial that these are accurate for both the input and output tensors.\r\n> 2. **Model Accuracy**: Post-quantization, some drop in accuracy is expected. It's important to evaluate the quantized model's performance on a representative dataset to understand the impact.\r\n> 3. **Quantization Aware Training (QAT)**: If you haven't already, consider using QAT, which can help the model to adapt to the lower precision and potentially improve the quantized model's performance.\r\n> 4. **Debugging**: Check for any potential bugs in the preprocessing and postprocessing steps. The \"dots\" you're seeing could be due to incorrect bounding box scaling or non-maximum suppression (NMS) issues.\r\n> 5. **Model Compatibility**: Ensure that the model architecture is compatible with quantization. Some operations may not quantize well, leading to significant degradation.\r\n> 6. **Version Mismatch**: Verify that you're using the same version of TensorFlow for both training and inference, as differences can lead to unexpected behavior.\r\n> \r\n> If you continue to face issues, consider providing more details on the quantization process you followed, and check out our documentation for any additional guidance. Keep in mind that quantization is a trade-off between model size, speed, and accuracy. ðŸ¤–ðŸ”\r\n\r\nThe thing that does not make sense for me is that I am exporting with the YoloV5 Ultranalytics way and executing with the same scripts the non quantized and quantized models, and the results have a huge difference.\r\n\r\nexport line:\r\n```\r\n!python yolov5/export.py --weights /content/drive/MyDrive/IA_Checkpoint_YoloV5/exp/weights/best.pt --img 224 --data yolov5/dataset/data.yaml --include tflite --int8\r\n```\r\n\r\nexecution line:\r\n```\r\n!python yolov5/detect.py --weights /content/drive/MyDrive/IA_Checkpoint_YoloV5/exp/weights/best-int8.tflite --img 224 --data /content/yolov5/dataset/data.yaml --conf 0.2 --source /content/yolov5/dataset/valid/images\r\n```\r\n\r\nsome results with the non quantized model:\r\n![image](https://github.com/ultralytics/yolov5/assets/72708214/1b7c5448-82cc-488b-a621-07fd0681e23c)\r\n\r\nsame results with the quantized model:\r\n![image](https://github.com/ultralytics/yolov5/assets/72708214/71899289-8910-458b-89b3-2665439a768d)\r\n\r\nMaybe is there something missing in the export.py line? I know the results can be different from both models but the difference is just too much, does not make sense for me"
      }
    ]
  },
  {
    "issue_number": 13615,
    "title": "void ASurvivalCharacter::Tick(float DeltaTime) {     Super::Tick(DeltaTime);      bool bShelter = IsUnderShelter();      ASurvivalHUD* HUD = Cast<ASurvivalHUD>(GetWorld()->GetFirstPlayerController()->GetHUD());     if (HUD)     {         HUD->UpdateShelterStatus(bShelter);     } }",
    "author": "olegsai",
    "state": "open",
    "created_at": "2025-06-06T05:25:32Z",
    "updated_at": "2025-06-06T13:15:38Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "pderrenger",
        "body": "Hi @olegsai, this appears to be Unreal Engine C++ code for a survival game character, which isn't related to YOLOv5 or computer vision. You might want to post this in an Unreal Engine development forum or repository instead. I'll close this issue as it doesn't pertain to YOLOv5."
      }
    ]
  },
  {
    "issue_number": 13616,
    "title": "void ASurvivalCharacter::WipeScreen() {     if (bWetOverlayActive && WetOverlayWidget && !bIsWiping)     {         bIsWiping = true;          // ðŸŽžï¸ Ð’Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ð¼ Ð°Ð½Ð¸Ð¼Ð°Ñ†Ð¸ÑŽ         PlayAnimMontage(WipeMontage);          // ðŸ’§ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ fade-out Ð¼Ð¾ÐºÑ€Ð¾Ð³Ð¾ ÑÐ»Ð¾Ñ         WetOverlayWidget->PlayAnimationByName(\"WetFadeOut\");          // ÐžÑ‡Ð¸ÑÑ‚Ð¸Ð¼ Ð¼Ð¾ÐºÑ€Ñ‹Ð¹ ÑÑ„Ñ„ÐµÐºÑ‚ Ð¿Ð¾ÑÐ»Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ         FTimerHandle Cleanup;         GetWorldTimerManager().SetTimer(Cleanup, [this]()         {             if (WetOverlayWidget)             {                 WetOverlayWidget->RemoveFromParent();                 WetOverlayWidget = nullptr;                 bWetOverlayActive = false;                 WetOverlayTimer = 0.0f;                 bIsWiping = false;             }         }, 1.2f, false);          UE_LOG(LogTemp, Warning, TEXT(\"ðŸ§½ ÐŸÑ€Ð¾Ñ‚Ð¸Ñ€ÐºÐ° ÑÐºÑ€Ð°Ð½Ð°: Ñ€ÑƒÐºÐ° + ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð¾ÐºÑ€Ð¾Ñ‚Ñ‹\"));     } }",
    "author": "olegsai",
    "state": "open",
    "created_at": "2025-06-06T05:30:09Z",
    "updated_at": "2025-06-06T12:10:17Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "pderrenger",
        "body": "This appears to be C++ code for an Unreal Engine project rather than a YOLOv5-related issue. Please close this issue as it's not relevant to the YOLOv5 repository - if you need help with Unreal Engine development, consider posting in appropriate game development forums or the Unreal Engine community instead."
      }
    ]
  },
  {
    "issue_number": 13608,
    "title": "Training instability",
    "author": "Taoboan1999",
    "state": "open",
    "created_at": "2025-05-27T06:37:57Z",
    "updated_at": "2025-06-04T15:28:19Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI'm training yolov12's model on my own dataset, and on map50 of the validation set, I often get 0.67â†’0.68â†’0.69â†’0.70â†’0.71â†’0.68â†’0.67â†’0.67â†’0.68â†’0.67â†’0.67\nSuch approximate changes in results. I ensured maximum batchsize=8 and maximum image size, tried reducing the learning rate lr0=0.001â†’0.0001â†’0.00001, and increased the parameters for data augmentation (mosaic).\n\nI can't change all the data in my training set and validation set, how can I change this bad status quo please\n\nTranslated with DeepL.com (free version)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Taoboan1999, thank you for reaching out and for your detailed description! ðŸš€ This is an automated response to help you get started, and an Ultralytics engineer will assist you soon.\n\nPlease visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for guidance, including quickstart guides for [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and tips for [Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us debug more effectively.\n\nIf your question is about custom training, please include as much information as possibleâ€”such as dataset samples, training logs, and the exact commands you are using.\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience! An Ultralytics engineer will review your issue and provide more specific guidance soon. ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Hi @Taoboan1999! The training instability you're experiencing with oscillating mAP50 values (0.67-0.71) is common and can be addressed with several adjustments. Try implementing learning rate scheduling with `--cos-lr` for cosine annealing, reduce `--momentum` from default 0.937 to 0.9, and consider using `--patience` for early stopping to prevent overfitting. You might also benefit from using [Exponential Moving Average (EMA)](https://docs.ultralytics.com/guides/model-training-tips/#exponential-moving-average-ema) which smooths model weights during training and often leads to more stable convergence patterns."
      },
      {
        "user": "Taoboan1999",
        "body": "> Hi [@Taoboan1999](https://github.com/Taoboan1999)! The training instability you're experiencing with oscillating mAP50 values (0.67-0.71) is common and can be addressed with several adjustments. Try implementing learning rate scheduling with `--cos-lr` for cosine annealing, reduce `--momentum` from default 0.937 to 0.9, and consider using `--patience` for early stopping to prevent overfitting. You might also benefit from using [Exponential Moving Average (EMA)](https://docs.ultralytics.com/guides/model-training-tips/#exponential-moving-average-ema) which smooths model weights during training and often leads to more stable convergence patterns.\n\nThank you very much for your patience, I will try the various methods you mentioned one by one and if any of them are more effective, I will post them"
      }
    ]
  },
  {
    "issue_number": 13607,
    "title": "detect.py crashed with multiple video sources specified in list.treams",
    "author": "regall00",
    "state": "open",
    "created_at": "2025-05-27T05:01:47Z",
    "updated_at": "2025-06-04T14:27:45Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\nI run detect.py with single/multiple video sources: perfectly works with single video, failed with 2+ video sources. Could you please help to debug? Thank you\n\n-----------cmd:\n python3 ./detect.py --weight yolov5s.pt --source list.streams --nosave\n\n-----------list.streams\n/yolov5/videos/video1.mp4\n/yolov5/videos/video1_cloned.mp4\n\n\n----------- Log\n0: 384x640 4 persons, 3 suitcases, 1 chair, 1 tv, 1: 384x640 3 persons, 1 dog, 1 chair, 2 tvs, 23.2ms\n0: 384x640 5 persons, 3 suitcases, 1 chair, 1 tv, 1: 384x640 3 persons, 1 chair, 2 tvs, 23.3ms\n0: 384x640 5 persons, 4 suitcases, 1 chair, 1 tv, 1: 384x640 3 persons, 1 chair, 1 tv, 23.4ms\n0: 384x640 4 persons, 5 suitcases, 1 chair, 1 tv, 1: 384x640 3 persons, 1 chair, 1 tv, 23.1ms\nWARNING âš ï¸ Video stream unresponsive, please check your IP camera connection.\n0: 384x640 5 persons, 4 suitcases, 1 chair, 1 tv, 1 cell phone, 1: 384x640 3 persons, 1 chair, 1 tv, 23.2ms\n0: 384x640 1: 384x640 3 persons, 1 chair, 23.7ms\nSpeed: 0.8ms pre-process, 13.5ms inference, 3.4ms NMS per image at shape (1, 3, 640, 640)\nFATAL: exception not rethrown\nFatal Python error: Aborted\n\nThread 0x0000ffff99e06ca0 (most recent call first):\n  <no Python frame>\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, \n..., \nscipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.cluster._vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering (total: 157)\nAborted (core dumped)\n\n### Environment\n\n-  YOLO: YOLOv5 ðŸš€ 2025-5-27 Python-3.10.12 torch-2.2.0 CUDA:0 (Orin, 62841MiB)\n- OS: Ubuntu 22.04.4 LTS\n- Python: Python 3.10.12\n\n### Minimal Reproducible Example\n\n```-----------work:\n#### file: list.streams\n/yolov5/videos/video1.mp4\n\n#### cmd:\npython3 ./detect.py --weight yolov5s.pt --source list.streams --nosave\n\n```\n\n```-----------failed:\n#### file: list.streams\n/yolov5/videos/video1.mp4\n/yolov5/videos/video1_cloned.mp4\n\n#### cmd:\npython3 ./detect.py --weight yolov5s.pt --source list.streams --nosave\n\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @regall00, thank you for reporting this issue with YOLOv5 ðŸš€! This is an automated response to help you get the most relevant support as quickly as possible. An Ultralytics engineer will also review your report and assist you soon.\n\nIf this is a ðŸ› Bug Report, please ensure you have provided a **minimum reproducible example** (MRE). From your description and logs, it looks like you've included the key files and commandsâ€”thank you! If you can further narrow down the problem or provide any additional error output, it would be helpful.\n\nTo help us debug and assist you more efficiently, please also verify:\n- Your [Python>=3.8.0](https://www.python.org/) environment and [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) are up to date, including [PyTorch>=1.8](https://pytorch.org/get-started/locally/).\n- You have tested in one of our recommended environments.\n\n## Quickstart\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Verified Environments\nYOLOv5 can be run in any of the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python, and PyTorch preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nFor more information, please check our [Tutorials](https://docs.ultralytics.com/yolov5/), including guides for [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nThank you for your detailed report and for helping to improve YOLOv5! ðŸ› ï¸"
      },
      {
        "user": "pderrenger",
        "body": "The crash with multiple video sources appears to be related to memory or threading issues when processing concurrent video streams. Please ensure you're using the latest YOLOv5 version by running `git pull` or reinstalling with `pip install -U ultralytics`.\n\nYou can try these workarounds: reduce the image size with `--imgsz 320`, limit threads with `--max-det 100`, or process videos sequentially by running detect.py separately for each video source instead of using the list.streams file."
      },
      {
        "user": "regall00",
        "body": "I suspect that the crash is not due to concurrent situation, but threads are not cleanly ended/handled: error happens when finishing read one video, the other video still has frames to read."
      }
    ]
  },
  {
    "issue_number": 13613,
    "title": "Confusion Matrix Issue After Training",
    "author": "smtalds-knt",
    "state": "open",
    "created_at": "2025-06-02T05:44:11Z",
    "updated_at": "2025-06-04T11:30:55Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI conducted a 300-epoch training using the BDD100K dataset. My test results are shown below. However, there seems to be a problem with the confusion matrix â€” the predictions appear incorrect or inconsistent.\n\nNote:\nBefore the training phase, I followed the instructions in this GitHub issue comment:\nhttps://github.com/Xilinx/Vitis-AI/issues/1252#issuecomment-1608431434\nSpecifically, I replaced the SiLU activation layer as recommended for compatibility with Vitis AI.\n\nCould this modification be related to the issue with the confusion matrix?\n\n![Image](https://github.com/user-attachments/assets/14220690-e3cf-42a6-a79f-8a9391a56b39)\n\n![Image](https://github.com/user-attachments/assets/30cc3ffa-52fa-40b8-a80e-66e85976e0ec)\n\n![Image](https://github.com/user-attachments/assets/5b3a8354-ba3b-4ecc-b831-8788f28af20d)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @smtalds-knt, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** (MRE) that clearly demonstrates the confusion matrix issue, including:\n- The exact commands used to train and evaluate\n- Any code changes (such as your activation layer modification)\n- Sample data or screenshots showing the problem\n\nIf this is a custom training â“ Question, please share as much detail as possible, such as dataset examples, training logs, and confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will review your issue and assist you soon! ðŸ”Ž"
      },
      {
        "user": "pderrenger",
        "body": "Yes, replacing the SiLU activation function can definitely impact your model's performance and confusion matrix results. SiLU is the default activation in YOLOv5 for good reason - it provides smooth gradients and typically better convergence compared to alternatives like ReLU.\n\nTo verify if the activation change is causing the confusion matrix inconsistencies, I'd recommend training a baseline model with the original SiLU activations on the same BDD100K dataset subset and comparing the [confusion matrix results](https://www.ultralytics.com/glossary/confusion-matrix). Also ensure you're using the latest YOLOv5 version as there have been improvements to validation metrics calculation.\n\nThe Vitis AI compatibility modification may be necessary for your deployment target, but it's likely contributing to the performance differences you're observing."
      }
    ]
  },
  {
    "issue_number": 13612,
    "title": "Log results to Comet after training has ended",
    "author": "9helix",
    "state": "open",
    "created_at": "2025-05-30T09:02:23Z",
    "updated_at": "2025-05-30T20:25:26Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI trained a model before I installed `comet_ml`. Now that I have installed it, is there a way to upload the results of my previous training session to Comet without me having to retrain the model again in order for the yolov5 to automatiaclly log the data to Comet?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @9helix, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nThis is an automated response ðŸ¤–â€”an Ultralytics engineer will review your question and assist you soon.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "pderrenger",
        "body": "You can manually log your previous training results to Comet using the [Comet Python SDK](https://www.comet.com/docs/v2/api-and-sdk/python-sdk/reference/API/). Create a new experiment and use `experiment.log_metric()`, `experiment.log_parameter()`, and `experiment.log_asset()` to upload your saved metrics, hyperparameters, and model files from your training run's output directory."
      }
    ]
  },
  {
    "issue_number": 13610,
    "title": "what is error of train.cache?",
    "author": "hitbuyi",
    "state": "open",
    "created_at": "2025-05-27T10:07:40Z",
    "updated_at": "2025-05-28T19:05:37Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n\n\n```\nmydata\nâ”œâ”€â”€ dataSet\nâ”œâ”€â”€ images\nâ”œâ”€â”€ labels\nâ””â”€â”€ label_xml\n```\nwhere dataSet includes trainval.txt,val.txt, image is png image, label includes txt lablefiles\ncontents of train.txt \n```\nD:\\Yolo\\v5\\myData\\images\\20250526101429_00001.png\nD:\\Yolo\\v5\\myData\\images\\20250526101429_00002.png\n...\n\n```\ndata cfg ymal is \n```\npath: D:\\Yolo\\v5\\mydata  # Replace with your actual path to mydata folde\ntrain: dataSet\\train.txt  # \nval: dataSet\\val.txt  #\n# Classes\nnc: 6  # number of classes\nnames: [ 'car', 'truck', 'pedestrian', 'bus', 'person', 'incense']  # class names,car,truck,pedestrian,bus,person,incense\n```\n\nan error happen \n\n```\nException has occurred: AssertionErrorï¼š\n\u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in D:\\Yolo\\v5\\myData\\dataSet\\train.cache, can not start training. See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data\n  File \"D:\\Yolo\\v5\\yolov5\\utils\\dataloaders.py\", line 611, in __init__\n    assert nf > 0 or not augment, f\"{prefix}No labels found in {cache_path}, can not start training. {HELP_URL}\"\n  File \"D:\\Yolo\\v5\\yolov5\\utils\\dataloaders.py\", line 184, in create_dataloader\n    dataset = LoadImagesAndLabels(\n  File \"D:\\Yolo\\v5\\yolov5\\train.py\", line 285, in train\n    train_loader, dataset = create_dataloader(\n  File \"D:\\Yolo\\v5\\yolov5\\train.py\", line 688, in main\n    train(opt.hyp, opt, device, callbacks)\n  File \"D:\\Yolo\\v5\\yolov5\\train.py\", line 986, in <module>\n    main(opt)\nAssertionError: \u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in D:\\Yolo\\v5\\myData\\dataSet\\train.cache, can not start training. See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data\n\n```\n\nwhat is train.cache? how to avoid this problem?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @hitbuyi, thank you for reaching out and providing detailed information about your dataset and error! This is an automated response to assist you as quickly as possible. An Ultralytics engineer will review your issue and provide further help soon ðŸ˜Š.\n\nTo get started with YOLOv5 ðŸš€, please refer to our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for step-by-step guides, including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/), which can help you set up your dataset and directory structure properly.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** (MRE) so we can help you debug more efficiently.\n\nIf this is a custom training â“ Question, try to include as much information as possible (such as dataset samples, config files, and logs), and review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) to ensure your setup aligns with recommended practices.\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 can be run in the following up-to-date verified environments (with all dependencies including [CUDA]/[CUDNN], [Python], and [PyTorch] preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nðŸ”Ž Please provide an MRE if you believe this is a bug, and check your label files and dataset structure according to our [Custom Training Tutorial](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/). Thank you for your patience!"
      },
      {
        "user": "pderrenger",
        "body": "The `train.cache` file is automatically generated by YOLOv5 to cache dataset information for faster loading. The error indicates that no valid labels were found for your training images.\n\nLooking at your configuration, the issue is likely with your dataset structure and paths. Your YAML points to `dataSet\\train.txt` which should contain relative paths to images, but you're using absolute paths. Try updating your `train.txt` to use relative paths from the dataset root:\n\n```\nimages/20250526101429_00001.png\nimages/20250526101429_00002.png\n```\n\nAlso ensure that for each image file you have a corresponding label file with the same name but `.txt` extension in the `labels` folder (e.g., `labels/20250526101429_00001.txt`). Delete any existing `.cache` files and retrain to regenerate them with the corrected paths."
      },
      {
        "user": "hitbuyi",
        "body": "> The `train.cache` file is automatically generated by YOLOv5 to cache dataset information for faster loading. The error indicates that no valid labels were found for your training images.\n> \n> Looking at your configuration, the issue is likely with your dataset structure and paths. Your YAML points to `dataSet\\train.txt` which should contain relative paths to images, but you're using absolute paths. Try updating your `train.txt` to use relative paths from the dataset root:\n> \n> ```\n> images/20250526101429_00001.png\n> images/20250526101429_00002.png\n> ```\n> \n> Also ensure that for each image file you have a corresponding label file with the same name but `.txt` extension in the `labels` folder (e.g., `labels/20250526101429_00001.txt`). Delete any existing `.cache` files and retrain to regenerate them with the corrected paths.\n\nThanks for reply.  I will have a try.  /images/, /labels/ are mandatory in the code, but there are no descriptions in your documents."
      }
    ]
  },
  {
    "issue_number": 13611,
    "title": "What if I use yolov5 in inner commercial project?",
    "author": "haohancom",
    "state": "open",
    "created_at": "2025-05-28T10:16:03Z",
    "updated_at": "2025-05-28T19:02:11Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi there,\n\nI'm considering using YOLOv5 in an internal commercial project focused solely on detecting potential fire risks. This project will only be used within our company's internal network and will not be accessible externally.\n\nGiven this context, do I need to purchase an enterprise license?\nIf not, am I required to open source my code?\n\nThank you!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @haohancom, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nThank you for your licensing question! This is an automated response to acknowledge your inquiryâ€”an Ultralytics engineer will review and assist you soon. In the meantime, you may want to review our [License](https://github.com/ultralytics/yolov5/blob/master/LICENSE) file and documentation for further details.\n\nIf you have any additional context or requirements, please feel free to add them to this issue to help us provide the most accurate guidance.\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "pderrenger",
        "body": "Hi @haohancom! \n\nFor internal commercial use of YOLOv5, you'll need an [Ultralytics Enterprise License](https://www.ultralytics.com/license) as the AGPL-3.0 license requires full open-sourcing of any derivative works, including internal usage. The AGPL-3.0 applies to all usage scenarios, not just external distribution, and mandates that projects incorporating YOLOv5 must release their entire source code publicly. \n\nFor commercial applications like your fire risk detection system, an Enterprise License provides the flexibility to use YOLOv5 without open-sourcing requirements and is specifically designed for business use cases."
      }
    ]
  },
  {
    "issue_number": 13609,
    "title": "how to write data cfg file?",
    "author": "hitbuyi",
    "state": "open",
    "created_at": "2025-05-27T09:38:53Z",
    "updated_at": "2025-05-28T14:47:28Z",
    "labels": [
      "documentation",
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nmydata\nâ”œâ”€â”€ dataSet\nâ”œâ”€â”€ image\nâ”œâ”€â”€ label\nâ””â”€â”€ label_xml\nwhere dataSet includes trainval.txt,val.txt, image is png image, label includes txt lablefiles,\nhow to write data cfg ymal files?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @hitbuyi, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will review your question and assist you soon ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "To create a YAML configuration file for your dataset structure, you'll need to specify the dataset path and split information. Here's an example based on your folder structure:\n\n```yaml\n# mydata.yaml\npath: /path/to/mydata  # dataset root dir\ntrain: dataSet/trainval.txt  # path to train images list (relative to 'path')\nval: dataSet/val.txt  # path to val images list (relative to 'path')\n\n# Classes\nnc: 80  # number of classes\nnames: ['person', 'bicycle', 'car', ...]  # class names\n```\n\nMake sure your `trainval.txt` and `val.txt` files contain relative paths to your images in the `image` folder, and that your label files in the `label` folder follow the YOLO format with corresponding filenames to your images."
      },
      {
        "user": "hitbuyi",
        "body": "> To create a YAML configuration file for your dataset structure, you'll need to specify the dataset path and split information. Here's an example based on your folder structure:\n> \n> # mydata.yaml\n> path: /path/to/mydata  # dataset root dir\n> train: dataSet/trainval.txt  # path to train images list (relative to 'path')\n> val: dataSet/val.txt  # path to val images list (relative to 'path')\n> \n> # Classes\n> nc: 80  # number of classes\n> names: ['person', 'bicycle', 'car', ...]  # class names\n> Make sure your `trainval.txt` and `val.txt` files contain relative paths to your images in the `image` folder, and that your label files in the `label` folder follow the YOLO format with corresponding filenames to your images.\n\n I think it should be `/images/ `and `/labels/`,  image files should be placed in` images `folder, and labels should be placed in` labels `folder"
      }
    ]
  },
  {
    "issue_number": 13606,
    "title": "some version's model  are not available  for downloading",
    "author": "hitbuyi",
    "state": "open",
    "created_at": "2025-05-27T03:24:08Z",
    "updated_at": "2025-05-27T21:02:12Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI found pretrained models  of v6.0 are not available, if the model is deleted, I suggest the link should be also deleted to avoid confusing\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @hitbuyi, thank you for your feedback and for bringing this to our attention! ðŸš€ This is an automated response to let you know that an Ultralytics engineer will also assist you soon.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** (MRE) to help us investigate further.\n\nIf you have questions regarding custom training or using different model versions, please provide as much detail as possibleâ€”including the links you attempted, any error messages, and your workflow. Also, make sure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 can be run in any of these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your contribution! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Thanks for bringing this to our attention! You're right that some older model links may be outdated. The YOLOv5 repository has evolved significantly since v6.0, and we recommend using the latest models which are actively maintained and offer better performance. You can find all current pretrained models in the [releases section](https://github.com/ultralytics/yolov5/releases) or by running `python train.py --weights yolov5s.pt` which will automatically download the latest version."
      }
    ]
  },
  {
    "issue_number": 10067,
    "title": "No module named 'psutil'",
    "author": "YYingH",
    "state": "closed",
    "created_at": "2022-11-07T18:32:29Z",
    "updated_at": "2025-05-27T00:36:09Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nDetection, PyTorch Hub\n\n### Bug\n\nAfter the YOLOv5 AutoCache Update, got an issue when doing model inferencing on detection, details are list in below:  \r\n\r\nDownloading: \"[https://github.com/ultralytics/yolov5/zipball/master\"](https://github.com/ultralytics/yolov5/zipball/master%22) to /root/.cache/torch/hub/master.zip\r\nTraceback (most recent call last):\r\n  File \"/app/app.py\", line 45, in <module>\r\n    yolo_model = torch.hub.load('ultralytics/yolov5', 'custom',\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/hub.py\", line 542, in load\r\n    model = _load_local(repo_or_dir, model, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/hub.py\", line 572, in _load_local\r\n    model = entry(*args, **kwargs)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 83, in custom\r\n    return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 33, in _create\r\n    from models.common import AutoShape, DetectMultiBackend\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py\", line 29, in <module>\r\n    from utils.dataloaders import exif_transpose, letterbox\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/utils/dataloaders.py\", line 22, in <module>\r\n    import psutil\r\nModuleNotFoundError: No module named 'psutil'\r\n\r\n\r\n\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @YYingH, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@YYingH psutil has been in requirements.txt for a while. Try `pip install -r requirements.txt`\r\n\r\nhttps://github.com/ultralytics/yolov5/blob/master/requirements.txt"
      },
      {
        "user": "darubiano",
        "body": "Hi, something similar is happening to me, when I try to load the model in a cloud function of Google cloud\r\n```python \r\nTraceback (most recent call last):\r\n  File \"/layers/google.python.pip/pip/bin/functions-framework\", line 8, in <module>\r\n    sys.exit(_cli())\r\n  File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/click/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/functions_framework/_cli.py\", line 37, in _cli\r\n    app = create_app(target, source, signature_type)\r\n  File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/functions_framework/__init__.py\", line 288, in create_app\r\n    spec.loader.exec_module(source_module)\r\n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\n  File \"/workspace/main.py\", line 7, in <module>\r\n    model = torch.hub.load('ultralytics/yolov5', 'custom', path=\"best.pt\")\r\n  File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/torch/hub.py\", line 399, in load\r\n    model = _load_local(repo_or_dir, model, *args, **kwargs)\r\n  File \"/layers/google.python.pip/pip/lib/python3.9/site-packages/torch/hub.py\", line 428, in _load_local\r\n    model = entry(*args, **kwargs)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 83, in custom\r\n    return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 33, in _create\r\n    from models.common import AutoShape, DetectMultiBackend\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py\", line 29, in <module>\r\n    from utils.dataloaders import exif_transpose, letterbox\r\n  File \"/root/.cache/torch/hub/ultralytics_yolov5_master/utils/dataloaders.py\", line 22, in <module>\r\n    import psutil\r\nModuleNotFoundError: No module named 'psutil'\r\n```\r\nCODE:\r\n```python \r\nimport cv2\r\nimport torch\r\nmodel = torch.hub.load('ultralytics/yolov5', 'custom', path=\"best.pt\", device='cpu')\r\ncap = cv2.VideoCapture(\"video.mp4\")\r\nwhile cap.isOpened():\r\n    success, frame = cap.read()\r\n    if success:\r\n        detect = model(frame)\r\n        info = detect.pandas().xyxy[0]\r\n        confidence_max = info['confidence'].max()\r\n        print(confidence_max)\r\n        if confidence_max >= COINFIDENCE:\r\n            cv2.imwrite(f'IMAGES/fallavial_{COUNT}_{int(confidence_max*100)}.jpg',np.squeeze(detect.render()))\r\n            COUNT+=1\r\n    else:\r\n        break\r\ncap.release()\r\n```\r\nThe requirements I use are:\r\n```python\r\nasttokens==2.1.0\r\nbackcall==0.2.0\r\ncertifi==2022.9.24\r\ncharset-normalizer==2.1.1\r\ncolorama==0.4.6\r\ncontourpy==1.0.6\r\ncycler==0.11.0\r\ndecorator==5.1.1\r\nexecuting==1.2.0\r\nfonttools==4.38.0\r\nidna==3.4\r\nipython==8.6.0\r\njedi==0.18.1\r\nkiwisolver==1.4.4\r\nmatplotlib==3.6.2\r\nmatplotlib-inline==0.1.6\r\nnumpy==1.23.4\r\nopencv-python==4.6.0.66\r\npackaging==21.3\r\npandas==1.5.1\r\nparso==0.8.3\r\npickleshare==0.7.5\r\nPillow==9.3.0\r\nprompt-toolkit==3.0.32\r\npsutil==5.9.4\r\npure-eval==0.2.2\r\nPygments==2.13.0\r\npyparsing==3.0.9\r\npython-dateutil==2.8.2\r\npytz==2022.6\r\nPyYAML==6.0\r\nrequests==2.28.1\r\nscipy==1.9.3\r\nseaborn==0.12.1\r\nsix==1.16.0\r\nstack-data==0.6.1\r\nhttps://download.pytorch.org/whl/cpu/torch-1.10.2%2Bcpu-cp39-cp39-linux_x86_64.whl\r\n#torch==1.13.0\r\ntorchvision==0.14.0\r\ntqdm==4.64.1\r\ntraitlets==5.5.0\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.12\r\nwcwidth==0.2.5\r\n```\r\nI install psutil but it doesn't recognize it, ðŸ˜ª\r\nI would really appreciate it if you can help me.\r\n\r\nUpdate ðŸ™ƒ:\r\nThe solution was to remove the version of torchvision\r\nNew requirements\r\n```python\r\n#\r\nnumpy==1.23.4\r\nopencv-python-headless\r\ngoogle-cloud-storage\r\n# versiones compatibles torch\r\nhttps://download.pytorch.org/whl/cpu/torch-1.10.2%2Bcpu-cp39-cp39-linux_x86_64.whl\r\nhttps://download.pytorch.org/whl/cpu/torchvision-0.11.3%2Bcpu-cp39-cp39-linux_x86_64.whl\r\n# librerias de yolov5\r\nasttokens==2.1.0\r\nbackcall==0.2.0\r\ncertifi==2022.9.24\r\ncharset-normalizer==2.1.1\r\ncolorama==0.4.6\r\ncontourpy==1.0.6\r\ncycler==0.11.0\r\ndecorator==5.1.1\r\nexecuting==1.2.0\r\nfonttools==4.38.0\r\nidna==3.4\r\nipython==8.6.0\r\njedi==0.18.1\r\nkiwisolver==1.4.4\r\nmatplotlib==3.6.2\r\nmatplotlib-inline==0.1.6\r\npackaging==21.3\r\npandas==1.5.1\r\nparso==0.8.3\r\npickleshare==0.7.5\r\nPillow==9.3.0\r\nprompt-toolkit==3.0.32\r\npsutil==5.9.4\r\npure-eval==0.2.2\r\nPygments==2.13.0\r\npyparsing==3.0.9\r\npython-dateutil==2.8.2\r\npytz==2022.6\r\nPyYAML==6.0\r\nrequests==2.28.1\r\nscipy==1.9.3\r\nseaborn==0.12.1\r\nsix==1.16.0\r\nstack-data==0.6.1\r\ntqdm==4.64.1\r\ntraitlets==5.5.0\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.12\r\nwcwidth==0.2.5\r\ngitpython\r\n```\r\n"
      }
    ]
  },
  {
    "issue_number": 12846,
    "title": "YOLOv5 interface - predict problem",
    "author": "paulikoe",
    "state": "closed",
    "created_at": "2024-03-24T15:13:59Z",
    "updated_at": "2025-05-25T20:11:08Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\r\n\r\n\r\n### YOLOv5 Component\r\n\r\nOther\r\n\r\n### Bug\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-21-e91aa8dd130a>](https://localhost:8080/#) in <cell line: 7>()\r\n      5 # Run batched inference on a list of images\r\n      6 source = '/content/gdrive/MyDrive/Data/Vid_and_pictures/20240227_102420.jpg'\r\n----> 7 results = model.predict(source, conf=0.5, imgsz=320, save=True, save_txt = True, save_conf=True)  # list of Results objects\r\n      8 '''\r\n      9 # Process results list\r\n\r\n3 frames\r\n[/usr/local/lib/python3.10/dist-packages/ultralytics/nn/autobackend.py](https://localhost:8080/#) in __init__(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\r\n    141         if nn_module:\r\n    142             model = weights.to(device)\r\n--> 143             model = model.fuse(verbose=verbose) if fuse else model\r\n    144             if hasattr(model, \"kpt_shape\"):\r\n    145                 kpt_shape = model.kpt_shape  # pose-only\r\n\r\nTypeError: BaseModel.fuse() got an unexpected keyword argument 'verbose'\r\n\r\n### Environment\r\n\r\n_No response_\r\n\r\n### Minimal Reproducible Example\r\n\r\n`from ultralytics import YOLO\r\nmodel = YOLO('/content/gdrive/MyDrive/yolov5_diplomka/yolov5/runs/train/exp3/weights/best.pt')\r\nsource = '/content/gdrive/MyDrive/Data/Vid_and_pictures/20240227_102420.jpg'\r\nresults = model.predict(source, conf=0.5, imgsz=320, save=True, save_txt = True, save_conf=True)  # list of Results objects\r\n\r\nfor result in results:\r\n    boxes = result.boxes  # Boxes object for bounding box outputs\r\n    masks = result.masks  # Masks object for segmentation masks outputs\r\n    keypoints = result.keypoints  # Keypoints object for pose outputs\r\n    probs = result.probs  # Probs object for classification outputs\r\n    result.show()  # display to screen\r\n    result.save(filename='result_predict.jpg')  # save to disk`\r\n\r\n\r\n\r\nSorry I am new in neural networks. Can you please solve my problem? I am trying to use my model on a picture.\r\n\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @paulikoe, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@paulikoe hello there! ðŸ˜Š It looks like you're encountering a `TypeError` with the `model.fuse()` method due to an unexpected keyword argument `'verbose'`. This occurs because the `fuse()` method in the version of YOLOv5 you are using does not support or expect a `'verbose'` argument.\n\nTo resolve this issue, you can simply remove the `'verbose'` argument from the `fuse()` method call. However, since this call is internal and based on the code snippet you provided, it appears you're not directly calling `fuse()` yourself.\n\nThis problem might be related to a specific version of the YOLOv5 codebase you are using. Ensure you are using the latest version of our YOLOv5 repository. If you are already on the latest version and still encounter this issue, it might have been inadvertently introduced in a recent update.\n\nIf you believe this is a bug with our code:\n1. Ensure your local repository is updated.\n2. Re-try your code with the latest changes.\n3. If the issue persists, considering you're willing to help out by submitting a PR, it'd be fantastic if you could first verify the issue exists on the main branch, and then feel free to submit a PR that removes or appropriately handles the `verbose` argument in the `fuse()` method call within our internal code.\n\nIf you're new to neural networks, don't worry! Issues like these are part of the learning process. For more detailed documentation on YOLOv5, please refer to our [Ultralytics Docs](https://docs.ultralytics.com/yolov5/), which can guide you on several aspects of using YOLOv5, including troubleshooting and best practices.\n\nKeep up the great work, and thank you for contributing to the YOLOv5 community!"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      }
    ]
  },
  {
    "issue_number": 7693,
    "title": "How to use a past version of yolov5 ",
    "author": "KTBE",
    "state": "closed",
    "created_at": "2022-05-04T08:02:49Z",
    "updated_at": "2025-05-24T15:20:28Z",
    "labels": [
      "Stale"
    ],
    "body": "Hi\r\n\r\nI am trying to use an older version of yolov5 (release 4 or 5 instead of 6.1). \r\nIs there any way to do that?",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @KTBE, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions\"><img src=\"https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg\" alt=\"CI CPU testing\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), validation ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "MarvinZhong",
        "body": "you can just download the past model from [here](https://github.com/ultralytics/yolov5/releases), \r\nand load the model as custom,\r\n`model = torch.hub.load('ultralytics/yolov5', 'custom', path='path/model.pt', force_reload=True)`"
      },
      {
        "user": "KTBE",
        "body": "Thanks for your reply. \r\n\r\nI wanted to train my own model with a previous version of yolov5 (release 4).\r\nWhat I finally did :\r\n-Download the release 4 and store it locally (google drive in order to use colab for training).\r\n-Then I add the SPPF to common.py\r\n-And I trained my model.\r\n\r\nLet me know if my procedure is not good. Thanks."
      }
    ]
  },
  {
    "issue_number": 13602,
    "title": "Custom Optimizers in YOLOv5",
    "author": "yoyokst125",
    "state": "open",
    "created_at": "2025-05-23T14:50:03Z",
    "updated_at": "2025-05-24T08:02:06Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n\"Is it possible for YOLOv5 to run a custom-made optimizer? If it's feasible, is there a tutorial for it?\"\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @yoyokst125, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will review your question and provide further assistance soon ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "You can implement custom optimizers in YOLOv5 by modifying the optimizer selection logic in `utils/torch_utils.py`. The `smart_optimizer()` function currently supports SGD, Adam, and AdamW optimizers, but you can extend it to include your custom optimizer by adding a new conditional branch and importing your optimizer class. For example, you could add `elif opt.optimizer == 'custom': optimizer = YourCustomOptimizer(g, **kwargs)` after the existing optimizer conditions, then specify `--optimizer custom` during training to use it."
      }
    ]
  },
  {
    "issue_number": 13513,
    "title": "Yolo v5 loading using torch 2.6",
    "author": "abcjoh",
    "state": "open",
    "created_at": "2025-02-18T13:10:09Z",
    "updated_at": "2025-05-22T11:35:33Z",
    "labels": [
      "question",
      "dependencies",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n**Code**\n        model = torch.hub.load(\"ultralytics/yolov5:v7.0\", 'custom', model_path, force_reload=True)\n \n**Error**\n/torch/hub/ultralytics_yolov5_v7.0/hubconf.py\", line 60, in _create\n    model = attempt_load(path, device=device, fuse=False)  # arbitrary model\n  File \"/home/.cache/torch/hub/ultralytics_yolov5_v7.0/models/experimental.py\", line 79, in attempt_load\n    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n  File \"/home/venv/lib/python3.10/site-packages/torch/serialization.py\", line 1470, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL models.yolo.DetectionModel was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DetectionModel])` or the `torch.serialization.safe_globals([DetectionModel])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n\n\n**Question**\nCan you please help me to resolve this I have tried to also add the weights_only = False but it is not working. Torch version is 2.6.\nIs yolov5 not compatible with this torch 2.6 version or am I missing something.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @abcjoh, thank you for your interest in YOLOv5 ðŸš€ and for reporting this issue! Let's get this resolved. Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for tasks like [Custom Model Loading](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and more.\n\nFrom your description, it appears the issue may be related to ðŸ› loading YOLOv5 weights with PyTorch 2.6. To help debug this, could you kindly provide a **minimum reproducible example (MRE)**? For example:\n\n- The exact steps or script youâ€™re using to reproduce the issue\n- Any model weights or trained checkpoints (if possible, without sensitive/private data)\n- The logs of errors you're encountering and the command used to run your script\n\nThis will help us investigate further.\n\n## Requirements\n\nYOLOv5 requires [**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). Please verify your environment setup:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone repo\ncd yolov5\npip install -r requirements.txt  # install packages\n```\n\nFor compatibility updates with new PyTorch versions (like 2.6), verifying all dependencies within an environment from scratch may help.\n\n## Quickstart Environments ðŸŒŸ  \n\nYOLOv5 can also be run in any of these up-to-date verified environments for hassle-free setups:\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud**: [Deep Learning VM Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon AWS**: [Deep Learning AMI Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Docker Setup Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Documentation\n\nYOLOv5 relies on the [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html) method for model weights loading. In your case, the changes in PyTorch 2.6 (such as the `weights_only` default behavior) seem to trigger the error. Ensuring up-to-date compatibility or downgrading PyTorch to 2.5 might temporarily resolve your issue ðŸ¤”.\n\nðŸ‘‰ **Pro Tip:** Try adding `torch.serialization.add_safe_globals([DetectionModel])` in your script before loading the model, as suggested by the PyTorch error message. This explicitly allows the model class for serialization.\n\nAn Ultralytics engineer will take a closer look at this soon. Thank you for your patience! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "To resolve the PyTorch 2.6 compatibility issue with YOLOv5, please ensure you're using the latest YOLOv5 code from the master branch (not the v7.0 release tag). Update with:\n\n```python\npip install -U git+https://github.com/ultralytics/yolov5.git\n```\n\nThen try loading your model again. This update includes compatibility improvements for newer PyTorch versions. If issues persist, confirm your model file is uncorrupted and from a trusted source. \n\nFor reference, YOLOv5 actively maintains PyTorch compatibility as shown in our release notes: [v7.0 updates](https://www.ultralytics.com/blog/introducing-instance-segmentation-in-yolov5-v7-0)."
      },
      {
        "user": "0evja",
        "body": "I'm having the same issue, my solution is to downgrade the versions of torch and torchvision to 2.5.1 and 0.20.1 so it works"
      }
    ]
  },
  {
    "issue_number": 13599,
    "title": "#ieeesoc Highlight effect missing for multiple question in ASK AI bot",
    "author": "denshaw-09",
    "state": "open",
    "created_at": "2025-05-19T10:29:13Z",
    "updated_at": "2025-05-20T15:27:55Z",
    "labels": [
      "enhancement",
      "question",
      "App"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nThe dynamic yellow highlight effect is only applied to the first prompt question, but it should be present for all three. This inconsistency affects the visual experience and may lead to confusion for users expecting a uniform highlight effect.\n\n**Expected Behavior**\n\nThe yellow highlight effect should appear on all three example questions in the build-in prompt.\n\n**Actual Behavior**\n\nCurrently, the highlight effect is applied only to the first question, while the other two do not have the same dynamic effect.\n\n*Steps to Reproduce*\n- Visit the affected site.\n- Observe the EXAMPLE QUESTIONS section.\n- Notice that the yellow highlight effect appears only on the first question.\n\nReference\n\n![Image](https://github.com/user-attachments/assets/bdc142d4-7c95-4e54-8298-28fe80c467f0)\n\n\n*Potential Cause*\n\nThis could be due to a CSS or JavaScript issue limiting the effect to a single element instead of applying it uniformly across all three.\n\n### Additional\n\n- A little hover effect would make it more dynamic \n",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @denshaw-09, thank you for bringing this to our attention and for your clear description and screenshot! ðŸš€ This is an automated response to help you get the fastest possible support. An Ultralytics engineer will review your issue and provide further assistance soon.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** (MRE) if you haven't already. This helps us debug and resolve the issue more efficiently.\n\nFor general troubleshooting and best practices, please review our [Tutorials](https://docs.ultralytics.com/yolov5/) and, if applicable, our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for helping us improve YOLOv5! ðŸ–Œï¸âœ¨"
      },
      {
        "user": "pderrenger",
        "body": "Hi @denshaw-09,\n\nThank you for reporting this UI inconsistency with the highlight effect. You're right - the highlighting should be consistent across all example questions for better user experience. I've logged this as a UI bug to be addressed in an upcoming release.\n\nYour suggestion about adding a hover effect is also appreciated and will be considered as we work on the fix. We'll update this issue once the changes have been implemented.\n\nThanks for helping us improve the YOLOv5 repository!"
      },
      {
        "user": "denshaw-09",
        "body": "It's nice to hear that @pderrenger \nIf possible I would like to work on this after the new release so kindly assign me the issue for further assistance in the process \nThanks"
      }
    ]
  },
  {
    "issue_number": 13598,
    "title": "Regarding the inference speed of the yolov 5 model",
    "author": "isakkk3",
    "state": "open",
    "created_at": "2025-05-19T09:05:16Z",
    "updated_at": "2025-05-20T05:05:32Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello, my self-trained YOLOv5s model has an identical architecture to the official version and uses the official weights, yet the inference time of my implementation is 70 milliseconds, showing such a significant discrepancy compared to the official benchmarks. Why does this happen despite meticulously following the official setup steps?\"\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @isakkk3, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nðŸ”” This is an automated response. An Ultralytics engineer will also assist you here soon!"
      },
      {
        "user": "pderrenger",
        "body": "Hi @isakkk3,\n\nThe inference time discrepancy is likely due to several factors that affect benchmarking results. The official benchmarks are typically measured under optimized conditions:\n\n1. Hardware: Benchmarks often use high-end GPUs in controlled environments\n2. Inference settings: Check if you're using half-precision (FP16) vs full-precision (FP32)\n3. Measurement methodology: Official benchmarks may exclude model loading time and measure only the forward pass\n4. Optimization: TensorRT export can significantly speed up inference compared to standard PyTorch\n\nYou can benchmark your model with the Ultralytics package using:\n```python\nfrom ultralytics.utils.benchmarks import benchmark\nbenchmark(model=\"your_yolov5s.pt\", imgsz=640, half=False, device=0)\n```\n\nTry optimizing with `half=True` and exploring different export formats like ONNX or TensorRT for better performance."
      },
      {
        "user": "isakkk3",
        "body": "After converting the ONNX model to RKNN format and deploying it on an RK3588 platform, I observed a significant performance discrepancy: my model's inference time is 75ms compared to the official benchmark of 35ms, despite using the same YOLOv5s pretrained weights and exclusively applying ReLU activation functions during training. Which step in this pipeline might be causing this degradation?\""
      }
    ]
  },
  {
    "issue_number": 8288,
    "title": "Can't train  with multi gpus ",
    "author": "donaldlee2008",
    "state": "closed",
    "created_at": "2022-06-21T18:46:59Z",
    "updated_at": "2025-05-19T11:00:14Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nos:WINDOWS 10 \r\ntwo GPU 3090TI\r\nï¼ˆpytorch) D:\\deeplearning\\majiang\\yolov561>python -m torch.distributed.run --nproc_per_node 2 train.py --batch 64  --data data/majiangVOC.yaml  --cfg models/majiangv5s.yaml --weights weights/yolov5s.pt  --device 0,1\r\nNOTE: Redirects are currently not supported in Windows or MacOs.\r\nWARNING:__main__:\r\n*****************************************\r\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\r\n*****************************************\r\ntrain: weights=weights/yolov5s.pt, cfg=models/majiangv5s.yaml, data=data/majiangVOC.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=300, batch_size=64, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=0,1, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\ngithub: skipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\r\nYOLOv5  2022-2-22 torch 1.10.2 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24564MiB)\r\n                                 CUDA:1 (NVIDIA GeForce RTX 3090 Ti, 24564MiB)\r\n\r\nAdded key: store_based_barrier_key:1 to store for rank: 0\r\nRank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 643, in <module>\r\n    main(opt)\r\n  File \"train.py\", line 539, in main\r\nWeights & Biases:     train(opt.hyp, opt, device, callbacks)run 'pip install wandb' to automatically track and visualize YOLOv5  runs (RECOMMENDED)\r\n\r\n  File \"train.py\", line 109, in train\r\nTensorBoard:     with torch_distributed_zero_first(LOCAL_RANK):Start with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\r\n\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"D:\\deeplearning\\majiang\\yolov561\\utils\\torch_utils.py\", line 39, in torch_distributed_zero_first\r\n    dist.barrier(device_ids=[local_rank])\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 2698, in barrier\r\n    \"for the selected backend {}\".format(get_backend(group))\r\nRuntimeError: Function argument device_ids not supported for the selected backend gloo\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 643, in <module>\r\n    main(opt)\r\n  File \"train.py\", line 539, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"train.py\", line 110, in train\r\n    data_dict = data_dict or check_dataset(data)  # check if None\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"D:\\deeplearning\\majiang\\yolov561\\utils\\torch_utils.py\", line 42, in torch_distributed_zero_first\r\n    dist.barrier(device_ids=[0])\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 2698, in barrier\r\n    \"for the selected backend {}\".format(get_backend(group))\r\nRuntimeError: Function argument device_ids not supported for the selected backend gloo\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4352) of binary: C:\\Users\\MH\\.conda\\envs\\pytorch\\python.exe\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\run.py\", line 723, in <module>\r\n    main()\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\errors\\__init__.py\", line 345, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\run.py\", line 719, in main\r\n    run(args)\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\run.py\", line 713, in run\r\n    )(*cmd_args)\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 131, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"C:\\Users\\MH\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 261, in launch_agent\r\n    failures=result.failures,\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\r\n============================================================\r\ntrain.py FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n[1]:\r\n  time      : 2022-06-21_16:01:53\r\n  host      : MH-Image\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : 1 (pid: 17752)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2022-06-21_16:01:53\r\n  host      : MH-Image\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : 1 (pid: 4352)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @donaldlee2008, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"CI CPU testing\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), validation ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@donaldlee2008 all DDP trainings should be run in Docker.\r\n\r\n\r\n## Environments\r\n\r\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\r\n\r\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\r\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\r\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello, this issue has been automatically marked as stale because it has not had recent activity. Please note it will be closed if no further activity occurs.\n\nAccess additional [YOLOv5](https://ultralytics.com/yolov5) ðŸš€ resources:\n- **Wiki** â€“ https://github.com/ultralytics/yolov5/wiki\n- **Tutorials** â€“ https://docs.ultralytics.com/yolov5\n- **Docs** â€“ https://docs.ultralytics.com\n\nAccess additional [Ultralytics](https://ultralytics.com) âš¡ resources:\n- **Ultralytics HUB** â€“ https://ultralytics.com/hub\n- **Vision API** â€“ https://ultralytics.com/yolov5\n- **About Us** â€“ https://ultralytics.com/about\n- **Join Our Team** â€“ https://ultralytics.com/work\n- **Contact Us** â€“ https://ultralytics.com/contact\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLOv5 ðŸš€ and Vision AI â­!"
      }
    ]
  },
  {
    "issue_number": 13597,
    "title": "adding normalization at inferece level for applying temp scaling in yolov8n",
    "author": "omidgh1379",
    "state": "open",
    "created_at": "2025-05-18T11:09:11Z",
    "updated_at": "2025-05-19T04:09:18Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "> Im finetune yolov8n on my own dataset with the goal of getting the raw logits and apply temp scaling to reduce the uncertainty. \n> \n> when I get the raw result from model.model(image) which normalization should I apply to match my traning normalization ? \n> \n> since when I apply without normizaiton I have high overconfidect model and when I divide by 255 my model is underconfident. \n> \n> \n>   def compare_with_original_model(self , valid_loader   ):\n> \n>         all_logits = []\n>         all_labels =[]\n> \n>         \n> \n>         with torch.no_grad():\n> \n>             self.temperature.data = torch.tensor([1.0] , device = self.temperature.device , dtype = torch.float32)\n>             for batch in valid_loader:\n>                 img = batch['img']\n>                 img = img / 255\n>                 \n>                 outputs = self.forward(img)\n>                 \n> \n>                 targets = {\n>                     'cls' : batch['cls'] , \n>                     'bboxes' : batch['bboxes'] , \n>                     'batch_idx' : batch['batch_idx']\n>                 }\n> \n>                 logits , labels = self._match_targets_to_outputs(outputs , targets)\n> \n>                 if logits is not None and labels is not None:\n>                     all_logits.append(logits)\n>                     all_labels.append(labels)\n> \n>         all_logits = torch.cat(all_logits , dim = 0)\n>         all_labels = torch.cat(all_labels , dim = 0).long()\n> \n> \n>         nll_criterion = nn.CrossEntropyLoss().cuda()\n>         ece_criterion = _ECE_criterion().cuda()\n> \n>         nll = nll_criterion(all_logits , all_labels.squeeze(1)).item()\n>         ece = ece_criterion(all_logits , all_labels.squeeze(1)).item()\n> \n>         print('all_logits shape is : ',all_logits.shape)\n>         print('all_labels shape is : ',all_labels.squeeze(1).shape)\n> \n>         print(f'NLL: {nll:.3f}, ECE: {ece:.3f}')\n>         print('entropy of the temperature scaled model is : ',self.confidence_entropy(all_logits))\n>         return nll , ece \n\n _Originally posted by @omidgh1379 in [#12063](https://github.com/ultralytics/yolov5/issues/12063#issuecomment-2888921953)_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @omidgh1379, thank you for reaching out and for your interest in YOLOv5 ðŸš€! This is an automated response to help you get startedâ€”an Ultralytics engineer will assist you further soon.\n\nFor questions involving inference normalization, temperature scaling, or custom model modifications, please provide a **minimum reproducible example (MRE)** if possible. This helps us understand and debug your scenario more efficiently.\n\nIf your issue relates to ðŸ› bugs, please share the smallest code snippet and dataset sample that reproduces the problem. For custom training or inference questions, include relevant code, logs, and any clarifying details about your dataset or model setup. You can also review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) for guidance.\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). Quick install:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\nYOLOv5 can be run in these verified environments, all dependencies (including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), [PyTorch](https://pytorch.org/)) preinstalled:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your contribution! ðŸ¥³"
      },
      {
        "user": "pderrenger",
        "body": "Hi @omidgh1379,\n\nI notice you're asking about YOLOv8n normalization in the YOLOv5 repository. For consistency between training and inference, you should apply the same normalization in both stages. Since you trained with images normalized by dividing by 255, you should maintain this during inference.\n\nIf you're experiencing confidence calibration issues, temperature scaling should be applied to the model's output logits after normalization, not as a replacement for input normalization. The division by 255 is correct for input preprocessing.\n\nFor optimal temperature scaling:\n1. Keep your input normalization consistent (img = img / 255)\n2. Apply temperature scaling to the output logits (dividing logits by your learned temperature parameter)\n3. Fine-tune the temperature parameter on a validation set\n\nIf you're specifically working with YOLOv8n, you might want to check the [Ultralytics YOLOv8 repository](https://github.com/ultralytics/ultralytics) for architecture-specific details, as there could be slight differences in how normalization is handled."
      }
    ]
  },
  {
    "issue_number": 12063,
    "title": "are the images used during training normalized by mean and standard deviations?",
    "author": "DarshnaZE",
    "state": "closed",
    "created_at": "2023-08-31T08:34:56Z",
    "updated_at": "2025-05-19T02:29:18Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am aware about the fact that few yolo models normalizes images using coco mean and std values. does yolov5 normalize images during training using mean and standard deviation? If yes, may i know where is the code snippet in the repo? if not, is there any way that i can integrate it in yolov5.\r\n\r\nThanks in adavnce.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @DarshnaZE, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@DarshnaZE yes, the images used for training in YOLOv5 are normalized by mean and standard deviations. This normalization is performed during the data preprocessing stage in the `datasets.py` file. Specifically, in the `LoadImagesAndLabels` class, the `__getitem__` method applies normalization using the default mean `[0.485, 0.456, 0.406]` and standard deviations `[0.229, 0.224, 0.225]`. You can find the code snippet responsible for this normalization [here](https://github.com/ultralytics/yolov5/blob/master/yolov5/data/datasets.py#L60-L61).\n\nIf you would like to change these mean and standard deviation values, you can simply modify them in the `__getitem__` method according to your requirements.\n\nLet us know if you have any further questions."
      },
      {
        "user": "DarshnaZE",
        "body": "@glenn-jocher first of all, thanks for the prompt reply. \r\nthe link https://github.com/ultralytics/yolov5/blob/master/yolov5/data/datasets.py that you pointed ended up with page not found error.\r\n![image](https://github.com/ultralytics/yolov5/assets/101351723/2d125b6d-f6e5-4389-b8e4-a2b3488c4807)\r\n\r\nthough i was able to find out the normalization snippet here https://github.com/ultralytics/yolov5/blob/master/utils/augmentations.py \r\n\r\nAlso, I want to clear that i am using YOLOv5/v8 for object detection task. for both the models, they use `Albumentations`  class for augmentations, correct me if i am wrong. Although `classify_albumentations` class uses those default mean and standard deviations for classification tasks, the `Albumentations` class does not do that."
      }
    ]
  },
  {
    "issue_number": 13589,
    "title": "How to properly handle class imbalance in YOLOv5m",
    "author": "rmarkovic00",
    "state": "open",
    "created_at": "2025-05-13T10:33:08Z",
    "updated_at": "2025-05-17T01:44:49Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi, \n\nI'm working on a YOLOv5m object detection project using a custom dataset with three classes: car, truck, and person. The class distribution is unbalanced (e.g. many more cars than trucks or persons).\n\nTo address this, I added the following line in my dataset.yaml file: weights: [1.0, 2.0, 3.0]\nAnd in hyp.no-augmentation.yaml, I set: cls_pw: 1.5\n\nMy goal is to help the model better learn underrepresented classes by giving them more weight during training.\n\nIs this the correct and recommended approach in YOLOv5 for handling class imbalance?\nOr is there a better way to improve performance on rare classes?\n\nThank you!\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @rmarkovic00, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n## Environments\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nðŸ”Ž This is an automated response. An Ultralytics engineer will review your question and provide further assistance soon!"
      },
      {
        "user": "pderrenger",
        "body": "Hi @rmarkovic00,\n\nYes, your approach for handling class imbalance in YOLOv5 is valid. Using class weights in dataset.yaml and adjusting cls_pw in your hyperparameters are good starting points.\n\nSome additional strategies you could implement:\n\n1. Data augmentation focused on minority classes (trucks and persons)\n2. Targeted dataset expansion by collecting more samples of underrepresented classes\n3. Adjust your IoU thresholds during evaluation to be class-specific\n4. Consider using the `--rect` flag during training, which may help with detecting objects at different scales\n\nFor severe imbalance, you might also try undersampling the majority class (cars) or implementing focal loss by adjusting the hyperparameters.\n\nYOLOv5 handles bias fairly well with proper configuration, but monitoring slice-level performance during training (how the model performs on each class separately) will help you identify if further adjustments are needed."
      },
      {
        "user": "soham-2006",
        "body": "If you want to actually guide the model to better learn rare classes during training, you need to modify the loss computation or re-balance the dataset or sampling.\n1. Use Focal Loss \nYOLOv5 supports Focal Loss, which is great for class imbalance\n\n2.  Oversampling Rare Classes (Data Level)\nDuplicate images with rare classes to balance the dataset.\n\nTools like Albumentations or custom scripts can help automate this.\n\n3. Manual Class Weighting in the Loss Function (Advanced)\nYOLOv5 doesnâ€™t support per-class loss weighting natively during training.\n\nBut you can modify the loss function in the code (loss.py) to manually apply class weights.\n\nRequires knowledge of PyTorch and understanding of YOLO loss structure.\n\n4. Synthetic Data Augmentation for Rare Classes\nUse tools like imgaug, Albumentations, or Roboflow to generate more samples for rare classes.\n\nHelps increase generalization without full duplication."
      }
    ]
  },
  {
    "issue_number": 13590,
    "title": "Incorrect GFLOPs calculation in model_info (reports ~16.4 GFLOPs instead of official 15.8 GFLOPs)",
    "author": "733ly",
    "state": "open",
    "created_at": "2025-05-14T08:40:04Z",
    "updated_at": "2025-05-16T13:30:32Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nWhen I ran the model previously, all of the printouts showed the YOLOv5s model as having 7,015,519 parameters and 15.8 GFLOPs. Today, after fusing layers, it suddenly reports 16.4 GFLOPs, and even after freshly cloning the repository it still shows 16.4. Why is this happening?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @733ly, thank you for bringing this up and for your detailed report! âš¡ï¸ This is an automated response to help you get startedâ€”an Ultralytics engineer will also review and assist you soon.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** (MRE) that demonstrates the GFLOPs calculation issue. This helps us debug efficiently.\n\nFor general questions or custom training, please include as much supporting information as possible, such as environment details, command outputs, and relevant logs. Also, check our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) for helpful guidance.\n\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To set up YOLOv5:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\nYOLOv5 can be run in any of the following up-to-date, verified environments (with all dependencies like [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience while we look into this! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Hi @733ly,\n\nThank you for reporting this discrepancy. You're correct that there's a difference between the reported GFLOPs (16.4) and the previously documented value (15.8). This can happen for several reasons:\n\n1. The `model_info()` function calculates FLOPs dynamically based on the current model architecture\n2. Layer fusion can affect FLOPs calculations as the model structure changes\n3. Recent YOLOv5 updates may have slightly modified the architecture or calculation method\n\nDifferent FLOPs calculation tools and methods often produce variations in the reported values. The calculation depends on how operations are counted and which operations are included in the total.\n\nIf you need a consistent metric for benchmarking or documentation purposes, you might want to specify which calculation tool and method you're using, as the exact calculation can vary between implementations.\n\nPaula Derrenger"
      },
      {
        "user": "733ly",
        "body": "Hi@pderrengerï¼Œ\n Thank you for answering my question. Iâ€™m still a bit confused, though. First, I didnâ€™t modify the original yolov5s.yaml, yet the reported GFLOPs jumped to 16.4. Second, I had my junior colleague verify this on his machine, and he also got 16.4. So why would the locally downloaded YOLOv5 code change simultaneously on different machinesâ€”could it be that the code is managed remotely rather than truly local? Finally, does this shift mean I need to redo all my experiments? After all, if Iâ€™m pursuing a lightweight improvement, metrics like parameter count and GFLOPs should still serve as meaningful references. Thanks again for your help, and I wish you all the best!"
      }
    ]
  },
  {
    "issue_number": 12238,
    "title": "WARNING âš ï¸ NMS time limit 2.100s exceeded",
    "author": "sanjanak-rbg",
    "state": "closed",
    "created_at": "2023-10-16T11:31:56Z",
    "updated_at": "2025-05-15T23:22:04Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nValidation\n\n### Bug\n\n 3/15      4.89G     0.5933     0.5576       1.16         66        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47429/47429 [2:18:29<00:00,  5.71it/s]\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2105/2968 [06:00<06:03,  2.37it/s]WARNING âš ï¸ NMS time limit 2.100s exceeded\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2108/2968 [27:17<72:53:17, 305.11s/it]WARNING âš ï¸ NMS time limit 2.100s exceeded\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2109/2968 [3:38:58<871:14:46, 3651.32s/it]^C\n\n### Environment\n\nyolov8,cuda gpu\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\nplease help me resolve this\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @sanjanak-rbg, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@sanjanak-rbg hi there! Thank you for reaching out. \n\nThe warning message you encountered, \"WARNING âš ï¸ NMS time limit 2.100s exceeded,\" indicates that the Non-Maximum Suppression (NMS) algorithm is taking longer than the specified time limit to execute. This can occur when the number of bounding box predictions is very high or the NMS algorithm implementation is slow.\n\nTo address this, you have a few options:\n\n1. Use a smaller image size during validation. By reducing the image size, you can decrease the number of bounding box predictions and potentially reduce the execution time of the NMS algorithm.\n\n2. Adjust the NMS time limit. You can increase the time limit for NMS by modifying the code.\n\n3. Improve the efficiency of the NMS algorithm implementation. If you are comfortable with modifying the YOLOv5 code, you can explore optimizing the NMS algorithm for your specific hardware or use a more efficient NMS algorithm implementation.\n\nPlease note that these options might have trade-offs in terms of accuracy or execution time. Additionally, sharing more details, such as your dataset characteristics and hardware configuration, can help the community provide more specific suggestions.\n\nThanks for your contribution and for being part of the YOLOv5 community! Let us know if you have any further questions or need additional assistance."
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      }
    ]
  },
  {
    "issue_number": 13585,
    "title": "YOLOV5 Inference on Intel Iris Graphics Card/GPU gives an RuntimeError: UR error",
    "author": "Radhika-Deeto",
    "state": "open",
    "created_at": "2025-05-09T11:34:23Z",
    "updated_at": "2025-05-15T12:07:06Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\nHi , YOLOV5 inference on Intel Graphics Card/GPU (xpu) gives the following error\n\n  File \"yolov5\\models\\common.py\", line 867, in forward\n    x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 to fp16/32\n**RuntimeError: UR error**\n\nPS: Inference works fine  with CPU\n\nPlease find Code Snippet Run for reference below\n\n\nimport torch\nimport cv2\nimport numpy as np\n\nif (torch.xpu.is_available()):\n    print('Yeaass')\nelse:\n    print('No')\n\ndevice = 'xpu' if torch.xpu.is_available() else 'cpu'\n#device='cpu'\n\nmodel =torch.hub.load('yolov5', \"custom\", path=\"yolov5s.pt\", source=\"local\") \nmodel.to(device)\n\nimageArray=cv2.imread('101080.jpg')\nprint(type(imageArray))\nprint(imageArray.dtype)\n#imArray=np.float16(imageArray)\n#print(imArray.shape)\n#print(imArray.dtype)\nresults = model(imageArray)\nprint(results)\n\n\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Radhika-Deeto, thank you for reporting this issue and for providing your code snippet! ðŸš€ This is an automated response to help you get startedâ€”an Ultralytics engineer will review your report and assist you soon.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us debug it. Your MRE should include the smallest amount of code and data required to reproduce the error.\n\nIf you are running into issues with inference on Intel GPUs, please also include:\n- The exact hardware and driver versions you are using\n- Your installed versions of Python, PyTorch, and any relevant libraries\n- The output of `pip list` and `torch.xpu.is_available()` if possible\n\nFor more information on custom training or usage, you can visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) and [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) guides. Also, make sure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience while we look into this! ðŸ› ï¸"
      },
      {
        "user": "pderrenger",
        "body": "Hi @Radhika-Deeto, thanks for reporting this issue. The \"RuntimeError: UR error\" suggests a compatibility problem between PyTorch's XPU backend and YOLOv5's preprocessing pipeline.\n\nFor Intel graphics hardware, I recommend using OpenVINO instead of the experimental XPU device in PyTorch. OpenVINO is specifically designed for optimizing inference on Intel hardware and provides much better performance and stability.\n\nYou can export your YOLOv5 model to OpenVINO format using:\n\n```python\n# Export the model to OpenVINO format\npython export.py --weights yolov5s.pt --include openvino --device cpu\n```\n\nThen use the exported model for inference with significantly better performance. According to our benchmarks, OpenVINO can provide up to 3x faster inference on Intel CPUs and GPUs compared to standard PyTorch implementations.\n\nFor a complete guide on optimizing YOLOv5 with OpenVINO, see the [OpenVINO integration documentation](https://docs.ultralytics.com/integrations/openvino/)."
      },
      {
        "user": "Radhika-Deeto",
        "body": "Thank you @pderrenger  for sharing that with me .I'm definitely going to go with the open Vino format considering it promises to offer a better performance . \n\nHowever , just for closure on this issue , should it not be possible to run a pytorch model as is on the Intel GPU ? I've made sure to install all the prerequisites that the Intel Graphics card would need for compatibility with Pytorch such as \n(1) Intel Graphics Driver ->  Intel Graphics Drive version 31.0.101.5522 or later is required for proper Intel GPU support in PyTorch \n (My Installed version was Version: 32.0.101.6790)\n(2) Pytorch ->  torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu  -> (My Installed version were torch                    2.7.0+xpu\ntorchaudio               2.7.0+xpu\ntorchvision              0.22.0+xpu)\n\nPlease do let me know if there is something I could do , additionally  to fix the problem and be able to run inferences on pytorch format for Ultralytics YOLOV5 ?"
      }
    ]
  },
  {
    "issue_number": 13575,
    "title": "What kind of tricks should I use to improve performance for near-infared detecion",
    "author": "JasonSloan",
    "state": "closed",
    "created_at": "2025-04-25T08:17:22Z",
    "updated_at": "2025-05-15T06:16:37Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi team,\n\nI'm currently training a person detection model using YOLOv11 on a dataset consisting of near-infrared (NIR) images. However, I'm facing some performance issues â€” particularly low recall â€” which I suspect are due to the low contrast and unique characteristics of NIR images (e.g., subtle texture, dim lighting).\n\n![Image](https://github.com/user-attachments/assets/cee2683c-332b-4639-b469-b77b54a8923f)\n\nCould you please advise on the following:\n\nAre there any recommended training strategies, augmentations, or preprocessing steps when working with NIR imagery?\n\nWould you suggest any specific model configurations or architecture tweaks that help in low-contrast scenarios?\n\nIs there support or guidance for domain adaptation or fine-tuning with limited NIR data?\n\nAre there known limitations or best practices for YOLOv11 in non-visible spectrum (e.g., NIR) conditions?\n\nThanks for your great work and looking forward to your suggestions!\n\nBest regards.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @JasonSloan, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for comprehensive guides, including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and advanced strategies like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question (such as your inquiry about NIR imagery and performance improvements), please provide as much detail as possible, including example images, training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response ðŸ¤–. An Ultralytics engineer will review your question and assist you soon!"
      },
      {
        "user": "pderrenger",
        "body": "Hi @JasonSloan,\n\nThanks for your question about NIR detection. Data augmentation is key for improving performance in this domain. I recommend:\n\n1. Implement Albumentations augmentation integration with YOLO11, focusing on CLAHE (Contrast Limited Adaptive Histogram Equalization) which specifically boosts contrast in dark/low-visibility areas - perfect for NIR imagery.\n\n2. Add augmentations like blur, median blur, and grayscale to help the model learn from varying image qualities, making it more robust for NIR conditions.\n\n3. Consider multi-stage training: first on visible spectrum images (transfer learning), then fine-tune with your NIR dataset.\n\n4. Experiment with a higher learning rate and smaller batch size for domain-specific adaptation.\n\n5. For low-contrast scenarios, try increasing the confidence threshold during inference to reduce false positives.\n\nThe [Albumentations integration](https://docs.ultralytics.com/integrations/albumentations/) documentation can help you implement these augmentations effectively. Let me know if you need more specific guidance!"
      },
      {
        "user": "JasonSloan",
        "body": "> Hi [@JasonSloan](https://github.com/JasonSloan),\n> \n> Thanks for your question about NIR detection. Data augmentation is key for improving performance in this domain. I recommend:\n> \n> 1. Implement Albumentations augmentation integration with YOLO11, focusing on CLAHE (Contrast Limited Adaptive Histogram Equalization) which specifically boosts contrast in dark/low-visibility areas - perfect for NIR imagery.\n> 2. Add augmentations like blur, median blur, and grayscale to help the model learn from varying image qualities, making it more robust for NIR conditions.\n> 3. Consider multi-stage training: first on visible spectrum images (transfer learning), then fine-tune with your NIR dataset.\n> 4. Experiment with a higher learning rate and smaller batch size for domain-specific adaptation.\n> 5. For low-contrast scenarios, try increasing the confidence threshold during inference to reduce false positives.\n> \n> The [Albumentations integration](https://docs.ultralytics.com/integrations/albumentations/) documentation can help you implement these augmentations effectively. Let me know if you need more specific guidance!\n\n\nHi [pderrenger](https://github.com/pderrenger),\n\nThank you for your detailed and helpful suggestions regarding NIR detection with YOLOv11. The recommendation to use CLAHE for contrast enhancement in low-visibility NIR imagery is particularly insightful.\n\nTo optimize its implementation, could you clarify whether CLAHE should be applied:\n\nâ€‹â€‹During trainingâ€‹â€‹ (as part of augmentations),\nâ€‹â€‹During inferenceâ€‹â€‹ (preprocessing input frames), or\nâ€‹â€‹Both stagesâ€‹â€‹ for consistency?\nAdditionally, are there any trade-offs (e.g., overfitting to enhanced contrasts if used only in training, or latency if applied during inference)?\nIf youâ€™re aware of any open-source NIR person detection datasets (e.g., for benchmarking or transfer learning), Iâ€™d be grateful for pointers.\n\nThanks again for your guidanceâ€”Iâ€™ll proceed with Albumentations integration while awaiting your advice on this nuance.\n\nBest regards,\nJason Sloan"
      }
    ]
  },
  {
    "issue_number": 1739,
    "title": "How to configure channel size",
    "author": "thhart",
    "state": "closed",
    "created_at": "2020-12-19T11:42:09Z",
    "updated_at": "2025-05-15T00:36:47Z",
    "labels": [
      "question"
    ],
    "body": "I checked the training tutorial but could not find a central configuration setting to specify the input channel size for training. Is this possible within yaml files or is it necessary to change something in yolo.py for instance?\r\nI tried to put ch into yaml but it looks like not being parsed.",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@thhart you can create a YOLOv5 model with non-default channel size with PyTorch Hub. See PyTorch Hub tutorial:\r\nhttps://docs.ultralytics.com/yolov5\r\n\r\nThe training dataloader defaults to 3 ch images, you'd have to manually modify it to your needs:\r\nhttps://github.com/ultralytics/yolov5/blob/ab0db8d1049409840c587564fc5fb4680acf0d07/utils/datasets.py#L336"
      },
      {
        "user": "glenn-jocher",
        "body": "@thhart by the way, `yaml['ch']` fields are not currently used,  but you could modify yolo.py directly to use these similar to now `yaml['nc']` is used or overridden.\r\n\r\nhttps://github.com/ultralytics/yolov5/blob/ab0db8d1049409840c587564fc5fb4680acf0d07/models/yolo.py#L69-L87"
      },
      {
        "user": "glenn-jocher",
        "body": "@thhart I would perhaps update yolo.py L84 to this to allow you to use a `ch` field in the yaml.\r\n\r\n```python\r\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[self.yaml.get('ch', ch)])  # model, savelist\r\n```"
      }
    ]
  },
  {
    "issue_number": 13584,
    "title": "yolov5.onnx view on netron",
    "author": "ashwinimate",
    "state": "closed",
    "created_at": "2025-05-09T07:29:00Z",
    "updated_at": "2025-05-12T22:51:46Z",
    "labels": [
      "question",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nhello sir/mam,\n i have downloaded the yolov5 ptretrain weights for v6.0 (yolo5s.pt) using folowing comnad\n\n!wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n\nand exported to onnx file. While visualizing  the backbone architecture on netron, the number of bottleneck in C3 should be 3, 6 9, in order. howeever upon observing the number of bollteneck in the C3 block of backbone that i got from are 1,2,3,1 respectivly. Could you please help me to find where i can get the correct pretrained weight for yolov5 (all models)? have i downloaded the wring weights.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @ashwinimate, thank you for reaching out and using YOLOv5 ðŸš€! This is an automated response to help guide you while an Ultralytics engineer reviews your question.\n\nPlease visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for step-by-step guides, including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** (MRE) showing your export process (code snippets, commands, and model files used). This will help us debug the issue more efficiently.\n\nIf you have a custom training â“ Question or are looking for specific pretrained weights, please share as much detail as possible, such as the exact model version, download source, and steps taken. Also confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will assist you soon! If you can, please update your issue with a reproducible example of your export workflow and clarify the exact model version and files you are comparing. Thank you for your patience and for helping improve YOLOv5! ðŸ˜Š"
      },
      {
        "user": "ashwinimate",
        "body": "as seen in the following image ther eis only one botttleneck in the c3 block in backbone. \nit should have been 3 bolltneck.\n\n![Image](https://github.com/user-attachments/assets/ce855a30-d94b-42ce-b4e8-aa79e3017d53)\n\n![Image](https://github.com/user-attachments/assets/f958bb22-2010-412e-8f10-7580e3fe2267)"
      },
      {
        "user": "pderrenger",
        "body": "Hello @ashwinimate,\n\nThe bottleneck counts you're observing in Netron (1, 2, 3, 1) are actually correct for the YOLOv5s v6.0 model. Each YOLOv5 size variant has a different number of bottlenecks in its C3 blocks:\n\n- YOLOv5s: 1, 2, 3, 1 bottlenecks\n- YOLOv5m: 2, 4, 6, 2 bottlenecks  \n- YOLOv5l: 3, 6, 9, 3 bottlenecks\n- YOLOv5x: 4, 8, 12, 4 bottlenecks\n\nThe \"3, 6, 9\" pattern you're referring to matches the YOLOv5l model, not YOLOv5s. Your download and export process was correct, and you have the proper weights for YOLOv5s v6.0."
      }
    ]
  },
  {
    "issue_number": 13496,
    "title": "Information needed for university project with yolov5",
    "author": "SDK1410",
    "state": "open",
    "created_at": "2025-01-22T11:34:38Z",
    "updated_at": "2025-05-12T16:31:34Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nGreetings.\nI am using a pretrained yolov5s model for lung nodules detection for a university project. \nI need to provide some basic information about the yolov5 model such as what are the **standard** \"activation functions (in hidden & output layers)\" and what type of \"optimizer\" is used. \nFurthermore, how many layers and neurons are used per layer?\nI have found some answers to this questions but they are not from reliable sources. So i cannot use them for my scientific paper, as i do not know if the provided information are correct. \n\nHope you can help me out.\nThank you in advance!\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @SDK1410, thank you for your interest in YOLOv5 ðŸš€! It's exciting to see YOLOv5 being utilized for a university project on lung nodule detection. Here's how you can proceed:\n\nFor detailed architecture-related questions like the ones you've asked (e.g., activation functions, optimizer types, layers, and neurons), we recommend exploring the YOLOv5 codebase itself for the most accurate and reliable information. The YOLOv5 model architecture and training setup are implemented in [models](https://github.com/ultralytics/yolov5/tree/master/models), [train.py](https://github.com/ultralytics/yolov5/blob/master/train.py), and [utils](https://github.com/ultralytics/yolov5/tree/master/utils). ðŸ“‚\n\nIf you're looking to validate implementation specifics:\n1. Review the main model configuration files in the [models directory](https://github.com/ultralytics/yolov5/tree/master/models). For `yolov5s`, specifically look at `yolov5s.yaml`.\n2. Check the training-related details in the [train.py script](https://github.com/ultralytics/yolov5/blob/master/train.py), especially optimizer configurations and learning rate schedules.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us debug it effectively.\n\nIf this is a â“ Question requiring further clarification or validation of scientific specifics, please ensure you've provided:\n- Detailed context on what you've reviewed so far.\n- Relevant examples or outputs from any experiments conducted using YOLOv5.\n\n## Resources\n\nFor further guidance, check out these helpful links:\n- ðŸŒŸ [YOLOv5 Tutorials](https://docs.ultralytics.com/yolov5/) for step-by-step custom training and advanced topics.\n- ðŸ”¥ [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/), which highlight critical insights on training and hyperparameter choices.\n- ðŸ“š [Documentation](https://docs.ultralytics.com/) for comprehensive coverage of Ultralytics and YOLOv5 topics.\n\n## Requirements\n\nEnsure your environment is set up correctly:\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). Use the following commands to get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Verified Environments\n\nFor quick experimentation, YOLOv5 can be run in these verified environments, fully pre-configured with dependencies:\n- **Notebooks with free GPU**: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\nIf the <a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a> badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are passing. These tests verify correct operation of all tasks including [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py).\n\nðŸ¤– This is an automated response to assist you as quickly as possible. An Ultralytics engineer will respond soon to provide further details or assistance!"
      },
      {
        "user": "pderrenger",
        "body": "@SDK1410 thank you for your detailed question! \n\nThe YOLOv5 model uses the **Leaky ReLU** activation function in its hidden layers and a **Sigmoid** activation function for the output layer in object detection tasks. Regarding the optimizer, YOLOv5 typically uses the **SGD (Stochastic Gradient Descent)** optimizer by default, but **Adam** is also supported and commonly used depending on the training configuration.\n\nFor specific details like the number of layers and neurons per layer, you can analyze the model directly using the `model.info()` method in YOLOv5. Here's an example:\n\n```python\nfrom ultralytics import YOLO\nmodel = YOLO('yolov5s.pt')  # Load pretrained YOLOv5s model\nmodel.info(detailed=True)   # Prints detailed layer and parameter information\n```\n\nThis will give you a comprehensive breakdown of the network architecture. For further technical details, I recommend referencing the [YOLOv5 repository](https://github.com/ultralytics/yolov5) or the official [Ultralytics documentation](https://docs.ultralytics.com/). Good luck with your project!"
      },
      {
        "user": "techieworld2",
        "body": "```\nfrom torchinfo import summary\nsummary(model.model, input_size=(1, 3, 640, 640))\n\n``` \n\nyou can use this also just make sure the size matches to your train data"
      }
    ]
  },
  {
    "issue_number": 11760,
    "title": "Multi-label classification yolov5",
    "author": "minhhotboy9x",
    "state": "closed",
    "created_at": "2023-06-23T06:12:00Z",
    "updated_at": "2025-05-11T15:06:33Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI have a dataset for multi-label classification, how can I set up yolov5-cls training for multi-label?\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @minhhotboy9x, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@minhhotboy9x hello! If you have a dataset for multi-label classification and would like to train yolov5-cls for multi-label tasks, you can follow these steps:\n\n1. Make sure your dataset is properly labeled with multiple labels for each image. Each label should be represented by a separate class index.\n\n2. Modify the `dataset.yaml` file in the `data` directory to include all the class names and their corresponding indices. Ensure that the `nc` parameter is set to the total number of classes in your dataset.\n\n3. Update the model configuration file (e.g., `yolov5s.yaml`, `yolov5m.yaml`, etc.) to use the classification head instead of the detection head. Set `classes` to the number of classes in your dataset.\n\n4. Train the model using the `train.py` script by specifying the appropriate `--img` size, `--batch-size`, and `--data` path to your modified `dataset.yaml` file. Additionally, set `--cfg` to the path of your modified model configuration file.\n\nBy following these steps, you should be able to train yolov5-cls for multi-label classification tasks.\n\nLet me know if you have any further questions!"
      },
      {
        "user": "minhhotboy9x",
        "body": "@glenn-jocher I watched an example on Roboflow and I saw that there is no need for a `dataset.yaml`, all I need is put images right on their folder labels. However, I wonder if my model will train data as single-label or multi-label. When I read `classify/train.py`, I saw that criterion model-cls uses is CrossEntropyLoss. Also, I see that there no `--cfg` in classify/train.py\r\n![image](https://github.com/ultralytics/yolov5/assets/85445183/e236c808-8da1-4342-ad3a-ffcd20fa0a50)\r\n"
      }
    ]
  },
  {
    "issue_number": 13418,
    "title": "Objectness loss, class loss and bounding box losses in YOLOv5",
    "author": "Shanky71",
    "state": "open",
    "created_at": "2024-11-18T10:16:53Z",
    "updated_at": "2025-05-10T10:14:34Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHey @glenn-jocher I hope you are doing well.  I have a small doubt about the objectness loss.\r\n\r\n- I have divided my data in 80% training, 15% testing and 5% validation. As shown in the below image, what can be the possible reason of the sudden spike in the initial part and also, why there is higher objectness loss  during validation than training. \r\n\r\n![image](https://github.com/user-attachments/assets/c51ea3d1-7725-4aa6-acfe-4a5d51e54f31)\r\n\r\n- Also out of the three losses (class, objectness, box) which loss is more important. If I consider only class loss doesn't it also account for objectness and box loss? \r\n\r\n@pderrenger @UltralyticsAssistant Can you please clarify it?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Shanky71, thank you for reaching out with your question about YOLOv5 ðŸš€! This is an automated response to help guide you, and an Ultralytics engineer will also assist soon.\n\nFor questions related to loss functions and their behavior during training and validation, providing a bit more context can be extremely helpful. If you have logs, specific configurations, or additional screenshots from your training, please share them to aid in diagnosing the issue.\n\nIf this is a ðŸ› Bug Report, please ensure you provide a **minimum reproducible example** so we can help you debug it accurately.\n\nIf you have questions about training specific to your dataset, please include as much information as possible, such as examples of your dataset images and any error logs. \n\nHere are a few things to check:\n- Have you ensured that your dataset and labels are formatted correctly?\n- Are you following the tips for the best training results? Sometimes tweaks in hyperparameters or batch sizes can affect loss curves significantly.\n- Is your validation dataset distinct and representative of your training data?\n\nRemember, the importance of each type of loss can depend on your specific use case. Generally, all three lossesâ€”class, objectness, and boxâ€”play crucial roles in effective model training. \n\nFeel free to provide more details, and we'll do our best to assist you further! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@Shanky71 the initial spike in objectness loss during validation could be due to the model adjusting to the dataset's characteristics early in training. Itâ€™s common to see higher losses in validation compared to training if thereâ€™s overfitting or variance in the validation set. Regarding importance, all three losses (class, objectness, box) are crucial as they each contribute to different aspects of learning: classifying objects, predicting their presence, and accurately locating them. Focusing solely on class loss would neglect the spatial precision and presence confidence provided by the other losses. For further insights, you may refer to the [YOLOv5 Loss Function documentation](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#5-loss-function) for a detailed explanation."
      },
      {
        "user": "Shanky71",
        "body": "Hey @pderrenger and @jkocherhans @timstokman @glenn-jocher ,I have a question related to trade-off between image resolution and performance . As observed from the below content that, inference time is decreasing with increase in resolution, which is counterintuitive. Can u help me with this. Is there any documentation regarding the same?\nThanks in advance\nin yolov5 object detection:\nSpeed:0.3 ms preprocess, 9.1 ms inference, 1.8 NMS per image at shape (1, 3, 640, 640)\nSpeed:0.4 ms preprocess, 8.3 ms inference, 1.1 NMS per image at shape (1, 3, 1024, 1024)\nSpeed:0.5 ms preprocess, 8.0 ms inference, 1.4 NMS per image at shape (1, 3, 2048, 2048)"
      }
    ]
  },
  {
    "issue_number": 13578,
    "title": "Use YOLOv5 'best.pt' file trained from a local Windows based PC on linux enviroment. Pathlib error",
    "author": "gabypc1991",
    "state": "closed",
    "created_at": "2025-04-27T00:41:35Z",
    "updated_at": "2025-05-01T23:11:00Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello.\n\nI trained the best.pt file and it was successful on a local Windows PC. Now i want work with inferences in another computer with this bets.pt file.\nThis error appears when i create the model with torch.hub.load\n\nThis test code works fine with the yolov5s.pt file:\n```\nimport torch\n\nmodel_path = './yolo/modelTrained/yolov5s.pt'\nmodel = torch.hub.load('./yolo/yolov5', 'custom', path=model_path,\n                                     source='local', force_reload=True)\n```\nTerminal output:\n```\nYOLOv5  v7.0-416-gfe1d4d99 Python-3.12.3 torch-2.6.0+cpu CPU\n\nFusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\nAdding AutoShape... \n```\n\nBut when i test with my best.pt file trained from another computer:\n```\nimport torch\n\nmodel_path = './yolo/modelTrained/best.pt'\nmodel = torch.hub.load('./yolo/yolov5', 'custom', path=model_path,\n                                     source='local', force_reload=True)\n```\nTerminal output:\n\n```\nYOLOv5  v7.0-416-gfe1d4d99 Python-3.12.3 torch-2.6.0+cpu CPU\n\nTraceback (most recent call last):\n  File \"\\yolo/yolov5\\hubconf.py\", line 85, in _create\n    model = attempt_load(path, device=device, fuse=False)  # arbitrary model\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\\yolo/yolov5\\models\\experimental.py\", line 98, in attempt_load\n    ckpt = torch.load(attempt_download(w), map_location=\"cpu\")  # load\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\utils\\patches.py\", line 114, in torch_load\n    return _torch_load(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py\", line 1471, in load\n    return _load(\n           ^^^^^^\n  File \"\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py\", line 1964, in _load\n    result = unpickler.load()\n             ^^^^^^^^^^^^^^^^\n  File \"\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py\", line 1953, in find_class\n    return super().find_class(mod_name, name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'pathlib._local'; 'pathlib' is not a package\n\nThe above exception was the direct cause of the following exception:\n\nException: No module named 'pathlib._local'; 'pathlib' is not a package. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.\n```\n\nThank you for any help.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @gabypc1991, thank you for reaching out and sharing your experience with YOLOv5 ðŸš€!\n\nThis is an automated response to help get you started. An Ultralytics engineer will review your issue and assist you soon.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** that includes the exact code, environment details, and steps to reproduce the error. This helps us troubleshoot more effectively.\n\nIf you have a custom training â“ question, please include as much detail as possible, such as dataset image examples and full training logs, and review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To install:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in the following verified environments (all with up-to-date dependencies such as [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation for YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nFor issues related to loading or transferring models between environments, you may find [this guide](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading) helpful.\n\nThank you again for your detailed report and for using YOLOv5! ðŸ¥‡"
      },
      {
        "user": "Y-T-G",
        "body": "You can add:\n\n```python\nimport os\nimport sys\nimport pathlib\n\nsys.modules[\"pathlib._local\"] = pathlib \nif os.name == 'nt':\n   pathlib.PosixPath = pathlib.WindowsPath\nelse:\n   pathlib.WindowsPath = pathlib.PosixPath\n```\n\nbefore your code"
      },
      {
        "user": "gabypc1991",
        "body": "> You can add:\n> \n> import os\n> import sys\n> import pathlib\n> \n> sys.modules[\"pathlib._local\"] = pathlib \n> if os.name == 'nt':\n>    pathlib.PosixPath = pathlib.WindowsPath\n> else:\n>    pathlib.WindowsPath = pathlib.PosixPath\n> \n> before your code\n\nThanks! it's work"
      }
    ]
  },
  {
    "issue_number": 13582,
    "title": "Retrain model by adding data",
    "author": "KnightInsight",
    "state": "open",
    "created_at": "2025-04-30T17:42:30Z",
    "updated_at": "2025-04-30T20:49:16Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi, I would like to retrain a model. Previously, it was trained on 1000 images. Now, I want to add one more image, but Iâ€™d prefer not to retrain the model using all 1001 images, as it takes a very long time to train. Is there any better solution?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @KnightInsight, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nðŸ¤– This is an automated response. An Ultralytics engineer will also assist you here soon!"
      },
      {
        "user": "pderrenger",
        "body": "Hi @KnightInsight,\n\nYou can use transfer learning to efficiently update your model with the new image. Start with your previously trained weights and run:\n\n```\npython train.py --weights path/to/previous/weights.pt --data your_data.yaml --epochs 10 --cache\n```\n\nYou can create a smaller YAML file containing just the new image or a small subset including it. Using fewer epochs (5-10) should be sufficient for incorporating the new information while leveraging your existing training.\n\nFor extremely efficient updates, consider using Ultralytics HUB's cloud training feature which offers data augmentation that could effectively multiply your single new image: https://www.ultralytics.com/blog/the-ultimate-guide-to-data-augmentation-in-2025\n\nLet me know if you need further assistance with this approach."
      }
    ]
  },
  {
    "issue_number": 10240,
    "title": "NotImplementedError: cannot instantiate 'PosixPath' on your system",
    "author": "VYRION-Ai",
    "state": "closed",
    "created_at": "2022-11-21T11:59:28Z",
    "updated_at": "2025-04-28T20:58:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\ni train classification and when i test it i got this error \r\n\r\n` raise NotImplementedError(\"cannot instantiate %r on your system\"\r\nNotImplementedError: cannot instantiate 'PosixPath' on your system\r\n`\r\n\r\nbut when i use yolov5n-cls.pt no error happened \n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@totoadel ðŸ‘‹ hi, thanks for letting us know about this possible problem with YOLOv5 ðŸš€. We've created a few short guidelines below to help users provide what we need in order to start investigating a possible problem.\r\n\r\n## How to create a Minimal, Reproducible Example\r\n\r\nWhen asking a question, people will be better able to provide help if you provide **code** that they can easily understand and use to **reproduce** the problem. This is referred to by community members as creating a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/). Your code that reproduces the problem should be:\r\n\r\n- âœ… **Minimal** â€“ Use as little code as possible to produce the problem\r\n- âœ… **Complete** â€“ Provide all parts someone else needs to reproduce the problem\r\n- âœ… **Reproducible** â€“ Test the code you're about to provide to make sure it reproduces the problem\r\n\r\nFor [Ultralytics](https://ultralytics.com/) to provide assistance your code should also be:\r\n\r\n- âœ… **Current** â€“ Verify that your code is up-to-date with GitHub [master](https://github.com/ultralytics/yolov5/tree/master), and if necessary `git pull` or `git clone` a new copy to ensure your problem has not already been solved in [master](https://github.com/ultralytics/yolov5/tree/master).\r\n- âœ… **Unmodified** â€“ Your problem must be reproducible using official YOLOv5 code without changes. [Ultralytics](https://ultralytics.com/) does not provide support for custom code âš ï¸.\r\n\r\nIf you believe your problem meets all the above criteria, please close this issue and raise a new one using the ðŸ› **Bug Report** [template](https://github.com/ultralytics/yolov5/issues/new/choose) with a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us better understand and diagnose your problem. \r\n\r\nThank you! ðŸ˜ƒ"
      },
      {
        "user": "V4A001",
        "body": "Suddenly, I run into exact the same situation. No idea how to get fore or backward with it. Did you get out of this situation?\r\n\r\n`\r\npython segment/predict.py --device cpu --weights best.pt --source data/images/bus.jpg\r\nsegment\\predict: weights=['best.pt'], source=data/images/bus.jpg, data=data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=cpu, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\predict-seg, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1, retina_masks=False\r\nYOLOv5  v7.0-56-gc0ca1d2 Python-3.10.8 torch-1.13.1+cpu CPU\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\myfolders\\yolov5\\segment\\predict.py\", line 284, in <module>\r\n    main(opt)\r\n  File \"D:\\myfolders\\yolov5\\segment\\predict.py\", line 279, in main\r\n    run(**vars(opt))\r\n  File \"C:\\Users\\me\\anaconda3\\envs\\v4a004\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"D:\\myfolders\\yolov5\\segment\\predict.py\", line 101, in run\r\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\r\n  File \"D:\\myfolders\\yolov5\\models\\common.py\", line 345, in __init__\r\n    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\r\n  File \"D:\\myfolders\\yolov5\\models\\experimental.py\", line 79, in attempt_load\r\n    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\r\n  File \"C:\\Users\\me\\anaconda3\\envs\\v4a004\\lib\\site-packages\\torch\\serialization.py\", line 789, in load\r\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\r\n  File \"C:\\Users\\me\\anaconda3\\envs\\v4a004\\lib\\site-packages\\torch\\serialization.py\", line 1131, in _load\r\n    result = unpickler.load()\r\n  File \"C:\\Users\\me\\anaconda3\\envs\\v4a004\\lib\\pathlib.py\", line 962, in __new__\r\n    raise NotImplementedError(\"cannot instantiate %r on your system\"\r\nNotImplementedError: cannot instantiate 'PosixPath' on your system\r\n`"
      },
      {
        "user": "akashAD98",
        "body": "@V4A001 @glenn-jocher  im also facing same issue with classification, on my local system using on cpu"
      }
    ]
  },
  {
    "issue_number": 13562,
    "title": "How can I increase my training speed when training a large training set with Yolov5?",
    "author": "Eliana-23",
    "state": "open",
    "created_at": "2025-04-16T09:51:54Z",
    "updated_at": "2025-04-28T18:31:37Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI'm using yolov5x to train on pictures with small flaws. The number of training sets is more than 2000, and it is usually enhanced 3 to 5 times. I want to make sure that I can detect small defects and improve the training speed as much as possible without changing the imgsz, but is there any good way to do this? My computer's graphics card is 3090.\n#13559 \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Eliana-23, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\n_This is an automated response. An Ultralytics engineer will assist you here soon!_ ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Hi @Eliana-23,\n\nTo improve training speed without changing image size, you can try:\n\n1. Use mixed precision training with `--half` flag to leverage your 3090's Tensor Cores\n2. Increase batch size (try 16, 32 or 64) for better GPU utilization\n3. Enable image caching with `--cache` to reduce I/O bottlenecks\n4. Consider using a slightly smaller model like YOLOv5l instead of YOLOv5x for a better speed/accuracy trade-off\n5. Enable multi-scale training (`--multi-scale`) which can help with small defect detection while maintaining speed\n\nFor even more performance, you could explore [Neural Magic's DeepSparse](https://www.ultralytics.com/blog/deploy-yolov5-with-neural-magics-deepsparse-for-gpu-class-performance-on-cpus) which provides GPU-class performance acceleration.\n\nLet me know if you have questions about implementing any of these optimizations."
      },
      {
        "user": "Eliana-23",
        "body": "Hello and thank you for your reply. At your suggestion, I enabled multi-size training and mixed-precision training. But the time has not become shorter, and the accuracy has not changed significantly, why is that? Is the number of my training sets too small?\nThis is the result of two training runs. 1. Mixed-precision and multi-scale training are not turned onï¼š\n300 epochs completed in 0.839 hours.\nOptimizer stripped from runs\\train\\exp49\\weights\\last.pt, 173.5MB\nOptimizer stripped from runs\\train\\exp49\\weights\\best.pt, 173.5MB\n\nValidating runs\\train\\exp49\\weights\\best.pt...\nFusing layers... \nYOLOv5x summary: 322 layers, 86388742 parameters, 0 gradients, 204.4 GFLOPs\n                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 00:00\n                   all         35         49      0.852      0.908       0.92      0.577\n                   YSA         35          8       0.95          1      0.995      0.298\n                    MX         35          4      0.603      0.405      0.413      0.297\n                    FS         35          1      0.932          1      0.995      0.697\n                  SJYS         35          1      0.718          1      0.995      0.895\n                  NQYS         35          2      0.823          1      0.995      0.696\n                  NQLD         35          4       0.91          1      0.995      0.732\n                  PTYS         35          1      0.763          1      0.995      0.597\n                  PTHX         35          1      0.916          1      0.995      0.597\n                  PTZW         35         10          1        0.9      0.962      0.414\n                  CRYS         35         12      0.925        0.5      0.629      0.305\n                 STZZD         35          2      0.858          1      0.995       0.25\n                    BY         35          1      0.772          1      0.995      0.895\n                 YJQCJ         35          2      0.906          1      0.995      0.821\nResults saved to runs\\train\\exp49\n2. After turning on mixed-precision and multi-scale training\n300 epochs completed in 1.046 hours.\nOptimizer stripped from runs\\train\\exp50\\weights\\last.pt, 173.5MB\nOptimizer stripped from runs\\train\\exp50\\weights\\best.pt, 173.5MB\n\nValidating runs\\train\\exp50\\weights\\best.pt...\nFusing layers... \nYOLOv5x summary: 322 layers, 86388742 parameters, 0 gradients, 204.4 GFLOPs\n                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 00:00\n                   all         35         49      0.736      0.804      0.886      0.455\n                   YSA         35          8      0.918          1      0.995      0.303\n                    MX         35          4      0.426       0.25      0.262      0.127\n                    FS         35          1          1          0      0.995      0.398\n                  SJYS         35          1      0.645          1      0.995      0.895\n                  NQYS         35          2      0.311      0.703      0.695      0.526\n                  NQLD         35          4      0.756          1      0.995      0.697\n                  PTYS         35          1      0.903          1      0.995      0.129\n                  PTHX         35          1      0.784          1      0.995     0.0995\n                  PTZW         35         10      0.873          1      0.995      0.314\n                  CRYS         35         12      0.789        0.5      0.613      0.235\n                 STZZD         35          2      0.816          1      0.995      0.448\n                    BY         35          1      0.624          1      0.995      0.895\n                 YJQCJ         35          2      0.728          1      0.995      0.846\nResults saved to runs\\train\\exp50"
      }
    ]
  },
  {
    "issue_number": 13579,
    "title": "\"What libraries should I install to use PyTorch and YOLOv5? I am using PyCharm and Visual Studio.\"",
    "author": "joseant7231",
    "state": "open",
    "created_at": "2025-04-27T17:09:46Z",
    "updated_at": "2025-04-28T05:35:52Z",
    "labels": [
      "question",
      "dependencies"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nGood morning, good afternoon, or good evening. I am using Visual Studio Code and also PyCharm with the Python language. I would like to perform image recognition on a video, but I am encountering errors with Torch and YOLOv5. Could you please help me by telling me which libraries I should install first? Thank you very much for your possible response.\n\n\n\n### Additional\n\n![Image](https://github.com/user-attachments/assets/6db9e84d-0ae2-421f-83f5-08e92d55f5d6)",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @joseant7231, thank you for your interest in YOLOv5 ðŸš€! This is an automated response to help you get started quicklyâ€”an Ultralytics engineer will also assist you soon.\n\nPlease visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for detailed guidesâ€”including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please include as much information as possible, such as dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nðŸ˜Š If you encounter further issues, please provide additional details or screenshots to help us assist you!"
      },
      {
        "user": "pderrenger",
        "body": "Hi @joseant7231,\n\nTo use YOLOv5 for video recognition, install the required dependencies with:\n\n```bash\npip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n```\n\nThis installs PyTorch and all necessary libraries. For video processing specifically, ensure OpenCV is properly installed:\n\n```bash\npip install opencv-python-headless\n```\n\nYou can also clone the repository and install everything at once:\n\n```bash\ngit clone https://github.com/ultralytics/yolov5\ncd yolov5\npip install -r requirements.txt\n```\n\nFor detailed setup instructions, check our [Quickstart guide](https://docs.ultralytics.com/quickstart/)."
      }
    ]
  },
  {
    "issue_number": 13576,
    "title": "How to find Average Precision (AP) for each class or category in YOLO-based models such as YOLOv5, YOLOv8, YOLOv11, or YOLOv12?",
    "author": "Shahriar-RL",
    "state": "closed",
    "created_at": "2025-04-26T13:10:57Z",
    "updated_at": "2025-04-27T00:25:37Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [ ] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nDuring training, Iâ€™d like to automatically display the Average Precision (AP), Precision (P), and Recall (R) for each detection category in every training epoch. What modifications to the code would enable this functionality?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Shahriar-RL, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response ðŸ¤–. An Ultralytics engineer will review your question and assist you here soon!"
      },
      {
        "user": "pderrenger",
        "body": "Hi @Shahriar-RL,\n\nYOLOv5 already calculates per-class metrics during validation, but they're not displayed in the default training output. You can access these metrics in several ways:\n\n1. Check the validation results in `runs/train/exp*/results.txt` which contains per-class AP values\n\n2. In your training loop, you can modify `test.py` or use the callback system to print these values during training\n\n3. For real-time visualization, integrate with experiment tracking tools like ClearML or W&B that automatically chart per-class metrics\n\nIf you need more granular control, the metrics are calculated in `utils/metrics.py` where you can add custom logging for your specific needs.\n\nWould you like a specific code example for any of these approaches?"
      }
    ]
  },
  {
    "issue_number": 13577,
    "title": "How to find Average Precision (AP) for each class or category",
    "author": "Shahriar-RL",
    "state": "open",
    "created_at": "2025-04-26T13:13:26Z",
    "updated_at": "2025-04-26T22:07:12Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n\nDuring training, Iâ€™d like to automatically display the Average Precision (AP), Precision (P), and Recall (R) for each detection category in every training epoch in YOLO based models like YOLOv5, YOLOv8, YOLOv11 or YOLOv12. What modifications to the code would enable this functionality?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Shahriar-RL, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and advanced topics such as [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much detail as possible, including dataset image examples, training logs, and confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response ðŸ¤–. An Ultralytics engineer will review your question and provide further assistance soon!"
      },
      {
        "user": "pderrenger",
        "body": "Hi @Shahriar-RL,\n\nYOLOv5 already calculates per-class metrics during validation, but only displays the mean values by default. To display individual class metrics during training, you can modify the `val.py` script.\n\nLook for the `Class` table printing in the `validate()` function and modify the logging section to print these values after each validation phase. Specifically:\n\n```python\n# Near line 636 in val.py where class metrics are calculated\nfor i, c in enumerate(ap_class):\n    LOGGER.info(f'Class {names[c]} - AP: {ap[i]:.4f}, Precision: {p[i]:.4f}, Recall: {r[i]:.4f}')\n```\n\nAlternatively, you can access these metrics via the callbacks system or by analyzing the `results.csv` file in the runs folder which contains this information.\n\nFor YOLOv8 and newer models, the implementation is different but follows similar principles within the Ultralytics package."
      }
    ]
  },
  {
    "issue_number": 12014,
    "title": "Deep learning problems (solved)",
    "author": "Mixisa",
    "state": "closed",
    "created_at": "2023-08-21T22:32:04Z",
    "updated_at": "2025-04-26T00:40:43Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [ ] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nMy problem is I cannot command the deep learning process to start. I watched / imitate a tutorial video on https://www.youtube.com/watch?v=0efnQCHbsyE&t=2219s (Stop Time : 49:40 Min) to complete my goal but sadly im still not done yet. And now I need some advices or solutions to go any further (Please considerate that im an amateur).\r\n\r\n#My command :\r\n\r\n`!cd yolov5 && python train.py --img 320 --batch 16 --epochs 2000 --data dataset.yml --hyp hyp.scratch.yaml --weights yolov5s.pt --workers  2`\r\n\r\n#Path of the problems:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\Desktop\\Timber(B)\\yolov5\\train.py\", line 72, in <module>\r\n    GIT_INFO = check_git_info()\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 81, in inner\r\n    return func(*args, **kwds)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Administrator\\Desktop\\Timber(B)\\yolov5\\utils\\general.py\", line 370, in check_git_info\r\n    import git\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\git\\__init__.py\", line 8, in <module>\r\n    from git._exc import *  # @NoMove @IgnorePep8\r\n    ^^^^^^^^^^^^^^^^^^^^^^\r\nModuleNotFoundError: No module named 'git._exc'\r\n```\r\n\r\n\r\nP.S\r\nI work with Python+Jupyter notebook.\r\nI already installed and reinstalled Git.\r\n\r\n\r\nThanks for you answer !",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @Mixisa, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@Mixisa this error seems to be caused by a missing dependency. Please try installing the `GitPython` package by running `!pip install gitpython` in your Jupyter notebook. After installing `GitPython`, try running your command again.\n\nIf the issue persists, it may be helpful to specify which version of Python and Jupyter notebook you are using, as well as confirming that you have installed all the required packages specified in the YOLOv5 repository's `requirements.txt` file.\n\nLet me know if you have any further questions or need additional assistance!"
      },
      {
        "user": "Mixisa",
        "body": "@glenn-jocher thanks for you answer ! I already installed `Gitpython` in my Jupyter notebook. ( `requirements.txt ` too)  \r\nMy Python version is 3.11.4\r\nJupyter notebook version is 7.0.2\r\n\r\n![Gitpython](https://github.com/ultralytics/yolov5/assets/142846331/3d7e51c2-94c9-46c1-acb5-287a9880eee6)\r\n![Requirements](https://github.com/ultralytics/yolov5/assets/142846331/881b8d1c-1afd-46cf-999a-da0de51e4031)\r\n\r\nIt's still not working"
      }
    ]
  },
  {
    "issue_number": 13574,
    "title": "[BUG]TypeError: cannot pickle 'torch._C._distributed_c10d.ProcessGroup' object",
    "author": "xiaoxiaodecheng",
    "state": "open",
    "created_at": "2025-04-24T12:53:26Z",
    "updated_at": "2025-04-25T01:47:22Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am using DeepSpeed to accelerate YOLOv5 training and have added the corresponding steps in the training file\nBelow is the code I added to the training file and my DeepSpeed configuration file\n\nmodel,optimizer,_,_ = deepspeed.initialize( args=opt, model=model, optimizer=optimizer, model_parameters=model.parameters(), config=opt.deepspeed_config_file )\n\nds_config.json\n\n { \"train_batch_size\": 16, \"gradient_accumulation_steps\": 1, \"zero_optimization\": { \"stage\": 0 }, \"zero_allow_untested_optimizer\": true }\n\nHowever, I encountered the following error during execution and am not sure how to resolve it\n\nep_module_on_host=False replace_with_kernel_inject=False\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] timers_config ................ enabled=True synchronized=True\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] train_batch_size ............. 16\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] train_micro_batch_size_per_gpu 16\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] use_data_before_expert_parallel_ False\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] use_node_local_storage ....... False\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] wall_clock_breakdown ......... False\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] weight_quantization_config ... None\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] world_size ................... 1\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] zero_allow_untested_optimizer True\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] zero_enabled ................. False\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] zero_force_ds_cpu_optimizer .. True\n[2025-04-24 20:06:55,076] [INFO] [config.py:1007:print] zero_optimization_stage ...... 0\n[2025-04-24 20:06:55,076] [INFO] [config.py:993:print_user_config] json = {\n\"train_batch_size\": 16,\n\"gradient_accumulation_steps\": 1,\n\"zero_optimization\": {\n\"stage\": 0\n},\n\"zero_allow_untested_optimizer\": true\n}\nTraceback (most recent call last):\nFile \"/home/admslc/code/yolov5/train.py\", line 986, in\nmain(opt)\nFile \"/home/admslc/code/yolov5/train.py\", line 854, in main\ntrain(opt.hyp, opt, device, callbacks)\nFile \"/home/admslc/code/yolov5/train.py\", line 260, in train\nema = ModelEMA(model) if RANK in {-1, 0} else None\nFile \"/home/admslc/code/yolov5/utils/torch_utils.py\", line 412, in init\nself.ema = deepcopy(de_parallel(model)).eval() # FP32 EMA\nFile \"/home/admslc/anaconda3/envs/env-py310/lib/python3.10/copy.py\", line 172, in deepcopy\ny = _reconstruct(x, memo, *rv)\nFile \"/home/admslc/anaconda3/envs/env-py310/lib/python3.10/copy.py\", line 271, in _reconstruct\nstate = deepcopy(state, memo)\nFile \"/home/admslc/anaconda3/envs/env-py310/lib/python3.10/copy.py\", line 146, in deepcopy\ny = copier(x, memo)\nFile \"/home/admslc/anaconda3/envs/env-py310/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\ny[deepcopy(key, memo)] = deepcopy(value, memo)\nFile \"/home/admslc/anaconda3/envs/env-py310/lib/python3.10/copy.py\", line 161, in deepcopy\nrv = reductor(4)\nTypeError: cannot pickle 'torch._C._distributed_c10d.ProcessGroup' object\n[2025-04-24 20:06:54,411] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 149527\n\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @xiaoxiaodecheng, thank you for reporting this issue with YOLOv5 and DeepSpeed integration ðŸš€! This is an automated response to help get your issue addressed as quickly as possible. An Ultralytics engineer will review your report and assist you soon.\n\nTo better assist you, could you please provide a **minimum reproducible example** (MRE) that demonstrates the error? This helps us replicate the issue and provide a more accurate solution.\n\nIn the meantime, please review the following resources to ensure your setup aligns with our recommendations:\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in any of the following verified, up-to-date environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nIf you have not already, please ensure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nThank you for your patience and for helping improve YOLOv5! ðŸ› ï¸"
      },
      {
        "user": "pderrenger",
        "body": "Hi @xiaoxiaodecheng,\n\nThis error occurs because the DeepSpeed-wrapped model contains non-picklable objects (specifically the `ProcessGroup`) which can't be deep-copied when creating the ModelEMA. \n\nTo fix this, you need to modify your code to extract the underlying model before creating the EMA:\n\n```python\n# Instead of this:\nema = ModelEMA(model) if RANK in {-1, 0} else None\n\n# Use this:\nif RANK in {-1, 0}:\n    # Get the underlying model from DeepSpeed's engine\n    unwrapped_model = model.module if hasattr(model, \"module\") else model\n    ema = ModelEMA(unwrapped_model)\nelse:\n    ema = None\n```\n\nThis extracts the base model without the non-picklable DeepSpeed components, allowing ModelEMA to work correctly. You might also need similar adjustments in other parts of the code where model deep copying happens."
      }
    ]
  },
  {
    "issue_number": 13572,
    "title": "Why is RGB used during training but BGR during inference in classification dataset?",
    "author": "devatwiai",
    "state": "closed",
    "created_at": "2025-04-24T05:05:31Z",
    "updated_at": "2025-04-24T12:00:21Z",
    "labels": [
      "question",
      "classify"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nIn `utils/dataloaders.py`, `ClassificationDataset`, images are converted to RGB only when `album_transforms` is used during training:\n\n```python\nsample = self.album_transforms(image=cv2.cvtColor(im, cv2.COLOR_BGR2RGB))[\"image\"]\n```\nHowever, during inference, the image remains in OpenCVâ€™s default BGR format:\n```python\nsample = self.torch_transforms(im)\n```\nWouldn't this cause issues? Also, normalization is done using RGB mean and std values for both.\nIs there a specific reason for not applying the same RGB conversion across both training and inference?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @devatwiai, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nðŸ¤– This is an automated response. An Ultralytics engineer will review your question and assist you soon!"
      },
      {
        "user": "Y-T-G",
        "body": "`classify_transforms` has `ToTensor()` which converts to RGB\n\nhttps://github.com/ultralytics/yolov5/blob/fe1d4d9947735473006c68513168fef093ff17ce/utils/augmentations.py#L370\n\nhttps://github.com/ultralytics/yolov5/blob/fe1d4d9947735473006c68513168fef093ff17ce/utils/augmentations.py#L436"
      },
      {
        "user": "devatwiai",
        "body": "@Y-T-G Oh, I thought we were using`torchvision.tranforms.ToTensor()`. Thanks for the clarification."
      }
    ]
  },
  {
    "issue_number": 13568,
    "title": "Why is there a difference between the model created by ultralytics.YOLO and the one loaded via torch.hub.load?",
    "author": "Felrix",
    "state": "closed",
    "created_at": "2025-04-19T10:00:45Z",
    "updated_at": "2025-04-24T11:49:40Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n![Image](https://github.com/user-attachments/assets/f797d7ca-958a-4f74-9142-ccfad2879f20)\nThe model created by ultralytics.YOLO has 4 dimensions, while the model loaded via torch.hub.load has 5 dimensions.\n\ncustom.yaml:\n```yaml\n# Parameters\nnc: 11 # number of classes\nch: 4\n\ndepth_multiple: 0.33 # model depth multiple\nwidth_multiple: 0.50 # layer channel multiple\n\n# YOLOv5 v6.0 backbone\nbackbone:\n  # [from, number, module, args]\n  - [-1, 1, Conv, [64, 6, 2, 2]] # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n  - [-1, 3, C3, [128]]\n  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n  - [-1, 6, C3, [256]]\n  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n  - [-1, 9, C3, [512]]\n  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n  - [-1, 3, C3, [1024]]\n  - [-1, 1, SPPF, [1024, 5]] # 9\n\n# YOLOv5 v6.0 head\nhead:\n  - [-1, 1, Conv, [512, 1, 1]]\n  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n  - [-1, 3, C3, [512, False]] # 13\n\n  - [-1, 1, Conv, [256, 1, 1]]\n  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n  - [-1, 3, C3, [256, False]] # 17 (P3/8-small)\n\n  - [-1, 1, Conv, [256, 3, 2]]\n  - [[-1, 14], 1, Concat, [1]] # cat head P4\n  - [-1, 3, C3, [512, False]] # 20 (P4/16-medium)\n\n  - [-1, 1, Conv, [512, 3, 2]]\n  - [[-1, 10], 1, Concat, [1]] # cat head P5\n  - [-1, 3, C3, [1024, False]] # 23 (P5/32-large)\n\n  - [[17, 20, 23], 1, Detect, [nc]] # Detect(P3, P4, P5)\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Felrix, thank you for your interest in YOLOv5 ðŸš€! This is an automated response to help you get started, and an Ultralytics engineer will also assist you soon.\n\nPlease visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for quickstart guides on tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and advanced features like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us debug the issue, including code snippets, environment details, and sample inputs if possible.\n\nIf this is a custom training â“ Question, please include as much information as possible, such as dataset image examples and training logs, and ensure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience! An Ultralytics engineer will review your question and assist you further soon. ðŸ› ï¸"
      },
      {
        "user": "pderrenger",
        "body": "Hi @Felrix,\n\nThis is an expected difference between the two loading methods. The `ultralytics.YOLO` method (from the ultralytics package) and `torch.hub.load` (original YOLOv5 repo) have slightly different implementations:\n\n1. The `ultralytics.YOLO` object returns tensors with shape [batch_size, num_classes+5, predictions] where the middle dimension contains the box coordinates, confidence, and class probabilities.\n\n2. The `torch.hub.load` method returns tensors with shape [batch_size, predictions, num_classes+5], effectively transposing those last two dimensions.\n\nBoth implementations are functionally equivalent but organized differently in memory. When processing results, you'll need to adjust your post-processing code according to which loading method you're using.\n\nIf you need consistent behavior between implementations, I'd recommend standardizing on one method throughout your project."
      }
    ]
  },
  {
    "issue_number": 13573,
    "title": "Hope a DyT Try",
    "author": "Chenpeel",
    "state": "open",
    "created_at": "2025-04-24T05:18:35Z",
    "updated_at": "2025-04-24T11:44:23Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "body": "I came across a paper suggesting DyT functions could replace traditional normalization methods. Surprisingly, using this normalization-free approach actually boosted performance in certain models. Does this sound practical enough for us to test it out?\n\n[Paper URL](https://arxiv.org/abs/2503.10622)",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Chenpeel, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for quickstart guides and advanced concepts, including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question or feature suggestion, please provide as much information as possible, including links to relevant research (like the one you shared), implementation details, and any initial experiments or logs if available. Also, verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response ðŸ¤– â€” an Ultralytics engineer will review your suggestion and assist you soon!"
      },
      {
        "user": "Y-T-G",
        "body": "Isn't this for transformers?"
      }
    ]
  },
  {
    "issue_number": 1665,
    "title": "Confusion Matrix weird results?",
    "author": "a-esp-1",
    "state": "closed",
    "created_at": "2020-12-10T16:18:37Z",
    "updated_at": "2025-04-24T06:51:16Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "## â”Question\r\nHi,\r\n\r\nI've seen you have update the repo (#1474) and add the Confusion Matrix. When I run it with my own dataset I get this Confusion Matrix:\r\n![confusion_matrix](https://user-images.githubusercontent.com/73425046/101797425-519eb780-3b0a-11eb-9fd9-177a28bd3552.png)\r\n\r\nMy mAP is 0.84 for this dataset, and for example for Airplane class, AP = 0.95.\r\n![ap](https://user-images.githubusercontent.com/73425046/101798045-f1f4dc00-3b0a-11eb-839d-750583a53ea5.PNG)\r\n\r\n\r\nSo I can't understand how is possible that in the Confusion Matrix [row=Airplane, column=Airplane] = 0.26, [row=Airplane, column=FN] = 0 and [row=FP, column=Airplane] = 0.73.\r\n\r\nAs I can understand this means that just 26% of detections are correct (TP), but it's not possible because AP(Airplane) = 0.95 and when I run detections Airplanes are well detected.\r\n\r\n## Additional context\r\nI'm getting this weird result not just with this dataset, if not with other 2.",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@a-esp-1 in object detection the confusion matrix is unfortunately somewhat uninformative, as the majority of mistakes will be with the background class rather than another class.\r\n\r\nConfusion matrix implementation is correct. Results will vary based on the confidence threshold however. As this threshold trends to 1.0 background FPs will trend towards zero. See https://github.com/kaanakan/object_detection_confusion_matrix/issues/7 for a more detailed discussion of this."
      },
      {
        "user": "glenn-jocher",
        "body": "@a-esp-1 the confusion matrix conf threshold is here. The default value is 0.25, which is aligned with the default inference setting in detect.py and the model autoshape wrapper.\r\nhttps://github.com/ultralytics/yolov5/blob/b2bef8f6d8e4c008bae72c211a186d75732fc213/utils/metrics.py#L107-L109\r\n\r\nSo theoretically the confusion matrix result should align with the default detect.py result. Are you seeing something different?\r\n"
      },
      {
        "user": "a-esp-1",
        "body": "I see. I might change conf threshold because in my detection I used 0.5, and here it is set to 0.25. I'm not complety sure but I think iou_thres was set to 0.65 (but I'm not 100% sure).\r\n\r\nThank you."
      }
    ]
  },
  {
    "issue_number": 13571,
    "title": "Val stage only use single GPU?",
    "author": "ylfeng7911",
    "state": "closed",
    "created_at": "2025-04-22T08:11:17Z",
    "updated_at": "2025-04-23T22:00:11Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nWhen I trained yolov5s with 4 1080Ti cards, I found that the speed of the training stage was much faster than that of the validation stage. Observing the source code of the validation in train.py, I think val_loader only use one GPU.\n`# Process 0\nif RANK in {-1, 0}:\nval_loader = create_dataloader(\nval_path,\nimgsz,\nbatch_size // WORLD_SIZE * 2,\ngs,\nsingle_cls,\nhyp=hyp,\ncache=None if noval else opt.cache,\nrect=True,\nrank=-1,\nworkers=workers * 2,\npad=0.5,\nprefix=colorstr(\"val: \"),\n)[0]`\nCan I use 4 Gpu for validation to speed up ï¼Ÿ\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @ylfeng7911, thank you for your question and for using YOLOv5 ðŸš€! This is an automated response to help get you startedâ€”an Ultralytics engineer will also review your issue soon.\n\nPlease visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for comprehensive guides, including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it. For questions like yours, sharing training logs, config details, and any code modifications can really help us assist you faster.\n\nAlso, check out our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) for optimization advice.\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). Quickstart:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 runs in these up-to-date environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience! An Ultralytics engineer will assist you here soon ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Hi @ylfeng7911,\n\nYou've correctly identified that validation in YOLOv5 currently runs only on a single GPU (rank 0). This is by design in the distributed training implementation.\n\nIn the current architecture, validation is not distributed across multiple GPUs for several reasons:\n1. It ensures consistent, deterministic validation results\n2. It simplifies metric aggregation and tracking\n3. It avoids synchronization overhead between GPUs for mAP calculations\n\nIf validation is becoming a bottleneck in your workflow, you could consider:\n- Reducing validation frequency with `--val-period n` to validate every n epochs\n- Using a smaller validation dataset during development\n- Increasing the validation batch size if your GPU memory allows\n\nImplementing multi-GPU validation would require significant changes to the codebase, particularly around metric computation and aggregation. It's something that could be considered for future versions, but isn't currently supported.\n\nLet me know if you have any other questions!"
      },
      {
        "user": "ylfeng7911",
        "body": "I get it, thank u!"
      }
    ]
  },
  {
    "issue_number": 13570,
    "title": "How to set up the code environment of yolov5 with a 5070ti graphics card (cuda12.8) ?",
    "author": "VansamaOffical",
    "state": "open",
    "created_at": "2025-04-20T15:06:13Z",
    "updated_at": "2025-04-20T18:19:38Z",
    "labels": [
      "question",
      "dependencies"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n5070ti has to use cuda12.8 and pytorch from this link:\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\nAfter installing and setting, pytorch is available( torch.cuda.is_available()==true ).\nHowever, there is a problem when I run detect.py:\nfrom utils.downloads import attempt_download          ModuleNotFoundError: No module named 'utils'\nBut \"utils\" has been provided already.\nWhat goes wrong? Maybe requirements.txt needs changing with new cuda environment?\nPlease give me instructions.\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @VansamaOffical, thank you for reaching out and for your interest in YOLOv5 ðŸš€! This is an automated response to help get you started. An Ultralytics engineer will review your issue and assist you further soon.\n\nFor setup instructions and troubleshooting, please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/), including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and [Model Training Tips](https://docs.ultralytics.com/guides/model-training-tips/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** (MRE) that demonstrates the issue. This helps us debug more efficiently.\n\nIf youâ€™re setting up with a custom CUDA/PyTorch environment, ensure you have installed all dependencies from [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\nAdditionally, please double-check that you are running scripts from the project root directory so relative imports (like `from utils.downloads import attempt_download`) work as expected.\n\n## Verified Environments\n\nYOLOv5 can be run in any of the following verified environments (all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks**: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud**: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon**: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are passing. CI tests verify correct operation of [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu.\n\nIf you continue experiencing issues, please provide:\n- The exact command you are running\n- A minimum reproducible example\n- Any relevant logs or stack traces\n\nThis will help us assist you more effectively! Thank you for your patience and for using YOLOv5 ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Hi @VansamaOffical,\n\nThis error typically occurs when running the script from the wrong directory. Make sure you're running `detect.py` from the root directory of the YOLOv5 repository. If you're using a terminal, navigate to the YOLOv5 directory first with `cd path/to/yolov5` before executing.\n\nIf that doesn't solve it, try installing YOLOv5 as a package with:\n```bash\ngit clone https://github.com/ultralytics/yolov5\ncd yolov5\npip install -e .\n```\n\nThe latest YOLOv5 should be compatible with newer CUDA versions through PyTorch, though you might need to update other dependencies. If problems persist, you might consider using CUDA 11.8 as it's more widely tested with YOLOv5.\n\nLet me know if this resolves your issue."
      }
    ]
  },
  {
    "issue_number": 13462,
    "title": "UnicodeDecodeError: 'gbk' codec can't decode byte 0x80 in position 233: illegal multibyte sequence",
    "author": "zhangsiying2001",
    "state": "open",
    "created_at": "2024-12-16T17:13:08Z",
    "updated_at": "2025-04-20T02:52:14Z",
    "labels": [
      "bug",
      "devops"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\nD:\\0_yyyzt\\AIM\\yolo\\yolov5> python train.py --weights yolov5s.pt --epochs 300 --batch-size 16 --workers 8 --data   D:\\0_yyyzt\\AIM\\yolo\\datasets\\zhengtu\\zhengtu.yaml\r\ntrain: weights=yolov5s.pt, cfg=, data=D:\\0_yyyzt\\AIM\\yolo\\datasets\\zhengtu\\zhengtu.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=300, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\r\ngithub: up to date with https://github.com/ultralytics/yolov5\r\nYOLOv5  v7.0-389-ge62a31b6 Python-3.11.9 torch-2.5.1+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\r\nCOMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\r\nCOMET INFO: Using 'D:\\\\0_yyyzt\\\\AIM\\\\yolo\\\\yolov5\\\\.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\r\nTraceback (most recent call last):\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\train.py\", line 986, in <module>\r\n    main(opt)\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\train.py\", line 688, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\train.py\", line 180, in train\r\n    loggers = Loggers(\r\n              ^^^^^^^^\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\utils\\loggers\\__init__.py\", line 153, in __init__\r\n    self.comet_logger = CometLogger(self.opt, self.hyp)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\utils\\loggers\\comet\\__init__.py\", line 102, in __init__\r\n    self.data_dict = self.check_dataset(self.opt.data)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\utils\\loggers\\comet\\__init__.py\", line 252, in check_dataset\r\n    data_config = yaml.safe_load(f)\r\n                  ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\__init__.py\", line 125, in safe_load\r\n    return load(stream, SafeLoader)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\__init__.py\", line 79, in load\r\n    loader = Loader(stream)\r\n             ^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\loader.py\", line 34, in __init__\r\n    Reader.__init__(self, stream)\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\reader.py\", line 85, in __init__\r\n    self.determine_encoding()\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\reader.py\", line 124, in determine_encoding\r\n    self.update_raw()\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\reader.py\", line 178, in update_raw\r\n    data = self.stream.read(size)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x80 in position 233: illegal multibyte sequence\r\nCOMET INFO: The process of logging environment details (conda environment, git patch) is underway. Please be patient as this may take some time.\r\nCOMET INFO: ---------------------------------------------------------------------------------------\r\nCOMET INFO: Comet.ml OfflineExperiment Summary\r\nCOMET INFO: ---------------------------------------------------------------------------------------\r\nCOMET INFO:   Data:\r\nCOMET INFO:     display_summary_level : 1\r\nCOMET INFO:     name                  : exp\r\nCOMET INFO:     url                   : [OfflineExperiment will get URL after upload]\r\nCOMET INFO:   Others:\r\nCOMET INFO:     Name               : exp\r\nCOMET INFO:     offline_experiment : True\r\nCOMET INFO:   Uploads:\r\nCOMET INFO:     environment details : 1\r\nCOMET INFO:     git metadata        : 1\r\nCOMET INFO:     installed packages  : 1\r\nCOMET INFO:\r\nCOMET INFO: Still saving offline stats to messages file before program termination (may take up to 120 seconds)\r\nCOMET INFO: Begin archiving the offline data.\r\nCOMET INFO: To upload this offline experiment, run:\r\n    comet upload D:\\0_yyyzt\\AIM\\yolo\\yolov5\\.cometml-runs\\e9b6f27f962c4a25bfeb02399ccf699f.zip\n\n### Environment\n\nOS=win py=3.11.9  use=cpu NoUse NVIDIA\n\n### Minimal Reproducible Example\n\nD:\\0_yyyzt\\AIM\\yolo\\yolov5> python train.py --weights yolov5s.pt --epochs 300 --batch-size 16 --workers 8 --data   D:\\0_yyyzt\\AIM\\yolo\\datasets\\zhengtu\\zhengtu.yaml\r\ntrain: weights=yolov5s.pt, cfg=, data=D:\\0_yyyzt\\AIM\\yolo\\datasets\\zhengtu\\zhengtu.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=300, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\r\ngithub: up to date with https://github.com/ultralytics/yolov5\r\nYOLOv5  v7.0-389-ge62a31b6 Python-3.11.9 torch-2.5.1+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\r\nCOMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\r\nCOMET INFO: Using 'D:\\\\0_yyyzt\\\\AIM\\\\yolo\\\\yolov5\\\\.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\r\nTraceback (most recent call last):\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\train.py\", line 986, in <module>\r\n    main(opt)\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\train.py\", line 688, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\train.py\", line 180, in train\r\n    loggers = Loggers(\r\n              ^^^^^^^^\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\utils\\loggers\\__init__.py\", line 153, in __init__\r\n    self.comet_logger = CometLogger(self.opt, self.hyp)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\utils\\loggers\\comet\\__init__.py\", line 102, in __init__\r\n    self.data_dict = self.check_dataset(self.opt.data)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\utils\\loggers\\comet\\__init__.py\", line 252, in check_dataset\r\n    data_config = yaml.safe_load(f)\r\n                  ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\__init__.py\", line 125, in safe_load\r\n    return load(stream, SafeLoader)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\__init__.py\", line 79, in load\r\n    loader = Loader(stream)\r\n             ^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\loader.py\", line 34, in __init__\r\n    Reader.__init__(self, stream)\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\reader.py\", line 85, in __init__\r\n    self.determine_encoding()\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\reader.py\", line 124, in determine_encoding\r\n    self.update_raw()\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\reader.py\", line 178, in update_raw\r\n    data = self.stream.read(size)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x80 in position 233: illegal multibyte sequence\r\nCOMET INFO: The process of logging environment details (conda environment, git patch) is underway. Please be patient as this may take some time.\r\nCOMET INFO: ---------------------------------------------------------------------------------------\r\nCOMET INFO: Comet.ml OfflineExperiment Summary\r\nCOMET INFO: ---------------------------------------------------------------------------------------\r\nCOMET INFO:   Data:\r\nCOMET INFO:     display_summary_level : 1\r\nCOMET INFO:     name                  : exp\r\nCOMET INFO:     url                   : [OfflineExperiment will get URL after upload]\r\nCOMET INFO:   Others:\r\nCOMET INFO:     Name               : exp\r\nCOMET INFO:     offline_experiment : True\r\nCOMET INFO:   Uploads:\r\nCOMET INFO:     environment details : 1\r\nCOMET INFO:     git metadata        : 1\r\nCOMET INFO:     installed packages  : 1\r\nCOMET INFO:\r\nCOMET INFO: Still saving offline stats to messages file before program termination (may take up to 120 seconds)\r\nCOMET INFO: Begin archiving the offline data.\r\nCOMET INFO: To upload this offline experiment, run:\r\n    comet upload D:\\0_yyyzt\\AIM\\yolo\\yolov5\\.cometml-runs\\e9b6f27f962c4a25bfeb02399ccf699f.zip\n\n### Additional\n\nD:\\0_yyyzt\\AIM\\yolo\\yolov5> python train.py --weights yolov5s.pt --epochs 300 --batch-size 16 --workers 8 --data   D:\\0_yyyzt\\AIM\\yolo\\datasets\\zhengtu\\zhengtu.yaml\r\ntrain: weights=yolov5s.pt, cfg=, data=D:\\0_yyyzt\\AIM\\yolo\\datasets\\zhengtu\\zhengtu.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=300, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\r\ngithub: up to date with https://github.com/ultralytics/yolov5\r\nYOLOv5  v7.0-389-ge62a31b6 Python-3.11.9 torch-2.5.1+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\r\nCOMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\r\nCOMET INFO: Using 'D:\\\\0_yyyzt\\\\AIM\\\\yolo\\\\yolov5\\\\.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\r\nTraceback (most recent call last):\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\train.py\", line 986, in <module>\r\n    main(opt)\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\train.py\", line 688, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\train.py\", line 180, in train\r\n    loggers = Loggers(\r\n              ^^^^^^^^\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\utils\\loggers\\__init__.py\", line 153, in __init__\r\n    self.comet_logger = CometLogger(self.opt, self.hyp)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\utils\\loggers\\comet\\__init__.py\", line 102, in __init__\r\n    self.data_dict = self.check_dataset(self.opt.data)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\0_yyyzt\\AIM\\yolo\\yolov5\\utils\\loggers\\comet\\__init__.py\", line 252, in check_dataset\r\n    data_config = yaml.safe_load(f)\r\n                  ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\__init__.py\", line 125, in safe_load\r\n    return load(stream, SafeLoader)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\__init__.py\", line 79, in load\r\n    loader = Loader(stream)\r\n             ^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\loader.py\", line 34, in __init__\r\n    Reader.__init__(self, stream)\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\reader.py\", line 85, in __init__\r\n    self.determine_encoding()\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\reader.py\", line 124, in determine_encoding\r\n    self.update_raw()\r\n  File \"C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yaml\\reader.py\", line 178, in update_raw\r\n    data = self.stream.read(size)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x80 in position 233: illegal multibyte sequence\r\nCOMET INFO: The process of logging environment details (conda environment, git patch) is underway. Please be patient as this may take some time.\r\nCOMET INFO: ---------------------------------------------------------------------------------------\r\nCOMET INFO: Comet.ml OfflineExperiment Summary\r\nCOMET INFO: ---------------------------------------------------------------------------------------\r\nCOMET INFO:   Data:\r\nCOMET INFO:     display_summary_level : 1\r\nCOMET INFO:     name                  : exp\r\nCOMET INFO:     url                   : [OfflineExperiment will get URL after upload]\r\nCOMET INFO:   Others:\r\nCOMET INFO:     Name               : exp\r\nCOMET INFO:     offline_experiment : True\r\nCOMET INFO:   Uploads:\r\nCOMET INFO:     environment details : 1\r\nCOMET INFO:     git metadata        : 1\r\nCOMET INFO:     installed packages  : 1\r\nCOMET INFO:\r\nCOMET INFO: Still saving offline stats to messages file before program termination (may take up to 120 seconds)\r\nCOMET INFO: Begin archiving the offline data.\r\nCOMET INFO: To upload this offline experiment, run:\r\n    comet upload D:\\0_yyyzt\\AIM\\yolo\\yolov5\\.cometml-runs\\e9b6f27f962c4a25bfeb02399ccf699f.zip\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @zhangsiying2001, thank you for your interest in YOLOv5 ðŸš€! It looks like you're encountering a `UnicodeDecodeError` while training.\n\nIf this is a ðŸ› Bug Report, please ensure you've included a **minimum reproducible example (MRE)** for us to investigate. Based on the provided log, the issue seems related to file encoding. It would be helpful if you could:\n\n1. Confirm the encoding of your `.yaml` file (e.g., use UTF-8 encoding).\n2. Share the contents of the file causing the issue, ensuring no sensitive information is included.\n3. Test running YOLOv5 using the recommended Python version and dependencies to rule out potential compatibility issues.\n\n## Requirements\n- Make sure you are using Python >= 3.8 and have all dependencies correctly installed. Running the command to install dependencies after cloning the repository can help ensure this.\n\n## Environments\nYOLOv5 can be run in various environments (local machine, cloud-based notebooks, or containers). Verify your environment is properly configured, especially for platform-specific settings like file encodings on Windows.\n\nThis is an automated response to help you get started. An Ultralytics engineer will take a closer look shortly to assist further ðŸ˜Š."
      },
      {
        "user": "pderrenger",
        "body": "The `UnicodeDecodeError` indicates an issue with reading your YAML file due to encoding mismatches. Ensure that your `zhengtu.yaml` file is saved with UTF-8 encoding. On Windows, you can do this using text editors like Notepad++ or Visual Studio Code by selecting \"Save with Encoding\" as UTF-8. After saving, try running the script again. Let me know if the issue persists!"
      }
    ]
  },
  {
    "issue_number": 12702,
    "title": "Why the model has background class ? Predicted as Background ? ",
    "author": "itsmefifa",
    "state": "closed",
    "created_at": "2024-02-03T19:05:20Z",
    "updated_at": "2025-04-16T09:04:54Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am currently working on a project using YOLO v5/v8 for classifying different grades of cocoa beans, and I have encountered some confusion regarding the background class during my training.\r\n\r\nAs shown in the attached confusion matrix, the model predicts the A class with 100% accuracy; however, the background is also being predicted as the A class. Moreover, class B is predicted as background 10%, I want to see the seeds that classified as background but I am not sure how to do. This is puzzling because I would expect the background class to be distinct from the A, B, and C cocoa bean classes.\r\n\r\nCould you please clarify the role of the background class in the training process? Do I need to include the background class in the calculations for True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN)?\r\n\r\nAlso, I would like to understand why the model might be misclassifying the background as another class. Is there a common reason for this occurrence, and could you suggest any adjustments or best practices to improve the differentiation between the background and the cocoa bean classes in the model predictions?\r\n![background](https://github.com/ultralytics/yolov5/assets/82084392/006a23a2-1ad2-4cce-9522-967d4658331f)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @itsmefifa, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@itsmefifa hello! Thanks for reaching out with your question.\n\nYOLOv5 does not explicitly use a background class during training. Instead, it learns to classify and localize objects of interest, and everything else is implicitly considered background. If you're seeing a \"background\" class in your confusion matrix, it might be due to a label in your dataset named as such.\n\nFor the misclassification issue, it's often due to similarities between the background and the classes or insufficient training data for the model to learn the differences. Here are a few suggestions to improve your model's performance:\n\n1. **Data Augmentation**: Apply various data augmentation techniques to help the model generalize better.\n2. **Review Dataset**: Ensure your dataset is clean and well-labeled, with distinct separation between classes and the background.\n3. **Class Balance**: Check if your dataset is imbalanced and consider techniques to balance it, such as oversampling the minority class.\n4. **Model Complexity**: If your model is too simple, it might not capture the complexity of the data. Conversely, a too complex model might overfit. Adjust the model size accordingly.\n5. **Hyperparameter Tuning**: Experiment with different hyperparameters, including learning rate, batch size, and others that might affect the training dynamics.\n\nFor analyzing misclassified background samples, you can manually inspect the images where the background is predicted as a class or vice versa. This can help you understand what the model is learning and guide you in improving your dataset.\n\nRemember, True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN) are terms generally used in binary classification problems. In the context of object detection, TP and FP are commonly used, while TN is not typically applicable as it would imply correctly identifying the absence of objects across the entire image, which is not usually quantified.\n\nFor more detailed guidance, please refer to our documentation at https://docs.ultralytics.com/yolov5/.\n\nKeep up the great work with your cocoa bean classification project! ðŸŒ±ðŸ«"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      }
    ]
  },
  {
    "issue_number": 13559,
    "title": "# path to yolov5",
    "author": "H-A-R-S-H-T",
    "state": "open",
    "created_at": "2025-04-11T18:25:53Z",
    "updated_at": "2025-04-11T22:16:26Z",
    "labels": [],
    "body": "undefined",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @H-A-R-S-H-T, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nâš¡ This is an automated response, but don't worry, an Ultralytics engineer will assist you soon! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Hi @H-A-R-S-H-T,\n\nYour issue title mentions \"path to yolov5\" but you haven't provided any specific details about your question or problem. \n\nIf you're trying to find or set up the correct path to YOLOv5, you can:\n- Clone the repository: `git clone https://github.com/ultralytics/yolov5`\n- Install via pip: `pip install -U yolov5`\n\nPlease provide more information about what you're trying to accomplish, and I'll be happy to help with more specific guidance.\n\nThank you,\nPaula"
      }
    ]
  },
  {
    "issue_number": 10966,
    "title": "mAP scores very different in val.py and when evaluated using pycocotools on custom dataset.",
    "author": "sanchit2843",
    "state": "closed",
    "created_at": "2023-02-13T07:40:40Z",
    "updated_at": "2025-04-10T07:25:03Z",
    "labels": [],
    "body": "Is this problem solved? I'm using [https://github.com/rafaelpadilla/review_object_detection_metrics](https://github.com/rafaelpadilla/review_object_detection_metrics) to evaluate the output I get from running detect.py on a custom dataset. The difference in mAP is around 11 points.\r\n\r\nI also used [[fifty one](https://docs.voxel51.com/tutorials/evaluate_detections.html#Evaluate-detections)](https://docs.voxel51.com/tutorials/evaluate_detections.html#Evaluate-detections), and the difference in result is almost the same. I'm unsure if I'm doing something wrong with the conversions or if there's a problem with the output of detect.py\r\n\r\nHi, I also tried with pycocotools, the results are even lower. Here is the script I used to convert the ground truth and prediction text files to json files required for pycocotools. Would really appreciate if you can take a look at it.\r\n\r\n```\r\nimport os\r\nimport json\r\n\r\nimport argparse\r\nimport json\r\nimport cv2\r\nimport os\r\nimport math\r\nfrom itertools import chain\r\nimport random\r\nimport numpy as np\r\nfrom icecream import ic\r\nimport multiprocessing as mp\r\nimport shutil\r\nimport json\r\nfrom tqdm import tqdm\r\nimport imagesize\r\n\r\n\r\ndef yolo_to_coco(\r\n    yolo_txt_path, image_dir, output_zip_path, base_json_path, yolo_detect_txt_path\r\n):\r\n\r\n    category_ids_dict_yolo = {0: \"a\", 2: \"b\", 1: \"c\"}\r\n    coco_json = json.load(open(base_json_path))\r\n    category_idx_dict_coco = {v[\"name\"]: v[\"id\"] for v in coco_json[\"categories\"]}\r\n    images_info = []\r\n    annotations_coco = []\r\n    detect_json = []\r\n    total_annotation_count = 1\r\n    for idx, image_name in tqdm(enumerate(os.listdir(image_dir))):\r\n        width, height = imagesize.get(os.path.join(image_dir, image_name))\r\n        images_info.append(\r\n            {\r\n                \"id\": idx + 1,\r\n                \"file_name\": image_name,\r\n                \"width\": width,\r\n                \"height\": height,\r\n                \"license\": 0,\r\n                \"flickr_url\": \"\",\r\n                \"coco_url\": \"\",\r\n                \"date_captured\": 0,\r\n            }\r\n        )\r\n        if (\r\n            os.path.exists(\r\n                os.path.join(yolo_txt_path, image_name.replace(\"png\", \"txt\"))\r\n            )\r\n            is False\r\n        ):\r\n            continue\r\n        with open(os.path.join(yolo_txt_path, image_name.replace(\"png\", \"txt\"))) as f:\r\n            annotations = f.readlines()\r\n\r\n        for annotation in annotations:\r\n            # convert yolo bounding boxes to coco\r\n            annotation = annotation.split(\" \")\r\n            category_id_coco = category_idx_dict_coco[\r\n                category_ids_dict_yolo[int(annotation[0])]\r\n            ]\r\n            bbox = [float(i) for i in annotation[1:5]]\r\n            x1, y1, w, h = bbox[0], bbox[1], bbox[2], bbox[3]\r\n            x1 = (x1 - w / 2) * width\r\n            y1 = (y1 - h / 2) * height\r\n            w = w * width\r\n            h = h * height\r\n            annotations_coco.append(\r\n                {\r\n                    \"id\": total_annotation_count,\r\n                    \"image_id\": idx + 1,\r\n                    \"category_id\": category_id_coco,\r\n                    \"area\": w * h,\r\n                    \"segmentation\": [],\r\n                    \"bbox\": [x1, y1, w, h],\r\n                    \"iscrowd\": 0,\r\n                    \"attributes\": {\"occluded\": False},\r\n                }\r\n            )\r\n            total_annotation_count += 1\r\n        if (\r\n            os.path.exists(\r\n                os.path.join(yolo_detect_txt_path, image_name.replace(\"png\", \"txt\"))\r\n            )\r\n            is False\r\n        ):\r\n            continue\r\n        with open(\r\n            os.path.join(yolo_detect_txt_path, image_name.replace(\"png\", \"txt\"))\r\n        ) as f:\r\n            annotations = f.readlines()\r\n        for annotation in annotations:\r\n            annotation = annotation.split(\" \")\r\n            bbox = [float(i) for i in annotation[1:5]]\r\n            x1, y1, w, h = bbox[0], bbox[1], bbox[2], bbox[3]\r\n            x1 = (x1 - w / 2) * width\r\n            y1 = (y1 - h / 2) * height\r\n            w = w * width\r\n            h = h * height\r\n            detect_json.append(\r\n                {\r\n                    \"image_id\": idx + 1,\r\n                    \"category_id\": category_idx_dict_coco[\r\n                        category_ids_dict_yolo[int(annotation[0])]\r\n                    ],\r\n                    \"bbox\": [x1, y1, w, h],\r\n                    \"score\": float(annotation[5]),\r\n                }\r\n            )\r\n    coco_json[\"images\"] = images_info\r\n    coco_json[\"annotations\"] = annotations_coco\r\n    os.makedirs(os.path.join(output_zip_path, \"temp\"), exist_ok=True)\r\n    with open(\r\n        os.path.join(output_zip_path, \"instances_defaults.json\"),\r\n        \"w\",\r\n    ) as f:\r\n        json.dump(coco_json, f)\r\n    with open(\r\n        os.path.join(output_zip_path, \"detect.json\"),\r\n        \"w\",\r\n    ) as f:\r\n        json.dump(detect_json, f)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    yolo_txt_path = \"test/labels\"\r\n    image_dir = \"test/images\"\r\n    yolo_detect_txt_path = \"/home/ubuntu/detection/runs/detect/conf_thresh_0.2/labels\"\r\n    output_zip_path = \"result_check1\"\r\n    os.makedirs(output_zip_path, exist_ok=True)\r\n    base_json_path = \"./instances_default.json\"\r\n    yolo_to_coco(\r\n        yolo_txt_path, image_dir, output_zip_path, base_json_path, yolo_detect_txt_path\r\n    )\r\n    from pycocotools.coco import COCO\r\n    from pycocotools.cocoeval import COCOeval\r\n\r\n    annType = \"bbox\"\r\n\r\n    cocoGt = COCO(os.path.join(output_zip_path, \"instances_defaults.json\"))\r\n    cocoDt = cocoGt.loadRes(os.path.join(output_zip_path, \"detect.json\"))\r\n\r\n    imgIds = sorted(cocoGt.getImgIds())\r\n\r\n    # running evaluation\r\n    cocoEval = COCOeval(cocoGt, cocoDt, annType)\r\n    cocoEval.params.imgIds = imgIds\r\n    # cocoEval.params.catIds = [3]\r\n    cocoEval.evaluate()\r\n    cocoEval.accumulate()\r\n    cocoEval.summarize()\r\n\r\n```\r\n\r\n_Originally posted by @sanchit2843 in https://github.com/ultralytics/yolov5/issues/5116#issuecomment-1425254854_\r\n            ",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @sanchit2843, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "trap-fish",
        "body": "@sanchit2843 - sorry to drag up a >2yr old issue, but I noticed you closed this a couple days later, did you work out what the issue was? I'm also getting different values between ultralytics `model.val` and pycocotools for mAP@50 and as far as I can tell, the configs are more or less the same"
      }
    ]
  },
  {
    "issue_number": 10444,
    "title": "How export to onnx with rectangular input shape?",
    "author": "tomyelrond",
    "state": "closed",
    "created_at": "2022-12-08T19:01:55Z",
    "updated_at": "2025-04-07T05:35:52Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI have a image dataset where each image has 1920x1080. My goal is to export trained model to ONNX with minimal padding. \r\n\r\nDo I understand correctly that the right way is to:\r\n- train model with `--rect`argument and with `--img-size` for example 832 (the long side)\r\n- export the trained model with similar aspect ratio as images 1920x1080. For example 832x480: `python export.py --weights model.pt --batch 1 --include onnx --img 480 832`\r\n\r\nIs it right?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @tomyelrond, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "JustasBart",
        "body": "Hi, that looks good, this is how I would normally export my .onnx models to be used via OpenCV C++:\r\n\r\n```\r\npython3 export.py \\\r\n--weights runs/train/exp/weights/best.pt \\\r\n--imgsz 736 1280 \\ # Height Width\r\n--simplify \\\r\n--optimize \\\r\n--include onnx\r\n```"
      },
      {
        "user": "tomyelrond",
        "body": "@JustasBart and you train with `--img 1280`? Do you use  `--rect` parameter for the training?"
      }
    ]
  },
  {
    "issue_number": 13526,
    "title": "Is there no backwards compatablility for yolov5?",
    "author": "guzman109",
    "state": "closed",
    "created_at": "2025-03-06T20:25:32Z",
    "updated_at": "2025-04-06T15:29:45Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI'm using the ultralytics library and loading the model via torch hub. Looks like NMS from ultralytics is different than the NMS from the yolov5 repo and I am getting completely different results between using both models as well.\n\nIs there no backwards compatibility?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @guzman109, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it. Specifically, for your scenario, please include:\n1. The exact code snippet you're using to load the model via `torch.hub` and perform inference.\n2. Details of the YOLOv5 version(s) you're comparing, and how you're loading the models in both cases.\n3. Examples of the input image(s) and the resulting outputs you're observing for comparison.\n\nIf this is a custom model or compatibility â“ Question, please provide as much information as possible, including the details of the models or libraries you're using. Additionally, consider ensuring that all setups are aligned with our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response to help guide you with helpful resources ðŸ’¡. An Ultralytics engineer will review your issue and assist you further shortly."
      },
      {
        "user": "pderrenger",
        "body": "@guzman109 yOLOv5 maintains backward compatibility for inference when using the **exact same model version** across both torch hub and local code. For optimal results:\n\n1. **Specify exact model version** in torch.hub.load():\n```python\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', version='v6.0')  # specify version tag\n```\n2. Verify you're using matching:\n   - Model architecture (v6.0+ has updated layers)\n   - Postprocessing (NMS implementation in `utils.general.non_max_suppression`)\n   - Package versions (`pip install -U ultralytics`)\n\nThe [YOLOv5 v6.0 release notes](https://ultralytics.com/blog/yolov5-v6-0-is-here) detail architectural changes that might affect compatibility. If issues persist, please share a minimum reproducible example per our [bug report guidelines](https://github.com/ultralytics/yolov5/issues/4032)."
      },
      {
        "user": "guzman109",
        "body": "So the output changed at some point to match the other yolo models? The non_max_suppression input has a different shape than older version of the model.\n\nAlso, is scale_boxes from utils/general.py the same as scale_boxes or scale_coords in ultralytics module?"
      }
    ]
  },
  {
    "issue_number": 3847,
    "title": "How to find out the maximum batch size that my machine can handle?",
    "author": "joneswilliam1",
    "state": "closed",
    "created_at": "2021-07-01T03:07:59Z",
    "updated_at": "2025-04-04T23:18:12Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "## â”Question\r\nHi everyone, I'm sorry if this is a basic question but I have an RTX 2070 machine and while training a Yolov5 model on around 3500 images I first set batch size to 32 and got memory error, then switched to 16 and the run started successfully. So is it like a trial and error or is there a way to actually determine the maximum batch size that you machine is able to handle. Thank you.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @joneswilliam1, thank you for your interest in ðŸš€ YOLOv5! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://www.ultralytics.com or email Glenn Jocher at glenn.jocher@ultralytics.com.\n\n## Requirements\n\nPython 3.8 or later with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including `torch>=1.7`. To install run:\n```bash\n$ pip install -r requirements.txt\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "wudashuo",
        "body": "CUDA memory error is related to your image size, batch size, and model size.\r\nYou can try batch size between 16 and 32 to determine the maximum batch size that your machine capable of.\r\n`--workers` also affect memory to some extent."
      },
      {
        "user": "glenn-jocher",
        "body": "@joneswilliam1 ðŸ‘‹ Hello! Thanks for asking about **CUDA memory issues**. YOLOv5 ðŸš€ can be trained on CPU, [single-GPU](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data), or [multi-GPU](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training). When training on GPU it is important to keep your batch-size small enough that you do not use all of your GPU memory, otherwise you will see a CUDA Out Of Memory (OOM) Error and your training will crash. You can observe your CUDA memory utilization using either the `nvidia-smi` command or by viewing your console output:\r\n\r\n<img width=\"987\" alt=\"Screenshot 2021-05-28 at 12 19 51\" src=\"https://user-images.githubusercontent.com/26833433/119969671-22dca680-bfaf-11eb-92af-5908f9743229.png\">\r\n\r\nIf you encounter a CUDA OOM error, the steps you can take to reduce your memory usage are:\r\n\r\n- Reduce `--batch-size`\r\n- Reduce `--img-size`\r\n- Reduce model size, i.e. from YOLOv5x -> YOLOv5l -> YOLOv5m -> YOLOv5s\r\n- Train with [multi-GPU](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training) at the same `--batch-size`\r\n- Upgrade your hardware to a larger GPU\r\n- Train on free GPU backends with up to 16GB of CUDA memory: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>"
      }
    ]
  },
  {
    "issue_number": 894,
    "title": "training stuck when loading dataset",
    "author": "ljmiao",
    "state": "closed",
    "created_at": "2020-09-01T12:43:04Z",
    "updated_at": "2025-04-02T02:25:23Z",
    "labels": [
      "question"
    ],
    "body": "## â”Question\r\nI follow the instructions to run coco128, \r\n```\r\npython train.py --img 640 --batch 16 --epochs 5 --data ./data/coco128.yaml --cfg ./models/yolov5s.yaml --weights '',\r\n```\r\nthe ouput is \r\n```\r\nImage sizes 640 train, 640 test\r\nUsing 8 dataloader workers\r\nStarting training for 5 epochs...\r\n\r\n     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\r\n  0%|                                                                                                    | 0/8 [00:00<?, ?it/s\r\n```\r\nthen it is stuck, I found that it is stucking at loading the dataset, \r\nin https://github.com/ultralytics/yolov5/blob/master/train.py#L244,  \r\n```\r\nfor i, (imgs, targets, paths, _) in pbar:\r\n```\r\nit just stops here, could you help me ?\r\n\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello @ljmiao, thank you for your interest in our work! Please visit our [Custom Training Tutorial](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) to get started, and see our [Jupyter Notebook](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb) <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>, [Docker Image](https://hub.docker.com/r/ultralytics/yolov5), and [Google Cloud Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) for example environments.\n\nIf this is a bug report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom model or data training question, please note Ultralytics does **not** provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:\n- **Cloud-based AI** systems operating on **hundreds of HD video streams in realtime.**\n- **Edge AI** integrated into custom iOS and Android apps for realtime **30 FPS video inference.**\n- **Custom data training**, hyperparameter evolution, and model exportation to any destination.\n\nFor more information please visit https://www.ultralytics.com."
      },
      {
        "user": "NanoCode012",
        "body": "Hi @ljmiao , could you try using another Environment listed above? \r\n\r\nI just ran your command in a new Docker, and there were no issues. "
      },
      {
        "user": "ljmiao",
        "body": "Thank you, I will try it"
      }
    ]
  },
  {
    "issue_number": 13177,
    "title": "Performance difference in model formats",
    "author": "Avaneesh-S",
    "state": "closed",
    "created_at": "2024-07-09T11:16:00Z",
    "updated_at": "2025-04-02T01:28:04Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nWhy is the performance of ONNX and TensorRT yolov5 models lesser than that of their pytorch equivalent model. \r\nI exported the model by following: [exporting yolov5](https://docs.ultralytics.com/yolov5/tutorials/model_export/#export-a-trained-yolov5-model)\r\n\r\nfor .pt model:\r\n![pt_model](https://github.com/ultralytics/yolov5/assets/110843612/a5675e0a-5bc7-43c2-9ee7-01a68edb60ee)\r\n\r\nfor .onnx model:\r\n![onnx_model](https://github.com/ultralytics/yolov5/assets/110843612/c0f21fed-2e84-4c5c-a905-276983bd04ca)\r\n\r\nfor .engine model:\r\n![tensorrt_model](https://github.com/ultralytics/yolov5/assets/110843612/6e39667f-2273-4d0b-832b-519369714af5)\r\n\r\nI have tested the above models on colab's T4 GPU and used the [val.py](https://github.com/ultralytics/yolov5/blob/master/val.py) script present in the yolov5 repository to test on the COCO 128 dataset. You can see that there is a slight performance drop from the .pt model to the .onnx model (the .onnx model and .engine model have same performance). \r\n\r\nThough the performance drop here is negligible, I feel it may change based on dataset.\r\nIs there a way to preserve the exact performance across exports? (specifically, I want to retain the performance of the pytorch model in the tensorrt model)\n\n### Additional\n\nNOTE: to export to tensorRT format (.engine file) I had to change numpy version to specifically 1.23. It would be good if this was mentioned with the documentation or updated to run with the latest version.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @Avaneesh-S, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@Avaneesh-S hello,\n\nThank you for reaching out and for providing detailed information about your issue. It's great to see that you've already followed the model export tutorial and tested the models on a Colab T4 GPU.\n\n### Performance Differences Across Model Formats\n\nPerformance differences between model formats can occur due to several factors, including differences in how each framework handles operations, optimizations, and precision. Here are a few points to consider:\n\n1. **Precision and Optimizations**: Ensure that the precision (FP16 vs. FP32) and any optimizations applied during export are consistent across formats. For instance, using the `--half` flag during export can reduce model size and potentially affect performance.\n\n2. **Export Parameters**: Verify that the export parameters are consistent. For example:\n    ```bash\n    python export.py --weights yolov5s.pt --include torchscript onnx engine --half\n    ```\n\n3. **Framework-Specific Optimizations**: TensorRT and ONNX may apply different optimizations that can affect performance. These optimizations are generally aimed at improving inference speed but can sometimes lead to slight variations in accuracy.\n\n4. **Validation Script**: Ensure that the validation script (`val.py`) is consistent and that no additional preprocessing or postprocessing steps are inadvertently altering the results.\n\n### Ensuring Consistent Performance\n\nTo preserve performance across exports, you can try the following:\n\n- **Consistency in Precision**: Ensure that you are using the same precision (FP16 or FP32) across all exports.\n- **Validation**: Use the same validation script and dataset for all formats to ensure consistency.\n- **Export Options**: Experiment with different export options to see if they affect performance. For example, using `--dynamic` for dynamic input shapes or `--simplify` for simplified ONNX models.\n\n### Example Export Command\n\nHereâ€™s an example command to export a model to multiple formats with consistent precision:\n\n```bash\npython export.py --weights yolov5s.pt --include torchscript onnx engine --half\n```\n\n### Documentation Note\n\nThank you for pointing out the issue with the numpy version for TensorRT export. We will look into updating the documentation to reflect this requirement.\n\nFor further details, you can refer to the [model export tutorial](https://docs.ultralytics.com/yolov5/tutorials/model_export/).\n\nIf the issue persists, please ensure you are using the latest versions of `torch` and the YOLOv5 repository. If you have any further questions or need additional assistance, feel free to ask!"
      },
      {
        "user": "Avaneesh-S",
        "body": "Hey @glenn-jocher, I have verified that the .pt model's weights are float16 by default using 'netron' website. So if i dont use --half while exporting I see that my .onnx model's are float32, so using --half shouldn't affect as much right?. \r\n ( Can't verify for .engine model on 'netron') \r\nexporting using --half means even while using [val.py](https://github.com/ultralytics/yolov5/blob/master/val.py) I have to use the --half. I noticed that using --half flag with val.py **changes the performance**: \r\n\r\nfor .pt model:\r\n![image](https://github.com/ultralytics/yolov5/assets/110843612/b5dec498-e4a1-4887-a6d2-1e050368d64a)\r\n\r\nfor .onnx model (exported using --half and --device 0):\r\n![image](https://github.com/ultralytics/yolov5/assets/110843612/bddfa4aa-9c68-4c61-8ada-051b849d4cda)\r\n\r\ncan notice the difference by comparing these results to ones I had originally shared.\r\nI have also verified that using --dynamic and --simplify did not affect the performance.\r\n**Do let me know if you know of any method to convert without losing performance (I am looking for a way to get the exact performance results of the .pt model on the .onnx and .engine models)** \r\n\r\nPS: I would have shared the screenshots of all the above runs including for tensorrt, but there is an issue with the yolov5 repo right now, which was not there previously(until yesterday for me). commands to fetch data from - https://ultralytics.com/assests/ is not running. it returns **301 moved permanently response**.\r\nIt does this only when getting data through linux terminals (I tried with wget), including the default running of \r\n [val.py](https://github.com/ultralytics/yolov5/blob/master/val.py) is not working on colab as well with the same error when it tries to fetch data from there (like the coco128 dataset). \r\nIt works on windows though, kindly look into this too. If resolved I can test the other methods.\r\n"
      }
    ]
  },
  {
    "issue_number": 13549,
    "title": "Train Yolo with packed RGB(A) pictures ?",
    "author": "LeRatierBretonnien",
    "state": "open",
    "created_at": "2025-03-27T07:22:27Z",
    "updated_at": "2025-03-29T23:57:01Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello, \n\nI would like to know if it is possible to train Yolo with RGB pictures but in **packed format**. So instead of having 3 planes (ie 1 R, 1 G, 1 B), the pixels are packed in a single plane, with 24 bits (or 32 bits) value for each pixel.\n\nI see that I can change the channels of the training process (ie for grayscale and depth, for example), but I do not see how to indicate that the picture is in fact a single plane with 24 bits (or 32), with 1 byte per channel.\n\nThanks for clarification/help, \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @LeRatierBretonnien, thank you for your question and interest in YOLOv5 ðŸš€! \n\nTo address your query about training YOLO with packed RGB(A) images, we would need additional details to assist you better. If this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)**, including code snippets, sample inputs, or any logs that can help us understand the behavior you're encountering.\n\nIf this is a â“ Question, we recommend sharing more context about your dataset, preprocessing steps, and any modifications you've made to the YOLOv5 codebase. This can help us provide a more tailored response. In the meantime, you can explore our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for insights on data preprocessing and training workflows, including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/).\n\n## Requirements\n\nEnsure you are running YOLOv5 in a supported environment with the correct dependencies:\n[**Python>=3.8.0**](https://www.python.org/) and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) installed with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt). To set up your environment:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 supports various environments, making it versatile for experimentation:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify the correct operation of YOLOv5's [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) across macOS, Windows, and Ubuntu platforms.\n\nThis is an automated response to provide initial guidance. An Ultralytics engineer will review your issue and provide additional assistance shortly. Thank you for your patience and for using YOLOv5! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@LeRatierBretonnien yOLOv5 models require input images in standard 3-channel RGB format. For packed RGB(A) formats, you'll need to preprocess your images to unpack them into standard 3-channel format (HWC shape) before training. You can implement this conversion in your custom dataset loader or preprocessing pipeline. \n\nFor guidance on custom data handling, see the [Ultralytics YOLOv5 dataset documentation](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#dataset-format). If you need to apply additional augmentations after conversion, consider exploring the [Albumentations integration guide](https://docs.ultralytics.com/integrations/albumentations/) for advanced preprocessing options."
      },
      {
        "user": "LeRatierBretonnien",
        "body": "Thanks for clarification and explanation, this is appreciated"
      }
    ]
  },
  {
    "issue_number": 12997,
    "title": "Guide on how to utilize 'Weighted Loss' method in yolov5 custom training",
    "author": "dilwolf",
    "state": "closed",
    "created_at": "2024-05-10T08:40:59Z",
    "updated_at": "2025-03-29T02:58:23Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nI am trying to detect football players, referees and ball on videos of  mounted camera in football matches.   From the nature of football analysis, I have an imbalanced dataset, overrepresented for player class and under-representation for class referee and ball, specifically for ball class, which is highly underrepresented. I am sure that my labels are accurate. \r\n\r\nBelow is the method I tried but did not help:  \r\n\r\n- manipulated ball and referee class instances in big part of dataset by excluding player class (because I have enough player class instances). However, this decreased model performance (usually not recommended by experts. I found it out later).\r\n\r\nNow, I am going to try the **`Weighted Loss`** method but I am not sure how to do it. I also found [this](https://github.com/ultralytics/ultralytics/issues/2703) for yolov8 but it was not clear. So, it would be great, if you give me a detailed guide on how to modify code. I am new.  \r\n\r\nFIY: My dataset class instance distribution `459:80:14` in data.yaml file order\r\n\r\nThank you so much !!!\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @dilwolf, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "Hello! ðŸ˜Š Great choice evolving your approach to tackle class imbalance using the **Weighted Loss** method in YOLOv5.\n\nTo implement weighted loss during your custom training with YOLOv5, you'll need to adjust your training configuration to account for the variance in class representation.\n\nHere's a concise way to do it:\n\n1. **Modify your `data.yaml` file** by adding a `class_weights` field. You want to give higher weights to the underrepresented classes. Given your class instance distribution `459:80:14`, you might consider a weighting like `[0.5, 2, 10]` reflecting the inverse proportion of the instances.\n\n   ```yaml\n   # data.yaml\n   train: path/to/train/dataset\n   val: path/to/val/dataset\n\n   nc: 3\n   names: ['player', 'referee', 'ball']\n   class_weights: [0.5, 2, 10]\n   ```\n\n2. **Adjust the loss computation in your training script**. Unfortunately, out-of-the-box YOLOv5 doesn't directly support weighted loss modification via the configuration file, so you'll need to manually tweak the loss computation within your training code, specifically when calculating the classification loss.\n\nUnderstanding and making these changes can be somewhat involved, especially if you're new to the codebase. If you need a deeper dive into implementing these changes, I highly recommend checking our detailed methodological guides and model training strategies in the YOLOv5 documentation here: [https://docs.ultralytics.com/yolov5/](https://docs.ultralytics.com/yolov5/)\n\nGood luck with your model training, and feel free to reach out with more questions as you progress! ðŸŒŸ"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      }
    ]
  },
  {
    "issue_number": 13533,
    "title": "maintain high resolution for object detection segmentation",
    "author": "dimasadef72",
    "state": "open",
    "created_at": "2025-03-13T03:51:40Z",
    "updated_at": "2025-03-28T17:19:33Z",
    "labels": [
      "question",
      "detect",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nMy current problem is when I input an image with a resolution of 20mp, then during the inference process Yolo will decrease the resolution. is there a method to maintain the number of pixels of the 20 mp input image for object detection using Yolo? because I want to detect rice fields with segmentation and calculate the area of rice fields (segmentation results) based on the number of image pixels. \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @pepsodent72, thank you for your interest in YOLOv5 ðŸš€! Maintaining high-resolution image inputs for tasks like object detection and segmentation is an important topic. Please explore our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for guidance, including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nIf this is a â“ Question, please provide additional details about your use case, such as:\n- Specific YOLO model or configuration being used\n- Any changes made to preprocessing or inference settings\n- Example images or logs that illustrate your issue\n\nIf this is a ðŸ› Bug Report, we kindly request a **minimum reproducible example (MRE)** to help us investigate efficiently.\n\n## Requirements\n\nEnsure your environment meets the following:\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To set up:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYou can run YOLOv5 in any of the following verified environments:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests validate YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py).\n\nThis is an automated response to help guide you ðŸ˜Š. An Ultralytics engineer will review your issue and provide assistance soon. Thank you for your patience and for using YOLOv5! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@pepsodent72 to maintain high-resolution segmentation for area calculation in YOLOv5, we recommend two approaches:\n\n1. **Native Inference**: Run inference at full resolution by setting `--imgsz` to your original image dimensions (e.g., `--imgsz 5472 3648` for 20MP). This requires sufficient GPU memory. Modify your inference command:\n```python\npython detect.py --weights your_model.pt --source your_image.jpg --imgsz 5472 3648\n```\n\n2. **Sliding Window Inference**: Process the image in tiles using a sliding window approach with `--conf` and `--overlap` adjustments to maintain precision. This is memory-efficient for large images:\n```python\nfrom models.common import DetectMultiBackend\nmodel = DetectMultiBackend('your_model.pt')\nresults = model(source, imgsz=img_size, stride=64, auto=False, augment=False)\n```\n\nFor area calculations, ensure your images have geospatial metadata (ground sampling distance) to convert pixel counts to real-world areas. The Ultralytics HUB offers built-in geospatial tools when working with satellite/drone imagery.\n\nFor implementation details, see:\n- Detection parameters in detect.py\n- Sliding window guidance in YOLOv5 documentation\n\n[Let us know](https://github.com/ultralytics/yolov5/discussions) if you need further assistance with your rice field segmentation project! ðŸŒ¾"
      },
      {
        "user": "dimasadef72",
        "body": "if I do segmentation detection on a 20mp input image, what is my training model? does it remain 640x640? \n\nthen is the sliding window accurate if it detects large objects such as rice fields? and if there are objects detected in frame 1 and 2? can the sliding window merge the detection results?\n\nthank you hope you are always healthy"
      }
    ]
  },
  {
    "issue_number": 13547,
    "title": "Regarding the output images of lr0, lr1 and lr2",
    "author": "lzqwq",
    "state": "open",
    "created_at": "2025-03-26T02:05:18Z",
    "updated_at": "2025-03-27T15:48:26Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI have run the experiment many times and outputted the images of lr0, lr1, lr2. I noticed that the data inside is not random. Is it possible to set it? The core problem I am facing now is that the previous experiments I ran with lr0, lr1, lr2 were relatively high, which is represented by the solid line in the figure, and the experimental effect was also better. However, recently the experiments have deteriorated. No matter how I modify the model, lr0, lr1, lr2 remain the same, which is represented by the dashed line in the figure.\nI would be very grateful if you could address my question.\n\n![Image](https://github.com/user-attachments/assets/71eb5812-4fd9-49db-991f-bb195b9ed4d9)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @lzqwq, thank you for reaching out and sharing your question about YOLOv5 ðŸš€! Your detailed description and attached image are helpful in understanding the issue.\n\nIf this is a ðŸ› Bug Report, could you please provide a **minimum reproducible example (MRE)**? This will help us better investigate the issue. Include the exact commands or scripts you used, along with any specific configurations or customizations.\n\nIf this is a â“ Question about custom training or hyperparameters, please share additional details such as:\n- Training logs or metrics\n- Dataset examples\n- Steps you've taken to modify the model or address the issue\n- Confirmation that you've reviewed our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)\n\n## Requirements\n\nEnsure you're using [**Python>=3.8.0**](https://www.python.org/) and have installed all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Verified Environments\n\nYOLOv5 runs smoothly in these environments (pre-installed with [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/)):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are passing. These tests ensure YOLOv5's functionality for training, validation, inference, export, and benchmarking across macOS, Windows, and Ubuntu.\n\nThis is an automated response to guide you as quickly as possible ðŸ˜Š. An Ultralytics engineer will review your issue and assist you soon. Thank you for your patience and for using YOLOv5! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@lzqwq the learning rate plots (lr0, lr1, lr2) in YOLOv5 training outputs are determined by the learning rate scheduler and hyperparameters in your `hyp.yaml` file. These curves follow a deterministic cosine annealing schedule with warmup, not random values. To modify them:\n\n1. Adjust the base learning rate (`lr0`) in your hyperparameter configuration\n2. Modify the scheduler parameters like `lrf` (final learning rate factor) in `train.py`\n\nIf you're seeing inconsistent behavior between experiments:\n- Verify you're using identical hyperparameter files for comparable runs\n- Ensure you're training with the same dataset versions\n- Confirm package versions match between experiments (`pip list | grep ultralytics`)\n\nFor detailed guidance on hyperparameter tuning, see the YOLOv5 Hyperparameter Evolution documentation."
      },
      {
        "user": "lzqwq",
        "body": "\n\n\n> [@lzqwq](https://github.com/lzqwq) the learning rate plots (lr0, lr1, lr2) in YOLOv5 training outputs are determined by the learning rate scheduler and hyperparameters in your `hyp.yaml` file. These curves follow a deterministic cosine annealing schedule with warmup, not random values. To modify them:\n> \n> 1. Adjust the base learning rate (`lr0`) in your hyperparameter configuration\n> 2. Modify the scheduler parameters like `lrf` (final learning rate factor) in `train.py`\n> \n> If you're seeing inconsistent behavior between experiments:\n> \n> * Verify you're using identical hyperparameter files for comparable runs\n> * Ensure you're training with the same dataset versions\n> * Confirm package versions match between experiments (`pip list | grep ultralytics`)\n> \n> For detailed guidance on hyperparameter tuning, see the YOLOv5 Hyperparameter Evolution documentation.\n\nOK. Thank you for your reply."
      }
    ]
  },
  {
    "issue_number": 12883,
    "title": "Retrain a custom model or train a new one",
    "author": "JrGxllxgo",
    "state": "closed",
    "created_at": "2024-04-04T12:56:23Z",
    "updated_at": "2025-03-25T14:48:30Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello everyone!\r\n\r\nI have been training a segmentation model. My dataset consists of +/- 2500 training images and +/- 700 validation images, in these images we have 2 classes to detect. I have trained it for 150 epochs and probably the 60% of the predictions are right but I want better results.\r\n\r\nCreating a new dataset with more images with the cases the model doesn't predict very well and retraining the model might be a good idea but I'm not sure, maybe training a new model with more images with wrong results is better.\r\n\r\nWhat do you recommend to me?\r\n\r\nThank you all!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@JrGxllxgo hello! ðŸ˜Š\n\nGreat to hear about your progress with training a segmentation model. Improving model performance is often a journey of iteration and experimentation.\n\nGiven your scenario, **expanding your dataset** with more images, especially those where the model currently underperforms, is a good strategy. Instead of training a new model from scratch, **retraining your existing model** with this enriched dataset can help. This approach leverages what the model has already learned and further fine-tunes it on the new challenging cases, potentially leading to better results.\n\nHereâ€™s a simple way to proceed:\n1. Augment your dataset with more images, focusing on the cases with poor predictions.\n2. Continue training your current model with the expanded dataset for additional epochs.\n\nThis method is generally more efficient and effective than starting anew since it builds on the existing knowledge base of your model.\n\nIf you have further questions or need clarification, feel free to ask. Happy training!"
      },
      {
        "user": "JrGxllxgo",
        "body": "Hello @glenn-jocher !!\r\n\r\nThanks for your recommendation! I'll augment my dataset, how many images you recommend me to ascend to?\r\n\r\nWhen I started the retrain I supose that the command to do it is \"python segment/train.pt --data path_to/my_yaml.yaml --weights myTrainedModel.pt --img 640\", is that right?\r\n\r\nAnd for the last question I have is if there is any tip or parameter that do the train more quickly, now my epochs are on 1h more or less and if I reduce this time is amazing, is there any thing I can do to reduce the time?\r\n\r\n\r\nThanks you for all!!!"
      },
      {
        "user": "glenn-jocher",
        "body": "Hello @JrGxllxgo!\n\nThrilled to see your enthusiasm! Let's tackle your questions:\n\n1. **Dataset Size:** There's no one-size-fits-all number for how many images you should add. It heavily depends on your model's current performance and the variability of your data. As a general rule, try to balance or double the number of images in scenarios where your model underperforms. However, quality over quantity always matters. Ensure the added images genuinely introduce new information or challenge the model in meaningful ways.\n\n2. **Retrain Command:** Your command looks almost correct! Just ensure you're pointing to the right script. It should be something like this for training:\n   ```python\n   python train.py --data path_to/my_yaml.yaml --weights myTrainedModel.pt --img 640\n   ```\n   Make sure to replace `segment/train.pt` with `train.py`.\n\n3. **Speeding Up Training:** To reduce training time, consider the following:\n   - **Reduce Batch Size:** A smaller batch size requires less memory, potentially speeding up the training, but watch out for performance impact.\n   - **Use a Faster GPU:** Hardware improvements can significantly reduce training time.\n   - **Freeze Early Layers:** Early in training, freezing the initial layers of the model can speed up training since these layers often learn generic features that don't need frequent updates.\n\nRemember, improving training time may sometimes come at the cost of model accuracy or generalization, so balance is key. \n\nHope this helps, and happy training!"
      }
    ]
  },
  {
    "issue_number": 13539,
    "title": "Issue with Tellu Organoid Classifier using Yolov5",
    "author": "djtigani",
    "state": "open",
    "created_at": "2025-03-21T18:02:05Z",
    "updated_at": "2025-03-25T13:12:11Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am having this issue with Tellu Intestinal Organoid Classifier https://colab.research.google.com/drive/1j-jutA52LlPuB4WGBDaoAhimqXC3xguX?usp=sharing \n\nHas worked perfectly before, but know I cannot get this code to run. any help would be appreciated, thanks.\n\ndetect: weights=['/content/yolov5/TelluWeights.pt'], source=/content/drive/MyDrive/BlatchleyLab/Projects/Role_of_integrins_in_symmetry_breaking/241217_DJT_SC_Rac1i_p30/D4, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.35, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=D4, exist_ok=False, line_thickness=3, hide_labels=True, hide_conf=False, half=False, dnn=False, vid_stride=1\nYOLOv5 ðŸš€ v7.0-0-g915bbf29 Python-3.11.11 torch-2.6.0+cu124 CPU\n\nTraceback (most recent call last):\n  File \"/content/yolov5/detect.py\", line 259, in <module>\n    main(opt)\n  File \"/content/yolov5/detect.py\", line 254, in main\n    run(**vars(opt))\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/content/yolov5/detect.py\", line 96, in run\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/content/yolov5/models/common.py\", line 345, in __init__\n    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/content/yolov5/models/experimental.py\", line 79, in attempt_load\n    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1470, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL models.yolo.Model was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Model])` or the `torch.serialization.safe_globals([Model])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n\\nThe script also accepts other parameters. See https://github.com/ultralytics/yolov5/blob/master/detect.py for all the options.\\n --max-detect 1000. Default is 1000; if you require more, add the parameter to the line of code above with a different value.\\n --hide-labels Adding this parameter will hide the name of the organoid class detected in the result images. Convenient in dense cultures and for publication purposes\\n\\nExample:\\n!python detect.py --weights $tellu --conf 0.35 --source $dir --save-txt --save-conf --name \"results\" --hide-labels --max-detect 2000\\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @djtigani, thank you for reaching out and for using YOLOv5 ðŸš€!\n\nIt seems you're encountering an issue with your Tellu Intestinal Organoid Classifier setup. To assist further, could you please provide a **minimum reproducible example** (MRE)? This will help us debug the issue more effectively. Specifically, please include:\n\n1. A clear description of your environment (e.g., Python version, PyTorch version, and any specific configurations you've set up).\n2. The exact commands or code snippets you are running.\n3. A sample of your input data or a detailed description of its structure (if applicable).\n4. Steps you've already tried to resolve the issue.\n\nIn the meantime, here are a few resources and steps that may help:\n\n## Requirements\n\nEnsure your environment meets the following:\n- [**Python>=3.8.0**](https://www.python.org/)\n- All dependencies in [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) are installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\nTo set up, run:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Common Troubleshooting\n\n- Make sure your weights file (`TelluWeights.pt`) is compatible with the current YOLOv5 version you are using. If you are using older weights, consider updating them or reverting to a compatible YOLOv5 version.\n- Check the PyTorch version installed in your environment. The error suggests a change in `torch.load` behavior. You may need to adjust the `weights_only` parameter or follow the recommendations in [PyTorch's documentation](https://pytorch.org/docs/stable/generated/torch.load.html).\n- If you suspect a corrupted weights file, try re-downloading it.\n\n## Environments\n\nYOLOv5 is verified to work in environments like:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\nCheck if all YOLOv5 CI tests are passing:\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, YOLOv5 tests for training, validation, inference, export, and benchmarks are all passing on macOS, Windows, and Ubuntu.\n\nThis is an automated response to help guide you. A member of the Ultralytics team will review your issue and assist you further as soon as possible. Thanks for your patience! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@djtigani this error occurs due to PyTorch 2.6's new security restrictions in `torch.load()`. Let's fix it with these steps:\n\n1. **Update YOLOv5**: First ensure you're using the latest YOLOv5 code:\n```python\n!git pull  # in your yolov5 directory\n```\n\n2. **PyTorch Version**: Either:\n- Downgrade PyTorch:\n```python\n!pip install torch==2.0.1  # compatible version\n```\n- Or modify your environment to trust the weights file (only if you fully trust the model source)\n\nFor full Google Colab setup guidance, see our [Training custom datasets with Ultralytics YOLOv8 in Google Colab](https://www.ultralytics.com/blog/training-custom-datasets-with-ultralytics-yolov8-in-google-colab) guide which covers similar environment configuration steps. Let us know if you still encounter issues after trying these solutions! ðŸš€"
      },
      {
        "user": "djtigani",
        "body": "@pderrenger Hi! thank for the advice. Where should I run the code provided in the googlecolab (https://colab.research.google.com/drive/1j-jutA52LlPuB4WGBDaoAhimqXC3xguX?usp=sharing)\n\nI tried running it after 1.2 and then got this result: \n\n![Image](https://github.com/user-attachments/assets/7ceb7b0d-b1b6-414f-ba6b-23928a4ad0e4)\n\nYou are not currently on a branch.\nPlease specify which branch you want to merge with.\nSee git-pull(1) for details.\n\n    git pull <remote> <branch>\n\nCollecting torch==2.0.1\n  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.0.0 (from torch==2.0.1)\n  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\nRequirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\nCollecting lit (from triton==2.0.0->torch==2.0.1)\n  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\nDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 619.9/619.9 MB 2.2 MB/s eta 0:00:00\nDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 317.1/317.1 MB 4.5 MB/s eta 0:00:00\nDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11.8/11.8 MB 96.7 MB/s eta 0:00:00\nDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21.0/21.0 MB 67.2 MB/s eta 0:00:00\nDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 849.3/849.3 kB 51.4 MB/s eta 0:00:00\nDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 557.1/557.1 MB 3.1 MB/s eta 0:00:00\nDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 168.4/168.4 MB 7.2 MB/s eta 0:00:00\nDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 54.6/54.6 MB 12.6 MB/s eta 0:00:00\nDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 102.6/102.6 MB 8.2 MB/s eta 0:00:00\nDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 173.2/173.2 MB 6.9 MB/s eta 0:00:00\nDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 177.1/177.1 MB 8.3 MB/s eta 0:00:00\nDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 98.6/98.6 kB 8.6 MB/s eta 0:00:00\nDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 63.3/63.3 MB 12.0 MB/s eta 0:00:00\nDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 96.4/96.4 kB 8.9 MB/s eta 0:00:00\nInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\nSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\nWARNING: The following packages were previously imported in this runtime:\n  [torch,torchgen]\nYou must restart the runtime in order to use newly installed versions.\n\nwhen I restart the session i am unable to run Tellu (step 3)\n\npython3: can't open file '/content/detect.py': [Errno 2] No such file or directory\n\\nThe script also accepts other parameters. See https://github.com/ultralytics/yolov5/blob/master/detect.py for all the options.\\n --max-detect 1000. Default is 1000; if you require more, add the parameter to the line of code above with a different value.\\n --hide-labels Adding this parameter will hide the name of the organoid class detected in the result images. Convenient in dense cultures and for publication purposes\\n\\nExample:\\n!python detect.py --weights $tellu --conf 0.35 --source $dir --save-txt --save-conf --name \"results\" --hide-labels --max-detect 2000\\n"
      }
    ]
  },
  {
    "issue_number": 12637,
    "title": "Export TensorRT engine - Device memory is insufficient to use tactic. - NX2 TX Jetson Platform. ",
    "author": "nicorl",
    "state": "closed",
    "created_at": "2024-01-16T11:18:44Z",
    "updated_at": "2025-03-25T09:58:58Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\nI have trained a PyTorch model (.pb) and I am looking to optimize it to consume less memory. The attempts to export it to TensorRT are causing issues because the device has limited available memory, and I'm unsure how to achieve a proper export.\r\n\r\nTrained through: \r\n```\r\npython train.py --data data/config_training.yaml --weights yolov5s.pt --img 640 --epochs 300 --device 0 --batch -1 --patience 15\r\n```\r\nLoading model on memory:\r\nVirtual environment with: Python 3.6.9, torch-1.8.0\r\nâ†’ The pytorch model (best.pt) requires approximately ~1.2GB of memory\r\nâ†’ The model's performance is good âœ“ ([best.pt](https://drive.google.com/file/d/1WZ-Yzz7SYqqLPvCGfvyzKLdCieDfA3YO/view?usp=drive_link))\r\n\r\nExporting to TensorRT over Jetson TX2 NX with Python 3.6.9, torch-1.8.0, TensorRT 8.0.1.6, ONNX 1.11.0\r\nExport command: \r\n\r\n```\r\nyolov5 export --weights best.pt --include [engine] --device 0 --imgsz [800,608] --data mydata.yaml --half --workspace 1\r\n```\r\n\r\nErrors & warning:\r\n```\r\nWARNING: âš ï¸ Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\r\n```\r\n\r\n```\r\n[TensorRT] ERROR: Tactic Device request: 277MB Available: 264MB. Device memory is insufficient to use tactic.\r\n[TensorRT] WARNING: Skipping tactic 3 due to oom error on requested size of 277 detected for tactic 5.\r\n[TensorRT] ERROR: Tactic Device request: 356MB Available: 267MB. Device memory is insufficient to use tactic.\r\n[TensorRT] WARNING: Skipping tactic 3 due to oom error on requested size of 356 detected for tactic 4.\r\n```\r\nFull log [here](https://drive.google.com/drive/u/0/folders/19G12tSoRmkNC03joFAosQLxUK5hnmLY4)\r\n\r\nThe model.engine file is generated, but when i load it, the performance is not good:\r\n![badResults](https://github.com/ultralytics/yolov5/assets/8526514/c0c3ea56-9e92-4e85-b448-fdb5ebbf7544)\r\n\r\nOn my PC with a larger amount of memory and Python v.3.10, the export process does not yield any warnings and works well. Because I canâ€™t increase the memory of my TX2 NX, I would like to understand how relevant the warnings are and what would be the best way to avoid them. I believe upgrading to Python 3.7 is one option, but I would like to know if there are possibilities to make it works on 3.6.9\r\n![goodResults](https://github.com/ultralytics/yolov5/assets/8526514/98eb72c5-e4d6-4d8f-98d1-2a8f497d9475)\r\n\r\nI've also put a [link in the Nvidia forums](https://forums.developer.nvidia.com/t/exporting-a-yolov5s-best-pb-model-to-engine-format-tensorrt/279123), but I think it's more appropriate here than there.\r\n\n\n### Environment\n\n**TensorRT Version**: 8.0.1.6\r\n**GPU Type**: NVIDIAÂ® Jetsonâ„¢ TX2 NX - 4 Core ARM A57 Complex and 2 core NVIDIA Denver2 64-bit CPU - 4GB LPDDR4 - NVIDIA Tegra X2, 3832MiB\r\n**Nvidia Driver Version**: nvidia-smi not installed\r\n**CUDA Version**: cuda_10.2_r440.TC440_70.29663091_0\r\n**Operating System + Version**: Linux aaeon-desktop 4.9.253-tegra  SMP PREEMPT Mon Jul 26 12:19:28 PDT 2021 aarch64 GNU/Linux\r\n**Python Version**: 3.6.9\r\n**PyTorch Version**: 1.8.0\r\n**ONNX Version**: 1.11.0\r\n**yolov5 Version**: 6.2.1\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @nicorl, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@nicorl hello! Thanks for the detailed issue report. It seems like you're encountering out-of-memory (OOM) errors when exporting your model to TensorRT on the Jetson TX2 NX platform. Here are a few suggestions that might help:\n\n1. **Reduce Workspace Size**: Try reducing the workspace size during export. Although you've set it to 1GB, you might need to go lower due to the limited memory on the TX2 NX.\n\n2. **Optimize Model**: If possible, optimize your model further before export. This could involve pruning or quantization to reduce the model size and memory requirements.\n\n3. **Python Version**: While upgrading to Python 3.7 might not directly solve the OOM issue, it's recommended to use the Python version that YOLOv5 is tested on for best compatibility.\n\n4. **Batch Size**: If you're using a batch size greater than 1 during inference, try reducing it to 1 to save memory.\n\n5. **Close Other Processes**: Ensure that no other memory-intensive processes are running on the device when exporting the model.\n\n6. **TensorRT Version**: Check if there's a more memory-efficient version of TensorRT available for your platform.\n\n7. **Model Precision**: You're using the `--half` flag which uses FP16 precision. Make sure that your device supports FP16 operations efficiently.\n\nIf these suggestions don't resolve the issue, consider posting the full error log and your export command in the issue for further assistance. Remember to check the Ultralytics Docs for more information on exporting models and troubleshooting.\n\nKeep in mind that the Jetson TX2 NX has limited memory, and some models may simply be too large to run efficiently on this platform. You might need to consider a more powerful device if memory constraints continue to be an issue.\n\nThanks for contributing to the YOLOv5 community, and we appreciate your efforts in making the model work on edge devices! ðŸš€"
      },
      {
        "user": "nicorl",
        "body": "Hi again,\r\nI've just tried to install all from the source\r\n- Python 3.7.12 \r\n- Torch 1.8.0\r\n- Torchvision 0.9.0 \r\n- CMake 3.21.4 \r\n- TensorRT 8.2 \r\n\r\nBut ultimately, generating the TensorRT 8.2 wheel has posed issues for which I cannot find any solution. Below, I provide all the executed commands as information for anyone reading this message. It's worth noting that the next steps will involve trying to use Python version 3.8 and the rest of the software in appropriate versions. As I make significant progress, I will leave further comments. \r\n\r\n[Nvidia forum link](https://forums.developer.nvidia.com/t/exporting-a-yolov5s-best-pb-model-to-engine-format-tensorrt/279123/8) where someone is helping to solve this\r\n"
      }
    ]
  },
  {
    "issue_number": 11768,
    "title": "å¦‚ä½•è®­ç»ƒå›¾ç‰‡å°ºå¯¸ä¸ºï¼ˆ640,480ï¼‰çš„æ¨¡åž‹",
    "author": "xingjunhong",
    "state": "closed",
    "created_at": "2023-06-25T02:31:52Z",
    "updated_at": "2025-03-24T04:23:11Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nç›®å‰å®˜æ–¹çš„æ¨¡åž‹è®­ç»ƒçš„å°ºå¯¸æ˜¯æ­£æ–¹å½¢å°ºå¯¸ï¼Œå¦‚ä½•è®­ç»ƒä¿®æ”¹ä¸ºé•¿æ–¹å½¢ï¼ˆ640,480ï¼‰å°ºå¯¸çš„æ¨¡åž‹ï¼Ÿ\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@xingjunhong hi there,\n\nTo train the YOLOv5 model with a rectangular shape such as (640, 480), you can modify the input size in the training configuration file. Simply edit the `--img` argument in the `--hyp` section of the `yaml` file to specify your desired dimensions (e.g., `--img: '640, 480'`). This will ensure that the input images are resized to the specified dimensions during training.\n\nPlease note that modifying the input size may affect the performance and accuracy of your model, so it's important to experiment with different sizes and evaluate the results accordingly.\n\nIf you have any further questions or encounter any issues, please don't hesitate to ask.\n\nBest regards,\nGlenn Jocher"
      },
      {
        "user": "xingjunhong",
        "body": "> @xingjunhong hi there,\r\n> \r\n> To train the YOLOv5 model with a rectangular shape such as (640, 480), you can modify the input size in the training configuration file. Simply edit the `--img` argument in the `--hyp` section of the `yaml` file to specify your desired dimensions (e.g., `--img: '640, 480'`). This will ensure that the input images are resized to the specified dimensions during training.\r\n> \r\n> Please note that modifying the input size may affect the performance and accuracy of your model, so it's important to experiment with different sizes and evaluate the results accordingly.\r\n> \r\n> If you have any further questions or encounter any issues, please don't hesitate to ask.\r\n> \r\n> Best regards, Glenn Jocher\r\n\r\nä¿®æ”¹è¿™ä¸ªå‚æ•°åŽï¼Œä¼šå‡ºçŽ°æŠ¥é”™ã€‚\r\nparser.add_argument('--imgsz', '--img', '--img-size', type=int, default=[640, 480], help='train, val image size (pixels)')\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 634, in <module>\r\n    main(opt)\r\n  File \"train.py\", line 528, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"train.py\", line 283, in train\r\n    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\r\n  File \"/mnt/hdd/home/xingjh/anaconda3/envs/yolov5/lib/python3.8/site-packages/tqdm/std.py\", line 1195, in __iter__\r\n    for obj in iterable:\r\n  File \"/mnt/hdd2/extension/xingjh/02.work/yolov5/utils/dataloaders.py\", line 172, in __iter__\r\n    yield next(self.iterator)\r\n  File \"/mnt/hdd/home/xingjh/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\r\n    data = self._next_data()\r\n  File \"/mnt/hdd/home/xingjh/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1085, in _next_data\r\n    return self._process_data(data)\r\n  File \"/mnt/hdd/home/xingjh/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1111, in _process_data\r\n    data.reraise()\r\n  File \"/mnt/hdd/home/xingjh/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/_utils.py\", line 428, in reraise\r\n    raise self.exc_type(msg)\r\nTypeError: Caught TypeError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File \"/mnt/hdd/home/xingjh/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/mnt/hdd/home/xingjh/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/mnt/hdd/home/xingjh/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/mnt/hdd2/extension/xingjh/02.work/yolov5/utils/dataloaders.py\", line 669, in __getitem__\r\n    img, (h0, w0), (h, w) = self.load_image(index)\r\n  File \"/mnt/hdd2/extension/xingjh/02.work/yolov5/utils/dataloaders.py\", line 737, in load_image\r\n    r = self.img_size / max(h0, w0)  # ratio\r\nTypeError: unsupported operand type(s) for /: 'list' and 'int'"
      },
      {
        "user": "glenn-jocher",
        "body": " @xingjunhong I apologize for the confusion. It seems that the issue you encountered is related to a type mismatch in the `imgsz` argument.\n\nTo resolve this error, make sure that the `imgsz` argument in the `--hyp` section of the `yaml` file is specified as a single integer (e.g., `imgsz: 640`). The `img_size` argument should be a single integer value, not a list. \n\nOnce you make this adjustment, you should be able to train the model successfully with the desired rectangular shape (640, 480).\n\nIf you have any further questions or need additional assistance, please feel free to ask.\n\nBest regards,\nGlenn Jocher"
      }
    ]
  },
  {
    "issue_number": 13541,
    "title": "gpu",
    "author": "wg231",
    "state": "open",
    "created_at": "2025-03-22T08:33:48Z",
    "updated_at": "2025-03-22T14:01:00Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\ni have 8g memory but print gpu_mem>8 and it trains \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @wg231, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us debug it, including the exact code or commands you're running, and your environment details.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including:\n- GPU details (e.g., model, CUDA version)\n- Training logs or screenshots of the output\n- Any modifications to the original code or model\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response, but an Ultralytics engineer will review your question and provide further assistance soon. Thank you for your patience! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@wg231 the GPU memory warning is a rough estimate. Actual training memory usage depends on your batch size, model size, and dataâ€”if it's working, you're likely within safe limits. Monitor real-time usage with `nvidia-smi` in terminal. For adjustments, see our Training Tips Guide, particularly the batch size section. Let us know if you encounter actual errors during training! ðŸš€"
      }
    ]
  },
  {
    "issue_number": 13537,
    "title": "how to set label smoothing in yolov8/yolov11?",
    "author": "xuan-xuan6",
    "state": "open",
    "created_at": "2025-03-20T07:50:55Z",
    "updated_at": "2025-03-21T10:35:15Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nhow to set label smoothing in yolov8/yolov11?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @xuan-xuan6, thank you for your interest in YOLOv5 ðŸš€! For guidance on YOLO8 or YOLO11, we recommend referring to the official [Ultralytics Documentation](https://docs.ultralytics.com/), which covers all the latest features and configurations, including label smoothing.\n\nIf this is a custom training â“ Question, please provide as much detail as possible, including your use case, relevant configurations, and any specific challenges you're facing. For the best results, ensure you're following the [Training Tips](https://docs.ultralytics.com/guides/model-training-tips/) for optimal model performance.\n\nIf this is a ðŸ› Bug Report, please include a **minimum reproducible example** to help us investigate the issue effectively.\n\n## Requirements\n\nMake sure your environment meets the following setup:\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To set up YOLOv5:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone repository\ncd yolov5\npip install -r requirements.txt  # install dependencies\n```\n\n## Environments\n\nYou can run YOLOv5 and other Ultralytics models in the following verified environments:\n- **Notebooks** with free GPUs: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. These tests validate the correct operation of YOLOv5 and related tools.\n\nThis is an automated response to assist you as quickly as possible. An Ultralytics engineer will review your issue and provide additional support soon. Thank you for your patience! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@xuan-xuan6 for YOLOv8 label smoothing: Set `label_smoothing` in your dataset YAML under `args` or pass it directly in the training command like `model.train(data=..., label_smoothing=0.1)`. See the Ultralytics YOLOv8 Training Documentation for details.\n\nYOLO11 isn't released yet - stay tuned to our GitHub repositories for updates on new releases! ðŸš€"
      },
      {
        "user": "shining-love",
        "body": "> [@xuan-xuan6](https://github.com/xuan-xuan6) for YOLOv8 label smoothing: Set `label_smoothing` in your dataset YAML under `args` or pass it directly in the training command like `model.train(data=..., label_smoothing=0.1)`. See the Ultralytics YOLOv8 Training Documentation for details.\n> \n> YOLO11 isn't released yet - stay tuned to our GitHub repositories for updates on new releases! ðŸš€\n\nThank you for your reply. I see that label-smoothing has been removed in the latest version of u. So, does that mean label-smoothing cannot be set in the latest version of u?"
      }
    ]
  },
  {
    "issue_number": 13362,
    "title": "NMS time limit issue on MPS, Works in CPU",
    "author": "shanalikhan",
    "state": "open",
    "created_at": "2024-10-16T15:14:51Z",
    "updated_at": "2025-03-20T08:49:14Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi, \r\nI'm using `yolo11n` \r\n\r\nThis line works\r\n```\r\nresults = model.train(data=\"/Users/shan/Documents/iqlogy/047/yolo8obb_2347/yoloobb_main.yaml\", epochs=1, imgsz=512)#,device=\"mps\")\r\n```\r\nGenerates the following output:\r\n\r\n```\r\ntrain: Scanning [/Users/shan/Documents/iqlogy/047/yolo8obb_2347/labels/train.cache...](https://file+.vscode-resource.vscode-cdn.net/Users/shan/Documents/iqlogy/047/yolo8obb_2347/labels/train.cache...) 2117 images, 500 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2117/2117 [00:00<?, ?it/s]\r\nval: Scanning [/Users/shan/Documents/iqlogy/047/yolo8obb_2347/labels/test.cache...](https://file+.vscode-resource.vscode-cdn.net/Users/shan/Documents/iqlogy/047/yolo8obb_2347/labels/test.cache...) 230 images, 60 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<?, ?it/s]\r\nPlotting labels to runs/detect/train132/labels.jpg... \r\n\r\noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \r\noptimizer: AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\r\nTensorBoard: model graph visualization added âœ…\r\nImage sizes 512 train, 512 val\r\nUsing 0 dataloader workers\r\nLogging results to runs/detect/train132\r\nStarting training for 1 epochs...\r\n\r\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n        1/1         0G      2.141      1.816      1.171         27        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [07:45<00:00,  3.50s/it]\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:22<00:00,  2.84s/it]\r\n                   all        230       1052      0.562      0.417       0.43      0.158\r\n\r\n1 epochs completed in 0.137 hours.\r\nOptimizer stripped from runs/detect/train132/weights/last.pt, 5.4MB\r\nOptimizer stripped from runs/detect/train132/weights/best.pt, 5.4MB\r\n\r\nValidating runs/detect/train132/weights/best.pt...\r\nWARNING âš ï¸ validating an untrained model YAML will result in 0 mAP.\r\nUltralytics 8.3.13 ðŸš€ Python-3.12.7 torch-2.4.0 CPU (Apple M3)\r\nYOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:21<00:00,  2.63s/it]\r\n                   all        230       1052      0.564      0.417      0.429      0.157\r\nSpeed: 0.4ms preprocess, 86.0ms inference, 0.0ms loss, 0.8ms postprocess per image\r\nResults saved to runs/detect/train132\r\n```\r\n\r\n\r\n\r\nNow converting from CPU to `mps` shows warning `WARNING âš ï¸ NMS time limit 3.600s exceeded`\r\n\r\n```\r\nresults = model.train(data=\"/Users/shan/Documents/iqlogy/047/yolo8obb_2347/yoloobb_main.yaml\", epochs=1, imgsz=512, optimizer='SGD',device=\"mps\",lr0=0.001,verbose=True)\r\n\r\n```\r\nTransferred 448/499 items from pretrained weights\r\nTensorBoard: Start with 'tensorboard --logdir runs/detect/train14', view at http://localhost:6006/\r\nFreezing layer 'model.23.dfl.conv.weight'\r\ntrain: Scanning [/Users/shan/Documents/iqlogy/047/yolo8obb_2347/labels/train.cache...](https://file+.vscode-resource.vscode-cdn.net/Users/shan/Documents/iqlogy/047/yolo8obb_2347/labels/train.cache...) 2117 images, 500 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2117/2117 [00:00<?, ?it/s]\r\nval: Scanning [/Users/shan/Documents/iqlogy/047/yolo8obb_2347/labels/test.cache...](https://file+.vscode-resource.vscode-cdn.net/Users/shan/Documents/iqlogy/047/yolo8obb_2347/labels/test.cache...) 230 images, 60 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<?, ?it/s]\r\nPlotting labels to runs/detect/train14/labels.jpg... \r\noptimizer: SGD(lr=0.001, momentum=0.937) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\r\nTensorBoard: model graph visualization added âœ…\r\nImage sizes 512 train, 512 val\r\nUsing 0 dataloader workers\r\nLogging results to runs/detect/train14\r\nStarting training for 1 epochs...\r\n\r\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n        1/1      4.06G      2.419      3.446      1.311         27        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [02:19<00:00,  1.05s/it]\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/8 [00:00<?, ?it/s]\r\nWARNING âš ï¸ NMS time limit 3.600s exceeded\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  12%|â–ˆâ–Ž        | 1/8 [00:12<01:29, 12.76s/it]\r\nWARNING âš ï¸ NMS time limit 3.600s exceeded\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:21<01:00, 10.16s/it]\r\nWARNING âš ï¸ NMS time limit 3.600s exceeded\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:29<00:46,  9.26s/it]\r\nWARNING âš ï¸ NMS time limit 3.600s exceeded\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:38<00:38,  9.60s/it]\r\n```\r\n```\r\n\r\n\r\nAm i missing something, how to optimise in MPS\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "pderrenger",
        "body": "@shanalikhan the NMS time limit warning on MPS might be due to hardware or software constraints. Ensure you're using the latest YOLOv5 version and PyTorch with MPS support. You can try reducing the batch size or image size to see if it helps. For more details, check the [multi-GPU training tutorial](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/)."
      },
      {
        "user": "ShreyJais",
        "body": "I am facing same issue using yolov8n "
      },
      {
        "user": "pderrenger",
        "body": "@ShreyJais thanks for reporting this issue. To help investigate, please ensure you're using the latest versions of both `ultralytics` and `torch`, as MPS support continues to improve. If the NMS delay persists, you might consider temporarily using CPU for validation (`device='cpu'`) or reducing image size/batch size. The team is actively working on optimizing MPS performance across all YOLO versions. \n\nFor specific YOLOv8 support, please create a minimum reproducible example using our [Bug Report Guidelines](https://github.com/ultralytics/ultralytics/issues/new/choose) in the Ultralytics YOLOv8 repo."
      }
    ]
  },
  {
    "issue_number": 4975,
    "title": "Roboflow for Datasets, Labeling, and Active Learning ðŸŒŸ",
    "author": "Jacobsolawetz",
    "state": "open",
    "created_at": "2021-09-28T14:00:45Z",
    "updated_at": "2025-03-19T21:41:24Z",
    "labels": [
      "documentation"
    ],
    "body": "You can now use Roboflow to organize, label, prepare, version, and host your datasets for training YOLOv5 ðŸš€ models. Roboflow is free to use with YOLOv5 if you make your workspace public. UPDATED 30 September 2021.\r\nâ€‹\r\n# Upload\r\nYou can upload your data to Roboflow via [web UI](https://docs.roboflow.com/adding-data), [rest API](https://docs.roboflow.com/adding-data/upload-api), or [python](https://docs.roboflow.com/python).\r\nâ€‹\r\n# Labeling\r\nAfter uploading data to Roboflow, you can label your data and review previous labels.\r\nâ€‹\r\n[![Roboflow Annotate](https://roboflow-darknet.s3.us-east-2.amazonaws.com/roboflow-annotate.gif)](https://roboflow.com/annotate)\r\nâ€‹\r\n# Versioning\r\nYou can make versions of your dataset with different preprocessing and offline augmentation options. YOLOv5 does online augmentations natively, so be intentional when layering Roboflow's offline augs on top.\r\nâ€‹\r\n![Roboflow Preprocessing](https://roboflow-darknet.s3.us-east-2.amazonaws.com/robolfow-preprocessing.png)\r\nâ€‹\r\n# Exporting Data\r\nYou can download your data in YOLOv5 format to quickly begin training.\r\nâ€‹\r\n```\r\nfrom roboflow import Roboflow\r\nrf = Roboflow(api_key=\"YOUR API KEY HERE\")\r\nproject = rf.workspace().project(\"YOUR PROJECT\")\r\ndataset = project.version(\"YOUR VERSION\").download(\"yolov5\")\r\n```\r\nâ€‹\r\n# Custom Training\r\nWe have released a custom training tutorial demonstrating all of the above capabilities. You can access the code here:\r\nâ€‹\r\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb)\r\nâ€‹\r\n# Active Learning\r\nThe real world is messy and your model will invariably encounter situations your dataset didn't anticipate. Using [active learning](https://blog.roboflow.com/what-is-active-learning/) is an important strategy to iteratively improve your dataset and model. With the Roboflow and YOLOv5 integration, you can quickly make improvements on your model deployments by using a battle tested machine learning pipeline.\r\nâ€‹\r\n<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png\"/></a></p>\r\nâ€‹\r\nPlease let us know of any curiosities or requests below ðŸ‘‡",
    "comments": [
      {
        "user": "gembancud",
        "body": "Its so painful trying to make supervisely work with the current release. My team has already established workflows in supervisely but getting the necessary updates make it feel like a hassle. Would the tradeoff for a public workspace be worth it in this case? Were at the phase of reiterating assisted labelling to make it easier to improve the overall model. Would roboflow help better than supervisely? Are there any migration tutorials?"
      },
      {
        "user": "glenn-jocher",
        "body": "@Jacobsolawetz just noticed some broken links above. Can you send us some fixes? Thanks!"
      },
      {
        "user": "glenn-jocher",
        "body": "@Jacobsolawetz pinging you again about the broken links above"
      }
    ]
  },
  {
    "issue_number": 13536,
    "title": "Why the confusion matrix when I test the network only prints the first row?",
    "author": "giuliohubb",
    "state": "open",
    "created_at": "2025-03-18T13:16:52Z",
    "updated_at": "2025-03-19T15:52:10Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello!!\n\nI am using YOLOv5 for a small object detection problem. \n\nWhen I try to test the model after the training on the test set I use the val.py file with task = test and it works correctly. The only issue is that when it saves the confusion matrix it prints only the first row (Here's two examples, one with normalise = True and one with normalise = False). \n\nDo you know why this is the case? I tried to look at the code but I've found no issues. I have three classes ['cosmics', 'stars', 'satellites'], cosmics are overrepresented - 63383 instances - followed by stars - 6365 instances - and satellites - 620 instances. The IOU thres is 0.5 and I haven't touched the conf_thres\n\n<img width=\"837\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e9955f65-c100-4a14-8f77-a1b83350bc74\" />\n\n<img width=\"843\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e72c9e32-a91c-4895-9907-84a3508fb33b\" />\n\n### Additional\n\nOne other thing: \n\nthe image size is 532, in training it has been rounded to 544 because it must be multiple of max stride 32. When I test the network if I put the same image size - 544 - it reaches a mAP = 0.658 while if I put image size = 640 it reaches a mAP = 0.852. Do you have an explanation for this behaviour? In training the image size was 544, reaching an mAP = 0.874. And the confusion matrix in the training is printed correctly (I put it down below). I have seen the documentation many times without finding those answers. Thank you very much for your help. \n\n<img width=\"822\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a7453005-291f-4452-86d9-6455c4ff6981\" />\n\n",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @giuliohubb, thank you for reaching out and for your detailed explanation and examples! ðŸ˜Š\n\nThis is an automated response to let you know that your issue is being reviewed. An Ultralytics engineer will assist you soon.\n\nIn the meantime, here are a few steps you can take to ensure we can better assist you:\n\n- If this is a ðŸ› **Bug Report**, please provide a **minimum reproducible example (MRE)**, including a clear description of the steps to reproduce the issue, your environment details (Python version, PyTorch version, etc.), and any relevant code snippets or configurations.\n  \n- If this is a â“ **Question**, please share as much detail as possible, including any relevant training logs, dataset details, and the specific commands you are using. You can also verify your setup against our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\nEnsure you are using [**Python>=3.8.0**](https://www.python.org/) and have installed all required dependencies from our [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt), including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To check or update your environment, you can follow these steps:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 supports several verified environments with all dependencies preinstalled, such as:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\nOur automated tests ensure that YOLOv5 works correctly across macOS, Windows, and Ubuntu for training, validation, inference, export, and benchmarking. You can check the status here:\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf the issue persists or you discover new details, feel free to update this thread. We appreciate your patience and contributions! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@giuliohubb the confusion matrix showing only the first row typically occurs when your model is overwhelmingly predicting one class (likely 'cosmics' due to severe class imbalance). This indicates the model isn't detecting other classes during validation. For small object detection, we recommend:\n\n1. **Class Imbalance Mitigation**: Try `class_weights` in your loss function or oversample minority classes\n2. **Input Size Consistency**: Use matching train/val image sizes (544x544 in your case). The mAP increase at 640 suggests your test data benefits from higher resolution, but this should be validated against your training setup\n\nFor confusion matrix interpretation, see our [Confusion Matrix guide](https://www.ultralytics.com/glossary/confusion-matrix). To verify predictions, inspect `model.predict()` outputs directly."
      },
      {
        "user": "giuliohubb",
        "body": "Thank you for your answer.\n\n'This indicates the model isn't detecting other classes during validation.' \n\nI don't think this was the case, indeed if you look at the first confusion matrix I sent you can clearly see the colour code in the shells correctly displayed, only the numbers are missing. I think it's an issue in the plots. Also if you look at the second confusion matrix you can see that 1180/6365 stars were misclassified as cosmics, but that means that the rest were correctly classified. \nAdditionally during the training the class imbalance was the same, but the confusion matrix has been printed correctly. \nI add here the images from the validation,  related to the prediction, showing that the model isn't detecting only cosmics. \n\n<img width=\"713\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/622960c4-6f05-4b47-997f-dda0081dea0f\" />"
      }
    ]
  },
  {
    "issue_number": 13517,
    "title": "yolov5 failed to work in esp32cam",
    "author": "Sadek-21",
    "state": "open",
    "created_at": "2025-02-21T19:54:34Z",
    "updated_at": "2025-03-17T04:37:57Z",
    "labels": [
      "question",
      "detect",
      "embedded"
    ],
    "body": "\n\nso I have this project for detecting objects and I use yolov5 to detect the object that I want and I do this step I try the code and its working in the cam in my laptop and now I want to detect with my ESP32-cam and the code didn't work in my esp32 it showing me video but its to slow and detecting the movement with delay and when I show in front of the espcam the object that I trained it didn't detected \n\nPleas if some one have an idea about this ",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Sadek-21, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us debug it. This could include your code snippets, configuration details, and steps to reproduce the issue. Without more specifics, itâ€™s challenging to identify the problem. For hardware limitations like those encountered with ESP32-Cam, please ensure to provide details about your deployment approach and environment.\n\nIf this is a custom training â“Question, kindly share more context, including:\n- Examples of your dataset and expected detection outputs\n- Logs or outputs from the model when running on your ESP32-Cam device\n- Details about the setup and any modifications youâ€™ve made to the YOLOv5 code or configuration\n\nAdditionally, YOLOv5 is designed to run most efficiently on devices with high compute capabilities (like GPUs or TPUs). Microcontrollers like the ESP32-Cam may not handle the computational requirements of YOLOv5 effectively because of limited processing power, resulting in slow performance. For such use cases, you might also consider exploring model quantization or smaller versions of the model designed explicitly for edge devices.\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 runs in various computational environments. However, for environments with limited resources, explore lightweight inference solutions or optimized deployments. See below for verified environments where YOLOv5 runs efficiently:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\nFor highly constrained environments like microcontrollers, look into options such as converting the YOLOv5 model to [ONNX](https://github.com/ultralytics/yolov5/blob/master/export.py) or TensorFlow Lite for edge deployments to optimize performance.\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify the correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nâš¡ This is an automated response. An Ultralytics engineer will review your issue and provide further assistance soon. Let us know if you have additional details to share in the meantime! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@Sadek-21 while YOLOv5 is effective on laptops, deploying it on an ESP32-CAM presents significant challenges due to the device's limited processing power and memory, as described in the article on deploying YOLOv5 with Neural Magic's DeepSparse, it highlights achieving GPU-class performance on CPUs, but even this might be too resource-intensive for an ESP32-CAM. [Deploy Ultralytics YOLOv5 With Neural Magicâ€™s DeepSparse for GPU-Class Performance on CPUs](https://www.ultralytics.com/blog/deploy-yolov5-with-neural-magics-deepsparse-for-gpu-class-performance-on-cpus)."
      },
      {
        "user": "Sadek-21",
        "body": "thank you so much ill try it and ill tel you if its work thank you \n"
      }
    ]
  },
  {
    "issue_number": 10452,
    "title": "Can I use polygon annotation to train Yolov5 for  object detection? ",
    "author": "VishnuDurairaj",
    "state": "closed",
    "created_at": "2022-12-09T08:59:42Z",
    "updated_at": "2025-03-17T03:13:34Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nCan I use polygon annotation to train Yolov5 for  object detection? \n\n### Additional\n\nI am wondering if we can use polygon annotation to train an object detection model will it perform better that rectangle annotation?. is it possible to train Yolov5 with polygon annotation?",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @VishnuDurairaj, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "ExtReMLapin",
        "body": "yes it's called segmentation (segm)"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello, this issue has been automatically marked as stale because it has not had recent activity. Please note it will be closed if no further activity occurs.\n\nAccess additional [YOLOv5](https://ultralytics.com/yolov5) ðŸš€ resources:\n- **Wiki** â€“ https://github.com/ultralytics/yolov5/wiki\n- **Tutorials** â€“ https://docs.ultralytics.com/yolov5\n- **Docs** â€“ https://docs.ultralytics.com\n\nAccess additional [Ultralytics](https://ultralytics.com) âš¡ resources:\n- **Ultralytics HUB** â€“ https://ultralytics.com/hub\n- **Vision API** â€“ https://ultralytics.com/yolov5\n- **About Us** â€“ https://ultralytics.com/about\n- **Join Our Team** â€“ https://ultralytics.com/work\n- **Contact Us** â€“ https://ultralytics.com/contact\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLOv5 ðŸš€ and Vision AI â­!"
      }
    ]
  },
  {
    "issue_number": 13529,
    "title": "drone resolutions for input yolov5",
    "author": "dimasadef72",
    "state": "open",
    "created_at": "2025-03-10T09:03:52Z",
    "updated_at": "2025-03-15T22:39:39Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nhalo\n\nI am using a drone and trying to detect rice fields using yolov5. after successfully detecting the segmentation, I used the Ground Sampling Distance method to calculate the area of the rice field segmentation based on the number of pixels. my problem now is that the drone image input is about 20MP, but I have tried yolov5 and resized it automatically to 3008 x 2272 pixels. is there any suggestion for yolo to maintain the number of 20 mp pixels?\n\nplease ask for the answer from the professionals, thank you good health always\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @pepsodent72, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training or implementation â“ Question, such as your scenario involving drone imagery and high-resolution input, please provide as much detail as possible, including:\n- Example drone images or data (if shareable)\n- Any specific YOLOv5 configuration or preprocessing steps you are using\n- Logs or output details during your tests\n- How resizing is being handled or any relevant code snippets\n\nAdditionally, it is worth noting that YOLOv5 automatically resizes inputs to the model's default training resolution. To work with high-resolution images (like 20MP), you may need to experiment with custom input sizes by modifying the `--img` parameter during training or inference. This could have implications on memory usage and performance, so adjustments may be necessary.\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in various environments, including:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response to help guide you. An Ultralytics engineer will assist you further as soon as possible. Let us know if you have additional details to share in the meantime! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@pepsodent72 for drone imagery at native 20MP resolution, we recommend:\n\n1. **Tile Inference**: Split large images into smaller tiles (e.g., 640x640) using sliding window detection. This maintains original resolution while accommodating YOLOv5's default input size. Example:\n```python\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\nresults = model('large_image.jpg', tile=512)  # tile size 512\n```\n\n2. **Adjust Input Size**: Train with higher resolution using `--img` argument (requires retraining):\n```python\npython train.py --img 1280  # maximum size depends on GPU memory\n```\n\n3. **Verify GSD Calibration**: Ensure your Ground Sampling Distance calculations account for drone altitude and sensor specs, as demonstrated in our [AI in agriculture solutions](https://www.ultralytics.com/solutions/ai-in-agriculture).\n\nFor commercial deployments requiring high-res processing, consider an [Ultralytics Enterprise License](https://ultralytics.com/license) for optimized large-scale implementations."
      },
      {
        "user": "dimasadef72",
        "body": "> [@pepsodent72](https://github.com/pepsodent72) for drone imagery at native 20MP resolution, we recommend:\n> \n> 1. **Tile Inference**: Split large images into smaller tiles (e.g., 640x640) using sliding window detection. This maintains original resolution while accommodating YOLOv5's default input size. Example:\n> \n> model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n> results = model('large_image.jpg', tile=512)  # tile size 512\n> 2. **Adjust Input Size**: Train with higher resolution using `--img` argument (requires retraining):\n> \n> python train.py --img 1280  # maximum size depends on GPU memory\n> 3. **Verify GSD Calibration**: Ensure your Ground Sampling Distance calculations account for drone altitude and sensor specs, as demonstrated in our [AI in agriculture solutions](https://www.ultralytics.com/solutions/ai-in-agriculture).\n> \n> For commercial deployments requiring high-res processing, consider an [Ultralytics Enterprise License](https://ultralytics.com/license) for optimized large-scale implementations.\n\nare methods sliding window and SAHI is different?"
      }
    ]
  },
  {
    "issue_number": 13535,
    "title": "yolo-improvement",
    "author": "bluesky950520",
    "state": "open",
    "created_at": "2025-03-15T03:04:10Z",
    "updated_at": "2025-03-15T21:53:28Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar feature requests.\n\n\n### Description\n\nYolo is very wonderful for my badge\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @bluesky950520, thank you for your enthusiasm for YOLOv5 ðŸš€! We're thrilled to hear you're enjoying it! \n\nIf you have ideas or improvements to share, we'd love to hear more details about your use case and any specific enhancements you're considering. If this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us understand and investigate it effectively.\n\nFor general improvements or contributions, you can also submit a **Pull Request (PR)** following our [Contributing Guidelines](https://github.com/ultralytics/yolov5/blob/master/CONTRIBUTING.md).\n\n## Resources to Get Started\n\n- â­ï¸ [YOLOv5 Tutorials](https://docs.ultralytics.com/yolov5/) for setup and advanced usage\n- [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/)\n- [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)\n\n## Requirements\n\nMake sure you have:\n[**Python>=3.8.0**](https://www.python.org/) with all dependencies installed:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYou can run YOLOv5 in the following verified environments:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>  \nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing.\n\nThis is an automated response to assist you quickly, but an Ultralytics engineer will follow up soon to provide further guidance ðŸ˜Š."
      },
      {
        "user": "pderrenger",
        "body": "Thank you for the kind words! We're glad you're finding YOLOv5 useful. The credit goes to our amazing open-source community and contributors. If you're interested in contributing, feel free to explore our [contributing guidelines](https://github.com/ultralytics/yolov5/blob/master/CONTRIBUTING.md) for ways to get involved."
      },
      {
        "user": "bluesky950520",
        "body": "thank you for your message.\nI hope you to be my friend"
      }
    ]
  },
  {
    "issue_number": 7655,
    "title": "train without validation data",
    "author": "valleyboytoo",
    "state": "closed",
    "created_at": "2022-05-01T04:46:49Z",
    "updated_at": "2025-03-13T18:49:07Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI wonder how to train without validation data. I got an error like below.\r\n\r\nKeyError: 'val' (When I delete the validation path in the yaml file)\r\nDataset not found âš ï¸, missing paths ['/kaggle/input/ts-detection-0430/valid/images'](When I just leave the path in the yaml file) ==>Of course, there is no 'valid' folder.\r\n\r\n- only train/test dataset(no validation)\r\n- 39,000 images => it's not working at Roboflow(max 10,000)\r\n- I had to split the dataset manually using codes\r\n- I tried sample data at Roboflow => Roboflow made validation path in yaml file.(I don't understand why) I set 0 validation in Roboflow.\r\n- I guess there must be a way to block the process of searching validation path when executing train.py\r\n- I was searching for hours about this, but I don't know what to do\r\n- If my dataset is not big, it must be easy with Roboflow. I manually split the dataset and want to train without validation. That's it. Thank you in advance.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @valleyboytoo, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions\"><img src=\"https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg\" alt=\"CI CPU testing\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), validation ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@valleyboytoo validation set is required to train."
      },
      {
        "user": "valleyboytoo",
        "body": "I got it. Thank you Glenn. I thought it an option."
      }
    ]
  },
  {
    "issue_number": 13534,
    "title": "val.pyé‡Œçš„taskä¸ºspeedæ—¶ï¼Œç²¾åº¦ä¼šä¸Šæ¶¨è¾ƒå¤šï¼Ÿ",
    "author": "lqh964165950",
    "state": "open",
    "created_at": "2025-03-13T08:50:50Z",
    "updated_at": "2025-03-13T17:31:48Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nval.pyé‡Œçš„taskä¸ºspeedæ—¶ï¼Œç²¾åº¦ä¼šä¸Šæ¶¨è¾ƒå¤šï¼Ÿ\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @lqh964165950, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including details about the specific `val.py` task, your setup, dataset examples, and logs, so we can better assist you. Additionally, please confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nðŸ” This is an automated response, but an Ultralytics engineer will respond to you soon to assist further!"
      },
      {
        "user": "pderrenger",
        "body": "@lqh964165950 åœ¨`val.py`ä¸­è®¾ç½®`task=speed`æ—¶ä¼šç¦ç”¨æ•°æ®åŠ è½½å»¶è¿Ÿã€æ•°æ®å¢žå¼ºå’ŒåŽå¤„ç†ç­‰éžæ ¸å¿ƒæ“ä½œï¼Œä¸“æ³¨äºŽæµ‹é‡çº¯æŽ¨ç†é€Ÿåº¦ã€‚è¿™ä¼šå‡å°‘å®žé™…æŽ¨ç†åœºæ™¯ä¸­çš„å¹²æ‰°å› ç´ ï¼Œä½†**ä¸ä»£è¡¨çœŸå®žç²¾åº¦**ã€‚è‹¥éœ€éªŒè¯ç²¾åº¦ï¼Œè¯·ä½¿ç”¨`task=val`æˆ–ä¿æŒé»˜è®¤å‚æ•°ï¼Œè¿™ä¼šå®Œæ•´æ‰§è¡Œé¢„å¤„ç†/åŽå¤„ç†æµç¨‹å¹¶åæ˜ çœŸå®žæ€§èƒ½æŒ‡æ ‡ã€‚\n\nç›¸å…³å®žçŽ°ç»†èŠ‚å¯å‚è€ƒYOLOv5çš„[æµ‹è¯•å’ŒéªŒè¯æ–‡æ¡£](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#test-and-validate)ã€‚å»ºè®®é€šè¿‡`python val.py --task val`èŽ·å–æ­£å¼è¯„ä¼°ç»“æžœã€‚"
      }
    ]
  },
  {
    "issue_number": 13530,
    "title": "How to make yolo11 training with added P2 layer convergeï¼Ÿ",
    "author": "starzero-hhh",
    "state": "open",
    "created_at": "2025-03-11T13:00:36Z",
    "updated_at": "2025-03-12T16:56:20Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I would like to ask a question about the training strategy, I am using yolo11 model I want to optimize the small target detection I now add P2 layer in yolo11 and then remove the P5 layer, normal can be trained up, I use layered optimization except the newly added P2 layer alone using the learning rate of 5e-4 optimization the other original existence of the layer using the learning rate of 1e-4 to optimize, I put the pre-training weights I loaded the pre-training weights yolo11n.pt into the old layer, now it can be trained but, box_loss and pose_loss keep jumping with very little downward trend, how do I need to optimize?\n\n![Image](https://github.com/user-attachments/assets/7028df9d-1fd9-479b-82c4-77101c5740f2)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @starzero-hhh, thank you for your interest in YOLOv5 ðŸš€! It looks like you're working on a customized YOLO model, which is very exciting! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/). These resources might provide some additional insights for your optimization strategy.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us debug it. This could include a snippet of your modified model code, your training script, or any other relevant details.\n\nIf this is a â“ Question or a custom training inquiry, please provide as much information as possible, including:\n- Detailed training configuration (e.g., learning rates, optimizer settings, etc.)\n- Dataset details and examples\n- Training logs and metrics, if available\n- Explanation of how the P2 layer was added and P5 layer removed\n\n## Requirements\n\nEnsure your environment meets the following requirements:\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To set up your environment:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 supports these verified environments for training and inference:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response, but an Ultralytics engineer will review your inquiry and assist you soon. Thank you for your patience! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@starzero-hhh for training stability when modifying YOLO11 architecture like adding P2 layers, we recommend these steps:\n\n1. **Learning Rate Adjustment**: Start with a lower base LR (1e-4 â†’ 3e-5) and use linear LR warmup for 100-500 iterations to stabilize new layer training. The current 5e-4 for new layers might be too aggressive initially.\n\n2. **Weight Initialization**: Ensure new P2 head layers are properly initialized. Use `model.model[-1].apply(init_weights)` after modifying architecture, where `init_weights` applies Kaiming initialization.\n\n3. **Anchor Adaptation**: Verify your anchors match the new P2 layer scale using `utils.autoanchor.check_anchors(dataset, model=model)`. Small objects typically require anchors < 10 pixels.\n\n4. **Gradient Clipping**: Add `gradient_clip_val: 1.0` in your training config to prevent exploding gradients in new layers.\n\nFor detailed guidance on architecture modifications, see the [Ultralytics YOLO11 documentation](https://docs.ultralytics.com/models/yolo11/#architecture-modifications). If issues persist, please provide a [minimum reproducible example](https://docs.ultralytics.com/guides/minimal-reproducible-example/) per our bug report guidelines."
      },
      {
        "user": "starzero-hhh",
        "body": "Thank you for your valuable comments, I will take them on board to train my model!\r\n&nbsp;\r\n\r\n\r\né¢...\r\n***@***.***\r\n\r\n\r\n\r\n&nbsp;\r\n\r\n\r\n\r\n\r\n------------------&nbsp;åŽŸå§‹é‚®ä»¶&nbsp;------------------\r\nå‘ä»¶äºº: \"Paula ***@***.***&gt;; \r\nå‘é€æ—¶é—´: 2025å¹´3æœˆ12æ—¥(æ˜ŸæœŸä¸‰) ä¸‹åˆ2:05\r\næ”¶ä»¶äºº: ***@***.***&gt;; \r\næŠ„é€: ***@***.***&gt;; ***@***.***&gt;; \r\nä¸»é¢˜: Re: [ultralytics/yolov5] How to make yolo11 training with added P2 layer convergeï¼Ÿ (Issue #13530)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\næ˜Ÿé›¶-hhhä¸ºäº†åœ¨ä¿®æ”¹YOLO11æž¶æž„ï¼ˆå¦‚æ·»åŠ P2å±‚ï¼‰æ—¶ä¿æŒè®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬å»ºè®®ä»¥ä¸‹æ­¥éª¤:\r\n\r\n\r\nå­¦ä¹ çŽ‡è°ƒæ•´ä»Žä½ŽåŸºæ•°LRï¼ˆ1e-4â†’3e-5ï¼‰å¼€å§‹ï¼Œå¹¶ä½¿ç”¨çº¿æ€§LRçƒ­èº«è¿›è¡Œ100-500æ¬¡è¿­ä»£ï¼Œä»¥ç¨³å®šæ–°å±‚è®­ç»ƒã€‚ç›®å‰ç”¨äºŽæ–°å±‚çš„5e-4æœ€åˆå¯èƒ½è¿‡äºŽæ¿€è¿›ã€‚\r\n\r\n\r\n\r\næƒé‡åˆå§‹åŒ–:ç¡®ä¿æ–°çš„P2å¤´å±‚æ­£ç¡®åˆå§‹åŒ–ã€‚ä½¿ç”¨model.model[-1].apply(init_weights)åœ¨ä¿®æ”¹æž¶æž„åŽï¼Œå…¶ä¸­åˆå§‹åŒ–æƒé‡é€‚ç”¨äºŽå¯æ˜Žåˆå§‹åŒ–ã€‚\r\n\r\n\r\n\r\né”šé€‚åº”:ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•éªŒè¯é”šæ˜¯å¦ç¬¦åˆæ–°çš„P2å±‚å°ºåº¦utils.autoanchor.check_anchorsï¼ˆæ•°æ®é›†ï¼Œmodel=modelï¼‰. å°å¯¹è±¡é€šå¸¸éœ€è¦<10åƒç´ çš„é”šã€‚\r\n\r\n\r\n\r\næ¸å˜å‰ªè£ï¼šæ·»åŠ æ¢¯åº¦_clip_val:1.0åœ¨æ‚¨çš„è®­ç»ƒé…ç½®ä¸­ï¼Œä»¥é˜²æ­¢æ–°å±‚ä¸­çš„æ¢¯åº¦çˆ†ç‚¸ã€‚\r\n\r\n\r\næœ‰å…³æž¶æž„ä¿®æ”¹çš„è¯¦ç»†æŒ‡å¯¼ï¼Œè¯·å‚é˜…Ultralytics YOLO10æ–‡æ¡£. å¦‚æžœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·æä¾›ä¸€ä¸ªæœ€å°å¯é‡çŽ°ç¤ºä¾‹æ ¹æ®æˆ‘ä»¬çš„é”™è¯¯æŠ¥å‘ŠæŒ‡å—ã€‚\r\n\r\n-\r\nç›´æŽ¥å›žå¤æ­¤é‚®ä»¶ï¼Œåœ¨GitHubä¸ŠæŸ¥çœ‹å®ƒï¼Œæˆ–å–æ¶ˆè®¢é˜….\r\n***@***.*** &gt;\r\npderrengerç•™ä¸‹è¯„è®º(ultralytics/yolov5#13530)\r\n\r\næ˜Ÿé›¶-hhhä¸ºäº†åœ¨ä¿®æ”¹YOLO11æž¶æž„ï¼ˆå¦‚æ·»åŠ P2å±‚ï¼‰æ—¶ä¿æŒè®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬å»ºè®®ä»¥ä¸‹æ­¥éª¤:\r\n\r\n\r\nå­¦ä¹ çŽ‡è°ƒæ•´ä»Žä½ŽåŸºæ•°LRï¼ˆ1e-4â†’3e-5ï¼‰å¼€å§‹ï¼Œå¹¶ä½¿ç”¨çº¿æ€§LRçƒ­èº«è¿›è¡Œ100-500æ¬¡è¿­ä»£ï¼Œä»¥ç¨³å®šæ–°å±‚è®­ç»ƒã€‚ç›®å‰ç”¨äºŽæ–°å±‚çš„5e-4æœ€åˆå¯èƒ½è¿‡äºŽæ¿€è¿›ã€‚\r\n\r\n\r\n\r\næƒé‡åˆå§‹åŒ–:ç¡®ä¿æ–°çš„P2å¤´å±‚æ­£ç¡®åˆå§‹åŒ–ã€‚ä½¿ç”¨model.model[-1].apply(init_weights)åœ¨ä¿®æ”¹æž¶æž„åŽï¼Œå…¶ä¸­åˆå§‹åŒ–æƒé‡é€‚ç”¨äºŽå¯æ˜Žåˆå§‹åŒ–ã€‚\r\n\r\n\r\n\r\né”šé€‚åº”:ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•éªŒè¯é”šæ˜¯å¦ç¬¦åˆæ–°çš„P2å±‚å°ºåº¦utils.autoanchor.check_anchorsï¼ˆæ•°æ®é›†ï¼Œmodel=modelï¼‰. å°å¯¹è±¡é€šå¸¸éœ€è¦<10åƒç´ çš„é”šã€‚\r\n\r\n\r\n\r\næ¸å˜å‰ªè£ï¼šæ·»åŠ æ¢¯åº¦_clip_val:1.0åœ¨æ‚¨çš„è®­ç»ƒé…ç½®ä¸­ï¼Œä»¥é˜²æ­¢æ–°å±‚ä¸­çš„æ¢¯åº¦çˆ†ç‚¸ã€‚\r\n\r\n\r\næœ‰å…³æž¶æž„ä¿®æ”¹çš„è¯¦ç»†æŒ‡å¯¼ï¼Œè¯·å‚é˜…Ultralytics YOLO10æ–‡æ¡£. å¦‚æžœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·æä¾›ä¸€ä¸ªæœ€å°å¯é‡çŽ°ç¤ºä¾‹æ ¹æ®æˆ‘ä»¬çš„é”™è¯¯æŠ¥å‘ŠæŒ‡å—ã€‚\r\n\r\n-\r\nç›´æŽ¥å›žå¤æ­¤é‚®ä»¶ï¼Œåœ¨GitHubä¸ŠæŸ¥çœ‹å®ƒï¼Œæˆ–å–æ¶ˆè®¢é˜….\r\n***@***.*** &gt;"
      }
    ]
  },
  {
    "issue_number": 8185,
    "title": "Interpreting training results and showing loss graph, YOLOv5s6",
    "author": "Carolinejone",
    "state": "closed",
    "created_at": "2022-06-13T03:36:48Z",
    "updated_at": "2025-03-12T07:54:41Z",
    "labels": [
      "Stale"
    ],
    "body": "Hello, I'm new to AI. I would like to ask for some guidance on my research. I'm doing my master thesis with YOLOv5. I'm trying to detect the anomaly of stay cables of cable-type bridges. \r\n I used two datasets; Dataset- 1 has 2049 source images and 4159 images in total after combining with some augmented images to the training dataset, and Dataset- 2 has 1823 source images and 3673 images in total after combining with some augmented images to the training dataset. Train, validation and test dataset split is as follow,\r\nDataset -1, 3.2k: 614: 380\r\nDataset -2, 2.9k: 552: 299 \r\nI followed custom YOLOv5 training from Roboflow and also used the Roboflow annotator with rectangular bounding boxes. I used batch size 64 and epoch 100 with pre-trained weight YOLOv5s6 and trained on COLAB pro plus for both datasets and the rest is the same as the custom YOLOv5 training tutorial.\r\nThis is the result for Dataset-1\r\n![results](https://user-images.githubusercontent.com/88965698/173272561-dc0951d6-8493-4118-9845-eefcf9f71e92.png)\r\nThis is the result for Dataset-2\r\n![results](https://user-images.githubusercontent.com/88965698/173273466-32b8e156-96dc-4f7d-a463-ef40871936b0.png)\r\n\r\nMy question is,\r\n\r\n1.  Is the 100 epochs enough to train the model for both datasets 1 and 2?.\r\n2. How to interpret the result tables including confusion matrics? (I already learned about Precision, Recall, mAP and F1 graph)\r\n3. Which evaluation matric should I use, mAP or F1 or both? (Cause I've read that for class imbalance problem F1 score is the appropriate metric even though I believe that my model does not suffer from class imbalance effect.)\r\n4. Is my model good enough or is it overfitting or is it needed to improve? \r\n5. Is the validation loss the only way to check overfitting?\r\n6. Please provide some useful articles or papers for reference.\r\nThank you so much for your help.\r\nEdit: adding more questions:\r\n7. What is the difference between the label graph from result tables and the class balance from the health check of Roboflow.\r\n8. How to interpret the results of label graph?\r\nDataset-1 label graph\r\n![labels](https://user-images.githubusercontent.com/88965698/173275397-3293777d-03cc-42e0-975a-517ae5d1f226.jpg)\r\nDataset-2 label graph\r\n![labels](https://user-images.githubusercontent.com/88965698/173275420-549324c8-7d03-4f7b-a323-d3d5bb9b5651.jpg)\r\nDataset-1 Class Balance\r\n![Capture](https://user-images.githubusercontent.com/88965698/173275618-cc44926a-f736-4d26-bed7-c56c872223d6.JPG)\r\nDataset-2 Class Balance\r\n![Capture](https://user-images.githubusercontent.com/88965698/173275650-460eef47-70a3-40a1-9f6b-313f20d78539.JPG)\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @Carolinejone, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"CI CPU testing\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), validation ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@Carolinejone ðŸ‘‹ Hello! Thanks for asking about improving YOLOv5 ðŸš€ training results. I would combine both your datasets into a single dataset for best results. Also you must train longer until you see overfitting, otherwise you have not trained long enough.\r\n\r\nMost of the time good results can be obtained with no changes to the models or training settings, **provided your dataset is sufficiently large and well labelled**. If at first you don't get good results, there are steps you might be able to take to improve, but we always recommend users **first train with all default settings** before considering any changes. This helps establish a performance baseline and spot areas for improvement.\r\n\r\nIf you have questions about your training results **we recommend you provide the maximum amount of information possible** if you expect a helpful response, including results plots (train losses, val losses, P, R, mAP), PR curve, confusion matrix, training mosaics, test results and dataset statistics images such as labels.png. All of these are located in your `project/name` directory, typically `yolov5/runs/train/exp`.\r\n\r\nWe've put together a full guide for users looking to get the best results on their YOLOv5 trainings below.\r\n\r\n## Dataset\r\n\r\n- **Images per class.** â‰¥ 1500 images per class recommended\r\n- **Instances per class.** â‰¥ 10000 instances (labeled objects) per class recommended\r\n- **Image variety.** Must be representative of deployed environment. For real-world use cases we recommend images from different times of day, different seasons, different weather, different lighting, different angles, different sources (scraped online, collected locally, different cameras) etc.\r\n- **Label consistency.** All instances of all classes in all images must be labelled. Partial labelling will not work.\r\n- **Label accuracy.** Labels must closely enclose each object. No space should exist between an object and it's bounding box. No objects should be missing a label.\r\n- **Label verification.** View `train_batch*.jpg` on train start to verify your labels appear correct, i.e. see [example](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#local-logging) mosaic.\r\n- **Background images.** Background images are images with no objects that are added to a dataset to reduce False Positives (FP). We recommend about 0-10% background images to help reduce FPs (COCO has 1000 background images for reference, 1% of the total). No labels are required for background images.\r\n\r\n<a href=\"https://arxiv.org/abs/1405.0312\"><img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/109398377-82b0ac00-78f1-11eb-9c76-cc7820669d0d.png\" alt=\"COCO Analysis\"></a>\r\n\r\n\r\n## Model Selection\r\n\r\nLarger models like YOLOv5x and [YOLOv5x6](https://github.com/ultralytics/yolov5/releases/tag/v5.0) will produce better results in nearly all cases, but have more parameters, require more CUDA memory to train, and are slower to run. For **mobile** deployments we recommend YOLOv5s/m, for **cloud** deployments we recommend YOLOv5l/x. See our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints) for a full comparison of all models. \r\n\r\n<p align=\"center\"><img width=\"700\" alt=\"YOLOv5 Models\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png\"></p>\r\n\r\n- **Start from Pretrained weights.** Recommended for small to medium sized datasets (i.e. [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml)). Pass the name of the model to the `--weights` argument. Models download automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases).\r\n```bash\r\npython train.py --data custom.yaml --weights yolov5s.pt\r\n                                             yolov5m.pt\r\n                                             yolov5l.pt\r\n                                             yolov5x.pt\r\n                                             custom_pretrained.pt\r\n```\r\n- **Start from Scratch.** Recommended for large datasets (i.e. [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [OIv6](https://storage.googleapis.com/openimages/web/index.html)). Pass the model architecture yaml you are interested in, along with an empty `--weights ''` argument:\r\n```bash\r\npython train.py --data custom.yaml --weights '' --cfg yolov5s.yaml\r\n                                                      yolov5m.yaml\r\n                                                      yolov5l.yaml\r\n                                                      yolov5x.yaml\r\n```\r\n\r\n\r\n## Training Settings\r\n\r\nBefore modifying anything, **first train with default settings to establish a performance baseline**. A full list of train.py settings can be found in the [train.py](https://github.com/ultralytics/yolov5/blob/master/train.py) argparser.\r\n\r\n- **Epochs.** Start with 300 epochs. If this overfits early then you can reduce epochs. If overfitting does not occur after 300 epochs, train longer, i.e. 600, 1200 etc epochs.\r\n- **Image size.** COCO trains at native resolution of `--img 640`, though due to the high amount of small objects in the dataset it can benefit from training at higher resolutions such as `--img 1280`. If there are many small objects then custom datasets will benefit from training at native or higher resolution. Best inference results are obtained at the same `--img` as the training was run at, i.e. if you train at `--img 1280` you should also test and detect at `--img 1280`.\r\n- **Batch size.** Use the largest `--batch-size` that your hardware allows for. Small batch sizes produce poor batchnorm statistics and should be avoided.\r\n- **Hyperparameters.** Default hyperparameters are in [hyp.scratch-low.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-low.yaml). We recommend you train with default hyperparameters first before thinking of modifying any. In general, increasing augmentation hyperparameters will reduce and delay overfitting, allowing for longer trainings and higher final mAP. Reduction in loss component gain hyperparameters like `hyp['obj']` will help reduce overfitting in those specific loss components. For an automated method of optimizing these hyperparameters, see our [Hyperparameter Evolution Tutorial](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\r\n\r\n## Further Reading\r\n\r\nIf you'd like to know more a good place to start is Karpathy's 'Recipe for Training Neural Networks', which has great ideas for training that apply broadly across all ML domains: http://karpathy.github.io/2019/04/25/recipe/\r\n\r\nGood luck ðŸ€ and let us know if you have any other questions!"
      },
      {
        "user": "Carolinejone",
        "body": "@glenn-jocher Thank you so much for your explanation. I intentionally separate the dataset into two as I would like to compare the impact on the model's accuracy by the two different test datasets having different drone distances from the target object. For that reason, I couldn't combine the two datasets to make it larger. \r\nI've tried to train the model for 300 epochs and the results show like this for dataset-1, the model overfits earlier. (I'm still running the dataset-2 but I'm sure the result will be the same).\r\n\r\n!python train.py --img 640 --batch 64 --epochs 300 --data {dataset.location}/data.yaml --weights yolov5s6.pt --cache\r\n\r\nCould you please explain how to prevent overfitting and if possible explain with some coding examples?\r\n\r\n![R_curve](https://user-images.githubusercontent.com/88965698/173377167-3bed7288-cb7a-47a7-b181-63c47c9b0150.png)\r\n![P_curve](https://user-images.githubusercontent.com/88965698/173377185-25eedf0c-b54f-491d-bc7c-38f6473252f2.png)\r\n![PR_curve](https://user-images.githubusercontent.com/88965698/173377195-4bfb8510-11b1-4b33-b412-fb536aef994e.png)\r\n![F1_curve](https://user-images.githubusercontent.com/88965698/173377207-ee2e4ef3-b196-4553-a1e0-e3ca555eba7e.png)\r\n\r\n![results](https://user-images.githubusercontent.com/88965698/173377134-586c9e50-20e8-4bf8-b2d8-e83e266b8de8.png)\r\n![confusion_matrix](https://user-images.githubusercontent.com/88965698/173377156-98198ff8-9bdb-4a4f-8991-7e1eafd114e4.png)\r\n\r\n![labels](https://user-images.githubusercontent.com/88965698/173377107-3e6bfe29-f6f6-472f-9388-7bfe49b595c8.jpg)\r\n"
      }
    ]
  },
  {
    "issue_number": 11840,
    "title": "Train YOLOv5 on Tiff (image and label format)",
    "author": "HelenVe",
    "state": "closed",
    "created_at": "2023-07-09T17:37:57Z",
    "updated_at": "2025-03-10T05:21:57Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello!\r\n\r\nI have a question regarding training of YOLOv5 with .tiff files. \r\nI have 10 grayscale images, each of them corresponds to a class. I stack and save them on a tif file, the shape per tif  is [10, image_width, image_height]. Some of these grayscale images can also be empty. I have a label file per tif, containing all the class bounding boxes. \r\nHowever when I start training, my metrics values are very low (see screenshot) so I must be doing something wrong. Is my logic incorrect, should I be saving the images in a different way?\r\n\r\nThank you for your time!\r\n\r\n![image](https://github.com/ultralytics/yolov5/assets/34419631/69825df3-20a9-4efe-a48d-d07760c7fd8c)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@HelenVe hi there!\n\nThank you for reaching out with your question. Training YOLOv5 with .tiff files should be possible. However, based on your description and the screenshot you provided, it seems that you might be encountering some issues with your data preparation or file format.\n\nTo help you further, could you please provide more details? Specifically:\n\n1. How are you currently converting and saving the grayscale images into the .tif file? Are you using any specific library or method?\n2. Could you please share an example of your label file formatting, so I can better understand how the bounding boxes are represented?\n\nHaving these additional details will allow me to provide you with more accurate guidance. Looking forward to assisting you!\n\nBest regards."
      },
      {
        "user": "HelenVe",
        "body": "@glenn-jocher Thank you very much for the quick reply! \r\n\r\nI am saving the tif files with tiffile, this is how I create them. \r\n![image](https://github.com/ultralytics/yolov5/assets/34419631/c57c7f00-474b-4142-b901-e448ed0b9fe3)\r\n\r\n2) The labels are based on the YOLOV5 formatting, one tif corresponds to one txt file. And one grayscale image only represents one class. Here is an example: \r\n\r\n1 0.17875 0.5375234521575984 0.010625 0.03470919324577861\r\n3 0.1525 0.5590994371482176 0.026875 0.06660412757973734\r\n5 0.176875 0.5121951219512195 0.04875 0.012195121951219513\r\n0 0.161875 0.5234521575984991 0.015 0.0028142589118198874\r\n0 0.19125 0.525328330206379 0.01125 0.001876172607879925\r\n8 0.175625 0.5375234521575984 0.0425 0.03470919324577861\r\n4 0.19875 0.5675422138836773 0.021875 0.04971857410881801\r\n9 0.180625 0.5656660412757973 0.028125 0.009380863039399626\r\n2 0.175 0.550656660412758 0.071875 0.08818011257035648\r\n7 0.154375 0.5572232645403377 0.03 0.07035647279549719\r\n6 0.1975 0.5581613508442776 0.024375 0.06848030018761726\r\n\r\n"
      },
      {
        "user": "glenn-jocher",
        "body": " @HelenVe You're welcome!\n\nThank you for providing more information. Your approach to saving the .tif files using the `tiffile` library seems correct.\n\nRegarding the label file formatting, it appears to follow the YOLOv5 format where each line represents a bounding box annotation. Each line starts with the class index (e.g., 1, 3, 5) followed by the normalized coordinates of the bounding box (x_center, y_center, width, height). \n\nFrom what you've described and the example you provided, there doesn't seem to be any issues with the file formats or labeling. However, there might be other factors affecting your training performance.\n\nTo investigate this further, here are a few suggestions:\n\n1. Ensure your dataset is diverse and representative: Make sure you have a sufficient number of training samples for each class and that they cover different variations, scales, and perspectives. This helps the model generalize better.\n\n2. Check for class imbalance: If some classes have significantly fewer instances than others, it can affect the model's ability to learn and classify objects accurately. Consider augmenting your dataset or using techniques like class weighting to address this.\n\n3. Experiment with different training configurations: Adjusting hyperparameters such as learning rate, batch size, and image size can significantly impact training performance. Try experimenting with different values to find the optimal configuration for your dataset.\n\n4. Increase the number of training steps: If the model's performance is still low after training for a sufficient number of epochs, consider increasing the number of training steps. This allows the model more time to learn and improve its performance.\n\nPlease give these suggestions a try and let me know if you observe any improvements in your training results. If you have any further questions or concerns, don't hesitate to ask.\n\nHappy training!"
      }
    ]
  },
  {
    "issue_number": 13528,
    "title": "Path(os.path.relpath(ROOT, Path.cwd()))",
    "author": "rickmoddy",
    "state": "open",
    "created_at": "2025-03-07T15:11:35Z",
    "updated_at": "2025-03-08T08:50:00Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nIs this line of code redundant? If I move the project to a different path and run the script from there, wouldn't the current working directory already be correct?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @rickmoddy, thank you for your interest in YOLOv5 ðŸš€! Your question is noted, and we appreciate your curiosity about the code implementation. An Ultralytics engineer will assist you soon to provide clarity on your inquiry ðŸ˜Š.\n\nIn the meantime, please explore our â­ï¸ [YOLOv5 Tutorials](https://docs.ultralytics.com/yolov5/) for detailed guides on code functionality, setups, and advanced features.\n\nTo ensure we can assist you effectively, if your question pertains to a possible ðŸ› bug or error, please provide a **minimum reproducible example**. This includes:\n\n1. The exact code snippets you are running.\n2. Details about your setup, such as your operating system, Python version, PyTorch version, and runtime environment.\n3. A description of how to reproduce the behavior, if relevant.\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To ensure your environment meets the required setup, follow these steps:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYou can also run YOLOv5 in any of the following cloud environments, which come preinstalled with dependencies:\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing âœ…. CI tests validate the performance and correctness of YOLOv5 scripts for training, validation, inference, export, and benchmarking across macOS, Windows, and Ubuntu daily and on every commit.\n\nWe appreciate your patience and input into improving YOLOv5! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@rickmoddy this line converts the `ROOT` directory path to be relative to the current working directory, which ensures consistent path handling when running scripts from different locations. It's intentional and serves to:  \n1. Maintain portable relative paths if executing from outside the project directory  \n2. Prevent absolute path dependencies in saved results  \nThe current implementation follows Python best practices for path management across different execution contexts."
      }
    ]
  },
  {
    "issue_number": 13527,
    "title": "[*] insert_final_newline = true  [*.py] insert_final_newline = true",
    "author": "olegsai",
    "state": "open",
    "created_at": "2025-03-07T12:25:30Z",
    "updated_at": "2025-03-08T06:21:25Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "pderrenger",
        "body": "@olegsai thanks for pointing this out! The YOLOv5 repository follows standard Python conventions where `.py` files should end with a newline, as recommended in [PEP 8](https://peps.python.org/pep-0008/#source-file-encoding). This is automatically handled in most editors, but feel free to enable this setting in your IDE/editor for consistency with Python best practices."
      }
    ]
  },
  {
    "issue_number": 13433,
    "title": "No speed improvement between FP16 and INT8 TensorRT models",
    "author": "ingtommi",
    "state": "open",
    "created_at": "2024-11-27T08:26:11Z",
    "updated_at": "2025-03-08T06:01:55Z",
    "labels": [
      "bug",
      "exports"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\r\n\r\n\r\n### YOLOv5 Component\r\n\r\nValidation\r\n\r\n### Bug\r\n\r\n![yolov5-issue](https://github.com/user-attachments/assets/b09333a7-8b9d-45ff-896b-188fcf0d8769)\r\n\r\nWhen validating my YOLOv5n both in **FP16** and **INT8** precision I see **no performance improvement for the INT8 version**, while accuracy and model size drop (which is ok!). I then checked with _trtexec_ and I again get the same latency: \r\n[yolov5n.txt](https://github.com/user-attachments/files/17931339/yolov5n.txt). \r\n\r\nSince this does not happens for latest YOLOs (where I see around 20% latency improvement), I was thinking that YOLOv5 does not have operations that benefit from INT8 on my current architecture (i.e. 16-bit is already fully optimized). \r\nCan you help me understanding if this is true or I am making any mistake?\r\n\r\n\r\n### Environment\r\n\r\n- YOLO: YOLOv5n v7.0 fine-tuned on custom dataset\r\n- TensorRT: 8.6.2.3\r\n- Device: NVIDIA Jetson Orin Nano 8GB\r\n\r\n### Minimal Reproducible Example\r\n\r\n```\r\npython val.py --weights yolo5n.engine --data data.yaml --batch 16 --task test\r\npython val.py --weights yolo5n-int8.engine --data data.yaml --batch 16 --task test\r\n\r\ntrtexec --loadEngine=yolo5n.engine --batch=1 --fp16\r\ntrtexec --loadEngine=yolo5n-int8.engine --batch=1 --best\r\n```\r\n\r\n### Additional\r\n\r\nModel files: [models.zip](https://github.com/user-attachments/files/17931436/models.zip)\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @ingtommi, thank you for your interest in YOLOv5 ðŸš€!  \n\nIt looks like you're encountering an issue with performance differences between FP16 and INT8 TensorRT models. Since this appears to be a ðŸ› Bug Report, we would appreciate it if you could provide a **minimum reproducible example (MRE)** to assist us in debugging this issue. This could include specific commands you used, a small sample of your dataset, or any additional logs that might help clarify the problem.\n\nPlease also double-check your environment to ensure compatibility:\n- **Python** version is 3.8 or higher.\n- YOLOv5 dependencies are properly installed using the `requirements.txt` file.\n- TensorRT and GPU drivers are updated and configured correctly for inference.  \n\nFor debugging, it might be helpful to test using different hardware or TensorRT versions to see if the issue persists. If this is related to specific YOLOv5 configurations, please share more details about your setup or the customizations you have made.  \n\nAn Ultralytics engineer will review this shortly and provide further assistanceâ€”thank you for your patience! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Thank you for your detailed report and testing effort! Your observation about minimal or no speed improvement with INT8 on YOLOv5 compared to FP16 is valid and may be attributed to hardware and architectural factors. Some architectures, particularly on devices like the Jetson Orin Nano, show limited benefits from INT8 due to high FP16 optimization. YOLOv5's operations might not fully utilize INT8 optimizations compared to newer YOLO versions with refined quantization-aware designs.\n\nIf verifying on a different architecture still shows discrepancies, it might indicate that INT8 calibration settings could be suboptimal or the TensorRT INT8 kernel isn't fully leveraged for YOLOv5. For further exploration, ensure calibration data is diverse and representative of deployment inputs. Additionally, testing with dynamic batch sizes or alternate precision configurations (e.g., mixing INT8/FP16) could be insightful.\n\nLet us know if you see different outcomes or need additional guidance! For reference, you can explore [this TensorRT guide](https://docs.ultralytics.com/integrations/tensorrt/) for further optimization techniques."
      },
      {
        "user": "Y-T-G",
        "body": "YOLOv5 doesn't support INT8 TensorRT exports."
      }
    ]
  },
  {
    "issue_number": 13008,
    "title": "YOLOv5 Output Size Issue",
    "author": "lllittleX",
    "state": "closed",
    "created_at": "2024-05-14T01:51:38Z",
    "updated_at": "2025-03-07T20:34:15Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nI want to convert the results to ONNX first, and then to RKNN. However, when testing the ONNX model, I found that my output sizes are as follows:\r\ntorch.Size([1, 15120, 133])\r\ntorch.Size([1, 3, 48, 80, 133])\r\ntorch.Size([1, 3, 24, 40, 133])\r\ntorch.Size([1, 3, 12, 20, 133])\r\nBut from what I understand, the usual size is 80,80 instead of 48,80. I would like to ask if this is normal? \r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @lllittleX, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@lllittleX hello! Thanks for reaching out with your question about the output size from the YOLOv5 model when converted to ONNX.\n\nThe output sizes you are seeing (e.g., `torch.Size([1, 3, 48, 80, 133])`) depend on the input image size you are using and the architecture of the model. Normally, YOLOv5 uses a grid that scales down from the input size by factors of 32, 16, and 8. If your input size isn't a multiple of these, the resulting grid sizes could appear unconventional, as is likely the case with a dimension of 80 instead of the usual 40 or 160, etc. \n\nEnsure that your input image size is a multiple of the largest factor (generally 32) to maintain consistent grid dimensions across the different layers. You can adjust this in your initial configuration before training or inference.\n  \nLet's verify that input sizes and model configurations align with the expected outputs. Good luck with your conversions! ðŸš€"
      },
      {
        "user": "VishnuRao27",
        "body": "Hi there!\nI searched quite a lot for this but I couldnt really find it anywhere, let me give a little bit of context:\nI am trying to perform knowledge distillation on yolov5n using yolov5x which are both already trained on my custom dataset containing 2 classes.\nI saw this issue #13124 where a tutorial for knowledge distillation was given and I see that my pretrained custom_model.pt when loaded through pytorch (autoshape False) in eval mode gives 2 outputs:\n1. A tensor of shape (batch_size, 25200, num_classes+5) ( [4, 25200, 7] in my case which is supposed to be [x, y, w, h, objectness_score, class_logits...] ? )\n2. A list (of length 3) of tensors of shape (batch_size, 3, 80, 80, num_classes+5), (batch_size, 3, 40, 40, num_classes+5) and (batch_size, 3, 20, 20, num_classes+5). I dont know what the 7 values here correspond to.\n\nThe same model in train mode gives only the 2nd output.\n\nAt first I thought just stacking them would give me the same tensor as output1 but then I compared the values to see they are completely different even after applying softmax to them. \nI experimented by actually applying softmax to the concatenated version of output 2, saving it and comparing it with the values of output1 saved without any modifications.\n\nNow for my problem:\nSince knowledge distillation relies on the output probabilities of the teacher model which is in eval mode, I figured that the raw outputs from the teacher can be used directly since it gives the probabilities of each class. But since the student is in the train mode, which gives only output 2, how do I convert this list of tensors of shape (batch_size, 3, x, x, num_classes+5) to a raw logs score like how I see in output1 ? \nI do not obviously want to switch to eval mode because I want to use these values to compute the kl_div loss and backpropogate.\nI can also just stack the output2 of student and teacher to give something with the shape of output1 and pass it to kl_div but then if the values are not actually the probability distributions of each class then it defeats the purpose of knowledge distillation."
      }
    ]
  },
  {
    "issue_number": 1479,
    "title": "Error 'AttributeError: 'str' object has no attribute 'get'' at running train.py",
    "author": "Yasin40",
    "state": "closed",
    "created_at": "2020-11-23T08:57:10Z",
    "updated_at": "2025-03-07T08:57:21Z",
    "labels": [
      "Stale"
    ],
    "body": "Hi,\r\nAccording this tutorial i decided train my custom data but i got this error:\r\n``python` train.py --batch 16 --epochs 25 --data roadside1.yaml --weights ./weights/yolov5s.pt\r\nYour branch is behind 'origin/master' by 25 commits, and can be fast-forwarded.\r\n  (use \"git pull\" to update your local branch)\r\n\r\nUsing torch 1.7.0 CUDA:0 (GeForce GTX 1060 6GB, 6077MB)\r\n\r\nNamespace(adam=False, batch_size=16, bucket='', cache_images=False, cfg='', data='roadside1.yaml', device='', epochs=25, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], local_rank=-1, log_imgs=16, multi_scale=False, name='exp', noautoanchor=False, nosave=False, notest=False, project='runs/train', rect=False, resume=False, save_dir='runs/train/exp4', single_cls=False, sync_bn=False, total_batch_size=16, weights='./weights/yolov5s.pt', workers=8, world_size=1)\r\nStart Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\r\nHyperparameters {'lr0': 0.01, 'lrf': 0.2, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0}\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 490, in <module>\r\n    train(hyp, opt, device, tb_writer, wandb)\r\n  File \"train.py\", line 69, in train\r\n    check_dataset(data_dict)  # check\r\n  File \"/home/anpr/adel/clones/yolov5/utils/general.py\", line 80, in check_dataset\r\n    val, s = dict.get('val'), dict.get('download')\r\nAttributeError: 'str' object has no attribute `'get'``\r\n\r\nanyone can help me?",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello @Yasin40, thank you for your interest in ðŸš€ YOLOv5! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://www.ultralytics.com or email Glenn Jocher at glenn.jocher@ultralytics.com.\n\n## Requirements\n\nPython 3.8 or later with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including `torch>=1.7`. To install run:\n```bash\n$ pip install -r requirements.txt\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab Notebook** with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n- **Kaggle Notebook** with free GPU: [https://www.kaggle.com/models/ultralytics/yolov5](https://www.kaggle.com/models/ultralytics/yolov5)\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) \n- **Docker Image** https://hub.docker.com/r/ultralytics/yolov5. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker)\n\n## Status\n\n![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/models/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Yasin40",
        "body": "\r\n> Hello @Yasin40, thank you for your interest in  YOLOv5! Please visit our  [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\r\n> \r\n> If this is a  Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\r\n> \r\n> If this is a custom training  Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\r\n> \r\n> For business inquiries or professional support requests please visit https://www.ultralytics.com or email Glenn Jocher at [glenn.jocher@ultralytics.com](mailto:glenn.jocher@ultralytics.com).\r\n> \r\n> ## Requirements\r\n> Python 3.8 or later with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including `torch>=1.7`. To install run:\r\n> \r\n> ```shell\r\n> $ pip install -r requirements.txt\r\n> ```\r\n> \r\n> ## Environments\r\n> YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\r\n> \r\n> * **Google Colab Notebook** with free GPU: [![Open In Colab](https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb)\r\n> * **Kaggle Notebook** with free GPU: https://www.kaggle.com/models/ultralytics/yolov5\r\n> * **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\r\n> * **Docker Image** https://hub.docker.com/r/ultralytics/yolov5. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) ![Docker Pulls](https://camo.githubusercontent.com/280faedaf431e4c0c24fdb30ec00a66d627404e5c4c498210d3f014dd58c2c7e/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f756c7472616c79746963732f796f6c6f76353f6c6f676f3d646f636b6572)\r\n> \r\n> ## Status\r\n> ![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\r\n> \r\n> If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/models/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\r\n\r\nI run this command but seems all requirments installed.\r\n```\r\nRequirement already satisfied: Cython in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.29.21)\r\nRequirement already satisfied: matplotlib>=3.2.2 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (3.3.3)\r\nRequirement already satisfied: numpy>=1.18.5 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.19.4)\r\nRequirement already satisfied: opencv-python>=4.1.2 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (4.4.0.46)\r\nRequirement already satisfied: pillow in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (8.0.1)\r\nRequirement already satisfied: PyYAML>=5.3 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (5.3.1)\r\nRequirement already satisfied: scipy>=1.4.1 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.5.4)\r\nRequirement already satisfied: tensorboard>=2.2 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (2.4.0)\r\nRequirement already satisfied: torch>=1.6.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (1.7.0)\r\nRequirement already satisfied: torchvision>=0.7.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (0.8.1)\r\nRequirement already satisfied: tqdm>=4.41.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (4.52.0)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.4.7)\r\nRequirement already satisfied: cycler>=0.10 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.3.1)\r\nRequirement already satisfied: python-dateutil>=2.1 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.1)\r\nRequirement already satisfied: grpcio>=1.24.3 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (1.33.2)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (1.7.0)\r\nRequirement already satisfied: protobuf>=3.6.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (3.14.0)\r\nRequirement already satisfied: six>=1.10.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (1.15.0)\r\nRequirement already satisfied: setuptools>=41.0.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (50.3.1.post20201107)\r\nRequirement already satisfied: absl-py>=0.4 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (0.11.0)\r\nRequirement already satisfied: requests<3,>=2.21.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (2.25.0)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (0.4.2)\r\nRequirement already satisfied: werkzeug>=0.11.15 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (1.0.1)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (1.23.0)\r\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (0.35.1)\r\nRequirement already satisfied: markdown>=2.6.8 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from tensorboard>=2.2->-r requirements.txt (line 11)) (3.3.3)\r\nRequirement already satisfied: typing-extensions in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from torch>=1.6.0->-r requirements.txt (line 12)) (3.7.4.3)\r\nRequirement already satisfied: future in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from torch>=1.6.0->-r requirements.txt (line 12)) (0.18.2)\r\nRequirement already satisfied: dataclasses in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from torch>=1.6.0->-r requirements.txt (line 12)) (0.6)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2->-r requirements.txt (line 11)) (3.0.4)\r\nRequirement already satisfied: idna<3,>=2.5 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2->-r requirements.txt (line 11)) (2.10)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2->-r requirements.txt (line 11)) (1.26.2)\r\nRequirement already satisfied: certifi>=2017.4.17 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2->-r requirements.txt (line 11)) (2020.6.20)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2->-r requirements.txt (line 11)) (1.3.0)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2->-r requirements.txt (line 11)) (4.1.1)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2->-r requirements.txt (line 11)) (0.2.8)\r\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2->-r requirements.txt (line 11)) (4.6)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2->-r requirements.txt (line 11)) (3.1.0)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/anpr/adel/envs/yolo/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2->-r requirements.txt (line 11)) (0.4.8)\r\n\r\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "> Your branch is behind 'origin/master' by 25 commits, and can be fast-forwarded.\r\n> (use \"git pull\" to update your local branch)\r\n\r\nYou may want to follow the recommendations printed to your screen."
      }
    ]
  },
  {
    "issue_number": 5061,
    "title": "How to check overfitting",
    "author": "aseprohman",
    "state": "closed",
    "created_at": "2021-10-06T02:32:16Z",
    "updated_at": "2025-03-05T07:40:05Z",
    "labels": [
      "question"
    ],
    "body": "## â”Question\r\n Hello @glenn-jocher,\r\nwhich graph box shows that the training process that I am doing is overfitting or not ( val/box_loss, val/obj_loss, etc ) or is there another method to check ? can you give me a little explanation about the difference between val/box_loss and val/obj_loss  \r\nmany thanks\r\n## Additional contex\r\n![results](https://user-images.githubusercontent.com/18188641/136130821-83a8fd1e-82b6-404c-b6af-c35ab146b88b.png)\r\nt\r\n",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@aseprohman validation losses should overfit or you haven't trained long enough.\r\n\r\nFor loss descriptions see original YOLO publications:\r\nhttps://pjreddie.com/publications/\r\n"
      },
      {
        "user": "aseprohman",
        "body": "which one chart should check to determine overfitting ? box_loss or obj_loss ?"
      },
      {
        "user": "glenn-jocher",
        "body": "@aseprohman overfitting can occur in any val loss component."
      }
    ]
  },
  {
    "issue_number": 13523,
    "title": "Incorrect Albumentations version in requirements.txt (needs to be >1.0.3 but <2.0)",
    "author": "ViKarp",
    "state": "open",
    "created_at": "2025-03-03T12:59:57Z",
    "updated_at": "2025-03-03T19:36:38Z",
    "labels": [
      "bug",
      "dependencies"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nTraining\n\n### Bug\n\nWhen installing dependencies for classification with augmentation in YOLOv5 from the requirements.txt file, I found that the version of the albumentations library is pinned to >=1.0.3. However, proper augmentation functionality requires a version of albumentations higher or equal than 1.0.3, yet lower than 2.0.\n\nRunning the scripts with version >=2 can lead to errors.\n\n### Environment\n\nUbuntu 22.04\nPython 3.10\nDefault requirements.txt from the repository\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @ViKarp, thank you for your interest in YOLOv5 ðŸš€ and for reporting this possible issue with Albumentations! \n\nFor ðŸ› **Bug Reports**, could you please provide a **minimum reproducible example** to help us debug the versioning issue? The details youâ€™ve provided are helpful, but a short snippet or specific script demonstrating the issue with Albumentations' version would be greatly appreciated.\n\nFor reference, please ensure you're using the correct environment:\n## Requirements\n[**Python>=3.8.0**](https://www.python.org/) with all dependencies installed from the [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt). To set up the environment, follow these steps:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install requirements\n```\n\nIf you encounter this issue while using one of our verified environments, weâ€™d recommend double-checking your setup against the options below:  \n## Verified Environments\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\nAdditionally, you might want to check out our â­ï¸ [guides](https://docs.ultralytics.com/yolov5/) to ensure everything is configured properly.\n\n## Status\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>  \nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI includes tests for training, validation, inference, export, and benchmarks.\n\nThis is an automated response to acknowledge your query; an Ultralytics engineer will assist you soon. Thank you for helping us improve YOLOv5! ðŸš€âœ¨"
      },
      {
        "user": "pderrenger",
        "body": "@ViKarp thank you for catching this! We welcome your PR to update the Albumentations version constraint in the YOLOv5 requirements.txt to `albumentations>=1.0.3,<2.0.0`. The team will review it promptly. For implementation details, see the Albumentations integration documentation."
      }
    ]
  },
  {
    "issue_number": 3993,
    "title": "Some tricks to improve v5~(Including biFPN, ASFF, and all kinds of data augmentations~ï¼‰Call for everyone~",
    "author": "SpongeBab",
    "state": "closed",
    "created_at": "2021-07-13T09:09:23Z",
    "updated_at": "2025-03-01T19:09:48Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "## ðŸš€ Feature\r\n\r\n@glenn-jocher \r\nHi, Do you have a plan to implement the yolov5 with biFPN?\r\n\r\n## Motivation\r\nWe can see the advance of biFPN in this paper: https://arxiv.org/pdf/1911.09070.pdf. It is better than PANet.\r\n\r\n\r\n## Additional context\r\nI think it can improves the mAP a lot.\r\n\r\n## Edit:A list of tricks( TBC ): What I want to do.\r\n\r\n- [ ] Multi-scale feature and prediction fusion: https://arxiv.org/abs/1612.03144\r\n      modified yolov5-p5 to Four Head prediction.\r\n- [ ] biFPN: https://arxiv.org/pdf/1911.09070.pdf\r\n- [ ] Adversarial learning: https://arxiv.org/pdf/2103.13886.pdf \r\n\r\n",
    "comments": [
      {
        "user": "SpongeBab",
        "body": "## ðŸš€ Feature\r\n\r\n@glenn-jocher \r\nAnd what about **adaptive average pooling**?\r\n\r\n"
      },
      {
        "user": "glenn-jocher",
        "body": "@SpongeBab BiFPN is used in EfficientDet models, the YOLOv5 heads are PANets.\r\n\r\nAdaptive average pooling is used in classification models to reduce the spatial dimensions down to 1. I use this in the YOLOv5 classifier branch for example in the Classify model (equivalent of Detect() module):\r\nhttps://github.com/ultralytics/yolov5/blob/8ee9fd15059e807374f52527951399e61d57b1b0/models/common.py#L378-L388"
      },
      {
        "user": "SpongeBab",
        "body": "@glenn-jocher  oh,I missed that.Thank you!\r\nBTW, emm, although it belongs to Efficientdet. But I think in Efficientdet the biFPN plays an important role. The mAP increase is mainly due to biFPN, which I think is applicable to V5. It might be able to bring about a big increase for v5."
      }
    ]
  },
  {
    "issue_number": 13519,
    "title": "Detection with Torch Hub Failing",
    "author": "llavkush",
    "state": "closed",
    "created_at": "2025-02-25T04:56:37Z",
    "updated_at": "2025-02-27T23:58:53Z",
    "labels": [
      "question",
      "dependencies",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI have ML project on Python 3.7.8, it was working fine, but recently I am getting error when loading model with  torch.hub.load \n\n File \"C:\\Users\\Strange/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py\", line 39, in <module>\n    from utils.dataloaders import exif_transpose, letterbox\n  File \"C:\\Users\\Strange/.cache\\torch\\hub\\ultralytics_yolov5_master\\utils\\dataloaders.py\", line 776\n    if mosaic := self.mosaic and random.random() < hyp[\"mosaic\"]:\n\nI know yolov5 require python > 3.8, but all current dependencies in my project is of 3.7.8, please suggest workaround to work with it\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @llavkush, thank you for your interest in YOLOv5 ðŸš€! It looks like you're encountering an issue while using YOLOv5 with Torch Hub. As you noted, YOLOv5 indeed requires Python >= 3.8 to function properly, and supporting Python 3.7.8 is outside the recommended environment.\n\nIf this is a ðŸ› Bug Report, please share a **minimum reproducible example (MRE)** of your code that demonstrates the issue. This will greatly help us to investigate further.\n\nIn the meantime, here are some helpful resources and recommendations:\n\n## Requirements\n\nYOLOv5 requires [**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\nIf you're unable to upgrade your Python version, consider running YOLOv5 in one of our verified environments where all dependencies are preinstalled:\n\n## Environments\n\nYOLOv5 can be run in the following up-to-date verified environments:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\nUsing one of these environments may allow you to bypass the need to modify your local setup.\n\n## Tutorials\n\nFor additional guidance:\n- â­ï¸ [YOLOv5 Tutorials](https://docs.ultralytics.com/yolov5/)\n- ðŸ¤– [Torch Hub Integration](https://pytorch.org/hub/ultralytics_yolov5/)\n\nLet us know if upgrading your environment or utilizing a verified environment resolves your issue. If not, please provide the requested MRE for further debugging. This is an automated response â€“ an Ultralytics engineer will take a closer look and assist you further soon. ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@llavkush to resolve the Python 3.7 compatibility issue with YOLOv5's walrus operator (`:=`), we recommend either:\n\n1. **Upgrading to Python â‰¥3.8** (officially required for YOLOv5), or  \n2. **Using YOLOv5 v6.2** which maintained Python 3.7 compatibility:  \n```python\nmodel = torch.hub.load('ultralytics/yolov5:v6.2', 'yolov5s')\n```\n\nFor details see the YOLOv5 v6.2 release notes. We strongly recommend updating to Python 3.8+ for continued support with modern versions."
      },
      {
        "user": "llavkush",
        "body": "@pderrenger thank you for the solution, this works. "
      }
    ]
  },
  {
    "issue_number": 13520,
    "title": "Improving Small Object Detection in Large Images Using Tiling",
    "author": "Uddeshya1052",
    "state": "open",
    "created_at": "2025-02-26T12:46:26Z",
    "updated_at": "2025-02-27T19:03:32Z",
    "labels": [
      "enhancement",
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nDescription\nI am working on detecting very small objects (e.g., screws) within large images (~4000x3000 pixels) of a perforated tank. However, I am facing challenges with low detection accuracy, likely due to the small object size relative to the full image.\n\nI believe tiling could be a good solution, but I am unsure of the best approach to integrate it into the training pipeline. Specifically:\n\nShould I label the full images first and then apply tiling, or should labeling be done after tiling?\nWhat is the recommended way to ensure consistency between tiles and the original annotations?\nAre there any best practices or existing implementations that could help with tiling for small object detection?\nAny guidance, references, or example scripts would be greatly appreciated.\n\nContext\nImage Size: ~4000x3000\nObjects: Very small (screws, etc.)\nModel: YOLO-based detection (but open to suggestions)\nThank you in advance for your help! ðŸš€\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Uddeshya1052, thank you for your interest in YOLOv5 ðŸš€! It sounds like you're tackling an interesting challenge in small object detection within large images. Please explore the following resources to potentially help address your question:\n\n### Resources\n\n1. â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) - Start from our Tutorials to better understand YOLOv5 capabilities, including tips for preparing datasets and optimizing detection.\n2. [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) - Learn how to improve model performance through data augmentation and anchoring techniques.\n3. [Discussions](https://github.com/ultralytics/yolov5/discussions) - Join the community to exchange tips about small object detection or tiling methods.\n\n### Recommendations\nIf tiling is your planned approach:\n1. Annotating **full images first** is generally recommended to ensure all objects are accounted for before splitting into tiles.\n2. Ensure tiling processes maintain annotation consistencyâ€”each tile must contain properly mapped labels relative to the objects within it. Open-source tools like `roboflow` or custom scripts can automate this step.\n3. Consider **data augmentation**, ensuring that the tiles resemble diverse real-world conditions and preventing overfitting to specific sizes or resolutions.\n\nAs YOLOv5 has high flexibility, you might also experiment with model hyperparameters like anchor sizes or train-scaled variants (e.g., YOLOv5x vs YOLOv5s) for detecting smaller objects ðŸª›. Check out [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/) for guidance.\n\n### Additional Resources\nYOLOv5 may be used in combination with diverse environments where tiling workflows can be implemented:\n- **Notebooks**: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- For automation, consider a **Docker Image**: See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n### Requirements\nEnsure you are running:\n- [Python>=3.8.0](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed\n- [PyTorch>=1.8](https://pytorch.org/get-started/locally/)\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n### Next Steps\nIf this is a â“ Question and you'd like further help, please:\n- Share additional details about your current progress, tiling script (if used), or results so far.\n- Provide examples of the images and labeling format to better assist.\n\nIf this is a ðŸ› Bug Report regarding YOLOv5 behavior, include a **minimum reproducible example (MRE)** for debugging.\n\nThis is an automated response to assist you efficiently ðŸ¤–. An Ultralytics engineer will review and provide additional guidance soon ðŸ”. Thank you again for your enthusiasm and for using YOLOv5! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@Uddeshya1052 for small object detection in large images with YOLOv5, we recommend:\n\n1. **Label Full Images First**: Annotate the original high-resolution images, then use a tiling script to split both images and labels into smaller tiles (e.g., 640x640). This maintains annotation consistency automatically.\n\n2. **Tiling Implementation**:\n```python\nfrom utils.dataloaders import LoadImagesAndLabels\n\ndataset = LoadImagesAndLabels(path, imgsz=640, augment=True)\n# Enable \"rectangular training\" with mosaic augmentation in your train.py call:\n# python train.py --rect --img 640\n```\n\n3. **Key Adjustments**:\n- Use `--img 640` (or larger)\n- Reduce anchor sizes in your YAML\n- Increase `--iou-thres` slightly\n\nFor implementation details, see the Ultralytics YOLOv5 documentation on training with mosaic augmentation and rectangular training. This approach effectively handles small objects while maintaining original annotation relationships."
      },
      {
        "user": "Uddeshya1052",
        "body": "@pderrenger Thank you for your answer! How can I get the tiling script? Is there an official source or example code available?"
      }
    ]
  },
  {
    "issue_number": 13521,
    "title": "object detection piÃ©ce metalique with camera basler",
    "author": "ninja8756",
    "state": "open",
    "created_at": "2025-02-26T14:34:47Z",
    "updated_at": "2025-02-27T01:32:04Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi âœŒ I am working on object Detection with yolov5 and I want to use basler camera to detect objects. how can I do that and get frames from the Basler camera and make detection? \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @ninja8756, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a â“ Question about using the Basler camera with YOLOv5, please ensure you have the necessary software to interface with your camera (e.g., Basler's [pypylon](https://github.com/basler/pypylon) library for Python) and integrate it to capture frames for processing. If you encounter issues during integration, try sharing some additional details like setup code snippets, errors, or steps causing difficulties, so we can better assist you.  \n\nIf you believe this is a ðŸ› Bug Report, please provide a **minimum reproducible example** (MRE), including code and steps to reproduce the issue, as well as details on your environment setup.\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in a variety of environments for seamless experimentation and deployment, including:\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nPlease note this is an automated response. An Ultralytics engineer will review your question and provide further assistance as soon as possible. Thank you for your patience! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@ninja8756 to integrate a Basler camera with YOLOv5 for object detection, you can use the `pypylon` library to capture frames and pass them to YOLOv5. Here's a minimal example:\n\n```python\nfrom ultralytics import YOLO\nfrom pypylon import pylon\n\nmodel = YOLO(\"yolov5s.pt\")  # Load YOLOv5 model\ncamera = pylon.InstantCamera(pylon.TlFactory.GetInstance().CreateFirstDevice())\ncamera.StartGrabbing(pylon.GrabStrategy_LatestImageOnly)\n\nwhile camera.IsGrabbing():\n    grabResult = camera.RetrieveResult(5000, pylon.TimeoutHandling_ThrowException)\n    if grabResult.GrabSucceeded():\n        img = grabResult.Array  # Get numpy array\n        results = model.predict(img)  # YOLOv5 inference\n        results[0].show()\n```\n\nInstall pypylon: `pip install pypylon`. For more details, see the Ultralytics YOLOv5 Predict documentation. Adjust camera settings as needed via Basler's Pylon API."
      }
    ]
  },
  {
    "issue_number": 10540,
    "title": "Environment does not support cv2.imshow() or PIL Image.show()",
    "author": "June-Joon",
    "state": "closed",
    "created_at": "2022-12-20T10:05:18Z",
    "updated_at": "2025-02-26T21:41:46Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI'm a beginner who just started programming.\r\n\r\nAs a trial, while running the basic code provided by ULOV5, I found that when I tried to run webcam, it did not run with the following statement.\r\n\r\nJust in case, I tried the basic colab tutorial, segment tutorial, classification tutorial, docker (cpu7.0), but only the same warning message came out and the webcam did not run.\r\n\r\nI can't solve it by referring to other questions/answers similar to mine, so can I get some help?\r\nI didn't change the code. I just run it.\r\n\r\n```\r\n!python detect.py --source 0  # webcam\r\n\r\n\r\ndetect: weights=yolov5s.pt, source=0, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\r\nYOLOv5 ðŸš€ v7.0-42-g5545ff3 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\r\n\r\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\r\n100% 14.1M/14.1M [00:00<00:00, 238MB/s]\r\n\r\nFusing layers... \r\nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\r\n**WARNING âš ï¸ Environment does not support cv2.imshow() or PIL Image.show()**\r\n\r\n[ WARN:0@14.066] global /io/opencv/modules/videoio/src/cap_v4l.cpp (902) open VIDEOIO(V4L2:/dev/video0): can't open camera by index\r\nTraceback (most recent call last):\r\n  File \"detect.py\", line 261, in <module>\r\n    main(opt)\r\n  File \"detect.py\", line 256, in main\r\n    run(**vars(opt))\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"detect.py\", line 106, in run\r\n    dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\r\n  File \"/content/yolov5/utils/dataloaders.py\", line 367, in __init__\r\n    assert cap.isOpened(), f'{st}Failed to open {s}'\r\nAssertionError: 1/1: 0... Failed to open 0\r\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @June-Joon, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@June-Joon webcam inference only works for local webcams, i.e. running YOLOv5 in a local python environment on a laptop webcam."
      },
      {
        "user": "June-Joon",
        "body": "@glenn-jocher \r\nThe same error message comes out when running on docker.\r\nIsn't the docker image a local python environment?"
      }
    ]
  },
  {
    "issue_number": 1104,
    "title": "Exported coreml don't have \"Class Labels\"",
    "author": "waheed0332",
    "state": "closed",
    "created_at": "2020-10-09T06:43:21Z",
    "updated_at": "2025-02-26T05:57:32Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "This is the conversion I did for SSD MobileNet\r\n<img width=\"1102\" alt=\"old\" src=\"https://user-images.githubusercontent.com/6158735/95551583-7b613280-0a24-11eb-9a60-1b4d99c3950a.png\">\r\nThis has class labels in it.\r\n\r\nHow do I add class labels for yolo?\r\n<img width=\"1102\" alt=\"new\" src=\"https://user-images.githubusercontent.com/6158735/95551564-743a2480-0a24-11eb-926c-9d4ec3f0e83a.png\">\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions."
      },
      {
        "user": "ALMerrill",
        "body": "Check out step four [at this link](https://coremltools.readme.io/docs/pytorch-conversion). You can add class labels from a file as you serialize the model"
      },
      {
        "user": "zetstream33",
        "body": "Did you solve it? Same Problem here."
      }
    ]
  },
  {
    "issue_number": 13509,
    "title": "Pypi uses AGPL",
    "author": "protonphoton",
    "state": "open",
    "created_at": "2025-02-12T03:25:53Z",
    "updated_at": "2025-02-25T20:06:30Z",
    "labels": [
      "invalid",
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I am recently trying out your yolov5 model but have some issues with your licensing. In this repo, clearly you are using AGPL but from your pypi package: https://pypi.org/project/ultralytics-yolov5/ , you have an Apache license. Does that mean as long as I am using the pypi package, I am not restricted by AGPL? If not, would you please update your license so we don't end up with license confusion? Thank you so much! \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "pderrenger",
        "body": "@ranranproton thanks for catching that! The AGPL-3.0 license applies to all Ultralytics YOLOv5 software and models as stated in our [LICENSE](https://github.com/ultralytics/yolov5/blob/master/LICENSE) file. The PyPI metadata has been corrected to reflect AGPL-3.0. For commercial licensing options that avoid AGPL requirements, see [Ultralytics Licensing](https://www.ultralytics.com/license). We appreciate your diligence!"
      },
      {
        "user": "protonphoton",
        "body": "Thanks @pderrenger . I have another question. If we started using yolov5 before there was even AGPL license couple years ago, should we change our license to AGPL or we can keep our previouse license? "
      },
      {
        "user": "pderrenger",
        "body": "@protonphoton if your usage commenced prior to the establishment of the AGPL-3.0 license and you have not updated to a version with AGPL-3.0, then your usage is governed by the terms that existed at that point, but any updates you make now would fall under AGPL-3.0 or require an [Ultralytics Enterprise License](https://www.ultralytics.com/license)."
      }
    ]
  },
  {
    "issue_number": 582,
    "title": "What's the difference between last.pt&best.pt, and which one should i use when i test my custom data?",
    "author": "PromiseXu1",
    "state": "closed",
    "created_at": "2020-07-31T07:15:29Z",
    "updated_at": "2025-02-25T19:56:33Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": " I have tried your work of yolov3 on my custom data, and when i train on yolov5  i find that yolov5 is more excellent! I just train for 100 epochs, the map50 have reached at 80! And i will train more epochs to see if it could be better .  \r\n## But **the question is that** when i finish the train, and start to test, which weight.pt should i choose? And what's the difference between last.pt and best.pt? \r\n Could you help me? Thanks sincerely.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello @PromiseXu1, thank you for your interest in our work! Please visit our [Custom Training Tutorial](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) to get started, and see our [Jupyter Notebook](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb) <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>, [Docker Image](https://hub.docker.com/r/ultralytics/yolov5), and [Google Cloud Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) for example environments.\n\nIf this is a bug report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom model or data training question, please note Ultralytics does **not** provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:\n- **Cloud-based AI** systems operating on **hundreds of HD video streams in realtime.**\n- **Edge AI** integrated into custom iOS and Android apps for realtime **30 FPS video inference.**\n- **Custom data training**, hyperparameter evolution, and model exportation to any destination.\n\nFor more information please visit https://www.ultralytics.com."
      },
      {
        "user": "glenn-jocher",
        "body": "@PromiseXu1 last.pt are the weights from the last epoch of training. best.pt are the best weights recorded during training."
      },
      {
        "user": "PromiseXu1",
        "body": "@glenn-jocher  ah it seems so obviously, thanks for your patient... but i find sometimes best.pt not always be there, and i find someone have mentioned it, are you dealing with it?  "
      }
    ]
  },
  {
    "issue_number": 13516,
    "title": "YOLO11 detection interpret and process output preds",
    "author": "poppyzy",
    "state": "open",
    "created_at": "2025-02-21T18:20:27Z",
    "updated_at": "2025-02-22T10:53:44Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nDoes anyone know how to interpret yolo11 detection results like (84, 2100), thanks\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @poppyzy, thank you for your interest in YOLOv5 ðŸš€! While YOLO11 is not an officially supported model in the YOLOv5 repository, weâ€™re happy to assist with your question as it relates to YOLO concepts. \n\nPlease visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for an overview of our supported models and workflows. For deeper insights into prediction outputs, examining our guides on inference and post-processing might prove helpful.\n\nIf this question is related to a custom or non-standard YOLO implementation, please provide as much information as possible, including:\n\n- **Details on your model**: Are you using any modifications or an entirely different fork of YOLO?\n- **Exact prediction outputs**: Please clarify the format (e.g., is it bounding boxes or class IDs?).\n- **Code snippets or specific methods**: Include any relevant code related to your inference process.\n\n## Requirements\n\nEnsure you are running on an up-to-date environment:\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). For YOLOv5, you can quickly set up as follows:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 and related workflows can be run within these verified environments (with pre-installed dependencies such as [CUDA](https://developer.nvidia.com/cuda), Python, and PyTorch):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. These tests verify the correct operation of YOLOv5 tools like [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py).\n\nThis is an automated response. An Ultralytics engineer will review and assist you further soon! ðŸ˜ƒ"
      },
      {
        "user": "pderrenger",
        "body": "@poppyzy while the YOLOv5 repo doesn't have YOLO11 in it, resources for understanding the output format of newer Ultralytics YOLO models, like YOLOv11 can be found here: [How to Use Ultralytics YOLO11 for Object Detection](https://www.ultralytics.com/blog/how-to-use-ultralytics-yolo11-for-object-detection) and [A guide to deep dive into object detection in 2025](https://www.ultralytics.com/blog/a-guide-to-deep-dive-into-object-detection-in-2025). These guides explain object detection and how models like YOLO11 work, which might help you interpret the detection results."
      }
    ]
  },
  {
    "issue_number": 13514,
    "title": "æ²¡æœ‰ç”Ÿæˆå›¾åƒ",
    "author": "cik-8672",
    "state": "open",
    "created_at": "2025-02-19T08:21:44Z",
    "updated_at": "2025-02-20T16:06:45Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nyolov5è®­ç»ƒå®ŒæˆåŽæ²¡æœ‰æ˜¾ç¤ºtrain_batch0.jpg\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @cik-8672, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us better understand and debug the issue. For this issue, an MRE might include the exact command you are running, your training configurations, and details about your environment (e.g., OS, Python, PyTorch version). Screenshots or logs demonstrating the problem would also be helpful! ðŸ˜Š\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset samples, training logs, and confirm that you followed our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\nPlease ensure you are using [**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) packages installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 is designed to run in a variety of environments. Please verify your setup using one of these methods:\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: See our [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: See our [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: Check our [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Debugging Steps\n\nFor your specific case of `train_batch0.jpg` not appearing, please confirm the following:\n1. Verify your training process is running without errors and log messages do not show any issues.\n2. Check that the `--save-period` or `--save-dir` settings are configured correctly to save visualizations and outputs.\n3. Confirm that the directory where `train_batch*.jpg` images are saved (default is under `runs/train/`) exists and is accessible.\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify the correct operation of YOLOv5 across all functions, platforms, and configurations.\n\nThis is an automated response to point you in the right direction ðŸ˜Š. An Ultralytics engineer will follow up shortly to assist you further! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@cik-8672 thanks for reaching out! It appears you're having trouble locating the `train_batch*.jpg` images after training. These images are created during training to visualize a batch of training data, and if they aren't appearing, double check your training settings and ensure that the script is running correctly, also make sure you are using the most recent version of YOLOv5 as there have been updates to areas of the code such as the Comet Logging and Visualization Integration."
      }
    ]
  },
  {
    "issue_number": 13515,
    "title": "code for the yaml file",
    "author": "rmarkovic00",
    "state": "open",
    "created_at": "2025-02-19T08:22:56Z",
    "updated_at": "2025-02-19T09:52:06Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi, I'm trying to pre-train the yolov5m model downloaded from the ultralitics site on my custom dataset which has 3 classes: car,truck person. Is it necessary to match the class ID with the class ID of the coco dataset in yaml file? For example: \n\nyaml file:\ntrain: /path_to_train_images/\nval: /path_to_val_images/\n\nnc: 8  # Broj klasa od 0 do 7 (moraÅ¡ ukljuÄiti sve klase do najveÄ‡eg ID-a!)\n\nnames: [\"Person\", \"Unknown\", \"Car\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Truck\"]\n\nor its just okej like this:\n\nyaml file:\ntrain: /path_to_train_images/\nval: /path_to_val_images/\n\nnc: 8  \n\nnames: [\"Person\", \"Unknown\", \"Car\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Truck\"]\n\nbecause in coco dataset person has ID=0 car=2 and truck=7.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @rmarkovic00, thank you for your interest in YOLOv5 ðŸš€ and for your question regarding custom dataset configuration! \n\nIt looks like you're trying to set up your YAML file for training YOLOv5 on a custom dataset. Here's some general guidance:\n\n- For **custom training questions**, it's important to ensure your dataset and YAML configuration are properly set up. The `names` in the YAML file should correspond to the classes in your dataset, and their indices (IDs) should match the order in which they appear in the `names` list, rather than aligning with COCO class IDs. YOLOv5 does not require any alignment with COCO for custom datasets. Please ensure your dataset annotations are consistent with your `names` list.\n\nIf this is a ðŸ› **Bug Report**, we kindly ask you to provide a **minimum reproducible example (MRE)**, including details on reproducible YAML configurations, relevant code snippets, debugging logs, and any other information to help us diagnose the issue more effectively ðŸš€.\n\n## Additional Resources\n\nPlease also refer to our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for more guidance:\n- Learn all about [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/).\n- Avoid common pitfalls with these [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\nYour environment should meet the following:\n- [**Python>=3.8.0**](https://www.python.org/) installed with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies, including [**PyTorch>=1.8**](https://pytorch.org/).\n\nTo verify installation, run:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Verified Environments\n\nYOLOv5 supports the following environments for training and running models:\n- Free GPU-enabled notebooks:  \n  <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- Cloud platforms with preconfigured environments:\n  - See the [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) for Google Cloud.\n  - Follow the [AWS Quickstart](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/) for Amazon's Deep Learning AMI.\n- Fully containerized training with [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>.\n\n## Status\n\nTo check current CI status, click below:  \n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>  \nIf the badge is green, all YOLOv5 Continuous Integration (CI) tests are passing âœ….\n\nAn Ultralytics engineer will assist you as soon as possible. Let us know if you have additional questions in the meantime! ðŸ˜Š"
      },
      {
        "user": "rmarkovic00",
        "body": "Thank you very much!\n"
      },
      {
        "user": "sriram-dsl",
        "body": "@rmarkovic00  When creating a custom dataset, there is no need to align your custom classes with COCO classes.  \n\nYou can define your own class names and structure, such as:  \n```yaml\nnc: 3  \nnames: [person, car, truck]  # or [car, person, truck] or [truck, person, car] â€“ the order doesn't matter  \n```  \nHowever, you must ensure that the labeling data is correctly aligned with the class names in your dataset.  \n\nFor example, if your labels are:  \n```\n0: person  \n1: car  \n2: truck  \n```\nThen, the `names` should be:  \n```yaml\nnames: [person, car, truck]\n```  \nAny CNN or ML model does not directly deal with class namesâ€”it only uses them for one-hot encoding and for human readability.  \n\n"
      }
    ]
  },
  {
    "issue_number": 1466,
    "title": "mAP bug at higher --conf",
    "author": "glenn-jocher",
    "state": "closed",
    "created_at": "2020-11-21T11:43:52Z",
    "updated_at": "2025-02-18T20:52:37Z",
    "labels": [
      "bug"
    ],
    "body": "A recent modification to the PR curve in pull request #1206 computation introduced a bug whereby mAP increases at higher --conf thresholds. This was caused by a change to the 'sentinel values' on the P and R vectors here:\r\n\r\n```python\r\n    # Append sentinel values to beginning and end\r\n    mrec = recall  # np.concatenate(([0.], recall, [recall[-1] + 1E-3]))\r\n    mpre = precision  # np.concatenate(([0.], precision, [0.]))\r\n```\r\n\r\nThe appropriate solution would be to reinstitute the old code, which drops the curves to zero after their last data point, or to interpolate it to zero at recall = 1. I'll experiment with both and implement a fix soon.\r\n\r\nThis does not affect any operations using the default test.py --conf 0.001, so I would imagine almost no users would be impacted by this, but it needs fixing in any case.",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "A third option would be to extrapolate the curves to zero based on their last known derivatives. I think np.interp has an option for this baked in, could be used in conjunction with np.clip(0,1)."
      },
      {
        "user": "glenn-jocher",
        "body": "Update on this. np.interp does not have built in extrapolation capability, we would  need to mode to scipy for that, so I think I will simply turn back the clock on the code updates introduced in PR #1206 "
      },
      {
        "user": "imyhxy",
        "body": "@glenn-jocher Hey man, current yolov5 codebase has this problem again. Can you solve this?"
      }
    ]
  },
  {
    "issue_number": 5052,
    "title": "box_loss, obj_loss, and cls_loss",
    "author": "karl-gardner",
    "state": "closed",
    "created_at": "2021-10-05T07:51:14Z",
    "updated_at": "2025-02-18T04:20:35Z",
    "labels": [
      "question"
    ],
    "body": "Hello Glenn et al.,\r\n\r\nI am wondering what all the different losses mean in the results figure and where I can learn more about this? If you can give the equations for these losses that would be great. Specifically the box, obj, and cls loss?  Is the box loss referring to the Generalized IOU loss (GIOU).\r\n\r\n![results](https://user-images.githubusercontent.com/91646805/135982399-be23f100-9735-43ff-a51e-59bb83c9740c.png)\r\n\r\nThanks,\r\n\r\nKarl Gardner | Texas Tech University",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@kgardner330 box loss is the regression loss for output xywh bounding boxes. Loss criteria in use is CIoU(). You can see details in loss.py:\r\nhttps://github.com/ultralytics/yolov5/blob/5afc9c25ef0874dff0c18267947ea4e8b03c90f4/utils/loss.py#L131-L137\r\n\r\nFor a general description of the YOLO losses you should read the first 3 YOLO papers:\r\nhttps://pjreddie.com/publications/"
      },
      {
        "user": "gepaohhh",
        "body": "Hello Glenn et al.,\r\nI am wonder why there is nothing in my graphics, weather is something wrong in my CUDA version\r\n![image](https://user-images.githubusercontent.com/119571806/205565878-30538645-d5be-4baa-a7b0-ff785e0f8d84.png)\r\n\r\n\r\n"
      },
      {
        "user": "glenn-jocher",
        "body": "@gepaohhh you are showing mAP so you do have validation data, not sure why your losses are nan (which are not plotted)."
      }
    ]
  },
  {
    "issue_number": 13154,
    "title": "how to get mIoU and mPA in yolov5_seg?",
    "author": "Powerfulidot",
    "state": "closed",
    "created_at": "2024-07-01T00:55:48Z",
    "updated_at": "2025-02-17T17:52:03Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nhow can i get mIoU and mPA in yolov5_seg validations? i want to do some comparative experiment with other semantic segmentation algorithms like Unet and PSPNet but they mostly have methods of evaluation like mIoU and mPA, which isnt included in yolov5_seg. \r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @Powerfulidot, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@Powerfulidot hello,\n\nThank you for reaching out and for your interest in using YOLOv5 for your segmentation tasks!\n\nTo calculate metrics like mean Intersection over Union (mIoU) and mean Pixel Accuracy (mPA) for YOLOv5 segmentation (`yolov5_seg`), you can follow these steps:\n\n1. **Update to the Latest Version**: Ensure you are using the latest version of YOLOv5 from the [Ultralytics repository](https://github.com/ultralytics/yolov5) and the latest version of `torch`. This ensures you have the latest features and bug fixes.\n\n2. **Custom Evaluation Script**: YOLOv5 does not natively include mIoU and mPA metrics in its validation script. However, you can modify the validation script to include these metrics. Below is an example of how you might calculate these metrics:\n\n```python\nimport torch\nimport numpy as np\nfrom utils.metrics import ConfusionMatrix\n\ndef calculate_mIoU_mPA(preds, targets, num_classes):\n    cm = ConfusionMatrix(num_classes)\n    cm.process_batch(preds, targets)\n    iou = cm.iou()\n    mIoU = iou.mean().item()\n    mPA = (cm.tp / (cm.tp + cm.fn)).mean().item()\n    return mIoU, mPA\n\n# Example usage during validation\n# preds and targets should be tensors of shape [batch_size, height, width]\npreds = torch.argmax(predictions, dim=1)  # Assuming predictions are logits\nmIoU, mPA = calculate_mIoU_mPA(preds, targets, num_classes=21)  # Adjust num_classes as needed\nprint(f'mIoU: {mIoU}, mPA: {mPA}')\n```\n\n3. **Integrate with YOLOv5 Validation**: You can integrate the above function into the YOLOv5 validation loop. Modify the `val.py` script to include calls to `calculate_mIoU_mPA` and print or log the results.\n\n4. **Comparative Experiments**: Once you have integrated these metrics, you can run your validation and compare the results with other semantic segmentation algorithms like Unet and PSPNet.\n\nIf you encounter any issues or need further assistance, please provide a minimum reproducible code example so we can better understand and address your specific situation. You can find guidance on creating a minimum reproducible example [here](https://docs.ultralytics.com/help/minimum_reproducible_example).\n\nWe hope this helps! If you have any more questions, feel free to ask. ðŸ˜Š"
      },
      {
        "user": "Powerfulidot",
        "body": "> @Powerfulidot hello,\r\n> \r\n> Thank you for reaching out and for your interest in using YOLOv5 for your segmentation tasks!\r\n> \r\n> To calculate metrics like mean Intersection over Union (mIoU) and mean Pixel Accuracy (mPA) for YOLOv5 segmentation (`yolov5_seg`), you can follow these steps:\r\n> \r\n>     1. **Update to the Latest Version**: Ensure you are using the latest version of YOLOv5 from the [Ultralytics repository](https://github.com/ultralytics/yolov5) and the latest version of `torch`. This ensures you have the latest features and bug fixes.\r\n> \r\n>     2. **Custom Evaluation Script**: YOLOv5 does not natively include mIoU and mPA metrics in its validation script. However, you can modify the validation script to include these metrics. Below is an example of how you might calculate these metrics:\r\n> \r\n> \r\n> ```python\r\n> import torch\r\n> import numpy as np\r\n> from utils.metrics import ConfusionMatrix\r\n> \r\n> def calculate_mIoU_mPA(preds, targets, num_classes):\r\n>     cm = ConfusionMatrix(num_classes)\r\n>     cm.process_batch(preds, targets)\r\n>     iou = cm.iou()\r\n>     mIoU = iou.mean().item()\r\n>     mPA = (cm.tp / (cm.tp + cm.fn)).mean().item()\r\n>     return mIoU, mPA\r\n> \r\n> # Example usage during validation\r\n> # preds and targets should be tensors of shape [batch_size, height, width]\r\n> preds = torch.argmax(predictions, dim=1)  # Assuming predictions are logits\r\n> mIoU, mPA = calculate_mIoU_mPA(preds, targets, num_classes=21)  # Adjust num_classes as needed\r\n> print(f'mIoU: {mIoU}, mPA: {mPA}')\r\n> ```\r\n> \r\n>     3. **Integrate with YOLOv5 Validation**: You can integrate the above function into the YOLOv5 validation loop. Modify the `val.py` script to include calls to `calculate_mIoU_mPA` and print or log the results.\r\n> \r\n>     4. **Comparative Experiments**: Once you have integrated these metrics, you can run your validation and compare the results with other semantic segmentation algorithms like Unet and PSPNet.\r\n> \r\n> \r\n> If you encounter any issues or need further assistance, please provide a minimum reproducible code example so we can better understand and address your specific situation. You can find guidance on creating a minimum reproducible example [here](https://docs.ultralytics.com/help/minimum_reproducible_example).\r\n> \r\n> We hope this helps! If you have any more questions, feel free to ask. ðŸ˜Š\r\n\r\nthank you so much for such quick reply but i ve achieved it just now. thank you anyway!"
      }
    ]
  },
  {
    "issue_number": 11734,
    "title": "Why different input image sizes influence the inference time?",
    "author": "kangarin",
    "state": "closed",
    "created_at": "2023-06-19T11:22:26Z",
    "updated_at": "2025-02-16T22:25:50Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nI'm working on a project that requires understanding the relationship between input video resolution and inference latency. I've looked at the code, but I don't understand why different input sizes result in significantly different inference latencies, given that the network's input is fixed at 640 * 640 * 3.\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @kangarin, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      },
      {
        "user": "glenn-jocher",
        "body": "@kangarin the input image size affects inference time because larger images require more data to be processed, impacting the computational load. Resizing the images to the network's input size allows for faster inference by reducing the computational complexity. For more details, you can refer to the Ultralytics Docs at https://docs.ultralytics.com/yolov5. If you have further questions, feel free to ask!"
      }
    ]
  },
  {
    "issue_number": 13505,
    "title": "Detecting Botox injection points in facial images",
    "author": "ebkablan",
    "state": "open",
    "created_at": "2025-02-06T12:44:42Z",
    "updated_at": "2025-02-16T19:13:27Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am currently working on detecting Botox injection points (very small objects) in facial images using the YOLOv5 model. The images have a resolution of 6240Ã—4160, and the injection points were labeled by an expert dermatologist using 40Ã—40 bounding boxes. I have trained different YOLOv5 versions (nano, small, medium) using pretrained models, but the precision and recall values remain low during training.\n\nSince YOLOv5 automatically applies the autoanchor feature, I expected better performance. However, the detection results suggest that the model may struggle with small object detection or resolution scaling. I would appreciate any insights or recommendations on improving detection accuracy, such as potential adjustments to the model configuration, anchor tuning, or alternative training strategies.\n\nLooking forward to your advice.\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @ebkablan, thank you for your interest in YOLOv5 ðŸš€! Detecting small objects like Botox injection points can indeed be challenging. Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) for guidance on custom data training and optimization techniques. For similar projects, you might find our [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) pages particularly useful.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us investigate further.\n\nIf this is a â“ Question or general request for advice, please include as much relevant detail as possible to help us assist you effectively, such as:\n- Example images with corresponding labels.\n- Your training settings (e.g., batch size, image size, epochs, augmentations).\n- Training logs, plots, or metrics (precision/recall, loss curves), if available.\n\n## Requirements\n\nEnsure your environment meets the following:\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To set up, simply:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in the following verified environments, which include pre-installed dependencies like [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn):\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify the functionality of YOLOv5 features like [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) across macOS, Windows, and Ubuntu.\n\nThis is an automated response, but don't worryâ€”an Ultralytics engineer will also review your issue and provide further assistance soon! ðŸ˜Š"
      },
      {
        "user": "ebkablan",
        "body": "![Image](https://github.com/user-attachments/assets/6658fa48-639d-4d57-b208-1d5a8e6408f8)\n\n[MERVECELIK_5.txt](https://github.com/user-attachments/files/18690264/MERVECELIK_5.txt) You can access an example image and label is here. batch size:4, image size:1280, epochs:300, augmentations: hyp-scratch-low.yaml"
      },
      {
        "user": "pderrenger",
        "body": "@ebkablan thanks for sharing the training details and sample image; for such small objects, you might try increasing the effective resolution (via cropping or tiling) and manually tailoring your anchor settings to better match the 40Ã—40 box size instead of relying solely on autoanchor, as referenced in our anchor-based detectors glossary (https://ultralytics.com/glossary/anchor-based-detectors)."
      }
    ]
  },
  {
    "issue_number": 13508,
    "title": "A problem about calculating confusion matrices",
    "author": "SwustLiC",
    "state": "open",
    "created_at": "2025-02-12T01:58:28Z",
    "updated_at": "2025-02-16T19:01:55Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\nin yolov5/utils/metics.py\n\nclass ConfusionMatrix:\n    \"\"\"Generates and visualizes a confusion matrix for evaluating object detection classification performance.\"\"\"\n\n    def __init__(self, nc, conf=0.25, iou_thres=0.45):\n        \"\"\"Initializes ConfusionMatrix with given number of classes, confidence, and IoU threshold.\"\"\"\n        self.matrix = np.zeros((nc + 1, nc + 1))\n        self.nc = nc  # number of classes\n        self.conf = conf\n        self.iou_thres = iou_thres\n\n    def process_batch(self, detections, labels):\n        \"\"\"\n        Return intersection-over-union (Jaccard index) of boxes.\n\n        Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n\n        Arguments:\n            detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n            labels (Array[M, 5]), class, x1, y1, x2, y2\n        Returns:\n            None, updates confusion matrix accordingly\n        \"\"\"\n        if detections is None:\n            gt_classes = labels.int()\n            for gc in gt_classes:\n                self.matrix[self.nc, gc] += 1  # background FN\n            return\n\n        detections = detections[detections[:, 4] > self.conf]\n        gt_classes = labels[:, 0].int()\n        detection_classes = detections[:, 5].int()\n        iou = box_iou(labels[:, 1:], detections[:, :4])\n\n        x = torch.where(iou > self.iou_thres)\n        if x[0].shape[0]:\n            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\n            if x[0].shape[0] > 1:\n                matches = matches[matches[:, 2].argsort()[::-1]]\n                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n                matches = matches[matches[:, 2].argsort()[::-1]]\n                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n        else:\n            matches = np.zeros((0, 3))\n\n        n = matches.shape[0] > 0\n        m0, m1, _ = matches.transpose().astype(int)\n        for i, gc in enumerate(gt_classes):\n            j = m0 == i\n            if n and sum(j) == 1:\n                self.matrix[detection_classes[m1[j]], gc] += 1  # correct\n            else:\n                self.matrix[self.nc, gc] += 1  # true background\n\n        if n:\n            for i, dc in enumerate(detection_classes):\n                if not any(m1 == i):\n                    self.matrix[dc, self.nc] += 1  # predicted background\nWhen the detection box does not match with gt, that is, when iou is 0, the detection box is not calculated into fp. Is this a special design or a bug. The confusion matrix in YOLOV11 is not quite the same\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "ðŸ‘‹ Hello @SwustLiC, thank you for your interest in YOLOv5 ðŸš€! Your detailed description regarding the confusion matrix behavior is much appreciated. To help us debug and respond effectively, could you kindly provide a **minimum reproducible example (MRE)**? This will allow us to replicate the issue on our side and better understand the behavior you've described. \n\nFor reference:\n- Please include any relevant code snippets, data (if possible), or steps to reproduce the behavior.\n- Highlight any dataset specifics or settings that may influence the result.\n\nIf you're curious about how to get started or need additional context, please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/), where you'll find helpful resources such as [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/) and guides for [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/).\n\n## Requirements\n\nEnsure that your environment meets the following:\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To set up:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 is verified in the following environments and could help to test for consistency:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## CI Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>  \nIf this badge is green, all Continuous Integration (CI) tests are passing. CI tests validate [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [detection](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) across macOS, Windows, and Ubuntu daily and for every commit.\n\nPlease note that this is an automated response to assist you quickly ðŸ˜Š. An Ultralytics engineer will follow up with you soon to provide further guidance."
      },
      {
        "user": "SwustLiC",
        "body": "detections: tensor([[9.8200e+02, 9.5500e+02, 1.1070e+03, 1.0460e+03, 9.1230e-01, 3.0000e+00]])\nlabels: tensor([[   3, 1195,  953, 1319, 1035]])\niou =0\nnow n=false and will not enter the following loop, so the predicted box will not be counted in the fp?\n      if n:\n                  for i, dc in enumerate(detection_classes):\n                      if not any(m1 == i):\n                          self.matrix[dc, self.nc] += 1  # predicted background"
      },
      {
        "user": "pderrenger",
        "body": "@SwustLiC the current logic intentionally counts unmatched predictions as background (FP) only when there are existing matches (`n=True`). Detections with no matches in the entire batch (`n=False`) are excluded from FP counts. This is a known design choice in YOLOv5's confusion matrix implementation to avoid overcounting in batch processing. For a strict per-detection FP count, you could modify the logic to check all detections against all labels regardless of batch matches. Would you like to submit a PR to propose an adjustment?"
      }
    ]
  },
  {
    "issue_number": 1333,
    "title": "where is yolov5 paper?",
    "author": "eeric",
    "state": "closed",
    "created_at": "2020-11-10T08:53:30Z",
    "updated_at": "2025-02-15T04:58:45Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "## â”Question\r\n\r\nwhere is paper\r\n## Additional context\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello @eeric, thank you for your interest in our work! Please visit our [Custom Training Tutorial](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) to get started, and see our [Jupyter Notebook](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb) <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>, [Docker Image](https://hub.docker.com/r/ultralytics/yolov5), and [Google Cloud Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) for example environments.\n\nIf this is a bug report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom model or data training question, please note Ultralytics does **not** provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:\n- **Cloud-based AI** systems operating on **hundreds of HD video streams in realtime.**\n- **Edge AI** integrated into custom iOS and Android apps for realtime **30 FPS video inference.**\n- **Custom data training**, hyperparameter evolution, and model exportation to any destination.\n\nFor more information please visit https://www.ultralytics.com."
      },
      {
        "user": "glenn-jocher",
        "body": "@eeric hi thanks for asking! We aim to publish a short paper to arxiv by the end of 2020. In the meantime please see https://doi.org/10.5281/zenodo.3908559 for DOI to cite this repo directory, and where you can also find bibtex and other exportable citation formats.\r\n\r\n[![DOI](https://zenodo.org/badge/264818686.svg)](https://zenodo.org/badge/latestdoi/264818686)"
      },
      {
        "user": "hujunchao",
        "body": "Looking forward to meeting YOLOv5 paper!"
      }
    ]
  },
  {
    "issue_number": 5710,
    "title": "GPL 3.0 license comments",
    "author": "JJrodny",
    "state": "closed",
    "created_at": "2021-11-18T16:52:06Z",
    "updated_at": "2025-02-14T11:30:14Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar feature requests.\n\n\n### Description\n\nFirst of all, let me say that I was wowed multiple times when training models using this repo. Each time the repo already does the thing that either I need and have implemented myself in other code (e.g. printing out first few training images to confirm my augmentations and data are good), or something that I didn't realize I needed but is super helpful (e.g. printing all of the labels analysis, printing a well thought-out confusion matrix, etc), it has wowed me multiple times, well done!\r\n\r\nThis issue is in relation to using models trained with this code commercially.\r\n\r\nI agree with the GPL 3.0 license for the training portion of this repo, that any improvements on training models should be shared with all, and GPL 3.0 ensures that: anyone who modifies the code _must_ also keep the open access GPL 3.0 license, so everyone can benefit from training even better models.\r\n\r\nThe problem I'm having is that I'm working on a commercial product which I'd like to include a Yolov5 trained model in it (because of the nice training tools Yolov5 has to make it easier to train models, and because I can add my own augmentations on the fly via  albumentations which is written in pytorch). I don't specifically want to modify the training code, I would just like to train a model using the Ultralytics code that's GPL3.0 licensed, and then use that model in my commercial product.\r\n\r\nNow, because of the GPL 3.0 license, I can't include any of the Ultralytics code into my product, (otherwise I'd have to publicly publish all of the code for my commercial product, because whatever GPL 3.0 code touches also becomes GPL 3.0).\r\n\r\nAFAIK I can however use the model trained by GPL 3.0 code, just like I can write a document in a GPL 3.0 word editor and the document isn't also GPL 3.0 licensed.\r\n\r\nAnd I can inference that model using torch and torchvision libraries, by loading the model file as JIT and inferencing the model with an input image.\r\n\r\nMy problem now, is that that output is just a tensor - the .pt model is the backbone of the model, and I need to implement the head to convert that tensor into bounding boxes; that tensor is the last tensor in the model architecture before we use the anchors to calculate the bounding boxes.\r\n\r\nNow I can't use Yolov5 Ultralytics' inference code that converts the tensor into bounding boxes in my project, becuase it's GPL 3.0 license requires me to make public all of the code that code touches (and thus my competitors will now just copy my code that touches the Ultralytics code)\r\n\r\nThis means in order for Yolov5 Ultralytics trained models to be used commercially, every commercial entity needs to write their own code to convert the tensor output into bounding boxes.\r\n\r\nMy request is for the Yolov5 Ultralytics repo to strike that balance between openness and usefulness. In order to keep this training code open and available for all to use, but also to allow commercial entities to use models trained by this code, I'd like to request that the inference code be put in a separate codebase (or a copy of it or something?) with a less stringent copyleft license that allows commercial entities to use that code in their codebases without forcing them to make their codebases public.\r\n\r\nThis would help Yolov5 Ultralytics to be used as widely as the original DarkNet Yolo repos, while still sharing any improvements in training better and better yolov5 models with everyone.\r\n\r\nI did some searching on this and found this https://github.com/ultralytics/yolov5/issues/5621 and this https://github.com/ultralytics/yolov5/issues/4716 issue, but they don't directly get at what I'm trying to describe.\r\n\r\nThanks for your consideration!\n\n### Use case\n\nWhile _technically_ commercial entities _can_ use the Ultralytics codebase, realistically no commercial entity will want to make their entire codebase public because in order to inference an Ultralytics Yolov5 model, we currently need to either use the Ultralytics code (which forces the company to reveal their own code to competitors), or write their own inference code.\r\n\r\nI am currently working on writing my own inference code so that our company can still use the great features this repo allows, but I doubt many other companies have the background and knowledge to do this themselves. Making only the inference code available under a less restrictive license will allow other companies and entities to make use of the Ultralytics codebase and fuel a larger adoption of the code and increase viewership - this is a great repo and more people need to see that!\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @JJrodny, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email Glenn Jocher at glenn.jocher@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.6.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\n$ git clone https://github.com/ultralytics/yolov5\n$ cd yolov5\n$ pip install -r requirements.txt\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions\"><img src=\"https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg\" alt=\"CI CPU testing\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), validation ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@JJrodny I've modified your incorrect issue title so as not to confuse users. GPL 3.0 clearly allows for commercial use. It's the very first usage example shown in\r\nhttps://github.com/ultralytics/yolov5/blob/master/LICENSE\r\n\r\n<img width=\"1230\" alt=\"Screenshot 2021-11-19 at 14 07 12\" src=\"https://user-images.githubusercontent.com/26833433/142627748-23f72ce6-89e6-469c-b235-a26a9f0ae855.png\">\r\n\r\n"
      },
      {
        "user": "JJrodny",
        "body": "Hey Glenn, thanks for replying!\r\n\r\nI've seen that before, that the ribbon for GPL 3.0 on github _says_ it's permitted to be used commercially, but the problem is it's so restrictive as to never practically be used commercially. Anyone who even links to this code (calls it like a library) must also make their own code open source. That's the problem GPLv3.0 has - companies have to make their code open source to use GPLv3.0 code, which makes it off-limits to many companies.\r\n\r\nHere's a quick link with more info on the GPLv3.0 license:\r\nhttps://tldrlegal.com/license/gnu-general-public-license-v3-(gpl-3)\r\nAnd under Disclose Source:\r\n> All code linked with GPL 3.0 source code must be disclosed under a GPL 3.0 compatible license.\r\n\r\nBut because of this problem, a similar version of GPLv3.0 was designed to give more people reason to use the code: LGPLv3.0\r\nhttps://tldrlegal.com/license/gnu-lesser-general-public-license-v3-(lgpl-3)\r\nAnd under Disclose Source:\r\n> If you distribute this library in an executable, you must make the source available for 3 years. \r\n\r\nThe difference here is that we can now call functions from the code, use the code as a library without having to make the rest of the company's IP-laden codebase open source.\r\n\r\nGPLv3.0 says if you even call this code your codebase must also become open source, whereas LGPLv3.0 says that if you call this code (without modifying the code, e.g. just treat the open source code as a library), then you don't have to make the rest of your company codebase open source as well.\r\n\r\nThis was the problem others have brought up, e.g. Kaggle doesn't accept GPLv3.0 for competitions, https://github.com/ultralytics/yolov5/issues/442, https://github.com/ultralytics/yolov5/issues/317, and others have pointed out GPLv3.0 allows only commercial Software as a Service (SaaS) uses (force customers to be connected to the internet and pay per image inferenced (unusable without internet access)), but not distribution of the software (allow customers to install a model on an internet-isolated machine once and allow to be run until a new model is needed) https://github.com/ultralytics/yolov5/issues/648\r\n\r\nI am not a lawyer, but as far as I understand it, what this means for a company, is that:\r\n\r\n- If it's a more permissive license like MIT or Apache, we'll be able to send a customer a packaged solution to e.g. detect visual defects in their manufacturing line, locate whales and other boats on boats out at sea, etc that includes the YOLOv5 codebase.\r\n\r\n- If it's under the LGPLv3.0 license we can send a customer a solution that requires them to download the YOLOv5 codebase as a library, and then our code calls functions in the library, and can still be used in those use cases.\r\n\r\n- If it's under the GPLv3.0 license, we can't send the customer a solution because we'll need to make our IP-laden codebase open source because some of it calls YOLOv5 code, so we'll have to use a different codebase (e.g. Alexey's v4 Scaled Darknet, or Detectron2, etc.). AFAIK only SaaS companies could use it, and then only for uses connected to the internet, (manufacturers rarely connect their PCs to the internet, and internet out at sea requires additional equipment that is not always reliable and can be costly in some instances)\r\n\r\nDon't change the license without talking to lawyers/HR or other people knowledgeable about it, but I would suggest Ultralytics' YOLOv5 license be changed to a more permissive version like LGPLv3.0, so that non-SaaS companies can also use YOLOv5.\r\n\r\nThanks for your consideration!\r\n-Jeff"
      }
    ]
  },
  {
    "issue_number": 13511,
    "title": "ä¸ºä»€ä¹ˆval.pyè®¡ç®—å‡ºçš„mAP50å’Œpycocotools.cocoè®¡ç®—å‡ºçš„AP50ç›¸å·®éžå¸¸å¤§",
    "author": "lqh964165950",
    "state": "open",
    "created_at": "2025-02-13T14:26:36Z",
    "updated_at": "2025-02-13T14:27:24Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n![Image](https://github.com/user-attachments/assets/b11316eb-afeb-42bc-9873-d405ea0814a7)\n\n![Image](https://github.com/user-attachments/assets/73203cb2-1349-427d-8aa6-2c3ec8894397)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "ðŸ‘‹ Hello @lqh964165950, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us debug it. This should include:\n1. The exact code, commands, and data used to produce the results.\n2. A clear description of the issue and the expected behavior.\n3. Any relevant screenshots, logs, or error messages.\n\nIf this is a â“ Question, please provide as much information as possible, including dataset examples, training logs, and details about how you are performing the evaluation (e.g., parameters, metrics, etc.), so we can better understand the discrepancy you are observing.\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will review your issue soon and provide additional assistance. Thank you for your patience! ðŸ˜Š"
      }
    ]
  },
  {
    "issue_number": 1777,
    "title": "ValueError: API key must be 40 characters long, yours was 1",
    "author": "zhangsong1213",
    "state": "closed",
    "created_at": "2020-12-25T10:06:21Z",
    "updated_at": "2025-02-13T11:04:55Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "when i run !wandb login \r\ni got  ValueError: API key must be 40 characters long, yours was 1\r\n![image](https://user-images.githubusercontent.com/48464621/103131466-da594e00-46db-11eb-8475-61daffd306e5.png)\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello @zhangsong1213, thank you for your interest in ðŸš€ YOLOv5! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://www.ultralytics.com or email Glenn Jocher at glenn.jocher@ultralytics.com.\n\n## Requirements\n\nPython 3.8 or later with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including `torch>=1.7`. To install run:\n```bash\n$ pip install -r requirements.txt\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab Notebook** with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n- **Kaggle Notebook** with free GPU: [https://www.kaggle.com/models/ultralytics/yolov5](https://www.kaggle.com/models/ultralytics/yolov5)\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) \n- **Docker Image** https://hub.docker.com/r/ultralytics/yolov5. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker)\n\n## Status\n\n![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/models/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@zhangsong1213 you can copy and paste from the login site provided by `wandb login` command. CTRL+V to paste the API key, which should be 40 characters as the message says."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "issue_number": 13507,
    "title": "How to solve the YOLO original model mAP50-95 results the more you run the better",
    "author": "lroy615",
    "state": "open",
    "created_at": "2025-02-12T01:36:53Z",
    "updated_at": "2025-02-12T01:37:36Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nTraining the original YOLOv5s model results ran about three or four times, each time the results are better and better, precison/recall/mAP50 values are almost the same each time, so the comparison is to look at the value of mAP50-95, but the original model the value of this value is better and better, the first 100 rounds of training of the mAP50-95 results are 0.86 or so, the latest The first 100 rounds of training the mAP50-95 result is 0.86 or so, the latest run is 0.89 or so, but I want to reproduce the first result, because my improved model although the result is trained, precison/recall/mAP50 value is almost the same as the original, it is already 0.98 or so, but the mAP50-95 run is 0.87 or so, I would like to ask how to solve this problem, can you help I reproduce the first result.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "ðŸ‘‹ Hello @lroy615, thank you for reaching out and for your interest in YOLOv5 ðŸš€! \n\nIt seems you have a detailed question regarding training reproducibility and mAP50-95 results across multiple runs. To assist you better and debug this behavior effectively, we kindly ask that you provide the following:\n\n- A **minimum reproducible example (MRE)**. This includes your training command, hyperparameter modifications, and any changes youâ€™ve made to the codebase, if applicable. Please also include details on the dataset you are using (even just general information, such as dataset size and structure).  \n- Logs or metrics from your training runs, if available, that highlight the observed behavior of mAP50-95 improving with each run.\n- Your current random seed setting (if any), as randomization can impact training reproducibility.\n\nIf you're aiming to reproduce a specific past result, you may want to revisit the conditions of that run, including software versions, random seed settings (if manually set), and training environment configuration.\n\nTo get started with YOLOv5 or ensure you have the required setup, please follow these steps:\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To set up YOLOv5:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in any of the following environments with dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled:\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Tutorials\n\nYou may also find the following resources helpful:\n- [Custom Training Guide](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) for ensuring optimal hyperparameters and setups for your use case.\n- [Model Training Tips](https://docs.ultralytics.com/guides/model-training-tips/) to achieve the best training results.\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a> \n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify the proper operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) across macOS, Windows, and Ubuntu daily.\n\nðŸŒŸ This is an automated response. Rest assured, an Ultralytics engineer will follow up to assist you further. We look forward to your updates and additional details!"
      }
    ]
  },
  {
    "issue_number": 388,
    "title": "How to get the coordinates of the bounding box in YOLO object detection?",
    "author": "milind-soni",
    "state": "closed",
    "created_at": "2020-07-13T18:38:58Z",
    "updated_at": "2025-02-08T22:23:12Z",
    "labels": [
      "question"
    ],
    "body": "## â”Question\r\nI need to get the bounding box coordinates generated in an image using the object detection. How do I achieve that\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello @milind-soni, thank you for your interest in our work! Please visit our [Custom Training Tutorial](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) to get started, and see our [Jupyter Notebook](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb) <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>, [Docker Image](https://hub.docker.com/r/ultralytics/yolov5), and [Google Cloud Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) for example environments.\n  \nIf this is a bug report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n  \nIf this is a custom model or data training question, please note that Ultralytics does **not** provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:\n- **Cloud-based AI** systems operating on **hundreds of HD video streams in realtime.**\n- **Edge AI** integrated into custom iOS and Android apps for realtime **30 FPS video inference.**\n- **Custom data training**, hyperparameter evolution, and model exportation to any destination.\n\nFor more information please visit https://www.ultralytics.com."
      },
      {
        "user": "glenn-jocher",
        "body": "@milind-soni detection results are available here:\r\nhttps://github.com/ultralytics/yolov5/blob/ea34f848a6afbe1fc0010745fdc5f356ed871909/detect.py#L92-L102"
      },
      {
        "user": "ankamdeepika",
        "body": "@glenn-jocher could you help me crop the detected objects using the bounding box coordinates "
      }
    ]
  },
  {
    "issue_number": 1185,
    "title": "Does yolov5 has the data augmentation of flip horizontally ? ",
    "author": "wwdok",
    "state": "closed",
    "created_at": "2020-10-21T08:57:35Z",
    "updated_at": "2025-02-07T20:34:14Z",
    "labels": [
      "question"
    ],
    "body": "## â”Question\r\nI see some basic data augmentations in train.py, but i don't see flip horizontally and mosaic and so on, so does yolov5 support other more data augmentations ?\r\n\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello @wwdok, thank you for your interest in our work! Please visit our [Custom Training Tutorial](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) to get started, and see our [Jupyter Notebook](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb) <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>, [Docker Image](https://hub.docker.com/r/ultralytics/yolov5), and [Google Cloud Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) for example environments.\n\nIf this is a bug report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom model or data training question, please note Ultralytics does **not** provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:\n- **Cloud-based AI** systems operating on **hundreds of HD video streams in realtime.**\n- **Edge AI** integrated into custom iOS and Android apps for realtime **30 FPS video inference.**\n- **Custom data training**, hyperparameter evolution, and model exportation to any destination.\n\nFor more information please visit https://www.ultralytics.com."
      },
      {
        "user": "glenn-jocher",
        "body": "Augmentation hyperparameters are located here:\r\nhttps://github.com/ultralytics/yolov5/blob/83deec133d62021b496ba6cc475c38e717098053/data/hyp.scratch.yaml#L1-L33\r\n\r\n"
      },
      {
        "user": "wwdok",
        "body": "@glenn-jocher Sorry, i find my repo is out of date, the newest repo already has fliplr hypeparameter ! Thanks !"
      }
    ]
  },
  {
    "issue_number": 10940,
    "title": "wandb.errors.UsageError: Invalid project name '../runs/train-cls': cannot contain characters '/,\\\\,#,?,%,:', found '/'",
    "author": "Cong-Wan",
    "state": "closed",
    "created_at": "2023-02-09T10:37:37Z",
    "updated_at": "2025-02-07T11:39:48Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nTraining\n\n### Bug\n\n`classify/train: model=/civi/5yolo-7.0/weights/yolov5m-cls.pt, data=/civi/DataSets/VehicleType_Classify, epochs=300, batch_size=128, imgsz=224, nosave=False, cache=None, device=0, workers=8, project=../runs/train-cls, name=230209_5m_VehicleType_Classify_1, exist_ok=False, pretrained=True, optimizer=SGD, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\r\ngithub: âš ï¸ YOLOv5 is out of date by 22 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\r\nYOLOv5 ðŸš€ v7.0-63-gcdd804d Python-3.8.0 torch-1.11.0 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\r\n\r\nTensorBoard: Start with 'tensorboard --logdir ../runs/train-cls', view at http://localhost:6006/\r\nwandb: ERROR Invalid project name '../runs/train-cls': cannot contain characters '/,\\\\,#,?,%,:', found '/'\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 333, in <module>\r\n    main(opt)\r\n  File \"train.py\", line 319, in main\r\n    train(opt, device)\r\n  File \"train.py\", line 72, in train\r\n    logger = GenericLogger(opt=opt, console_logger=LOGGER) if RANK in {-1, 0} else None\r\n  File \"/civi/5yolo-7.0/utils/loggers/__init__.py\", line 341, in __init__\r\n    self.wandb = wandb.init(project=web_project_name(str(opt.project)),\r\n  File \"/civi/anaconda/envs/v5/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 1129, in init\r\n    wi.setup(kwargs)\r\n  File \"/civi/anaconda/envs/v5/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 288, in setup\r\n    settings._apply_init(kwargs)\r\n  File \"/civi/anaconda/envs/v5/lib/python3.8/site-packages/wandb/sdk/wandb_settings.py\", line 1535, in _apply_init\r\n    self.update(init_settings, source=Source.INIT)\r\n  File \"/civi/anaconda/envs/v5/lib/python3.8/site-packages/wandb/sdk/wandb_settings.py\", line 1230, in update\r\n    self.__dict__[key].update(value, source)\r\n  File \"/civi/anaconda/envs/v5/lib/python3.8/site-packages/wandb/sdk/wandb_settings.py\", line 336, in update\r\n    self._value = self._validate(self._preprocess(value))\r\n  File \"/civi/anaconda/envs/v5/lib/python3.8/site-packages/wandb/sdk/wandb_settings.py\", line 297, in _validate\r\n    if not v(value):\r\n  File \"/civi/anaconda/envs/v5/lib/python3.8/site-packages/wandb/sdk/wandb_settings.py\", line 764, in _validate_project\r\n    raise UsageError(\r\nwandb.errors.UsageError: Invalid project name '../runs/train-cls': cannot contain characters '/,\\\\,#,?,%,:', found '/'\r\n`\r\nI have located a similar question in the issue, but the solution to it was not provided in that thread.\r\nI am encountering the same error as well. I have utilized the latest code and updated all required packages. Despite running the program on an Ubuntu system, the issue persists and continues to produce the same error. I have also attempted to disable the wandb component, but this has not resolved the problem.\n\n### Environment\n\nYOLOv5 ðŸš€ v7.0-63-gcdd804d Python-3.8.0 torch-1.11.0 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "Cong-Wan",
        "body": "I got the reason!\r\n\r\nopen the file\r\n>  utils/loggers/__init__.py\r\nfind this class\r\n![image](https://user-images.githubusercontent.com/71088400/219288576-e152426d-3bd8-4cdb-9228-ec9918c26804.png)\r\n\r\njust add `os.path.basename` before the `web_project_name(str(opt.project))`\r\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello, this issue has been automatically marked as stale because it has not had recent activity. Please note it will be closed if no further activity occurs.\n\nAccess additional [YOLOv5](https://ultralytics.com/yolov5) ðŸš€ resources:\n- **Wiki** â€“ https://docs.ultralytics.com/yolov5\n- **Tutorials** â€“ https://docs.ultralytics.com/yolov5\n- **Docs** â€“ https://docs.ultralytics.com\n\nAccess additional [Ultralytics](https://ultralytics.com) âš¡ resources:\n- **Ultralytics HUB** â€“ https://ultralytics.com/hub\n- **Vision API** â€“ https://ultralytics.com/yolov5\n- **About Us** â€“ https://ultralytics.com/about\n- **Join Our Team** â€“ https://ultralytics.com/work\n- **Contact Us** â€“ https://ultralytics.com/contact\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLOv5 ðŸš€ and Vision AI â­!"
      },
      {
        "user": "amadeok",
        "body": "it seems it's been updated , now file is C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\ultralytics\\utils\\callbacks\\wb.py, should not pass a folder path to the wandb project parameter, this works:\r\n```\r\ndef on_pretrain_routine_start(trainer):\r\n    import os\r\n    \"\"\"Initiate and start project if module is present.\"\"\"\r\n    wb.run or wb.init(project=os.path.basename(trainer.args.project) if trainer.args.project else 'YOLOv8', name=trainer.args.name, config=vars(trainer.args))\r\n\r\n```"
      }
    ]
  },
  {
    "issue_number": 13502,
    "title": "Label detection",
    "author": "Uddeshya1052",
    "state": "open",
    "created_at": "2025-01-30T13:57:20Z",
    "updated_at": "2025-02-06T23:01:24Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n\nI am using YOLO to detect labels and then extract the text within the detected regions. However, Iâ€™m facing an issue with background color variations. If the background color of the label changes, the model struggles to detect it. I donâ€™t have enough images with different background colors to train the model.\n\nWould it be a good approach to train the model using grayscale images to generalize for any background color? Or are there alternative techniques or preprocessing steps that could help improve detection robustness in this scenario? Any suggestions or ideas would be greatly appreciated.\n Thank you!\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Uddeshya1052, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a custom training â“ Question, your approach to generalizing detection (e.g., by experimenting with grayscale images or other preprocessing steps) is valid and worth investigating. However, to provide more targeted assistance, please share more details about your dataset, training setup, and any preprocessing techniques you've already tried. Additionally, verify you're considering our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/). \n\nFor now, here are a few suggestions to improve robustness:\n1. **Augmentation Techniques**: YOLOv5 already offers powerful augmentation options out of the box. Ensure you are leveraging augmentations like `hsv_h`, `hsv_s`, and `hsv_v` for color variance. You can modify these in the training configuration.\n2. **Dataset Expansion Ideas**: You might generate synthetic images with varied backgrounds using tools like [Albumentations](https://albumentations.ai/) or Photoshop. Adding diverse data can greatly improve generalization.\n3. **Grayscale Approach**: Converting your dataset to grayscale before training could reduce dependency on color features. Experimenting here could be insightful. \n   \n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response, but no worries ðŸ˜Šâ€”an Ultralytics engineer will assist you further as soon as possible!"
      },
      {
        "user": "pderrenger",
        "body": "@Uddeshya1052 for improved robustness against background variations in YOLOv5, we recommend:  \n1. Leveraging YOLOv5's built-in augmentations (`--hsv_h`, `--hsv_s`, `--hsv_v` in train.py) to simulate color variations  \n2. Adding background images (0-10% of dataset) per our [training tips guide](https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/#background-images)  \n3. Generating synthetic training data with varied backgrounds using tools like Photoshop/Python  \n\nGrayscale conversion alone typically isn't sufficient. Focus on data diversity through augmentation. If you need more specific guidance, please share your dataset statistics and example training mosaics from `runs/train/exp/train_batch*.jpg`."
      },
      {
        "user": "Uddeshya1052",
        "body": "@pderrenger Thank you for your suggestion. I tried these settings, but unfortunately, the performance decreased. I don't have a dataset with different colors. How can I still solve this issue? My application is equipment labeling detection, where labels are pasted on devices with different background colors."
      }
    ]
  },
  {
    "issue_number": 12328,
    "title": "Issue Encountered: SSL Certificate Verification Error During YOLOv5 Training",
    "author": "siddharthlanke",
    "state": "closed",
    "created_at": "2023-11-06T07:12:35Z",
    "updated_at": "2025-02-06T18:38:02Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am a user of the YOLOv5 implementation by Ultralytics and I've encountered an SSL certificate verification error during the training process. The issue seems to be related to the download of a required font file.\r\n\r\nError Description:\r\nUpon initiating the YOLOv5 training process, the script attempts to download a font file. However, the SSL certificate verification fails due to an expired certificate on the server. Consequently, the training process is interrupted.\r\n\r\nError Message:\r\n[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1002)\r\n\r\nI kindly request your assistance in rectifying this SSL certificate issue to enable a smooth training process. This appears to be a server-side SSL certificate problem impacting the download of necessary files.\r\n\r\nThank you for your attention to this matter. Please let me know if there's any additional information or logs required from my end to assist in resolving this issue.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @siddharthlanke, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@siddharthlanke hi there! Thank you for reaching out to us regarding the SSL certificate verification error during YOLOv5 training.\n\nWe apologize for the inconvenience you are facing. This SSL certificate issue is related to the download of a font file required by the YOLOv5 training process. It seems that the certificate on the server has expired, causing the verification failure.\n\nTo rectify this, we will update the SSL certificate on the server as soon as possible to ensure a smooth training process for all users. We appreciate your patience while we work on resolving this issue.\n\nIf you have any additional information or logs that could assist us in addressing this problem, please feel free to share them with us. We will do our best to resolve this issue promptly.\n\nThank you for bringing this to our attention, and for your understanding. The YOLOv5 community and the Ultralytics team appreciate your support as we strive to provide the best user experience possible.\n\nLet us know if you have any further questions or concerns.\n\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      }
    ]
  },
  {
    "issue_number": 980,
    "title": "Guidance regarding transfer learning with COCO to add additional classes",
    "author": "shayanalibhatti",
    "state": "closed",
    "created_at": "2020-09-16T11:31:54Z",
    "updated_at": "2025-02-01T15:36:47Z",
    "labels": [
      "Stale"
    ],
    "body": "Hi, I have a question regarding transfer learning. I want to use COCO pretrained weights and add another class of my own. For this, I added the class, made sure .yaml file contains that class and labeling is also done accordingly so that the newly added class has index 80 as 0-79 are for COCO. In total there are 81 classes.\r\n\r\nNow the issue is that how to do transfer learning such that previous learned COCO representations remain retained ? In the following link Mr. Jocher strongly advised not to freeze layers with YOLOv3 and hence --transfer argument was removed.\r\nhttps://github.com/ultralytics/yolov3/issues/106#issuecomment-600834027\r\n\r\nBut if layers are not frozen, then wouldnt new class learning override weights of previous COCO learned weights to learn new class ? I have used the pretrained yolov5x.pt weights, with training data containing only new class. but results are not good as it does not recognize COCO's person class. It seems that the model in this case just learns new class and forgets previous learning. \r\n\r\nPlease guide if anyone has done transfer learning successfully with YOLOv5, as in they fine tuned model so that it recognized COCO classes and their newly added class. If you can tell how you did it, that would be great help.",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "This is very easy. You simply append your new dataset, labelled with class 80 onward, to coco.yaml `train:`, and train normally:\r\n`python train.py --data coco.yaml`\r\n\r\nhttps://github.com/ultralytics/yolov5/blob/5a9c5c1d3b02c8796e6af153d57af5821d8db0d6/data/coco.yaml#L12-L13"
      },
      {
        "user": "daynial132",
        "body": "Thank you for the answer.\r\n\r\nit will need to train the whole dataset again with 80 coco class already trained, and 1 new class.\r\nit will take a lot of time as well as lot of computation. \r\nIs there a way by just adding 1 class? not tainting all 80 + 1 class again\r\n"
      },
      {
        "user": "shayanalibhatti",
        "body": "Exactly, retraining dataset is not desired as it can take weeks. Here is what I have tried. I created dataset based on new class that also contained COCO objects. Then I appended new class to COCO classes and made sure all labels are fine. \r\n\r\nThen I froze the layers except for Detect head to fine tune it to new class using the following:\r\n\r\nhttps://gist.github.com/shayanalibhatti/7955bc7a09cbfcd67843f7159d1d8924\r\n\r\nBut I am not seeing any good results .... on new class or COCO classes."
      }
    ]
  },
  {
    "issue_number": 12548,
    "title": "AssertionError: train: No labels found in /content/drive/MyDrive/Colab/Fishai-10/train/labels.cache, can not start training.",
    "author": "lpkpaco",
    "state": "closed",
    "created_at": "2023-12-23T14:31:45Z",
    "updated_at": "2025-01-29T15:06:34Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am currently using Google Colab and Roboflow to generate and train my dataset. However, I get this error message everytime when I start training.\r\nI have looked up for solutions, like using !chmod to grant access, maintaining the directory structure etc. These methods did not work.\r\nWhat should I do?\r\n![image](https://github.com/ultralytics/yolov5/assets/50955568/b54af7bd-29df-4740-a4c0-860183e2cc18)\r\n\r\nHere is the code that I used:\r\n`!chmod +w /content/drive/MyDrive/Colab/Fishai-10/train/labels\r\n !chmod +w /content/drive/MyDrive/Colab/Fishai-10/valid/labels\r\n !python /content/drive/MyDrive/Colab/yolov5/train.py --batch 100 --epochs 2000 --img 640 --data \r\n '/content/drive/MyDrive/Colab/Fishai-10/data.yaml' --weights yolov5s.pt --patience 100\r\n from google.colab import runtime\r\n runtime.unassign()`\r\n\r\nAnd I get:\r\n[error_message.txt](https://github.com/ultralytics/yolov5/files/13759355/error_message.txt)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @lpkpaco, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@lpkpaco hey there! It seems like the training process is unable to find any labels in the specified location. Here's what you can do to troubleshoot.\n\nFirst, ensure that your label files are correctly located in the `/content/drive/MyDrive/Colab/Fishai-10/train/labels.cache` directory.\n\nNext, check that the label files are in the correct format and contain the expected annotations.\n\nFinally, make sure that the path to the label files is correctly specified in the data.yaml file.\n\nLet me know if you need more help with this!"
      },
      {
        "user": "lpkpaco",
        "body": "This is my data.yaml:\r\n![image](https://github.com/ultralytics/yolov5/assets/50955568/457fb66b-e517-4597-8a56-54d76777ef29)\r\n\r\nI think it is correct.\r\nFor more information, here are a few images for training and their label txt files:\r\n![Photo-2023-12-21-16-51-58_jpg rf 6ecd133bbbfd1eb26fcfd274c976a19e](https://github.com/ultralytics/yolov5/assets/50955568/ec4f7422-6975-439e-b672-daac19a59761)\r\n![IMG_6924_JPG rf 523331e9a276657f2fe07e2318ffaf25](https://github.com/ultralytics/yolov5/assets/50955568/d3c624c3-3bfb-43e6-b6a5-807ede0b66f7)\r\n![Photo-2023-12-18-16-14-43_jpg rf 9ecfc7daa2f5abe2b679cd0aea08b484](https://github.com/ultralytics/yolov5/assets/50955568/fd7f2a24-255f-4149-b442-37e96af322cb)\r\n[Photo-2023-12-21-16-51-58_jpg.rf.6ecd133bbbfd1eb26fcfd274c976a19e.txt](https://github.com/ultralytics/yolov5/files/13760753/Photo-2023-12-21-16-51-58_jpg.rf.6ecd133bbbfd1eb26fcfd274c976a19e.txt)\r\n[IMG_6924_JPG.rf.523331e9a276657f2fe07e2318ffaf25.txt](https://github.com/ultralytics/yolov5/files/13760754/IMG_6924_JPG.rf.523331e9a276657f2fe07e2318ffaf25.txt)\r\n[Photo-2023-12-18-16-14-43_jpg.rf.9ecfc7daa2f5abe2b679cd0aea08b484.txt](https://github.com/ultralytics/yolov5/files/13760756/Photo-2023-12-18-16-14-43_jpg.rf.9ecfc7daa2f5abe2b679cd0aea08b484.txt)\r\n\r\nAnd here is the labels.cache file (I couldn't drop it here in GitHub as the file type is not supported):\r\n[https://drive.google.com/file/d/1rZnvHzgIDU3V6UKioMNMido-vuEelXMx/view?usp=sharing](labels.cache)\r\n\r\nI don't know what's going wrong.\r\nThanks for your help.\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 13457,
    "title": "Example images always inferred when running detect.py - how can I stop this? ",
    "author": "essair",
    "state": "open",
    "created_at": "2024-12-12T11:27:56Z",
    "updated_at": "2025-01-29T02:43:20Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nDetection\n\n### Bug\n\nThis is the output I get when running detect.py - each single time it runs the example images with the default model and then infers on my images with my model!\r\n\r\n\r\ndetect: weights=../../yolov5/yolov5s.pt, source=../../yolov5/data/images, data=../../yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=../../yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\r\nYOLOv5 ðŸš€ v7.0-389-ge62a31b6 Python-3.8.10 torch-2.4.1+cu121 CPU\r\n\r\nFusing layers... \r\nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\r\nimage 1/2 /home/name/yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 37.6ms\r\nimage 2/2 /home/name/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 32.2ms\r\nSpeed: 0.2ms pre-process, 34.9ms inference, 0.9ms NMS per image at shape (1, 3, 640, 640)\r\nResults saved to ../../yolov5/runs/detect/exp418\r\nYOLOv5 ðŸš€ v7.0-389-ge62a31b6 Python-3.8.10 torch-2.4.1+cu121 CPU\r\n\r\nFusing layers... \r\nModel summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\r\nimage 1/2 /dev/shm/2024-12-09/0299/3f2a3b04-70d0-4c88-a17d-9dec8fbfe9fb_00000000f6acf5d2_Z2EnnHkxxRg_day_2024-12-09T120039.jpg: 384x640 (no detections), 34.0ms\r\nimage 2/2 /dev/shm/2024-12-09/0299/9425430e-21f5-435c-a613-1dc4d0d60585_00000000f6acf5d2_G1yv3N-8gWM_night_2024-12-09T000035.jpg: 384x640 (no detections), 30.6ms\n\n### Environment\n\nYOLOv5 ðŸš€ v7.0-389-ge62a31b6 Python-3.8.10 torch-2.4.1+cu121 CPU\r\nOS: Pop OS 20.04\n\n### Minimal Reproducible Example\n\nI'm running detect.py as part of a pipeline processing many images. The pipeline is in R, here is where I call detect.py:\r\n\r\nreticulate::use_virtualenv(virtualenv = file.path(user_path, \"yolov5/venv\"), required=TRUE)\r\n\r\nreticulate::source_python(file.path(user_path, \"yolov5/detect.py\"))\r\nrun(source = image_folder, weights = model,\r\n    save_csv = TRUE, save_conf = TRUE, save_crop = TRUE, save_txt = TRUE,\r\n    exist_ok = FALSE, name= results_temp_folder, nosave = nosave_settings)\r\n\r\n\r\nEvery single time I run this line for each batch of images - thats several thousand times! - it first runs the detection on the example images, saving the results to the nth run in the yolov5 folder and then it runs on my images. Its costing me time and space on my computer! And I would be very grateful if someone can help me to prevent it from running!\n\n### Additional\n\nI've tried running my script on another computer with a more up to date OS: Ubuntu 24 and Python 3.12 the exact same things happens! \n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "pderrenger",
        "body": "ðŸ‘‹ Hello @essair, thank you for your interest in YOLOv5 ðŸš€! Please make sure you have reviewed the YOLOv5 documentation and guides to help troubleshoot. They include helpful resources for topics ranging from custom data training to model debugging.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it. This will help us investigate the issue effectively.\n\nIf this is a custom training â“ Question, please include as much information as possible, such as your dataset structure, image examples, and any training logs. Make sure you are following the best practices for achieving optimal training results.\n\n## Requirements\n\nEnsure you are using **Python>=3.8.0** with all dependencies installed, including **PyTorch>=1.8**. You can set up YOLOv5 locally by cloning the repository, navigating to the folder, and installing the required dependencies.\n\n## Environments\n\nYOLOv5 can run in various verified environments, including cloud notebooks with free GPUs, Google Cloud Deep Learning VMs, AWS Deep Learning AMIs, and Docker images with all dependencies preinstalled. Make sure your selected environment is up to date.\n\n## Status\n\nVerify the status of YOLOv5 GitHub Actions Continuous Integration (CI) tests for the latest functionality. These tests verify proper model operation across various tasks such as training, validation, inference, and export.\n\nThis is an automated response to ensure you get help as quickly as possible. An Ultralytics engineer will respond soon to provide additional assistance. Thank you for your patience! ðŸ˜Š"
      },
      {
        "user": "essair",
        "body": "Does anyone have ideas how to solve this problem? Its slowing my work down and I'd be really grateful if someone has advice on how to stop it!"
      },
      {
        "user": "pderrenger",
        "body": "@essair it seems like your `detect.py` script is being executed twice per run, likely due to how it's being called in your pipeline. To avoid the example images being processed, ensure your pipeline calls `run()` only once and verify that no default arguments (like `source=../../yolov5/data/images`) are unintentionally triggering inference on the example images. Additionally, check if your R script or `reticulate` might be reinitializing the script unintentionally. If the issue persists, consider running `detect.py` manually with your desired arguments to confirm the behavior outside the R environment."
      }
    ]
  },
  {
    "issue_number": 13500,
    "title": "Error in loading yolov5 in local windows 11",
    "author": "pratyooshraj",
    "state": "closed",
    "created_at": "2025-01-27T10:20:17Z",
    "updated_at": "2025-01-28T16:54:13Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nDetection\n\n### Bug\n\nCurrently running windows 11 and python 3.11. I trained my custom model using yolov5 using my custom data set in google colab. The model is used to detect sign language vowels.\n`!python train.py --img 416 --batch 16 --epochs 10 --data '/content/YOLO_vowels/data.yaml' --cfg ./models/custom_yolov5s.yaml --weights 'yolov5s.pt' --name yolov5s_vowels_results  --cache disk --workers 4`\nThe resulting best.pt in yolov5s_vowels_results i have downloaded and renamed. But an error occurs when i run the model in my device. I also tried running the pretrained yolov5s.pt model in my local device, which runs properly. Could you help me with the error.\n\nCode\n```\nimport torch\nimport os\n\nprint(\"Number of GPU: \", torch.cuda.device_count())\nprint(\"GPU Name: \", torch.cuda.get_device_name())\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"D:/Programming/cuda_test/yolov5/vowels_only_5epochs.pt\" ,force_reload=True)  \n```\nError\n\n```\nPS D:\\Programming\\cuda_test> python test1.py\nNumber of GPU:  1\nGPU Name:  NVIDIA GeForce GTX 1650\nUsing device: cuda\nDownloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\ACER/.cache\\torch\\hub\\master.zip\nYOLOv5  2025-1-27 Python-3.11.4 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n```\n---success in pretrained model\n\n```\nFusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\nAdding AutoShape... \nDownloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\ACER/.cache\\torch\\hub\\master.zip\nYOLOv5  2025-1-27 Python-3.11.4 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n```\n---Error in running custom model\n\n```\nTraceback (most recent call last):\n  File \"C:\\Users\\ACER/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py\", line 70, in _create\n    model = DetectMultiBackend(path, device=device, fuse=autoshape)  # detection model\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ACER/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py\", line 489, in __init__\n    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ACER/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\experimental.py\", line 98, in attempt_load\n    ckpt = torch.load(attempt_download(w), map_location=\"cpu\")  # load\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Programming\\cuda_test\\.venv\\Lib\\site-packages\\ultralytics\\utils\\patches.py\", line 86, in torch_load\n    return _torch_load(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Programming\\cuda_test\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 1360, in load\n    return _load(\n           ^^^^^^\n  File \"D:\\Programming\\cuda_test\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 1848, in _load\n    result = unpickler.load()\n             ^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Python311\\Lib\\pathlib.py\", line 873, in __new__\n    raise NotImplementedError(\"cannot instantiate %r on your system\"\nNotImplementedError: cannot instantiate 'PosixPath' on your system\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\ACER/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py\", line 85, in _create\n    model = attempt_load(path, device=device, fuse=False)  # arbitrary model\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ACER/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\experimental.py\", line 98, in attempt_load\n    ckpt = torch.load(attempt_download(w), map_location=\"cpu\")  # load\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Programming\\cuda_test\\.venv\\Lib\\site-packages\\ultralytics\\utils\\patches.py\", line 86, in torch_load\n    return _torch_load(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Programming\\cuda_test\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 1360, in load\n    return _load(\n           ^^^^^^\n  File \"D:\\Programming\\cuda_test\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 1848, in _load\n    result = unpickler.load()\n             ^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Python311\\Lib\\pathlib.py\", line 873, in __new__\n    raise NotImplementedError(\"cannot instantiate %r on your system\"\nNotImplementedError: cannot instantiate 'PosixPath' on your system\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Programming\\cuda_test\\test1.py\", line 14, in <module>\n    model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"D:/Programming/cuda_test/yolov5/vowels_only_5epochs.pt\" ,force_reload=True)  # local model\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Programming\\cuda_test\\.venv\\Lib\\site-packages\\torch\\hub.py\", line 647, in load\n    model = _load_local(repo_or_dir, model, *args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Programming\\cuda_test\\.venv\\Lib\\site-packages\\torch\\hub.py\", line 676, in _load_local\n    model = entry(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ACER/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py\", line 135, in custom\n    return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ACER/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py\", line 103, in _create\n    raise Exception(s) from e\nException: cannot instantiate 'PosixPath' on your system. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.\n```\n\nI tried to run my custom trained model for computer vision, trained in google colab and downloaded in windows 11. Instead of running an error occurs. However in google colab, correct detection and testing images were shown.\n\n### Environment\n\n-YOLO -v5 ultralytics==8.3.68, torch==2.5.1+cu124\n-OS: windows 11\n\n### Minimal Reproducible Example\n\n```\nimport torch\nimport os\n\nprint(\"Number of GPU: \", torch.cuda.device_count())\nprint(\"GPU Name: \", torch.cuda.get_device_name())\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"D:/Programming/cuda_test/yolov5/vowels_only_5epochs.pt\" ,force_reload=True)  \n```\n\n### Additional\n\nis it due to the ultralytics version. How can i downgrade to yolov5\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @pratyooshraj, thank you for your interest in YOLOv5 ðŸš€! \n\nYour error seems related to loading your custom-trained model on your local setup. For a detailed diagnosis and resolution, we'll need some additional information. In the meantime, here are a few steps to help troubleshoot and debug:\n\n---\n\n### Debug Checklist ðŸ›\n1. **Ensure Compatibility**:\n   - Verify that your **model file (`vowels_only_5epochs.pt`) is not corrupted** and was downloaded properly.\n   - Confirm you are using compatible versions of Python, PyTorch, and YOLOv5. Based on your description, your local setup uses `torch 2.5.1`, which might not be fully compatible. YOLOv5 supports [**PyTorch>=1.8**](https://pytorch.org/) for stable operation.\n\n2. **Force Reload**:\n   - Clear your `.cache` directory where the repo is stored by deleting `C:\\Users\\ACER\\.cache\\torch\\hub\\` and running the script again with `force_reload=True` as you've already specified.\n\n3. **Pathlib Error**:\n   - The error `cannot instantiate 'PosixPath' on your system` usually arises because Python's `pathlib` behaves differently across platforms (Windows vs Unix). This might indicate compatibility or serialization issues in the `vowels_only_5epochs.pt`. Re-save the model with the current YOLOv5 version in Colab and retry.\n\n---\n\n### Instructions âœ…\n\nFor additional assistance, please:\n- Provide the **exact YOLOv5 version** you used in Google Colab to train your model.\n- Share a **Minimum Reproducible Example (MRE)** that includes the Colab training pipeline and your local system pipeline with as much detail as possible.\n- Verify you've followed the official [PyTorch Hub Model Loading guide](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading) and ensure there are no discrepancies when loading pretrained/custom models.\n  \n---\n\n### Requirements ðŸ’¡\n\nEnsure all requirements are met by running the following setup commands for YOLOv5 locally:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone YOLOv5 repository\ncd yolov5\npip install -r requirements.txt  # install requirements\n``` \n\nFor safe compatibility, consider running YOLOv5 in one of our verified environments. These include:\n- **Notebooks** (free GPU options): <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n- **Docker** for isolated environments: See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/).\n\n---\n\n### Next Steps ðŸ”\nIf you're unable to resolve this, an Ultralytics engineer will assist you further soon. In the meantime:\n- Double-check your **model path, environment setup**, and **dependencies**.  \n- Share additional logs or information that might help reproduce the issue on our end.\n\nThank you for bringing this to our attention! ðŸ˜Š"
      },
      {
        "user": "pratyooshraj",
        "body": "used this command to get the YOLOversion. Trained the model on 25th jan 2025\n!git clone https://github.com/ultralytics/yolov5.git\n\nTraining command\n`!python train.py --img 416 --batch 16 --epochs 10 --data '/content/YOLO_vowels/data.yaml' --cfg ./models/custom_yolov5s.yaml --weights 'yolov5s.pt' --name yolov5s_vowels_results  --cache disk --workers 4`\n\nMRE\n```\nimport torch\n\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"D:\\Programming\\cuda_test\\yolov5\\vowels_only_5epochs.pt\", force_reload=True) \n```\n"
      },
      {
        "user": "pderrenger",
        "body": "The error likely stems from incompatibilities between the YOLOv5 version you trained your model with and the version you're using to load it. Since you cloned the YOLOv5 repo on January 25, 2025, it may include changes that differ from the version installed via PyTorch Hub. Ensure the versions match by using the same codebase for training and inference. \n\nInstead of PyTorch Hub, try running your script directly from the cloned YOLOv5 directory:\n\n```python\nfrom models.common import DetectMultiBackend\n\nmodel = DetectMultiBackend(\"D:/Programming/cuda_test/yolov5/vowels_only_5epochs.pt\", device='cuda')\n```\n\nAlternatively, ensure the latest YOLOv5 repo is cloned and retry. More details on custom model loading are available [here](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/)."
      }
    ]
  },
  {
    "issue_number": 3656,
    "title": "Label corrupted. I can't solved.",
    "author": "SpongeBab",
    "state": "closed",
    "created_at": "2021-06-17T08:16:27Z",
    "updated_at": "2025-01-27T15:35:32Z",
    "labels": [
      "question"
    ],
    "body": "## â”Question\r\nWhy is my label corrupted? I can make sure that my picture is in good condition, and I have generated the TXT label correctly.\r\n\r\nI have successfully trained my own dataset before. But today when I was generating the label as usual and training another new data set, there were a lot of label that were corrupted. I checked the label carefully.\r\n\r\nCan you give me some help?\r\n\r\nI looked in the `datasets.py`, like` def verify_image_label(args)`. But I can't find the mistake.\r\n\r\nAnd the ap is 0....\r\n## Additional context\r\n```\r\ntrain: Scanning 'E:\\qxdetection\\train' images and labels...4300 found, 0 missing, 0 empty, 1408 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4300/4300 [00:31<00:00, 137.61it/s]\r\ntrain: New cache created: E:\\qxdetection\\train.cache\r\nval: Scanning 'E:\\qxdetection\\valid' images and labels...343 found, 0 missing, 0 empty, 124 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 343/343 [00:10<00:00, 33.86it/s] \r\nval: New cache created: E:\\qxdetection\\valid.cache\r\nPlotting labels... \r\n\r\nautoanchor: Analyzing anchors... anchors/target = 4.12, Best Possible Recall (BPR) = 0.9910\r\nImage sizes 1280 train, 1280 test\r\nUsing 0 dataloader workers\r\nLogging results to runs\\train\\exp7\r\nStarting training for 200 epochs...\r\n\r\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\r\n     0/199     6.19G   0.08685   0.03775   0.04768    0.1723         1      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2892/2892 [40:56<00:00,  1.18it/s]\r\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [00:43<00:00,  2.50it/s]\r\n                 all        219          0          0          0          0          0\r\n```",
    "comments": [
      {
        "user": "SpongeBab",
        "body": "![image](https://user-images.githubusercontent.com/65898238/122363740-c48e5c80-cf8b-11eb-94dd-33b08cfafa3d.png)\r\noh, I think there is something wrong with the way I divide the data set. There is a class of pictures that are not found in the training set but only in the valid set, so AP cannot be calculated. \r\nBut I still haven't found the cause of the label corrupted.\r\n@glenn-jocher "
      },
      {
        "user": "glenn-jocher",
        "body": "@SpongeBab if you delete your *.cache files in your dataset it will recache and display the cause of all corrupted images and/or labels."
      },
      {
        "user": "SpongeBab",
        "body": "@glenn-jocher \r\nI have deleted the train and valid cache.And it show this again:\r\n```\r\ntrain: Scanning 'E:\\qxdetection\\train' images and labels...4300 found, 0 missing, 0 empty, 1408 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4300/4300 [00:31<00:00, 137.61it/s]\r\ntrain: New cache created: E:\\qxdetection\\train.cache\r\nval: Scanning 'E:\\qxdetection\\valid' images and labels...343 found, 0 missing, 0 empty, 124 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 343/343 [00:10<00:00, 33.86it/s] \r\nval: New cache created: E:\\qxdetection\\valid.cache\r\n```\r\nI wonder how it can judge if a image is corrupted? I looked at the code, but I couldn't find anything to judge by. Can you tell me? Or give me some guide.Thank you very much~"
      }
    ]
  },
  {
    "issue_number": 13499,
    "title": "Can't resize annotation text size and color of text",
    "author": "MakMoinee",
    "state": "open",
    "created_at": "2025-01-23T16:36:39Z",
    "updated_at": "2025-01-24T04:01:38Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi, I'm having trouble with the annotation. please provide me fix or code to make this code below work:. how did you guys work it out? \n\nmodel = torch.hub.load('ultralytics/yolov5', 'custom', path='./result.pt', verbose=False)\nresults = model(image)\n\nThis is the only implementation it works so if you could find workaround please help me\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @MakMoinee, thank you for your interest in YOLOv5 ðŸš€!  \n\nPlease visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).  \n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it. If you're looking to modify inference display properties like annotation text size or color, this may require some adjustments in the code outputs, and further detail on your implementation will help us guide you more effectively.  \n\nIf this is a custom training â“ Question, please provide as much information as possible, such as:\n- Code snippet of your attempt\n- Dataset image examples (if relevant)\n- Any logs or error messages encountered  \nAlso, verify you're following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).  \n\n## Requirements  \n\nEnsure your environment meets the following minimum requirements:  \n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:  \n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone  \ncd yolov5  \npip install -r requirements.txt  # install  \n```  \n\n## Environments  \n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies like [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):  \n\n- **Notebooks** with free GPU:  \n  <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a>  \n  <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>  \n  <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>  \n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)  \n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)  \n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)  \n  <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>  \n\n## Status  \n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>  \n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.  \n\n---\n\nThis is an automated response ðŸ¤–. An Ultralytics engineer will review your query and provide additional support as soon as possible ðŸ™‚"
      },
      {
        "user": "pderrenger",
        "body": "@MakMoinee to modify the annotation text size and color, you can use the `.plot()` method from the `results` object and customize parameters like `font_size` and `font`. For example:\n\n```python\nresults = model(image)\nfor result in results:\n    annotated_image = result.plot(font_size=20, font='Arial.ttf')  # Adjust font size and type\n```\n\nIf you're working with bounding box annotations, the `Annotator` class also allows direct control over text and box attributes. For additional details, refer to the [results.plot documentation](https://docs.ultralytics.com/reference/engine/results/#plot) or the [annotator utilities](https://docs.ultralytics.com/usage/simple-utilities/).\n\nLet us know if you need further clarification!"
      }
    ]
  },
  {
    "issue_number": 13498,
    "title": "freetype font and pillow",
    "author": "vahidajalluian",
    "state": "open",
    "created_at": "2025-01-23T02:40:46Z",
    "updated_at": "2025-01-23T19:35:22Z",
    "labels": [
      "bug",
      "dependencies"
    ],
    "body": "It seems in Pillow library version over 9.5 getsize method has been removed. So either change the method or limit the library to be installed to 9.5.\nHere is the log I get for now.\n \n```\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/vahidajalluian/yolov5-7.0/utils/plots.py\", line 305, in plot_images\n    annotator.box_label(box, label, color=color)\n  File \"/home/vahidajalluian/yolov5-7.0/utils/plots.py\", line 91, in box_label\n    w, h = self.font.getsize(label)  # text width, height\nAttributeError: 'FreeTypeFont' object has no attribute 'getsize'\nException in thread Thread-6 (plot_images):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/vahidajalluian/yolov5-7.0/utils/plots.py\", line 305, in plot_images\n    annotator.box_label(box, label, color=color)\n  File \"/home/vahidajalluian/yolov5-7.0/utils/plots.py\", line 91, in box_label\n    w, h = self.font.getsize(label)  # text width, height\nAttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n```\n",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @vahidajalluian, thank you for bringing up this issue and for your interest in YOLOv5 ðŸš€! It seems youâ€™ve encountered a compatibility issue related to the `getsize` method in the Pillow library that's affecting the FreeTypeFont class.\n\n### Next Steps:\nIf this is a ðŸ› Bug Report, could you please provide a **minimum reproducible example (MRE)** that demonstrates this issue? This will help us reproduce and debug the problem more effectively. The MRE should include:\n- The YOLOv5 version (commit hash, if possible) you are using.\n- The exact steps/commands you ran leading up to the error.\n- Your current environment details â€” OS, Python version, and dependencies (e.g., `pip freeze` output).\n- Any additional context that might help us investigate.\n\n### Temporary Workaround:\nUntil this issue is resolved, you may try using a specific version of Pillow (<= 9.5.0) as a workaround:\n```bash\npip install Pillow==9.5.0\n```\n\n### Requirements\nEnsure your Python and dependencies match our [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt):\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n### Environments\nYOLOv5 supports running in various environments â€” you can try switching environments as an alternative:\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud**: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon**: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n### Source Code Check\nTo troubleshoot further, you may also review the affected file and line of code where the error is reported:\n```python\nFile \"/home/vahidajalluian/yolov5-7.0/utils/plots.py\", line 91\n```\nYou can compare this to the latest version of YOLOv5's [utils/plots.py](https://github.com/ultralytics/yolov5/blob/master/utils/plots.py) to see if updates or fixes are already available.\n\n---\n\nThis is an automated response to help you get started ðŸ˜Š! An Ultralytics engineer will review your issue and provide further assistance as soon as possible. Thank you for your patience as we work together to ensure a smooth experience with YOLOv5! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@vahidajalluian thank you for bringing this to our attention! The issue arises because Pillow versions above 9.5 have removed the `getsize` method from the `FreeTypeFont` class, replacing it with `getbbox`. To resolve this, you can either:\n\n1. Downgrade Pillow to version 9.5 using `pip install pillow==9.5.0`.\n2. Update the affected part of the code to use `getbbox` instead of `getsize`. For example:\n\n```python\nw, h = self.font.getbbox(label)[2:]  # text width, height\n```\n\nPlease ensure you are using the latest YOLOv5 version as well to avoid other compatibility issues. Let us know if you need further assistance!"
      }
    ]
  },
  {
    "issue_number": 11890,
    "title": "Image not found yolo v8l",
    "author": "puneet2059",
    "state": "closed",
    "created_at": "2023-07-22T13:46:19Z",
    "updated_at": "2025-01-22T13:28:13Z",
    "labels": [
      "Stale"
    ],
    "body": "I am training a yolo v8l model with input image 512*512 (3400 images) .\r\nModel will run for 35 epochs and then show \r\n\r\n\r\n\r\nAssertionError                            Traceback (most recent call last)\r\n[<ipython-input-2-3f8ccf2d3a02>](https://localhost:8080/#) in <cell line: 3>()\r\n      1 from ultralytics import YOLO\r\n      2 model=YOLO(\"yolov8l.yaml\")\r\n----> 3 results=model.train(data=\"/content/drive/MyDrive/Colab Notebooks/yolo/data.yaml\",epochs=50,batch=2)\r\n\r\n7 frames\r\n[/usr/local/lib/python3.10/dist-packages/torch/_utils.py](https://localhost:8080/#) in reraise(self)\r\n    642             # instantiate since we don't know how to\r\n    643             raise RuntimeError(msg) from None\r\n--> 644         raise exception\r\n    645 \r\n    646 \r\n\r\nAssertionError: Caught AssertionError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/data/base.py\", line 180, in __getitem__\r\n    return self.transforms(self.get_label_info(index))\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/data/base.py\", line 184, in get_label_info\r\n    label[\"img\"], label[\"ori_shape\"], label[\"resized_shape\"] = self.load_image(index)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/data/base.py\", line 123, in load_image\r\n    assert im is not None, f\"Image Not Found {f}\"\r\nAssertionError: Image Not Found /content/drive/MyDrive/Colab Notebooks/yolo/train/images/sub_image_1_0_IMG_0393.JPG\r\n\r\n\r\nThis error keeps changing with different image names, but when I check my images and label folder, everything is there. I referred to previously given solutions like removing the cache file if present and checking the presence of image and the right format.\r\nBut everything looks pefect.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @puneet2059, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@puneet2059 make sure to verify that the image paths in your dataset configuration file (`data.yaml`) are correct. The error message suggests that the DataLoader is unable to find the image at the specified path (`/content/drive/MyDrive/Colab Notebooks/yolo/train/images/sub_image_1_0_IMG_0393.JPG`).\n\nPlease double-check the following:\n- The image file exists in the specified directory.\n- The file name is spelled correctly, including the file extension.\n- The path is correctly specified in the `data.yaml` file.\n\nIf you have confirmed that the image paths are correct, try printing out some of the image paths during data loading to see if there is anything unusual. You can add a few lines of code to YOLOv5's `load_image` function to print the image path before the assertion:\n\n```python\n...\ndef load_image(self, index):\n    image_path = self.image_files[index]  # Add this line to print the image path\n    print(f\"Loading image: {image_path}\")\n    im = ...\n    ...\n```\n\nPlease let me know if you continue to encounter issues or need any further assistance."
      },
      {
        "user": "puneet2059",
        "body": "hi I did this to check whether I have all the files or not \r\nimport os\r\n\r\ndef check_files_exist(image_folder, label_folder):\r\n    image_files = os.listdir(image_folder)\r\n    label_files = os.listdir(label_folder)\r\n    \r\n    missing_images = []\r\n    missing_labels = []\r\n    \r\n    for image_file in image_files:\r\n        image_path = os.path.join(image_folder, image_file)\r\n        label_file = image_file.replace(\".jpg\", \".txt\")\r\n        label_path = os.path.join(label_folder, label_file)\r\n        \r\n        if not os.path.exists(image_path):\r\n            missing_images.append(image_file)\r\n        \r\n        if not os.path.exists(label_path):\r\n            missing_labels.append(label_file)\r\n    \r\n    return missing_images, missing_labels\r\n\r\n# Example usage:\r\nimages_folder = \"/content/drive/MyDrive/Colab Notebooks/train/images\"\r\nlabels_folder = \"/content/drive/MyDrive/Colab Notebooks/train/labels\"\r\n\r\nmissing_images, missing_labels = check_files_exist(images_folder, labels_folder)\r\n\r\nif len(missing_images) > 0:\r\n    print(\"Missing image files:\")\r\n    for image_file in missing_images:\r\n        print(image_file)\r\n\r\nif len(missing_labels) > 0:\r\n    print(\"Missing label files:\")\r\n    for label_file in missing_labels:\r\n        print(label_file)\r\nit is showing names of the missing label with respect to images but  if I search for these files in my drive they are present there with the proper label and images,\r\n what should I do ?\r\n"
      }
    ]
  },
  {
    "issue_number": 12140,
    "title": "How to export onnx models with different input shapes?",
    "author": "xiaoche-24",
    "state": "closed",
    "created_at": "2023-09-18T06:46:39Z",
    "updated_at": "2025-01-22T07:13:14Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI have successfully obtained a six-channel input pt model. How can I convert it to an onnx model? What modifications need to be made to export.py?I ran export.py directly and reported the following error:\r\nTraceback (most recent call last):\r\nFile \"export.py\", line 653, in\r\nmain(opt)\r\nFile \"export.py\", line 648, in main\r\nrun(**vars(opt))\r\nFile \"/home/lixiaojun/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\nreturn func(*args, **kwargs)\r\nFile \"export.py\", line 549, in run\r\ny = model(im) # dry runs\r\nFile \"/home/lixiaojun/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\r\nreturn forward_call(*input, **kwargs)\r\nFile \"/home/lixiaojun/workfile/yolov5-master/models/yolo.py\", line 245, in forward\r\nreturn self._forward_once(x, profile, visualize) # single-scale inference, train\r\nFile \"/home/lixiaojun/workfile/yolov5-master/models/yolo.py\", line 121, in _forward_once\r\nx = m(x) # run\r\nFile \"/home/lixiaojun/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\r\nreturn forward_call(*input, **kwargs)\r\nFile \"/home/lixiaojun/workfile/yolov5-master/models/common.py\", line 3013, in forward_fuse\r\nreturn self.act(self.conv(x))\r\nFile \"/home/lixiaojun/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\r\nreturn forward_call(*input, **kwargs)\r\nFile \"/home/lixiaojun/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\nreturn self._conv_forward(input, self.weight, self.bias)\r\nFile \"/home/lixiaojun/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\nreturn F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: Given groups=1, weight of size [32, 3, 6, 6], expected input[1, 6, 640, 640] to have 3 channels, but got 6 channels instead\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @xiaoche-24, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@xiaoche-24 to export ONNX models with different input shapes in YOLOv5, you need to make modifications to the `export.py` script. Specifically, you need to provide the `--inp` argument to specify the desired input shape.\n\nFor example, to export an ONNX model with an input shape of 640x640, you can run the following command:\n\n```\npython export.py --weights path/to/weights.pt --img 640 --inp 640 --batch 1\n```\n\nMake sure to replace `path/to/weights.pt` with the path to your trained model weights.\n\nRegarding the error you encountered, it seems like there is a mismatch between the number of input channels in your model and the expected number of channels. Please ensure that your model and input data have consistent channel dimensions.\n\nIf you have any further questions or issues, feel free to ask."
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      }
    ]
  },
  {
    "issue_number": 13495,
    "title": "The accuracy of using Yolov5 for dust detection in ports is not high, and there will be a large number of false detections, especially at night",
    "author": "hdjhasd",
    "state": "open",
    "created_at": "2025-01-21T06:32:33Z",
    "updated_at": "2025-01-21T18:48:29Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nThe accuracy of using Yolov5 for dust detection in ports is not high, and there will be a large number of false detections, especially at night\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @hdjhasd, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for common tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced optimization techniques like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)** to help us investigate this further. This should include details such as custom dataset samples, relevant training or inference code snippets, and complete logs for better debugging.\n\nIf this is a â“ Question related to your dust detection task, please make sure to share:\n- Information about your custom dataset (e.g., size, quality, class balance)\n- Examples of false detections, particularly at night, to better understand the issue\n- Details of the model configuration or modifications you may have made\n- Your training logs and hyperparameters used (learning rate, batch size, etc.)\n- Steps you've already taken using our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To set up an environment:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in the following verified environments, all preconfigured with essential dependencies like [CUDA](https://developer.nvidia.com/cuda), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/):\n\n- **Notebooks** with free GPUs: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are passing. These tests confirm YOLOv5's functionality for [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) across various environments.\n\nThis is an automated response to assist you faster ðŸ˜Š. An Ultralytics engineer will take a look at your issue and provide additional support soon."
      },
      {
        "user": "pderrenger",
        "body": "@hdjhasd thank you for your question. For improving accuracy and reducing false detections, especially in challenging scenarios like night-time dust detection, ensure your training dataset includes diverse and representative images of the deployment environment (e.g., day and night conditions). Verify that annotations are accurate and consistent, and consider adding background images to reduce false positives. You can also experiment with hyperparameter tuning or use Test-Time Augmentation (TTA) for better results. For more guidance, refer to the [Tips for Best Training Results](https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/) documentation. Let us know if you need further clarification!"
      }
    ]
  },
  {
    "issue_number": 13473,
    "title": "Query/Issue with Custom YOLOv5 Model and ONNX Export",
    "author": "AbhirupSinha1811",
    "state": "open",
    "created_at": "2024-12-27T13:28:10Z",
    "updated_at": "2025-01-21T12:40:28Z",
    "labels": [
      "bug",
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nDetection, Export\n\n### Bug\n\nI am working with a custom-trained YOLOv5 model that was trained on a dataset with 4 classes. After exporting the model to ONNX format, I am facing discrepancies in the output tensor shape and class configurations, which are creating confusion and potential issues in downstream tasks. Below, I outline the details of my observations, potential root causes, and attempts to resolve the issue.\n\n### Environment\n\nyolov5s.pt, ubuntu 22.04, in own system.\n\n### Minimal Reproducible Example\n\nnormal detection code from\"https://github.com/arindal1/yolov5-onnx-object-recognition/blob/main/yolov5.py\"\n\n### Additional\n\nObservations:\r\n\r\nCustom Model Details:\r\n\r\nThe .pt model was trained on a dataset with 4 classes (bird, drone, helicopter, jetplane).\r\n\r\nWhen inspecting the .pt model, the number of classes is confirmed as 4 both in the names field and in the nc parameter from the data.yaml.\r\n\r\nThe .pt model performs as expected, detecting all 4 classes correctly during inference.\r\n\r\nONNX Export Details:\r\n\r\nAfter exporting the model to ONNX, the output tensor shape is reported as [1, 8, 8400].\r\n\r\nThe 8 indicates the number of output channels in the detection head, which suggests it is configured for only 3 classes (5 + 3 = 8 instead of 5 + 4 = 9).\r\n\r\nThis is inconsistent with the .pt model, which was trained on 4 classes.\r\n\r\nWhen checking the ONNX model metadata, the class names (bird, drone, helicopter, jetplane) are correctly stored, indicating 4 classes in the metadata.\r\n\r\nComparison with Default COCO Model:\r\n\r\nFor reference, the output tensor shape of a YOLOv5 model trained on the COCO dataset (80 classes) is [1, 25200, 85].\r\n\r\nHere, 85 = 5 + 80 (5 for bounding box attributes + 80 for classes).\r\n\r\nThis format aligns with the expected configuration for YOLO models.\r\n\r\nKey Issues:\r\n\r\nMismatch in Output Tensor Shape:\r\n\r\nThe ONNX modelâ€™s output tensor shape suggests it is configured for only 3 classes ([1, 8, 8400]), despite the .pt model being trained on 4 classes.\r\n\r\nThis raises concerns about whether the ONNX model will correctly detect all 4 classes.\r\n\r\nPotential Causes of the Issue:\r\n\r\nThe detection head in the .pt model might have been misconfigured during training or export.\r\n\r\nFor 4 classes, the detection headâ€™s out_channels should be 5 + 4 = 9, but it appears to be set to 8.\r\n\r\nThe ONNX export process might not be correctly handling the modelâ€™s class configuration.\r\n\r\nImplications for Object Detection:\r\n\r\nIf the ONNX model is truly configured for only 3 classes, it may fail to detect one of the classes or produce incorrect predictions.\r\n\r\nSteps Taken to Debug:\r\n\r\nInspected Detection Head of .pt Model:\r\n\r\nVerified the out_channels of the detection head (last layer).\r\n\r\nThe .pt modelâ€™s detection head is confirmed to have out_channels = 8, indicating a configuration for 3 classes.\r\n\r\nThis discrepancy persists despite the model being trained on 4 classes.\r\n\r\nVerified ONNX Model Metadata:\r\n\r\nExtracted metadata from the ONNX model, which correctly lists 4 class names (bird, drone, helicopter, jetplane).\r\n\r\nTried Re-exporting the Model:\r\n\r\nRe-exported the .pt model to ONNX using the official YOLOv5 export script.\r\n\r\nThe issue with the output tensor shape ([1, 8, 8400]) remains.\r\n\r\nRequest for Assistance:\r\n\r\nClarification on Detection Head Configuration:\r\n\r\nCould this issue arise from a misconfiguration of the detection head during training? If so, how can I fix it without retraining the model?\r\n\r\nIs there a way to manually adjust the detection headâ€™s out_channels in the .pt model and re-export it to ONNX?\r\n\r\nONNX Export Process:\r\n\r\nAre there known issues with the YOLOv5 ONNX export script that could cause this mismatch?\r\n\r\nHow can I ensure the ONNX modelâ€™s detection head is correctly configured for 4 classes?\r\n\r\nGeneral Guidance:\r\n\r\nWhat steps can I take to verify that the ONNX model will correctly detect all 4 classes?\r\n\r\nAre there tools or scripts you recommend for validating the ONNX modelâ€™s outputs?\r\n\r\nAdditional Context:\r\n\r\nultralytics - 2.4.1\r\nPyTorch Version: 2.4.1\r\n\r\nONNX Runtime Version:1.16.3\r\n\r\nThank you for your assistance in resolving this issue!\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @AbhirupSinha1811, thank you for your detailed report and for using YOLOv5 ðŸš€! Your observations and debugging steps are very thorough, which is highly appreciated.\n\nIf this is indeed a ðŸ› Bug Report, we kindly request a **minimum reproducible example (MRE)** to better assist in debugging this issue. An MRE would ideally contain simplified, complete code snippets and/or instructions to reproduce the ONNX export and the tensor shape discrepancy.\n\nFrom the context provided, here are a few steps you can double-check:\n\n1. **Detection Head Configuration**: Ensure the YOLOv5 detection head reflects the correct `out_channels` value (which should match `5 + number_of_classes`) of the dataset both before and after training.\n2. **ONNX Metadata**: Validate that the ONNX model metadata and the number of classes defined match the expected configurations.\n3. **Re-export Process**: Try re-exporting the model using the official export script with verbose logging enabled to identify any discrepancies during the export process.\n\n## Requirements\nEnsure you are using **Python>=3.8** with all dependencies installed correctly. Install requirements using:\n```bash\npip install -r requirements.txt\n```\n\n## Verified Environments\nThe ONNX export process is generally supported on environments such as notebooks, cloud platforms, or Docker. Make sure your training and export environments meet the dependencies, including PyTorch, CUDA, and ONNX runtime versions.\n\nAdditionally, it's worth confirming if the issue persists when running the export script on different setups or versions.\n\nThis is an automated response, but don't worry! An Ultralytics engineer will review your issue promptly to provide further assistance. In the meantime, feel free to share any additional findings or code snippets that could help us debug further ðŸš€."
      },
      {
        "user": "pderrenger",
        "body": "@AbhirupSinha1811 thank you for providing a detailed explanation of the issue. Based on your observations, it seems the problem stems from a misconfigured detection head in the `.pt` model. Here are some points to address your concerns:\n\n1. **Detection Head Configuration:**\n   - The mismatch in `out_channels` (8 instead of 9) indicates the model was trained with an incorrect detection head configuration for 4 classes. Unfortunately, this cannot be fixed without retraining the model, as the detection head's architecture is defined during training. \n\n2. **ONNX Export Process:**\n   - The YOLOv5 export script correctly uses the configuration of the `.pt` model for ONNX conversion. Since the `.pt` model itself is misconfigured, the ONNX model inherits the same issue. There are no known bugs in the export script that would alter the class configuration during conversion.\n\n3. **Manual Adjustment (Without Retraining):**\n   - While directly modifying the detection head's `out_channels` in the `.pt` model is theoretically possible, it is not recommended. Adjusting this manually would require significant changes to the model's architecture and weights, which is error-prone and may lead to unreliable results.\n\n4. **Validation of ONNX Outputs:**\n   - To verify the ONNX model's behavior, you can test it using the `detect.py` script in ONNX mode:\n     ```bash\n     python detect.py --weights model.onnx --img-size 640 --dnn\n     ```\n   - If issues persist, visualizing the model using [Netron](https://netron.app/) can help confirm the final layer's configuration.\n\nTo resolve this issue definitively, it is recommended to retrain the model with the correct class configuration (4 classes). If you suspect a training script issue, ensure you are using the latest YOLOv5 version and verify the `data.yaml` and training parameters before starting.\n\nFeel free to share further observations or questions. The YOLO community and Ultralytics team are here to help!"
      },
      {
        "user": "AbhirupSinha1811",
        "body": "Hello, after check the detection head of the yolo .pt model what I'm get is given below:-\r\n\"Dectection Head Output Channels\": 68\r\n\"Number of Classes\": 4\r\n\"class Name\":[\r\n'bird', 'drone', 'helicopter', 'jetplane']\r\n\r\n1. Detection Head and Output Channels\r\n\r\nWhy does the detection head of my custom YOLOv5s model have 68 output channels when it was trained on 4 classes? Shouldnâ€™t it be 27 (3 Ã— (5 + 4) for 4 classes and 3 anchors)?\r\nCould this mismatch have happened during training? How can I check and fix it?\r\n\r\n2. Inference Behavior\r\nThen why the model detects all 4 classes correctly during inference with the .pt model??\r\n\r\nHow does it handle the extra channels?\r\nIs this behavior consistent across all formats like ONNX or TensorRT?\r\n\r\n3. ONNX Export and Output Shape\r\nWhen exporting to ONNX, the output tensor shape is [1, 8, 8400] instead of [1, 27, grid_cells] for 4 classes. Why is this happening, and how can I fix it?\r\nCould the extra detection head channels (68) be causing this issue?\r\n\r\n4. Debugging and Fixing\r\nHow can I verify the number of classes (nc) and output channels used during training?\r\nIs there a way to fix the detection headâ€™s output channels post-training without retraining?\r\n\r\n5. Recommendations\r\nWhatâ€™s the best way to ensure the detection head matches the number of classes during training and export?\r\nAre there tools or scripts to avoid issues like this during ONNX export?\r\n\r\nKey Observations to Share\r\nDetection Head Output Channels: 68.\r\nNumber of Classes (nc) in YAML: 4.\r\nClass Names: ['bird', 'drone', 'helicopter', 'jetplane'].\r\nONNX Output Tensor Shape: [1, 8, 8400].\r\n\r\nBehavior: Model detects all 4 classes correctly during inference with .pt but shows unexpected behavior during ONNX export."
      }
    ]
  },
  {
    "issue_number": 13494,
    "title": "Fine-Tuning a Pre-Trained YOLOv10 Model Using QLoRA and Converting to TensorFlow Lite (TFLite)",
    "author": "vinujaj",
    "state": "open",
    "created_at": "2025-01-20T11:48:38Z",
    "updated_at": "2025-01-21T05:09:21Z",
    "labels": [
      "question",
      "exports"
    ],
    "body": "**I have searched similar issues but couldn't able to find anything.**\n\n\n### Question:\n\nI want to fine-tune my pre-trained yolov10 model. I specifically want to use Quantized Lorank Adaptation. Then I'm playing to convert the fine-tuned model into Tflite. Could you provide me with the steps and a tutorial on how to fine-tune the pre-trained yolov10 model using QLoRA?\nI found this hard to do.\n\nThank You.",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @vinujaj, thank you for your interest in YOLOv5 ðŸš€! It seems like you're diving into fine-tuning and quantization, very exciting! While YOLOv10 is not an official release from Ultralytics (our latest official releases are YOLOv5 ðŸš€ and YOLOv8 ðŸŒŸ), you can still explore similar workflows built around YOLOv5 and YOLOv8 models.\n\nIf you are looking to perform custom fine-tuning and quantization tasks like the one you mentioned, please note the following suggestions and resources:\n\n### Key Resources ðŸ”—\n\n- **[YOLOv5 Tutorials](https://docs.ultralytics.com/yolov5/):** Start here to get familiar with YOLOv5 training and inference workflows. For custom data, check out the [Custom Data Training Guide](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/).    \n- **[YOLOv8 Docs](https://docs.ultralytics.com/):** YOLOv8 is our latest model family with cutting-edge features for training and export. Check for similar TFLite export workflows in YOLOv8 [Export Guide](https://docs.ultralytics.com/modes/export/).\n- **[Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/):** Follow these tips to achieve optimal performance when training or fine-tuning models.\n\n### Notes ðŸ› ï¸  \n\n- **QLoRA:** At this time, direct QLoRA (Quantized LoRA) adaptation is not officially supported in YOLOv5 or YOLOv8 workflows. However, leveraging external libraries for LoRA-based fine-tuning on PyTorch models may be possible. We encourage you to provide additional details or code examples of your pipeline for further guidance.  \n- **TFLite Conversion:** YOLOv8 offers native TFLite export functionality, but for YOLOv5, you may need third-party solutions or custom scripts. Ensure your TFLite conversion steps account for any additional quantization changes from QLoRA.  \n\n### Help Us Help You ðŸ˜Š\n\nTo better assist you, please include the following, if applicable:\n1. Details about your **pre-trained model** (e.g., architecture, weights used, etc.)\n2. Steps or scripts youâ€™ve tried for fine-tuning using QLoRA.\n3. Errors or issues youâ€™re encountering with the TFLite conversion process.\n4. Logs or any code snippets that can help us understand the problem more clearly.\n\n### Requirements\n\nHere are the minimal repo requirements to replicate your issue or explore solutions:\n```bash\ngit clone https://github.com/ultralytics/yolov5\ncd yolov5\npip install -r requirements.txt\n```\n\n### Environments\n\nYou can also explore YOLOv5 on the following platforms for faster experimentation:\n- **Free GPUs:** <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n\n### Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>  \nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) CI tests are passing. CI tests verify functionality for training, validation, inference, export, and benchmarking daily.\n\n---\n\nðŸš¨ This is an automated response to help guide you. An Ultralytics engineer will review your question and provide additional assistance shortly."
      },
      {
        "user": "pderrenger",
        "body": "Thank you for your question! Currently, Ultralytics YOLO does not officially support QLoRA (Quantized Low-Rank Adaptation) for fine-tuning. However, you can fine-tune a YOLOv5 or newer YOLO models using standard methods and then convert the fine-tuned model to TFLite.\n\nTo fine-tune your model, refer to the YOLOv5 [training documentation](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/). Once fine-tuning is complete, you can follow the TFLite export guide [here](https://docs.ultralytics.com/integrations/tflite/) to convert the model for deployment.\n\nIf you require QLoRA-specific workflows, you may need to integrate external tools or frameworks. Let us know if you have further questions!"
      }
    ]
  },
  {
    "issue_number": 13492,
    "title": "Why I use multi GPUs to train a model, then the training details log missed? YOLOv8å¤šGPUè®­ç»ƒä¼šä¸¢å¤±æ—¥å¿—å—ï¼Ÿ",
    "author": "BGMer7",
    "state": "closed",
    "created_at": "2025-01-17T07:19:27Z",
    "updated_at": "2025-01-18T21:30:03Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI used YOLOv8 to train a model to ectract Facial Features, the mission is here: https://www.kaggle.com/datasets/osmankagankurnaz/facial-feature-extraction-dataset\n\nActually, it is not a fatel error, it's just a little question, but no YOLOv8 repo is found, so I posted here for help.\nIn kaggle, it provides 2 Tesla P4 GPU to accelarate, many people just use device='0' to make only one GPU in use, I make full use of these 2 GPUs, and it works. But the only difference and the problem is the training details logs are missing.\n\nThis is the code which uses only one GPU:\n```python\nresults = self.model.train(\n            data=DATA_YAML,       # data.yaml file from Roboflow\n            epochs=20,            # number of epochs\n            imgsz=640,            # image size\n            batch=32,             # batch size\n            name='yolov8_custom', # folder name for training results\n            device='0',           # '0' for GPU, 'cpu' for CPU\n            patience=50,          # early stopping patience\n            save=True,            # save best model\n            pretrained=True,      # use pretrained weights\n            plots=True,           # save training plots\n            cache=True,           # enable caching\n            verbose=True,         # Enable verbose logging\n            workers=2\n        )\n```\nand this is the code which uses two GPUs:\n```python\ncuda.init()\n        device_count = cuda.Device.count()\n        device = ','.join(str(i) for i in range(device_count)) if device_count > 0 else 'cpu'\n\n        # Initialize the wandb logging before training\n        # wandb.init(project=\"yolo_training\", config={\"epochs\": 20, \"batch_size\": 16})\n        results = self.model.train(\n            data=DATA_YAML,       # data.yaml file from Roboflow\n            epochs=20,            # number of epochs\n            imgsz=640,            # image size\n            batch=32,             # batch size\n            name='yolov8_custom', # folder name for training results\n            device=device,        # '0' for GPU, 'cpu' for CPU\n            patience=50,          # early stopping patience\n            save=True,            # save best model\n            pretrained=True,      # use pretrained weights\n            plots=True,           # save training plots\n            cache=True,           # enable caching\n            verbose=True,         # Enable verbose logging\n            workers=2\n        )\n```\n\nä¸€ä¸ªGPUè·‘çš„è¯å°±æ˜¯è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼Œæ¯è½®è®­ç»ƒä¹‹åŽéƒ½ä¼šæœ‰æŒ‡æ ‡è¾“å‡ºï¼Œä½†æ˜¯å¤šä¸ªæ—¥å¿—å°±å˜æˆåªæœ‰æœ€ç»ˆæ¨¡åž‹æœ‰è¾“å‡ºï¼Œä¸­é—´çš„è®­ç»ƒè¿‡ç¨‹å°±æ²¡æœ‰æ—¥å¿—äº†ã€‚\n\n**This is the training logs with only one GPU, the output is very detailed, and it printed the metrics after every epoch**\n```text\nUltralytics 8.3.62 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\nengine/trainer: task=detect, mode=train, model=yolov8m.pt, data=/kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/data.yaml, epochs=20, time=None, patience=50, batch=32, imgsz=640, save=True, save_period=-1, cache=True, device=0, workers=8, project=None, name=yolov8_custom3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/yolov8_custom3\nOverriding model.yaml nc=80 with nc=5\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n...\nTensorBoard: Start with 'tensorboard --logdir runs/detect/yolov8_custom3', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks...\nAMP: checks passed âœ…\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?c56fa883-3627-4114-bc78-1ed71e3193ef) or open in a [text editor](command:workbench.action.openLargeOutput?c56fa883-3627-4114-bc78-1ed71e3193ef). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\ntrain: Scanning /kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/train/labels... 457 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 457/457 [00:00<00:00, 672.14it/s]\ntrain: WARNING âš ï¸ Cache directory /kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/train is not writeable, cache not saved.\n\nWARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\ntrain: Caching images (0.5GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 457/457 [00:00<00:00, 613.71it/s]\nalbumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\nval: Scanning /kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/valid/labels... 126 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:00<00:00, 345.11it/s]\nval: WARNING âš ï¸ Cache directory /kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/valid is not writeable, cache not saved.\n\nWARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\nval: Caching images (0.1GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:00<00:00, 150.01it/s]\nPlotting labels to runs/detect/yolov8_custom3/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.001111, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\nTensorBoard: model graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to runs/detect/yolov8_custom3\nStarting training for 20 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/20      13.6G      1.772      3.051      1.781         61        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:14<00:00,  1.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.65s/it]\n                   all        126        685      0.919      0.912      0.942      0.672\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/20      13.6G     0.9436     0.8229      1.096         78        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:14<00:00,  1.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.12s/it]\n                   all        126        685      0.926      0.921      0.968      0.719\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/20      13.6G     0.9125     0.6598      1.077         80        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.03s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.09s/it]\n                   all        126        685      0.819      0.856      0.875      0.618\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/20      13.6G      0.883     0.6061      1.074         91        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:16<00:00,  1.07s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.08s/it]\n                   all        126        685      0.957       0.96      0.983      0.753\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/20      13.6G     0.8521     0.5585      1.061         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.04s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.06s/it]\n                   all        126        685       0.96      0.949      0.989      0.759\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/20      13.2G     0.8453     0.5445       1.05        101        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.03s/it]\n                   all        126        685      0.931       0.97      0.986      0.754\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/20      13.6G     0.8498     0.5344      1.058         64        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.06s/it]\n                   all        126        685      0.921      0.963      0.986      0.745\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/20      13.2G     0.8276     0.5133      1.056         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.03s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.05s/it]\n                   all        126        685       0.96      0.967      0.991      0.775\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/20      13.6G     0.8036     0.4939      1.027        106        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.03s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.05s/it]\n                   all        126        685      0.964      0.955      0.984      0.772\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/20      13.1G     0.7984     0.4807      1.033         60        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.03s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.05it/s]\n                   all        126        685      0.983      0.977      0.993      0.801\n\nClosing dataloader mosaic\nalbumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      11/20      13.5G      0.709     0.4234     0.9871         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:16<00:00,  1.09s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.05s/it]\n                   all        126        685      0.962      0.981      0.987      0.798\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      12/20      13.2G     0.7006     0.4037     0.9814         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.03it/s]\n                   all        126        685      0.968      0.986      0.986       0.79\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      13/20      13.6G     0.6802     0.3952     0.9669         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.03s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.05s/it]\n                   all        126        685      0.984      0.991      0.993      0.801\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      14/20      13.1G     0.6713      0.388      0.959         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.06it/s]\n                   all        126        685      0.988      0.984      0.994      0.812\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      15/20      13.6G     0.6565     0.3693     0.9504         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.04s/it]\n                   all        126        685      0.989      0.995      0.995      0.826\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      16/20      13.2G     0.6381     0.3588     0.9381         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.03it/s]\n                   all        126        685      0.983      0.998      0.995      0.836\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      17/20      13.5G     0.6197     0.3422     0.9381         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.05s/it]\n                   all        126        685       0.99      0.996      0.995      0.832\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      18/20      13.1G     0.6092     0.3295     0.9337         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.03s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.05it/s]\n                   all        126        685      0.993      0.997      0.994       0.84\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      19/20      13.1G     0.5917     0.3167     0.9202         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.06it/s]\n                   all        126        685      0.996      0.998      0.995      0.844\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      20/20      13.2G      0.569     0.3065     0.9168         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:15<00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.05it/s]\n                   all        126        685      0.997      0.998      0.995      0.847\n\n\n20 epochs completed in 0.107 hours.\nOptimizer stripped from runs/detect/yolov8_custom3/weights/last.pt, 52.0MB\nOptimizer stripped from runs/detect/yolov8_custom3/weights/best.pt, 52.0MB\n\nValidating runs/detect/yolov8_custom3/weights/best.pt...\nUltralytics 8.3.62 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 218 layers, 25,842,655 parameters, 0 gradients, 78.7 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.34s/it]\n                   all        126        685      0.997      0.998      0.995      0.848\n                   eye        126        138          1      0.996      0.995      0.839\n               eyebrow        126        144      0.993      0.999      0.994      0.796\n                   lip        126        129      0.998          1      0.995      0.881\n        mustache-beard        126        146      0.993      0.998      0.995      0.844\n                  nose        126        128          1      0.997      0.995      0.879\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\nSpeed: 0.2ms preprocess, 12.0ms inference, 0.0ms loss, 1.1ms postprocess per image\nResults saved to runs/detect/yolov8_custom3\nUltralytics 8.3.62 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 218 layers, 25,842,655 parameters, 0 gradients, 78.7 GFLOPs\nval: Scanning /kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/valid/labels... 126 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:00<00:00, 604.81it/s]\nval: WARNING âš ï¸ Cache directory /kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/valid is not writeable, cache not saved.\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:04<00:00,  1.73it/s]\n                   all        126        685      0.989      0.995      0.995      0.855\n                   eye        126        138      0.996      0.993      0.995      0.822\n               eyebrow        126        144      0.973      0.994      0.994      0.817\n                   lip        126        129      0.988          1      0.995      0.884\n        mustache-beard        126        146      0.986      0.992      0.995      0.869\n                  nose        126        128          1      0.996      0.995      0.882\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\nSpeed: 0.5ms preprocess, 24.1ms inference, 0.0ms loss, 2.6ms postprocess per image\nResults saved to runs/detect/val3\n\nâœ¨ Training completed!\nðŸ’¾ Best model saved at: runs/detect/yolov8_custom/weights/best.pt\n\nðŸ“Š Model Performance Summary:\n==================================================\nPrecision: 0.989\nRecall: 0.995\nmAP50: 0.995\nmAP50-95: 0.855\n==================================================\n```\n\n**But when chect to 2 GPUs, the output only contains the result of the final model**\n```text\nUltralytics 8.3.62 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n                                                       CUDA:1 (Tesla T4, 15095MiB)\nengine/trainer: task=detect, mode=train, model=yolov8m.pt, data=/kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/data.yaml, epochs=20, time=None, patience=50, batch=32, imgsz=640, save=True, save_period=-1, cache=True, device=0,1, workers=32, project=None, name=yolov8_custom4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/yolov8_custom4\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 16.9MB/s]\nOverriding model.yaml nc=80 with nc=5\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n 22        [15, 18, 21]  1   3778591  ultralytics.nn.modules.head.Detect           [5, [192, 384, 576]]          \nModel summary: 295 layers, 25,859,215 parameters, 25,859,199 gradients, 79.1 GFLOPs\n\nTransferred 469/475 items from pretrained weights\nDDP: debug command /usr/bin/python3 -m torch.distributed.run --nproc_per_node 2 --master_port 50831 /root/.config/Ultralytics/DDP/_temp_4t1q3l3o139581780929232.py\nUltralytics 8.3.62 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 218 layers, 25,842,655 parameters, 0 gradients, 78.7 GFLOPs\nval: Scanning /kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/valid/labels... 126 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [00:00<00:00, 747.79it/s]\nval: WARNING âš ï¸ Cache directory /kaggle/input/facial-feature-extraction-dataset/Facial Feature Extraction Dataset/valid is not writeable, cache not saved.\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:04<00:00,  1.99it/s]\n                   all        126        685      0.989      0.995      0.995      0.855\n                   eye        126        138      0.996      0.993      0.995      0.822\n               eyebrow        126        144      0.973      0.994      0.994      0.817\n                   lip        126        129      0.988          1      0.995      0.884\n        mustache-beard        126        146      0.986      0.992      0.995      0.869\n                  nose        126        128          1      0.996      0.995      0.882\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\nSpeed: 0.2ms preprocess, 22.9ms inference, 0.0ms loss, 3.8ms postprocess per image\nResults saved to runs/detect/val4\n\nâœ¨ Training completed!\nðŸ’¾ Best model saved at: runs/detect/yolov8_custom/weights/best.pt\n\nðŸ“Š Model Performance Summary:\n==================================================\nPrecision: 0.989\nRecall: 0.995\nmAP50: 0.995\nmAP50-95: 0.855\n==================================================\n```\n\nHas anyone met this before?\nAnd I find it seems when I use only one GPU, Api invokes tensorflow, when 2 GPUs, it invokes pytorch. \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @BGMer7, thank you for your interest in YOLOv5 ðŸš€! \n\nFor your question regarding multi-GPU training with YOLOv8, it's important to note that this repository is specifically for YOLOv5. However, since YOLOv8 shares some similarities, we might be able to assist you here! \n\nIf this is a ðŸ› Bug Report, could you please provide a **minimum reproducible example (MRE)**, including all necessary code and steps to replicate the issue? This will help us investigate the behavior further.\n\nFor custom training â“ Questions, sharing additional details, such as setup specifics, logs, or configurations, would assist in diagnosing the issue. It might also be worth checking if your multi-GPU setup affects logging by trying configurations like adjusting verbosity or distributed training options.\n\n## Requirements\n\nYOLOv5 and YOLOv8 require Python>=3.8.0 with all required dependencies installed. Ensure your environment is configured correctly and up to date.\n\n## Environments\n\nBoth YOLOv5 and YOLOv8 can be run in various environments, such as local setups, cloud-based GPUs, or Docker images. Verify your current setup matches recommended configurations, including ensuring that all GPUs are properly initialized and recognized.\n\n## Status\n\nIf you are receiving training logs when using a single GPU but missing them for a multi-GPU setup, it is possible that output redirection or distributed training settings are affecting the logs. When training on multiple GPUs, frameworks like PyTorch may modify how and where logs are written.\n\nThis is an automated response ðŸ› ï¸, but rest assured, an Ultralytics engineer will review your issue and provide further assistance soon. Let us know if you can provide any additional information in the meantime that might help clarify this behavior! ðŸ˜Š"
      },
      {
        "user": "BGMer7",
        "body": "Here I attach some more details here,\nI executed this command to let environment detect 2 gpus, \n```python\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\n!source ~/.bashrc\n```\notherwise, when using `torch.cuda.device_count` it will return 1.\nAfter executing this command, the result will be 2."
      },
      {
        "user": "pderrenger",
        "body": "@BGMer7 thank you for providing additional details. Setting `CUDA_VISIBLE_DEVICES` is a valid method to specify which GPUs are visible to your script. However, missing detailed logs during multi-GPU training is likely due to the use of `torch.distributed.run`, which handles Distributed Data Parallel (DDP) training. Logs are often only output from the main process. To ensure you capture detailed logs, you can:\n\n1. Run the training with the `--verbose` flag to enhance logging.\n2. Check if the logs are aggregated in the `runs/` directory or in the `wandb`/`TensorBoard` integrations if enabled.\n3. Use the latest version of the repository to ensure any potential issues with logging in DDP mode are resolved.\n\nIf the issue persists, please confirm your DDP setup and training script alignment with [Ultralytics' multi-GPU training guide](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/)."
      }
    ]
  },
  {
    "issue_number": 13213,
    "title": "AttributeError: 'tuple' object has no attribute 'to'",
    "author": "stillbetter",
    "state": "open",
    "created_at": "2024-07-24T02:33:35Z",
    "updated_at": "2025-01-15T18:52:15Z",
    "labels": [
      "bug",
      "question"
    ],
    "body": "https://github.com/ultralytics/yolov5/blob/d6112173f5b2b809ec7f3ff1f8048ac0e465092c/models/experimental.py#L99\r\n\r\n\r\nI got an error like:\r\n\r\n```\r\n  File \"export.py\", line 1346, in run\r\n    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\r\n  File \"/home/mi/PycharmProjects/yolov5/models/experimental.py\", line 99, in attempt_load\r\n    ckpt = (ckpt.get(\"ema\") or ckpt[\"model\"]).to(device).float()  # FP32 model\r\nAttributeError: 'tuple' object has no attribute 'to'\r\n\r\n```\r\nShould I just remove ckpt.get(\"ema\") or ckpt[\"model\"] in the code?\r\n\r\n",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@stillbetter hi there,\n\nThank you for reaching out and providing detailed information about the issue you're encountering. The error you're seeing, `AttributeError: 'tuple' object has no attribute 'to'`, suggests that the `ckpt` variable is being assigned a tuple instead of the expected dictionary.\n\nThis issue can sometimes occur if the `attempt_load` function is not handling the weights file correctly. Here are a few steps you can take to troubleshoot and resolve this:\n\n1. **Verify the Weights File**: Ensure that the weights file you are using is correctly formatted and not corrupted. You can try downloading the weights file again from the official YOLOv5 release page.\n\n2. **Update to the Latest Version**: Make sure you are using the latest version of YOLOv5. The issue might have been fixed in a more recent update. You can update your local repository with:\n    ```bash\n    git pull\n    ```\n\n3. **Check the `attempt_load` Function**: The `attempt_load` function should return a model object, not a tuple. Ensure that the function is correctly implemented. Here is a snippet of how it should look:\n    ```python\n    def attempt_load(weights, map_location=None, inplace=False, fuse=True):\n        # Load model\n        ckpt = torch.load(weights, map_location=map_location)  # load checkpoint\n        model = ckpt['model'].float()  # FP32 model\n        if fuse:\n            model.fuse()\n        return model\n    ```\n\n4. **Debugging**: Add print statements to check the type and content of `ckpt` before the line causing the error:\n    ```python\n    print(type(ckpt))\n    print(ckpt)\n    ```\n\nIf the issue persists after these steps, please provide additional details such as the version of YOLOv5 you are using and the exact command you ran. This will help us further diagnose the problem.\n\nFeel free to reach out with any more questions or updates on your progress. The YOLO community and the Ultralytics team are here to help! ðŸ˜Š"
      },
      {
        "user": "vjayd",
        "body": "I am still seeing the same error with this command for yolo11n model\npython main.py --model yolov8n_full_integer_quant.tflite --img image.jpg --conf 0.25 --iou 0.45 --metadata \"metadata.yaml\""
      },
      {
        "user": "pderrenger",
        "body": "@vjayd the error likely stems from using a YOLOv5-based workflow with a YOLOv8 TFLite model. YOLOv5 does not natively support YOLOv8 models or TFLite formats. To resolve this, please use the [Ultralytics YOLO repository](https://github.com/ultralytics/ultralytics) which supports YOLOv8 and TFLite directly."
      }
    ]
  },
  {
    "issue_number": 9694,
    "title": "Fail to export CoreML model with decode layer and NMS",
    "author": "Qwin",
    "state": "closed",
    "created_at": "2022-10-04T20:48:16Z",
    "updated_at": "2025-01-12T18:56:03Z",
    "labels": [
      "Stale"
    ],
    "body": "Hello everyone,\r\n\r\nI been working on this for days now without avail, so was wondering if anyone here could help me out. I first trained up a model using the following dataset on kaggle : https://www.kaggle.com/datasets/taranmarley/sptire\r\n\r\nI used the colab script on this github to train my model. It works flawlessly I can even see perfect detection of my tires with detect. \r\n\r\nNow here is where the problem comes, when I try to export the model to coreML everything goes well however I get only 1 output out of the model (which my guess is one matrix with the results):\r\n\r\n<img width=\"433\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1026038/193922644-72636176-c198-4837-a8d3-6e5c3e3210b5.png\">\r\n\r\nafter it has been exported it runs a decode layer on the spec and adds NMS. Which both fail with the following error:\r\n<img width=\"1427\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1026038/193922873-22fe17c9-81ed-4b73-a63a-dec618506665.png\">\r\n\r\nMy guess is that the output is incorrect of the model or that it has changed and the script that I am using to export it is outdated and still expects 2 outputs while there is only 1. If anyone here could help me just getting my yolov5 model converted correctly to coreml I would be really grateful. \r\n\r\nHere is the export script that I am running:\r\nhttps://colab.research.google.com/drive/1uR738UTlzI7apqeN0qr6mQ5ke_a5SKa8?usp=sharing\r\n\r\nHere is the blog that I followed to run this script:\r\nhttps://rockyshikoku.medium.com/convert-yolov5-to-coreml-also-add-a-decode-layer-113408b7a848\r\n\r\nP.S. Please let me know if additional info is needed.",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@Qwin Ultralytics HUB exports YOLOv5 pipelined CoreML models with NMS etc. See https://hub.ultralytics.com/"
      },
      {
        "user": "glenn-jocher",
        "body": "<img width=\"1235\" alt=\"Screenshot 2022-10-05 at 21 27 59\" src=\"https://user-images.githubusercontent.com/26833433/194145725-1256862c-c42c-40e6-be4b-4f8534f12193.png\">\r\n"
      },
      {
        "user": "Qwin",
        "body": "@glenn-jocher thank you so much it worked!!! Is the export code for iOS opensource that the hub is using and might I take a look at it. I just want to see what I was doing wrong and what the difference is between the script I used from the blog and the hub used. I know I exported it using export.py (which my guess the ultralytics hub does the same) but then it uses a custom NMS code to get iOS values out of it, and I am curious as to what its doing to manipulate the model. \r\n\r\nI have a feeling its because the script above was using 2 matrices while the script the hub is using as input is 1 matrix to calculate the missing NMS layer."
      }
    ]
  },
  {
    "issue_number": 13485,
    "title": "Significant Differences in Evaluation Results on the Validation Set Between `train.py` During Training and `test.py` in YOLOv5 5.0",
    "author": "3210448723",
    "state": "open",
    "created_at": "2025-01-09T12:21:19Z",
    "updated_at": "2025-01-10T02:56:45Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n# [YOLOv5 5.0ç‰ˆæœ¬](https://github.com/ultralytics/yolov5/releases/tag/v5.0)åœ¨train.pyè®­ç»ƒè¿‡ç¨‹ä¸­åœ¨éªŒè¯é›†ä¸Šçš„è¯„ä¼°ç»“æžœä¸Žtest.pyåœ¨éªŒè¯é›†ä¸Šçš„è¯„ä¼°ç»“æžœå…·æœ‰æ˜¾è‘—å·®å¼‚\r\n\r\nWhy is this happening? The results in `test.py` are much higher than those obtained during validation after adjusting the number of epochs, and this phenomenon occurs in most epochs. The results from `test.py` are extremely ideal and do not match the actual performance. Below is a portion of the output logs.\r\nä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿtestä¸­çš„ç»“æžœæ¯”æ”¹epochåŽè¿›è¡ŒéªŒè¯çš„ç»“æžœé«˜äº†å¾ˆå¤šï¼Œè€Œä¸”å¤§å¤šæ•°epochéƒ½æœ‰è¿™æ ·çš„çŽ°è±¡ï¼Œtest.pyçš„ç»“æžœæžå…¶ç†æƒ³ï¼Œä¸Žå®žé™…ä¸ç¬¦ã€‚ä¸‹é¢æ˜¯éƒ¨åˆ†è¾“å‡ºæ—¥å¿—\r\n\r\n## Evaluation Output of `train.py` on the Validation Set at Epoch 115\r\n\r\n```bash\r\n2024-12-12 14:37:44,182 - INFO - YOLOv5 ðŸš€ 5211d5c torch 2.4.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24154.375MB)\r\n                                   CUDA:1 (NVIDIA GeForce RTX 3090, 24154.375MB)\r\n                                   CUDA:2 (NVIDIA GeForce RTX 3090, 24154.375MB)\r\n                                   CUDA:3 (NVIDIA GeForce RTX 3090, 24154.375MB)\r\n\r\n2024-12-12 14:37:44,192 - INFO - Namespace(adam=False, artifact_alias='latest', batch_size=32, bbox_interval=-1, bucket='', cache_images=True, cfg='', data='data/fankou/EnhancedDataset.yaml', device='0,1,2,3', entity=None, epochs=300, evolve=False, exist_ok=False, global_rank=-1, hyp='data/fankou/hyp.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='exp', noautoanchor=False, nosave=False, notest=False, offline=True, project='runs/train', quad=False, rect=False, resume=True, save_dir='runs/train/exp', save_period=1, single_cls=False, sync_bn=False, total_batch_size=32, upload_dataset=False, weights='./runs/train/exp/weights/last.pt', workers=8, world_size=1)\r\n2024-12-12 14:37:44,193 - INFO - \u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\r\n2024-12-12 14:37:44,194 - INFO - \u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.0375, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, label_smoothing=0.0\r\n2024-12-12 14:37:47,177 - INFO - \r\n                 from  n    params  module                                  arguments                     \r\n2024-12-12 14:37:47,181 - INFO -   0                -1  1      7040  models.common.Focus                     [3, 64, 3]                    \r\n2024-12-12 14:37:47,182 - INFO -   1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n2024-12-12 14:37:47,185 - INFO -   2                -1  1    156928  models.common.C3                        [128, 128, 3]                 \r\n2024-12-12 14:37:47,187 - INFO -   3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \r\n2024-12-12 14:37:47,199 - INFO -   4                -1  1   1611264  models.common.C3                        [256, 256, 9]                 \r\n2024-12-12 14:37:47,205 - INFO -   5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \r\n2024-12-12 14:37:47,248 - INFO -   6                -1  1   6433792  models.common.C3                        [512, 512, 9]                 \r\n2024-12-12 14:37:47,277 - INFO -   7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \r\n2024-12-12 14:37:47,296 - INFO -   8                -1  1   2624512  models.common.SPP                       [1024, 1024, [5, 9, 13]]      \r\n2024-12-12 14:37:47,359 - INFO -   9                -1  1   9971712  models.common.C3                        [1024, 1024, 3, False]        \r\n2024-12-12 14:37:47,363 - INFO -  10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \r\n2024-12-12 14:37:47,363 - INFO -  11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n2024-12-12 14:37:47,363 - INFO -  12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n2024-12-12 14:37:47,382 - INFO -  13                -1  1   2757632  models.common.C3                        [1024, 512, 3, False]         \r\n2024-12-12 14:37:47,383 - INFO -  14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \r\n2024-12-12 14:37:47,383 - INFO -  15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n2024-12-12 14:37:47,383 - INFO -  16           [-1, 4]  1         0  models.common.Concat                    [1]                           \r\n2024-12-12 14:37:47,390 - INFO -  17                -1  1    690688  models.common.C3                        [512, 256, 3, False]          \r\n2024-12-12 14:37:47,394 - INFO -  18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \r\n2024-12-12 14:37:47,394 - INFO -  19          [-1, 14]  1         0  models.common.Concat                    [1]                           \r\n2024-12-12 14:37:47,411 - INFO -  20                -1  1   2495488  models.common.C3                        [512, 512, 3, False]          \r\n2024-12-12 14:37:47,426 - INFO -  21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \r\n2024-12-12 14:37:47,426 - INFO -  22          [-1, 10]  1         0  models.common.Concat                    [1]                           \r\n2024-12-12 14:37:47,490 - INFO -  23                -1  1   9971712  models.common.C3                        [1024, 1024, 3, False]        \r\n2024-12-12 14:37:47,491 - INFO -  24      [17, 20, 23]  1     59235  models.yolo.Detect                      [6, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\r\n2024-12-12 14:37:47,781 - INFO - Model Summary: 499 layers, 46658275 parameters, 46658275 gradients, 114.6 GFLOPS\r\n2024-12-12 14:37:47,781 - INFO - \r\n2024-12-12 14:37:47,890 - INFO - Transferred 650/650 items from ./runs/train/exp/weights/last.pt\r\n2024-12-12 14:37:47,966 - INFO - Scaled weight_decay = 0.0005\r\n2024-12-12 14:37:47,970 - INFO - Optimizer groups: 110 .bias, 110 conv.weight, 107 other\r\n2024-12-12 14:39:57,377 - INFO - Image sizes 640 train, 640 test\r\nUsing 8 dataloader workers\r\nLogging results to runs/train/exp\r\nStarting training for 300 epochs...\r\n\r\n2024-12-13 00:16:49,333 - INFO - \r\n\u001b[34m\u001b[1mtest:\u001b[0m data: {'train': ['/home/user/yuanjinmin/æ•°æ®é›†/æ¨¡åž‹è®­ç»ƒ/train', '/home/user/yuanjinmin/dataset/obj_train_data/train_pro'], 'val': ['/home/user/yuanjinmin/æ•°æ®é›†/æ¨¡åž‹è®­ç»ƒ/val', '/home/user/yuanjinmin/dataset/obj_train_data/val_pro'], 'nc': 6, 'names': ['unhelmet', 'helmet', 'cigarette', 'fire', 'smoke', 'safebelt']}, weight: None, batch_size: 64, imgsz: 640, conf_thres: 0.001, iou_thres: 0.6, save_json: False, single_cls: False, augment: False, verbose: True, dataloader: <utils.datasets.InfiniteDataLoader object at 0x784bc87e5100>, save_dir: runs/train/exp, save_txt: False, save_hybrid: False, save_conf: False, plots: False, wandb_logger: <utils.wandb_logging.wandb_utils.WandbLogger object at 0x784bd86e1610>, compute_loss: <utils.loss.ComputeLoss object at 0x784bd5c4e790>, half_precision: True, is_coco: False\r\n2024-12-13 00:17:12,163 - INFO -                Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95\r\n2024-12-13 00:17:12,163 - INFO -                  all        3925       22419       0.745       0.731       0.722       0.406\r\n2024-12-13 00:17:12,164 - INFO -             unhelmet        3925        9246       0.903       0.939       0.938       0.538\r\n2024-12-13 00:17:12,164 - INFO -               helmet        3925       10645        0.86       0.927       0.943       0.754\r\n2024-12-13 00:17:12,164 - INFO -            cigarette        3925         761       0.631       0.618       0.594       0.229\r\n2024-12-13 00:17:12,164 - INFO -                 fire        3925         808        0.57        0.64         0.6       0.325\r\n2024-12-13 00:17:12,164 - INFO -                smoke        3925         717       0.602       0.351        0.35       0.135\r\n2024-12-13 00:17:12,164 - INFO -             safebelt        3925         242       0.904       0.913        0.91       0.457\r\n```\r\n\r\n## Evaluation Output of `test.py` on the Validation Set for the Model at Epoch 115\r\n\r\n```bash\r\n2025-01-09 20:15:37,890 - INFO - Namespace(augment=False, batch_size=64, conf_thres=0.001, data='data/fankou/EnhancedDataset.yaml', device='0,1,2,3', exist_ok=False, img_size=640, iou_thres=0.6, name='exp', project='runs/test', save_conf=True, save_hybrid=True, save_json=False, save_txt=True, single_cls=False, task='val', verbose=True, weights='runs/train/exp/weights/epoch_115.pt')\r\n2025-01-09 20:15:39,100 - INFO - Fusing layers...\r\n2025-01-09 20:15:40,267 - INFO - Model Summary: 392 layers, 46627491 parameters, 0 gradients, 114.0 GFLOPS\r\n2025-01-09 20:17:21,449 - INFO -                Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95\r\n2025-01-09 20:17:21,449 - INFO -                  all        3925       22419           1           1       0.995       0.995\r\n2025-01-09 20:17:21,449 - INFO -             unhelmet        3925        9246           1           1       0.996       0.996\r\n2025-01-09 20:17:21,449 - INFO -               helmet        3925       10645           1       0.999       0.996       0.996\r\n2025-01-09 20:17:21,449 - INFO -            cigarette        3925         761           1           1       0.995       0.995\r\n2025-01-09 20:17:21,450 - INFO -                 fire        3925         808           1           1       0.995       0.995\r\n2025-01-09 20:17:21,450 - INFO -                smoke        3925         717           1           1       0.995       0.995\r\n2025-01-09 20:17:21,450 - INFO -             safebelt        3925         242           1           1       0.995       0.995\r\n2025-01-09 20:17:21,450 - INFO - Speed: 3.2/1.8/4.9 ms inference/NMS/total per 640x640 image at batch-size 64\r\n2025-01-09 20:17:22,088 - INFO - Results saved to runs/test/exp\r\n3925 labels saved to runs/test/exp/labels\r\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @3210448723, thank you for bringing this to our attention and for your interest in YOLOv5 ðŸš€! \n\nIt seems you're encountering differences in evaluation metrics between `train.py` and `test.py`. This discrepancy might arise due to differences in how the evaluation is performed during training versus testing. To assist you better, could you please share a **minimum reproducible example (MRE)**? This should include:\n\n- The exact commands you used for both `train.py` and `test.py`.\n- Relevant portions of your dataset or configuration files.\n- Specific details about your training and testing pipelines (e.g., augmentations, hyperparameters, evaluation settings).\n- Versions of YOLOv5, Python, and PyTorch being used.\n\nAdditionally, ensure that your environment satisfies these **minimum requirements**:\n- Python>=3.8.0\n- All dependencies installed as per the `requirements.txt` included in the repository\n- PyTorch>=1.8 and correctly set up CUDA (if using GPU)\n\nIf applicable, confirm whether you are running YOLOv5 in a local environment or in a cloud-based environment (such as Colab, Paperspace, etc.).\n\nThis is an automated response to help guide resolution, and an Ultralytics engineer will assist you further soon. Let us know if you need additional clarification! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@3210448723 the significant differences in evaluation results between `train.py` and `test.py` likely stem from differences in evaluation configurations, such as augmentation settings, confidence thresholds, or IoU thresholds. During training, `train.py` typically uses validation with partial augmentations and real-time adjustments, while `test.py` evaluates the model in a purely inference-focused environment without training-specific nuances.\n\nTo investigate further:\n\n1. Ensure both scripts use consistent configurations for evaluation (e.g., `--augment`, `imgsz`, `conf_thres`, `iou_thres`).\n2. Check if the dataset and preprocessing steps are identical for both scripts.\n3. Confirm the `test.py` command is evaluating the same checkpoint as the one saved during training.\n\nFor additional details on validation differences, consult the [YOLOv5 validation documentation](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/). Let me know if you need further clarification!"
      }
    ]
  },
  {
    "issue_number": 13380,
    "title": "realtime detection on jetson using pi camera ",
    "author": "Munia-AK",
    "state": "closed",
    "created_at": "2024-10-24T11:13:52Z",
    "updated_at": "2025-01-09T23:33:13Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nI'm trying to run real time detection on jetson nano using custom fine tuned yolov5s model and pi camera. I already did this using below script which worked:\r\n![Screenshot from 2024-10-24 11-57-39](https://github.com/user-attachments/assets/2376c9b4-9699-4406-89db-640f8ef2425e)\r\n\r\nHowever I don't want to use torch.hub for loading the model because it requires internet connection. I need to make it work without internet. I tried in the same code to load the model from it's path without using torch.hub.load, for example:\r\n**model = 'best.py'**\r\nand then send each frame as input directly like: **results = model(frame)**\r\nbut this didn't work and gave error\r\n\r\nI know the solution lies in the detect.py script. I ran the detect.py script with webcam on jetson nano without internet connection and it worked.\r\nSo, I made two attempts, in first attempt I used same previous code but took the parts of loading the model from detect.py and added them to the code to replace torch.hub.load, like this:\r\n![Screenshot from 2024-10-24 13-25-05](https://github.com/user-attachments/assets/4ea0fe9f-e9c6-475b-b0dd-d57b4ed708c1)\r\n\r\nbut this gave this error too that I wasn't able to fix in the end:\r\n  _File \"detect__.py\", line 71, in <module>\r\n\tresult_img = result.render()[0]  # Render the detection and get the image\r\nAttributeError: 'Tensor' object has no attribute 'render_\r\n\r\nIn the second try I was editing the detect.script attempting to add the command that runs the pi camera using GStreamer pipeline. Specifically I edited the following three parts believing that I should edit the webcam sections by making the code runs pi camera instead of webcam when --source 0 is chosen.:\r\nafter editing:\r\n\r\n**part 1:**\r\ndef run(\r\n    weights=ROOT / \"yolov5s.pt\",  # model path or triton URL\r\n    source = \"nvarguscamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)NV12, framerate=(fraction)30/1 ! nvvidconv flip-method=0 ! video/x-raw, width=(int)1280, height=(int)720, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\"\r\n.\r\n.\r\n.\r\n**part 2:**\r\n![Screenshot from 2024-10-24 11-58-50](https://github.com/user-attachments/assets/e990c287-c26d-4c65-a0e2-4fe6dc10992e)\r\n\r\n**part 3:**\r\n![Screenshot from 2024-10-24 12-13-08](https://github.com/user-attachments/assets/b0717b28-fb9e-44f6-a7be-f0d0aab30f96)\r\n\r\nbut this didn't work and threw an error.\r\n\r\n![Screenshot from 2024-10-24 13-01-49](https://github.com/user-attachments/assets/676a7294-9883-4eb8-9511-2cfec71695b2)\r\n\r\nI couldn't fix any of the errors of all attempts. So I'm not sure whether I'm following the right path but if this is doable then can you please guide me onto how to make pi camera do the detection without using torch.hub.load\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Munia-AK, thank you for reaching out with your query on YOLOv5 ðŸš€!\n\nIt seems you're aiming to run real-time detection on a Jetson Nano using a Pi camera without relying on an internet connection for torch.hub. You're on the right track with modifying `detect.py`. This kind of setup can indeed be a bit tricky!\n\nPlease make sure to provide a **minimum reproducible example** that can help us debug the situation. This will assist in pinpointing what might be going wrong with the changes you made.\n\nHere are a few steps and resources that might help:\n\n- Ensure you have set up your environment correctly. YOLOv5 requires [**Python>=3.8.0**](https://www.python.org/) and [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) installed. You can set up the environment with:\n  ```bash\n  git clone https://github.com/ultralytics/yolov5  # clone\n  cd yolov5\n  pip install -r requirements.txt  # install\n  ```\n\n- Since you're working with a Jetson Nano, ensure your GStreamer pipeline for the Pi camera is correctly configured. Double-check the syntax and compatibility of your GStreamer string.\n\n- If you are using CUDA on your Jetson Nano, ensure it's properly installed and functional to leverage GPU acceleration.\n\nIf you're considering alternatives, you might want to check out the newer [YOLOv8](https://github.com/ultralytics/ultralytics) model, designed to be fast and efficient. You can install it using:\n  ```bash\n  pip install ultralytics\n  ```\n\nAn Ultralytics engineer will assist you soon! Meanwhile, please explore our [Tutorials](https://docs.ultralytics.com/yolov5/) for more guidance, including [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nFeel free to share any additional details or screenshots that might aid in diagnosing the issue. Good luck, and we're here to support you! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@Munia-AK to run YOLOv5 on a Jetson Nano without internet, you can load the model directly using PyTorch. First, ensure your model is saved as a `.pt` file. Then, load it with `torch.load()` and perform inference using the model's `forward()` method. Here's a basic example:\n\n```python\nimport torch\nfrom models.common import DetectMultiBackend\n\n# Load model\nmodel = DetectMultiBackend('best.pt', device='cuda')  # Adjust path as necessary\n\n# Perform inference\nresults = model(frame)  # Replace 'frame' with your input data\n```\n\nEnsure your environment is set up with all necessary dependencies from the `requirements.txt` file. If you encounter specific errors, please share them for further assistance."
      },
      {
        "user": "Munia-AK",
        "body": "I found the solution for my problem.. thanks!"
      }
    ]
  },
  {
    "issue_number": 12622,
    "title": "I have a problem with low confidence in detecting",
    "author": "thorww",
    "state": "closed",
    "created_at": "2024-01-13T15:38:23Z",
    "updated_at": "2025-01-08T16:15:52Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am trying to use yolo5 to train medical images, and the actual training and verification performance indexes are very good, reaching about 0.9. However, in the reasoning process, the actual frame selection confidence of the image is very low, there are very few boxes higher than 0.5, and the boxes with high confidence in an image rarely cross the actual target frame. May I ask what causes this phenomenon? Or are there any good solutions\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @thorww, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@thorww hello! It's great to hear that you're achieving good performance metrics during training and validation with your medical images. If you're experiencing low confidence scores during inference, here are a few suggestions that might help:\n\n1. **Threshold Adjustment**: Try adjusting the confidence threshold during inference. The default value might be too high for your specific use case.\n\n2. **Data Distribution**: Ensure that the data distribution of your training set closely matches that of your inference set. Differences in distributions can lead to poor generalization.\n\n3. **Augmentation**: Review your augmentation strategies. Over-augmentation can sometimes lead to a model that is too invariant to the features that are actually important for detection.\n\n4. **Model Overfitting**: High validation metrics but poor inference performance could indicate overfitting. Consider using techniques like dropout, data augmentation, or gathering more diverse training data.\n\n5. **Post-Processing**: Look into the Non-Maximum Suppression (NMS) settings. Incorrect NMS parameters can lead to missed detections or low confidence scores.\n\n6. **Model Selection**: If you're using a smaller YOLOv5 model (like YOLOv5s), consider using a larger one (like YOLOv5l or YOLOv5x) which might capture more details relevant for medical images.\n\n7. **Batch Size and Learning Rate**: If you have not already, experiment with different batch sizes and learning rates, as these can significantly affect model performance.\n\n8. **Review Annotations**: Double-check your annotations to ensure they are accurate. Inaccurate or inconsistent annotations can lead to poor model performance.\n\nFor more detailed guidance, please refer to our documentation. If the issue persists, feel free to provide more details such as the command you're using for inference, and we can look into it further. Keep up the good work, and remember that the YOLO community and the Ultralytics team are here to support you! ðŸ˜ŠðŸ‘"
      },
      {
        "user": "thorww",
        "body": "@glenn-jocher Thank you very much for your reply. I am trying your scheme gradually, but I have encountered a more serious problem: At present, there is only one target region for each image in my data set, and several regions with slightly higher confidence in each image tend to be quite different from the reality when reasoning. And even if I use the data set that can reach 0.9 under verification conditions for reasoning, this situation will still occur. I have tried to use some schemes, but they have not been well solved. Could you tell me the cause of this problem or the solution\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 10185,
    "title": "RuntimeError: The size of tensor a (1520) must match the size of tensor b (4) at non-singleton dimension 1",
    "author": "huranzhijian",
    "state": "closed",
    "created_at": "2022-11-17T12:13:22Z",
    "updated_at": "2025-01-05T21:28:01Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\r\n\r\n\r\n### YOLOv5 Component\r\n\r\nTraining\r\n\r\n### Bug\r\n\r\nr = wh[:, None] / k[None]\r\nRuntimeError: The size of tensor a (1520) must match the size of tensor b (4) at non-singleton dimension 1\r\nwhen i am traning my own datasets,  the image size is 1280 * 10,  and i set 4 anchors\r\n![QQå›¾ç‰‡20221117201042](https://user-images.githubusercontent.com/40172397/202443880-ecec7767-dc49-4582-8142-32f2a7f16b7d.png)\r\n![QQå›¾ç‰‡20221117201100](https://user-images.githubusercontent.com/40172397/202443895-4f6c38e4-9d25-4eab-869f-cf0b55075b09.png)\r\n\r\n\r\n\r\n### Environment\r\n\r\ncentos and windows\r\npytorch                   1.12.1\r\n\r\n### Minimal Reproducible Example\r\n\r\n_No response_\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @huranzhijian, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "ðŸ‘‹ Hello! Thanks for asking about model anchors. YOLOv5 ðŸš€ uses a new [Ultralytics](https://ultralytics.com/) algorithm called **AutoAnchor** for anchor verification and generation before training starts.\r\n\r\nAutoanchor will analyse your anchors against your dataset and training settings (like `--img-size`), and will adjust your anchors as necessary if it determines the original anchors are a poor fit, or if an **anchor count** was specified in your model.yaml rather than **anchor values**, i.e. \r\n```yaml\r\n# Specify anchor count (per layer)\r\nanchors: 3\r\n\r\n# --OR-- Specify anchor values manually\r\nanchors:\r\n  - [10,13, 16,30, 33,23]  # P3/8\r\n  - [30,61, 62,45, 59,119]  # P4/16\r\n  - [116,90, 156,198, 373,326]  # P5/32\r\n```\r\n\r\nWhen generating new anchors, autoanchor first applies a kmeans function against your dataset labels (scaled to your training `--img-size`), and uses kmeans centroids as initial conditions for a [Genetic Evolution](https://en.wikipedia.org/wiki/Genetic_algorithm) (GE) algorithm. The GE algorithm will evolve all anchors for 1000 generations under default settings, using CIoU loss (same regression loss used during training) combined with [Best Possible Recall](https://en.wikipedia.org/wiki/Precision_and_recall) (BPR) as its fitness function.\r\n\r\nNotebook example: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n\r\n<img width=\"1200\" alt=\"Screenshot 2022-03-24 at 10 58 28\" src=\"https://user-images.githubusercontent.com/26833433/159891298-8b15f80b-f5b0-4fa9-8824-7380bae32a76.png\">\r\n\r\n**No action is required on your part** to use autoanchor. If you would like to force manual anchors for any reason, you can skip autoanchor with the `--noautoanchor` flag:\r\n```shell\r\npython train.py --noautoanchor\r\n```\r\n\r\nFor more details on AutoAnchor see:\r\nhttps://github.com/ultralytics/yolov5/blob/master/utils/autoanchor.py\r\n\r\nGood luck ðŸ€ and let us know if you have any other questions!"
      },
      {
        "user": "huranzhijian",
        "body": "> ðŸ‘‹ Hello! Thanks for asking about model anchors. YOLOv5 ðŸš€ uses a new [Ultralytics](https://ultralytics.com/) algorithm called **AutoAnchor** for anchor verification and generation before training starts.\r\n> \r\n> Autoanchor will analyse your anchors against your dataset and training settings (like `--img-size`), and will adjust your anchors as necessary if it determines the original anchors are a poor fit, or if an **anchor count** was specified in your model.yaml rather than **anchor values**, i.e.\r\n> \r\n> ```yaml\r\n> # Specify anchor count (per layer)\r\n> anchors: 3\r\n> \r\n> # --OR-- Specify anchor values manually\r\n> anchors:\r\n>   - [10,13, 16,30, 33,23]  # P3/8\r\n>   - [30,61, 62,45, 59,119]  # P4/16\r\n>   - [116,90, 156,198, 373,326]  # P5/32\r\n> ```\r\n> \r\n> When generating new anchors, autoanchor first applies a kmeans function against your dataset labels (scaled to your training `--img-size`), and uses kmeans centroids as initial conditions for a [Genetic Evolution](https://en.wikipedia.org/wiki/Genetic_algorithm) (GE) algorithm. The GE algorithm will evolve all anchors for 1000 generations under default settings, using CIoU loss (same regression loss used during training) combined with [Best Possible Recall](https://en.wikipedia.org/wiki/Precision_and_recall) (BPR) as its fitness function.\r\n> \r\n> Notebook example: [![Open In Colab](https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb) [![Open In Kaggle](https://camo.githubusercontent.com/a08ca511178e691ace596a95d334f73cf4ce06e83a5c4a5169b8bb68cac27bef/68747470733a2f2f6b6167676c652e636f6d2f7374617469632f696d616765732f6f70656e2d696e2d6b6167676c652e737667)](https://www.kaggle.com/models/ultralytics/yolov5)\r\n> \r\n> <img alt=\"Screenshot 2022-03-24 at 10 58 28\" width=\"1200\" src=\"https://user-images.githubusercontent.com/26833433/159891298-8b15f80b-f5b0-4fa9-8824-7380bae32a76.png\">\r\n> \r\n> **No action is required on your part** to use autoanchor. If you would like to force manual anchors for any reason, you can skip autoanchor with the `--noautoanchor` flag:\r\n> \r\n> ```shell\r\n> python train.py --noautoanchor\r\n> ```\r\n> \r\n> For more details on AutoAnchor see: https://github.com/ultralytics/yolov5/blob/master/utils/autoanchor.py\r\n> \r\n> Good luck ðŸ€ and let us know if you have any other questions!\r\n\r\nwhen I set the number of the anchor is 3,it will be ok, but when set it 4,it will hava the problem."
      }
    ]
  },
  {
    "issue_number": 7606,
    "title": "Loading a custom trained model with PyTorch Hub does not work. Error: invalid load key, 'v'.. Cache may be out of date",
    "author": "pasmai",
    "state": "closed",
    "created_at": "2022-04-27T10:17:29Z",
    "updated_at": "2025-01-03T13:17:16Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nPyTorch Hub\n\n### Bug\n\nWhen I try to load a custom trained model with PyTorch Hub, I receive an error that there is an invalid load key 'v'.\r\n\r\nThe exception appears when loading both `force_reload` set to `True` and `False`.\r\nCan anybody offer some help?\r\n\r\n\r\n\r\n\r\n\r\n## Exception log:\r\n```sh\r\nStartin server...\r\nLoading model:  yolov5m.pt\r\nDownloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /home/pmaillard/.cache/torch/hub/master.zip\r\nYOLOv5 ðŸš€ 2022-4-27 torch 1.10.0+cu113 CUDA:0 (NVIDIA TITAN RTX, 24220MiB)\r\n\r\nFusing layers...\r\nModel summary: 378 layers, 35254692 parameters, 0 gradients, 49.0 GFLOPs\r\nAdding AutoShape...\r\nDownloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /home/pmaillard/.cache/torch/hub/master.zip\r\nYOLOv5 ðŸš€ 2022-4-27 torch 1.10.0+cu113 CUDA:0 (NVIDIA TITAN RTX, 24220MiB)\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/pmaillard/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 47, in _create\r\n    model = DetectMultiBackend(path, device=device)  # download/load FP32 model\r\n  File \"/home/pmaillard/.cache/torch/hub/ultralytics_yolov5_master/models/common.py\", line 307, in __init__\r\n    model = attempt_load(weights if isinstance(weights, list) else w, map_location=device)\r\n  File \"/home/pmaillard/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py\", line 96, in attempt_load\r\n    ckpt = torch.load(attempt_download(w), map_location=map_location)  # load\r\n  File \"/home/pmaillard/github/student-object-classification/Source/Server/.venv/lib/python3.8/site-packages/torch/serialization.py\", line 608, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/home/pmaillard/github/student-object-classification/Source/Server/.venv/lib/python3.8/site-packages/torch/serialization.py\", line 777, in _legacy_load\r\n    magic_number = pickle_module.load(f, **pickle_load_args)\r\n_pickle.UnpicklingError: invalid load key, 'v'.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"server.py\", line 174, in <module>\r\n    models = load_models()\r\n  File \"server.py\", line 132, in load_models\r\n    torch.hub.load('ultralytics/yolov5',\r\n  File \"/home/pmaillard/github/student-object-classification/Source/Server/.venv/lib/python3.8/site-packages/torch/hub.py\", line 399, in load\r\n    model = _load_local(repo_or_dir, model, *args, **kwargs)\r\n  File \"/home/pmaillard/github/student-object-classification/Source/Server/.venv/lib/python3.8/site-packages/torch/hub.py\", line 428, in _load_local\r\n    model = entry(*args, **kwargs)\r\n  File \"/home/pmaillard/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 71, in custom\r\n    return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\r\n  File \"/home/pmaillard/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\", line 66, in _create\r\n    raise Exception(s) from e\r\nException: invalid load key, 'v'.. Cache may be out of date, try `force_reload=True` or see https://github.com/ultralytics/yolov5/issues/36 for help.\r\n```\r\n\r\n\r\n## Loading model in server:\r\n```py\r\n    for r, d, f in os.walk(MODELS_DIRECTORY):\r\n        for file in f:\r\n            if \".pt\" not in file:\r\n                continue\r\n\r\n            name = os.path.splitext(file)[0]\r\n            print(\"Loading model: \", name)\r\n            dict_of_models[name] = \\\r\n                torch.hub.load('ultralytics/yolov5',\r\n                               'custom',\r\n                               path=os.path.join(r, file),\r\n                               force_reload=True,\r\n                               autoshape=True)\r\n```\r\n\r\n\r\n\n\n### Environment\n\n- Python 3.8.10\r\n- Ubuntu 20.04.3 LTS\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "pasmai",
        "body": "The issue was that the models were stored in Git LFS.\r\n\r\nFix by downloading them manually with `git pull lfs`."
      },
      {
        "user": "nqa112",
        "body": "> The issue was that the models were stored in Git LFS.\r\n> \r\n> Fix by downloading them manually with `git pull lfs`.\r\n\r\nThanks a lot !\r\nTo anyone who also faces this error, on Google Colab, I have a LFS yolo weights file in my repo, just do this to get all LFS file in repo:\r\n```\r\n!git clone [repo name you want to clone]\r\n!git lfs pull\r\n```\r\nThe output, in my case, should be this:\r\n```\r\nGit LFS: (1 of 1 files) 160.20 MB / 160.20 MB\r\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@nqa112 you're welcome! Thanks for sharing your solution for downloading LFS files in your repo. It will be helpful for others encountering a similar issue. If you have any more questions or need further assistance, feel free to ask!"
      }
    ]
  },
  {
    "issue_number": 13477,
    "title": "save-csv command header bug",
    "author": "aligh993",
    "state": "closed",
    "created_at": "2024-12-30T21:09:47Z",
    "updated_at": "2025-01-03T09:36:08Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\r\n\r\n\r\n### YOLOv5 Component\r\n\r\nDetection\r\n\r\n### Bug\r\n\r\nThe save-csv command did not write the CSV header. \r\nFor Example: $ python detect.py --weights yolov5m.pt --source 0 --save-csv\r\n[predictions.csv](https://github.com/user-attachments/files/18277768/predictions.csv)\r\n\r\n\r\n### Environment\r\n\r\n_No response_\r\n\r\n### Minimal Reproducible Example\r\n\r\n_No response_\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @aligh993, thank you for reporting this potential issue with YOLOv5 ðŸš€! \n\nIf this is indeed a ðŸ› bug, we kindly ask you to provide a **minimum reproducible example (MRE)**. This will greatly help us investigate and resolve the issue more efficiently. An MRE typically includes a clear description of the steps taken, the exact commands or code used, example data if applicable, and the observed versus expected behavior.\n\nYour willingness to submit a pull request is fantastic! An Ultralytics engineer will also review this issue and assist you shortly ðŸ˜Š."
      },
      {
        "user": "pderrenger",
        "body": "@aligh993 thank you for reporting this issue! It seems like the CSV header might not be written when using the `to_csv` method. Could you verify if you are using the latest YOLOv5 version? If the issue persists, you may consider customizing the `to_csv` method by passing additional arguments, such as `header=True`. For example:\n\n```python\ncsv_result = results[0].to_csv(header=True)\n```\n\nIf this doesn't resolve your issue, please feel free to provide a minimal reproducible example, and we'll assist further. Looking forward to your PR if you find a fix!"
      },
      {
        "user": "aligh993",
        "body": "@pderrenger thank you so much. Yes, I'm using the latest version of YOLOv5, but the issue still persists. I found a fix in 'detect.py' and requested a PR at this link: https://github.com/ultralytics/yolov5/pull/13472"
      }
    ]
  },
  {
    "issue_number": 13481,
    "title": "Error While using onnxruntime GPU Inference in cpp",
    "author": "Abish7",
    "state": "open",
    "created_at": "2025-01-03T04:06:20Z",
    "updated_at": "2025-01-03T07:22:40Z",
    "labels": [
      "bug",
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nDetection\n\n### Bug\n\n[E:onnxruntime:, sequential_executor.cc:368 onnxruntime::SequentialExecutor::Execute] Non-zero status code returned while running Sigmoid node. Name:'/model.0/act/Sigmoid' Status Message: CUDA error cudaErrorNoKernelImageForDevice:no kernel image is available for execution on the device                                 Non-zero status code returned while running Sigmoid node. Name:'/model.0/act/Sigmoid' Status Message: CUDA error cudaErrorNoKernelImageForDevice:no kernel image is available for execution on the device \r\n\r\nI am getting this while session run.\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Abish7, thank you for your interest in YOLOv5 ðŸš€! To help us assist you better, please ensure you provide a **minimum reproducible example (MRE)** that we can use to debug the issue you are facing with ONNX GPU inference. An example could include details such as:\n\n1. A small snippet of the code you are using.\n2. The specific GPU and CUDA version you have installed.\n3. Any modifications you may have made to the YOLOv5 repository or exported ONNX model.\n4. A description of the steps to reproduce the error.\n\nFor your environment, ensure you are using up-to-date dependencies, including Python, PyTorch, and CUDA. Installing the dependencies specified in the `requirements.txt` file and matching system CUDA with your PyTorch version are critical for compatibility.\n\nYou can also try running YOLOv5 in verified environments such as Google Colab, Paperspace, Docker, or similar setups for compliance. If possible, test your workflow there to rule out any environment-specific issues.\n\nThis is an automated response to guide you, and an Ultralytics engineer will review your issue and assist you further soon! Let us know if you have more details to share ðŸ˜ŠðŸš€"
      },
      {
        "user": "Abish7",
        "body": "![Screenshot 2025-01-03 094202](https://github.com/user-attachments/assets/2a0599f3-71c1-426b-8990-aff4568644c8)\r\n\r\nGPU : NVIDIA Geforce GT 710\r\nCUDA Version : 11.4\r\nOnnxRuntime GPU : 1.12.1\r\nwhile exporting i have changed the model size to 2016\r\nwhile running onnxruntime gpu model is loaded in gpu but in session.run it crashes."
      },
      {
        "user": "pderrenger",
        "body": "@Abish7 the error indicates that your GPU, the NVIDIA GeForce GT 710, does not support CUDA compute capability required by ONNX Runtime GPU kernels. The GT 710 has a compute capability of 3.5, while ONNX Runtime GPU typically requires a minimum of 5.0. Unfortunately, you'll need a more capable GPU or switch to CPU inference for compatibility. For more details, refer to the [ONNX Runtime GPU requirements](https://onnxruntime.ai/docs/build/eps.html#cuda) and verify compatibility."
      }
    ]
  },
  {
    "issue_number": 13478,
    "title": "Is possible to converting a .pt PyTorch model to a .pth file ?",
    "author": "chathuryaprasad",
    "state": "open",
    "created_at": "2024-12-31T06:55:55Z",
    "updated_at": "2024-12-31T15:00:51Z",
    "labels": [
      "question",
      "exports"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nIs possible to converting a .pt PyTorch model to a .pth file ?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @CPrasa, thank you for your interest in YOLOv5 ðŸš€! Converting a `.pt` PyTorch model to a `.pth` file is an interesting question. \n\nIf this is a â“ Question, please provide as much detail as possible about your use case or requirements. This will help us provide better guidance. If this is a ðŸ› Bug Report, we kindly request a **minimum reproducible example (MRE)** to help us debug and assist you more effectively.\n\n## Requirements\n\nEnsure you are using **Python>=3.8.0** and have all required dependencies installed, including **PyTorch>=1.8**. To set up YOLOv5:\n```bash\ngit clone the repository from the source  # clone\ncd into the yolov5 directory\npip install required dependencies listed in requirements.txt\n```\n\n## Environments\n\nYOLOv5 can run on various verified environments including Jupyter Notebooks with free GPU runtime, cloud platforms like Google Cloud or AWS, or in a Docker environment with pre-installed dependencies.\n\n## Status\n\nYou can check the status of YOLOv5's integrations, including training, validation, and inference workflows, to ensure functionality is operational.\n\nThis is an automated response to help guide you, but no worriesâ€”a human Ultralytics engineer will take a closer look and assist you further soon! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@CPrasa yes, a `.pt` PyTorch model can be converted to a `.pth` file as both formats are interchangeable and represent serialized PyTorch models. You can achieve this by loading the `.pt` model using PyTorch and saving it directly as a `.pth` file using `torch.save`. Here's an example:\n\n```python\nimport torch\n\n# Load the .pt model\nmodel = torch.load('model.pt')\n\n# Save as .pth\ntorch.save(model, 'model.pth')\n```\n\nLet me know if you encounter any issues!"
      }
    ]
  },
  {
    "issue_number": 13476,
    "title": "mAP50è®¡ç®—ä¸Žiou_thresçš„å…³ç³»",
    "author": "lqh964165950",
    "state": "open",
    "created_at": "2024-12-30T14:19:02Z",
    "updated_at": "2024-12-31T04:41:46Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nmAP50è®¡ç®—ä¸Žiou_thresæœ‰ä»€ä¹ˆå…³ç³»ï¼Œè¿è¡Œval.pyæ–‡ä»¶æ—¶ï¼Œæ”¹å˜iou_thresä¸º0.5ï¼Œç²¾åº¦ä¸Šå‡äº†ï¼Œå¬å›žçŽ‡ç¨å¾®ä¸‹é™äº†ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ\r\n![iou_thresh](https://github.com/user-attachments/assets/6202a939-d8f6-47b8-bc56-e39b2de25b4e)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @lqh964165950, thank you for your interest in YOLOv5 ðŸš€! The relationship between mAP@0.5 and `iou_thres` is an important topic in evaluating object detection performance.\n\nIf this is a â“ question regarding custom evaluations or training, please provide as much detail as possible, such as specific evaluation settings, modifications made to the `val.py` script, and any relevant logs or outputs, to help us interpret your observations.\n\nIf this is a ðŸ› Bug Report, please include a **minimum reproducible example (MRE)** detailing the exact steps you followed, along with the environment and version information, to help us identify the cause.\n\n## Requirements\n\nEnsure you're using **Python>=3.8.0** with all requirements installed. To reinstall dependencies, you can run:\n```bash\npip install -r requirements.txt\n```\n\n## Environments\n\nYOLOv5 supports various development environments, including Jupyter Notebooks, cloud platforms, and Docker. Please verify compatibility and ensure all dependencies are correctly installed to avoid potential issues.\n\n## Status\n\nMake sure that your development environment is up-to-date. The YOLOv5 repository includes Continuous Integration (CI) testing to verify correctness across multiple operations, including training, validation, inference, export, and benchmarks on common operating systems.\n\nThis is an automated response to assist you promptly. An Ultralytics engineer will review your issue and provide further assistance soon. Let us know if you have additional information or clarifications in the meantime! ðŸ˜Š"
      },
      {
        "user": "LZLwoaini",
        "body": "MAPæ˜¯æ‰€æœ‰ç±»åˆ«APçš„å‡å€¼ï¼ŒAPç”±ç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡ç¡®å®šï¼Œè€ŒIoU é˜ˆå€¼ã€confidence ï¼ˆç½®ä¿¡åº¦ï¼‰é˜ˆå€¼å½±å“ç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡çš„è®¡ç®—ã€‚"
      },
      {
        "user": "pderrenger",
        "body": "@LZLwoaini æ‚¨æåˆ°å¾—å¾ˆæ­£ç¡®ï¼ŒmAP æ˜¯æ‰€æœ‰ç±»åˆ« AP çš„å‡å€¼ï¼Œè€Œ AP æ˜¯ç”±ç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡å†³å®šçš„ã€‚è¿è¡Œ `val.py` æ—¶æ›´æ”¹ IoU é˜ˆå€¼ä¼šå½±å“ç²¾ç¡®çŽ‡ä¸Žå¬å›žçŽ‡çš„å¹³è¡¡ï¼šæé«˜ IoU é˜ˆå€¼ï¼ˆå¦‚ 0.5ï¼‰ï¼Œä¼šå€¾å‘äºŽæ›´ä¸¥æ ¼çš„åŒ¹é…æ ‡å‡†ï¼Œé€šå¸¸ä¼šæé«˜ç²¾ç¡®çŽ‡ä½†ç¨å¾®é™ä½Žå¬å›žçŽ‡ï¼Œå› ä¸ºæ£€æµ‹æ¡†éœ€è¦æ›´é«˜çš„é‡å åº¦æ‰èƒ½è¢«è§†ä¸ºæ­£ç¡®ã€‚æ‚¨å¯ä»¥æ ¹æ®å…·ä½“åº”ç”¨éœ€æ±‚è°ƒæ•´ IoU é˜ˆå€¼ä»¥ä¼˜åŒ–ç»“æžœã€‚æ›´å¤šå…³äºŽ mAP å’Œ IoU çš„ä¿¡æ¯ï¼Œè¯·å‚è€ƒ [Ultralytics Glossary: mAP](https://www.ultralytics.com/glossary/mean-average-precision-map)ã€‚"
      }
    ]
  },
  {
    "issue_number": 13466,
    "title": "detect  GPU data-stream",
    "author": "LZLwoaini",
    "state": "open",
    "created_at": "2024-12-18T07:18:38Z",
    "updated_at": "2024-12-30T16:01:21Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHow to check the data-stream during GPU environment inference, such as which data is parallel and which data is serial. In other words, which part of the data is accelerated by GPU. Thanksï¼ï¼\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @LZLwoaini, thank you for your interest in YOLOv5 ðŸš€! It looks like you are asking about data streams and GPU environment inference. An Ultralytics engineer will review your question and assist you soon.\n\nIn the meantime, please note the following to assist with any debugging or inquiries:\n\n- If this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us understand and debug the issue.\n- If this is a custom training â“ Question, please give as much detail as possible, including dataset image examples, training logs, and the exact steps youâ€™ve followed. Ensure youâ€™re adhering to best practices for training efficiency and performance.\n  \nTo ensure smooth operation, make sure youâ€™re using **Python>=3.8** and have all required dependencies installed, including **PyTorch>=1.8**. You can install these dependencies via the repository's `requirements.txt` file. \n\nWe support various environments for running YOLOv5, including notebooks, cloud platforms, and Docker. Please ensure your environment is fully set up and updated for optimal GPU utilization.\n\nLet us know if you need further clarification, and thank you for using YOLOv5 ðŸŒŸ!"
      },
      {
        "user": "pderrenger",
        "body": "@LZLwoaini to analyze the GPU data stream during inference and determine which operations are parallel or serial, you can use profiling tools like [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) or PyTorch's [autograd profiler](https://pytorch.org/tutorials/recipes/recipes/profiler.html). These tools allow you to visualize GPU utilization and identify which parts of the process are GPU-accelerated. For YOLOv5 specifically, ensure you run inference with `device='cuda'` to leverage GPU acceleration. Let us know if you encounter any issues!"
      },
      {
        "user": "LZLwoaini",
        "body": "> @LZLwoaini to analyze the GPU data stream during inference and determine which operations are parallel or serial, you can use profiling tools like [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) or PyTorch's [autograd profiler](https://pytorch.org/tutorials/recipes/recipes/profiler.html). These tools allow you to visualize GPU utilization and identify which parts of the process are GPU-accelerated. For YOLOv5 specifically, ensure you run inference with `device='cuda'` to leverage GPU acceleration. Let us know if you encounter any issues!\r\n\r\nOK!ï¼Thank you for your answer, I will give it a try."
      }
    ]
  },
  {
    "issue_number": 11555,
    "title": "convert to tflite",
    "author": "nurulamelia2",
    "state": "closed",
    "created_at": "2023-05-20T10:08:32Z",
    "updated_at": "2024-12-30T13:37:38Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nhello, I want to ask. How do you convert best.pt to tflite using yolov5 on google collab? I've tried converting, but after running the result is different. \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @nurulamelia2, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@nurulamelia2 hi there! Thank you for your interest in YOLOv5. To convert best.pt to tflite, you can use the `export.py` script with the `--weights` argument to specify the path to your `.pt` file and `--train` to specify the name of the exported `.tflite` file. Here is an example command to export to tflite:\n\n```\n!python export.py --weights /path/to/best.pt --img 640 --batch 1 --train --name yolov5s-export\n```\n\nAfter running this command, you will have a `yolov5s-export.tflite` file in the `yolov5s-export` folder. Please note that this is just a suggested command, and you might need to adjust the arguments based on your specific use case. Let me know if you have any other questions."
      },
      {
        "user": "nurulamelia2",
        "body": "sorry I want to ask again, after I did a run like this \r\n#!python export.py --weights runs/train/yolov5s_results/weights/best.pt --include tflite\r\nthere is indeed a .tflite file but it is not my model. My model is sign language while when I try to check the results the file is a model that shows object detection 'car', and others. While my best.pt model is correct located here runs/train/yolov5s_results/weights/best.pt. What is the solution?\r\n\r\nI tried !python export.py --weights runs/train/yolov5s_results/weights/best.pt --img 640 --batch 16 --train --name yolov5s_results, but it doesn't work either.\r\n\r\nCan you provide a solution for me? thank you in advance"
      }
    ]
  },
  {
    "issue_number": 8468,
    "title": "Inconsistent detections after custom training",
    "author": "kaminetzky",
    "state": "closed",
    "created_at": "2022-07-04T17:08:31Z",
    "updated_at": "2024-12-27T13:17:03Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nHey there!\r\n\r\nI'm training a YOLOv5 network for detecting wrenches in X-ray images of baggage. I'm training YOLOv5X with thousands of images for 300 epochs. Training images contain simulated wrenches while testing images contain real wrenches. All parameters are the default ones except for the batch size (set to 36 for 3 GPUs).\r\n\r\nI noticed in my last run that tiny differences between images result in considerably different predictions. For example, the viewing angle changes slightly or the image is cropped differently. Here are some examples:\r\n\r\n![P05561](https://user-images.githubusercontent.com/11022568/177195376-625c0744-e98d-41c8-be00-d0ee674a4e81.jpg)\r\n![P05562](https://user-images.githubusercontent.com/11022568/177195381-918689e4-e163-4dfd-b2aa-b852137b1ff7.jpg)\r\n\r\n![P01297](https://user-images.githubusercontent.com/11022568/177195469-aad58941-497d-4150-abf6-5c2063d43f20.jpg)\r\n![P01298](https://user-images.githubusercontent.com/11022568/177195492-d60c08d8-8cec-4fc9-97d7-909f66b1b6fe.jpg)\r\n\r\nHas anybody faced a similar issue or know how to fix it? Any help is appreciated.\r\n\r\nThanks!\r\n\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @kaminetzky, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"CI CPU testing\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), validation ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@kaminetzky ðŸ‘‹ Hello! Thanks for asking about improving YOLOv5 ðŸš€ training results. \r\n\r\nMost of the time good results can be obtained with no changes to the models or training settings, **provided your dataset is sufficiently large and well labelled**. If at first you don't get good results, there are steps you might be able to take to improve, but we always recommend users **first train with all default settings** before considering any changes. This helps establish a performance baseline and spot areas for improvement.\r\n\r\nIf you have questions about your training results **we recommend you provide the maximum amount of information possible** if you expect a helpful response, including results plots (train losses, val losses, P, R, mAP), PR curve, confusion matrix, training mosaics, test results and dataset statistics images such as labels.png. All of these are located in your `project/name` directory, typically `yolov5/runs/train/exp`.\r\n\r\nWe've put together a full guide for users looking to get the best results on their YOLOv5 trainings below.\r\n\r\n## Dataset\r\n\r\n- **Images per class.** â‰¥ 1500 images per class recommended\r\n- **Instances per class.** â‰¥ 10000 instances (labeled objects) per class recommended\r\n- **Image variety.** Must be representative of deployed environment. For real-world use cases we recommend images from different times of day, different seasons, different weather, different lighting, different angles, different sources (scraped online, collected locally, different cameras) etc.\r\n- **Label consistency.** All instances of all classes in all images must be labelled. Partial labelling will not work.\r\n- **Label accuracy.** Labels must closely enclose each object. No space should exist between an object and it's bounding box. No objects should be missing a label.\r\n- **Label verification.** View `train_batch*.jpg` on train start to verify your labels appear correct, i.e. see [example](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#local-logging) mosaic.\r\n- **Background images.** Background images are images with no objects that are added to a dataset to reduce False Positives (FP). We recommend about 0-10% background images to help reduce FPs (COCO has 1000 background images for reference, 1% of the total). No labels are required for background images.\r\n\r\n<a href=\"https://arxiv.org/abs/1405.0312\"><img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/109398377-82b0ac00-78f1-11eb-9c76-cc7820669d0d.png\" alt=\"COCO Analysis\"></a>\r\n\r\n\r\n## Model Selection\r\n\r\nLarger models like YOLOv5x and [YOLOv5x6](https://github.com/ultralytics/yolov5/releases/tag/v5.0) will produce better results in nearly all cases, but have more parameters, require more CUDA memory to train, and are slower to run. For **mobile** deployments we recommend YOLOv5s/m, for **cloud** deployments we recommend YOLOv5l/x. See our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints) for a full comparison of all models. \r\n\r\n<p align=\"center\"><img width=\"700\" alt=\"YOLOv5 Models\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png\"></p>\r\n\r\n- **Start from Pretrained weights.** Recommended for small to medium sized datasets (i.e. [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml)). Pass the name of the model to the `--weights` argument. Models download automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases).\r\n```bash\r\npython train.py --data custom.yaml --weights yolov5s.pt\r\n                                             yolov5m.pt\r\n                                             yolov5l.pt\r\n                                             yolov5x.pt\r\n                                             custom_pretrained.pt\r\n```\r\n- **Start from Scratch.** Recommended for large datasets (i.e. [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [OIv6](https://storage.googleapis.com/openimages/web/index.html)). Pass the model architecture yaml you are interested in, along with an empty `--weights ''` argument:\r\n```bash\r\npython train.py --data custom.yaml --weights '' --cfg yolov5s.yaml\r\n                                                      yolov5m.yaml\r\n                                                      yolov5l.yaml\r\n                                                      yolov5x.yaml\r\n```\r\n\r\n\r\n## Training Settings\r\n\r\nBefore modifying anything, **first train with default settings to establish a performance baseline**. A full list of train.py settings can be found in the [train.py](https://github.com/ultralytics/yolov5/blob/master/train.py) argparser.\r\n\r\n- **Epochs.** Start with 300 epochs. If this overfits early then you can reduce epochs. If overfitting does not occur after 300 epochs, train longer, i.e. 600, 1200 etc epochs.\r\n- **Image size.** COCO trains at native resolution of `--img 640`, though due to the high amount of small objects in the dataset it can benefit from training at higher resolutions such as `--img 1280`. If there are many small objects then custom datasets will benefit from training at native or higher resolution. Best inference results are obtained at the same `--img` as the training was run at, i.e. if you train at `--img 1280` you should also test and detect at `--img 1280`.\r\n- **Batch size.** Use the largest `--batch-size` that your hardware allows for. Small batch sizes produce poor batchnorm statistics and should be avoided.\r\n- **Hyperparameters.** Default hyperparameters are in [hyp.scratch-low.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-low.yaml). We recommend you train with default hyperparameters first before thinking of modifying any. In general, increasing augmentation hyperparameters will reduce and delay overfitting, allowing for longer trainings and higher final mAP. Reduction in loss component gain hyperparameters like `hyp['obj']` will help reduce overfitting in those specific loss components. For an automated method of optimizing these hyperparameters, see our [Hyperparameter Evolution Tutorial](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\r\n\r\n## Further Reading\r\n\r\nIf you'd like to know more a good place to start is Karpathy's 'Recipe for Training Neural Networks', which has great ideas for training that apply broadly across all ML domains: http://karpathy.github.io/2019/04/25/recipe/\r\n\r\nGood luck ðŸ€ and let us know if you have any other questions!"
      },
      {
        "user": "kaminetzky",
        "body": "Hey Glenn! Thanks for the quick reply. I'm following all YOLO training tips. I'll add the requested data ASAP.\r\n\r\nI just find it odd that tiny changes between the images result in significant differences in the predictions. I thought that maybe somebody has seen something like this."
      }
    ]
  },
  {
    "issue_number": 8208,
    "title": "Save wrong predictions on validation ",
    "author": "arslanan",
    "state": "closed",
    "created_at": "2022-06-14T11:57:02Z",
    "updated_at": "2024-12-26T10:03:53Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi, \r\nSo i trained a model that seems to be working quite well and I would like to inspect the remaining errors on the test set. \r\nI tried to use detect.py to save images with bounding boxes but my test set is large and most results are good. \r\n\r\nIs there a way (with val.py i guess) to output filename of wrong predictions ? \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @arslanan, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"CI CPU testing\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), validation ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@arslanan val.py operates per IoU threshold and per class, but not per image yet, which is what would allow for a per-image results output you mention. This is in our pipeline for later this year, but if you'd like to contribute with a PR to val.py for this functionality that would be a great help for all other users in your shoes.\r\n\r\nPlease see our âœ… [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started."
      },
      {
        "user": "arslanan",
        "body": "Thanks for the answer! "
      }
    ]
  },
  {
    "issue_number": 13469,
    "title": "How to compute loss using eval mode in val. py file for YOLOv5",
    "author": "BIT-QiuYu",
    "state": "open",
    "created_at": "2024-12-22T06:04:25Z",
    "updated_at": "2024-12-22T19:19:42Z",
    "labels": [
      "question",
      "research"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nDue to research requirements, I need to calculate the loss function value for each input image in the `eval mode` of the YOLO V5 model\r\nI modified the `run` function in the `val. py` file\r\nThe `compute_loss` variable was specified as `ComputeLoss (model)` in it\r\n```python\r\n    # Configure\r\n    model.eval()\r\n    compute_loss = ComputeLoss(model)\r\n    cuda = device.type != \"cpu\"\r\n    is_coco = isinstance(data.get(\"val\"), str) and data[\"val\"].endswith(f\"coco{os.sep}val2017.txt\")  # COCO dataset\r\n    nc = 1 if single_cls else int(data[\"nc\"])  # number of classes\r\n    iouv = torch.linspace(0.5, 0.95, 10, device=device)  # iou vector for mAP@0.5:0.95\r\n    niou = iouv.numel()\r\n```\r\nThe following error will occurï¼š\r\n```python\r\nTraceback (most recent call last):\r\n  File \"val.py\", line 626, in <module>\r\n    main(opt)\r\n  File \"val.py\", line 597, in main\r\n    run(**vars(opt))\r\n  File \"xxxxxxxxx/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"val.py\", line 299, in run\r\n    compute_loss = ComputeLoss(model)\r\n                   ^^^^^^^^^^^^^^^^^^\r\n  File \"utils/loss.py\", line 115, in __init__\r\n    h = model.hyp  # hyperparameters\r\n        ^^^^^^^^^\r\n  File \"xxxxxxxxxx/.local/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1931, in __getattr__\r\n    raise AttributeError(\r\nAttributeError: 'DetectMultiBackend' object has no attribute 'hyp'\r\n```\r\nWhen I imitate the training mode and adding the 'hyp' attribute to the YOLOv5 model using 'data/hyps/hyp.satch-low-yaml' will result in the following errorï¼š\r\n```python\r\nTraceback (most recent call last):\r\n  File \"val.py\", line 626, in <module>\r\n    main(opt)\r\n  File \"val.py\", line 597, in main\r\n    run(**vars(opt))\r\n  File \"xxxxxxx/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"val.py\", line 299, in run\r\n    compute_loss = ComputeLoss(model)\r\n                   ^^^^^^^^^^^^^^^^^^\r\n  File \"utils/loss.py\", line 129, in __init__\r\n    m = de_parallel(model).model[-1]  # Detect() module\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^\r\nTypeError: 'DetectionModel' object is not subscriptable\r\n```\r\nI really need the loss value.I look forward to your reply. I would be extremely grateful\r\nIf it's not possible to directly modify `val. py` to achieve the goal, use \r\n```python\r\ntorch.hub.load(\"yolo.pt\")\r\n```\r\n or \r\n```python\r\nfrom ultralytics import YOLO\r\nModel=YOLO (\"yolo5. pt\")\r\n```\r\n and other methods can achieve the goalï¼Œ I also look forward to your reply. I would greatly appreciate it\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @BIT-QiuYu, thank you for your interest in YOLOv5 ðŸš€! \n\nFor research use cases like yours, itâ€™s great to see modifications aimed at customized loss calculations using `val.py`. From the error messages provided, it appears that there may be discrepancies in how model attributes and the loss computation are being accessed or initialized. \n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example (MRE)**, including detailed steps that consistently reproduce the issue. This will help us debug the exact behaviors and assist you better.\n\nIf this is a custom training â“ Question, please ensure you share as much contextual information as possible, including the specific YOLOv5 version being used, the hyperparameters file (if modified), and any environment details such as Python and PyTorch versions. Verifying that you are following the best practices for custom modifications may also help avoid potential pitfalls.\n\n## Requirements\nEnsure you have the following:\n- Python>=3.8.0 \n- PyTorch>=1.8 \n- All YOLOv5 repository dependencies installed using `pip install -r requirements.txt`.\n\n## Environments\nYOLOv5 supports various environments, such as Jupyter Notebooks with a GPU, cloud environments like Google Cloud and AWS, and running inside Docker containers. Testing across multiple environments may help isolate the issue.\n\n## Status\nYou can also confirm the functionality of your YOLOv5 installation by checking the status of the Continuous Integration (CI) tests, which verify training, validation, inference, export, and benchmarking processes.\n\nThis is an automated response to guide you better on your query. An Ultralytics engineer will take a detailed look and assist you further shortly ðŸ˜Š."
      },
      {
        "user": "pderrenger",
        "body": "To compute the loss in evaluation mode using YOLOv5, you can adjust your approach as follows:\n\n1. **Issue with `model.hyp`**: The `DetectMultiBackend` object does not contain the `hyp` attribute by default in evaluation mode. To resolve this, manually load the hyperparameters (e.g., `hyp.scratch-low.yaml`) and attach them to the model:\n    ```python\n    from utils.general import check_file\n    from utils.yaml import yaml_load\n\n    model.hyp = yaml_load(check_file('data/hyps/hyp.scratch-low.yaml'))  # Load hyperparameters\n    ```\n\n2. **Issue with `model[-1]`**: The error `DetectionModel object is not subscriptable` occurs because `de_parallel(model).model` is not correctly referencing the detection layer. Instead, ensure you pass the correct model structure:\n    ```python\n    from utils.general import de_parallel\n    compute_loss = ComputeLoss(de_parallel(model))  # Ensure correct model is passed\n    ```\n\n3. **Alternative Approach**: Using the `Ultralytics YOLO` API is simpler and avoids modifying `val.py`. You can compute losses during validation by iterating over the validation dataset and manually using the `ComputeLoss` class:\n    ```python\n    from ultralytics import YOLO\n    from utils.loss import ComputeLoss\n    from utils.datasets import create_dataloader\n    from utils.general import check_dataset\n\n    model = YOLO('yolov5s.pt').model  # Load YOLOv5 model\n    model.hyp = yaml_load('data/hyps/hyp.scratch-low.yaml')  # Load hyperparameters\n    compute_loss = ComputeLoss(model)\n\n    # Load validation data\n    data = check_dataset('data/coco128.yaml')  # Update with your dataset\n    dataloader = create_dataloader(data['val'], 640, 16, 0, pad=0.5, rect=True, prefix='val: ')[0]\n\n    # Compute loss for each batch\n    model.eval()\n    for batch in dataloader:\n        imgs, targets, paths, shapes = batch\n        preds = model(imgs)  # Forward pass\n        loss, _ = compute_loss(preds, targets)  # Compute loss\n        print(f'Loss: {loss}')\n    ```\n\nThis method avoids directly modifying `val.py` and uses the existing API effectively.\n\nIf you encounter further issues, ensure you are using the latest YOLOv5 version from the [repository](https://github.com/ultralytics/yolov5). Let us know if you need additional guidance!"
      }
    ]
  },
  {
    "issue_number": 13416,
    "title": "Loss computation sometimes cause nan values",
    "author": "tobymuller233",
    "state": "open",
    "created_at": "2024-11-15T08:42:56Z",
    "updated_at": "2024-12-22T17:30:19Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nTraining\n\n### Bug\n\nThese days when I'm trying to fine tune my model after pruning by training for several epochs, I found that loss value becomes nan from time to time. By setting breakpoints and checking, I found that there's a bug in [metrics.py](https://github.com/ultralytics/yolov5/blob/1435a8eed6b16d125e7808c81969a0c879d6b8a0/utils/metrics.py#L239)\r\nSometimes, if the prediction of some bounding box has a width or height of 0, it turns out to be nan values! Since in CIoU computation, h2 and h1 are used as dividers [here](https://github.com/ultralytics/yolov5/blob/1435a8eed6b16d125e7808c81969a0c879d6b8a0/utils/metrics.py#L265).\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @tobymuller233, thank you for your interest in YOLOv5 ðŸš€! It seems like you're encountering a nan values issue during training, and there might be a potential bug in the `metrics.py` file. To assist, we'll need a bit more information.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us understand and debug the issue. This would include steps to replicate the bug, relevant sections of your code, and any specific error messages.\n\nAdditionally, it would be helpful to know more about your environment setup, such as the version of Python, PyTorch, and any other dependencies you are using.\n\nIf you have any further insights, like dataset characteristics or specific conditions that might trigger this issue, do share those as well.\n\nPlease note that this is an automated response, and an Ultralytics engineer will review your issue and provide further assistance soon. Thank you for your patience and help in improving YOLOv5! ðŸš€âœ¨"
      },
      {
        "user": "pderrenger",
        "body": "@tobymuller233 thank you for reporting this potential issue with loss computation. You've identified an important edge case where predictions with zero width or height could cause NaN values during CIoU loss calculation.\n\nBefore proceeding with a PR, please verify this behavior using the latest version of YOLOv5 as there have been several loss computation improvements. If you can provide a minimal reproducible example (MRE) following our [MRE guide](https://docs.ultralytics.com/help/minimum_reproducible_example/), it would help us investigate the issue more effectively.\n\nFor now, you could add a small epsilon value to prevent division by zero in the height calculations. However, we should also investigate why the model is predicting zero-sized bounding boxes during training, as this may indicate other underlying issues with the training process or data.\n\nIf you'd like to submit a PR, please ensure it includes:\n1. The MRE demonstrating the issue\n2. Your proposed fix\n3. Test cases verifying the solution"
      },
      {
        "user": "DianaMaz",
        "body": "@pderrenger I've also encountered the same edge case. \r\n\r\nHere's how I ran into this issue:\r\nI'm training on a dataset called \"RarePlanes\" (The real section of the dataset). At the beginning, I trained a model using the Silu activation function. Now, I'm fine-tuning the same model just with LeakyRelu as the activation function, and I'm using the previous Silu-trained model as my initial weights. \r\nWhen I'm training the Leakyrelu model (yolov3-tiny model, lr0=0.01, with AdamW optimzier, with the same hyp as scratch low) I'm consisently encountering NAN gradients, after just one batch. \r\n\r\nThe solution I've found is, I've modified the bbox_iou by adding an eplison to the width and height to prevent them from being zero. This resolves the NAN issue, but of course it affects the training results.\r\n"
      }
    ]
  },
  {
    "issue_number": 13468,
    "title": "Bug Report",
    "author": "gitover22",
    "state": "closed",
    "created_at": "2024-12-21T14:45:59Z",
    "updated_at": "2024-12-21T14:47:16Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nValidation\n\n### Bug\n\nin MLU370 driverï¼šcant find module \n\n### Environment\n\nUbuntu20.04\r\nMLU370 M8\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "gitover22",
        "body": "sorryï¼Œerror in me"
      },
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @gitover22, thank you for your interest in YOLOv5 ðŸš€! Please explore our Tutorials to get started, where you can find quickstart guides for simple tasks like Custom Data Training all the way to advanced concepts like Hyperparameter Evolution.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it, as no MRE was provided in your initial report.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our tips for optimal training results.\n\n## Requirements\n\n**Python>=3.8.0** with all requirements installed, including **PyTorch>=1.8**. To get started:\n```bash\ngit clone the YOLOv5 repository  # clone\ncd into the cloned directory\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in environments such as Notebooks with free GPUs, Google Cloud Deep Learning VMs, Amazon Deep Learning AMIs, and Docker Images. Please ensure your environment is up to date with all dependencies preinstalled, including CUDA, CUDNN, Python, and PyTorch.\n\n## Status\n\nIf all YOLOv5 Continuous Integration (CI) tests on GitHub Actions are currently passing, this indicates the correct operation of YOLOv5 training, validation, inference, export, and benchmarks on macOS, Windows, and Ubuntu.\n\nThank you for raising this issue! Please note that this is an automated response, and an Ultralytics engineer will assist you shortly ðŸ˜Š"
      }
    ]
  },
  {
    "issue_number": 22,
    "title": "No module named 'models'",
    "author": "yxxxqqq",
    "state": "closed",
    "created_at": "2020-06-07T12:08:11Z",
    "updated_at": "2024-12-19T11:57:23Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "##### My environment and problem:\r\n\r\n - Python3.7\r\n - PyTorch1.5\r\n - same as the requirements.txt\r\n\r\n**There is no problem for object detection, and it's a great job, thank you!**\r\n\r\n**However, I want to use this repo as a detector in my project, which is the first stage. But I can't use 'torch.load()' to load the weights you provided, get the error as follows:**\r\n\r\n```\r\nself.model = torch.load(self.weight_path, map_location=self.device)['model']\r\n  File \"torch1.5-py37/lib/python3.7/site-packages/torch/serialization.py\", line 593, in load\r\nreturn _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"torch1.5-py37/lib/python3.7/site-packages/torch/serialization.py\", line 773, in _legacy_load\r\nresult = unpickler.load()\r\nModuleNotFoundError: No module named 'models.yolo'\r\n```\r\n\r\n\r\n\r\n- I found this issue in another repo, [https://github.com/pytorch/pytorch/issues/18325](https://github.com/pytorch/pytorch/issues/18325), and I found that using 'torch.save(the_model, PAHT)' will have many problems, as stated by the official:\r\n\r\n```\r\ntorch.save(the_model, PATH)\r\nthe_model = torch.load(PATH)\r\n\r\nHowever in this case, the serialized data is bound to the specific classes and the exact directory structure used, so it can break in various ways when used in other projects, or after some serious refactors.\r\n```\r\n\r\n\r\n\r\n##### My solution\r\n\r\n- Re-saved the weights in original repo structure using the method that saves and loads only the model parameters\r\n\r\n```\\\r\ntorch.save(the_model.state_dict(), PATH)\r\nthe_model = TheModelClass(*args, **kwargs)\r\nthe_model.load_state_dict(torch.load(PATH))\r\n\r\nand My code as follows:\r\nweights='weights/yolov5m.pt'\r\ndevice = torch_utils.select_device(device='cpu' if ONNX_EXPORT else '0')\r\nmodel = torch.load(weights, map_location=device)['model']\r\ntorch.save(model.state_dict(), 'weights/yolov5m_resave.pt')\r\n```\r\n\r\n- So I use the new method to load weights\r\n\r\n```\r\nfrom models.yolo import Model\r\nyaml_path='models/yolov5m.yaml'\r\nnew_weights='weights/yolov5m_resave.pt'\r\nmodel = Model(yaml_path).to(device)\r\nmodel.load_state_dict(torch.load(new_weights))\r\n```\r\n\r\n- After that, I found I can get the same model and parameters as 'torch.load()' that you used, and the code can run. **But I got a new problem!!!**\r\n\r\n\r\n\r\n##### New problem\r\n\r\n- **I can get the detection results before NMS, but after the NMS, there is '[None]'**, My print as follows: \r\n\r\n```\r\nbefore nms: tensor([[[5.57901e+00, 5.70358e+00, 2.26364e+01,  ..., 1.07860e-03, 9.78606e-04, 1.86649e-03],\r\n         [1.35772e+01, 5.58121e+00, 2.83575e+01,  ..., 7.84854e-04, 6.75088e-04, 1.18259e-03],\r\n         [2.03256e+01, 5.90291e+00, 2.71849e+01,  ..., 1.05030e-03, 7.25093e-04, 1.90396e-03],\r\n         ...,\r\n         [3.39442e+02, 3.87110e+02, 1.64121e+02,  ..., 1.63732e-02, 5.22475e-03, 1.01126e-02],\r\n         [3.65044e+02, 3.88645e+02, 1.44507e+02,  ..., 1.25172e-02, 4.94093e-03, 9.01083e-03],\r\n         [3.91104e+02, 3.97117e+02, 1.44332e+02,  ..., 1.07815e-02, 4.93309e-03, 8.51673e-03]]], device='cuda:0')\r\n         \r\nafter nms: [None]\r\n```\r\n\r\n- I use my resaved weights to test the images in inference, there are no boxes.\r\n\r\n**I don't know what's the problem it is? And I don't understand why you use this save method instead of another more flexible way?  About my problem, do you have any good ideas? Thank you very much!**\r\n\r\n\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Hello @yxxxqqq, thank you for your interest in our work! Please visit our [Custom Training Tutorial](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) to get started, and see our [Google Colab Notebook](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb), [Docker Image](https://hub.docker.com/r/ultralytics/yolov5), and [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) for example environments.\n  \nIf this is a bug report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n  \nIf this is a custom model or data training question, please note that Ultralytics does **not** provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:\n- **Cloud-based AI** surveillance systems operating on **hundreds of HD video streams in realtime.**  \n- **Edge AI** integrated into custom iOS and Android apps for realtime **30 FPS video inference.**  \n- **Custom data training**, hyperparameter evolution, and model exportation to any destination.  \n  \nFor more information please visit https://www.ultralytics.com."
      },
      {
        "user": "glenn-jocher",
        "body": "@yxxxqqq thanks for your feedback. Yes you are correct, we use the current method, saving and loading the entire model. In the past we use the alternative method https://github.com/ultralytics/yolov3, of creating a model from a cfg file, and then replacing the random weights with the checkpoints weights using a state_dict().\r\n\r\nThis method caused two problems. The first is that initialization is slower, as a model is created with random weights, and then those random weights are replaced with the checkpoint weights, creating duplication of effort. The second, and main problem, was that a user was required to supply two items to load a model for inference or testing (the weights and cfg), instead of a single item. This places extra requirements on the user, and introduces a failure point during usage, as the user would often incorrectly match weights with incompatible cfg (i.e. yolov3-spp.pt with yolov3.cfg), leading to errors and confusion, and them raising issues and bug reports, using our time. \r\n\r\nSo we view the current method as the lesser of two evils. The main downside we see are SourceChangeWarnings that are generated when the modules the model is built on are updated since it was created."
      },
      {
        "user": "yxxxqqq",
        "body": "@glenn-jocher Thanks for your reply!  I have solved the 'SourceChangeWarnings' by the code you provided.\r\n```\r\nmodel = torch.load(weights, map_location=device)['model']\r\ntorch.save(torch.load(weights, map_location=device), weights)  # update model if SourceChangeWarning\r\n```\r\n**But the problems I said still exists:**\r\n- **Problem 1: When I use yolov5 outside the repo and use \"torch.load(the_mode, PATH)\", I got the error \"No module named 'models'\"**\r\n- **Problem 2: I resaved the model.state_dict() in the repo, and use \"model.load_state_dict()\" out the repo, but I can't get any detection results. I use the resaved weights in the repo, nothing as well. Details as follow(same image, but different load method):**\r\n```\r\n1. use original weights, torch.load()\r\npred before nms: tensor([[[5.38951e+00, 6.87055e+00, 1.14993e+01,  ..., 1.90228e-03, 1.01164e-03, 2.54049e-03],\r\n         [7.83045e+00, 6.57221e+00, 1.45590e+01,  ..., 1.57367e-03, 8.64962e-04, 2.01560e-03],\r\n         [2.25311e+01, 5.58812e+00, 1.23454e+01,  ..., 1.72529e-03, 9.21386e-04, 2.28453e-03],\r\n         ...,\r\n         [4.31154e+02, 6.14794e+02, 1.36958e+02,  ..., 1.80755e-03, 1.52067e-03, 1.51791e-03],\r\n         [4.56398e+02, 6.17055e+02, 1.22339e+02,  ..., 2.12122e-03, 1.61005e-03, 1.63509e-03],\r\n         [4.91976e+02, 6.23088e+02, 1.45217e+02,  ..., 3.99010e-03, 1.72312e-03, 2.11344e-03]]], device='cuda:0')\r\n         \r\npred after nms: [tensor([[ 44.06211, 235.47171, 162.47781, 537.28436,   0.91711,   0.00000],\r\n        [146.72403, 240.72610, 219.93156, 511.04062,   0.90797,   0.00000],\r\n        [412.23538, 237.46272, 497.78629, 522.23077,   0.89330,   0.00000],\r\n        [ 22.67275, 135.73569, 490.28171, 438.86267,   0.74369,   5.00000],\r\n        [ 16.38007, 324.36755,  63.95830, 529.78113,   0.54598,   0.00000]], device='cuda:0')]\r\n\r\n\r\n\r\n2. use resaved weights, model.load_state_dict()\r\npred before nms: tensor([[[5.39362e+00, 5.79549e+00, 2.25946e+01,  ..., 1.25067e-03, 1.00686e-03, 1.47676e-03],\r\n         [1.25392e+01, 5.98638e+00, 2.68692e+01,  ..., 9.48603e-04, 8.45199e-04, 1.03681e-03],\r\n         [2.11967e+01, 5.65385e+00, 2.41934e+01,  ..., 1.24312e-03, 9.92147e-04, 1.58688e-03],\r\n         ...,\r\n         [4.33180e+02, 6.20522e+02, 1.69033e+02,  ..., 5.71506e-03, 3.09453e-03, 3.54823e-03],\r\n         [4.61483e+02, 6.20247e+02, 1.54342e+02,  ..., 7.58316e-03, 3.30421e-03, 3.97864e-03],\r\n         [4.91035e+02, 6.24763e+02, 1.59548e+02,  ..., 9.68921e-03, 3.65757e-03, 4.65747e-03]]], device='cuda:0')\r\n\r\npred after nms: [None]\r\n```\r\n"
      }
    ]
  },
  {
    "issue_number": 13467,
    "title": "Yolov5 model capturing other objects other than the intended or desired object ",
    "author": "youssef66677",
    "state": "open",
    "created_at": "2024-12-18T13:10:26Z",
    "updated_at": "2024-12-19T10:43:56Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nI have a YOLO V5 m model for a dataset to detect a certain product but after training a dataset of a variety of pictures for the product in different cases and scenarios and various lighting conditions with no repetition for the cases, on 50 epochs and the Yolo v5 medium architecture and batch size of 16. What I have noticed is that the model is dependent on the color features, meaning that the objects have colors yellow and its shades and graidents. What should I do to fix this problem? Kindly help me, also kindly find the used dataset for training below\r\n\r\n\r\nThe product desired to be detected:\r\n\r\n![3](https://github.com/user-attachments/assets/1b3bab93-ddc7-4294-aea8-ec5047b2b870)\r\n![IMG_4712](https://github.com/user-attachments/assets/188f56e7-8ded-4bac-8680-59279e56afde)\r\n\r\nThe other products that are detected:\r\n![1](https://github.com/user-attachments/assets/3da26693-ccfb-4a70-859c-121bf4941d9e)\r\n![2](https://github.com/user-attachments/assets/4943a0c9-675c-419d-94ef-b90ae659b587)\r\n\r\nThe dataset used in training (aprox 450 images) in google drive:\r\nhttps://drive.google.com/file/d/1oBH28fbyhEmabLLx2bPXpEMQ0s2zhg__/view?usp=sharing\r\n\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @youssef66677, thank you for your interest in YOLOv5 ðŸš€! This is an automated response to help guide you while an Ultralytics engineer reviews your issue soon.\n\nFrom your description, it seems like your model may be overfitting or potentially relying too heavily on color features for prediction. To provide better assistance, we need a bit more information.\n\n### If this is a ðŸ› Bug Report:\nPlease ensure that you provide a **minimum reproducible example (MRE)**. This includes a clear code snippet or setup that reproduces the issue, along with relevant logs or error messages. \n\n### If this is a custom training â“ Question:\nProviding as much detail as possible is key. In addition to the images and dataset link you've shared, consider elaborating on the following:\n- Validation and training metrics (mAP, precision, recall).\n- Training visualizations such as loss curves and accuracy trends over epochs.\n- If you are using augmentation techniques, specify the methods used.\n- Confirm you're following the best practices outlined in YOLOv5's guidelines (e.g., ensuring dataset balance, annotation quality, etc.).\n\n### Requirements\nEnsure that your software environment meets YOLOv5's requirements, including Python>=3.8.0 and PyTorch>=1.8 installed correctly along with the other libraries in the `requirements.txt` file. Running a fresh setup can also help rule out dependency issues.\n\n### Environments\nYOLOv5 can run in multiple verified environments, such as Colab notebooks, local machines, Docker, and cloud platforms, all of which support GPU acceleration. Please confirm your training environment and configuration.\n\n### Status\nOur Continuous Integration (CI) tests verify YOLOv5 functionality daily across supported platforms. You can check these results before investigating further.\n\nProviding more specific details and a concise reproducible setup will help us pinpoint the issue and guide you effectively. We're here to help! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@youssef66677 thank you for reaching out. Based on your observations, it seems the model might be overfitting to color features in your dataset. Here are a few suggestions to address this issue:\n\n1. **Increase Dataset Size and Variety:** With only ~450 images, your dataset may be insufficient for robust feature learning. Aim for at least 1500 images per class with diverse backgrounds, lighting, and object variations ([reference](https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/#dataset)).\n\n2. **Data Augmentation:** Apply augmentation techniques like color jitter, grayscale conversion, and random saturation/hue shifts during training to reduce reliance on color features. YOLOv5 supports these through the `hyp.yaml` file.\n\n3. **Add Background Images:** Include images without your target product to reduce false positives. Background images help the model distinguish between relevant and irrelevant features ([reference](https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/#dataset)).\n\n4. **Fine-tune Hyperparameters:** Adjust learning rate, confidence threshold, and class weights to encourage better generalization.\n\n5. **Analyze Labels:** Verify your labeling consistency to ensure accurate bounding boxes and eliminate any bias in annotations.\n\nPlease try these steps and retrain the model. If the issue persists, feel free to share additional information such as your loss curves, mAP scores, and confusion matrix for further analysis."
      }
    ]
  },
  {
    "issue_number": 13465,
    "title": "Inconsistent TFLite Model Results Between detect.py and Custom Inference Code",
    "author": "yAlqubati",
    "state": "open",
    "created_at": "2024-12-17T18:32:43Z",
    "updated_at": "2024-12-18T16:09:30Z",
    "labels": [
      "question",
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nDescription:\r\n\r\nHi,\r\nI've converted a YOLOV5s model to a Tflite model using the export.py script provided in YOLOv5. The output came with the name `(best-fp16.tflite).` It works well when used with the official `detect.py` script. The predictions are accurate, with good bounding box alignment and confidence scores.\r\n\r\nHowever, when I perform inference using my custom Python code, the results are noticeably worse:\r\n- Bounding boxes are misaligned.\r\n- Confidence scores are significantly lower.\r\n\r\nhere is the detect.py output\r\n![test](https://github.com/user-attachments/assets/463e0bca-837b-46f9-a297-3c50e4ce28d8)\r\n\r\nhere is the custom code output:\r\n![detection_original](https://github.com/user-attachments/assets/940c72ee-c841-4102-a837-5196b4e54e6c)\r\n\r\nknow the labels in the output are incorrect (e.g., \"person\") because I forgot to update coco.yaml, but the main issue lies in the quality of the detections.\r\n\r\n**the input shape is [1, 640, 640, 3]\r\nthe output shape is [1, 25200, 7]**\r\n\r\n\r\nhere is my custom code for detection:\r\n\r\n```\r\n`\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load TFLite model\r\ninterpreter = tf.lite.Interpreter(model_path=\"best-fp16.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output details\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Function to preprocess the image\r\ndef preprocess_image(image_path, input_size=(640, 640)):\r\n    # Load the image\r\n    image = cv2.imread(image_path)\r\n    if image is None:\r\n        raise ValueError(\"Image not found or could not be loaded\")\r\n    \r\n    # Convert image to RGB\r\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n    \r\n    # Resize image to the model input size (640x640)\r\n    image_resized = cv2.resize(image, input_size)\r\n    \r\n    # Normalize pixel values to [0, 1]\r\n    image_resized = image_resized / 255.0\r\n\r\n    # Add batch dimension (shape becomes [1, 640, 640, 3])\r\n    input_image = np.expand_dims(image_resized, axis=0).astype(np.float32)\r\n    \r\n    return input_image, image  # Return processed image and original image\r\n\r\n# Function to decode raw YOLOv5 output\r\ndef decode_output(output_data, input_size, confidence_threshold=0.2, iou_threshold=0.4):\r\n    boxes = output_data[..., :4]  # Extract bounding box data\r\n    confidence = output_data[..., 4]  # Extract confidence scores\r\n    class_probs = output_data[..., 5:]  # Extract class probabilities\r\n\r\n    # Compute final confidence scores\r\n    scores = confidence[..., np.newaxis] * class_probs\r\n\r\n    # Filter predictions based on confidence threshold\r\n    valid_detections = np.where(scores.max(axis=-1) > confidence_threshold)\r\n    boxes = boxes[valid_detections]\r\n    scores = scores[valid_detections]\r\n    class_ids = np.argmax(scores, axis=-1)\r\n    \r\n    # Scale boxes to input size (assumes input_size is square)\r\n    input_h, input_w = input_size\r\n    boxes[:, 0] *= input_w  # Scale x_center\r\n    boxes[:, 1] *= input_h  # Scale y_center\r\n    boxes[:, 2] *= input_w  # Scale width\r\n    boxes[:, 3] *= input_h  # Scale height\r\n    \r\n    # Convert boxes from (x_center, y_center, width, height) to (xmin, ymin, xmax, ymax)\r\n    boxes[:, 0] = boxes[:, 0] - boxes[:, 2] / 2  # xmin\r\n    boxes[:, 1] = boxes[:, 1] - boxes[:, 3] / 2  # ymin\r\n    boxes[:, 2] = boxes[:, 0] + boxes[:, 2]      # xmax\r\n    boxes[:, 3] = boxes[:, 1] + boxes[:, 3]      # ymax\r\n\r\n    # Perform Non-Maximum Suppression (NMS)\r\n    nms_indices = tf.image.non_max_suppression(\r\n        boxes,\r\n        scores.max(axis=-1),\r\n        max_output_size=50,  # Max number of detections\r\n        iou_threshold=iou_threshold,\r\n        score_threshold=confidence_threshold\r\n    ).numpy()\r\n\r\n    # Return final filtered boxes, scores, and class IDs\r\n    return boxes[nms_indices], scores[nms_indices].max(axis=-1), class_ids[nms_indices]\r\n\r\n# Function to draw bounding boxes on an image\r\ndef draw_boxes(image, boxes, scores, class_ids, input_size, rescale=False):\r\n    image_draw = image.copy()\r\n    if rescale:\r\n        # Rescale boxes to input size\r\n        original_h, original_w = image.shape[:2]\r\n        boxes[:, [0, 2]] *= (original_w / input_size[0])  # Scale X coordinates\r\n        boxes[:, [1, 3]] *= (original_h / input_size[1])  # Scale Y coordinates\r\n\r\n    # Draw bounding boxes\r\n    for box, score, class_id in zip(boxes, scores, class_ids):\r\n        if score > 0.2:  # Debug: Lower threshold for easier testing\r\n            xmin, ymin, xmax, ymax = box.astype(int)\r\n            label = f\"Class {int(class_id)}: {score:.2f}\"\r\n            \r\n            # Draw bounding box\r\n            cv2.rectangle(image_draw, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\r\n            \r\n            # Draw label\r\n            cv2.putText(image_draw, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n    return image_draw\r\n\r\n## Function to run inference and draw bounding boxes\r\ndef predict_and_draw_boxes(image_path, interpreter, input_size=(640, 640)):\r\n    # Preprocess the image\r\n    input_image, original_image = preprocess_image(image_path, input_size)\r\n    print(\"Input image shape:\", input_image.shape)  # Debugging info\r\n    print(\"Original image shape:\", original_image.shape)  # Debugging info\r\n\r\n    # Set input tensor\r\n    interpreter.set_tensor(input_details[0]['index'], input_image)\r\n    \r\n    # Run inference\r\n    interpreter.invoke()\r\n    \r\n    # Get output tensor\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    print(\"Output data shape:\", output_data.shape)  # Debugging info\r\n    \r\n    # Decode output\r\n    boxes, scores, class_ids = decode_output(output_data[0], input_size, confidence_threshold=0.2)\r\n\r\n    # Draw bounding boxes on the original image\r\n    original_boxes_image = draw_boxes(original_image, boxes, scores, class_ids, input_size, rescale=True)\r\n    \r\n    # Save the image with the bounding boxes\r\n    original_output_path = \"detection_original.jpg\"\r\n    cv2.imwrite(original_output_path, original_boxes_image)\r\n    \r\n    print(f\"Image with detections saved as {original_output_path}\")\r\n\r\n# Run prediction and draw bounding boxes\r\npredict_and_draw_boxes(\"test2.jpg\", interpreter)\r\n`\r\n```\r\n\r\nhow can I get the same result as the detect.py, do I need to include any preprocessing for the image or postprocessing for the output?\r\n\r\nI want to have the same result as the detect.py so that I can convert it to flutter and make detections from phones \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @yAlqubati, thank you for your interest in YOLOv5 ðŸš€! It looks like you're experiencing differences between `detect.py` results and your custom inference script. This is a great question and an Ultralytics engineer will assist you soon!\n\nFor a ðŸ› Bug Report, could you please confirm if `detect.py` and your custom code are using the same preprocessing and postprocessing logic? Differences here often lead to inconsistencies in results.\n\nIf not already done, providing a **minimum reproducible example (MRE)** with a simplified version of your test image, model, and code is immensely helpful for debugging.\n\n### Requirements\n\nEnsure that you are using **Python>=3.8.0** with all necessary libraries installed and that the environment is correctly set up with matching TensorFlow Lite configurations.\n\n### Additional Debugging Tips\n\n- Verify that both the `detect.py` script and your custom code are normalizing images in the same way (e.g., pixel value scaling to [0, 1], image resizing dimensions, etc.).\n- Check that the YOLOv5 model postprocessing steps, such as Non-Maximum Suppression (NMS), confidence thresholds, and class probability decoding, are consistent with YOLOv5's implementation.\n- Confirm that your input tensor shape `([1, 640, 640, 3])` matches what YOLOv5 expects. Ensure there are no deviations in input formats or scaling.\n\nOnce these items are cross-verified, aligning the results should be easier. An engineer will follow up shortly with further recommendations ðŸ˜Š."
      },
      {
        "user": "pderrenger",
        "body": "@yAlqubati the difference in results between `detect.py` and your custom code is likely due to inconsistencies in image preprocessing and/or output postprocessing. YOLOv5's `detect.py` handles these details precisely, so you will need to align your custom code accordingly.\n\n### Suggestions:\n\n1. **Image Preprocessing**:\n   - Ensure that the input image is normalized to `[0, 1]` and matches the TFLite model's expected input format.\n   - Verify that the input tensor is scaled appropriately using quantization parameters if the TFLite model is quantized (e.g., FP16 or INT8). You can check this by inspecting `input_details[0]['quantization']`.\n\n2. **Postprocessing**:\n   - YOLOv5 outputs are in normalized (0-1) `x_center, y_center, width, height` format. Ensure you correctly scale and convert these to `(xmin, ymin, xmax, ymax)` coordinates relative to the original image dimensions.\n   - Use non-maximum suppression (NMS) parameters consistent with `detect.py`. The confidence threshold and IoU threshold in your code (`confidence_threshold=0.2`, `iou_threshold=0.4`) may differ from the defaults in `detect.py`.\n\n3. **Debugging**:\n   - Compare the input tensor passed to the TFLite model in your custom code with the one in `detect.py` to ensure they are identical.\n   - Inspect the raw model outputs (before postprocessing) in both cases to confirm they match.\n\nFor reference, you can review the YOLOv5 TFLite inference example provided in the [YOLOv5 TFLite Export Guide](https://docs.ultralytics.com/yolov5/tutorials/model_export/). It includes preprocessing and postprocessing steps that align with `detect.py`.\n\nLet us know if you encounter further issues!"
      },
      {
        "user": "yAlqubati",
        "body": "Thanks for the suggestions! I've verified that the image preprocessing follows your steps, and the model is FP32, **not quantized**â€”would this be an issue?\r\n\r\nFor postprocessing, Iâ€™ve ensured correct scaling and NMS parameters (`confidence_threshold=0.2, iou_threshold=0.4`). The input tensor shape (`[1, 640, 640, 3]`) is the same in both the custom code and `detect.py`. However, I couldn't print the output shape in `detect.py`. Could you provide guidance on how to resolve this or any other steps I should check?\r\n\r\nLet me know if thereâ€™s anything else I can try!"
      }
    ]
  },
  {
    "issue_number": 13459,
    "title": "ä¸ºä»€ä¹ˆmacä¼šæŠ›å‡ºè¿™ä¸ªé—®é¢˜",
    "author": "sj746",
    "state": "open",
    "created_at": "2024-12-13T06:32:50Z",
    "updated_at": "2024-12-18T04:41:01Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n![ä¸Šä¼ æˆªå±2024-12-13 14.30.25.png...]()ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\r\n\n\n### Additional\n\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\r\n",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @sj746, thank you for your interest in YOLOv5 ðŸš€!\n\nIt seems like you are encountering a certificate verification issue. If this is a ðŸ› Bug Report, could you please provide a **minimum reproducible example (MRE)**? This would greatly help us diagnose the issue. Be sure to include all relevant details such as scripts, configurations, and any necessary context.\n\nIf you're asking a â“ Question, please share any additional information that could help us assist you better. For instance, the Python version, operating system details, and how you installed dependencies would be valuable.\n\n## Requirements\n\nEnsure your Python version is 3.8.0 or newer and all required dependencies are installed. Make sure PyTorch version is 1.8 or above as well.\n\n## Environments\n\nYOLOv5 supports many tested environments such as Notebooks with GPU, cloud platforms, or Docker. Double-check that your environment meets the expected setup.\n\n## Status â¬†ï¸\n\nLastly, you can check our continuous integration (CI) badge on the repository page to see if the latest tests have passed. Our CI tests regularly verify YOLOv5 operations on a variety of platforms.\n\nðŸ˜„ This is an automated reply to get you started. An Ultralytics engineer will review your question and assist you further soon!"
      },
      {
        "user": "sj746",
        "body": "æˆ‘çš„æ˜¯python3.10\r\n"
      },
      {
        "user": "sj746",
        "body": "(.venv) dengbiao@dengbiaodeMacBook-Air yolov5-master % python train.py --data new.yaml --weights yolov5s.pt --img 640\r\ntrain: weights=yolov5s.pt, cfg=, data=new.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\r\ngithub: skipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\r\nYOLOv5 ðŸš€ 2024-11-25 Python-3.10.11 torch-2.2.2 CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nComet: run 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\r\nTensorBoard: Start with 'tensorboard --logdir runs/train', view at http://localhost:6006/\r\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /Users/dengbiao/Library/Application Support/Ultralytics/Arial.ttf...\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 1348, in do_open\r\n    h.request(req.get_method(), req.selector, req.data, headers,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1283, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1329, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1278, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1038, in _send_output\r\n    self.send(msg)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 976, in send\r\n    self.connect()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1455, in connect\r\n    self.sock = self._context.wrap_socket(self.sock,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 513, in wrap_socket\r\n    return self.sslsocket_class._create(\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 1071, in _create\r\n    self.do_handshake()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 1342, in do_handshake\r\n    self._sslobj.do_handshake()\r\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/train.py\", line 987, in <module>\r\n    main(opt)\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/train.py\", line 689, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/train.py\", line 204, in train\r\n    data_dict = data_dict or check_dataset(data)  # check if None\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/utils/general.py\", line 583, in check_dataset\r\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\", progress=True)  # download fonts\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/utils/general.py\", line 519, in check_font\r\n    torch.hub.download_url_to_file(url, str(file), progress=progress)\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/.venv/lib/python3.10/site-packages/torch/hub.py\", line 620, in download_url_to_file\r\n    u = urlopen(req)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 216, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 519, in open\r\n    response = self._open(req, data)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 536, in _open\r\n    result = self._call_chain(self.handle_open, protocol, protocol +\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 496, in _call_chain\r\n    result = func(*args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 1391, in https_open\r\n    return self.do_open(http.client.HTTPSConnection, req,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 1351, in do_open\r\n    raise URLError(err)\r\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)>\r\n(.venv) dengbiao@dengbiaodeMacBook-Air yolov5-master % python train.py --data new.yaml --weights yolov5s.pt --img 640\r\ntrain: weights=yolov5s.pt, cfg=, data=new.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\r\ngithub: skipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\r\nYOLOv5 ðŸš€ 2024-11-25 Python-3.10.11 torch-2.2.2 CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nComet: run 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\r\nTensorBoard: Start with 'tensorboard --logdir runs/train', view at http://localhost:6006/\r\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /Users/dengbiao/Library/Application Support/Ultralytics/Arial.ttf...\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 1348, in do_open\r\n    h.request(req.get_method(), req.selector, req.data, headers,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1283, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1329, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1278, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1038, in _send_output\r\n    self.send(msg)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 976, in send\r\n    self.connect()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1455, in connect\r\n    self.sock = self._context.wrap_socket(self.sock,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 513, in wrap_socket\r\n    return self.sslsocket_class._create(\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 1071, in _create\r\n    self.do_handshake()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 1342, in do_handshake\r\n    self._sslobj.do_handshake()\r\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/train.py\", line 987, in <module>\r\n    main(opt)\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/train.py\", line 689, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/train.py\", line 204, in train\r\n    data_dict = data_dict or check_dataset(data)  # check if None\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/utils/general.py\", line 583, in check_dataset\r\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\", progress=True)  # download fonts\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/utils/general.py\", line 519, in check_font\r\n    torch.hub.download_url_to_file(url, str(file), progress=progress)\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/.venv/lib/python3.10/site-packages/torch/hub.py\", line 620, in download_url_to_file\r\n    u = urlopen(req)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 216, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 519, in open\r\n    response = self._open(req, data)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 536, in _open\r\n    result = self._call_chain(self.handle_open, protocol, protocol +\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 496, in _call_chain\r\n    result = func(*args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 1391, in https_open\r\n    return self.do_open(http.client.HTTPSConnection, req,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 1351, in do_open\r\n    raise URLError(err)\r\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)>\r\n(.venv) dengbiao@dengbiaodeMacBook-Air yolov5-master % python train.py --data new.yaml --weights yolov5s.pt --img 640\r\ntrain: weights=yolov5s.pt, cfg=, data=new.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\r\ngithub: skipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\r\nYOLOv5 ðŸš€ 2024-11-25 Python-3.10.11 torch-2.2.2 CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nComet: run 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\r\nTensorBoard: Start with 'tensorboard --logdir runs/train', view at http://localhost:6006/\r\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /Users/dengbiao/Library/Application Support/Ultralytics/Arial.ttf...\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 1348, in do_open\r\n    h.request(req.get_method(), req.selector, req.data, headers,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1283, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1329, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1278, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1038, in _send_output\r\n    self.send(msg)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 976, in send\r\n    self.connect()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1455, in connect\r\n    self.sock = self._context.wrap_socket(self.sock,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 513, in wrap_socket\r\n    return self.sslsocket_class._create(\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 1071, in _create\r\n    self.do_handshake()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 1342, in do_handshake\r\n    self._sslobj.do_handshake()\r\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/train.py\", line 987, in <module>\r\n    main(opt)\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/train.py\", line 689, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/train.py\", line 204, in train\r\n    data_dict = data_dict or check_dataset(data)  # check if None\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/utils/general.py\", line 583, in check_dataset\r\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\", progress=True)  # download fonts\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/yolov5-master/utils/general.py\", line 519, in check_font\r\n    torch.hub.download_url_to_file(url, str(file), progress=progress)\r\n  File \"/Users/dengbiao/PycharmProjects/pythonProject1/.venv/lib/python3.10/site-packages/torch/hub.py\", line 620, in download_url_to_file\r\n    u = urlopen(req)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 216, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 519, in open\r\n    response = self._open(req, data)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 536, in _open\r\n    result = self._call_chain(self.handle_open, protocol, protocol +\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 496, in _call_chain\r\n    result = func(*args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 1391, in https_open\r\n    return self.do_open(http.client.HTTPSConnection, req,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py\", line 1351, in do_open\r\n    raise URLError(err)\r\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)>\r\n"
      }
    ]
  },
  {
    "issue_number": 12645,
    "title": "Negative-width bounding boxes when running on M1 (mps) HW",
    "author": "agentmorris",
    "state": "closed",
    "created_at": "2024-01-18T04:15:23Z",
    "updated_at": "2024-12-18T02:36:46Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am running a trained YOLOv5x6 model using val.py with the --save_json option.  I have a few images where the resulting .json file includes one or more boxes with negative width values (not negative x or y values, which seem normal and are discussed in other issues, but negative _width_ values), but I have only observed this behavior when running on M1 hardware (with the \"mps\" device).  All the boxes I've observed that have this property are low-ish confidence, but neither the confidence nor the absolute width/height are so low that I would call this a \"junk bounding box\".\r\n\r\nI get identical, non-negative boxes processing the same images with the same weights on CUDA or CPU devices.  Just in case the default precision is different on M1, I tried this with and without half-precision inference, but could not replicate the negative boxes on CUDA/CPU devices.\r\n\r\nSo, this is a two-part question:\r\n\r\n1. Are negative-width bounding boxes expected, and/or is there an interpretation of these boxes?  Should I expect to take the absolute value of the width and add it to the x value as if it were positive to get the right edge of the box, or should still add the (negative) width to the x value to get the other edge of the box?\r\n\r\n2. In general, are there common patterns of differences between M1 and CUDA/CPU devices?  Are results expected to be identical to reasonable precision?\r\n\r\nIf the answers are \"no, that should never happen\" and \"no, they should all be identical\", I will try to provide a reproducible example.  I don't currently have permission to share the offending images, so I'm trying to see whether there's a logical explanation before going down that path.\r\n\r\nThanks!\r\n\r\n-Dan\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@agentmorris hi Dan,\n\nThanks for reaching out with your detailed observations. Negative-width bounding boxes are indeed not expected behavior, and there is no standard interpretation for such boxes in the context of YOLOv5. The bounding box coordinates should always result in positive width and height values, as they represent the dimensions of the detected object.\n\nRegarding your two-part question:\n\n1. Negative-width bounding boxes are not expected. The width (and height) should always be positive, as they represent the size of the bounding box. If you encounter negative values, it's likely indicative of an issue that needs to be addressed.\n\n2. While results between M1 (mps) and CUDA/CPU devices should be very similar, minor differences can occur due to floating-point precision and hardware-specific optimizations. However, these differences should not result in negative dimensions for bounding boxes.\n\nIt seems like you might have stumbled upon a hardware-specific issue with the M1 (mps) device. If you can provide a reproducible example without sharing the original images, that would be very helpful. You might try to replicate the issue with publicly available images or synthetic data that triggers the same behavior.\n\nIn the meantime, I recommend checking the latest updates and documentation for any related fixes or advice on handling such cases. If the issue persists, please open an issue with as much detail as possible, and we'll look into it further."
      },
      {
        "user": "agentmorris",
        "body": "Thank you for the quick response; I will try to create a reproducible example on a public image."
      },
      {
        "user": "glenn-jocher",
        "body": "@agentmorris You're welcome, Dan! Creating a reproducible example on a public image would be extremely helpful. If you encounter any further issues or have additional questions while doing so, please don't hesitate to reach out. Good luck, and we appreciate your efforts in helping improve YOLOv5! ðŸ˜ŠðŸ‘"
      }
    ]
  },
  {
    "issue_number": 8786,
    "title": "How do I run CSI camera on YOLOV5?",
    "author": "medic-lab",
    "state": "closed",
    "created_at": "2022-07-29T18:43:50Z",
    "updated_at": "2024-12-17T17:30:00Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello,\r\nI can run CSI camera on Jetson Xavier NX without a specific purpose but when i trying to run the camera with YoloV5. I'm having issues because the camera is not defined in YoloV5.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @medic-lab, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"CI CPU testing\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), validation ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@medic-lab connected webcams are generally accessed via their indices, i.e.\r\n\r\n```\r\npython detect.py --source 0\r\npython detect.py --source 1\r\npython detect.py --source 2\r\n...\r\n```"
      },
      {
        "user": "medic-lab",
        "body": "Well, my camera is not considered as a webcam. It's not connected via USB so `--source 0` or above doesn't work for me. CSI cameras have different port but i don't know how to define it on YoloV5. \r\n\r\nI can only run the camera with the code below on terminal:\r\n`gst-launch-1.0 nvarguscamerasrc sensor_id=0 ! 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! nvvidconv flip-method=0 ! 'video/x-raw,width=960, height=616' ! nvvidconv ! nvegltransform ! nveglglessink -e` \r\n\r\n![WhatsApp Image 2022-07-30 at 16 36 38](https://user-images.githubusercontent.com/84999910/181916871-5fd4a996-62a4-4e33-a53d-dbcc543c1710.jpeg)\r\n"
      }
    ]
  },
  {
    "issue_number": 2611,
    "title": "Four channel image training",
    "author": "Sophia-11",
    "state": "closed",
    "created_at": "2021-03-26T08:33:38Z",
    "updated_at": "2024-12-17T15:15:51Z",
    "labels": [
      "question",
      "TODO",
      "Stale"
    ],
    "body": "## â”Question\r\nHello, may I ask you a question about the yolov5? if I have two pictures and I want to combine them into 4 channels for training, what should I do?Thank you for your help\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @Sophia-11, thank you for your interest in ðŸš€ YOLOv5! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://www.ultralytics.com or email Glenn Jocher at glenn.jocher@ultralytics.com.\n\n## Requirements\n\nPython 3.8 or later with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies installed, including `torch>=1.7`. To install run:\n```bash\n$ pip install -r requirements.txt\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n\n## Status\n\n![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/models/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@Sophia-11 I don't understand how 2 images can be combined with 4 channels. Can you supply details or point to documentation/sources for this use case?"
      },
      {
        "user": "Sophia-11",
        "body": "> @Sophia-11 I don't understand how 2 images can be combined with 4 channels. Can you supply details or point to documentation/sources for this use case?\r\n\r\nSorry, I didn't describe it clearly. I have two data sources. The first data has 3 channels of RGB, and the second data source is a single channel. The two data have been registered. The two data sources have the same bounding box. How to train these 4 channels at the same time? Thank you! @glenn-jocher"
      }
    ]
  },
  {
    "issue_number": 13463,
    "title": "onnxruntime inference",
    "author": "ZCzzzzzz",
    "state": "open",
    "created_at": "2024-12-17T09:44:42Z",
    "updated_at": "2024-12-17T15:06:48Z",
    "labels": [
      "question",
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI've converted yolov5s.pt to yolov5s.onnx,Now I want to use onnxruntime for inference,but all the final result is 0,like this:\r\n\"Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\"\r\n\n\n### Additional\n\nthe python code is :\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport onnxruntime as ort\r\nfrom pathlib import Path\r\nfrom tqdm import tqdm\r\nfrom pycocotools.coco import COCO\r\nfrom pycocotools.cocoeval import COCOeval\r\nfrom utils.general import coco80_to_coco91_class\r\nimport json\r\n\r\nDATASET_PATH = '/COCO2017'  \r\nMODEL_PATH = './yolov5s.onnx'  \r\nIMG_SIZE = 640  \r\nCONF_THRESH = 0.1  \r\nIOU_THRESH = 0.6  \r\n\r\n\r\ndata_paths = {\r\n    \"train_images\": os.path.join(DATASET_PATH, \"train2017.txt\"),\r\n    \"val_images\": os.path.join(DATASET_PATH, \"val2017.txt\"),\r\n    \"annotations_train\": os.path.join(DATASET_PATH, \"annotations\", \"instances_train2017.json\"),\r\n    \"annotations_val\": os.path.join(DATASET_PATH, \"annotations\", \"instances_val2017.json\"),\r\n}\r\n\r\n\r\ndef xywh2xyxy(x):\r\n    y = np.copy(x)\r\n    y[..., 0] = x[..., 0] - x[..., 2] / 2\r\n    y[..., 1] = x[..., 1] - x[..., 3] / 2\r\n    y[..., 2] = x[..., 0] + x[..., 2] / 2\r\n    y[..., 3] = x[..., 1] + x[..., 3] / 2\r\n    return y\r\n\r\ndef xyxy2xywh(x):\r\n    y = np.copy(x)\r\n    y[..., 0] = (x[..., 0] + x[..., 2]) / 2\r\n    y[..., 1] = (x[..., 1] + x[..., 3]) / 2\r\n    y[..., 2] = x[..., 2] - x[..., 0]\r\n    y[..., 3] = x[..., 3] - x[..., 1]\r\n    return y\r\n\r\n\r\ndef non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, max_det=300):\r\n    xc = prediction[..., 4] > conf_thres\r\n    output = [np.zeros((0, 6))] * prediction.shape[0]\r\n    for xi, x in enumerate(prediction):\r\n        x = x[xc[xi]]\r\n        if not x.shape[0]:\r\n            continue\r\n        x[:, 5:] *= x[:, 4:5]\r\n        box = xywh2xyxy(x[:, :4])\r\n        conf = x[:, 4]\r\n        j = np.argmax(x[:, 5:], axis=1)\r\n        x = np.concatenate((box, conf[:, None], j[:, None]), axis=1)[conf > conf_thres]\r\n\r\n        if not x.shape[0]:\r\n            continue\r\n        c = x[:, 5:6] * 4096\r\n        boxes, scores = x[:, :4] + c, x[:, 4]\r\n        i = cv2.dnn.NMSBoxes(boxes.tolist(), scores.tolist(), conf_thres, iou_thres)\r\n        output[xi] = x[i].reshape(-1, 6)[:max_det]\r\n    return output\r\n\r\n\r\ndef preprocess_image(image_path):\r\n    img = cv2.imread(image_path)\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\r\n    img = img.transpose(2, 0, 1).astype(np.float32)\r\n    img /= 255.0\r\n    return np.expand_dims(img, axis=0)\r\n\r\n\r\ndef infer_with_onnxruntime(session, img_tensor):\r\n    input_name = session.get_inputs()[0].name\r\n    outputs = session.run(None, {input_name: img_tensor})\r\n    return outputs[0]\r\n\r\n\r\ndef save_coco_results(predictions, image_ids, coco, output_file):\r\n    results = []\r\n#    class_map = {i: cat_id for i, cat_id in enumerate(sorted(coco.getCatIds()))}  \r\n    class_map = coco80_to_coco91_class()  \r\n#    print(\"Class map:\", class_map)  \r\n    \r\n    for preds, img_id in zip(predictions, image_ids):\r\n        for pred in preds[0]:\r\n#            print(\"Shape of pred:\", pred.shape)\r\n            box = pred[:4]\r\n            conf = pred[4]\r\n#            print(\"Value of pred[5]:\", pred[5])\r\n            cls = int(pred[5])\r\n            if cls >= len(class_map):\r\n                print(f\"Skipping invalid class index: {cls}\")\r\n                continue\r\n#            category_id = class_map.get(cls, None)\r\n            category_id = class_map[cls]\r\n#            if category_id is None:\r\n#                print(f\"Unknown class ID: {cls}, skipping...\")\r\n#                continue\r\n            print(f\"Image ID: {img_id}, Category ID: {category_id}, Box: {box}, Score: {conf}\")\r\n            print(f\"Predicted COCO80 cls: {cls}, Mapped COCO91 category_id: {category_id}\")\r\n            box = xyxy2xywh(np.array(box).reshape(1, 4))[0]\r\n            box = [max(0, round(x, 3)) for x in box]\r\n            results.append({\r\n                \"image_id\": int(img_id),\r\n                \"category_id\": category_id,\r\n                \"bbox\": box,\r\n                \"score\": round(conf, 5)\r\n            })\r\n    with open(output_file, 'w') as f:\r\n        json.dump(results, f)\r\n\r\n\r\ndef run_inference(images_file, annotations_file, session, dataset_name):\r\n    coco = COCO(annotations_file)\r\n    with open(images_file) as f:\r\n        image_paths = [line.strip() for line in f.readlines()]\r\n    predictions, image_ids = [], []\r\n\r\n\r\n    for img_path in tqdm(image_paths, desc=f\"inference: {dataset_name}\"):\r\n        img_tensor = preprocess_image(os.path.join(DATASET_PATH, img_path))\r\n        preds = infer_with_onnxruntime(session, img_tensor)\r\n        preds = non_max_suppression(preds, conf_thres=CONF_THRESH, iou_thres=IOU_THRESH)\r\n\r\n        predictions.append(preds)\r\n        image_ids.append(Path(img_path).stem)\r\n\r\n    output_file = f\"coco_predictions_{dataset_name}.json\"\r\n    save_coco_results(predictions, image_ids, coco, output_file)\r\n    coco_dt = coco.loadRes(output_file)\r\n    coco_eval = COCOeval(coco, coco_dt, iouType=\"bbox\")\r\n    coco_eval.evaluate()\r\n    coco_eval.accumulate()\r\n    coco_eval.summarize()\r\n\r\ndef main():\r\n    session = ort.InferenceSession(MODEL_PATH, providers=[\"CUDAExecutionProvider\"])\r\n\r\n\r\n#    run_inference(data_paths[\"train_images\"], data_paths[\"annotations_train\"], session, \"train2017\")\r\n    run_inference(data_paths[\"val_images\"], data_paths[\"annotations_val\"], session, \"val2017\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @ZCzzzzzz, thank you for your interest in YOLOv5 ðŸš€! \n\nIt seems you are running into issues when using ONNX Runtime for inference with your YOLOv5 model, leading to zero results in your evaluation metrics. To assist you better, could you kindly provide a **minimum reproducible example (MRE)**? This should include the specific input data, model conversion details, and any additional configurations you used. The goal is to allow us to reproduce and debug the issue effectively. ðŸ™\n\nIn the meantime, please ensure the following:\n- You are using **Python>=3.8.0** with all required dependencies installed, including PyTorch and ONNX Runtime.\n- You followed the proper export process when converting your `yolov5s.pt` model to ONNX format.\n- You verified that the ONNX model outputs are as expected before proceeding with COCO evaluation.\n\nAdditionally, double-check the preprocessing pipeline to confirm that the inputs to your ONNX model match the same format and normalization as expected by the original PyTorch model.\n\nThis is an automated response, but an Ultralytics engineer will review the details of your issue and assist you further soon. Thank you for your patience! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@ZCzzzzzz it seems your ONNX runtime inference is producing zero results, which could be due to several factors. Please check the following:\n\n1. **Input preprocessing**: Ensure the input images are preprocessed correctly to match the input format used during training. Verify the resizing, normalization, and image dimension order (e.g., RGB vs. BGR).\n\n2. **Model outputs**: Double-check that the ONNX model outputs match the expected format. You may use tools like Netron to inspect the model structure.\n\n3. **Non-Max Suppression (NMS)**: Validate your custom NMS implementation. Consider testing the ONNX model with simpler inputs to confirm predictions before applying NMS.\n\n4. **Export issues**: Ensure the ONNX model was exported correctly using the recommended YOLOv5 export command:\n   ```\n   python export.py --weights yolov5s.pt --img 640 --batch 1 --device 0 --include onnx\n   ```\n\n5. **ONNX runtime setup**: Verify that your ONNX runtime environment (e.g., CUDAExecutionProvider) is set up correctly.\n\nIf the issue persists, try running inference with the PyTorch model to confirm that the problem is isolated to the ONNX workflow. For more details, refer to the [YOLOv5 ONNX Export Guide](https://github.com/ultralytics/yolov5/issues?q=ONNX)."
      }
    ]
  },
  {
    "issue_number": 13338,
    "title": "ONNXRuntime-Cpp and ONNXRuntime python give different results:",
    "author": "devendraswamy",
    "state": "open",
    "created_at": "2024-10-01T09:34:20Z",
    "updated_at": "2024-12-17T12:34:25Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am facing the problem with YOLOV5 model. While I am testing my Python ONNX code, all the bounding box (bbox) values are correct. However, when I perform the same process with my C++ code, I am getting incorrect bbox values.\r\n\r\nthe image processed in ptyhon code:\r\nimage_data = np.expand_dims(image_data, axis=0) # Add batch dimension\r\n\r\nand feed that image to python pyd file (c++ inference file complied to pyd)\r\n\r\nauto output_tensors = session.Run(Ort::RunOptions{ nullptr }, input_names, &input_tensor, 1, output_names, 1);\n\n### Additional\n\ncomplied or build C++ code is:\r\n\r\n#include <onnxruntime_cxx_api.h>\r\n#include <pybind11/pybind11.h>\r\n#include <pybind11/numpy.h>\r\n#include <vector>\r\n#include <array>\r\n#include <string>\r\n#include <iostream>\r\n\r\nusing namespace std;\r\nnamespace py = pybind11;\r\n\r\nclass OnnxModel {\r\npublic:\r\n    OnnxModel(const std::string& model_path)\r\n        : env(ORT_LOGGING_LEVEL_WARNING, \"OnnxModel\"),\r\n          session(env, std::wstring(model_path.begin(), model_path.end()).c_str(), Ort::SessionOptions()) \r\n    {\r\n        Ort::AllocatorWithDefaultOptions allocator;\r\n\r\n        // Get input and output names as Ort::AllocatedStringPtr\r\n        Ort::AllocatedStringPtr input_name_alloc = session.GetInputNameAllocated(0, allocator);\r\n        Ort::AllocatedStringPtr output_name_alloc = session.GetOutputNameAllocated(0, allocator);\r\n\r\n        // Convert the Ort::AllocatedStringPtr to std::string using the get() method\r\n        input_name = std::string(input_name_alloc.get());\r\n        output_name = std::string(output_name_alloc.get());\r\n\r\n        // Optional: Print the input and output names for debugging\r\n        std::cout << \"Input name: \" << input_name << std::endl;\r\n        std::cout << \"Output name: \" << output_name << std::endl;\r\n    }\r\n\r\n    // Accept a 4D numpy array: (batch_size, channels, height, width)\r\n    py::array_t<float> run(py::array_t<float> input_array) {\r\n        // Request a buffer from the numpy array\r\n        py::buffer_info buf = input_array.request();\r\n\r\n        // Check that the input is indeed a 4-dimensional array\r\n        if (buf.ndim != 4) {\r\n            throw std::runtime_error(\"Input should be a 4-dimensional array (batch_size, channels, height, width)\");\r\n        }\r\n\r\n        // Convert numpy array data to std::vector<float>\r\n        std::vector<float> input_data(static_cast<float*>(buf.ptr), \r\n                                      static_cast<float*>(buf.ptr) + buf.size);\r\n\r\n        // Run the inference\r\n        return run_inf(input_data, {1, 3, 640, 640});  // Adjust shape based on your model's input\r\n    }\r\n\r\n    py::array_t<float> run_inf(const std::vector<float>& input_data, const std::array<int64_t, 4>& input_shape) {\r\n        // Create input tensor\r\n        Ort::MemoryInfo memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);\r\n        Ort::Value input_tensor = Ort::Value::CreateTensor<float>(\r\n            memory_info,\r\n            const_cast<float*>(input_data.data()),\r\n            input_data.size(),\r\n            input_shape.data(),\r\n            input_shape.size()\r\n        );\r\n\r\n        // Prepare input and output names\r\n        const char* input_names[] = { input_name.c_str() };\r\n        const char* output_names[] = { output_name.c_str() };\r\n\r\n        // Run the model\r\n        auto output_tensors = session.Run(Ort::RunOptions{ nullptr }, input_names, &input_tensor, 1, output_names, 1);\r\n\r\n        // Get the output data\r\n        float* output_data = output_tensors[0].GetTensorMutableData<float>();\r\n        size_t output_count = output_tensors[0].GetTensorTypeAndShapeInfo().GetElementCount();\r\n\r\n        // Create a numpy array from the output data\r\n        return py::array_t<float>(output_count, output_data);\r\n    }\r\n\r\nprivate:\r\n    Ort::Env env;\r\n    Ort::Session session;\r\n    std::string input_name;\r\n    std::string output_name;\r\n};\r\n\r\nPYBIND11_MODULE(onnx_loader, m) {\r\n    py::class_<OnnxModel>(m, \"OnnxModel\")\r\n        .def(py::init<const std::string&>())\r\n        .def(\"run\", &OnnxModel::run);\r\n}\r\n\r\nImage feeding from python code:\r\n\r\n# Function to preprocess the image\r\ndef preprocess_image(image_path, input_size=(640, 640)):\r\n    # Load the image using OpenCV\r\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)  # Load image in color mode\r\n    if image is None:\r\n        raise ValueError(f\"Could not open or find the image: {image_path}\")\r\n    # Convert from BGR to RGB format\r\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n    # Resize the image to match the input size expected by the model\r\n    image = cv2.resize(image, input_size)\r\n    # Normalize the image to [0, 1] range\r\n    image = image.astype(np.float32) / 255.0  # Convert to float and normalize\r\n    # Rearrange the image to CHW format (1, C, H, W)\r\n    image_data = np.transpose(image, (2, 0, 1))  # Convert to CHW format\r\n    image_data = np.expand_dims(image_data, axis=0)  # Add batch dimension\r\n    print(f\"Image preprocessed: type = {type(image_data)}, shape = {image_data.shape}\")\r\n    return image_data, image  # Return the preprocessed image data\r\n\r\n",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @devendraswamy, thank you for reaching out with your issue regarding YOLOv5 ðŸš€! This is an automated response to guide you further, and an Ultralytics engineer will be with you soon.\n\nPlease make sure you are following our [Tutorials](https://docs.ultralytics.com/yolov5/) to ensure your setup is correct. They provide a helpful starting point for concepts including [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) and [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** so we can better assist you.\n\nIn your case, ensure that both Python and C++ environments use the same preprocessing steps and ONNX model settings. Discrepancies could lead to different outputs.\n\n## Requirements\n\nEnsure you have [**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed and are using [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To set up:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in various environments. Consider using these resources with dependencies preinstalled, including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/):\n\n- **Notebooks** with free GPU support: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n\n- **Google Cloud** Deep Learning VM: Check our [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n\n- **Amazon** Deep Learning AMI: See our [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n\n- **Docker Image**: Check our [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nA green badge means all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are passing. These tests verify correct operation on macOS, Windows, and Ubuntu.\n\n## Introducing YOLOv8 ðŸš€\n\nCheck out [YOLOv8](https://github.com/ultralytics/ultralytics), our latest model designed for superior performance in object detection, segmentation, and classification. Get started with:\n```bash\npip install ultralytics\n```\n\nFeel free to provide more details as needed. We'll get back to you soon! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@devendraswamy it seems like the discrepancy might be due to differences in preprocessing or input tensor handling between your Python and C++ implementations. Ensure that both environments use the same image preprocessing steps and input tensor shapes. Additionally, verify that the ONNX model and runtime versions are consistent across both implementations. If the issue persists, consider checking the ONNX model's input and output names and dimensions in both environments to ensure they match."
      },
      {
        "user": "ZCzzzzzz",
        "body": "hello,Can you provide a copy of the Python version of the code for ort inference? I have some questions on my side"
      }
    ]
  },
  {
    "issue_number": 13445,
    "title": "Problem converting Yolov5n .pt to .tflite",
    "author": "juliermeSilva",
    "state": "open",
    "created_at": "2024-12-06T01:58:09Z",
    "updated_at": "2024-12-16T13:57:49Z",
    "labels": [
      "bug",
      "detect",
      "exports"
    ],
    "body": "Hello everyone.\r\nI converted a Yolov5 nano network from **_.pt_** format to **_.tflite_** format. The network only has two classes (**Black Ball** and **Silver Ball**). When using the .tflite file, the network stopped detecting Silver Ball. It only detects Black Ball. This behavior does not occur when I use the .pt format.\r\nThe code used for training is this:\r\n```\r\n!pip install ultralytics\r\n\r\nfrom ultralytics import YOLO\r\nmodel = YOLO(\"yolov5n.yaml\")\r\n\r\nimport wandb\r\nfrom kaggle_secrets import UserSecretsClient\r\nuser_secrets = UserSecretsClient()\r\nsecret_value_0 = user_secrets.get_secret(\"wandb_api\")\r\nwandb.login(key=secret_value_0)\r\n\r\nresults = model.train(\r\n    data='/kaggle/input/dataset/ball-black-silver-yolov5n/data.yaml', \r\n    epochs=100, \r\n    patience=30, \r\n    device='0,1', \r\n    model='yolov5n.pt', \r\n    lr0=0.001, \r\n    freeze=10,\r\n    hsv_h=0.015,      \r\n    hsv_s=0.7,        \r\n    hsv_v=0.4,        \r\n    degrees=10.0,     \r\n    translate=0.1,    \r\n    scale=0.5,        \r\n    shear=2.0,        \r\n    fliplr=0.5,       \r\n    mosaic=1.0,       \r\n    mixup=0.2,        \r\n    warmup_epochs=3.0,\r\n    cos_lr=True,\r\n    weight_decay=0.0001\r\n)\r\n\r\n#Export from .pt to .onnx\r\nmodel.export(format='onnx')\r\n```\r\n\r\nMy file data.yaml:\r\n```\r\ntrain: /kaggle/input/dataset/ball-black-silver-yolov5n/train/images\r\nval: /kaggle/input/dataset/ball-black-silver-yolov5n/valid/images\r\ntest: /kaggle/input/dataset/ball-black-silver-yolov5n/test/images\r\n\r\nnc: 2\r\nnames: ['Black Ball', 'Silver Ball']\r\n\r\n\r\nroboflow:\r\n  workspace: testingai-dfp2w\r\n  project: testing-ai-obr\r\n  version: 2\r\n  license: CC BY 4.0\r\n  url: https://universe.roboflow.com/testingai-dfp2w/testing-ai-obr/dataset/2\r\n```\r\n\r\n\r\nThe yolov5n.pt and yolov5n.onnx formats worked perfectly on a more robust computer (e.g. a university laptop).\r\nI now need to embed the Yolov5n network on a Raspberry Pi 3B.\r\nI then converted the network from the .pt format to the .tflite format.\r\nThe conversion code used was this:\r\n```\r\n!pip install ultralytics\r\n\r\nfrom ultralytics import YOLO\r\nprint(\"Sucess load python libs!\")\r\n\r\n!cp /kaggle/input/models-obr/yolov5n-obr.pt /kaggle/working\r\npt_model_path = \"/kaggle/working/yolov5n-obr.pt\"\r\nmodel = YOLO(pt_model_path)\r\nprint(\"Sucess load yolov5n-obr!\")\r\n\r\nmodel = YOLO(pt_model_path)\r\nprint(model.names)\r\n\r\nmodel.export(format=\"tflite\", imgsz=640, optimize=None)\r\n\r\n!yolo val task=detect model=/kaggle/working/yolov5n-obr_saved_model/yolov5n-obr_float32.tflite imgsz=640 data=/kaggle/input/dataset/ball-black-silver-yolov5n/data.yaml\r\n```\r\nValidation results:\r\n```\r\nUltralytics 8.3.43 ðŸš€ Python-3.10.14 torch-2.4.0 CPU (Intel Xeon 2.00GHz)\r\nLoading /kaggle/working/yolov5n-obr_saved_model/yolov5n-obr_float32.tflite for TensorFlow Lite inference...\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nSetting batch=1 input of shape (1, 3, 640, 640)\r\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\r\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 17.2MB/s]\r\nval: Scanning /kaggle/input/dataset/ball-black-silver-yolov5n/valid/labels... 71\r\nval: WARNING âš ï¸ Cache directory /kaggle/input/dataset/ball-black-silver-yolov5n/valid is not writeable, cache not saved.\r\n                 Class     Images  Instances      Box(P          R      mAP50  m\r\n                   all        710       1891      0.997       0.98      0.993      0.871\r\n            Black Ball        571        690      0.996      0.974      0.992      0.868\r\n           Silver Ball        673       1201      0.998      0.986      0.994      0.875\r\nSpeed: 0.9ms preprocess, 129.1ms inference, 0.0ms loss, 0.8ms postprocess per image\r\nResults saved to runs/detect/val\r\nðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\r\n```\r\n\r\nThe training code and the conversion code were all run on the Kaggle platform.\r\nEverything was going perfectly until I tried to detect silver balls.\r\nThis is my test image (**img1.jpg**):\r\n![img1](https://github.com/user-attachments/assets/e37a4c96-3e2a-4c05-8377-5da8ad9dd887)\r\n\r\nThis is the code I am using to perform inferences using the yolov5n-obr_float32_v2.tflite network (**tflite_black_silver_test_3.py**):\r\n```\r\nimport tflite_runtime.interpreter as tflite\r\nimport cv2\r\nimport numpy as np\r\n\r\nid_img = 1\r\n\r\n# Paths to the model and label map\r\nMODEL_PATH = \"yolov5n-obr_float32_v2.tflite\"\r\nLABELMAP_PATH = \"labelmap.txt\"\r\n\r\ndef calculate_iou(box1, box2):\r\n    x1 = max(box1[0], box2[0])\r\n    y1 = max(box1[1], box2[1])\r\n    x2 = min(box1[2], box2[2])\r\n    y2 = min(box1[3], box2[3])\r\n    # Intersection area    \r\n    intersection = max(0, x2 - x1) * max(0, y2 - y1)     \r\n    # Areas of the bounding boxes\r\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\r\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\r\n    # Union area        \r\n    union = box1_area + box2_area - intersection     \r\n    return intersection / union if union > 0 else 0\r\n\r\n\r\ndef nms(boxes, confidences, iou_threshold=0.5):    \r\n    indices = np.argsort(confidences)[::-1]  # Sort by confidence (descending)\r\n    keep = []\r\n    while len(indices) > 0:\r\n        current = indices[0]\r\n        keep.append(current)\r\n        others = indices[1:]        \r\n        # Calculate IoU of the current box with the others\r\n        ious = [calculate_iou(boxes[current], boxes[idx]) for idx in others]\r\n        # Keep only boxes with IoU below the threshold\r\n        indices = [idx for i, idx in enumerate(others) if ious[i] <= iou_threshold]    \r\n    return keep\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Load the label map\r\n    with open(LABELMAP_PATH, \"r\") as f:\r\n        labels = [line.strip() for line in f.readlines()]\r\n\r\n    # Load the model\r\n    interpreter = tflite.Interpreter(model_path=MODEL_PATH)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get tensor details\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    \r\n    # Load a test image with OpenCV\r\n    image_path = \"img\" + str(id_img) + \".jpg\"\r\n    image = cv2.imread(image_path)\r\n    original_height, original_width, _ = image.shape\r\n\r\n    # Resize the image to match the model's input size\r\n    input_size = input_details[0]['shape'][1:3]\r\n    resized_image = cv2.resize(image, (input_size[1], input_size[0]))\r\n\r\n    # Check the expected type of the input tensor\r\n    if input_details[0]['dtype'] == np.uint8:\r\n        input_data = np.expand_dims(resized_image, axis=0).astype(np.uint8)\r\n    else:\r\n        input_data = np.expand_dims(resized_image.astype(np.float32) / 255.0, axis=0)  # Normalization\r\n\r\n    # Perform inference\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n    interpreter.invoke()\r\n\r\n    # Get the results\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n    # Reshape for easier manipulation\r\n    output_data = np.squeeze(output_data)  # Remove dimensions of size 1 (e.g., batch size)\r\n    \r\n    # Store predictions for NMS\r\n    all_boxes = []\r\n    all_confidences = []\r\n    all_labels = []\r\n\r\n    # Iterate through the predictions\r\n    for i in range(output_data.shape[1]):  # Iterate through 8400 predictions\r\n        x_center, y_center, width, height, confidence, class_id = output_data[:, i]\r\n        # Filter by confidence\r\n        if confidence >= 0.65:  # Adjust confidence threshold as needed\r\n            # Calculate normalized coordinates (x_min, y_min, x_max, y_max)\r\n            x_min = x_center - (width / 2)\r\n            y_min = y_center - (height / 2)\r\n            x_max = x_center + (width / 2)\r\n            y_max = y_center + (height / 2)\r\n            # Convert normalized coordinates to absolute (denormalization)\r\n            y_min_abs = int(y_min * original_height)\r\n            x_min_abs = int(x_min * original_width)\r\n            y_max_abs = int(y_max * original_height)\r\n            x_max_abs = int(x_max * original_width)\r\n            all_boxes.append([x_min_abs, y_min_abs, x_max_abs, y_max_abs])\r\n            all_confidences.append(confidence)\r\n            all_labels.append(int(class_id))\r\n\r\n    # Apply Non-Maximum Suppression (NMS)\r\n    nms_indices = nms(all_boxes, all_confidences, iou_threshold=0.5)\r\n\r\n    # Display results filtered by NMS\r\n    for idx in nms_indices:\r\n        box = all_boxes[idx]\r\n        confidence = all_confidences[idx]\r\n        label = all_labels[idx]\r\n        cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\r\n        cv2.putText(image, f\"{labels[label]}: {confidence:.2f}\", (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n\r\n    # Save the image with bounding boxes\r\n    output_path = \"result.jpg\"\r\n    cv2.imwrite(output_path, image)\r\n```\r\n\r\nThe **resulting image** from running the tflite_black_silver_test_3.py code is the one below. Note that the silver balls are not being detected, only the black ball.\r\n![result4](https://github.com/user-attachments/assets/06174278-79ef-4066-a879-4626e22e3553)\r\n\r\n\r\n**Kaggle settings for conversion:**\r\nultralytics-8.3.43\r\nInstalled dependencies:\r\n```\r\nUltralytics 8.3.43 ðŸš€ Python-3.10.14 torch-2.4.0 CPU (Intel Xeon 2.00GHz)\r\nYOLOv5n summary (fused): 211 layers, 2,182,054 parameters, 0 gradients, 5.8 GFLOPs\r\n\r\nPyTorch: starting from '/kaggle/working/yolov5n-obr.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (4.4 MB)\r\nrequirements: Ultralytics requirements ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'onnx2tf>1.17.5,<=1.22.3', 'onnxslim>=0.1.31', 'tflite_support', 'onnxruntime'] not found, attempting AutoUpdate...\r\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com/\r\nCollecting sng4onnx>=1.0.1\r\n  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting onnx_graphsurgeon>=0.3.26\r\n  Downloading onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl.metadata (8.1 kB)\r\nCollecting onnx2tf<=1.22.3,>1.17.5\r\n  Downloading onnx2tf-1.22.3-py3-none-any.whl.metadata (136 kB)\r\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 136.6/136.6 kB 6.3 MB/s eta 0:00:00\r\nCollecting onnxslim>=0.1.31\r\n  Downloading onnxslim-0.1.43-py3-none-any.whl.metadata (4.2 kB)\r\nCollecting tflite_support\r\n  Downloading tflite_support-0.4.4-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.4 kB)\r\nCollecting onnxruntime\r\n  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\r\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from onnx_graphsurgeon>=0.3.26) (1.26.4)\r\nRequirement already satisfied: onnx>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from onnx_graphsurgeon>=0.3.26) (1.17.0)\r\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxslim>=0.1.31) (1.13.3)\r\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxslim>=0.1.31) (21.3)\r\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tflite_support) (1.4.0)\r\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tflite_support) (24.3.25)\r\nRequirement already satisfied: protobuf<4,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from tflite_support) (3.20.3)\r\nCollecting sounddevice>=0.4.4 (from tflite_support)\r\n  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\r\nRequirement already satisfied: pybind11>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from tflite_support) (2.13.6)\r\nCollecting coloredlogs (from onnxruntime)\r\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\nRequirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->tflite_support) (1.16.0)\r\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\r\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxslim>=0.1.31) (3.1.2)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxslim>=0.1.31) (1.3.0)\r\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->tflite_support) (2.22)\r\nDownloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\r\nDownloading onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl (56 kB)\r\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 56.4/56.4 kB 230.3 MB/s eta 0:00:00\r\nDownloading onnx2tf-1.22.3-py3-none-any.whl (435 kB)\r\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 435.0/435.0 kB 31.3 MB/s eta 0:00:00\r\nDownloading onnxslim-0.1.43-py3-none-any.whl (142 kB)\r\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 142.9/142.9 kB 300.7 MB/s eta 0:00:00\r\nDownloading tflite_support-0.4.4-cp310-cp310-manylinux2014_x86_64.whl (60.8 MB)\r\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 60.8/60.8 MB 198.1 MB/s eta 0:00:00a 0:00:01\r\nDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\r\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.3/13.3 MB 209.2 MB/s eta 0:00:00 0:00:01\r\nDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\r\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46.0/46.0 kB 248.4 MB/s eta 0:00:00\r\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 86.8/86.8 kB 282.8 MB/s eta 0:00:00\r\nInstalling collected packages: sng4onnx, onnx2tf, humanfriendly, sounddevice, onnxslim, onnx_graphsurgeon, coloredlogs, tflite_support, onnxruntime\r\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx2tf-1.22.3 onnx_graphsurgeon-0.5.2 onnxruntime-1.20.1 onnxslim-0.1.43 sng4onnx-1.0.4 sounddevice-0.5.1 tflite_support-0.4.4\r\n\r\nrequirements: AutoUpdate success âœ… 13.3s, installed 6 packages: ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'onnx2tf>1.17.5,<=1.22.3', 'onnxslim>=0.1.31', 'tflite_support', 'onnxruntime']\r\n```\r\n**Settings on the Raspberry PI 3B:**\r\nPython: 3.11.2\r\ntflite-runtime 2.14.0\r\n\r\n\r\nWhat could I be doing wrong?\r\n\r\nWhy does the network work perfectly in the .pt format but this problem occurs in the .tflite format?\r\n\r\nI appreciate any help!\r\n\r\n_Originally posted by @juliermeSilva in https://github.com/ultralytics/yolov5/discussions/13444_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @juliermeSilva, thank you for your interest in YOLOv5 ðŸš€! This is an automated response to help guide you, and an Ultralytics engineer will assist you further shortly.\n\nIf this is a ðŸ› Bug Report, it would be helpful to provide a **minimum reproducible example (MRE)**. In this case, please ensure that any custom code, dataset details, or additional information impacting the `.pt` to `.tflite` conversion is included. The information you've provided so far is quite detailed, which is great! Ensuring consistency in the training, exporting, and inference pipelines is critical for debugging discrepancies.\n\nFor general issues with custom training and inference â“, please make sure:\n- Your dataset is labeled accurately and corresponds well to your `data.yaml`.\n- You're performing proper validation of the `.tflite` model after export.\n- You're adhering to best practices for quantization (if applicable) or export formats when converting models for edge devices like Raspberry Pi.\n\nRegarding your Python environment:\n- Please confirm you are using **Python>=3.8.0**, and that all dependencies, such as **PyTorch**, TensorFlow Lite runtime, and OpenCV, are up to date.\n\nIf you're utilizing a custom inference pipeline (like `tflite_runtime.interpreter` in your case), ensure that input preprocessing (e.g., resizing, normalization) matches the expected format of the converted model.\n\nLastly, if the `.pt` to `.tflite` conversion step was performed with specific optimizations, such as quantization, simplifying your model/export settings to ensure compatibility may help narrow down the issue.\n\nLet us know if further clarification is needed, and thank you for sharing your detailed report! ðŸ˜Š"
      },
      {
        "user": "juliermeSilva",
        "body": "Sending extra data that I forgot to send in the previous post.\r\nImage generated by the NETRON application from the **yolov5n-obr.onnx** file generated by the conversion process using Ultralytics (code in the previous post):\r\n![Captura de tela de 2024-12-06 19-57-17](https://github.com/user-attachments/assets/f7156c22-2ec0-4638-9d39-087995250fee)\r\n\r\n\r\nImage generated by the NETRON application from the **yolov5n-obr.tflite** file generated by the conversion process using Ultralytics (code in the previous post):\r\n![Captura de tela de 2024-12-06 19-57-48](https://github.com/user-attachments/assets/93895ebd-7221-49d9-93c5-1f0f2bc579b5)\r\n\r\nIt is possible to observe using NETRON that in the .tflite file no information about the classes that the model manipulates is visible. However, the .onnx file explicitly displays information about the model classes in Metadata.\r\n\r\nI do not know if this is normal behavior for a .tflite file or if there really is a problem in the conversion from Yolov5n to .tflite using Ultralytics.\r\n"
      },
      {
        "user": "juliermeSilva",
        "body": "Hello everyone.\r\nI ran some more tests.\r\nI converted versions of the **YOLO 5, 8 and 11** models already trained by the Ultralytics team. I tested different model sizes (Nano, Small, Medium, Large, XLarge).\r\nAlways using this command:\r\n```\r\n#model = YOLO(\"yolov5n.pt\") # YOLOV5 nano: Model pre-trained on COCO base\r\n#model = YOLO(\"yolov5s.pt\") # YOLOV5 small: Model pre-trained on COCO base\r\n#model = YOLO(\"yolov5m.pt\") # YOLOV5 small: Model pre-trained on COCO base\r\n#model = YOLO(\"yolov5l.pt\") # YOLOV5 small: Model pre-trained on COCO base\r\n#model = YOLO(\"yolov5x.pt\") # YOLOV5 small: Model pre-trained on COCO base\r\n\r\n#model = YOLO(\"yolov8n.pt\") # YOLOV8s small: Model pre-trained on COCO base\r\nmodel = YOLO(\"yolov8s.pt\") # YOLOV8s small: Model pre-trained on COCO base\r\n\r\nmodel.export(format=\"tflite\", imgsz=640, optimize=None)\r\n```\r\n\r\nThe result remained the same as reported in previous posts, that is, only the first class was detected by the models. The others were ignored. I don't know why this is happening when I use the tflite_runtime library to perform inferences.\r\n\r\nI then used the Ultralytics library to load the **yolov5n-obr.tflite** model and test inferences, describing the results in the terminal. Note that the model behaves as expected, making correct inferences for all test images.\r\n\r\nCode used:\r\n```\r\n# Inference yolov5n-obr.tflite\r\nfrom ultralytics import YOLO\r\nmodel_path = \"/kaggle/working/yolov5n-obr_saved_model/yolov5n-obr_float32.tflite\"\r\nimage_path = \"/kaggle/input/test-black-silver-ball/img1.jpg\"\r\n#image_path = \"/kaggle/input/test-black-silver-ball/img2.jpg\"\r\n#image_path = \"/kaggle/input/test-black-silver-ball/img3.jpg\"\r\n#image_path = \"/kaggle/input/test-black-silver-ball/img4.jpg\"\r\n\r\n# Load the YOLO model in TFLite format\r\nmodel = YOLO(model_path)\r\n\r\n# Perform the inference\r\nresults = model(image_path)\r\n\r\n# Check if there was any inference\r\ndetected_classes = []\r\nfor result in results:  \r\n    if (result.boxes is not None):  \r\n        for box in result.boxes:  \r\n            cls_id = int(box.cls[0])  \r\n            conf = box.conf[0]  \r\n            if (conf > 0.5):\r\n                detected_classes.append(result.names[cls_id])\r\n\r\n# Display detected ball types\r\nprint(\"Types of balls detected in the image:\")\r\nif (detected_classes):\r\n    for detected_class in detected_classes:\r\n        print(f\"- {detected_class}\")\r\nelse:\r\n    print(\"No ball detected.\")\r\n```\r\n\r\nExample result (for i**mg1.jpg**):\r\n```\r\nLoading /kaggle/working/yolov5n-obr_saved_model/yolov5n-obr_float32.tflite for TensorFlow Lite inference...\r\n\r\nimage 1/1 /kaggle/input/test-black-silver-ball/img1.jpg: 640x640 1 Black Ball, 2 Silver Balls, 211.4ms\r\nSpeed: 3.2ms preprocess, 211.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\r\nTypes of balls detected in the image:\r\n- Black Ball\r\n- Silver Ball\r\n- Silver Ball\r\n```\r\n\r\n\r\nI hope this information helps in the process of figuring out why the **tflite_runtime** library is not able to correctly infer all the classes in which the model was trained.\r\n\r\nI am available if any further information is needed to find the solution to this problem.\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 13141,
    "title": "how to convert pt to onnx to trt",
    "author": "gdfapokgdpafog",
    "state": "closed",
    "created_at": "2024-06-27T03:11:08Z",
    "updated_at": "2024-12-16T10:28:50Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nhow to convert pt to onnx to trt\n\n### Additional\n\nim doing this\r\n\r\npython export.py --weights best.pt --include onnx --opset 12\r\n\r\nafter trtexec --onnx=best.onnx --saveEngine=best.trt\r\n\r\nafter I try to load the model I get this\r\n![image](https://github.com/ultralytics/yolov5/assets/173977570/253a6fa6-9616-48bc-884e-9619323b8926)\r\n\r\nI used to be able to do it, but six months later I forgot how I did it.\r\n\r\nPlease help ",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @gdfapokgdpafog, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "gdfapokgdpafog",
        "body": "cuda 11.6\r\n\r\ntensorrt 8.4.1.5\r\n\r\npytorch 1.9.0"
      },
      {
        "user": "glenn-jocher",
        "body": "@gdfapokgdpafog hello,\n\nThank you for reaching out! It looks like you're on the right track with exporting your model from PyTorch to ONNX and then to TensorRT. Let's go through the steps to ensure everything is set up correctly.\n\n1. **Export to ONNX**:\n   You've already done this with:\n   ```bash\n   python export.py --weights best.pt --include onnx --opset 12\n   ```\n   This should generate `best.onnx`.\n\n2. **Convert ONNX to TensorRT**:\n   Using `trtexec` is the correct approach:\n   ```bash\n   trtexec --onnx=best.onnx --saveEngine=best.trt\n   ```\n\n3. **Loading the TensorRT Engine**:\n   Ensure that your environment is correctly set up to use TensorRT. Sometimes, issues can arise from mismatched versions or incorrect paths.\n\nGiven the error message you encountered, it seems there might be an issue with the TensorRT engine creation. Here are a few things to check:\n\n- **Compatibility**: Ensure that your CUDA, TensorRT, and PyTorch versions are compatible. You mentioned using CUDA 11.6, TensorRT 8.4.1.5, and PyTorch 1.9.0. These should generally be compatible, but it's always good to double-check the [NVIDIA compatibility matrix](https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html).\n\n- **ONNX Model**: Verify that the ONNX model is correctly exported and can be loaded without errors. You can use the `onnx` Python package to check the model:\n  ```python\n  import onnx\n\n  model = onnx.load(\"best.onnx\")\n  onnx.checker.check_model(model)\n  ```\n\n- **TensorRT Logs**: When running `trtexec`, add the `--verbose` flag to get more detailed logs, which can help diagnose the issue:\n  ```bash\n  trtexec --onnx=best.onnx --saveEngine=best.trt --verbose\n  ```\n\nIf the issue persists, please provide any additional logs or error messages you receive. This will help us better understand the problem and provide more targeted assistance.\n\nFor more detailed instructions on exporting models, you can refer to the [Ultralytics YOLOv5 Model Export Documentation](https://docs.ultralytics.com/yolov5/tutorials/model_export/).\n\nFeel free to reach out if you have any further questions or need additional assistance. The YOLO community and the Ultralytics team are here to help!"
      }
    ]
  },
  {
    "issue_number": 11395,
    "title": "tflite inference with tflite_runtime",
    "author": "Vikram12301",
    "state": "closed",
    "created_at": "2023-04-19T16:12:04Z",
    "updated_at": "2024-12-15T21:17:06Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"best-fp16.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\nimage = Image.open(\"image.tif\")\r\nimage = image.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\r\nimage_data = np.array(image).astype(np.float32)\r\nimage_data /= 255.0\r\nimage_data = np.expand_dims(image_data, axis=0)\r\ninterpreter.set_tensor(input_details[0]['index'], image_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n\r\nUsing the above code for inference. But getting a random array as output. But I am getting the expected answer as output when inferencing with torch.hub\r\n\r\nExpected result:\r\ntensor([[5.01359e+02, 1.09255e+03, 6.92128e+02, 1.11753e+03, 9.24892e-01, 0.00000e+00]])\r\n\r\nResult I got for the above code:\r\n[[[  0.0053059    0.004525    0.014499    0.011126  3.7081e-05     0.99995]\r\n  [  0.0052993   0.0029876    0.021002    0.013376  4.0981e-05     0.99999]\r\n  [  0.0054641    0.005734   0.0095944    0.034277  8.8067e-06     0.99998]\r\n  ...\r\n  [    0.97123     0.96511    0.026191    0.075059  2.2332e-07     0.99999]\r\n  [    0.96813     0.95563      0.0322     0.20285  2.2449e-07     0.99999]\r\n  [    0.94753     0.98328     0.17076     0.02539  2.7026e-06     0.99998]]]\r\n\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @Vikram12301, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/#tutorials) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@Vikram12301 based on the code you've provided, it appears that you are trying to run inference on a TensorFlow Lite model using the `tf.lite.Interpreter` API. However, you may be getting unexpected results as the output you've shared is a 3-Dimensional numpy array, whereas the expected output is a 2-Dimensional PyTorch tensor. \n\nIt is recommended to check if your TensorFlow Lite model was exported with the same input and output dimensions and data types as the PyTorch model you're comparing it to. Additionally, you could use the `interpreter.get_output_details()` API to check the dimensions and data types of the output tensor.\n\nIf you still face issues, you can share more details such as the TensorFlow Lite model file, and we will try our best to help you."
      },
      {
        "user": "Vikram12301",
        "body": "@glenn-jocher The output for 'interpreter.get_output_details()' is\r\n\r\n[{'name': 'StatefulPartitionedCall:0',\r\n  'index': 532,\r\n  'shape': array([    1, 25200,     6], dtype=int32),\r\n  'shape_signature': array([    1, 25200,     6], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0),\r\n  'quantization_parameters': {'scales': array([], dtype=float32),\r\n   'zero_points': array([], dtype=int32),\r\n   'quantized_dimension': 0},\r\n  'sparsity_parameters': {}}]\r\n\r\nBut the output for the below code\r\n\r\n```\r\nimport torch\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\nmodel = torch.hub.load('ultralytics/yolov5', 'custom','best-fp16.tflite' )\r\ninterpreter = tf.lite.Interpreter(model_path=\"best-fp16.tflite\")\r\ninput_details = interpreter.get_input_details()\r\nimage = Image.open('image.tif')\r\nimage = image.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\r\nresults = model(image)\r\n# Results\r\nresults.print()\r\nprint(results.xyxy[0])\r\n```\r\n\r\nSpeed: 8.6ms pre-process, 577.2ms inference, 2.8ms NMS per image at shape (1, 3, 640, 640)\r\ntensor([[255.95584, 423.64117, 344.48886, 433.97974,   0.90273,   0.00000]])\r\n\r\n\r\nThe same .tflite is working when using torch.hub and not working with tf.lite.Interpreter"
      }
    ]
  },
  {
    "issue_number": 13461,
    "title": "val_loss nan",
    "author": "lqh964165950",
    "state": "open",
    "created_at": "2024-12-14T09:04:58Z",
    "updated_at": "2024-12-14T20:25:57Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nå¯¹yolov5è¿›è¡Œæ”¹è¿›ï¼Œåœ¨headå’Œneckä¹‹é—´åŠ äº†ä¸€ä¸ªç‰¹å¾å¢žå¼ºæ¨¡å—ï¼Œå´å‡ºçŽ°å¦‚ä¸‹é—®é¢˜ï¼ŒéªŒè¯æŸå¤±æœ‰ä¸€æ®µæ—¶é—´ä¸ºnanï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ\r\n![val_nan](https://github.com/user-attachments/assets/106fa6c5-3ef0-4411-b634-d269b9514104)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @lqh964165950, thank you for your interest in YOLOv5 ðŸš€! It sounds like you've made some interesting custom modifications to YOLOv5 by adding a feature enhancement module. Let's work together to troubleshoot this validation loss issue.\n\nIf this is a ðŸ› Bug Report, we kindly request a **minimum reproducible example** to help us debug the problem. This includes:\n1. A clear explanation of the changes you made to the YOLOv5 model, especially the feature enhancement module you added.\n2. The exact steps and commands used to train and validate the model.\n3. Logs and outputs from your experiments, including any warnings or errors.\n4. Details of your dataset, including structure and image counts (if applicable).\n\nIf this is a custom training â“ Question, please provide as much detailed information as possible. Be sure to include screenshots or examples of your dataset, training logs, and loss plots. Additionally, check that you're following best practices for training, such as carefully tuning learning rates, verifying dataset quality, and using appropriate augmentation techniques.\n\n## Requirements\n\nEnsure you are using [**Python>=3.8.0**] with all necessary packages installed, including [**PyTorch>=1.8**]. To set up the environment:\n```bash\ngit clone the YOLOv5 repository  # clone\ncd into the directory\npip install requirements from the requirements file  # install\n```\n\n## Environments\n\nYOLOv5 supports multiple verified environments for running models, including notebooks with free GPU access, Google Cloud, Amazon AMI, and Docker. Please ensure your environment dependencies like CUDA, cuDNN, Python, and PyTorch are up to date, as out-of-date setups often cause instability.\n\n## Status\n\nIf all the tests in the YOLOv5 Continuous Integration (CI) workflow are passing, this indicates the base code is functioning correctly, and modifications are likely contributing to the issue. You can verify the training, validation, inference, export, and benchmarking features on various operating systems like macOS, Windows, and Ubuntu.\n\nðŸ” This is an automated response to help provide initial guidance. An Ultralytics engineer will take a look at your issue and assist you further as soon as possible."
      },
      {
        "user": "pderrenger",
        "body": "@lqh964165950 the issue of validation loss becoming `nan` often indicates instability in the training process. Since you've modified the YOLOv5 architecture by adding a feature enhancement module between the neck and head, the problem could stem from the following:\n\n1. **Gradient Instabilities**: Ensure that your modifications do not introduce exploding gradients. You can monitor gradients through debugging or by enabling gradient clipping.\n2. **Loss Computation**: Validate that the outputs from your feature enhancement module are compatible with the loss function expectations.\n3. **Learning Rate**: Experiment with lowering the learning rate, as architectural changes can affect training stability.\n4. **Data Issues**: Ensure your dataset is properly formatted and does not contain corrupted or inconsistent labels.\n\nFor debugging, consider starting with a smaller dataset and enabling verbose logging. Additionally, verify whether this issue persists with the latest YOLOv5 version. If the `nan` issue continues, inspect your custom module and its impact on the network's forward and backward passes. \n\nFor more details on YOLOv5 loss computation, refer to [this documentation](https://docs.ultralytics.com/reference/utils/loss/)."
      }
    ]
  },
  {
    "issue_number": 12911,
    "title": "Exception: cannot instantiate 'WindowsPath' on your system. Cache may be out of date",
    "author": "Codefreediver",
    "state": "closed",
    "created_at": "2024-04-12T19:35:41Z",
    "updated_at": "2024-12-13T18:08:15Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello friends,\r\n\r\nI'm having problem for the inference stage of this Kaggle competition:\r\n[https://www.kaggle.com/competitions/tensorflow-great-barrier-reef](url)\r\n\r\nI trained my model using the following code:\r\n```python\r\nFOLD`      = 1 \r\nDIM       = 2400\r\nMODEL     = 'yolov5n'\r\nBATCH     = 4\r\nEPOCHS    = 10\r\nOPTMIZER  = 'Adam'\r\n\r\n!python train.py\" --img {DIM}\\\r\n--batch {BATCH}\\\r\n--epochs {EPOCHS}\\\r\n--optimizer {OPTMIZER}\\\r\n--data \"D:\\Kaggle\\cots\\gbr.yaml\"\\\r\n--hyp \"D:\\Kaggle\\cots\\hyp.yaml\"\\\r\n--weights {MODEL}.pt\\\r\n```\r\n\r\nI download the pretrain weights \"best.pt\" from Wandb, than I uploaded it to my Kaggle notebook dataset. I tried to load the pretrain model for inference, but I'm really having a hard time:\r\n\r\n```python\r\nimport torch\r\nROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\r\n\r\nCKPT_PATH = '/kaggle/input/tfeqimg-2400/Final.pt' \r\nIMG_SIZE  = 2400\r\nCONF      = 0.25\r\nIOU       = 0.40\r\nAUGMENT   = True\r\nmodel = torch.hub.load('/kaggle/input/yolov7-0/yolov5-7.0', 'custom', source='local',force_reload = True , path=CKPT_PATH)`\r\n```\r\n\r\nAnd the error is: \r\n\r\n```python\r\n`YOLOv5 ðŸš€ 2024-4-12 Python-3.10.13 torch-2.1.2+cpu CPU\r\n\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\nFile /kaggle/input/yolov7-0/yolov5-7.0/hubconf.py:49, in _create(name, pretrained, channels, classes, autoshape, verbose, device)\r\n     48 try:\r\n---> 49     model = DetectMultiBackend(path, device=device, fuse=autoshape)  # detection model\r\n     50     if autoshape:\r\n\r\nFile /kaggle/input/yolov7-0/yolov5-7.0/models/common.py:345, in DetectMultiBackend.__init__(self, weights, device, dnn, data, fp16, fuse)\r\n    344 if pt:  # PyTorch\r\n--> 345     model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\r\n    346     stride = max(int(model.stride.max()), 32)  # model stride\r\n\r\nFile /kaggle/input/yolov7-0/yolov5-7.0/models/experimental.py:79, in attempt_load(weights, device, inplace, fuse)\r\n     78 for w in weights if isinstance(weights, list) else [weights]:\r\n---> 79     ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\r\n     80     ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/serialization.py:1014, in load(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\r\n   1013                 raise pickle.UnpicklingError(UNSAFE_MESSAGE + str(e)) from None\r\n-> 1014         return _load(opened_zipfile,\r\n   1015                      map_location,\r\n   1016                      pickle_module,\r\n   1017                      overall_storage=overall_storage,\r\n   1018                      **pickle_load_args)\r\n   1019 if mmap:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/serialization.py:1422, in _load(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\r\n   1421 unpickler.persistent_load = persistent_load\r\n-> 1422 result = unpickler.load()\r\n   1424 torch._utils._validate_loaded_sparse_tensors()\r\n\r\nFile /opt/conda/lib/python3.10/pathlib.py:962, in Path.__new__(cls, *args, **kwargs)\r\n    961 if not self._flavour.is_supported:\r\n--> 962     raise NotImplementedError(\"cannot instantiate %r on your system\"\r\n    963                               % (cls.__name__,))\r\n    964 return self\r\n\r\nNotImplementedError: cannot instantiate 'WindowsPath' on your system\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotImplementedError                       Traceback (most recent call last)\r\nFile /kaggle/input/yolov7-0/yolov5-7.0/hubconf.py:60, in _create(name, pretrained, channels, classes, autoshape, verbose, device)\r\n     59     except Exception:\r\n---> 60         model = attempt_load(path, device=device, fuse=False)  # arbitrary model\r\n     61 else:\r\n\r\nFile /kaggle/input/yolov7-0/yolov5-7.0/models/experimental.py:79, in attempt_load(weights, device, inplace, fuse)\r\n     78 for w in weights if isinstance(weights, list) else [weights]:\r\n---> 79     ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\r\n     80     ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/serialization.py:1014, in load(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\r\n   1013                 raise pickle.UnpicklingError(UNSAFE_MESSAGE + str(e)) from None\r\n-> 1014         return _load(opened_zipfile,\r\n   1015                      map_location,\r\n   1016                      pickle_module,\r\n   1017                      overall_storage=overall_storage,\r\n   1018                      **pickle_load_args)\r\n   1019 if mmap:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/serialization.py:1422, in _load(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\r\n   1421 unpickler.persistent_load = persistent_load\r\n-> 1422 result = unpickler.load()\r\n   1424 torch._utils._validate_loaded_sparse_tensors()\r\n\r\nFile /opt/conda/lib/python3.10/pathlib.py:962, in Path.__new__(cls, *args, **kwargs)\r\n    961 if not self._flavour.is_supported:\r\n--> 962     raise NotImplementedError(\"cannot instantiate %r on your system\"\r\n    963                               % (cls.__name__,))\r\n    964 return self\r\n\r\nNotImplementedError: cannot instantiate 'WindowsPath' on your system\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nException                                 Traceback (most recent call last)\r\nCell In[6], line 3\r\n      1 import torch\r\n----> 3 model = torch.hub.load('/kaggle/input/yolov7-0/yolov5-7.0', 'custom', source='local',force_reload = True , path=CKPT_PATH)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/hub.py:566, in load(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\r\n    562 if source == 'github':\r\n    563     repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \"load\",\r\n    564                                        verbose=verbose, skip_validation=skip_validation)\r\n--> 566 model = _load_local(repo_or_dir, model, *args, **kwargs)\r\n    567 return model\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/hub.py:595, in _load_local(hubconf_dir, model, *args, **kwargs)\r\n    592     hub_module = _import_module(MODULE_HUBCONF, hubconf_path)\r\n    594     entry = _load_entry_from_hubconf(hub_module, model)\r\n--> 595     model = entry(*args, **kwargs)\r\n    597 return model\r\n\r\nFile /kaggle/input/yolov7-0/yolov5-7.0/hubconf.py:83, in custom(path, autoshape, _verbose, device)\r\n     81 def custom(path='path/to/model.pt', autoshape=True, _verbose=True, device=None):\r\n     82     # YOLOv5 custom or local model\r\n---> 83     return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\r\n\r\nFile /kaggle/input/yolov7-0/yolov5-7.0/hubconf.py:78, in _create(name, pretrained, channels, classes, autoshape, verbose, device)\r\n     76 help_url = '[https://github.com/ultralytics/yolov5/issues/36](https://github.com/ultralytics/yolov5/issues/36%3C/span%3E%3Cspan) style=\"color:rgb(175,0,0)\">'\r\n     77 s = f'{e}. Cache may be out of date, try `force_reload=True` or see {help_url} for help.'\r\n---> 78 raise Exception(s) from e\r\n\r\nException: cannot instantiate 'WindowsPath' on your system. Cache may be out of date, try `force_reload=True` or see https://github.com/ultralytics/yolov5/issues/36 for help.\r\n```\r\n\r\nAs a newbie to both Kaggle and Yolo, I tried many solutions but failed. Please help me, thank you.\r\n\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @Codefreediver, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "Hello! Thanks for reaching out. ðŸ˜Š It seems the issue you're encountering is related to the path handling on different operating systems, specifically when loading your model weights. This is a common snag when paths formatted for Windows systems (using `WindowsPath`) are used in environments that do not support this path type, such as Linux-based systems often used in Kaggle notebooks.\n\nThe error message `NotImplementedError: cannot instantiate 'WindowsPath' on your system` suggests that the path object specific to Windows is not recognized on the Kaggle platform. As a solution, make sure your paths are defined in a way that's compatible with Unix-like systems, which typically use forward slashes (`/`) for paths. Also, ensure that your file paths are correctly pointing to the location where your weights are stored on Kaggle.\n\nHere's a slight adjustment to your code where you load the model:\n\n```python\nimport torch\n\n# Ensure the path is correctly specified for Unix-like systems\nCKPT_PATH = '/kaggle/input/tfeqimg-2400/Final.pt'\n\n# Loading the model\nmodel = torch.hub.load('kaggle/input/yolov7-0/yolov5-7.0', 'custom', source='local', path=CKPT_PATH, force_reload=True)\n```\n\nPlease double-check that the path `'/kaggle/input/tfeqimg-2400/Final.pt'` actually leads to your weights file in the Kaggle environment.\n\nIf you continue to have issues or face any other questions, feel free to ask. The YOLO community and the Ultralytics team are always here to help. Happy coding! ðŸš€"
      },
      {
        "user": "mgnavarrete",
        "body": "I encountered the error `cannot instantiate 'WindowsPath' on your system`, and none of the suggested solutions helped.\r\n\r\nTo provide some context, I was trying to run a custom YOLOv5 model on CVAT or simply on my WSL, but every time I attempted it, I got the same error.\r\n\r\nI tried various solutions, but none worked. Eventually, I realized that I had trained my model on a Windows system using Windows CMD. This caused the error to persist when I tried to run the model on Linux, Ubuntu, or WSL on Windows.\r\n\r\nWhat I ended up doing was retraining the model, this time using an Ubuntu system. I used Google Colab for training, then tested the model on Ubuntu and CVAT, and it worked fine.\r\n\r\nI created a [repository](https://github.com/mgnavarrete/trainYoloIPYNB.git) with the Jupyter notebook and the data that I used to train the model on Colab.\r\n\r\nI hope this helps anyone encountering a similar problem, or perhaps it could fix the 'PosixPath' error to. "
      }
    ]
  },
  {
    "issue_number": 13453,
    "title": "conv2d() received an invalid combination of arguments",
    "author": "niusme",
    "state": "open",
    "created_at": "2024-12-08T13:24:11Z",
    "updated_at": "2024-12-13T10:18:27Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n## environment\r\nwindows10\r\npython3.8\r\n\r\n## question\r\nI used the trained model to detect. The following code throws an error\r\n```\r\nimport pathlib\r\n\r\nimport torch\r\nfrom PIL import Image\r\nimport numpy as np\r\nfrom pathlib import Path\r\n\r\npathlib.PosixPath = pathlib.WindowsPath\r\nmodel = torch.load(r'D:\\py\\yolo\\yolov5\\mymodel\\testbest.pt', map_location=torch.device('cpu'))['model'].float()\r\nmodel.eval()\r\n\r\n\r\nresults = model(r'D:\\py\\code\\dnfm-yolo-tutorial\\naima\\28.png')  \r\n\r\n\r\nresults.print()  \r\nresults.show()   \r\n\r\n```\r\n\r\nthe error \r\n```\r\nTraceback (most recent call last):\r\n  File \"D:/py/PyCharm 2024.1.6/plugins/python/helpers/pydev/pydevd.py\", line 1551, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"D:\\py\\PyCharm 2024.1.6\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"D:\\py\\yolo\\yolov5\\test.py\", line 13, in <module>\r\n    results = model(r'D:\\py\\code\\dnfm-yolo-tutorial\\naima\\28.png')  \r\n  File \"D:\\py\\yolo\\yolov5\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"D:\\py\\yolo\\yolov5\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\py\\yolo\\yolov5\\models\\yolo.py\", line 267, in forward\r\n    return self._forward_once(x, profile, visualize)  # single-scale inference, train\r\n  File \"D:\\py\\yolo\\yolov5\\models\\yolo.py\", line 167, in _forward_once\r\n    x = m(x)  # run\r\n  File \"D:\\py\\yolo\\yolov5\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"D:\\py\\yolo\\yolov5\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\py\\yolo\\yolov5\\models\\common.py\", line 86, in forward\r\n    return self.act(self.bn(self.conv(x)))\r\n  File \"D:\\py\\yolo\\yolov5\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"D:\\py\\yolo\\yolov5\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\py\\yolo\\yolov5\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 458, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"D:\\py\\yolo\\yolov5\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 454, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nTypeError: conv2d() received an invalid combination of arguments - got (str, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\r\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\r\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\r\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\r\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\r\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @niusme, thank you for your interest in YOLOv5 ðŸš€! \n\nIt looks like you're encountering an issue during inference with a trained model. This may be related to how the input is being passed to the model. For better assistance, could you please provide a **minimum reproducible example** including the following details?\n\n- The full code with all modifications made to the original YOLOv5 repository, if any\n- Steps to reproduce the error\n- A description of the exact YOLOv5 version or commit hash being used\n- Information about your environment (e.g., Python version, PyTorch version, and whether you're running on CPU/GPU)\n\nIf this is related to custom training, ensure that your workflow aligns with best practices for data preparation, training, and inference, including correctly preparing the inputs for the model.\n\nHere are some tips to troubleshoot while we investigate further:\n1. Double-check the input being passed to the model (e.g., path to the image). Ensure it is of the correct type and format.\n2. Ensure your dependencies like PyTorch are up-to-date with the correct versions required for YOLOv5.\n3. Test the model with a small example to isolate where the issue might be occurring.\n\nThis is an automated response to assist you efficiently. An Ultralytics engineer will review your issue and provide further assistance soon ðŸ˜Š. Thank you for your patience!"
      },
      {
        "user": "pderrenger",
        "body": "@niusme the issue arises because the `model()` function in the provided code attempts to pass a string (the file path of the image) when it expects a `Tensor` as input. YOLOv5 models do not directly process file paths; an image or its corresponding data needs to be loaded into a tensor first.\n\nHereâ€™s how you can resolve it:\n\nReplace the `results = model(...)` line with the following:\n\n```python\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n# Load image and preprocess\nimage_path = r'D:\\py\\code\\dnfm-yolo-tutorial\\naima\\28.png'\nimage = Image.open(image_path).convert('RGB')\ntransform = transforms.ToTensor()\nimage_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n\n# Pass tensor to the model for inference\nresults = model(image_tensor)\n```\n\nIf the problem persists, ensure your `testbest.pt` model is correctly trained and compatible with YOLOv5. Always make sure you're using the latest version of the YOLOv5 repository and PyTorch library for compatibility. You can refer to the [model inference documentation](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/) for further guidance. Let us know if you encounter additional issues!"
      },
      {
        "user": "BrunoKreiner",
        "body": "> @niusme the issue arises because the `model()` function in the provided code attempts to pass a string (the file path of the image) when it expects a `Tensor` as input. YOLOv5 models do not directly process file paths; an image or its corresponding data needs to be loaded into a tensor first.\r\n> \r\n> Hereâ€™s how you can resolve it:\r\n> \r\n> Replace the `results = model(...)` line with the following:\r\n> \r\n> ```python\r\n> from PIL import Image\r\n> import torchvision.transforms as transforms\r\n> \r\n> # Load image and preprocess\r\n> image_path = r'D:\\py\\code\\dnfm-yolo-tutorial\\naima\\28.png'\r\n> image = Image.open(image_path).convert('RGB')\r\n> transform = transforms.ToTensor()\r\n> image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\r\n> \r\n> # Pass tensor to the model for inference\r\n> results = model(image_tensor)\r\n> ```\r\n> \r\n> If the problem persists, ensure your `testbest.pt` model is correctly trained and compatible with YOLOv5. Always make sure you're using the latest version of the YOLOv5 repository and PyTorch library for compatibility. You can refer to the [model inference documentation](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/) for further guidance. Let us know if you encounter additional issues!\r\n\r\nthe image needs to be transformed to 640x640 first right? so replace this\r\n`transform = transforms.ToTensor()`\r\nwith \r\n`transform = transforms.Compose([transforms.Resize((640, 640)), transforms.ToTensor()])`\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 13460,
    "title": "Windows Path Error ",
    "author": "Rahulx911",
    "state": "open",
    "created_at": "2024-12-13T07:55:15Z",
    "updated_at": "2024-12-13T07:59:32Z",
    "labels": [
      "question",
      "dependencies",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nimport platform\r\nimport pathlib\r\n\r\nif platform.system() != 'Windows':\r\n    pathlib.WindowsPath = pathlib.PosixPath\r\n\r\nimport subprocess\r\nimport os\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\nimport time\r\nfrom paddleocr import PaddleOCR\r\nimport re\r\nfrom flask import Blueprint, request, jsonify\r\nfrom werkzeug.utils import secure_filename\r\nfrom models.database import Box,PackedItem\r\nfrom app import db\r\nfrom datetime import datetime\r\nfrom dotenv import load_dotenv\r\nfrom pathlib import Path\r\nload_dotenv()\r\n# YOLO_WEIGHTS_PATH = r\"C:\\Users\\tanya\\OneDrive\\Pictures\\web_app\\backend\\models\\best.pt\"\r\n# DETECT=r\"C:\\Users\\tanya\\OneDrive\\Pictures\\web_app\\backend\\yolov5\\detect.py\"\r\n# DETECT_RUNS=r\"C:\\Users\\tanya\\OneDrive\\Pictures\\web_app\\backend\\yolov5\\runs\\detect\"\r\n\r\nYOLO_WEIGHTS_PATH = \"/root/backend/models/best.pt\"\r\nDETECT = \"/root/backend/yolov5/detect.py\"\r\nDETECT_RUNS = \"/root/backend/yolov5/runs/detect\"\r\n\r\nocr = PaddleOCR(use_angle_cls=True, lang='en', det_db_thresh=0.3, det_db_box_thresh=0.5)\r\n\r\ndetected_texts = []\r\n\r\n\r\n# Create Blueprint\r\ndetect_front_side_blueprint = Blueprint('detect_front_side', __name__)\r\n\r\ndef run_yolo_inference(image_path):\r\n\"detect_front_side.py\" [dos] 249L, 8997B                                                                                                                 6,0-1         Top\r\n\r\nhere i want to use poxix path but internal files of yolov5 are runt able to take this\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Rahulx911, thank you for your interest in YOLOv5 ðŸš€! We're here to assist you. Based on your issue description, it seems like you are running into a path compatibility issue with YOLOv5 on Windows versus a Linux-like environment, particularly with Posix and Windows paths.\n\nIf this is a ðŸ› Bug Report, could you please provide a **minimum reproducible example (MRE)** to help us thoroughly debug the issue? An MRE would include a clear, minimal snippet of code or setup required to reproduce your problem, as well as any relevant error logs or traceback details.\n\nIf this is a general â“ Question or relates to custom integration, please share more details about your current setup, including any modifications you've made to the default YOLOv5 code and the context in which this issue occurs (e.g., environment, hardware, dependencies).\n\n## Requirements\nEnsure you're running YOLOv5 with **Python>=3.8.0** and all required dependencies installed. All Python packages should match those listed in `requirements.txt`, and you need a compatible version of PyTorch (>=1.8), as well as a correctly configured environment with GPU/CUDA support, if applicable.\n\n## Environments\nYOLOv5 works in a wide range of environments, including Jupyter Notebooks, Google Colab, Docker, and traditional setups. To isolate the issue, we recommend testing it in a fresh and verified environment to rule out environment-specific issues.\n\n## Status\nPlease check if all YOLOv5 CI tests are currently passing for the latest commit. This ensures the latest repository version is stable.\n\nðŸš€ This is an automated response, but rest assured that an Ultralytics engineer will review your issue shortly and provide further guidance. A bit more detail from your side will streamline the debugging process! ðŸ˜Š"
      },
      {
        "user": "Rahulx911",
        "body": "I have checked no changes i made in internal files but still this error @UltralyticsAssistant "
      }
    ]
  },
  {
    "issue_number": 13456,
    "title": "Add LSTM at the beginning",
    "author": "RayDu111",
    "state": "open",
    "created_at": "2024-12-12T06:59:20Z",
    "updated_at": "2024-12-13T00:59:09Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi, my data includes the time information. So I tried to add LSTM in yolo to extract  the time information. I want to concat the  image and LSTM result, then put it to the layer 0 which is 'Conv' in yolov5s.yaml. But I have no idea which layer or API presents the image. Because only layer  0 could get the image as the input which is 'Conv' in yolov5s.yaml. Could you please give me some advice? Thank you.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "pderrenger",
        "body": "ðŸ‘‹ Hello @RayDu111, thank you for your interest in YOLOv5 ðŸš€! It sounds like you're working on an exciting project involving time-series data and YOLOv5 integration.\n\nIf this is a ðŸ› Bug Report, please include a **minimum reproducible example (MRE)** that we can run to better understand and debug your issue.\n\nIf this is a custom training or implementation â“ Question, please share additional details such as your proposed modifications, relevant code snippets, and error messages (if any). Providing this information will help us better assist you.\n\nTo ensure your environment is properly set up, please verify that you have Python>=3.8.0 installed along with all the required dependencies. You can do this by cloning the YOLOv5 repository, navigating to the folder, and installing the requirements using pip.\n\nYOLOv5 is compatible with various environments, and you can run it on Jupyter Notebooks, Google Colab, cloud setups like AWS or GCP, or locally using Docker or installed dependencies. Let us know how you're running your code so we can assist further.\n\nAn Ultralytics engineer will review your request and provide personalized assistance soon! ðŸ˜Š"
      },
      {
        "user": "RayDu111",
        "body": "I had work it out. I added a Identity mapping layer which is layer 0. So I get the API of image. Thank you."
      },
      {
        "user": "pderrenger",
        "body": "@RayDu111 great to hear that you resolved it by adding an identity mapping layer! If you have further questions or need assistance with any modifications, feel free to reach out. Thanks for contributing to the YOLO community!"
      }
    ]
  },
  {
    "issue_number": 13458,
    "title": "detecting objects with (almost) same centercoordinates",
    "author": "dk-teknologisk-bgd",
    "state": "open",
    "created_at": "2024-12-12T12:51:46Z",
    "updated_at": "2024-12-12T12:52:49Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI am wondering, how the yolov8 (or 11) would handle detecting e.g. two objects (of different class) with the same center-coordinates? \r\nI remember when I back in the days implemented part of the yolo3 training loop in Keras, that each output \"pixel\" would be encoded to only detect a single object. The output grid was always 1/32 the size of the input, and hence this could occur from time to time, but rarely.\r\nToday I am considering using the yolov8 for a new project, in which two objects would often have roughly the same center. More often if the output grid is still 1/32 of the input res, then for a 640x640 network, any two objects within 20x20pixels would end up in the same output grid cell.\r\n\r\nDoes anyone know, if this is still an issue today for yolov8 / 11, or only for the \"old\" yolov3?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "pderrenger",
        "body": "ðŸ‘‹ Hello @dk-teknologisk-bgd, thank you for your interest in YOLOv5 ðŸš€! Your question raises an interesting point about object detection with overlapping or similar center coordinates. While Ultralytics YOLO models have evolved significantly since YOLOv3, specific technical nuances like the one you mentioned (object encoding and grid resolution) can still depend on various factors, such as the model architecture and the training process.\n\nIf this is a â“ Question regarding custom training or inference behavior, please share additional details about your project setup, including any datasets or use cases you're experimenting with. Verifying the trained model's outputs in situations similar to your described scenario could also yield critical insights.\n\nIf this involves a ðŸ› Bug or unexpected behavior with YOLOv5, please provide a **minimum reproducible example (MRE)** to help us assist you more efficiently.\n\n## Requirements\n\nEnsure you have **Python>=3.8.0** installed with all dependencies configured correctly, including an appropriate version of **PyTorch>=1.8**. Begin with the repositoryâ€™s setup instructions to confirm everything is operational.\n\n## Environments\n\nYOLOv5 is compatible with various testing and training environments, including notebooks with free GPUs, Docker setups, and cloud services with pre-installed machine learning dependencies. For your specific concerns about overlapping objects, experimenting with one of these configurations could provide additional clarity.\n\n## Status\n\nPlease check the repository's Continuous Integration (CI) badge to confirm that all tests are passing. CI ensures compatibility and proper functionality across different use cases, including training, validation, and inference processes.\n\nThis is an automated response to guide you through initial troubleshooting steps. An Ultralytics engineer will also review your question and offer further assistance soon âœ¨."
      }
    ]
  },
  {
    "issue_number": 11454,
    "title": "Interpretation Training Results (M) vs. (B)",
    "author": "katiefux",
    "state": "closed",
    "created_at": "2023-04-28T11:43:09Z",
    "updated_at": "2024-12-11T10:33:37Z",
    "labels": [
      "Stale"
    ],
    "body": "Dear reader,\r\n\r\nI have trained a model and i have a question regarding the results file that shows the loss/precision/recall/mAP... of every epoch:\r\nWhat is the meaning of (M) vs. (B) in brackets for the measures precision, recall, mAP_0.5 and mAP_0.5:0.95?\r\n\r\n![image](https://user-images.githubusercontent.com/64419179/235138416-18300a47-0237-4e35-8595-f9f2c9d29697.png)\r\n\r\nThanks a lot in advance for a response!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @katiefux, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/).\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nWe're excited to announce the launch of our latest state-of-the-art (SOTA) object detection model for 2023 - [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€!\n\nDesigned to be fast, accurate, and easy to use, YOLOv8 is an ideal choice for a wide range of object detection, image segmentation and image classification tasks. With YOLOv8, you'll be able to quickly and accurately detect objects in real-time, streamline your workflows, and achieve new levels of accuracy in your projects.\n\nCheck out our [YOLOv8 Docs](https://docs.ultralytics.com/) for details and get started with:\n```bash\npip install ultralytics\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "@katiefux hello!\r\n\r\n(M) refers to \"macro\" average, which calculates the average performance metric over each class. (B) refers to \"best\" performance metric achieved by the model during the training process. So for example, \"mAP_0.5 (B)\" would refer to the best mean average precision at an IoU threshold of 0.5 achieved by the model during training, while \"mAP_0.5 (M)\" would refer to the macro average mean average precision at an IoU threshold of 0.5 across all classes.\r\n\r\nI hope this answers your question! Let us know if there's anything else we can help you with."
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      }
    ]
  },
  {
    "issue_number": 13401,
    "title": "Failing at small detection !!",
    "author": "Manueljohnson063",
    "state": "open",
    "created_at": "2024-11-06T10:22:59Z",
    "updated_at": "2024-12-10T18:26:57Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nThank you for the great repository!\r\n\r\nIâ€™m currently working on a driver smoking detection project. I have trained my model with nearly 5,000 images; however, it mistakenly identifies light glare as a cigarette, due to the similarity in appearance. My model has only one class, labeled \"cigarette.\" Could you suggest some modifications to improve its accuracy?@Glenn\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Manueljohnson063, thank you for your interest in YOLOv5 ðŸš€!\n\nFor improving the accuracy of your model, especially in distinguishing similar-looking objects, you might consider the following approaches:\n\n1. **Dataset Enhancement**: Increase your dataset, particularly with more examples of correctly and incorrectly identified objects. Ensuring a variety of lighting conditions and angles can help.\n\n2. **Data Augmentation**: Use techniques such as adding noise, adjusting brightness/contrast, and flipping images to enhance your training data.\n\n3. **Hyperparameter Tuning**: Experiment with different hyperparameters to see if they bring improvements in accuracy.\n\n4. **Model Architecture**: Try different model sizes or architectures within YOLOv5 to find the best fit for your task.\n\n5. **Loss Functions**: Modify loss functions to better penalize the misclassification errors specific to your problem.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nThis is an automated response, and an Ultralytics engineer will also assist you soon. If you have further questions or need more detailed assistance, please feel free to reply with additional information, such as your dataset examples and training logs.\n\nAnd don't miss out on the possibilities with the latest [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸ†• for potentially enhanced performance in your project! Check out our docs for more info.\n\nHappy experimenting ðŸ”!"
      },
      {
        "user": "pderrenger",
        "body": "@Manueljohnson063 to improve accuracy, consider increasing your dataset size with more diverse images, using data augmentation to enhance variability, and adjusting the confidence threshold to reduce false positives. You might also try training with a higher resolution or using a larger model like YOLOv5x for better feature extraction."
      },
      {
        "user": "Manueljohnson063",
        "body": "Hai ,I  have a yolov5s model downloaded from opensource.when i try to export it i am facing error\r\n``\r\n**python export.py --weights /home/vkchlt0616/Downloads/face_detection_yolov5s.pt  --include onnx --opset=11 --imgsz 640 --batch-size 1**\r\n__export: data=data/coco128.yaml, weights=['/home/vkchlt0616/Downloads/face_detection_yolov5s.pt'], imgsz=[320], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, cache=, simplify=False, mlmodel=False, opset=11, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\r\nYOLOv5 ðŸš€ v7.0-385-gb968b2d7 Python-3.8.19 torch-1.8.0 CPU\r\n\r\nFusing layers... \r\nnewYOLOv5s summary: 224 layers, 7053910 parameters, 0 gradients\r\nTraceback (most recent call last):\r\n  File \"export.py\", line 1546, in <module>\r\n    main(opt)\r\n  File \"export.py\", line 1541, in main\r\n    run(**vars(opt))\r\n  File \"/home/vkchlt0616/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"export.py\", line 1407, in run\r\n    y = model(im)  # dry runs\r\n  File \"/home/vkchlt0616/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/vkchlt0616/DOMS/smokingDet/yolov5/models/yolo.py\", line 270, in forward\r\n    return self._forward_once(x, profile, visualize)  # single-scale inference, train\r\n  File \"/home/vkchlt0616/DOMS/smokingDet/yolov5/models/yolo.py\", line 169, in _forward_once\r\n    x = m(x)  # run\r\n  File \"/home/vkchlt0616/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/vkchlt0616/DOMS/smokingDet/yolov5/models/common.py\", line 356, in forward\r\n    return self.conv(torch.cat((x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]), 1))\r\n  File \"/home/vkchlt0616/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/vkchlt0616/DOMS/smokingDet/yolov5/models/common.py\", line 91, in forward_fuse\r\n    return self.act(self.conv(x))\r\n  File \"/home/vkchlt0616/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/vkchlt0616/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 399, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/home/vkchlt0616/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 395, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: Given groups=1, weight of size [32, 12, 3, 3], expected input[1, 48, 160, 160] to have 12 channels, but got 48 channels instead__\r\n\r\n@glenn-jocher \r\n\r\n**But the model is detecting face properly from web cam .The issue only come during exporting to onnx.Please give me the suggestion....** \r\n"
      }
    ]
  },
  {
    "issue_number": 13454,
    "title": "autolabel objects",
    "author": "BitStrawber",
    "state": "open",
    "created_at": "2024-12-10T09:27:04Z",
    "updated_at": "2024-12-10T16:32:29Z",
    "labels": [
      "enhancement",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar feature requests.\n\n\n### Description\n\nIt's a large work to label the object one by one, especially when the datatset is pretty large. Maybe there could be something ued to generate label initially\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @BitStrawber, thank you for your interest in YOLOv5 ðŸš€! Your suggestion regarding automated labeling is an interesting one, as managing large datasets can indeed be tedious.\n\nFor now, have you explored features like [Ultralytics Label Studio integration](https://github.com/heartexlabs/label-studio), or tools like [Roboflow](https://roboflow.com) for labeling assistance? These can potentially speed up the dataset preparation process. In addition â€” while YOLOv-possible solutions exploration here poweredRequest?) automating"
      },
      {
        "user": "pderrenger",
        "body": "@BitStrawber thank you for your suggestion! For automating initial labeling, you can consider tools like [Roboflow Annotate](https://roboflow.com/annotate?ref=ultralytics), which supports features like pre-labeling using pre-trained models. Alternatively, you could apply a YOLOv5 model trained on a similar dataset to auto-generate labels and then refine them manually. For annotation tools and workflows, you can refer to the [Train Custom Data Guide](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/). Let us know if you have further questions!"
      }
    ]
  },
  {
    "issue_number": 12,
    "title": "Train Custom Data Tutorial â­",
    "author": "glenn-jocher",
    "state": "open",
    "created_at": "2020-06-03T17:32:57Z",
    "updated_at": "2024-12-10T09:10:07Z",
    "labels": [
      "documentation"
    ],
    "body": "ðŸ“š This guide explains how to train your own **custom dataset** with YOLOv5 ðŸš€. See YOLOv5 [Docs](https://docs.ultralytics.com/yolov5) for additional details. UPDATED 13 April 2023.\r\n\r\n\r\n## Before You Start\r\n\r\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\r\n\r\n```bash\r\ngit clone https://github.com/ultralytics/yolov5  # clone\r\ncd yolov5\r\npip install -r requirements.txt  # install\r\n```\r\n\r\n## Train On Custom Data\r\n\r\n<a href=\"https://bit.ly/ultralytics_hub\" target=\"_blank\">\r\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/integrations-loop.png\"></a>\r\n<br>\r\n<br>\r\n\r\nCreating a custom model to detect your objects is an iterative process of collecting and organizing images, labeling your objects of interest, training a model, deploying it into the wild to make predictions, and then using that deployed model to collect examples of edge cases to repeat and improve.\r\n\r\n### 1. Create Dataset\r\n\r\nYOLOv5 models must be trained on labelled data in order to learn classes of objects in that data. There are two options for creating your dataset before you start training:\r\n\r\n<details open markdown>\r\n<summary>Use <a href=\"https://roboflow.com/?ref=ultralytics\">Roboflow</a> to create your dataset in YOLO format  â­</summary>\r\n\r\n### 1.1 Collect Images\r\n\r\nYour model will learn by example. Training on images similar to the ones it will see in the wild is of the utmost importance. Ideally, you will collect a wide variety of images from the same configuration (camera, angle, lighting, etc.) as you will ultimately deploy your project.\r\n\r\nIf this is not possible, you can start from [a public dataset](https://universe.roboflow.com/?ref=ultralytics) to train your initial model and then [sample images from the wild during inference](https://blog.roboflow.com/computer-vision-active-learning-tips/?ref=ultralytics) to improve your dataset and model iteratively.\r\n\r\n### 1.2 Create Labels\r\n\r\nOnce you have collected images, you will need to annotate the objects of interest to create a ground truth for your model to learn from.\r\n\r\n<p align=\"center\"><a href=\"https://app.roboflow.com/?model=yolov5&ref=ultralytics\" title=\"Create a Free Roboflow Account\"><img width=\"450\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a275ad4b4ac20cd2e21a_roboflow-annotate.gif\" /></a></p>\r\n\r\n[Roboflow Annotate](https://roboflow.com/annotate?ref=ultralytics) is a simple\r\nweb-based tool for managing and labeling your images with your team and exporting\r\nthem in [YOLOv5's annotation format](https://roboflow.com/formats/yolov5-pytorch-txt?ref=ultralytics).\r\n\r\n### 1.3 Prepare Dataset for YOLOv5\r\n\r\nWhether you [label your images with Roboflow](https://roboflow.com/annotate?ref=ultralytics) or not, you can use it to convert your dataset into YOLO format, create a YOLOv5 YAML configuration file, and host it for importing into your training script.\r\n\r\n[Create a free Roboflow account](https://app.roboflow.com/?model=yolov5&ref=ultralytics)\r\nand upload your dataset to a `Public` workspace, label any unannotated images,\r\nthen generate and export a version of your dataset in `YOLOv5 Pytorch` format.\r\n\r\nNote: YOLOv5 does online augmentation during training, so we do not recommend\r\napplying any augmentation steps in Roboflow for training with YOLOv5. But we\r\nrecommend applying the following preprocessing steps:\r\n\r\n<p align=\"center\"><img width=\"450\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a273477fccf42a0fd3d6_roboflow-preprocessing.png\" title=\"Recommended Preprocessing Steps\" /></p>\r\n\r\n* **Auto-Orient** - to strip EXIF orientation from your images.\r\n* **Resize (Stretch)** - to the square input size of your model (640x640 is the YOLOv5 default).\r\n\r\nGenerating a version will give you a point in time snapshot of your dataset so\r\nyou can always go back and compare your future model training runs against it,\r\neven if you add more images or change its configuration later.\r\n\r\n<p align=\"center\"><img width=\"450\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a2733fd1da943619934e_roboflow-export.png\" title=\"Export in YOLOv5 Format\" /></p>\r\n\r\nExport in `YOLOv5 Pytorch` format, then copy the snippet into your training\r\nscript or notebook to download your dataset.\r\n\r\n<p align=\"center\"><img width=\"450\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a273a92e4f5cb72594df_roboflow-snippet.png\" title=\"Roboflow dataset download snippet\" /></p>\r\n\r\nNow continue with `2. Select a Model`.\r\n</details>\r\n\r\n<details>\r\n<summary>Or manually prepare your dataset</summary>\r\n\r\n### 1.1 Create dataset.yaml\r\n\r\n[COCO128](https://www.kaggle.com/ultralytics/coco128) is an example small tutorial dataset composed of the first 128 images in [COCO](http://cocodataset.org/#home) train2017. These same 128 images are used for both training and validation to verify our training pipeline is capable of overfitting. [data/coco128.yaml](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), shown below, is the dataset config file that defines 1) the dataset root directory `path` and relative paths to `train` / `val` / `test` image directories (or *.txt files with image paths) and 2) a class `names` dictionary:\r\n```yaml\r\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\r\npath: ../datasets/coco128  # dataset root dir\r\ntrain: images/train2017  # train images (relative to 'path') 128 images\r\nval: images/train2017  # val images (relative to 'path') 128 images\r\ntest:  # test images (optional)\r\n\r\n# Classes (80 COCO classes)\r\nnames:\r\n  0: person\r\n  1: bicycle\r\n  2: car\r\n  ...\r\n  77: teddy bear\r\n  78: hair drier\r\n  79: toothbrush\r\n```\r\n\r\n### 1.2 Create Labels\r\n\r\nAfter using an annotation tool to label your images, export your labels to **YOLO format**, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The `*.txt` file specifications are:\r\n\r\n- One row per object\r\n- Each row is `class x_center y_center width height` format.\r\n- Box coordinates must be in **normalized xywh** format (from 0 - 1). If your boxes are in pixels, divide `x_center` and `width` by image width, and `y_center` and `height` by image height.\r\n- Class numbers are zero-indexed (start from 0).\r\n\r\n<p align=\"center\"><img width=\"750\" src=\"https://user-images.githubusercontent.com/26833433/91506361-c7965000-e886-11ea-8291-c72b98c25eec.jpg\"></p>\r\n\r\nThe label file corresponding to the above image contains 2 persons (class `0`) and a tie (class `27`):\r\n\r\n<p align=\"center\"><img width=\"428\" src=\"https://user-images.githubusercontent.com/26833433/112467037-d2568c00-8d66-11eb-8796-55402ac0d62f.png\"></p>\r\n\r\n\r\n### 1.3 Organize Directories\r\n\r\nOrganize your train and val images and labels according to the example below. YOLOv5 assumes  `/coco128` is inside a `/datasets` directory **next to** the `/yolov5` directory. **YOLOv5 locates labels automatically for each image** by replacing the last instance of `/images/` in each image path with `/labels/`. For example:\r\n```bash\r\n../datasets/coco128/images/im0.jpg  # image\r\n../datasets/coco128/labels/im0.txt  # label\r\n```\r\n\r\n<p align=\"center\"><img width=\"700\" src=\"https://user-images.githubusercontent.com/26833433/134436012-65111ad1-9541-4853-81a6-f19a3468b75f.png\"></p>\r\n\r\n</details>\r\n\r\n### 2. Select a Model\r\n\r\nSelect a pretrained model to start training from. Here we select [YOLOv5s](https://github.com/ultralytics/yolov5/blob/master/models/yolov5s.yaml), the second-smallest and fastest model available. See our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints) for a full comparison of all models.\r\n\r\n<p align=\"center\"><img width=\"800\" alt=\"YOLOv5 Models\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png\"></p>\r\n\r\n### 3. Train\r\n\r\nTrain a YOLOv5s model on COCO128 by specifying dataset, batch-size, image size and either pretrained `--weights yolov5s.pt` (recommended), or randomly initialized `--weights '' --cfg yolov5s.yaml` (not recommended). Pretrained weights are auto-downloaded from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases).\r\n\r\n```bash\r\n# Train YOLOv5s on COCO128 for 3 epochs\r\n$ python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt\r\n```\r\n\r\nðŸ’¡ ProTip: Add `--cache ram` or `--cache disk` to speed up training (requires significant RAM/disk resources).  \r\nðŸ’¡ ProTip: Always train from a local dataset. Mounted or network drives like Google Drive will be very slow. \r\n\r\nAll training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc. For more details see the Training section of our tutorial notebook. <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n\r\n### 4. Visualize\r\n\r\n#### Comet Logging and Visualization ðŸŒŸ NEW\r\n\r\n[Comet](https://bit.ly/yolov5-readme-comet) is now fully integrated with YOLOv5. Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with [Comet Custom Panels](https://bit.ly/yolov5-colab-comet-panels)! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes! \r\n\r\nGetting started is easy:\r\n```shell\r\npip install comet_ml  # 1. install\r\nexport COMET_API_KEY=<Your API Key>  # 2. paste API key\r\npython train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt  # 3. train\r\n```\r\n\r\nTo learn more about all of the supported Comet features for this integration, check out the [Comet Tutorial](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/comet). If you'd like to learn more about Comet, head over to our [documentation](https://bit.ly/yolov5-colab-comet-docs). Get started by trying out the Comet Colab Notebook:\r\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RG0WOQyxlDlo5Km8GogJpIEJlg_5lyYO?usp=sharing)\r\n\r\n<img width=\"1920\" alt=\"yolo-ui\" src=\"https://user-images.githubusercontent.com/26833433/202851203-164e94e1-2238-46dd-91f8-de020e9d6b41.png\">\r\n\r\n#### ClearML Logging and Automation ðŸŒŸ NEW\r\n\r\n[ClearML](https://cutt.ly/yolov5-notebook-clearml) is completely integrated into YOLOv5 to track your experimentation, manage dataset versions and even remotely execute training runs. To enable ClearML:\r\n\r\n- `pip install clearml`\r\n- run `clearml-init` to connect to a ClearML server (**deploy your own open-source server [here](https://github.com/allegroai/clearml-server)**, or use our free hosted server [here](https://cutt.ly/yolov5-notebook-clearml))\r\n\r\nYou'll get all the great expected features from an experiment manager: live updates, model upload, experiment comparison etc. but ClearML also tracks uncommitted changes and installed packages for example. Thanks to that ClearML Tasks (which is what we call experiments) are also reproducible on different machines! With only 1 extra line, we can schedule a YOLOv5 training task on a queue to be executed by any number of ClearML Agents (workers).\r\n\r\nYou can use ClearML Data to version your dataset and then pass it to YOLOv5 simply using its unique ID. This will help you keep track of your data without adding extra hassle. Explore the [ClearML Tutorial](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/clearml) for details!\r\n\r\n<a href=\"https://cutt.ly/yolov5-notebook-clearml\">\r\n<img alt=\"ClearML Experiment Management UI\" src=\"https://github.com/thepycoder/clearml_screenshots/raw/main/scalars.jpg\" width=\"1280\"/></a>\r\n\r\n\r\n#### Local Logging\r\n\r\nTraining results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.\r\n\r\nThis directory contains train and val statistics, mosaics, labels, predictions and augmentated mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices. \r\n\r\n<img alt=\"Local logging results\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/image-local_logging.jpg\" width=\"1280\"/>\r\n\r\nResults file `results.csv` is updated after each epoch, and then plotted as `results.png` (below) after training completes. You can also plot any `results.csv` file manually:\r\n\r\n```python\r\nfrom utils.plots import plot_results\r\nplot_results('path/to/results.csv')  # plot 'results.csv' as 'results.png'\r\n```\r\n\r\n<p align=\"center\"><img width=\"800\" alt=\"results.png\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/results.png\"></p>\r\n\r\n\r\n\r\n## Next Steps\r\n\r\nOnce your model is trained you can use your best checkpoint `best.pt` to:\r\n* Run [CLI](https://github.com/ultralytics/yolov5#quick-start-examples) or [Python](https://github.com/ultralytics/yolov5/issues/36) inference on new images and videos\r\n* [Validate](https://github.com/ultralytics/yolov5/blob/master/val.py) accuracy on train, val and test splits\r\n* [Export](https://github.com/ultralytics/yolov5/issues/251) to TensorFlow, Keras, ONNX, TFlite, TF.js, CoreML and TensorRT formats\r\n* [Evolve](https://github.com/ultralytics/yolov5/issues/607) hyperparameters to improve performance\r\n* [Improve](https://docs.roboflow.com/adding-data/upload-api?ref=ultralytics) your model by sampling real-world images and adding them to your dataset\r\n\r\n\r\n## Environments\r\n\r\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\r\n\r\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\r\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\r\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\r\n\r\n\r\n## Status\r\n\r\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\r\n\r\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\r\n",
    "comments": [
      {
        "user": "shenglih",
        "body": " I used a my_training.txt file that includes a list of training images instead of a path to the folder of images and annotations, but it always returns AssertionError: No images found in /path_to_my_txt_file/my_training.txt. Could anyone kindly give some pointers to where it went wrong? Thanks"
      },
      {
        "user": "zuoxiang95",
        "body": "I get the same error @shenglih "
      },
      {
        "user": "synked16",
        "body": "hey  @glenn-jocher  can i train a model on images with size 450x600?? "
      }
    ]
  },
  {
    "issue_number": 13436,
    "title": "Guidance/Libary Needed for ONNX Model Post-Processing",
    "author": "Throws100",
    "state": "open",
    "created_at": "2024-11-27T16:14:09Z",
    "updated_at": "2024-12-07T10:11:09Z",
    "labels": [
      "question",
      "exports"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI've trained a custom YOLOv5 model that performs well, and I've exported it as an ONNX file. However, the output from the ONNX model differs from my expectations. After researching, I understand that ONNX outputs require additional post-processing to match the results seen with the PyTorch model.\r\n\r\nUnfortunately, I'm struggling to implement this post-processing manually. Since deploying models in other languages is a common task, I was wondering if anyone could point me to an existing component or library that handles YOLOv5 ONNX post-processing.\r\n\r\nAny guidance or resources would be greatly appreciated!\r\nThank you, Seb\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Throws100, thank you for your interest in YOLOv5 ðŸš€! Your question about ONNX model post-processing is noted. This is an automated response, but an Ultralytics engineer will assist you further as soon as possible ðŸ˜Š.\n\nFor now, if this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it. This should include any relevant code snippets and a description of inputs and outputs. \n\nIf this is a custom training â“ Question, please provide as much detail as possible, including examples of outputs and specific steps you've followed. \n\n## Requirements\n\nYOLOv5 requires **Python>=3.8.0** with all dependencies correctly installed. To ensure your environment is up to date, you can start by cloning the YOLOv5 repository, navigating to it, and installing required packages as follows:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 models can be executed in various verified environments, including free GPU notebooks, cloud platforms, and Docker containers. These environments include all necessary prerequisites like Python, CUDA, CUDNN, and PyTorch preinstalled.\n\n## Status\n\nOur continuous integration (CI) tests verify the correct operation of core YOLOv5 components such as training, validation, inference, export, and benchmarking on multiple platforms daily. If an issue is related to one of these workflows, rest assured our engineers will investigate thoroughly.\n\nThank you for your patience while we review your query further! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@Throws100 thank you for your query, Seb. For YOLOv5 ONNX post-processing, youâ€™ll need to handle the post-processing manually, as ONNX outputs raw model predictions. This typically includes applying non-maximum suppression (NMS), decoding bounding box coordinates, and reshaping logits. Libraries like [ONNX Runtime](https://onnxruntime.ai/) or OpenCVâ€™s DNN module might help.\n\nRefer to the following for a detailed example:  \n[YOLOv5 ONNX Export Documentation](https://github.com/ultralytics/yolov5/issues?q=is%3Aopen+is%3Aissue+onnx).  \n\nIf youâ€™re still encountering issues, you might want to explore discussions or examples from community repositories like [Hexmagic/ONNX-yolov5](https://github.com/Hexmagic/ONNX-yolov5). Let us know if further clarification is needed!"
      },
      {
        "user": "Throws100",
        "body": "Thanks I managed to get my custom onnx model to work I have used this: [https://github.com/robinZhao/object-detect-java](https://github.com/robinZhao/object-detect-java)\r\nThanks for your help:)"
      }
    ]
  },
  {
    "issue_number": 11712,
    "title": "YOLOv5 loss",
    "author": "MZeinab",
    "state": "closed",
    "created_at": "2023-06-15T14:58:52Z",
    "updated_at": "2024-12-07T06:09:59Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\n@glenn-jocher\r\nHi, could you please write the exact formula that you used in loss.py, I don't understand what exactly it is,\r\nit will be so useful for us and even for adding it to our articles. I checked the loss.py, if I understood correctly you mean this as loss:\r\n\r\n` def call(self, p, targets): # predictions, targets\r\nlcls = torch.zeros(1, device=self.device) # class loss\r\nlbox = torch.zeros(1, device=self.device) # box loss\r\nlobj = torch.zeros(1, device=self.device) # object loss\r\ntcls, tbox, indices, anchors = self.build_targets(p, targets) # targets\r\n\r\n    # Losses\r\n    for i, pi in enumerate(p):  # layer index, layer predictions\r\n        b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx\r\n        tobj = torch.zeros(pi.shape[:4], dtype=pi.dtype, device=self.device)  # target obj\r\n\r\n        n = b.shape[0]  # number of targets\r\n        if n:\r\n            # pxy, pwh, _, pcls = pi[b, a, gj, gi].tensor_split((2, 4, 5), dim=1)  # faster, requires torch 1.8.0\r\n            pxy, pwh, _, pcls = pi[b, a, gj, gi].split((2, 2, 1, self.nc), 1)  # target-subset of predictions\r\n\r\n            # Regression\r\n            pxy = pxy.sigmoid() * 2 - 0.5\r\n            pwh = (pwh.sigmoid() * 2) ** 2 * anchors[i]\r\n            pbox = torch.cat((pxy, pwh), 1)  # predicted box\r\n            iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze()  # iou(prediction, target)\r\n            lbox += (1.0 - iou).mean()  # iou loss\r\n\r\n            # Objectness\r\n            iou = iou.detach().clamp(0).type(tobj.dtype)\r\n            if self.sort_obj_iou:\r\n                j = iou.argsort()\r\n                b, a, gj, gi, iou = b[j], a[j], gj[j], gi[j], iou[j]\r\n            if self.gr < 1:\r\n                iou = (1.0 - self.gr) + self.gr * iou\r\n            tobj[b, a, gj, gi] = iou  # iou ratio\r\n\r\n            # Classification\r\n            if self.nc > 1:  # cls loss (only if multiple classes)\r\n                t = torch.full_like(pcls, self.cn, device=self.device)  # targets\r\n                t[range(n), tcls[i]] = self.cp\r\n                lcls += self.BCEcls(pcls, t)  # BCE\r\n\r\n            # Append targets to text file\r\n            # with open('targets.txt', 'a') as file:\r\n            #     [file.write('%11.5g ' * 4 % tuple(x) + '\\n') for x in torch.cat((txy[i], twh[i]), 1)]\r\n\r\n        obji = self.BCEobj(pi[..., 4], tobj)\r\n        lobj += obji * self.balance[i]  # obj loss\r\n        if self.autobalance:\r\n            self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()\r\n\r\n    if self.autobalance:\r\n        self.balance = [x / self.balance[self.ssi] for x in self.balance]\r\n    lbox *= self.hyp['box']\r\n    lobj *= self.hyp['obj']\r\n    lcls *= self.hyp['cls']\r\n    bs = tobj.shape[0]  # batch size\r\n\r\n    return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()`\r\n\r\n\r\nI trained my custom model with your friendly tutorials, yolov5 v6.0, I want to write a report about it and explain the exact loss function, but I am really confused. I'll appreciate it if you could guide.\r\nBest of all,\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      },
      {
        "user": "glenn-jocher",
        "body": "@MZeinab hi there ðŸ‘‹,\n\nThe loss function you've pasted is from YOLOv5 loss.py, which calculates the combined box, object, and class loss. If you'd like to understand the detailed mathematical formulation, I'd recommend referring to the YOLOv5 documentation at https://docs.ultralytics.com/yolov5. \n\nThe loss function encompasses several components such as objectness, classification, and regression, and accounts for factors like class balance and optimization hyperparameters. It's important to understand and communicate these concepts accurately in your report.\n\nIf you have any specific questions or need further assistance, feel free to ask. Happy to help!\n\nBest regards,"
      },
      {
        "user": "LapTQ",
        "body": "@glenn-jocher Hi there, I would like to ask about the implementation details of the objectness loss. But first, would you mind confirm if I understand correctly:\r\n\r\n1. Given a list of GT boxes, the `build_targets` method will assign each GT box to every anchor-boxes on the grid that fits (one GT box can be assigned to many anchor-boxes).\r\n2. Given these assigned anchor-boxes, the `__call__` method will get the prediction result corresponding to them, then compute IoU with the GT boxes from which the anchor-boxes were assigned. This IoU score is used as the objectness score stored in the `tobj` variable.\r\n3. For the other anchor-boxes that was NOT assigned, the objectness scores are set as `0` in `tobj`.\r\n4. The objectness loss is computed as BCE loss between the predicted objectness and the `tobj`.\r\n\r\nIf I understand all those correctly, then I wonder whether, in step 2, choosing the IoU as the objectness score to store in `tobj` is adequate? Why don't we store 1 in `tobj` for these assigned anchor-boxes?\r\n\r\nThank you!"
      }
    ]
  },
  {
    "issue_number": 13448,
    "title": "Keeping track of transformations in YOLOWorld",
    "author": "toobatehreem",
    "state": "open",
    "created_at": "2024-12-06T07:13:27Z",
    "updated_at": "2024-12-07T05:57:33Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI want to keep track of what and how the transformations are applied to an image. For example, after mosaic, when 4 images are stitched together, I want to know which images were stitched together, what are the pixel values for each part (like the starting and ending pixel values of each sub-image in the image so I know that this part of the mosaic sample belongs to this original image), and which bounding boxes belong to which sub-image. How can I keep track of these?\r\n\r\nAlso, in yoloworld, keeping a track of only mosaic augmentation will be enough or the bounding boxes are transformed somewhere further as well?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @toobatehreem, thank you for your interest in YOLOv5 ðŸš€! This is an automated response to help guide you while an Ultralytics engineer will assist you shortly.\n\nTo answer your query, detailed tracking of transformations like mosaic augmentation can require custom logging or debugging in the code. If you'd like us to assist further, please provide more specifics about your setup and code modifications, or a **minimum reproducible example** to help us better understand your request.\n\n## Requirements\nVerify that you are using **Python>=3.8.0**, and all required dependencies are installed correctly. You can check and install dependencies following the instructions in the `requirements.txt` file in the repository.\n\n## Environments\nYOLOv5 can be run seamlessly in numerous verified environments:\n- Notebooks with free GPU.\n- Cloud platforms like Google Cloud or Amazon AWS.\n- Local environments configured for Python and PyTorch.\n\nIf you are modifying the dataset or training pipeline, please ensure transformations like mosaic are being applied using the components provided in the official repository to keep compatibility.\n\n## Next Steps\nPlease clarify your use case or share an example that includes:\n- A description of the data being used.\n- Code snippets showcasing where and how transformations are applied.\n- Any relevant logs or observations during training.\n\nAn Ultralytics team member will review your request shortly. ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@toobatehreem to track transformations like mosaic augmentation in YOLOv5, you can modify the `mosaic` function in the `datasets.py` file. Inside the function, you can log or store details such as source image indexes, their positions in the mosaic, and their corresponding bounding boxes. You can also add debug statements or save intermediate mosaic details for verification.\n\nRegarding further transformations, bounding boxes might be adjusted again depending on additional augmentations like scaling, flipping, or random rotations, which are applied after the mosaic process. To trace these changes, inspect other augmentation functions in `datasets.py` like `random_perspective`.\n\nFor a deeper understanding, explore and modify the dataset creation pipeline in the source code. Let us know if you encounter specific issues while implementing this!"
      }
    ]
  },
  {
    "issue_number": 13450,
    "title": "Calculating a 'Detection Rate' | YOLOv5",
    "author": "kyrangraves",
    "state": "open",
    "created_at": "2024-12-06T17:33:19Z",
    "updated_at": "2024-12-07T04:21:37Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "Hi All, \r\n\r\nI have trained a bunch of YOLOv5 models with varying parameters to detect different coral morphologies from ROV imagery. I want to modify the val.py script to create an evaluation metric that determines the % of ground truth objects that have been correctly detected, irrespective of whether the assigned label (classification) is correct. If I'm correct, the out-the-box val.py script determines a true positive when your defined IoU threshold **and** correct classification are met. I'm essentially wanting to remove the classification element of this.\r\n\r\nBefore I try, has anybody written a bit of script like this before or know of another simpler way of calculating this metric?\r\n\r\nAll the best - Kyran\r\n\r\n_Originally posted by @kyrangraves in https://github.com/ultralytics/yolov5/discussions/13449_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @kyrangraves, thank you for your interest in YOLOv5 ðŸš€! Your project detecting coral morphologies sounds fascinating ðŸŒŠðŸ .\n\nRegarding your query, modifying the evaluation metric to ignore classification while maintaining IoU thresholds is indeed achievable but requires some custom edits. If you'd like assistance with this, we recommend sharing a detailed description of the modifications you're planning to make in the `val.py` script. Providing a **minimum reproducible example (MRE)**â€”such as a small dataset or sample configuration filesâ€”would greatly help us understand your requirements better.\n\nIf this is a â“ Question rather than a ðŸ› Bug Report, please also share any additional information, such as your dataset structure, training logs, and validations conducted so far. This will ensure we can provide the most accurate guidance. Make sure to also check our training tips for best practices to enhance experimental results.\n\n### Requirements\n\nMake sure you are using **Python>=3.8.0** with all dependencies installed from `requirements.txt`, including **PyTorch>=1.8**. To verify your setup, clone the YOLOv5 repository, navigate to its directory, and install the required dependencies with `pip`.\n\nYOLOv5 supports various run environments, including Jupyter notebooks with GPU, as well as cloud platforms like Google Cloud and AWS. These come preconfigured with dependencies like CUDA, cuDNN, Python, and PyTorch.\n\n### Continuous Integration\n\nIf you're encountering unexpected training or evaluation results, consider checking the status of YOLOv5â€™s CI tests. Green status indicates that all training, validation, inference, export, and benchmark tests are passing on the latest commit.\n\nFinally, this is an automated response ðŸ¤–, but an Ultralytics engineer will review your issue and assist as soon as possible. Good luck with your modifications! ðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@kyrangraves hi Kyran,  \n\nYou're correct that YOLOv5's default `val.py` script considers both the IoU threshold and correct classification to determine true positives. To calculate detection rate based purely on IoU (ignoring classification correctness), you can modify the `process_batch` function in `val.py` to focus solely on IoU when identifying matches between predictions and ground truth.  \n\nAlternatively, you can adjust the evaluation pipeline to compute this metric post hoc by analyzing output predictions stored in `runs/detect/val` or directly manipulating the `results` object returned by `model.val()`.\n\nFor customization guidance, you may find it helpful to review YOLOv5's [validation implementation](https://github.com/ultralytics/yolov5/blob/master/val.py). If possible, share your modifications with the communityâ€”it might help others tackling similar challenges!  \n\nAll the best with detecting coral morphologies!"
      }
    ]
  },
  {
    "issue_number": 10137,
    "title": "calculate size of dataset after augmentation",
    "author": "myasser63",
    "state": "closed",
    "created_at": "2022-11-12T09:41:18Z",
    "updated_at": "2024-12-06T14:10:50Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHow can I know the number of the dataset after the default augmentation in `hyp.yaml`?\r\n\r\nalso why the training time of yolov5 with the default augmentation is the same with no augmentation as I know increasing the size of the dataset will increase training time while using external augmentations increases the training time\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "ðŸ‘‹ Hello! Thanks for asking about **image augmentation**. YOLOv5 ðŸš€ applies online imagespace and colorspace augmentations in the trainloader (but not the val_loader) to present a new and unique augmented [Mosaic](https://github.com/ultralytics/yolov5/blob/90b7895d652c3bd3d361b2d6e9aee900fd67f5f7/utils/datasets.py#L678-L732) (original image + 3 random images) each time an image is loaded for training. **Images are never presented twice in the same way**.\r\n\r\n![YOLOv5 augmentation](https://user-images.githubusercontent.com/26833433/120995721-f3cfed00-c785-11eb-8ee2-b6ef2fa205e8.jpg)\r\n\r\n### Augmentation Hyperparameters\r\n\r\nThe hyperparameters used to define these augmentations are in your hyperparameter file (default `data/hyp.scratch.yaml`) defined when training:\r\n```\r\npython train.py --hyp hyp.scratch-low.yaml\r\n```\r\nhttps://github.com/ultralytics/yolov5/blob/b94b59e199047aa8bf2cdd4401ae9f5f42b929e6/data/hyps/hyp.scratch-low.yaml#L6-L34\r\n\r\n### Augmentation Previews\r\n\r\nYou can view the effect of your augmentation policy in your train_batch*.jpg images once training starts. These images will be in your train logging directory, typically `yolov5/runs/train/exp`:\r\n\r\n`train_batch0.jpg` shows train batch 0 mosaics and labels:  \r\n<img src=\"https://user-images.githubusercontent.com/26833433/131255960-b536647f-7c61-4f60-bbc5-cb2544d71b2a.jpg\" width=\"800\">\r\n\r\n\r\n### YOLOv5 Albumentations Integration\r\n\r\n[YOLOv5](https://github.com/ultralytics/yolov5) ðŸš€ is now fully integrated with [Albumentations](https://github.com/albumentations-team/albumentations), a popular open-source image augmentation package. Now you can train the world's best Vision AI models even better with custom Albumentations ðŸ˜ƒ! \r\n\r\nPR https://github.com/ultralytics/yolov5/pull/3882 implements this integration, which will automatically apply Albumentations transforms during YOLOv5 training if `albumentations>=1.0.3` is installed in your environment. See https://github.com/ultralytics/yolov5/pull/3882 for full details.\r\n\r\nExample `train_batch0.jpg` on COCO128 dataset with Blur, MedianBlur and ToGray. See the YOLOv5 [Notebooks](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb) to reproduce: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n\r\n<img src=\"https://user-images.githubusercontent.com/26833433/124400879-ff331b80-dd25-11eb-9b67-fe85ac4ca104.jpg\" width=\"800\">\r\n\r\nGood luck ðŸ€ and let us know if you have any other questions!"
      },
      {
        "user": "myasser63",
        "body": "@glenn-jocher  Can you please explain why the training time of yolov5 with the default augmentation is the same with no augmentation as I know increasing the size of the dataset will increase training time while using external augmentations increases the training time"
      },
      {
        "user": "glenn-jocher",
        "body": "@myasser63 dataset size stays the same. No new images are saved, only new views are passed by the trainloader."
      }
    ]
  },
  {
    "issue_number": 13447,
    "title": "training stuck",
    "author": "passingdragon",
    "state": "closed",
    "created_at": "2024-12-06T06:35:00Z",
    "updated_at": "2024-12-06T10:43:26Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI used your framework to modify yolo and define a model by myself, but when I was training, why did I get stuck at the beginning, in the position shown below\r\n![error](https://github.com/user-attachments/assets/a37720a5-db53-4b75-9cc4-480415853e03)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @passingdragon, thank you for your interest in YOLOv5 ðŸš€! It seems like you're facing an issue with training when using a custom-defined model. \n\nIf this is a ðŸ› Bug Report, could you please provide a **minimum reproducible example (MRE)**? This would help us understand and debug the issue much faster. Include details such as the code changes you've made, the exact command you used to start training, and a snippet of the terminal output where it gets stuck.\n\nAdditionally, confirm that your environment matches the expected requirements, particularly Python versions (`>=3.8.0`) and PyTorch versions (`>=1.8`), and that all dependencies are installed correctly.\n\nYOLOv5 can be run in several environments, including local setups, Google Colab, Docker containers, AWS, or GCP. If relevant, let us know the environment you're using, as well as if you are leveraging a GPU for training.\n\nThis is an automated response to assist you as quickly as possible. An Ultralytics engineer will also review your issue and provide further help soon. ðŸ˜Š"
      },
      {
        "user": "passingdragon",
        "body": "I can run the source code successfully, but the training gets stuck after I modify the model"
      },
      {
        "user": "pderrenger",
        "body": "@passingdragon thank you for your question. When encountering training issues after modifying the model, the problem often lies in the custom changes. Please check the following:\n\n1. Ensure your modified model structure aligns with the dimensions and outputs expected by YOLOv5, especially for layers like detection heads.\n2. Verify your `.yaml` configuration for the new model (e.g., `nc` for number of classes and anchors).\n3. Debug by running with `--verbose` to identify specific points where it stalls.\n4. Confirm that dependencies are up-to-date and test with the unmodified base model to ensure the issue is not external.\n\nLet us know if you need further clarification or encounter specific errors!"
      }
    ]
  },
  {
    "issue_number": 13162,
    "title": "metric question",
    "author": "HuKai97",
    "state": "closed",
    "created_at": "2024-07-03T16:02:31Z",
    "updated_at": "2024-12-06T07:22:13Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\n(yolov5) F:\\Tensorrt\\yolov5>python val.py\r\nval: data=data\\coco.yaml, weights=yolov5s.pt, batch_size=4, imgsz=640, conf_thres=0.1, iou_thres=0.45, max_det=300, task=val, device=0, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs\\val, name=exp, exist_ok=False, half=False, dnn=False\r\nWARNING  confidence threshold 0.1 > 0.001 produces invalid results\r\nYOLOv5  v7.0-334-g100a423b Python-3.10.13 torch-2.1.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\r\n\r\nFusing layers...\r\nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\r\nval: Scanning F:\\LSR\\datasets\\coco\\labels\\val2017... 4952 images, 48 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:09<00:00, 503.19it/s]\r\nval: WARNING  Cache directory F:\\LSR\\datasets\\coco\\labels is not writeable: [WinError 183] : 'F:\\\\LSR\\\\datasets\\\\coco\\\\labels\\\\val2017.cache.npy' -> 'F:\\\\LSR\\\\datasets\\\\coco\\\\labels\\\\val2017.cache'\r\n                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [00:49<00:00, 25.46it/s]\r\n                   all       5000      36335      0.661      0.525      0.597      0.412\r\nSpeed: 0.1ms pre-process, 2.9ms inference, 1.0ms NMS per image at shape (4, 3, 640, 640)\r\n\r\nEvaluating pycocotools mAP... saving runs\\val\\exp5\\yolov5s_predictions.json...\r\nloading annotations into memory...\r\nDone (t=0.53s)\r\ncreating index...\r\nindex created!\r\nLoading and preparing results...\r\nDONE (t=0.41s)\r\ncreating index...\r\nindex created!\r\nRunning per image evaluation...\r\nEvaluate annotation type *bbox*\r\nDONE (t=6.09s).\r\nAccumulating evaluation results...\r\nDONE (t=1.55s).\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.002\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.004\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.004\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.007\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.005\r\nResults saved to runs\\val\\exp5\r\n\r\nsave_json=True, coco api metrics almost 0ï¼Œbut val metric is rightï¼Œwhy?\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@HuKai97 hello,\n\nThank you for reaching out and providing detailed information about the issue you're encountering. It appears that your validation metrics are not aligning with the COCO API metrics, which are showing almost zero values.\n\nTo better assist you, could you please provide a minimal reproducible example of your code? This will help us investigate the issue more effectively. You can refer to our guide on creating a minimal reproducible example here: [Minimum Reproducible Example](https://docs.ultralytics.com/help/minimum_reproducible_example).\n\nIn the meantime, please ensure that you are using the latest versions of `torch` and the YOLOv5 repository. You can update your repository and dependencies with the following commands:\n\n```bash\ngit pull  # update YOLOv5 repo\npip install -r requirements.txt  # update dependencies\n```\n\nAdditionally, the warning message `confidence threshold 0.1 > 0.001 produces invalid results` suggests that the confidence threshold you are using might be too high. You might want to try lowering it to see if it affects the results.\n\nHere's a quick example of how you might adjust the confidence threshold:\n\n```bash\npython val.py --weights yolov5s.pt --data coco.yaml --img 640 --conf-thres 0.001 --iou-thres 0.45 --max-det 300 --device 0 --save-json\n```\n\nThis should help ensure that the confidence threshold is not impacting your results.\n\nPlease let us know if the issue persists after trying these steps, and don't hesitate to share the minimal reproducible example for further investigation."
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO ðŸš€ and Vision AI â­"
      },
      {
        "user": "ys-jo",
        "body": "Hello,\r\nI had a similar problem. \r\nIf you are conducting the evaluation with coco dataset, it would be good to check if the variable of is_coco is True on val.py ."
      }
    ]
  },
  {
    "issue_number": 10587,
    "title": "pytorch torch.jit.save()  å®žçŽ°ä¿å­˜è·¯å¾„ä¸ºä¸­æ–‡å‡ºé”™",
    "author": "bswli",
    "state": "closed",
    "created_at": "2022-12-24T10:48:49Z",
    "updated_at": "2024-12-06T04:47:09Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\ntorch.jit.save() ä¿å­˜æ¨¡åž‹    ä¿å­˜åœ°å€è·¯å¾„å­˜åœ¨ä¸­æ–‡ï¼Œå‡ºçŽ°å¼‚å¸¸æŠ¥é”™ï¼Œ\r\nå¦‚ä½•å®žçŽ°torch.jit.save()  ä¿å­˜åœ°å€å¯ä»¥å­˜åœ¨ä¸­æ–‡ï¼Ÿï¼Ÿï¼Ÿ\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @gsaergserg, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "bswli",
        "body": "![image](https://user-images.githubusercontent.com/105101984/209487234-0cac2b51-63aa-47ca-b418-ba026c14fdce.png)\r\n![image](https://user-images.githubusercontent.com/105101984/209487258-701a88e4-3235-44c6-8061-3f399fdde581.png)\r\n"
      },
      {
        "user": "bswli",
        "body": "![image](https://user-images.githubusercontent.com/105101984/209487745-c10f4e0c-0580-44d5-98ab-00d9d8ba68f6.png)\r\nä¸Šè¿°str(f)å­—ç¬¦ä¸²è·¯å¾„  ä¸­å­˜åœ¨ä¸­æ–‡ å°±ä¼šæŠ¥é”™"
      }
    ]
  },
  {
    "issue_number": 6998,
    "title": "YOLOv5 (6.0/6.1) brief summary",
    "author": "WZMIAOMIAO",
    "state": "open",
    "created_at": "2022-03-16T04:39:06Z",
    "updated_at": "2024-12-05T21:48:27Z",
    "labels": [
      "documentation"
    ],
    "body": "# Content\r\n\r\n+ [1. Model Structure](#1)\r\n+ [2. Data Augmentation](#2)\r\n+ [3. Training Strategies](#3)\r\n+ [4. Others](#4)\r\n    - [4.1 Compute Losses](#41)\r\n    - [4.2 Balance Losses](#42)\r\n    - [4.2 Eliminate Grid Sensitivity](#43)\r\n    - [4.3 Build Targets](#44)\r\n\r\n\r\n<a name=\"1\"></a>\r\n## 1. Model Structure\r\nYOLOv5 (v6.0/6.1) consists of:\r\n- **Backbone**: `New CSP-Darknet53`\r\n- **Neck**: `SPPF`, `New CSP-PAN`\r\n- **Head**: `YOLOv3 Head`\r\n\r\nModel structure (`yolov5l.yaml`):\r\n\r\n![yolov5](https://user-images.githubusercontent.com/31005897/172404576-c260dcf9-76bb-4bc8-b6a9-f2d987792583.png)\r\n\r\n\r\nSome minor changes compared to previous versions:\r\n1. Replace the `Focus` structure with `6x6 Conv2d`(more efficient, refer #4825)  \r\n2. Replace the `SPP` structure with `SPPF`(more than double the speed)\r\n\r\n<details>\r\n<summary>test code</summary>\r\n\r\n```python\r\nimport time\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass SPP(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.maxpool1 = nn.MaxPool2d(5, 1, padding=2)\r\n        self.maxpool2 = nn.MaxPool2d(9, 1, padding=4)\r\n        self.maxpool3 = nn.MaxPool2d(13, 1, padding=6)\r\n\r\n    def forward(self, x):\r\n        o1 = self.maxpool1(x)\r\n        o2 = self.maxpool2(x)\r\n        o3 = self.maxpool3(x)\r\n        return torch.cat([x, o1, o2, o3], dim=1)\r\n\r\n\r\nclass SPPF(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.maxpool = nn.MaxPool2d(5, 1, padding=2)\r\n\r\n    def forward(self, x):\r\n        o1 = self.maxpool(x)\r\n        o2 = self.maxpool(o1)\r\n        o3 = self.maxpool(o2)\r\n        return torch.cat([x, o1, o2, o3], dim=1)\r\n\r\n\r\ndef main():\r\n    input_tensor = torch.rand(8, 32, 16, 16)\r\n    spp = SPP()\r\n    sppf = SPPF()\r\n    output1 = spp(input_tensor)\r\n    output2 = sppf(input_tensor)\r\n\r\n    print(torch.equal(output1, output2))\r\n\r\n    t_start = time.time()\r\n    for _ in range(100):\r\n        spp(input_tensor)\r\n    print(f\"spp time: {time.time() - t_start}\")\r\n\r\n    t_start = time.time()\r\n    for _ in range(100):\r\n        sppf(input_tensor)\r\n    print(f\"sppf time: {time.time() - t_start}\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nresult:\r\n```\r\nTrue\r\nspp time: 0.5373051166534424\r\nsppf time: 0.20780706405639648\r\n```\r\n\r\n</details>\r\n\r\n\r\n\r\n<a name=\"2\"></a>\r\n## 2. Data Augmentation\r\n- Mosaic\r\n<img src=\"https://user-images.githubusercontent.com/31005897/159109235-c7aad8f2-1d4f-41f9-8d5f-b2fde6f2885e.png#pic_center\" width=80%>\r\n\r\n- Copy paste\r\n<img src=\"https://user-images.githubusercontent.com/31005897/159116277-91b45033-6bec-4f82-afc4-41138866628e.png#pic_center\" width=80%>\r\n\r\n- Random affine(Rotation, Scale, Translation and Shear)\r\n<img src=\"https://user-images.githubusercontent.com/31005897/159109326-45cd5acb-14fa-43e7-9235-0f21b0021c7d.png#pic_center\" width=80%>\r\n\r\n- MixUp\r\n<img src=\"https://user-images.githubusercontent.com/31005897/159109361-3b24333b-f481-478b-ae00-df7838f0b5cd.png#pic_center\" width=80%>\r\n\r\n- Albumentations\r\n- Augment HSV(Hue, Saturation, Value)\r\n<img src=\"https://user-images.githubusercontent.com/31005897/159109407-83d100ba-1aba-4f4b-aa03-4f048f815981.png#pic_center\" width=80%>\r\n\r\n- Random horizontal flip\r\n<img src=\"https://user-images.githubusercontent.com/31005897/159109429-0d44619a-a76a-49eb-bfc0-6709860c043e.png#pic_center\" width=80%>\r\n\r\n\r\n\r\n<a name=\"3\"></a>\r\n## 3. Training Strategies\r\n- Multi-scale training(0.5~1.5x)\r\n- AutoAnchor(For training custom data)\r\n- Warmup and Cosine LR scheduler\r\n- EMA(Exponential Moving Average)\r\n- Mixed precision\r\n- Evolve hyper-parameters\r\n\r\n\r\n\r\n<a name=\"4\"></a>\r\n## 4. Others\r\n<a name=\"41\"></a>\r\n### 4.1 Compute Losses\r\nThe YOLOv5 loss consists of three parts: \r\n- Classes loss(BCE loss)\r\n- Objectness loss(BCE loss)\r\n- Location loss(CIoU loss)\r\n\r\n![loss](https://latex.codecogs.com/svg.image?Loss=\\lambda_1L_{cls}+\\lambda_2L_{obj}+\\lambda_3L_{loc})\r\n\r\n<a name=\"42\"></a>\r\n### 4.2 Balance Losses\r\nThe objectness losses of the three prediction layers(`P3`, `P4`, `P5`) are weighted differently. The balance weights are `[4.0, 1.0, 0.4]` respectively.\r\n\r\n![obj_loss](https://latex.codecogs.com/svg.image?L_{obj}=4.0\\cdot&space;L_{obj}^{small}+1.0\\cdot&space;L_{obj}^{medium}+0.4\\cdot&space;L_{obj}^{large})\r\n\r\n<a name=\"43\"></a>\r\n### 4.3 Eliminate Grid Sensitivity\r\nIn YOLOv2 and YOLOv3, the formula for calculating the predicted target information is:  \r\n\r\n![b_x](https://latex.codecogs.com/svg.image?b_x=\\sigma(t_x)+c_x)  \r\n![b_y](https://latex.codecogs.com/svg.image?b_y=\\sigma(t_y)+c_y)  \r\n![b_w](https://latex.codecogs.com/svg.image?b_w=p_w\\cdot&space;e^{t_w})  \r\n![b_h](https://latex.codecogs.com/svg.image?b_h=p_h\\cdot&space;e^{t_h})\r\n\r\n<img src=\"https://user-images.githubusercontent.com/31005897/158508027-8bf63c28-8290-467b-8a3e-4ad09235001a.png#pic_center\" width=40%>\r\n\r\n\r\n\r\nIn YOLOv5, the formula is:  \r\n\r\n![bx](https://latex.codecogs.com/svg.image?b_x=(2\\cdot\\sigma(t_x)-0.5)+c_x)  \r\n![by](https://latex.codecogs.com/svg.image?b_y=(2\\cdot\\sigma(t_y)-0.5)+c_y)  \r\n![bw](https://latex.codecogs.com/svg.image?b_w=p_w\\cdot(2\\cdot\\sigma(t_w))^2)    \r\n![bh](https://latex.codecogs.com/svg.image?b_h=p_h\\cdot(2\\cdot\\sigma(t_h))^2)  \r\n\r\nCompare the center point offset before and after scaling. The center point offset range is adjusted from (0, 1) to (-0.5, 1.5).\r\nTherefore, offset can easily get 0 or 1.\r\n\r\n<img src=\"https://user-images.githubusercontent.com/31005897/158508052-c24bc5e8-05c1-4154-ac97-2e1ec71f582e.png#pic_center\" width=40%>\r\n\r\nCompare the height and width scaling ratio(relative to anchor) before and after adjustment. The original yolo/darknet box equations have a serious flaw. Width and Height are completely unbounded as they are simply out=exp(in), which is dangerous, as it can lead to runaway gradients, instabilities, NaN losses and ultimately a complete loss of training. [refer this issue](https://github.com/ultralytics/yolov5/issues/471#issuecomment-662009779)\r\n\r\n<img src=\"https://user-images.githubusercontent.com/31005897/158508089-5ac0c7a3-6358-44b7-863e-a6e45babb842.png#pic_center\" width=40%>\r\n\r\n\r\n<a name=\"44\"></a>\r\n### 4.4 Build Targets\r\nMatch positive samples:\r\n- Calculate the aspect ratio of GT and Anchor Templates\r\n\r\n![rw](https://latex.codecogs.com/svg.image?r_w=w_{gt}/w_{at})\r\n\r\n![rh](https://latex.codecogs.com/svg.image?r_h=h_{gt}/h_{at})\r\n\r\n![rwmax](https://latex.codecogs.com/svg.image?r_w^{max}=max(r_w,1/r_w))\r\n\r\n![rhmax](https://latex.codecogs.com/svg.image?r_h^{max}=max(r_h,1/r_h))\r\n\r\n![rmax](https://latex.codecogs.com/svg.image?r^{max}=max(r_w^{max},r_h^{max}))\r\n\r\n![match](https://latex.codecogs.com/svg.image?r^{max}<{\\rm&space;anchor_t})\r\n\r\n<img src=\"https://user-images.githubusercontent.com/31005897/158508119-fbb2e483-7b8c-4975-8e1f-f510d367f8ff.png#pic_center\" width=70%>\r\n\r\n- Assign the successfully matched Anchor Templates to the corresponding cells\r\n<img src=\"https://user-images.githubusercontent.com/31005897/158508771-b6e7cab4-8de6-47f9-9abf-cdf14c275dfe.png#pic_center\" width=70%>\r\n\r\n- Because the center point offset range is adjusted from (0, 1) to (-0.5, 1.5). GT Box can be assigned to more anchors.\r\n<img src=\"https://user-images.githubusercontent.com/31005897/158508139-9db4e8c2-cf96-47e0-bc80-35d11512f296.png#pic_center\" width=70%>\r\n\r\n## Environments\r\n\r\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\r\n\r\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\r\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\r\n- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\r\n\r\n\r\n## Status\r\n\r\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\r\n\r\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\r\n",
    "comments": [
      {
        "user": "WZMIAOMIAO",
        "body": "@glenn-jocher hi, today I briefly summarized yolov5(v6.0).  Please help to see if there are any problems or put forward better suggestions. Some schematic diagrams or contents will be added later.  Thank you for your great work."
      },
      {
        "user": "zlj-ky",
        "body": "hi, 'prediction layers(P3, P4, P5) are weighted differently', how do I find it in the code, and further, modify itï¼Ÿ"
      },
      {
        "user": "WZMIAOMIAO",
        "body": "> hi, 'prediction layers(P3, P4, P5) are weighted differently', how do I find it in the code, and further, modify itï¼Ÿ\r\n\r\nhttps://github.com/ultralytics/yolov5/blob/c09fb2aa95b6ca86c460aa106e2308805649feb9/utils/loss.py#L111\r\nand\r\n\r\nhttps://github.com/ultralytics/yolov5/blob/c09fb2aa95b6ca86c460aa106e2308805649feb9/utils/loss.py#L156"
      }
    ]
  },
  {
    "issue_number": 13442,
    "title": "ImportError: cannot import name 'secure filename' from 'utilsâ€™ (E:(haut_ codelyoL0v5-flask-masterlutils__init__.py",
    "author": "serbbda",
    "state": "open",
    "created_at": "2024-12-05T02:54:52Z",
    "updated_at": "2024-12-05T19:41:11Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\nImportError: cannot import name 'secure filename' from 'utilsâ€™ (E:(haut_ codelyoL0v5-flask-masterlutils__init__.py\n\n### Environment\n\nOSï¼špycharmï¼Œwindows\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @serbbda, thank you for your interest in YOLOv5 ðŸš€! It looks like you're encountering an `ImportError`. \n\nIf this is a ðŸ› Bug Report, could you please provide a **minimum reproducible example** to help us debug it? This includes code snippets, the exact steps you're taking, and any additional details about the error you're experiencing.\n\nIf this is a custom training â“ Question, please include more information such as dataset examples, training logs, and ensure you're following recommended practices for training YOLOv5 models.\n\n## Requirements\n\nEnsure you are using **Python>=3.8.0** with all required dependencies installed. You can verify this by setting up your environment from the repository and running the commands:\n```bash\ngit clone the repository  # clone\ncd yolov5\npip install -r requirements.txt  # install dependencies\n```\n\n## Environments\n\nYOLOv5 can be run in numerous environments, including local machines, cloud platforms, and Docker containers. Itâ€™s important to have all dependencies up-to-date, including Python, PyTorch, CUDA, and CUDNN.\n\n## Status\n\nIf all repository tests are passing, the issue might be related to your specific setup or configuration. Please include more detailed logs or context for further assistance.\n\nThis is an automated response to guide you, but an Ultralytics engineer will review your issue and provide additional support soon ðŸ™‚"
      },
      {
        "user": "serbbda",
        "body": "Thanks a lotã€‚\r\nHow can I solve this problem?\r\nImportError: cannot import name 'secure filename' from 'utilsâ€™ (E:(haut_ codelyoL0v5-flask-masterlutils__init__.py)\r\nThanks very much for your reply"
      },
      {
        "user": "pderrenger",
        "body": "@serbbda the error suggests that the import of `secure_filename` is failing because it's not part of YOLOv5's `utils` module. This function is typically part of Flask (`from werkzeug.utils import secure_filename`). Verify and replace the import with `from werkzeug.utils import secure_filename` in your code. Let me know if you face further issues!"
      }
    ]
  },
  {
    "issue_number": 8065,
    "title": "How can we speed up Yolov5??",
    "author": "VyasVedant",
    "state": "closed",
    "created_at": "2022-06-01T08:29:06Z",
    "updated_at": "2024-12-04T21:16:25Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nCan anyone suggest how can I speed up program since I am using cpu and no gpu, it takes lot of time so are there any other module which can help code to speed up @glenn-jocher ???\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "ðŸ‘‹ Hello! Thanks for asking about **inference speed issues**. PyTorch Hub speeds will vary by hardware, software, model, inference settings, etc. Our default example in Colab with a V100 looks like this:\r\n\r\n<img width=\"1462\" alt=\"Screen Shot 2022-05-03 at 10 20 39 AM\" src=\"https://user-images.githubusercontent.com/26833433/166505744-b7fe9b9a-d2b3-4e95-9441-fa540e0804ef.png\">\r\n\r\n\r\nYOLOv5 ðŸš€ can be run on CPU (i.e. `--device cpu`, slow) or GPU if available (i.e. `--device 0`, faster). You can determine your inference device by viewing the YOLOv5 console output:\r\n\r\n### detect.py inference\r\n```bash\r\npython detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/\r\n```\r\n\r\n<img width=\"1039\" alt=\"Screen Shot 2022-05-03 at 2 48 42 PM\" src=\"https://user-images.githubusercontent.com/26833433/166572240-ae598b34-1084-40f0-a19b-bf00fd3f9050.png\">\r\n\r\n\r\n### YOLOv5 PyTorch [Hub](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading) inference\r\n```python\r\nimport torch\r\n\r\n# Model\r\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\r\n\r\n# Images\r\ndir = 'https://ultralytics.com/images/'\r\nimgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\r\n\r\n# Inference\r\nresults = model(imgs)\r\nresults.print()  # or .show(), .save()\r\n# Speed: 631.5ms pre-process, 19.2ms inference, 1.6ms NMS per image at shape (2, 3, 640, 640)\r\n```\r\n\r\n### Increase Speeds\r\n\r\nIf you would like to increase your inference speed some options are:\r\n\r\n- Use batched inference with [YOLOv5 PyTorch Hub](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading)\r\n- Reduce `--img-size`, i.e. 1280 -> 640 -> 320\r\n- Reduce model size, i.e. YOLOv5x -> YOLOv5l -> YOLOv5m -> YOLOv5s -> YOLOv5n\r\n- Use half precision FP16 inference with `python detect.py --half` and `python val.py --half`\r\n- Use a faster GPUs, i.e.: P100 -> V100 -> A100\r\n- [Export](https://docs.ultralytics.com/yolov5/tutorials/model_export) to ONNX or OpenVINO for up to 3x CPU speedup ([CPU Benchmarks](https://github.com/ultralytics/yolov5/pull/6613))\r\n- [Export](https://docs.ultralytics.com/yolov5/tutorials/model_export) to TensorRT for up to 5x GPU speedup ([GPU Benchmarks](https://github.com/ultralytics/yolov5/pull/6963))\r\n- Use a free GPU backends with up to 16GB of CUDA memory: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n\r\nGood luck ðŸ€ and let us know if you have any other questions!"
      },
      {
        "user": "dnth",
        "body": "@VyasVedant you can also consider using DeepSparse to significantly speedup the inference speed. I wrote a tutorial on my blog on how I got 180+ FPS inference on a CPU using only 4 cores.\r\n\r\nHope you'll find it useful ðŸ‘‡\r\n\r\nhttps://dicksonneoh.com/portfolio/supercharging_yolov5_180_fps_cpu/\r\n\r\nI also wrote a tutorial on OpenVINO ðŸ‘‡\r\n\r\nhttps://dicksonneoh.com/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/"
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello, this issue has been automatically marked as stale because it has not had recent activity. Please note it will be closed if no further activity occurs.\n\nAccess additional [YOLOv5](https://ultralytics.com/yolov5) ðŸš€ resources:\n- **Wiki** â€“ https://github.com/ultralytics/yolov5/wiki\n- **Tutorials** â€“ https://docs.ultralytics.com/yolov5\n- **Docs** â€“ https://docs.ultralytics.com\n\nAccess additional [Ultralytics](https://ultralytics.com) âš¡ resources:\n- **Ultralytics HUB** â€“ https://ultralytics.com/hub\n- **Vision API** â€“ https://ultralytics.com/yolov5\n- **About Us** â€“ https://ultralytics.com/about\n- **Join Our Team** â€“ https://ultralytics.com/work\n- **Contact Us** â€“ https://ultralytics.com/contact\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLOv5 ðŸš€ and Vision AI â­!"
      }
    ]
  },
  {
    "issue_number": 13437,
    "title": "Setting Custom Anchors in YOLOv5 for Mismatched Bounding Box Sizes",
    "author": "shinzokuro",
    "state": "closed",
    "created_at": "2024-11-28T02:10:29Z",
    "updated_at": "2024-12-03T07:05:24Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHi,\r\n\r\nI'm facing a challenge with my object detection model. My training data is quite limited, so I am using a different dataset for training and reserving my primary dataset for testing. However, there's a significant mismatch between the bounding box sizes in the training and test datasets. The objects in the test dataset are much closer to the camera, resulting in larger bounding boxes compared to the training dataset.\r\n\r\nTo address this, I decided to use the anchor sizes derived from the test dataset as custom anchors during the training process. After researching and experimenting, I found a way to set custom anchors by:\r\n\r\n1. Modifying the **hyperparameter file** to include my custom anchor sizes:\r\n   ![image](https://github.com/user-attachments/assets/6a2b900e-4efd-43ba-ac0c-4c5fcf243431)\r\n\r\n2. Changing a line in the YOLOv5 source code (`yolov5/models/yolo.py`):\r\n   ![image](https://github.com/user-attachments/assets/7cf84157-cfc5-4b39-9cfe-7b584fdeab5a)\r\n\r\n3. Enabling the `--noautoanchor` flag during training.\r\n\r\nI tried using the `model.yaml` file to set custom anchors as referenced in this [[GitHub issue](https://github.com/ultralytics/yolov5/issues/6838)](https://github.com/ultralytics/yolov5/issues/6838), but that approach didn't work for me. \r\n\r\nWas my approach to setting custom anchors correct? Are there better or more efficient ways to achieve this? I'd greatly appreciate any insights or feedback.\r\n\r\nThanks in advance!\r\n\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @shinzokuro, thank you for your interest in YOLOv5 ðŸš€! If you are looking to apply custom anchors for your training setup, it seems like youâ€™ve put a lot of thought into your approach. Well done! Here are a few additional pointers to guide you:\n\nIf this is a ðŸ› Bug Report, we recommend sharing a **minimum reproducible example (MRE)**, including your model configurations, dataset details (if possible), and the exact steps you followed. This information will help us identify and debug any potential issues more efficiently.\n\nFor custom training or feature adjustment â“ Questions, please provide additional context, such as your dataset distribution, training logs, and any errors or unexpected behaviors youâ€™ve encountered during testing. This will help us better understand the challenges you are facing.\n\n## Requirements\nMake sure you have Python>=3.8.0 installed, along with PyTorch>=1.8 and all dependencies specified in the `requirements.txt` file. To set up your environment:\n```bash\ngit clone the YOLOv5 repository  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\nYOLOv5 can be used on local machines or via online environments such as Notebooks (e.g., Colab, Kaggle), cloud services with pre-installed deep learning environments (e.g., Google Cloud, AWS), or Docker containers. Using one of these verified environments can streamline your workflow and ensure compatibility with all dependencies.\n\n## Status\nIf you believe this might be related to an issue in the codebase, kindly check if the YOLOv5 Continuous Integration (CI) tests are passing. CI tests ensure the functionality of training, validation, inference, and other processes in YOLOv5 across supported platforms.\n\nThis is an automated response ðŸ› ï¸ to help address your query more quickly. Rest assured, an Ultralytics engineer will review this soon and provide further assistance. If you encounter any reproducibility issues or need clarification, feel free to provide updates here! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Thank you for your detailed explanation. Your steps for setting custom anchors, including modifying the hyperparameter file, altering the YOLOv5 source code, and using the `--noautoanchor` flag, are valid. However, an easier approach would be to modify the anchors directly in the `model.yaml` file under the `anchors` parameter without altering the source code. Ensure the `noautoanchor` flag is enabled during training to prevent automatic anchor updates. You can find additional guidance on custom anchors in the [YOLOv5 repository](https://github.com/ultralytics/yolov5/issues/6838). If the issue persists, verify whether you're using the latest YOLOv5 version and that your anchor dimensions align with the test dataset bounding box distribution."
      }
    ]
  },
  {
    "issue_number": 13336,
    "title": "Cropping training images along with their respective annotations",
    "author": "andualemw1",
    "state": "closed",
    "created_at": "2024-09-27T14:09:23Z",
    "updated_at": "2024-12-03T03:11:39Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nAs we know, YOLO supports both square and rectangular images. However, for speed and dataset size considerations, I want to crop an image from 1280x1280 to 640x640. YOLO annotations/labels are originally created based on the imageâ€™s width and height. How can I bridge the gap in training the dataset before and after cropping the image while keeping the annotations unchanged?\r\n\r\nthanks in advance!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @andualemw1, thank you for your interest in YOLOv5 ðŸš€! An Ultralytics engineer will assist you soon.\n\nTo get started with cropping images while retaining annotations, you might find our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5/) helpful. You can explore guides for tasks such as [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/) where managing image sizes and annotations is discussed.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it.\n\nFor custom training questions, provide as much information as possible, including dataset image examples and training logs. Verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Requirements\n\nEnsure you have [**Python>=3.8.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed, including [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). To get started, run:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in verified environments, including:\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py), and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n## Introducing YOLOv8 ðŸš€\n\nExplore our latest object detection model, [YOLOv8](https://github.com/ultralytics/ultralytics) ðŸš€! Designed for speed and accuracy, perfect for a wide range of tasks. Discover more in our [YOLOv8 Docs](https://docs.ultralytics.com/) and get started with:\n```bash\npip install ultralytics\n```\n\nFeel free to provide further details to help us address your question! ðŸ”"
      },
      {
        "user": "pderrenger",
        "body": "@andualemw1 to crop your images from 1280x1280 to 640x640 while maintaining correct YOLO annotations, you'll need to adjust the annotations to match the new image dimensions. This involves recalculating the bounding box coordinates based on the new crop position. You can automate this process using a script to update the annotations accordingly. If you need further guidance, consider checking out image processing libraries like OpenCV or PIL for assistance with cropping and recalculating coordinates."
      },
      {
        "user": "andualemw1",
        "body": "Thank you so much for your reply support, this has been my quest for quite a while, I will do accordingly."
      }
    ]
  },
  {
    "issue_number": 6142,
    "title": "model config explain",
    "author": "iumyx2612",
    "state": "closed",
    "created_at": "2021-12-31T08:01:16Z",
    "updated_at": "2024-12-02T07:46:47Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nCan you clearly explain the config file, for example yolov5s.yaml\r\n```\r\n# YOLOv5 backbone\r\nbackbone:\r\n  # [from, number, module, args]\r\n  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\r\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\r\n   [-1, 3, C3, [128]],\r\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\r\n   [-1, 9, C3, [256]],\r\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\r\n   [-1, 9, C3, [512]],\r\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\r\n   [-1, 1, SPP, [1024, [5, 9, 13]]],\r\n   [-1, 3, C3, [1024, False]],  # 9\r\n  ]\r\n```\r\nI understand that `module` is the module class from models/common.py\r\nBut what is `from`, `number` and `args`?\r\nAnd what is the meaning of the comments like `# 0-P1/2`, `# 1-P2/4` etc.\r\nAnd how did a string from *.yaml file can be cast to a module class in yolo.py [line 251](https://github.com/ultralytics/yolov5/blob/master/models/yolo.py#L251)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@iumyx2612 \r\n\r\n`from`: from which layer the module input comes from. Uses python syntax so -1 indicates prior layer.\r\n`number`: indicates the number of times a module repeats or how many repeats repeatable modules like C3 use\r\n`args`: module arguments (input channels inherited automatically)"
      },
      {
        "user": "iumyx2612",
        "body": "> @iumyx2612\r\n> \r\n> `from`: from which layer the module input comes from. Uses python syntax so -1 indicates prior layer. `number`: indicates the number of times a module repeats or how many repeats repeatable modules like C3 use `args`: module arguments (input channels inherited automatically)\r\n\r\nFor example: \r\n`[-1, 1, Conv, [128, 3, 2]],  # 1-P2/4` \r\nshould be:\r\n`Conv(c1=what_ever_channel_from_prior_layer, c2=128, k=3, s=2)`\r\nAm I right?"
      },
      {
        "user": "glenn-jocher",
        "body": "@iumyx2612 yes exactly, that's right!"
      }
    ]
  },
  {
    "issue_number": 10491,
    "title": "INT8 quantization for CoreML models does not work on macOS",
    "author": "dokluch",
    "state": "closed",
    "created_at": "2022-12-13T13:17:22Z",
    "updated_at": "2024-12-01T11:57:38Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nExport\n\n### Bug\n\nPassing `--int8` flag to any export.py command on macOS produces the following error.\r\n\r\n```\r\nCoreML: starting export with coremltools 6.1...\r\nTuple detected at graph output. This will be flattened in the converted model.\r\nConverting PyTorch Frontend ==> MIL Ops: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 607/609 [00:00<00:00, 4497.54 ops/s]\r\nRunning MIL Common passes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 430.07 passes/s]\r\nRunning MIL Clean up passes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 248.74 passes/s]\r\nTranslating MIL ==> NeuralNetwork Ops: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 632/632 [00:00<00:00, 641.14 ops/s]\r\nQuantizing using kmeans_lut quantization\r\nOptimizing Neural Network before Quantization:\r\nFinished optimizing network. Quantizing neural network..\r\nQuantizing layer input.1 of type convolution\r\nCoreML: export failure âŒ 12.7s: scikit-learn package required for k-means quantization\r\n```\r\n\r\nAll requirements are installed, sklearn can be imported inside an interactive session.\r\n--half flag works, other export options work properly\n\n### Environment\n\n- Macbook M1 max\r\n- Conda environment with YOLOv5 ðŸš€ v7.0-32-g357cde9 Python-3.10.8 torch-1.13.0 CPU\n\n### Minimal Reproducible Example\n\n```\r\nexport.py --weights \"weights.pt\" --include coreml --int8\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "bfialkoff",
        "body": "Is there a solution for this issue? I have the same problem"
      },
      {
        "user": "glenn-jocher",
        "body": "@bfialkoff hi there! \n\nI'm sorry to hear that you're experiencing the same issue. Currently, the `--int8` flag does not seem to work when using the export command on macOS, even when all requirements are installed properly. We're aware of the issue and are looking into it. \n\nIn the meantime, the `--half` flag may be used as an alternative. If there are any updates or fixes regarding this issue, we will be sure to announce it on our GitHub repository. Thank you for bringing this to our attention and for your patience. \n\nIf you have any further questions or concerns, please don't hesitate to ask. \n\nBest, \nTeam Ultralytics"
      },
      {
        "user": "ahbpp",
        "body": "Hi there!\r\nIt's a bit late, but `pip install scikit-learn==1.1.2` worked for me\r\nIt looks like `coremltools` has strict requirements for  `scikit-learn`\r\n"
      }
    ]
  },
  {
    "issue_number": 9178,
    "title": "Corrupt JPEG data: 130816 extraneous bytes before marker 0xdb",
    "author": "realgump",
    "state": "closed",
    "created_at": "2022-08-27T14:59:05Z",
    "updated_at": "2024-12-01T10:10:18Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\n_No response_\n\n### Bug\n\nWhen training, there is some error occurs:\r\n`Corrupt JPEG data: 130816 extraneous bytes before marker 0xdb`.\n\n### Environment\n\n_No response_\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\nThere has been a commit to fix that in #4548. However, the errors are raised by _opencv-python_ while the auto-fix is writen by _pillow_. I found my jpg images pass the _pillow_ verification but still cannot be read by _opencv-python_, the verify code does nothing but increases time consumption and raise lots of unneccessary warnings. Why not use both _opencv-python_ or _pillow_ in the auto-fix period and the training period?\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@realgump hi, thank you for your feature suggestion on how to improve YOLOv5 ðŸš€!\r\n\r\nThe fastest and easiest way to incorporate your ideas into the official codebase is to submit a Pull Request (PR) implementing your idea, and if applicable providing before and after profiling/inference/training results to help us understand the improvement your feature provides. This allows us to directly see the changes in the code and to understand how they affect workflows and performance.\r\n\r\nPlease see our âœ… [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started."
      },
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello, this issue has been automatically marked as stale because it has not had recent activity. Please note it will be closed if no further activity occurs.\n\nAccess additional [YOLOv5](https://ultralytics.com/yolov5) ðŸš€ resources:\n- **Wiki** â€“ https://github.com/ultralytics/yolov5/wiki\n- **Tutorials** â€“ https://docs.ultralytics.com/yolov5\n- **Docs** â€“ https://docs.ultralytics.com\n\nAccess additional [Ultralytics](https://ultralytics.com) âš¡ resources:\n- **Ultralytics HUB** â€“ https://ultralytics.com/hub\n- **Vision API** â€“ https://ultralytics.com/yolov5\n- **About Us** â€“ https://ultralytics.com/about\n- **Join Our Team** â€“ https://ultralytics.com/work\n- **Contact Us** â€“ https://ultralytics.com/contact\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLOv5 ðŸš€ and Vision AI â­!"
      },
      {
        "user": "halqadasi",
        "body": "We read each JPEG image in the specified folder using OpenCV and overwrite it by re-saving with a quality setting of 95. This re-encoding process fixes any corruption issues by standardizing the image data format.\r\n\r\n    for filename in tqdm(image_files, desc=\"Re-encoding images\"):\r\n\r\n        image_path = os.path.join(folder_path, filename)\r\n        img = cv2.imread(image_path)\r\n\r\n        if img is not None:\r\n           # Re-encode and overwrite the image\r\n           cv2.imwrite(image_path, img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\r\n        else:\r\n            print(f\"Failed to read image data: {image_path}\")\r\n\r\n**This solution fixed my problem after long time of debugging.** \r\n"
      }
    ]
  },
  {
    "issue_number": 13396,
    "title": "How to read yolo11 tflite output tensor",
    "author": "francesco-clementi-92",
    "state": "open",
    "created_at": "2024-11-01T21:34:46Z",
    "updated_at": "2024-11-30T14:38:47Z",
    "labels": [
      "question",
      "exports"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I read every online post on how to decode yolo tflite output tensor data.\r\nMy current setup is react-native + react-native-vision-camera + vision-camera-resize-plugin + react-native-fast-tflite.\r\n\r\nI have done the training of a yolo11n models on a custom dataset and generated first an int8 with nms tflite model, then only an int8, then a simple tflite, but with each one of them my life is getting harder and harder. I cannot use ultralytics on javascript so I need to decode manually the output.\r\n\r\nCurrenlty I'm trying to run my model which as:\r\ninput: tensor: float32[1,320,320,3]\r\noutput: tensor: float32[1,5,2100]\r\n\r\nSo no quantization applied.\r\nResizing the frame is as easy as doing:\r\n```\r\nresize(frame, {\r\n          scale: {\r\n            width: 320,\r\n            height: 320\r\n          },\r\n          pixelFormat: 'rgb',\r\n          dataType: 'float32'\r\n        });\r\n```\r\n\r\nThen I run the tflite model:\r\n`const outputTensor = model.runSync([data]);`\r\n\r\nNow the hardest part.\r\n\r\nMy model has only one class.\r\n\r\nFrom the output, I understand that there is one input batch (1), 5 detection attributes `[x, y, width, height, confidence]` and 2100 detection candidates.\r\n\r\nI don't know why, but it seems that the output is a one dimensional array.\r\n\r\nIf I only focus on the confidence with this code:\r\n```\r\nfor (let i = 0; i < output[0].length; i += 5) {\r\n          const confidence = predictions[i+4];\r\n}\r\n```\r\nI get confidence score bigger than 1, so it's not possible.\r\n\r\nAny help would be appreciated :) \r\n \r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @francesco-clementi-92, thank you for reaching out about your YOLOv5 inquiry ðŸš€! We appreciate your interest and your detailed description of the issue you're facing.\n\nFor your current situation working with the YOLO model outputs in a React Native environment, it can indeed be tricky to manually decode the TFLite output tensors. Since the output tensor you are referring to seems to be producing unexpected values, such as confidence scores greater than 1, this could be due to the tensor's interpretation or values being misaligned or improperly scaled.\n\nIf this is a ðŸ› Bug Report, could you please provide a **minimum reproducible example**? This will help us to understand and potentially replicate the issue.\n\nIf this is related to your custom training setup or inference â“ Question, including additional information like your model configuration, sample inputs, and any logs or errors you're encountering can be very helpful. Also, ensure you have followed best practices for dataset preparation and training.\n\n## Requirements\n\nTo ensure everything is set up correctly, please use Python>=3.8.0 with all dependencies installed. You can follow these basic steps to set up your environment:\n\n```bash\ngit clone the repository\ncd into the directory\npip install dependencies\n```\n\n## Environments\n\nPlease verify that your environment is up-to-date and configured correctly. YOLOv5 can be run in various environments, including local machines with required packages, or cloud-based solutions where everything is pre-configured.\n\n## Status\n\nEnsure that you have the latest updates from the repository, and if you need more assistance, our community and contributors are here to help!\n\nLastly, we're excited about our latest development, YOLOv8, which might bring significant improvements and added features to your workflow, so consider exploring it as well!\n\nThis is an automated response to guide you while an Ultralytics engineer will also assist soon. Feel free to continue providing any additional information that might help us assist you better. ðŸ˜Š"
      },
      {
        "user": "francesco-clementi-92",
        "body": "Just to give an example, these are the first values of the output:\r\n`0.62, 0.75, 0.84, 0.93, 0.63, 0.76, 0.83, 0.94, 0.61, 0.74, 0.84, 0.95, 0.62, 0.75, 0.85, 0.95, 0.62, 0.75, 0.85, 0.95, 0.62, 0.75, 0.86, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.88, 0.95, 0.62, 0.74, 0.88, 0.95, 0.62, 0.74, 0.88, 0.95, 0.62, 0.74, 0.88, 0.95, 0.62, 0.74, 0.88, 0.95, 0.62, 0.74, 0.88, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.61, 0.74, 0.87, 0.95, 0.61, 0.74, 0.87, 0.95, 0.61, 0.74, 0.87, 0.95, 0.62, 0.74, 0.87, 0.95, 0.61, 0.74, 0.86, 0.95, 0.61, 0.74, 0.86, 0.95, 0.62, 0.74, 0.86, 0.94, 0.62, 0.75, 0.86, 0.94, 0.62, 0.84, 0.62, 0.87, 0.62, 0.89, 0.63, 0.91, 0.64, 0.94, 0.64, 0.94, 0.64, 0.95, 0.64, 0.95, 0.64, 0.95, 0.64, 0.95, 0.64, 0.95, 0.64, 0.95, 0.64, 0.95, 0.64, 0.94, 0.63, 0.94, 0.63, 0.94, 0.63, 0.94, 0.63, 0.95, 0.67, 0.95, 0.72, 0.95, 0.71, 0.79, 0.84, 0.86, 0.87, 0.86, 0.86, 0.85, 0.84, 0.85, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.63, 0.63, 0.64, 0.64, 0.64, 0.63, 0.64, 0.64, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.69, 0.69, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.73, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.78, 0.79, 0.79, 0.79, 0.79, 0.79, 0.79, 0.79, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.84, 0.84, 0.87, 0.86, 0.86, 0.86, 0.86, 0.86, 0.87, 0.86, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.88, 0.91, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.90, 0.93, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.91, 0.96, 0.97, 0.97, 0.97, 0.97, 0.96, 0.96, 0.94, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.96, 0.60, 0.61, 0.60, 0.65, 0.66, 0.65, 0.64, 0.70, 0.71, 0.69, 0.68, 0.75, 0.76, 0.74, 0.71, 0.80, 0.81, 0.77, 0.75, 0.86, 0.86, 0.79, 0.80, 0.91, 0.91, 0.84, 0.98, 0.94, 0.94, 0.97, 0.77, 0.71, 0.64, 0.90, 0.78, 0.66, 0.93, 0.78, 0.65, 0.92, 0.78, 0.65, 0.91, 0.79, 0.64, 0.91, 0.79, 0.63, 0.89, 0.78, 0.62, 0.88, 0.77, 0.62, 0.87, 0.77, 0.61, 0.87, 0.77, 0.62, 0.87, 0.77, 0.62, 0.88, 0.77, 0.62, 0.88, 0.77, 0.62, 0.88, 0.78, 0.61, 0.88, 0.78, 0.61, 0.88, 0.77, 0.60, 0.88, 0.77, 0.88, 0.77, 0.85, 0.75, 0.74, 0.65, 0.99, 1.00, 1.00, 0.99, 0.99, 1.00, 1.00, 0.99, 1.01, 1.03, 0.60, 0.60, 0.60, 0.61, 0.78, 0.70, 0.90, 0.81, 0.93, 0.86, 0.94, 0.90, 0.95, 0.92, 0.66`"
      },
      {
        "user": "pderrenger",
        "body": "@francesco-clementi-92 it looks like the output values you're seeing might be due to incorrect scaling or normalization. Ensure that your model's output is being interpreted correctly, particularly the confidence scores, which should be between 0 and 1. You might need to apply a sigmoid function to the confidence values if they aren't already scaled. If the issue persists, please verify with the latest version of the YOLOv5 package."
      }
    ]
  },
  {
    "issue_number": 13439,
    "title": "How to save model (export) upon program exit",
    "author": "Mitutoyum",
    "state": "open",
    "created_at": "2024-11-29T04:59:54Z",
    "updated_at": "2024-11-29T14:21:47Z",
    "labels": [
      "question",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHow can i make yolo to automically export when the program exits? or atleast save it after each epoch\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Mitutoyum, thank you for your interest in YOLOv5 ðŸš€! Please review the YOLOv5 documentation and tutorials to explore solutions for tasks like exporting or saving models.\n\nIf this is a custom training â“ Question, to assist you better, please provide as much information as possible, including details about your training process or any relevant logs. If your query relates to a specific feature like saving models, you may explore the available export options or saving checkpoints during training to ensure your progress is correctly handled.\n\nIf this is a potential ðŸ› Bug Report, please share a **minimum reproducible example** that demonstrates the issue. This will help us better understand and debug your situation.\n\n## Requirements\n\nEnsure you have Python 3.8.0 or newer installed along with all YOLOv5 dependencies, and verify compatibility with PyTorch 1.8 or greater.\n\n## Environments\n\nYOLOv5 is designed to run on various platforms, including cloud notebooks, local machines, or containerized environments like Docker. Ensure all dependencies, including CUDA, Python, and PyTorch, are installed and up to date on your platform.\n\n## Status\n\nTo verify proper operation, run YOLOv5's training, validation, or inference pipelines in your selected environment. Ensure all tests pass successfully.\n\nThis is an automated response to help guide you in the right direction ðŸ˜Š. An Ultralytics team member will review and assist you further as soon as possible!"
      },
      {
        "user": "pderrenger",
        "body": "@Mitutoyum to automatically export a YOLO model upon program exit, you can utilize Python's signal handling to trigger the export on exit. For example:\n\n```python\nimport signal\nfrom ultralytics import YOLO\n\n# Load your YOLO model\nmodel = YOLO(\"path/to/best.pt\")\n\n# Define export function\ndef export_model(signal_received, frame):\n    print(\"Exporting model on exit...\")\n    model.export(format=\"onnx\")  # Change format as needed\n    exit(0)\n\n# Set up signal handler for graceful exit\nsignal.signal(signal.SIGINT, export_model)\nsignal.signal(signal.SIGTERM, export_model)\n\n# Training process\nmodel.train(data=\"data.yaml\", epochs=10)\n```\n\nAlternatively, to save the model after each epoch, you can do so within the training loop by customizing the existing `train()` method. For further details on model export, refer to the [Export Documentation](https://docs.ultralytics.com/modes/export/)."
      }
    ]
  },
  {
    "issue_number": 13434,
    "title": "WARNING âš ï¸ NMS time limit 2.100s exceeded",
    "author": "lqh964165950",
    "state": "open",
    "created_at": "2024-11-27T12:43:48Z",
    "updated_at": "2024-11-27T21:13:23Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nä¸ºä»€ä¹ˆè®­ç»ƒçš„æ¯ä¸€è½®éƒ½ä¼šå‡ºçŽ°è¿™ä¸ªè­¦å‘Šï¼Ÿè¯¥æ€Žä¹ˆè§£å†³è¿™ä¸ªé—®é¢˜ã€‚\r\n![image](https://github.com/user-attachments/assets/0f61b8bb-91fb-460d-9dd3-772deeb1b004)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @lqh964165950, thank you for your interest in YOLOv5 ðŸš€! \n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug the issue, such as your system details, training parameters, and code snippet. Reproducing the issue on a smaller dataset can also be helpful.\n\nIf this is a custom training â“ Question, please include as much information as possible, such as training arguments and logs, and ensure you are following best practices for training YOLOv5 models.\n\n## Requirements\n\nPlease ensure you have Python >= 3.8.0 and all required dependencies installed, including PyTorch >= 1.8. Verify your setup by cloning the YOLOv5 repository, navigating to its directory, and running the relevant scripts.\n\n## Environments\n\nYOLOv5 can be run in diverse environments, including Jupyter notebooks, cloud services like Google Cloud or AWS, or even Docker containers. Make sure your runtime is properly configured and equipped with the necessary dependencies like CUDA, cuDNN, Python, and PyTorch.\n\n## Status\n\nOur Continuous Integration (CI) system verifies the correct operation of YOLOv5 across various processes, such as training, validation, inference, export, and benchmarks, and helps ensure its reliability on Windows, Ubuntu, and MacOS.\n\nThis is an automated response to help you get started. An Ultralytics engineer will follow up with you soon to assist further. Let us know if you can provide additional details to help us understand and address your issue. ðŸ˜ŠðŸš€"
      },
      {
        "user": "pderrenger",
        "body": "@lqh964165950 thank you for reaching out. The \"NMS time limit exceeded\" warning indicates that the Non-Maximum Suppression (NMS) step during inference is taking longer than expected, potentially due to a high number of predictions per image. To address this:\n\n1. Try reducing the number of predictions by increasing the `conf-thres` value (e.g., `--conf 0.5`) in your training or inference command.\n2. Ensure youâ€™re using the latest version of YOLOv5 and the Ultralytics package as updates may resolve performance issues. Update with `pip install --upgrade ultralytics`.\n3. For further optimization, consider lowering the image size (`--img-size`) or debugging for unusually complex images.\n\nIf the issue persists, feel free to share additional details on your training settings."
      }
    ]
  },
  {
    "issue_number": 9679,
    "title": "What is confidence threshold",
    "author": "284nnuS",
    "state": "closed",
    "created_at": "2022-10-03T06:41:58Z",
    "updated_at": "2024-11-26T22:27:03Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello , i'm a newbiew. So can i ask u guys about :\r\nparser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\r\nI want to understand about this hyper param. Where can i find this to understand\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "maxPrakken",
        "body": "The confidence determines how certain the model is that the prediction received matches to a certain class. the threshold determines what the threshold for labeling something as something should be. lets say you have a confidence threshold of 0.6, which means the model will have to be at least 60% sure the object you're trying to classify is that object before it'll label it.\r\n\r\nhttps://support.ultimate.ai/hc/en-us/articles/7941166026258-Confidence-Thresholds\r\n\r\nhere's a article about it if you want to learn more"
      },
      {
        "user": "284nnuS",
        "body": "> The confidence determines how certain the model is that the prediction received matches to a certain class. the threshold determines what the threshold for labeling something as something should be. lets say you have a confidence threshold of 0.6, which means the model will have to be at least 60% sure the object you're trying to classify is that object before it'll label it.\r\n> \r\n> https://support.ultimate.ai/hc/en-us/articles/7941166026258-Confidence-Thresholds\r\n> \r\n> here's a article about it if you want to learn more\r\n\r\nDoes the confidence is related to IoU ? For example, As you say conf 0.6 which means the model will have at least 60% sure. But what IoU does it have? \r\nHope that you can reply to me,\r\nMany thanks"
      },
      {
        "user": "maxPrakken",
        "body": "> > The confidence determines how certain the model is that the prediction received matches to a certain class. the threshold determines what the threshold for labeling something as something should be. lets say you have a confidence threshold of 0.6, which means the model will have to be at least 60% sure the object you're trying to classify is that object before it'll label it.\r\n> > https://support.ultimate.ai/hc/en-us/articles/7941166026258-Confidence-Thresholds\r\n> > here's a article about it if you want to learn more\r\n> \r\n> Does the confidence is related to IoU ? For example, As you say conf 0.6 which means the model will have at least 60% sure. But what IoU does it have? Hope that you can reply to me, Many thanks\r\n\r\n\"confidence threshold is the minimum score that the model will consider the prediction to be a true prediction (otherwise it will ignore this prediction entirely). IoU threshold is the minimum overlap between ground truth and prediction boxes for the prediction to be considered a true positive.\" These two values are used to calculate the mAP but are not directly related. IoU just like confidence is a value that is extracted from the results of your model. \r\n\r\nI hope this helped you understand better. Below I'll also link a stack overflow post that explains the concept pretty well in the accepted answer, this answer also relates to YOLOv5 so it should be applicable.\r\n\r\nhttps://stackoverflow.com/questions/68527004/selecting-an-iou-and-confidence-threshold-for-evaluation-of-model-performance"
      }
    ]
  },
  {
    "issue_number": 13430,
    "title": "How to evaluate VisDrone-VID dataset? ",
    "author": "Wzh10032",
    "state": "open",
    "created_at": "2024-11-23T01:38:43Z",
    "updated_at": "2024-11-24T20:44:05Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHow to evaluate Task 2: Object Detection in Videos in VisDrone-Dataset? The existing [VisDrone.yaml](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml) seems to only support VisDrone-DET dataset.\r\n![image](https://github.com/user-attachments/assets/0ee5fb09-9d08-43e5-bb2a-e08b103ef2ad)\r\n\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @Wzh10032, thank you for your interest in YOLOv5 ðŸš€! It looks like you are trying to evaluate the VisDrone-VID dataset. To assist you better, if this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it. \n\nIf this is a custom training â“ Question, it would be helpful to include additional details such as dataset image examples, any custom YAML files you might be using, and your training logs. Also, make sure you are following best practices outlined in our training guides.\n\n## Requirements\n\nEnsure you have [Python>=3.8.0] with all necessary dependencies installed, including PyTorch>=1.8. You can start by cloning the YOLOv5 repository and installing the required packages:\n```bash\ngit clone the yolov5 repository  # clone\ncd into the yolov5 directory\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 can be run in various environments, including cloud-based notebooks with free GPU access, Google Cloud Deep Learning VMs, Amazon Deep Learning AMIs, and Docker images. This flexibility allows you to choose an environment that best suits your needs.\n\n## Status\n\nCheck the current status of YOLOv5 continuous integration tests in the GitHub Actions page to verify if all tests are passing.\n\nThis response is automated, but an Ultralytics engineer will be with you shortly to provide further assistance. If you have additional questions in the meantime, feel free to update this issue. ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@Wzh10032 thank you for your question! To evaluate Task 2: Object Detection in Videos (VisDrone-VID), you'll need to adapt the dataset configuration and ensure compatibility with the video evaluation process. Unfortunately, the current `VisDrone.yaml` is tailored for static image detection (VisDrone-DET). To proceed:\n\n1. Prepare the video dataset in YOLO format by splitting video frames into images and ensuring proper annotations.\n2. Create a custom YAML file to match your dataset's structure for videos.\n3. For evaluation, use the same process as object detection but account for temporal aspects by evaluating predictions across frames.\n\nFor detailed guidance on dataset formatting, YOLO workflows, and evaluation, refer to the [Ultralytics documentation](https://docs.ultralytics.com/datasets/detect/visdrone/). Let us know if you encounter specific issues!"
      },
      {
        "user": "Wzh10032",
        "body": "> @Wzh10032 thank you for your question! To evaluate Task 2: Object Detection in Videos (VisDrone-VID), you'll need to adapt the dataset configuration and ensure compatibility with the video evaluation process. Unfortunately, the current `VisDrone.yaml` is tailored for static image detection (VisDrone-DET). To proceed:\r\n> \r\n> 1. Prepare the video dataset in YOLO format by splitting video frames into images and ensuring proper annotations.\r\n> 2. Create a custom YAML file to match your dataset's structure for videos.\r\n> 3. For evaluation, use the same process as object detection but account for temporal aspects by evaluating predictions across frames.\r\n> \r\n> For detailed guidance on dataset formatting, YOLO workflows, and evaluation, refer to the [Ultralytics documentation](https://docs.ultralytics.com/datasets/detect/visdrone/). Let us know if you encounter specific issues!\r\n\r\nThanks for your reply, I will try to solve this problem."
      }
    ]
  },
  {
    "issue_number": 13427,
    "title": "å¦‚ä½•åœ¨yolov5ä¸­æ·»åŠ FPSå’ŒmAPsè¯„ä»·æŒ‡æ ‡ï¼Ÿ",
    "author": "lqh964165950",
    "state": "open",
    "created_at": "2024-11-22T03:00:31Z",
    "updated_at": "2024-11-24T10:09:08Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nå¦‚ä½•åœ¨yolov5ä¸­æ·»åŠ FPSå’ŒmAPsè¯„ä»·æŒ‡æ ‡ï¼Ÿ\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @lqh964165950, thank you for your interest in YOLOv5 ðŸš€! \n\nIf you're looking to add FPS and mAP evaluation metrics in YOLOv5, please make sure you are familiar with the evaluation process outlined in our official documentation. \n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us debug it. This helps us understand the context better and provide a more accurate solution to the issue.\n\nFor custom training â“ questions, please provide as much detail as possible, including examples of your dataset images and any training logs you have. Ensure you are following our guidelines for best training results, which cover topics like model hyperparameters and data preparation.\n\n## Requirements\n\nTo run YOLOv5, you need Python>=3.8.0 with all required dependencies installed. You can set up your environment by cloning the YOLOv5 repository and installing the dependencies listed in the requirements file.\n\n## Environments\n\nYOLOv5 can be executed in various environments such as cloud-based notebooks with free GPU support, Google Cloud, Amazon Deep Learning AMI, and through Docker images. \n\n## Status\n\nIf you check the status badge in our GitHub repository, it indicates whether all Continuous Integration (CI) tests are currently passing. This badge gives you a quick overview of the repository's health and functionality.\n\nThis is an automated response, but rest assured an Ultralytics engineer will assist you soon! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@lqh964165950 to add FPS and mAP evaluation metrics in YOLOv5, you can utilize the validation script `val.py` for mAP calculations. FPS can be measured by running inference using `detect.py` while timing the inference process. For customized benchmarking, consider integrating tools and metrics into `val.py` or using external profiling libraries. You may find these guides helpful:\n\n- [Validation guide](https://github.com/ultralytics/yolov5/blob/master/val.py) for mAP evaluation.\n- [Inference script](https://github.com/ultralytics/yolov5/blob/master/detect.py) for measuring FPS. \n\nFeel free to adapt the code for specific metric computations. Let us know if you encounter issues!"
      }
    ]
  },
  {
    "issue_number": 10372,
    "title": "Is it possible to show segmentation and object detection at the same time?",
    "author": "stphtan94117",
    "state": "closed",
    "created_at": "2022-12-02T07:28:18Z",
    "updated_at": "2024-11-24T03:58:59Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nsome object used segmentation detection ,some object use bbox detection.\r\n\r\nFor example, the scene is on the road. \r\nI want to use segmentation to detect potholes, but the road markings which use object detection (BBOX)\r\nI don't know if it is possible to show both models at the same time?\r\n\r\nI want to achieve both segmentation and object detection in one task.\r\nI don't want to separate the two model training.\r\n\r\nvery thanks.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "ðŸ‘‹ Hello @stphtan94117, thank you for your interest in YOLOv5 ðŸš€! Please visit our â­ï¸ [Tutorials](https://docs.ultralytics.com/yolov5) to get started, where you can find quickstart guides for simple tasks like [Custom Data Training](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) all the way to advanced concepts like [Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\n\nIf this is a ðŸ› Bug Report, please provide screenshots and **minimum viable code to reproduce your issue**, otherwise we can not help you.\n\nIf this is a custom training â“ Question, please provide as much information as possible, including dataset images, training logs, screenshots, and a public link to online [W&B logging](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#visualize) if available.\n\nFor business inquiries or professional support requests please visit https://ultralytics.com or email support@ultralytics.com.\n\n## Requirements\n\n[**Python>=3.7.0**](https://www.python.org/) with all [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) installed including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). To get started:\n```bash\ngit clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n```\n\n## Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on MacOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@stphtan94117 segmentation models are also detection models, but not vice versa. i.e.:\r\n\r\n<img width=\"775\" alt=\"Screenshot 2022-12-03 at 12 16 44\" src=\"https://user-images.githubusercontent.com/26833433/205460226-f6a0ed6c-81ac-4b60-858e-297418bcf391.png\">\r\n"
      },
      {
        "user": "stphtan94117",
        "body": "@glenn-jocher \r\ni mean some object displayed segmantation,other object only show bbox in the picture.\r\nTake your bus picture as an example, bus object displayed green polygon,\r\nthree person object only show bbox without polygon(NO segmentation)\r\n\r\ni want to save time to label because segmentation label takes me a lot of time\r\nSo I want to be lazy and use bbox for some objects, and use segmentation for key objects.\r\n\r\n\r\nif can display both typeï¼Œwhat happened when use --save-txt?\r\nbecause bbox is xywh, seg is x1y1x2y2....\r\n"
      }
    ]
  },
  {
    "issue_number": 13424,
    "title": "There seems to be an error in the calculation of the fitness function.",
    "author": "thgpddl",
    "state": "closed",
    "created_at": "2024-11-21T09:04:28Z",
    "updated_at": "2024-11-23T09:09:21Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and found no similar bug report.\n\n\n### YOLOv5 Component\n\nTraining\n\n### Bug\n\n```python\r\n# utils/metrics.py\r\ndef fitness(x):\r\n    # Model fitness as a weighted combination of metrics\r\n    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\r\n    P=x[0][0]\r\n    R=x[0][1]\r\n    map50=x[0][2]\r\n    map5095=x[0][12]\r\n    return (np.array([[P,R,map50,map5095]])*w).sum(1)\r\n```\r\n\r\nThe fitness function in utils/metrics.py determines if it is the best checkpoint by calculating the weighted sum of the four metrics, but the code seems to have x[:, :4] standing for [P, R, mAP@0.5,mAP@0.55] instead of the expected [P, R, mAP@0.5,mAP@0.5:0.95]\n\n### Environment\n\nNone\n\n### Minimal Reproducible Example\n\nNone\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @thgpddl, thank you for your interest in YOLOv5 ðŸš€!\n\nIt looks like you've come across a potential issue with the fitness function in the training component. If this is indeed a ðŸ› Bug Report, could you please provide a **minimum reproducible example**? This will be incredibly helpful for us to diagnose and address the issue effectively.\n\nAn Ultralytics engineer will review your issue soon and provide further assistance. In the meantime, make sure you've followed all the recommended steps in the installation guide, and that your environment meets the minimum requirements with Python, PyTorch, and any other dependencies correctly installed.\n\nYOLOv5 is designed to run in various environments like Notebooks with free GPU, Google Cloud, Amazon Deep Learning AMIs, and Docker. Ensure your current setup aligns with one of these configurations to prevent environment-specific issues.\n\nThank you for your patience and for helping us improve YOLOv5! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "Thank you for pointing this out. The fitness function should indeed calculate the weighted sum of the metrics as [P, R, mAP @thgpddl.5, mAP@0.5:0.95], with the weights specified as [0.0, 0.0, 0.1, 0.9]. Ensure that your indexing for these metrics is correct. If you are still encountering issues, please verify that you are using the latest version of YOLOv5 from the [repository](https://github.com/ultralytics/yolov5). If the problem persists, feel free to submit a pull request with your proposed fix."
      }
    ]
  },
  {
    "issue_number": 13428,
    "title": "What is the difference between \"SPPF/SPP -> C3\" and \"C3 -> SPPF/SPP\" in backbone ?",
    "author": "sunshanlu",
    "state": "open",
    "created_at": "2024-11-22T07:11:39Z",
    "updated_at": "2024-11-22T13:16:35Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I found that the last two layers of the backbone of the configuration file of version v5.0 of YOLOv5 use \"C3 -> SPP\", while the configuration file of version v6.0 uses \"C3-> SPPF\". I would like to ask what is the reason for this? Thank you.\r\n\r\n![image](https://github.com/user-attachments/assets/8f1386f2-1986-432e-971f-cea8849b534d)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @sunshanlu, thank you for your interest in YOLOv5 ðŸš€! This is an automated response to assist you while an Ultralytics engineer reviews your question.\n\nRegarding your query about the differences between \"SPPF/SPP -> C3\" and \"C3 -> SPPF/SPP\" in different YOLOv5 versions, this pertains to architectural changes aimed at improving model performance and efficiency. \n\nFor detailed explanations on updates and improvements between versions, you can refer to the YOLOv5 release notes and documentation in the repository. Meanwhile, if you have any specific questions about these layers or their configurations, feel free to provide more details, and an Ultralytics engineer will assist you soon. ðŸ˜Š\n\nIf this is a ðŸ”„ follow-up question, please include the context or examples that can help us provide a more accurate answer.\n\n## Requirements\n\nEnsure you are using Python>=3.8.0 with all necessary requirements installed, including PyTorch>=1.8. This ensures compatibility and access to all recent features and improvements.\n\nPlease follow the usual setup flow to ensure everything is properly configured for your experiments.\n\n## Environments\n\nYOLOv5 can be used in various environments, including cloud instances and local setups, each offering different advantages such as GPU acceleration.\n\n## Status\n\nCheck the latest Continuous Integration (CI) status for up-to-date verifications and test passes, which confirm the functionality of YOLOv5 across different operations and platforms.\n\nIf you need further assistance with your setup or experiments, feel free to reach out! ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@sunshanlu the change from \"C3 -> SPP\" in YOLOv5 v5.0 to \"C3 -> SPPF\" in v6.0 is primarily due to performance improvements. The SPPF (Spatial Pyramid Pooling - Fast) module replaces SPP to enhance processing speed while maintaining similar functionality. SPPF is designed to be more efficient, significantly reducing the processing time. You can find more details about this change in the [YOLOv5 architecture documentation](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/)."
      }
    ]
  },
  {
    "issue_number": 13425,
    "title": "using --rect  to train, mAP a little bit low",
    "author": "wzf19947",
    "state": "open",
    "created_at": "2024-11-21T10:19:18Z",
    "updated_at": "2024-11-22T04:34:16Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI trained with two models, the only difference is using --rect param or not , it turns out using --rect  to train, the mAP is much lower and grow up lower than don't use it, is is normal? \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @wzf19947, thank you for your interest in YOLOv5 ðŸš€! I see you've encountered an issue with using the `--rect` parameter during training. This is an automated response to acknowledge your issue, and an Ultralytics engineer will assist you soon.\n\nIf this is a ðŸ› Bug Report, please provide a **minimum reproducible example** to help us understand and debug it more effectively.\n\nFor questions related to custom training or parameter tuning, please provide as much detail as possible, such as dataset examples, training logs, and ensure you're following recommended practices to achieve the best training results.\n\n## Requirements\n\nEnsure you have Python>=3.8.0 with all dependencies installed, including PyTorch>=1.8. You can do this by cloning the YOLOv5 repository and installing the necessary requirements using `pip install -r requirements.txt`.\n\n## Environments\n\nYOLOv5 offers various verified environments such as Notebooks with free GPU access, Google Cloud's Deep Learning VM, Amazon's Deep Learning AMI, and Docker images, each with all dependencies, including CUDA, CUDNN, Python, and PyTorch, preinstalled.\n\n## Status\n\nCheck our Continuous Integration (CI) tests on GitHub Actions, which verify the correct operation of YOLOv5's training, validation, inference, export, and benchmarks on multiple platforms. If these tests are passing, YOLOv5 should operate correctly in its supported environments.\n\nFeel free to provide the additional information to assist us in resolving your query. ðŸ˜Š"
      },
      {
        "user": "pderrenger",
        "body": "@wzf19947 using the `--rect` parameter can result in lower mAP because it alters the aspect ratio of images during training, which might affect the model's learning efficiency. It's recommended to use rectangular training only when dealing with datasets where maintaining the original aspect ratio is crucial, such as when objects have a consistent orientation or size. Otherwise, consider training without `--rect` for potentially better mAP results. For more insights, refer to the [YOLOv5 documentation](https://github.com/ultralytics/yolov5)."
      }
    ]
  },
  {
    "issue_number": 13422,
    "title": "Why is there a sudden drop in accuracy?",
    "author": "lqh964165950",
    "state": "open",
    "created_at": "2024-11-19T14:11:25Z",
    "updated_at": "2024-11-21T12:02:00Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.\n\n\n### Question\n\nI have made some improvements in yolov5, but when training the improved modelï¼Œsometimes there is a sudden drop in mAP50, \r\nprecision and recall.I want to know why this happend.\r\n![Uploading æ›²çº¿.pngâ€¦]()\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "ðŸ‘‹ Hello @lqh964165950, thank you for reaching out about YOLOv5 ðŸš€! This is an automated response to help guide you through resolving your issue. An Ultralytics engineer will also assist you soon.\n\nFor your question regarding the sudden drop in mAP50, precision, and recall, here are some general considerations:\n\n- If this is a ðŸ› Bug Report, please ensure to provide a **minimum reproducible example** which includes the code modifications you've made and any unique data that could help us debug the problem.\n\n- If this is a custom training â“ Question, provide as much detail as possible. This might include dataset image examples, specific model changes, training logs, and verify you are following the best practices for achieving optimal model performance.\n\n## Requirements\n\nEnsure you have **Python>=3.8.0** installed and all necessary dependencies are included as per the requirements for YOLOv5.\n\n## Environments\n\nYOLOv5 can run in multiple environments, such as Notebooks with free GPU, Google Cloud Deep Learning VMs, Amazon Deep Learning AMIs, or using the Docker Image.\n\n## Status\n\nCheck the YOLOv5 CI (Continuous Integration) tests to ensure all operations like training, validation, inference, export, and benchmarks are functioning correctly. These tests occur every 24 hours and on every commit across different operating systems such as macOS, Windows, and Ubuntu.\n\nFeel free to follow these steps, and let us know if you need more specific support based on your detailed findings! ðŸ˜Š"
      },
      {
        "user": "lqh964165950",
        "body": "\r\n![plot](https://github.com/user-attachments/assets/c61b1431-3d1c-4748-a9b3-558a912a48e9)\r\n"
      },
      {
        "user": "pderrenger",
        "body": "@lqh964165950 a sudden drop in mAP, precision, and recall during training can be due to several factors such as changes in dataset quality, learning rate fluctuations, or model overfitting. Ensure your dataset annotations are accurate and consistent. You might also want to experiment with different learning rates or utilize techniques like data augmentation to improve model generalization. If the issue persists, consider using pretrained weights as a starting point to stabilize training. You can find more details on these topics in the [YOLO Common Issues guide](https://docs.ultralytics.com/guides/yolo-common-issues/)."
      }
    ]
  }
]